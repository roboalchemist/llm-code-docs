# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

**See also:** [AGENTS.md](./AGENTS.md) for quick guidance on finding and using documentation in this repository.
**See also:** [auto-doc GOALS](https://gitea.roboalch.com/roboalchemist/auto-doc/src/branch/master/GOALS.md) for pipeline quality principles.

## Project Overview

This repository provides centralized, AI-readable documentation extracted from 500+ frameworks, libraries, and developer tools. It includes automated extraction tools and a pipeline (`auto_doc.py`) that keeps documentation current with upstream sources.

**Purpose**: Serve as a comprehensive documentation hub optimized for LLM consumption and AI-assisted development.

## Repository Architecture

### Three-Tier Documentation Structure

1. **llms.txt-Compliant Documentation** (`docs/llms-txt/`) - **HIGHEST PRIORITY**
   - **343 sites** following the llms.txt standard (https://llmstxt.org/)
   - **348 sites** registered in `scripts/llms-sites.yaml`
   - Each site in its own subdirectory: `docs/llms-txt/{site-name}/`
   - Parallel downloads with 15 concurrent workers
   - File-level caching with 23-hour freshness window
   - Examples: `docs/llms-txt/anthropic/`, `docs/llms-txt/vercel-ai-sdk/`, `docs/llms-txt/bun/`

2. **Git-Based Documentation Extractions** (`docs/github-scraped/`)
   - **159 repositories** configured in `scripts/repo_config.yaml`
   - Each repo specifies: `repo_url`, `source_folder`, `target_folder`, `branch`
   - All target folders under `docs/github-scraped/`
   - Examples: `fastapi/`, `python-docs/`, `go-docs/`, `sqlalchemy/`, `react-router/`, `pytorch/`

3. **Web-Scraped Documentation** (`docs/web-scraped/`)
   - **138 documentation sets** from custom per-site scrapers
   - Each scraper is a standalone Python script in `scripts/`
   - Examples: `datadog-api/`, `electron/`, `express/`, `laravel/`, `storybook/`

### Configuration Files

- **`scripts/llms-sites.yaml`** - Central registry of 348 llms.txt-compliant sites
  - Structure: `name`, `base_url`, `description`, optional `rate_limit_seconds`
  - Alphabetically sorted by name
  - Used by `llms-txt-scraper.py` for bulk downloads

- **`scripts/repo_config.yaml`** - Git repository extraction config
  - Top-level keys: `settings`, `repositories`
  - 159 repositories with: `repo_url`, `source_folder`, `target_folder`, `branch`
  - All target folders under `docs/github-scraped/`

### Scripts Architecture

**Primary Tools:**
- `auto_doc.py` - **Unified pipeline CLI** (typer) — probe, fetch, validate, add, plow
- `llms-txt-scraper.py` - Bulk llms.txt downloader (parallel, cached)
- `validate-markdown.py` - Markdown quality validation
- `update-index.py` - Documentation index updater

**Per-Site Scrapers** (~170 scripts):
- Each named `{library}-docs.py` or `{library}-extract.py`
- Standalone Python scripts for web-scraped sources
- Generated by `auto_doc.py` pipeline or written manually

## auto_doc.py Pipeline

The main automation tool for adding new documentation sources. Integrates with trckr for ticket tracking.

```bash
# Probe a library — discover all doc sources, score them
python3 scripts/auto_doc.py probe <library-name>

# Fetch docs from a chosen source
python3 scripts/auto_doc.py fetch <library-name> --source-type llms-txt --url <url>

# Add a library end-to-end (probe → decide → fetch → clean → review → commit)
python3 scripts/auto_doc.py add <library-name>

# Batch-process DOCS tickets from trckr queue
python3 scripts/auto_doc.py plow [--limit N] [--dry-run]

# Validate markdown quality
python3 scripts/auto_doc.py validate

# Report current doc status
python3 scripts/auto_doc.py status
```

**Pipeline stages** (for `add` and `plow`):
1. **Probe** — Exa search (25 results), GitHub search (25 results), llms.txt probing
2. **Decide** — Deterministic rules pick best source (llms-txt > github > web scraper > skip)
3. **Fetch** — Download from chosen source (llms-txt, github, or web scraper)
4. **Clean** — Markdownlint cleanup
5. **Review** — GLM-5 quality review (MANDATORY — never skip)
6. **Commit** — Git add and commit to master

## Common Development Workflows

### Update llms.txt Sites

```bash
# Update single site
python3 scripts/llms-txt-scraper.py --site anthropic

# Update multiple sites
python3 scripts/llms-txt-scraper.py --site vercel-ai-sdk --site langchain

# Force re-download (ignore 23hr cache)
python3 scripts/llms-txt-scraper.py --force

# Control parallelism
python3 scripts/llms-txt-scraper.py --workers 20

# Download modes
python3 scripts/llms-txt-scraper.py --mode full        # Only llms-full.txt
python3 scripts/llms-txt-scraper.py --mode individual  # Only individual .md files
python3 scripts/llms-txt-scraper.py --mode both        # Default: both files
```

### Add New Documentation (Automated)

```bash
# Fully automated — probes, decides, fetches, cleans, reviews, commits
python3 scripts/auto_doc.py add <library-name>

# Batch mode — processes tickets from trckr DOCS project queue
python3 scripts/auto_doc.py plow --limit 10
```

### Add New llms.txt Site (Manual)

1. Add to `scripts/llms-sites.yaml` in alphabetical order
2. Download: `python3 scripts/llms-txt-scraper.py --site new-site-name`
3. Verify: `ls -lh docs/llms-txt/new-site-name/`

## File-Level Caching System

The llms-txt scraper implements smart caching:

- **Cache Duration**: 23 hours (configurable in `is_file_recent()`)
- **Behavior**: Files downloaded within last 23 hours are skipped unless `--force` is used
- **Override**: Use `--force` flag to re-download all files regardless of age

## Git Workflow

- Default branch is **`master`** (not `main`)
- Push to `origin master`
- GitHub push protection may block pushes containing test API keys in documentation — redact and amend

**Common test credentials to redact:**
- Stripe test keys: `sk_test_[A-Za-z0-9]{24,}`
- Discord bot tokens: Long base64-like strings
- API proxy passwords: `auto_[a-z0-9]{32}`

## Repository Statistics

Current scale:
- **348 llms.txt sites** in YAML configuration
- **343 documentation directories** in `docs/llms-txt/`
- **159 Git-based repository extractions** in `docs/github-scraped/`
- **138 web-scraped documentation sets** in `docs/web-scraped/`
- **56,000+ markdown/RST files** across all sources
- **5.4GB+ total documentation** optimized for AI consumption

### Directory Structure
```
llm-code-docs/
├── docs/llms-txt/           # llms.txt standard docs (343 sites)
├── docs/github-scraped/     # Git repository extractions (159 repos)
├── docs/web-scraped/        # Custom web scrapers (138 sets)
└── scripts/                 # Pipeline CLI, scrapers, and config files
    ├── auto_doc.py          # Unified pipeline CLI
    ├── llms-txt-scraper.py  # Bulk llms.txt downloader
    ├── llms-sites.yaml      # llms.txt site registry
    ├── repo_config.yaml     # Git repo extraction config
    └── *-docs.py / *-extract.py  # Per-site web scrapers
```

## Key Design Principles

1. **Idempotent updates** - Re-running scripts is safe and efficient
2. **Fail-soft** - Individual site failures don't stop bulk operations
3. **Smart caching** - Avoid redundant downloads with time-based freshness checks
4. **Parallel execution** - Maximize throughput with concurrent workers
5. **Quality verification** - Post-download checks ensure complete content capture
6. **Git-friendly naming** - Descriptive folder names, not generic "docs-N" patterns
7. **llms.txt priority** - llms.txt sources are preferred over github-scraped or web-scraped
8. **Mandatory review** - GLM-5 review gate is never optional; correctness over throughput

Updated: 2026-02-20
