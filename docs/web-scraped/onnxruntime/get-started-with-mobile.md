# Source: https://onnxruntime.ai/docs/get-started/with-mobile.html

# [![](data:image/svg+xml;base64,PHN2ZyB2aWV3Ym94PSIwIDAgMTYgMTYiIGFyaWEtaGlkZGVuPSJ0cnVlIj48dXNlIHhsaW5rOmhyZWY9IiNzdmctbGluayIgLz48L3N2Zz4=)](#get-started-with-onnx-runtime-mobile) Get started with ONNX Runtime Mobile

ORT Mobile allows you to run model inferencing on mobile devices (iOS and Android).

## [![](data:image/svg+xml;base64,PHN2ZyB2aWV3Ym94PSIwIDAgMTYgMTYiIGFyaWEtaGlkZGVuPSJ0cnVlIj48dXNlIHhsaW5rOmhyZWY9IiNzdmctbGluayIgLz48L3N2Zz4=)](#reference) Reference

- [Install ONNX Runtime Mobile](/docs/install/#install-on-web-and-mobile)
- [Tutorials: Deploy on mobile](/docs/tutorials/mobile/)
- Build from source: [Android](/docs/build/android.html) / [iOS](/docs/build/ios.html)
- [ORT Mobile Operators](/docs/reference/operators/MobileOps.html)
- [Model Export Helpers](/docs/tutorials/mobile/helpers/)
- [ORT Format Model Runtime Optimization](/docs/performance/model-optimizations/ort-format-model-runtime-optimization.html)