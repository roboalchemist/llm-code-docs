# Introduction -

Source: https://docs.lambda.ai/education/

---

# Introduction [#](#introduction)

- [Using Multi-Instance GPU (MIG)](using-mig/)

## Generative AI (GAI) [#](#generative-ai-gai)

- [How to serve the FLUX.1 prompt-to-image models using Lambda Cloud on-demand instances](generative-ai/flux-prompt-to-image/)
- [Fine-tuning the Mochi video generation model on GH200](fine-tune-mochi-gh200/)

## Large language models (LLMs) [#](#large-language-models-llms)

- [Deploying a Llama 3 inference endpoint](large-language-models/deploying-a-llama-3-inference-endpoint/)
- [Deploying Llama 3.2 3B in a Kubernetes (K8s) cluster](large-language-models/k8s-ollama-llama-3-2/)
- [Using KubeAI to deploy Nous Research's Hermes 3 and other LLMs](large-language-models/kubeai-hermes-3/)
- [Serving Llama 3.1 405B on a Lambda 1-Click Cluster](large-language-models/serving-llama-3-1-405b/)
- [Serving the Llama 3.1 8B and 70B models using Lambda Cloud on-demand instances](large-language-models/serving-llama-3-1-docker/)
- [Running DeepSeek-R1 70B using Ollama](large-language-models/deepseek-r1-ollama/)
- [Running Nemotron 3 Nano using vLLM](large-language-models/deploying-nemotron-3-nano/)

## Linux usage and system administration [#](#linux-usage-and-system-administration)

- [Basic Linux commands and system administration](linux-usage/basic-linux-commands-and-system-administration/)
- [Configuring Software RAID](linux-usage/configuring-software-raid/)
- [Lambda Stack and recovery images](linux-usage/lambda-stack-and-recovery-images/)
- [Troubleshooting and debugging](linux-usage/troubleshooting-and-debugging/)
- [Using the Lambda bug report to troubleshoot your system](linux-usage/using-the-lambda-bug-report-to-troubleshoot-your-system/)
- [Using the nvidia-bug-report.log file to troubleshoot your system](linux-usage/using-the-nvidia-bug-report.log-file-to-troubleshoot-your-system/)

## Programming [#](#programming)

- [Virtual environments and Docker containers](programming/virtual-environments-containers/)
- [Running Hugging Face Transformers and Diffusers on an NVIDIA GH200 instance](running-huggingface-diffusers-transformers-gh200/)

## Scheduling and orchestration [#](#scheduling-and-orchestration)

- [Orchestrating AI workloads with dstack](scheduling-and-orchestration/orchestrating-workloads-with-dstack/)
- [Using SkyPilot to deploy a Kubernetes cluster](scheduling-and-orchestration/skypilot-deploy-kubernetes/)

## Benchmarking [#](#benchmarking)

- [Running a PyTorchÂ®-based benchmark on an NVIDIA GH200 instance](running-benchmark-gh200/)
