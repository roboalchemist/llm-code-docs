# Source: https://ml-explore.github.io/mlx/build/html/python/nn/_autosummary/mlx.nn.Dropout3d.html

[]

[[ ]](https://github.com/ml-explore/mlx "Source repository")

- [[ ] [.rst]](../../../_sources/python/nn/_autosummary/mlx.nn.Dropout3d.rst "Download source file")
- [ ] [.pdf]

[ ]

[]

# mlx.nn.Dropout3d

## Contents

- [[`Dropout3d`]](#mlx.nn.Dropout3d)

# mlx.nn.Dropout3d[\#](#mlx-nn-dropout3d "Link to this heading")

*[class][ ]*[[Dropout3d]][(]*[[p]][[:]][ ][[[float]](https://docs.python.org/3/library/functions.html#float "(in Python v3.14)")][ ][[=]][ ][[0.5]]*[)][\#](#mlx.nn.Dropout3d "Link to this definition")

:   Apply 3D channel-wise dropout during training.

    Randomly zero out entire channels independently with probability [\\(p\\)]. This layer expects the channels to be last, i.e., the input shape should be NDHWC or DHWC where: N is the batch dimension, D is the depth, H is the input image height, W is the input image width, and C is the number of input channels.

    The remaining channels are scaled by [\\(\\frac\\)] to maintain the expected value of each element. Unlike traditional dropout, which zeros individual entries, this layer zeros entire channels. This is often beneficial for convolutional layers processing 3D data, like in medical imaging or video processing.

    Parameters[:]

    :   **p** ([*float*](https://docs.python.org/3/library/functions.html#float "(in Python v3.14)")) -- Probability of zeroing a channel during training.

    Methods

    ::: pst-scrollable-table-container
    :::

[](mlx.nn.Dropout2d.html "previous page")

previous

mlx.nn.Dropout2d

[](mlx.nn.Embedding.html "next page")

next

mlx.nn.Embedding

Contents

- [[`Dropout3d`]](#mlx.nn.Dropout3d)

By MLX Contributors

Â© Copyright 2023, Apple.\