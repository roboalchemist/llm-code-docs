# Source: https://docs.vllm.ai/en/stable/models/hardware_supported_models/xpu/

[![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjAgMCAyNCAyNCI+PHBhdGggZD0iTTEwIDIwSDZWNGg3djVoNXYzLjFsMi0yVjhsLTYtNkg2Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDR6bTEwLjItN2MuMSAwIC4zLjEuNC4ybDEuMyAxLjNjLjIuMi4yLjYgMCAuOGwtMSAxLTIuMS0yLjEgMS0xYy4xLS4xLjItLjIuNC0uMm0wIDMuOUwxNC4xIDIzSDEydi0yLjFsNi4xLTYuMXoiPjwvcGF0aD48L3N2Zz4=)](https://github.com/vllm-project/vllm/edit/main/docs/models/hardware_supported_models/xpu.md "Edit this page")

# XPU - Intel¬Æ GPUs[¬∂](#xpu-intel-gpus "Permanent link") 

## Validated Hardware[¬∂](#validated-hardware "Permanent link")

  Hardware
  --------------------------------------------------------------------------------------------------------------------------------------------------
  [Intel¬Æ Arc‚Ñ¢ Pro B-Series Graphics](https://www.intel.com/content/www/us/en/products/docs/discrete-gpus/arc/workstations/b-series/overview.html)

## Supported Models[¬∂](#supported-models "Permanent link")

### Text-only Language Models[¬∂](#text-only-language-models "Permanent link")

  Model                                       Architecture                                       FP16   Dynamic FP8   MXFP4
  ------------------------------------------- -------------------------------------------------- ------ ------------- -------
  openai/gpt-oss-20b                          GPTForCausalLM                                                          ‚úÖ
  openai/gpt-oss-120b                         GPTForCausalLM                                                          ‚úÖ
  deepseek-ai/DeepSeek-R1-Distill-Llama-8B    LlamaForCausalLM                                   ‚úÖ     ‚úÖ            
  deepseek-ai/DeepSeek-R1-Distill-Qwen-14B    QwenForCausalLM                                    ‚úÖ     ‚úÖ            
  deepseek-ai/DeepSeek-R1-Distill-Qwen-32B    QwenForCausalLM                                    ‚úÖ     ‚úÖ            
  deepseek-ai/DeepSeek-R1-Distill-Llama-70B   LlamaForCausalLM                                   ‚úÖ     ‚úÖ            
  Qwen/Qwen2.5-72B-Instruct                   Qwen2ForCausalLM                                   ‚úÖ     ‚úÖ            
  Qwen/Qwen3-14B                              Qwen3ForCausalLM                                   ‚úÖ     ‚úÖ            
  Qwen/Qwen3-32B                              Qwen3ForCausalLM                                   ‚úÖ     ‚úÖ            
  Qwen/Qwen3-30B-A3B                          Qwen3ForCausalLM                                   ‚úÖ     ‚úÖ            
  Qwen/Qwen3-30B-A3B-GPTQ-Int4                Qwen3ForCausalLM                                   ‚úÖ     ‚úÖ            
  Qwen/Qwen3-coder-30B-A3B-Instruct           Qwen3ForCausalLM                                   ‚úÖ     ‚úÖ            
  Qwen/QwQ-32B                                QwenForCausalLM                                    ‚úÖ     ‚úÖ            
  deepseek-ai/DeepSeek-V2-Lite                DeepSeekForCausalLM                                ‚úÖ     ‚úÖ            
  meta-llama/Llama-3.1-8B-Instruct            LlamaForCausalLM                                   ‚úÖ     ‚úÖ            
  baichuan-inc/Baichuan2-13B-Chat             BaichuanForCausalLM                                ‚úÖ     ‚úÖ            
  THUDM/GLM-4-9B-chat                         GLMForCausalLM                                     ‚úÖ     ‚úÖ            
  THUDM/CodeGeex4-All-9B                      CodeGeexForCausalLM                                ‚úÖ     ‚úÖ            
  chuhac/TeleChat2-35B                        LlamaForCausalLM (TeleChat2 based on Llama arch)   ‚úÖ     ‚úÖ            
  01-ai/Yi1.5-34B-Chat                        YiForCausalLM                                      ‚úÖ     ‚úÖ            
  THUDM/CodeGeex4-All-9B                      CodeGeexForCausalLM                                ‚úÖ     ‚úÖ            
  deepseek-ai/DeepSeek-Coder-33B-base         DeepSeekCoderForCausalLM                           ‚úÖ     ‚úÖ            
  baichuan-inc/Baichuan2-13B-Chat             BaichuanForCausalLM                                ‚úÖ     ‚úÖ            
  meta-llama/Llama-2-13b-chat-hf              LlamaForCausalLM                                   ‚úÖ     ‚úÖ            
  THUDM/CodeGeex4-All-9B                      CodeGeexForCausalLM                                ‚úÖ     ‚úÖ            
  Qwen/Qwen1.5-14B-Chat                       QwenForCausalLM                                    ‚úÖ     ‚úÖ            
  Qwen/Qwen1.5-32B-Chat                       QwenForCausalLM                                    ‚úÖ     ‚úÖ            

### Multimodal Language Models[¬∂](#multimodal-language-models "Permanent link")

  Model                          Architecture                       FP16   Dynamic FP8   MXFP4
  ------------------------------ ---------------------------------- ------ ------------- -------
  OpenGVLab/InternVL3_5-8B       InternVLForConditionalGeneration   ‚úÖ     ‚úÖ            
  OpenGVLab/InternVL3_5-14B      InternVLForConditionalGeneration   ‚úÖ     ‚úÖ            
  OpenGVLab/InternVL3_5-38B      InternVLForConditionalGeneration   ‚úÖ     ‚úÖ            
  Qwen/Qwen2-VL-7B-Instruct      Qwen2VLForConditionalGeneration    ‚úÖ     ‚úÖ            
  Qwen/Qwen2.5-VL-72B-Instruct   Qwen2VLForConditionalGeneration    ‚úÖ     ‚úÖ            
  Qwen/Qwen2.5-VL-32B-Instruct   Qwen2VLForConditionalGeneration    ‚úÖ     ‚úÖ            
  THUDM/GLM-4v-9B                GLM4vForConditionalGeneration      ‚úÖ     ‚úÖ            
  openbmb/MiniCPM-V-4            MiniCPMVForConditionalGeneration   ‚úÖ     ‚úÖ            

### Embedding and Reranker Language Models[¬∂](#embedding-and-reranker-language-models "Permanent link")

  Model                     Architecture                     FP16   Dynamic FP8   MXFP4
  ------------------------- -------------------------------- ------ ------------- -------
  Qwen/Qwen3-Embedding-8B   Qwen3ForTextEmbedding            ‚úÖ     ‚úÖ            
  Qwen/Qwen3-Reranker-8B    Qwen3ForSequenceClassification   ‚úÖ     ‚úÖ            

‚úÖ Runs and optimized.\
üü® Runs and correct but not optimized to green yet.\
‚ùå Does not pass accuracy test or does not run.

[ [ ![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjAgMCAyNCAyNCI+PHBhdGggZD0iTTIxIDEzLjFjLS4xIDAtLjMuMS0uNC4ybC0xIDEgMi4xIDIuMSAxLTFjLjItLjIuMi0uNiAwLS44bC0xLjMtMS4zYy0uMS0uMS0uMi0uMi0uNC0uMm0tMS45IDEuOC02LjEgNlYyM2gyLjFsNi4xLTYuMXpNMTIuNSA3djUuMmw0IDIuNC0xIDFMMTEgMTNWN3pNMTEgMjEuOWMtNS4xLS41LTktNC44LTktOS45QzIgNi41IDYuNSAyIDEyIDJjNS4zIDAgOS42IDQuMSAxMCA5LjMtLjMtLjEtLjYtLjItMS0uMnMtLjcuMS0xIC4yQzE5LjYgNy4yIDE2LjIgNCAxMiA0Yy00LjQgMC04IDMuNi04IDggMCA0LjEgMy4xIDcuNSA3LjEgNy45bC0uMS4yeiI+PC9wYXRoPjwvc3ZnPg==) ] [November 27, 2025] ]