# Source: https://docs.vllm.ai/en/stable/examples/offline_inference/logits_processor/

[![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjAgMCAyNCAyNCI+PHBhdGggZD0iTTEwIDIwSDZWNGg3djVoNXYzLjFsMi0yVjhsLTYtNkg2Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDR6bTEwLjItN2MuMSAwIC4zLjEuNC4ybDEuMyAxLjNjLjIuMi4yLjYgMCAuOGwtMSAxLTIuMS0yLjEgMS0xYy4xLS4xLjItLjIuNC0uMm0wIDMuOUwxNC4xIDIzSDEydi0yLjFsNi4xLTYuMXoiPjwvcGF0aD48L3N2Zz4=)](https://github.com/vllm-project/vllm/edit/main/docs/examples/offline_inference/logits_processor.md "Edit this page")

# Logits Processor[¶](#logits-processor "Permanent link")

Source <https://github.com/vllm-project/vllm/tree/main/examples/offline_inference/logits_processor>.

## Custom[¶](#custom "Permanent link")

    # SPDX-License-Identifier: Apache-2.0
    # SPDX-FileCopyrightText: Copyright contributors to the vLLM project

    """This example demonstrates instantiating vLLM with a custom logits processor
    class object.

    For a basic example of implementing a custom logits processor, see
    the `DummyLogitsProcessor` implementation in `vllm/test_utils.py`.

    For testing purposes, a dummy logits processor is employed which, if
    `target_token` is passed as a keyword argument to `SamplingParams.extra_args`,
    will mask out all tokens except `target_token`.

    A batch is constructed with `temperature=0.0` and 50% of requests specifying
    `target_token`, and for these requests - and *only* these requests - we
    expect the `target_token` to be decoded in each step, yielding an output
    similar to that shown below:

    Generated Outputs:
    ------------------------------------------------------------
    Prompt:    'Hello, my name is'
    Output:    " ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '"
    ------------------------------------------------------------
    Prompt:    'The president of the United States is'
    Output:    " not a racist. He is a racist.\nHe's a racist because he"
    ------------------------------------------------------------
    Prompt:    'The capital of France is'
    Output:    ' also also also also also also also also also also also also also
                 also also also'
    ------------------------------------------------------------
    Prompt:    'The future of AI is'
    Output:    ' in the hands of the people.\n\nThe future of AI is in the'
    ------------------------------------------------------------
    """

    from typing import Any

    import torch

    from vllm import LLM, SamplingParams
    from vllm.config import VllmConfig
    from vllm.v1.sample.logits_processor import (
        BatchUpdate,
        LogitsProcessor,
    )
    from vllm.v1.sample.logits_processor.builtin import process_dict_updates

    # Hypothetical custom logits processor
    class DummyLogitsProcessor(LogitsProcessor):
        """Fake logit processor to support unit testing and examples"""

        @classmethod
        def validate_params(cls, params: SamplingParams):
            target_token: Any | None = params.extra_args and params.extra_args.get(
                "target_token"
            )
            if target_token is not None and not isinstance(target_token, int):
                raise ValueError(
                    f"target_token value   is not int"
                )

        def __init__(
            self, vllm_config: VllmConfig, device: torch.device, is_pin_memory: bool
        ):
            self.req_info: dict[int, int] = 

        def is_argmax_invariant(self) -> bool:
            return False

        def update_state(self, batch_update: BatchUpdate | None):
            def extract_extra_arg(params: SamplingParams) -> int | None:
                self.validate_params(params)
                return params.extra_args and params.extra_args.get("target_token")

            process_dict_updates(
                self.req_info,
                batch_update,
                # This function returns the LP's per-request state based on the
                # request details, or None if this LP does not apply to the
                # request.
                lambda params, _, __: extract_extra_arg(params),
            )

        def apply(self, logits: torch.Tensor) -> torch.Tensor:
            if not self.req_info:
                return logits

            # Save target values before modification
            cols = torch.tensor(
                list(self.req_info.values()), dtype=torch.long, device=logits.device
            )
            rows = torch.tensor(
                list(self.req_info.keys()), dtype=torch.long, device=logits.device
            )
            values_to_keep = logits[rows, cols].clone()

            # Mask all but target tokens
            logits[rows] = float("-inf")
            logits[rows, cols] = values_to_keep

            return logits

    # Sample prompts.
    prompts = [
        "Hello, my name is",
        "The president of the United States is",
        "The capital of France is",
        "The future of AI is",
    ]
    # Create a mixture of requests which do and don't utilize the dummy logitproc
    sampling_params_list = [
        SamplingParams(temperature=0.0, extra_args=),
        SamplingParams(temperature=0.0),
        SamplingParams(temperature=0.0, extra_args=),
        SamplingParams(temperature=0.0),
    ]

    def main():
        # Create an LLM.
        llm = LLM(
            model="facebook/opt-125m",
            logits_processors=[DummyLogitsProcessor],
        )
        # Generate texts from the prompts.
        # The output is a list of RequestOutput objects
        # that contain the prompt, generated text, and other information.
        outputs = llm.generate(prompts, sampling_params_list)
        # Print the outputs.
        print("\nGenerated Outputs:\n" + "-" * 60)
        for output in outputs:
            prompt = output.prompt
            generated_text = output.outputs[0].text
            print(f"Prompt:    ")
            print(f"Output:    ")
            print("-" * 60)

    if __name__ == "__main__":
        main()

## Custom Req[¶](#custom-req "Permanent link")

    # SPDX-License-Identifier: Apache-2.0
    # SPDX-FileCopyrightText: Copyright contributors to the vLLM project

    """This example demonstrates wrapping a request-level logits processor to be
    compatible with vLLM's batch-level logits processing

    For demo purposes, a dummy logits processor is employed which, if
    `target_token` is passed as a keyword argument to `SamplingParams.extra_args`,
    will mask out all tokens except `target_token`. This logits processor can be
    applied to a vector of logits associated with a single decode step for a single
    request. The logits processor cannot be applied to a request which does not
    pass in a `target_token` custom argument.

    The request-level dummy logits processor is wrapped to create a batch-level
    logits processor, which can apply the logits processor to output logits from
    all requests in the persistent batch in a given decode step. For requests which
    do not provide a `target_token` argument, the corresponding row of `logits`
    will not be modified.

    A batch is constructed with `temperature=0.0` and 50% of requests specifying
    `target_token`, and for these requests - and *only* these requests - we
    expect the `target_token` to be decoded in each step, yielding an output
    similar to that shown below:

    Generated Outputs:
    ------------------------------------------------------------
    Prompt:    'Hello, my name is'
    Output:    " ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '"
    ------------------------------------------------------------
    Prompt:    'The president of the United States is'
    Output:    " not a racist. He is a racist.\nHe's a racist because he"
    ------------------------------------------------------------
    Prompt:    'The capital of France is'
    Output:    ' also also also also also also also also also also also also also
                 also also also'
    ------------------------------------------------------------
    Prompt:    'The future of AI is'
    Output:    ' in the hands of the people.\n\nThe future of AI is in the'
    ------------------------------------------------------------
    """

    from typing import Any

    import torch

    from vllm import LLM, SamplingParams
    from vllm.logger import init_logger
    from vllm.v1.sample.logits_processor import (
        AdapterLogitsProcessor,
        RequestLogitsProcessor,
    )

    logger = init_logger(__name__)

    class DummyPerReqLogitsProcessor:
        """The request-level logits processor masks out all logits except the
        token id identified by `target_token`"""

        def __init__(self, target_token: int) -> None:
            """Specify `target_token`"""
            self.target_token = target_token

        def __call__(
            self,
            output_ids: list[int],
            logits: torch.Tensor,
        ) -> torch.Tensor:
            val_to_keep = logits[self.target_token].item()
            logits[:] = float("-inf")
            logits[self.target_token] = val_to_keep
            return logits

    class WrappedPerReqLogitsProcessor(AdapterLogitsProcessor):
        """Example of wrapping a fake request-level logit processor to create a
        batch-level logits processor"""

        @classmethod
        def validate_params(cls, params: SamplingParams):
            target_token: Any | None = params.extra_args and params.extra_args.get(
                "target_token"
            )
            if target_token is not None and not isinstance(target_token, int):
                raise ValueError(f"target_token value  is not int")

        def is_argmax_invariant(self) -> bool:
            return False

        def new_req_logits_processor(
            self,
            params: SamplingParams,
        ) -> RequestLogitsProcessor | None:
            """This method returns a new request-level logits processor, customized
            to the `target_token` value associated with a particular request.

            Returns None if the logits processor should not be applied to the
            particular request. To use the logits processor the request must have
            a "target_token" custom argument with an integer value.

            Args:
              params: per-request sampling params

            Returns:
              `Callable` request logits processor, or None
            """
            target_token: Any | None = params.extra_args and params.extra_args.get(
                "target_token"
            )
            if target_token is None:
                return None
            return DummyPerReqLogitsProcessor(target_token)

    # Sample prompts.
    prompts = [
        "Hello, my name is",
        "The president of the United States is",
        "The capital of France is",
        "The future of AI is",
    ]
    # Create a mixture of requests which do and don't utilize the dummy logitproc
    sampling_params_list = [
        SamplingParams(temperature=0.0, extra_args=),
        SamplingParams(temperature=0.0),
        SamplingParams(temperature=0.0, extra_args=),
        SamplingParams(temperature=0.0),
    ]

    def main():
        # Create an LLM.
        llm = LLM(
            model="facebook/opt-125m",
            logits_processors=[WrappedPerReqLogitsProcessor],
        )
        # Generate texts from the prompts.
        # The output is a list of RequestOutput objects
        # that contain the prompt, generated text, and other information.
        outputs = llm.generate(prompts, sampling_params_list)
        # Print the outputs.
        print("\nGenerated Outputs:\n" + "-" * 60)
        for output in outputs:
            prompt = output.prompt
            generated_text = output.outputs[0].text
            print(f"Prompt:    ")
            print(f"Output:    ")
            print("-" * 60)

    if __name__ == "__main__":
        main()

## Custom Req Init[¶](#custom-req-init "Permanent link")

    # SPDX-License-Identifier: Apache-2.0
    # SPDX-FileCopyrightText: Copyright contributors to the vLLM project

    """This example demonstrates a special case of wrapping a request-level logits
    processor, namely the case where it is necessary to utilize engine config or
    environment info passed to the constructor. The subclass must override the
    wrapper base class `__init__()` method to access the engine config, the device
    identifier, or the flag which indicates whether pinned memory is available.

    For demo purposes, a request-level dummy logits processor is employed which
    causes the same token (`target_token`) to be decoded in each step. The
    request-level dummy logits processor is wrapped to create a batch-level logits
    processor, which can apply the logits processor to output logits from all
    requests in the persistent batch in a given decode step.

    The wrapped dummy logits processor below models a scenario where we must
    disable the logits processor on non-"cuda" platforms. The wrapper base class
    `__init__()` is overridden in order to check this condition and set a flag.

    A batch is constructed with `temperature=0.0` and 50% of requests specifying
    `target_token`, and for these requests - and *only* these requests - we
    expect that on a "cuda" device the output will look something like:

    Generated Outputs:
    ------------------------------------------------------------
    Prompt:    'Hello, my name is'
    Output:    " ' ' ' ' ' ' ' ' ' ' ' ' ' ' ' '"
    ------------------------------------------------------------
    Prompt:    'The president of the United States is'
    Output:    " not a racist. He is a racist.\nHe's a racist because he"
    ------------------------------------------------------------
    Prompt:    'The capital of France is'
    Output:    ' also also also also also also also also also also also also also
                 also also also'
    ------------------------------------------------------------
    Prompt:    'The future of AI is'
    Output:    ' in the hands of the people.\n\nThe future of AI is in the'
    ------------------------------------------------------------

    which indicates that the logits processor is running. However, on a non-"cuda"
    device, the first and third requests would not repeat the same token.
    """

    import torch

    from vllm import LLM, SamplingParams
    from vllm.config import VllmConfig
    from vllm.logger import init_logger
    from vllm.v1.sample.logits_processor import (
        AdapterLogitsProcessor,
        RequestLogitsProcessor,
    )

    logger = init_logger(__name__)

    class DummyPerReqLogitsProcessor:
        """The request-level logits processor masks out all logits except the
        token id identified by `target_token`"""

        def __init__(self, target_token: int) -> None:
            """Specify `target_token`"""
            self.target_token = target_token

        def __call__(
            self,
            output_ids: list[int],
            logits: torch.Tensor,
        ) -> torch.Tensor:
            val_to_keep = logits[self.target_token].item()
            logits[:] = float("-inf")
            logits[self.target_token] = val_to_keep
            return logits

    class WrappedPerReqLogitsProcessor(AdapterLogitsProcessor):
        """Example of overriding the wrapper class `__init__()` in order to utilize
        info about the device type"""

        @classmethod
        def validate_params(cls, params: SamplingParams):
            target_token = params.extra_args and params.extra_args.get("target_token")
            if target_token is not None and not isinstance(target_token, int):
                raise ValueError(
                    f"`target_token` has to be an integer, got ."
                )

        def __init__(
            self, vllm_config: VllmConfig, device: torch.device, is_pin_memory: bool
        ):
            super().__init__(vllm_config, device, is_pin_memory)
            self.is_cuda = device.type == "cuda"

        def is_argmax_invariant(self) -> bool:
            return False

        def new_req_logits_processor(
            self,
            params: SamplingParams,
        ) -> RequestLogitsProcessor | None:
            """This method returns a new request-level logits processor, customized
            to the `target_token` value associated with a particular request.

            Returns None if the logits processor should not be applied to the
            particular request. To use the logits processor the request must have
            a "target_token" custom argument with an integer value, and the device
            must be "cuda"-type

            Args:
              params: per-request sampling params

            Returns:
              `Callable` request logits processor, or None
            """
            if (
                not self.is_cuda
                or (
                    target_token := params.extra_args
                    and params.extra_args.get("target_token")
                )
                is None
            ):
                return None
            return DummyPerReqLogitsProcessor(target_token)

    # Sample prompts.
    prompts = [
        "Hello, my name is",
        "The president of the United States is",
        "The capital of France is",
        "The future of AI is",
    ]
    # Create a mixture of requests which do and don't utilize the dummy logitproc
    sampling_params_list = [
        SamplingParams(temperature=0.0, extra_args=),
        SamplingParams(temperature=0.0),
        SamplingParams(temperature=0.0, extra_args=),
        SamplingParams(temperature=0.0),
    ]

    def main():
        # Create an LLM.
        llm = LLM(
            model="facebook/opt-125m",
            logits_processors=[WrappedPerReqLogitsProcessor],
        )
        # Generate texts from the prompts.
        # The output is a list of RequestOutput objects
        # that contain the prompt, generated text, and other information.
        outputs = llm.generate(prompts, sampling_params_list)
        # Print the outputs.
        print("\nGenerated Outputs:\n" + "-" * 60)
        for output in outputs:
            prompt = output.prompt
            generated_text = output.outputs[0].text
            print(f"Prompt:    ")
            print(f"Output:    ")
            print("-" * 60)

    if __name__ == "__main__":
        main()