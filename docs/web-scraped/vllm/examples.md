# Source: https://docs.vllm.ai/en/stable/examples/

[![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjAgMCAyNCAyNCI+PHBhdGggZD0iTTEwIDIwSDZWNGg3djVoNXYzLjFsMi0yVjhsLTYtNkg2Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDR6bTEwLjItN2MuMSAwIC4zLjEuNC4ybDEuMyAxLjNjLjIuMi4yLjYgMCAuOGwtMSAxLTIuMS0yLjEgMS0xYy4xLS4xLjItLjIuNC0uMm0wIDMuOUwxNC4xIDIzSDEydi0yLjFsNi4xLTYuMXoiPjwvcGF0aD48L3N2Zz4=)](https://github.com/vllm-project/vllm/edit/main/docs/examples/README.md "Edit this page")

# Examples[Â¶](#examples "Permanent link")

vLLM\'s examples are split into three categories:

-   If you are using vLLM from within Python code, see the *Offline Inference* section.
-   If you are using vLLM from an HTTP application or client, see the *Online Serving* section.
-   For examples of using some of vLLM\'s advanced features (e.g. LMCache or Tensorizer) which are not specific to either of the above use cases, see the *Others* section.