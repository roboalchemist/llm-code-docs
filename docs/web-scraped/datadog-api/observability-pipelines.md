# Source: https://docs.datadoghq.com/api/latest/observability-pipelines/

[Read the 2025 State of Containers and Serverless Report! Read the State of Containers and Serverless Report! ](https://www.datadoghq.com/state-of-containers-and-serverless/?utm_source=inbound&utm_medium=corpsite-display&utm_campaign=int-infra-ww-announcement-product-announcement-containers-serverless-2025)
  * [Product](https://www.datadoghq.com/product/)
The integrated platform for monitoring & security
[View Product Pricing](https://www.datadoghq.com/pricing/)
Observability
End-to-end, simplified visibility into your stack’s health & performance
Infrastructure
    * [Infrastructure Monitoring](https://www.datadoghq.com/product/infrastructure-monitoring/)
    * [Metrics](https://www.datadoghq.com/product/metrics/)
    * [Network Monitoring](https://www.datadoghq.com/product/network-monitoring/)
    * [Container Monitoring](https://www.datadoghq.com/product/container-monitoring/)
    * [Kubernetes Autoscaling](https://www.datadoghq.com/product/kubernetes-autoscaling/)
    * [Serverless](https://www.datadoghq.com/product/serverless-monitoring/)
    * [Cloud Cost Management](https://www.datadoghq.com/product/cloud-cost-management/)
    * [Cloudcraft](https://www.datadoghq.com/product/cloudcraft/)
    * [Storage Management](https://www.datadoghq.com/product/storage-management/)
Applications
    * [Application Performance Monitoring](https://www.datadoghq.com/product/apm/)
    * [Universal Service Monitoring](https://www.datadoghq.com/product/universal-service-monitoring/)
    * [Continuous Profiler](https://www.datadoghq.com/product/code-profiling/)
    * [Dynamic Instrumentation](https://www.datadoghq.com/product/dynamic-instrumentation/)
    * [LLM Observability](https://www.datadoghq.com/product/llm-observability/)
Data
    * [Database Monitoring](https://www.datadoghq.com/product/database-monitoring/)
    * [Data Streams Monitoring](https://www.datadoghq.com/product/data-streams-monitoring/)
    * [Quality Monitoring](https://www.datadoghq.com/product/data-observability/quality-monitoring/)
    * [Jobs Monitoring](https://www.datadoghq.com/product/data-observability/jobs-monitoring/)
Logs
    * [Log Management](https://www.datadoghq.com/product/log-management/)
    * [Sensitive Data Scanner](https://www.datadoghq.com/product/sensitive-data-scanner/)
    * [Audit Trail](https://www.datadoghq.com/product/audit-trail/)
    * [Observability Pipelines](https://www.datadoghq.com/product/observability-pipelines/)
    * [Error Tracking](https://www.datadoghq.com/product/error-tracking/)
    * [CloudPrem](https://www.datadoghq.com/product/cloudprem/)
Infrastructure
Applications
Data
Logs
Security
Detect, prioritize, and respond to threats in real-time
Code Security
    * [Code Security](https://www.datadoghq.com/product/code-security/)
    * [Software Composition Analysis](https://www.datadoghq.com/product/software-composition-analysis/)
    * [Static Code Analysis (SAST)](https://www.datadoghq.com/product/sast/)
    * [Runtime Code Analysis (IAST)](https://www.datadoghq.com/product/iast/)
    * [IaC Security](https://www.datadoghq.com/product/iac-security)
    * [Secret Scanning](https://www.datadoghq.com/product/secret-scanning/)
Cloud Security
    * [Cloud Security](https://www.datadoghq.com/product/cloud-security/)
    * [Cloud Security Posture Management](https://www.datadoghq.com/product/cloud-security/#posture-management)
    * [Cloud Infrastructure Entitlement Management](https://www.datadoghq.com/product/cloud-security/#entitlement-management)
    * [Vulnerability Management](https://www.datadoghq.com/product/cloud-security/#vulnerability-management)
    * [Compliance](https://www.datadoghq.com/product/cloud-security/#compliance)
Threat Management
    * [Cloud SIEM](https://www.datadoghq.com/product/cloud-siem/)
    * [Workload Protection](https://www.datadoghq.com/product/workload-protection/)
    * [App and API Protection](https://www.datadoghq.com/product/app-and-api-protection/)
    * [Sensitive Data Scanner](https://www.datadoghq.com/product/sensitive-data-scanner/)
Security Labs
    * [Security Labs Research](https://securitylabs.datadoghq.com/)
    * [Open Source Projects](https://opensource.datadoghq.com/)
Digital Experience
Optimize front-end performance and enhance user experiences
Digital Experience
    * [Browser Real User Monitoring](https://www.datadoghq.com/product/real-user-monitoring/)
    * [Mobile Real User Monitoring](https://www.datadoghq.com/product/real-user-monitoring/mobile-rum/)
    * [Product Analytics](https://www.datadoghq.com/product/product-analytics/)
    * [Session Replay](https://www.datadoghq.com/product/real-user-monitoring/session-replay/)
    * [Synthetic Monitoring](https://www.datadoghq.com/product/synthetic-monitoring/)
    * [Mobile App Testing](https://www.datadoghq.com/product/mobile-app-testing/)
    * [Error Tracking](https://www.datadoghq.com/product/error-tracking/)
Related Products
    * [Continuous Testing](https://www.datadoghq.com/product/continuous-testing/)
    * [Dashboards](https://www.datadoghq.com/product/platform/dashboards/)
    * [Application Performance Monitoring](https://www.datadoghq.com/product/apm/)
Software Delivery
Build, test, secure and ship quality code faster
Software Delivery
    * [Internal Developer Portal](https://www.datadoghq.com/product/internal-developer-portal/)
    * [CI Visibility](https://www.datadoghq.com/product/ci-cd-monitoring/)
    * [Test Optimization](https://www.datadoghq.com/product/test-optimization/)
    * [Continuous Testing](https://www.datadoghq.com/product/continuous-testing/)
    * [IDE Plugins](https://www.datadoghq.com/product/platform/ides/)
    * [DORA Metrics](https://www.datadoghq.com/product/platform/dora-metrics/)
    * [Feature Flags](https://www.datadoghq.com/product/feature-flags/)
    * [Code Coverage](https://www.datadoghq.com/product/code-coverage/)
Related Products
    * [Software Composition Analysis](https://www.datadoghq.com/product/software-composition-analysis/)
    * [Application Performance Monitoring](https://www.datadoghq.com/product/apm/)
    * [Synthetic Monitoring](https://www.datadoghq.com/product/synthetic-monitoring/)
    * [Browser Real User Monitoring](https://www.datadoghq.com/product/real-user-monitoring/)
    * [Workflow Automation](https://www.datadoghq.com/product/workflow-automation/)
    * [integrations](https://www.datadoghq.com/product/platform/integrations/)
Service Management
Integrated, streamlined workflows for faster time-to-resolution
Service Management
    * [Incident Response](https://www.datadoghq.com/product/incident-response/)
    * [Software Catalog](https://www.datadoghq.com/product/software-catalog/)
    * [Service Level Objectives](https://www.datadoghq.com/product/service-level-objectives/)
    * [Case Management](https://www.datadoghq.com/product/case-management/)
Actions
    * [Workflow Automation](https://www.datadoghq.com/product/workflow-automation/)
    * [App Builder](https://www.datadoghq.com/product/app-builder/)
Agentic & Embedded
    * [Bits AI SRE](https://www.datadoghq.com/product/ai/bits-ai-sre/)
    * [Watchdog](https://www.datadoghq.com/product/platform/watchdog/)
    * [Event Management](https://www.datadoghq.com/product/event-management/)
AI
Monitor and improve model performance. Pinpoint root causes and detect anomalies
AI Observability
    * [LLM Observability](https://www.datadoghq.com/product/llm-observability/)
    * [AI Integrations](https://www.datadoghq.com/product/platform/integrations/#cat-aiml)
Agentic & Embedded
    * [Bits AI Agents](https://www.datadoghq.com/product/ai/bits-ai-agents/)
    * [Bits AI SRE](https://www.datadoghq.com/product/ai/bits-ai-sre/)
    * [Watchdog](https://www.datadoghq.com/product/platform/watchdog/)
    * [Event Management](https://www.datadoghq.com/product/event-management/)
Related Products
    * [Incident Response](https://www.datadoghq.com/product/incident-response/)
    * [Workflow Automation](https://www.datadoghq.com/product/workflow-automation/)
    * [Application Performance Monitoring](https://www.datadoghq.com/product/apm/)
    * [Universal Service Monitoring](https://www.datadoghq.com/product/universal-service-monitoring/)
    * [Log Management](https://www.datadoghq.com/product/log-management/)
Platform Capabilities
Built-in features & integrations that power the Datadog platform
Built-in Features
    * [Bits AI Agents](https://www.datadoghq.com/product/ai/bits-ai-agents/)
    * [Metrics](https://www.datadoghq.com/product/metrics/)
    * [Watchdog](https://www.datadoghq.com/product/platform/watchdog/)
    * [Alerts](https://www.datadoghq.com/product/platform/alerts/)
    * [Dashboards](https://www.datadoghq.com/product/platform/dashboards/)
    * [Notebooks](https://docs.datadoghq.com/notebooks/)
    * [Mobile App](https://docs.datadoghq.com/service_management/mobile/?tab=ios)
    * [Fleet Automation](https://www.datadoghq.com/product/fleet-automation/)
    * [Access Control](https://docs.datadoghq.com/account_management/rbac/?tab=datadogapplication)
    * [DORA Metrics](https://www.datadoghq.com/product/platform/dora-metrics/)
Workflows & Collaboration
    * [Incident Response](https://www.datadoghq.com/product/incident-response/)
    * [Case Management](https://www.datadoghq.com/product/case-management/)
    * [Event Management](https://www.datadoghq.com/product/event-management/)
    * [Workflow Automation](https://www.datadoghq.com/product/workflow-automation/)
    * [App Builder](https://www.datadoghq.com/product/app-builder/)
    * [Cloudcraft](https://www.datadoghq.com/product/cloudcraft/)
    * [CoScreen](https://www.datadoghq.com/product/coscreen/)
    * [Teams](https://docs.datadoghq.com/account_management/teams/)
Extensibility
    * [OpenTelemetry](https://www.datadoghq.com/solutions/opentelemetry/)
    * [integrations](https://www.datadoghq.com/product/platform/integrations/)
    * [IDE Plugins](https://www.datadoghq.com/product/platform/ides/)
    * [API](https://docs.datadoghq.com/api/)
    * [Marketplace](https://www.datadoghq.com/marketplacepartners/)
  * [Customers](https://www.datadoghq.com/customers/)
  * [Pricing](https://www.datadoghq.com/pricing/)
  * [Solutions](https://www.datadoghq.com/)
Industry
      * [Financial Services](https://www.datadoghq.com/solutions/financial-services/)
      * [Manufacturing & Logistics](https://www.datadoghq.com/solutions/manufacturing-logistics/)
      * [Healthcare/Life Sciences](https://www.datadoghq.com/solutions/healthcare/)
      * [Retail/E-Commerce](https://www.datadoghq.com/solutions/retail-ecommerce/)
      * [Government](https://www.datadoghq.com/solutions/government/)
      * [Education](https://www.datadoghq.com/solutions/education/)
      * [Media & Entertainment](https://www.datadoghq.com/solutions/media-entertainment/)
      * [Technology](https://www.datadoghq.com/solutions/technology/)
      * [Gaming](https://www.datadoghq.com/solutions/gaming/)
Technology
      * [Amazon Web Services Monitoring](https://www.datadoghq.com/solutions/aws/)
      * [Azure Monitoring](https://www.datadoghq.com/solutions/azure/)
      * [Google Cloud Monitoring](https://www.datadoghq.com/solutions/googlecloud/)
      * [Oracle Cloud Monitoring](https://www.datadoghq.com/solutions/oci-monitoring/)
      * [Kubernetes Monitoring](https://www.datadoghq.com/solutions/kubernetes/)
      * [Red Hat OpenShift](https://www.datadoghq.com/solutions/openshift/)
      * [Pivotal Platform](https://www.datadoghq.com/solutions/pivotal-platform/)
      * [OpenAI](https://www.datadoghq.com/solutions/openai/)
      * [SAP Monitoring](https://www.datadoghq.com/solutions/sap-monitoring/)
      * [OpenTelemetry](https://www.datadoghq.com/solutions/opentelemetry/)
Use-case
      * [Application Security](https://www.datadoghq.com/solutions/application-security/)
      * [Cloud Migration](https://www.datadoghq.com/solutions/cloud-migration/)
      * [Monitoring Consolidation](https://www.datadoghq.com/solutions/monitoring-consolidation/)
      * [Unified Commerce Monitoring](https://www.datadoghq.com/solutions/unified-commerce-monitoring/)
      * [SOAR](https://www.datadoghq.com/solutions/soar/)
      * [DevOps](https://www.datadoghq.com/solutions/devops/)
      * [FinOps](https://www.datadoghq.com/solutions/finops/)
      * [Shift-Left Testing](https://www.datadoghq.com/solutions/shift-left-testing/)
      * [Digital Experience Monitoring](https://www.datadoghq.com/solutions/digital-experience-monitoring/)
      * [Security Analytics](https://www.datadoghq.com/solutions/security-analytics/)
      * [Compliance for CIS Benchmarks](https://www.datadoghq.com/solutions/security/cis-benchmarks/aws/)
      * [Hybrid Cloud Monitoring](https://www.datadoghq.com/solutions/hybrid-cloud-monitoring/)
      * [IoT Monitoring](https://www.datadoghq.com/solutions/iot-monitoring/)
      * [Real-Time BI](https://www.datadoghq.com/solutions/real-time-business-intelligence/)
      * [On-Premises Monitoring](https://www.datadoghq.com/solutions/on-premises-monitoring/)
      * [Log Analysis & Correlation](https://www.datadoghq.com/solutions/log-analysis-and-correlation/)
      * [CNAPP](https://www.datadoghq.com/solutions/cnapp/)
  * [Docs](https://docs.datadoghq.com/)


[![DataDog](https://datadog-docs.imgix.net/img/dd_logo_n_70x75.png?ch=Width,DPR&fit=max&auto=format&w=70&h=75) ![DataDog](https://datadog-docs.imgix.net/img/dd-logo-n-200.png?ch=Width,DPR&fit=max&auto=format&h=14&auto=format&w=807) White modal up arrow Looking for Datadog logos? You can find the logo assets on our press page. Download Media Assets ](https://docs.datadoghq.com/)
  * [About](https://www.datadoghq.com/about/leadership/)
    * [Contact](https://www.datadoghq.com/about/contact/)
    * [Partners](https://www.datadoghq.com/partner/network/)
    * [Latest News](https://www.datadoghq.com/about/latest-news/press-releases/)
    * [Events & Webinars](https://www.datadoghq.com/events-webinars/)
    * [Leadership](https://www.datadoghq.com/about/leadership/)
    * [Careers](https://careers.datadoghq.com/)
    * [Analyst Reports](https://www.datadoghq.com/about/analyst/)
    * [Investor Relations](https://investors.datadoghq.com/)
    * [ESG Report](https://www.datadoghq.com/esg-report/)
    * [Trust Hub](https://www.datadoghq.com/trust/)
  * [Blog](https://www.datadoghq.com/blog/)
    * [The Monitor](https://www.datadoghq.com/blog/)
    * [Engineering](https://www.datadoghq.com/blog/engineering/)
    * [AI](https://www.datadoghq.com/blog/ai/)
    * [Security Labs](https://securitylabs.datadoghq.com/)
  * [Login](https://app.datadoghq.com/)
  * [](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [GET STARTED FREE FREE TRIAL](https://www.datadoghq.com/)


[![Datadog Logo](https://datadog-docs.imgix.net/img/datadog_rbg_n_2x.png?fm=png&auto=format&lossless=1)](https://docs.datadoghq.com/)
Toggle navigation
[ Home ](https://www.datadoghq.com)[ Docs ](https://docs.datadoghq.com/)[ API](https://docs.datadoghq.com/api/)
  * [Essentials ](https://docs.datadoghq.com/api/latest/observability-pipelines/)
    * [Getting Started](https://docs.datadoghq.com/getting_started/)
      * [Agent](https://docs.datadoghq.com/getting_started/agent/)
      * [API](https://docs.datadoghq.com/getting_started/api/)
      * [APM Tracing](https://docs.datadoghq.com/getting_started/tracing/)
      * [Containers](https://docs.datadoghq.com/getting_started/containers/)
        * [Autodiscovery](https://docs.datadoghq.com/getting_started/containers/autodiscovery)
        * [Datadog Operator](https://docs.datadoghq.com/getting_started/containers/datadog_operator)
      * [Dashboards](https://docs.datadoghq.com/getting_started/dashboards/)
      * [Database Monitoring](https://docs.datadoghq.com/getting_started/database_monitoring/)
      * [Datadog](https://docs.datadoghq.com/getting_started/application/)
      * [Datadog Site](https://docs.datadoghq.com/getting_started/site/)
      * [DevSecOps](https://docs.datadoghq.com/getting_started/devsecops)
      * [Incident Management](https://docs.datadoghq.com/getting_started/incident_management/)
      * [Integrations](https://docs.datadoghq.com/getting_started/integrations/)
        * [AWS](https://docs.datadoghq.com/getting_started/integrations/aws/)
        * [Azure](https://docs.datadoghq.com/getting_started/integrations/azure/)
        * [Google Cloud](https://docs.datadoghq.com/getting_started/integrations/google_cloud/)
        * [Terraform](https://docs.datadoghq.com/getting_started/integrations/terraform/)
      * [Internal Developer Portal](https://docs.datadoghq.com/getting_started/internal_developer_portal/)
      * [Logs](https://docs.datadoghq.com/getting_started/logs/)
      * [Monitors](https://docs.datadoghq.com/getting_started/monitors/)
      * [Notebooks](https://docs.datadoghq.com/getting_started/notebooks/)
      * [OpenTelemetry](https://docs.datadoghq.com/getting_started/opentelemetry/)
      * [Profiler](https://docs.datadoghq.com/getting_started/profiler/)
      * [Search](https://docs.datadoghq.com/getting_started/search/)
        * [Product-Specific Search](https://docs.datadoghq.com/getting_started/search/product_specific_reference)
      * [Session Replay](https://docs.datadoghq.com/getting_started/session_replay/)
      * [Security](https://docs.datadoghq.com/getting_started/security/)
        * [App and API Protection](https://docs.datadoghq.com/getting_started/security/application_security)
        * [Cloud Security](https://docs.datadoghq.com/getting_started/security/cloud_security_management/)
        * [Cloud SIEM](https://docs.datadoghq.com/getting_started/security/cloud_siem/)
        * [Code Security](https://docs.datadoghq.com/getting_started/code_security/)
      * [Serverless for AWS Lambda](https://docs.datadoghq.com/getting_started/serverless/)
      * [Software Delivery](https://docs.datadoghq.com/getting_started/software_delivery/)
        * [CI Visibility](https://docs.datadoghq.com/getting_started/ci_visibility/)
        * [Feature Flags](https://docs.datadoghq.com/getting_started/feature_flags/)
        * [Test Optimization](https://docs.datadoghq.com/getting_started/test_optimization/)
        * [Test Impact Analysis](https://docs.datadoghq.com/getting_started/test_impact_analysis/)
      * [Synthetic Monitoring and Testing](https://docs.datadoghq.com/getting_started/synthetics/)
        * [API Tests](https://docs.datadoghq.com/getting_started/synthetics/api_test)
        * [Browser Tests](https://docs.datadoghq.com/getting_started/synthetics/browser_test)
        * [Mobile App Tests](https://docs.datadoghq.com/getting_started/synthetics/mobile_app_testing)
        * [Continuous Testing](https://docs.datadoghq.com/getting_started/continuous_testing/)
        * [Private Locations](https://docs.datadoghq.com/getting_started/synthetics/private_location)
      * [Tags](https://docs.datadoghq.com/getting_started/tagging/)
        * [Assigning Tags](https://docs.datadoghq.com/getting_started/tagging/assigning_tags)
        * [Unified Service Tagging](https://docs.datadoghq.com/getting_started/tagging/unified_service_tagging)
        * [Using Tags](https://docs.datadoghq.com/getting_started/tagging/using_tags)
      * [Workflow Automation](https://docs.datadoghq.com/getting_started/workflow_automation/)
      * [Learning Center](https://docs.datadoghq.com/getting_started/learning_center/)
      * [Support](https://docs.datadoghq.com/getting_started/support/)
    * [Glossary](https://docs.datadoghq.com/glossary/)
    * [Standard Attributes](https://docs.datadoghq.com/standard-attributes)
    * [Guides](https://docs.datadoghq.com/all_guides/)
    * [Agent](https://docs.datadoghq.com/agent/)
      * [Architecture](https://docs.datadoghq.com/agent/architecture/)
      * [IoT](https://docs.datadoghq.com/agent/iot/)
      * [Supported Platforms](https://docs.datadoghq.com/agent/supported_platforms/)
        * [AIX](https://docs.datadoghq.com/agent/supported_platforms/aix/)
        * [Linux](https://docs.datadoghq.com/agent/supported_platforms/linux/)
        * [Ansible](https://docs.datadoghq.com/agent/supported_platforms/ansible/)
        * [Chef](https://docs.datadoghq.com/agent/supported_platforms/chef/)
        * [Heroku](https://docs.datadoghq.com/agent/supported_platforms/heroku/)
        * [MacOS](https://docs.datadoghq.com/agent/supported_platforms/osx/)
        * [Puppet](https://docs.datadoghq.com/agent/supported_platforms/puppet/)
        * [SaltStack](https://docs.datadoghq.com/agent/supported_platforms/saltstack/)
        * [SCCM](https://docs.datadoghq.com/agent/supported_platforms/sccm/)
        * [Windows](https://docs.datadoghq.com/agent/supported_platforms/windows/)
        * [From Source](https://docs.datadoghq.com/agent/supported_platforms/source/)
      * [Log Collection](https://docs.datadoghq.com/agent/logs/)
        * [Log Agent tags](https://docs.datadoghq.com/agent/logs/agent_tags/)
        * [Advanced Configurations](https://docs.datadoghq.com/agent/logs/advanced_log_collection)
        * [Proxy](https://docs.datadoghq.com/agent/logs/proxy)
        * [Transport](https://docs.datadoghq.com/agent/logs/log_transport)
        * [Multi-Line Detection](https://docs.datadoghq.com/agent/logs/auto_multiline_detection)
      * [Configuration](https://docs.datadoghq.com/agent/configuration)
        * [Commands](https://docs.datadoghq.com/agent/configuration/agent-commands/)
        * [Configuration Files](https://docs.datadoghq.com/agent/configuration/agent-configuration-files/)
        * [Log Files](https://docs.datadoghq.com/agent/configuration/agent-log-files/)
        * [Status Page](https://docs.datadoghq.com/agent/configuration/agent-status-page/)
        * [Network Traffic](https://docs.datadoghq.com/agent/configuration/network/)
        * [Proxy Configuration](https://docs.datadoghq.com/agent/configuration/proxy/)
        * [FIPS Compliance](https://docs.datadoghq.com/agent/configuration/fips-compliance/)
        * [Dual Shipping](https://docs.datadoghq.com/agent/configuration/dual-shipping/)
        * [Secrets Management](https://docs.datadoghq.com/agent/configuration/secrets-management/)
      * [Fleet Automation](https://docs.datadoghq.com/agent/fleet_automation)
        * [Remote Agent Management](https://docs.datadoghq.com/agent/fleet_automation/remote_management)
      * [Troubleshooting](https://docs.datadoghq.com/agent/troubleshooting/)
        * [Container Hostname Detection](https://docs.datadoghq.com/agent/troubleshooting/hostname_containers/)
        * [Debug Mode](https://docs.datadoghq.com/agent/troubleshooting/debug_mode/)
        * [Agent Flare](https://docs.datadoghq.com/agent/troubleshooting/send_a_flare/)
        * [Agent Check Status](https://docs.datadoghq.com/agent/troubleshooting/agent_check_status/)
        * [NTP Issues](https://docs.datadoghq.com/agent/troubleshooting/ntp/)
        * [Permission Issues](https://docs.datadoghq.com/agent/troubleshooting/permissions/)
        * [Integrations Issues](https://docs.datadoghq.com/agent/troubleshooting/integrations/)
        * [Site Issues](https://docs.datadoghq.com/agent/troubleshooting/site/)
        * [Autodiscovery Issues](https://docs.datadoghq.com/agent/troubleshooting/autodiscovery/)
        * [Windows Container Issues](https://docs.datadoghq.com/agent/troubleshooting/windows_containers)
        * [Agent Runtime Configuration](https://docs.datadoghq.com/agent/troubleshooting/config)
        * [High CPU or Memory Consumption](https://docs.datadoghq.com/agent/troubleshooting/high_memory_usage/)
      * [Guides](https://docs.datadoghq.com/agent/guide/)
      * [Data Security](https://docs.datadoghq.com/data_security/agent/)
    * [Integrations](https://docs.datadoghq.com/integrations/)
      * [Guides](https://docs.datadoghq.com/integrations/guide/)
    * [Developers](https://docs.datadoghq.com/developers/)
      * [Authorization](https://docs.datadoghq.com/developers/authorization/)
        * [OAuth2 in Datadog](https://docs.datadoghq.com/developers/authorization/oauth2_in_datadog/)
        * [Authorization Endpoints](https://docs.datadoghq.com/developers/authorization/oauth2_endpoints/)
      * [DogStatsD](https://docs.datadoghq.com/developers/dogstatsd/)
        * [Datagram Format](https://docs.datadoghq.com/developers/dogstatsd/datagram_shell)
        * [Unix Domain Socket](https://docs.datadoghq.com/developers/dogstatsd/unix_socket)
        * [High Throughput Data](https://docs.datadoghq.com/developers/dogstatsd/high_throughput/)
        * [Data Aggregation](https://docs.datadoghq.com/developers/dogstatsd/data_aggregation/)
        * [DogStatsD Mapper](https://docs.datadoghq.com/developers/dogstatsd/dogstatsd_mapper/)
      * [Custom Checks](https://docs.datadoghq.com/developers/custom_checks/)
        * [Writing a Custom Agent Check](https://docs.datadoghq.com/developers/custom_checks/write_agent_check/)
        * [Writing a Custom OpenMetrics Check](https://docs.datadoghq.com/developers/custom_checks/prometheus/)
      * [Integrations](https://docs.datadoghq.com/developers/integrations/)
        * [Build an Integration with Datadog](https://docs.datadoghq.com/developers/integrations/build_integration/)
        * [Create an Agent-based Integration](https://docs.datadoghq.com/developers/integrations/agent_integration/)
        * [Create an API-based Integration](https://docs.datadoghq.com/developers/integrations/api_integration/)
        * [Create a Log Pipeline](https://docs.datadoghq.com/developers/integrations/log_pipeline/)
        * [Integration Assets Reference](https://docs.datadoghq.com/developers/integrations/check_references/)
        * [Build a Marketplace Offering](https://docs.datadoghq.com/developers/integrations/marketplace_offering/)
        * [Create an Integration Dashboard](https://docs.datadoghq.com/developers/integrations/create-an-integration-dashboard)
        * [Create a Monitor Template](https://docs.datadoghq.com/developers/integrations/create-an-integration-monitor-template)
        * [Create a Cloud SIEM Detection Rule](https://docs.datadoghq.com/developers/integrations/create-a-cloud-siem-detection-rule)
        * [Install Agent Integration Developer Tool](https://docs.datadoghq.com/developers/integrations/python/)
      * [Service Checks](https://docs.datadoghq.com/developers/service_checks/)
        * [Submission - Agent Check](https://docs.datadoghq.com/developers/service_checks/agent_service_checks_submission/)
        * [Submission - DogStatsD](https://docs.datadoghq.com/developers/service_checks/dogstatsd_service_checks_submission/)
        * [Submission - API](https://docs.datadoghq.com/api/v1/service-checks/)
      * [IDE Plugins](https://docs.datadoghq.com/developers/ide_plugins/)
        * [JetBrains IDEs](https://docs.datadoghq.com/developers/ide_plugins/idea/)
        * [VS Code & Cursor](https://docs.datadoghq.com/developers/ide_plugins/vscode/)
      * [Community](https://docs.datadoghq.com/developers/community/)
        * [Libraries](https://docs.datadoghq.com/developers/community/libraries/)
      * [Guides](https://docs.datadoghq.com/developers/guide/)
    * [OpenTelemetry](https://docs.datadoghq.com/opentelemetry/)
      * [Getting Started](https://docs.datadoghq.com/opentelemetry/getting_started/)
        * [Datadog Example Application](https://docs.datadoghq.com/opentelemetry/getting_started/datadog_example)
        * [OpenTelemetry Demo Application](https://docs.datadoghq.com/opentelemetry/getting_started/otel_demo_to_datadog)
      * [Feature Compatibility](https://docs.datadoghq.com/opentelemetry/compatibility/)
      * [Instrument Your Applications](https://docs.datadoghq.com/opentelemetry/instrument/)
        * [OTel SDKs](https://docs.datadoghq.com/opentelemetry/instrument/otel_sdks/)
        * [OTel APIs with Datadog SDKs](https://docs.datadoghq.com/opentelemetry/instrument/api_support)
        * [OTel Instrumentation Libraries](https://docs.datadoghq.com/opentelemetry/instrument/instrumentation_libraries/)
        * [Configuration](https://docs.datadoghq.com/opentelemetry/config/environment_variable_support/)
      * [Send Data to Datadog](https://docs.datadoghq.com/opentelemetry/setup/)
        * [DDOT Collector (Recommended)](https://docs.datadoghq.com/opentelemetry/setup/ddot_collector)
        * [Other Setup Options](https://docs.datadoghq.com/opentelemetry/setup/)
      * [Semantic Mapping](https://docs.datadoghq.com/opentelemetry/mapping/)
        * [Resource Attribute Mapping](https://docs.datadoghq.com/opentelemetry/mapping/semantic_mapping/)
        * [Metrics Mapping](https://docs.datadoghq.com/opentelemetry/mapping/metrics_mapping/)
        * [Infrastructure Host Mapping](https://docs.datadoghq.com/opentelemetry/mapping/host_metadata/)
        * [Hostname Mapping](https://docs.datadoghq.com/opentelemetry/mapping/hostname/)
        * [Service-entry Spans Mapping](https://docs.datadoghq.com/opentelemetry/mapping/service_entry_spans/)
      * [Ingestion Sampling](https://docs.datadoghq.com/opentelemetry/ingestion_sampling)
      * [Correlate Data](https://docs.datadoghq.com/opentelemetry/correlate/)
        * [Logs and Traces](https://docs.datadoghq.com/opentelemetry/correlate/logs_and_traces/)
        * [Metrics and Traces](https://docs.datadoghq.com/opentelemetry/correlate/metrics_and_traces/)
        * [RUM and Traces](https://docs.datadoghq.com/opentelemetry/correlate/rum_and_traces/)
        * [DBM and Traces](https://docs.datadoghq.com/opentelemetry/correlate/dbm_and_traces/)
      * [Integrations](https://docs.datadoghq.com/opentelemetry/integrations/)
        * [Apache Metrics](https://docs.datadoghq.com/opentelemetry/integrations/apache_metrics/)
        * [Apache Spark Metrics](https://docs.datadoghq.com/opentelemetry/integrations/spark_metrics/)
        * [Collector Health Metrics](https://docs.datadoghq.com/opentelemetry/integrations/collector_health_metrics/)
        * [Datadog Extension](https://docs.datadoghq.com/opentelemetry/integrations/datadog_extension/)
        * [Docker Metrics](https://docs.datadoghq.com/opentelemetry/integrations/docker_metrics/)
        * [HAProxy Metrics](https://docs.datadoghq.com/opentelemetry/integrations/haproxy_metrics/)
        * [Host Metrics](https://docs.datadoghq.com/opentelemetry/integrations/host_metrics/)
        * [IIS Metrics](https://docs.datadoghq.com/opentelemetry/integrations/iis_metrics/)
        * [Kafka Metrics](https://docs.datadoghq.com/opentelemetry/integrations/kafka_metrics/)
        * [Kubernetes Metrics](https://docs.datadoghq.com/opentelemetry/integrations/kubernetes_metrics/)
        * [MySQL Metrics](https://docs.datadoghq.com/opentelemetry/integrations/mysql_metrics/)
        * [NGINX Metrics](https://docs.datadoghq.com/opentelemetry/integrations/nginx_metrics/)
        * [Podman Metrics](https://docs.datadoghq.com/opentelemetry/integrations/podman_metrics/)
        * [Runtime Metrics](https://docs.datadoghq.com/opentelemetry/integrations/runtime_metrics/)
        * [Trace Metrics](https://docs.datadoghq.com/opentelemetry/integrations/trace_metrics/)
      * [Troubleshooting](https://docs.datadoghq.com/opentelemetry/troubleshooting/)
      * [Guides and Resources](https://docs.datadoghq.com/opentelemetry/guide)
        * [Produce Delta Temporality Metrics](https://docs.datadoghq.com/opentelemetry/guide/otlp_delta_temporality)
        * [Visualize Histograms as Heatmaps](https://docs.datadoghq.com/opentelemetry/guide/otlp_histogram_heatmaps)
        * [Migration Guides](https://docs.datadoghq.com/opentelemetry/migrate/)
      * [Reference](https://docs.datadoghq.com/opentelemetry/reference)
        * [Terms and Concepts](https://docs.datadoghq.com/opentelemetry/reference/concepts)
        * [Trace Context Propagation](https://docs.datadoghq.com/opentelemetry/reference/trace_context_propagation)
        * [Trace IDs](https://docs.datadoghq.com/opentelemetry/reference/trace_ids)
        * [OTLP Metric Types](https://docs.datadoghq.com/opentelemetry/reference/otlp_metric_types)
    * [Administrator's Guide](https://docs.datadoghq.com/administrators_guide/)
      * [Getting Started](https://docs.datadoghq.com/administrators_guide/getting_started/)
      * [Plan](https://docs.datadoghq.com/administrators_guide/plan/)
      * [Build](https://docs.datadoghq.com/administrators_guide/build/)
      * [Run](https://docs.datadoghq.com/administrators_guide/run/)
    * [API](https://docs.datadoghq.com/api/)
    * [Partners](https://docs.datadoghq.com/partners/)
    * [Datadog Mobile App](https://docs.datadoghq.com/mobile/)
      * [Enterprise Configuration](https://docs.datadoghq.com/mobile/enterprise_configuration)
      * [Datadog for Intune](https://docs.datadoghq.com/mobile/datadog_for_intune)
      * [Shortcut Configurations](https://docs.datadoghq.com/mobile/shortcut_configurations)
      * [Push Notifications](https://docs.datadoghq.com/mobile/push_notification)
      * [Widgets](https://docs.datadoghq.com/mobile/widgets)
      * [Guides](https://docs.datadoghq.com/mobile/guide)
    * [DDSQL Reference](https://docs.datadoghq.com/ddsql_reference/)
      * [Data Directory](https://docs.datadoghq.com/ddsql_reference/data_directory/)
    * [CoScreen](https://docs.datadoghq.com/coscreen/)
      * [Troubleshooting](https://docs.datadoghq.com/coscreen/troubleshooting)
    * [CoTerm](https://docs.datadoghq.com/coterm/)
      * [Install](https://docs.datadoghq.com/coterm/install)
      * [Using CoTerm](https://docs.datadoghq.com/coterm/usage)
      * [Configuration Rules](https://docs.datadoghq.com/coterm/rules)
    * [Remote Configuration](https://docs.datadoghq.com/remote_configuration)
    * [Cloudcraft (Standalone)](https://docs.datadoghq.com/cloudcraft/)
      * [Getting Started](https://docs.datadoghq.com/cloudcraft/getting-started/)
      * [Account Management](https://docs.datadoghq.com/cloudcraft/account-management/)
      * [Components: Common](https://docs.datadoghq.com/cloudcraft/components-common/)
      * [Components: Azure](https://docs.datadoghq.com/cloudcraft/components-azure/)
      * [Components: AWS](https://docs.datadoghq.com/cloudcraft/components-aws/)
      * [Advanced](https://docs.datadoghq.com/cloudcraft/advanced/)
      * [FAQ](https://docs.datadoghq.com/cloudcraft/faq/)
      * [API](https://docs.datadoghq.com/cloudcraft/api)
        * [AWS Accounts](https://docs.datadoghq.com/cloudcraft/api/aws-accounts/)
        * [Azure Accounts](https://docs.datadoghq.com/cloudcraft/api/azure-accounts/)
        * [Blueprints](https://docs.datadoghq.com/cloudcraft/api/blueprints/)
        * [Budgets](https://docs.datadoghq.com/cloudcraft/api/budgets/)
        * [Teams](https://docs.datadoghq.com/cloudcraft/api/teams/)
        * [Users](https://docs.datadoghq.com/cloudcraft/api/users/)
  * [In The App ](https://docs.datadoghq.com/api/latest/observability-pipelines/)
    * [Dashboards](https://docs.datadoghq.com/dashboards/)
      * [Configure](https://docs.datadoghq.com/dashboards/configure/)
      * [Dashboard List](https://docs.datadoghq.com/dashboards/list/)
      * [Widgets](https://docs.datadoghq.com/dashboards/widgets/)
        * [Configuration](https://docs.datadoghq.com/dashboards/widgets/configuration/)
        * [Widget Types](https://docs.datadoghq.com/dashboards/widgets/types/)
      * [Querying](https://docs.datadoghq.com/dashboards/querying/)
      * [Functions](https://docs.datadoghq.com/dashboards/functions/)
        * [Algorithms](https://docs.datadoghq.com/dashboards/functions/algorithms/)
        * [Arithmetic](https://docs.datadoghq.com/dashboards/functions/arithmetic/)
        * [Count](https://docs.datadoghq.com/dashboards/functions/count/)
        * [Exclusion](https://docs.datadoghq.com/dashboards/functions/exclusion/)
        * [Interpolation](https://docs.datadoghq.com/dashboards/functions/interpolation/)
        * [Rank](https://docs.datadoghq.com/dashboards/functions/rank/)
        * [Rate](https://docs.datadoghq.com/dashboards/functions/rate/)
        * [Regression](https://docs.datadoghq.com/dashboards/functions/regression/)
        * [Rollup](https://docs.datadoghq.com/dashboards/functions/rollup/)
        * [Smoothing](https://docs.datadoghq.com/dashboards/functions/smoothing/)
        * [Timeshift](https://docs.datadoghq.com/dashboards/functions/timeshift/)
        * [Beta](https://docs.datadoghq.com/dashboards/functions/beta/)
      * [Graph Insights](https://docs.datadoghq.com/dashboards/graph_insights)
        * [Metric Correlations](https://docs.datadoghq.com/dashboards/graph_insights/correlations/)
        * [Watchdog Explains](https://docs.datadoghq.com/dashboards/graph_insights/watchdog_explains/)
      * [Template Variables](https://docs.datadoghq.com/dashboards/template_variables/)
      * [Overlays](https://docs.datadoghq.com/dashboards/change_overlays/)
      * [Annotations](https://docs.datadoghq.com/dashboards/annotations/)
      * [Guides](https://docs.datadoghq.com/dashboards/guide/)
      * [Sharing](https://docs.datadoghq.com/dashboards/sharing/)
        * [Shared Dashboards](https://docs.datadoghq.com/dashboards/sharing/shared_dashboards)
        * [Share Graphs](https://docs.datadoghq.com/dashboards/sharing/graphs)
        * [Scheduled Reports](https://docs.datadoghq.com/dashboards/sharing/scheduled_reports)
    * [Notebooks](https://docs.datadoghq.com/notebooks/)
      * [Analysis Features](https://docs.datadoghq.com/notebooks/advanced_analysis/)
        * [Getting Started](https://docs.datadoghq.com/notebooks/advanced_analysis/getting_started/)
      * [Guides](https://docs.datadoghq.com/notebooks/guide)
    * [DDSQL Editor](https://docs.datadoghq.com/ddsql_editor/)
    * [Reference Tables](https://docs.datadoghq.com/reference_tables/)
    * [Sheets](https://docs.datadoghq.com/sheets/)
      * [Functions and Operators](https://docs.datadoghq.com/sheets/functions_operators/)
      * [Guides](https://docs.datadoghq.com/sheets/guide/)
    * [Monitors and Alerting](https://docs.datadoghq.com/monitors/)
      * [Draft Monitors](https://docs.datadoghq.com/monitors/draft/)
      * [Configure Monitors](https://docs.datadoghq.com/monitors/configuration/)
      * [Monitor Templates](https://docs.datadoghq.com/monitors/templates/)
      * [Monitor Types](https://docs.datadoghq.com/monitors/types/)
        * [Host](https://docs.datadoghq.com/monitors/types/host/)
        * [Metric](https://docs.datadoghq.com/monitors/types/metric/)
        * [Analysis](https://docs.datadoghq.com/monitors/types/analysis/)
        * [Anomaly](https://docs.datadoghq.com/monitors/types/anomaly/)
        * [APM](https://docs.datadoghq.com/monitors/types/apm/)
        * [Audit Trail](https://docs.datadoghq.com/monitors/types/audit_trail/)
        * [Change](https://docs.datadoghq.com/monitors/types/change-alert/)
        * [CI/CD & Test](https://docs.datadoghq.com/monitors/types/ci/)
        * [Cloud Cost](https://docs.datadoghq.com/monitors/types/cloud_cost/)
        * [Composite](https://docs.datadoghq.com/monitors/types/composite/)
        * [Database Monitoring](https://docs.datadoghq.com/monitors/types/database_monitoring/)
        * [Error Tracking](https://docs.datadoghq.com/monitors/types/error_tracking/)
        * [Event](https://docs.datadoghq.com/monitors/types/event/)
        * [Forecast](https://docs.datadoghq.com/monitors/types/forecasts/)
        * [Integration](https://docs.datadoghq.com/monitors/types/integration/)
        * [Live Process](https://docs.datadoghq.com/monitors/types/process/)
        * [Logs](https://docs.datadoghq.com/monitors/types/log/)
        * [Network](https://docs.datadoghq.com/monitors/types/network/)
        * [Cloud Network Monitoring](https://docs.datadoghq.com/monitors/types/cloud_network_monitoring/)
        * [NetFlow](https://docs.datadoghq.com/monitors/types/netflow/)
        * [Outlier](https://docs.datadoghq.com/monitors/types/outlier/)
        * [Process Check](https://docs.datadoghq.com/monitors/types/process_check/)
        * [Real User Monitoring](https://docs.datadoghq.com/monitors/types/real_user_monitoring/)
        * [Service Check](https://docs.datadoghq.com/monitors/types/service_check/)
        * [SLO Alerts](https://docs.datadoghq.com/monitors/types/slo/)
        * [Synthetic Monitoring](https://docs.datadoghq.com/monitors/types/synthetic_monitoring/)
        * [Watchdog](https://docs.datadoghq.com/monitors/types/watchdog/)
      * [Notifications](https://docs.datadoghq.com/monitors/notify/)
        * [Notification Rules](https://docs.datadoghq.com/monitors/notify/notification_rules/)
        * [Variables](https://docs.datadoghq.com/monitors/notify/variables/)
      * [Downtimes](https://docs.datadoghq.com/monitors/downtimes/)
        * [Examples](https://docs.datadoghq.com/monitors/downtimes/examples)
      * [Manage Monitors](https://docs.datadoghq.com/monitors/manage/)
        * [Search Monitors](https://docs.datadoghq.com/monitors/manage/search/)
        * [Check Summary](https://docs.datadoghq.com/monitors/manage/check_summary/)
      * [Monitor Status](https://docs.datadoghq.com/monitors/status/status_page)
        * [Status Graphs](https://docs.datadoghq.com/monitors/status/graphs)
        * [Status Events](https://docs.datadoghq.com/monitors/status/events)
      * [Monitor Settings](https://docs.datadoghq.com/monitors/settings/)
      * [Monitor Quality](https://docs.datadoghq.com/monitors/quality/)
      * [Guides](https://docs.datadoghq.com/monitors/guide/)
    * [Service Level Objectives](https://docs.datadoghq.com/service_level_objectives/)
      * [Monitor-based SLOs](https://docs.datadoghq.com/service_level_objectives/monitor/)
      * [Metric-based SLOs](https://docs.datadoghq.com/service_level_objectives/metric/)
      * [Time Slice SLOs](https://docs.datadoghq.com/service_level_objectives/time_slice/)
      * [Error Budget Alerts](https://docs.datadoghq.com/service_level_objectives/error_budget/)
      * [Burn Rate Alerts](https://docs.datadoghq.com/service_level_objectives/burn_rate/)
      * [Guides](https://docs.datadoghq.com/service_level_objectives/guide/)
    * [Metrics](https://docs.datadoghq.com/metrics/)
      * [Custom Metrics](https://docs.datadoghq.com/metrics/custom_metrics/)
        * [Metric Type Modifiers](https://docs.datadoghq.com/metrics/custom_metrics/type_modifiers/)
        * [Historical Metrics Ingestion](https://docs.datadoghq.com/metrics/custom_metrics/historical_metrics/)
        * [Submission - Agent Check](https://docs.datadoghq.com/metrics/custom_metrics/agent_metrics_submission/)
        * [Submission - DogStatsD](https://docs.datadoghq.com/metrics/custom_metrics/dogstatsd_metrics_submission/)
        * [Submission - Powershell](https://docs.datadoghq.com/metrics/custom_metrics/powershell_metrics_submission)
        * [Submission - API](https://docs.datadoghq.com/api/latest/metrics/#submit-metrics)
      * [OpenTelemetry Metrics](https://docs.datadoghq.com/metrics/open_telemetry/)
        * [OTLP Metric Types](https://docs.datadoghq.com/metrics/open_telemetry/otlp_metric_types)
        * [Query OpenTelemetry Metrics](https://docs.datadoghq.com/metrics/open_telemetry/query_metrics/)
      * [Metrics Types](https://docs.datadoghq.com/metrics/types/)
      * [Distributions](https://docs.datadoghq.com/metrics/distributions/)
      * [Overview](https://docs.datadoghq.com/metrics/overview/)
      * [Explorer](https://docs.datadoghq.com/metrics/explorer/)
        * [Metrics Units](https://docs.datadoghq.com/metrics/units/)
      * [Summary](https://docs.datadoghq.com/metrics/summary/)
      * [Volume](https://docs.datadoghq.com/metrics/volume/)
      * [Advanced Filtering](https://docs.datadoghq.com/metrics/advanced-filtering/)
      * [Nested Queries](https://docs.datadoghq.com/metrics/nested_queries/)
      * [Composite Metrics Queries](https://docs.datadoghq.com/metrics/composite_metrics_queries)
      * [Derived Metrics](https://docs.datadoghq.com/metrics/derived-metrics)
      * [Metrics Without Limits™](https://docs.datadoghq.com/metrics/metrics-without-limits/)
      * [Guides](https://docs.datadoghq.com/metrics/guide)
    * [Watchdog](https://docs.datadoghq.com/watchdog/)
      * [Alerts](https://docs.datadoghq.com/watchdog/alerts)
      * [Impact Analysis](https://docs.datadoghq.com/watchdog/impact_analysis/)
      * [RCA](https://docs.datadoghq.com/watchdog/rca/)
      * [Insights](https://docs.datadoghq.com/watchdog/insights)
      * [Faulty Deployment Detection](https://docs.datadoghq.com/watchdog/faulty_deployment_detection/)
      * [Faulty Cloud & SaaS API Detection](https://docs.datadoghq.com/watchdog/faulty_cloud_saas_api_detection)
    * [Bits AI](https://docs.datadoghq.com/bits_ai/)
      * [Bits AI SRE](https://docs.datadoghq.com/bits_ai/bits_ai_sre)
        * [Investigate issues](https://docs.datadoghq.com/bits_ai/bits_ai_sre/investigate_issues)
        * [Remediate issues](https://docs.datadoghq.com/bits_ai/bits_ai_sre/remediate_issues)
        * [Bits AI SRE integrations and settings](https://docs.datadoghq.com/bits_ai/bits_ai_sre/configure)
        * [Help Bits learn](https://docs.datadoghq.com/bits_ai/bits_ai_sre/help_bits_learn)
        * [Chat with Bits AI SRE](https://docs.datadoghq.com/bits_ai/bits_ai_sre/chat_bits_ai_sre)
      * [Bits AI Dev Agent](https://docs.datadoghq.com/bits_ai/bits_ai_dev_agent)
        * [Setup](https://docs.datadoghq.com/bits_ai/bits_ai_dev_agent/setup)
      * [Chat with Bits AI](https://docs.datadoghq.com/bits_ai/chat_with_bits_ai)
      * [MCP Server](https://docs.datadoghq.com/bits_ai/mcp_server)
    * [Internal Developer Portal](https://docs.datadoghq.com/internal_developer_portal/)
      * [Software Catalog](https://docs.datadoghq.com/internal_developer_portal/software_catalog/)
        * [Set Up](https://docs.datadoghq.com/internal_developer_portal/software_catalog/set_up)
        * [Entity Model](https://docs.datadoghq.com/internal_developer_portal/software_catalog/entity_model)
        * [Troubleshooting](https://docs.datadoghq.com/internal_developer_portal/software_catalog/troubleshooting)
      * [Scorecards](https://docs.datadoghq.com/internal_developer_portal/scorecards)
        * [Scorecard Configuration](https://docs.datadoghq.com/internal_developer_portal/scorecards/scorecard_configuration)
        * [Custom Rules](https://docs.datadoghq.com/internal_developer_portal/scorecards/custom_rules)
        * [Using Scorecards](https://docs.datadoghq.com/internal_developer_portal/scorecards/using_scorecards)
      * [Self-Service Actions](https://docs.datadoghq.com/internal_developer_portal/self_service_actions)
        * [Software Templates](https://docs.datadoghq.com/internal_developer_portal/self_service_actions/software_templates)
      * [Engineering Reports](https://docs.datadoghq.com/internal_developer_portal/eng_reports)
        * [Reliability Overview](https://docs.datadoghq.com/internal_developer_portal/eng_reports/reliability_overview)
        * [Scorecards Performance](https://docs.datadoghq.com/internal_developer_portal/eng_reports/scorecards_performance)
        * [DORA Metrics](https://docs.datadoghq.com/internal_developer_portal/eng_reports/dora_metrics)
        * [Custom Reports](https://docs.datadoghq.com/internal_developer_portal/eng_reports/custom_reports)
      * [Developer Homepage](https://docs.datadoghq.com/internal_developer_portal/developer_homepage)
      * [Campaigns](https://docs.datadoghq.com/internal_developer_portal/campaigns)
      * [External Provider Status](https://docs.datadoghq.com/internal_developer_portal/external_provider_status)
      * [Plugins](https://docs.datadoghq.com/internal_developer_portal/plugins)
      * [Integrations](https://docs.datadoghq.com/internal_developer_portal/integrations)
      * [Use Cases](https://docs.datadoghq.com/internal_developer_portal/use_cases)
        * [API Management](https://docs.datadoghq.com/internal_developer_portal/use_cases/api_management)
        * [Cloud Cost Management](https://docs.datadoghq.com/internal_developer_portal/use_cases/cloud_cost_management)
        * [App and API Protection](https://docs.datadoghq.com/internal_developer_portal/use_cases/appsec_management)
        * [Developer Onboarding](https://docs.datadoghq.com/internal_developer_portal/use_cases/dev_onboarding)
        * [Dependency Management](https://docs.datadoghq.com/internal_developer_portal/use_cases/dependency_management)
        * [Production Readiness](https://docs.datadoghq.com/internal_developer_portal/use_cases/production_readiness)
        * [Incident Response](https://docs.datadoghq.com/internal_developer_portal/use_cases/incident_response)
        * [CI Pipeline Visibility](https://docs.datadoghq.com/internal_developer_portal/use_cases/pipeline_visibility)
      * [Onboarding Guide](https://docs.datadoghq.com/internal_developer_portal/onboarding_guide)
    * [Error Tracking](https://docs.datadoghq.com/error_tracking/)
      * [Explorer](https://docs.datadoghq.com/error_tracking/explorer)
      * [Issue States](https://docs.datadoghq.com/error_tracking/issue_states)
      * [Regression Detection](https://docs.datadoghq.com/error_tracking/regression_detection)
      * [Suspected Causes](https://docs.datadoghq.com/error_tracking/suspected_causes)
      * [Error Grouping](https://docs.datadoghq.com/error_tracking/error_grouping)
      * [Bits AI Dev Agent](https://docs.datadoghq.com/bits_ai/bits_ai_dev_agent)
      * [Monitors](https://docs.datadoghq.com/error_tracking/monitors)
      * [Issue Correlation](https://docs.datadoghq.com/error_tracking/issue_correlation)
      * [Identify Suspect Commits](https://docs.datadoghq.com/error_tracking/suspect_commits)
      * [Auto Assign](https://docs.datadoghq.com/error_tracking/auto_assign)
      * [Issue Team Ownership](https://docs.datadoghq.com/error_tracking/issue_team_ownership)
      * [Track Browser and Mobile Errors](https://docs.datadoghq.com/error_tracking/frontend)
        * [Browser Error Tracking](https://docs.datadoghq.com/error_tracking/frontend/browser)
        * [Collecting Browser Errors](https://docs.datadoghq.com/error_tracking/frontend/collecting_browser_errors)
        * [Mobile Crash Tracking](https://docs.datadoghq.com/error_tracking/frontend/mobile)
        * [Replay Errors](https://docs.datadoghq.com/error_tracking/frontend/replay_errors)
        * [Real User Monitoring](https://docs.datadoghq.com/error_tracking/rum)
        * [Logs](https://docs.datadoghq.com/error_tracking/frontend/logs)
      * [Track Backend Errors](https://docs.datadoghq.com/error_tracking/backend)
        * [Getting Started](https://docs.datadoghq.com/error_tracking/backend/getting_started)
        * [Exception Replay](https://docs.datadoghq.com/error_tracking/backend/exception_replay)
        * [Capturing Handled Errors](https://docs.datadoghq.com/error_tracking/backend/capturing_handled_errors)
        * [APM](https://docs.datadoghq.com/error_tracking/apm)
        * [Logs](https://docs.datadoghq.com/error_tracking/backend/logs)
      * [Manage Data Collection](https://docs.datadoghq.com/error_tracking/manage_data_collection)
      * [Troubleshooting](https://docs.datadoghq.com/error_tracking/troubleshooting)
      * [Guides](https://docs.datadoghq.com/error_tracking/guides)
    * [Change Tracking](https://docs.datadoghq.com/change_tracking/)
      * [Feature Flags](https://docs.datadoghq.com/change_tracking/feature_flags/)
    * [Event Management](https://docs.datadoghq.com/events/)
      * [Ingest Events](https://docs.datadoghq.com/events/ingest/)
      * [Pipelines and Processors](https://docs.datadoghq.com/events/pipelines_and_processors/)
        * [Aggregation Key Processor](https://docs.datadoghq.com/events/pipelines_and_processors/aggregation_key)
        * [Arithmetic Processor](https://docs.datadoghq.com/events/pipelines_and_processors/arithmetic_processor)
        * [Date Remapper](https://docs.datadoghq.com/events/pipelines_and_processors/date_remapper)
        * [Category Processor](https://docs.datadoghq.com/events/pipelines_and_processors/category_processor)
        * [Grok Parser](https://docs.datadoghq.com/events/pipelines_and_processors/grok_parser)
        * [Lookup Processor](https://docs.datadoghq.com/events/pipelines_and_processors/lookup_processor)
        * [Remapper](https://docs.datadoghq.com/events/pipelines_and_processors/remapper)
        * [Service Remapper](https://docs.datadoghq.com/events/pipelines_and_processors/service_remapper)
        * [Status Remapper](https://docs.datadoghq.com/events/pipelines_and_processors/status_remapper)
        * [String Builder Processor](https://docs.datadoghq.com/events/pipelines_and_processors/string_builder_processor)
      * [Explorer](https://docs.datadoghq.com/events/explorer/)
        * [Searching](https://docs.datadoghq.com/events/explorer/searching)
        * [Navigate the Explorer](https://docs.datadoghq.com/events/explorer/navigate)
        * [Customization](https://docs.datadoghq.com/events/explorer/customization)
        * [Facets](https://docs.datadoghq.com/events/explorer/facets)
        * [Attributes](https://docs.datadoghq.com/events/explorer/attributes)
        * [Notifications](https://docs.datadoghq.com/events/explorer/notifications)
        * [Analytics](https://docs.datadoghq.com/events/explorer/analytics)
        * [Saved Views](https://docs.datadoghq.com/events/explorer/saved_views)
      * [Triage Inbox](https://docs.datadoghq.com/events/triage_inbox)
      * [Correlation](https://docs.datadoghq.com/events/correlation/)
        * [Configuration](https://docs.datadoghq.com/events/correlation/configuration)
        * [Triaging & Notifying](https://docs.datadoghq.com/events/correlation/triage_and_notify)
        * [Analytics](https://docs.datadoghq.com/events/correlation/analytics)
      * [Guides](https://docs.datadoghq.com/events/guides/)
  * [Incident Response ](https://docs.datadoghq.com/api/latest/observability-pipelines/)
    * [Incident Management](https://docs.datadoghq.com/incident_response/incident_management/)
      * [Declare an Incident](https://docs.datadoghq.com/incident_response/incident_management/declare)
      * [Describe an Incident](https://docs.datadoghq.com/incident_response/incident_management/describe)
      * [Response Team](https://docs.datadoghq.com/incident_response/incident_management/response_team)
      * [Notification](https://docs.datadoghq.com/incident_response/incident_management/notification)
      * [Investigate an Incident](https://docs.datadoghq.com/incident_response/incident_management/investigate)
        * [Timeline](https://docs.datadoghq.com/incident_response/incident_management/investigate/timeline)
        * [Follow-ups](https://docs.datadoghq.com/incident_response/incident_management/follow-ups)
      * [Incident AI](https://docs.datadoghq.com/incident_response/incident_management/incident_ai)
      * [Incident Settings](https://docs.datadoghq.com/incident_response/incident_management/incident_settings)
        * [Information](https://docs.datadoghq.com/incident_response/incident_management/incident_settings/information)
        * [Property Fields](https://docs.datadoghq.com/incident_response/incident_management/incident_settings/property_fields)
        * [Responder Types](https://docs.datadoghq.com/incident_response/incident_management/incident_settings/responder_types)
        * [Integrations](https://docs.datadoghq.com/incident_response/incident_management/incident_settings/integrations)
        * [Notification Rules](https://docs.datadoghq.com/incident_response/incident_management/incident_settings/notification_rules)
        * [Templates](https://docs.datadoghq.com/incident_response/incident_management/incident_settings/templates)
      * [Incident Analytics](https://docs.datadoghq.com/incident_response/incident_management/analytics)
      * [Integrations](https://docs.datadoghq.com/incident_response/incident_management/integrations)
        * [Slack](https://docs.datadoghq.com/incident_response/incident_management/integrations/slack)
        * [Microsoft Teams](https://docs.datadoghq.com/incident_response/incident_management/integrations/microsoft_teams)
        * [Jira](https://docs.datadoghq.com/incident_response/incident_management/integrations/jira)
        * [ServiceNow](https://docs.datadoghq.com/incident_response/incident_management/integrations/servicenow)
        * [Status Pages](https://docs.datadoghq.com/incident_response/incident_management/integrations/status_pages)
        * [Atlassian Statuspage](https://docs.datadoghq.com/incident_response/incident_management/integrations/statuspage)
      * [Datadog Clipboard](https://docs.datadoghq.com/incident_response/incident_management/datadog_clipboard)
    * [On-Call](https://docs.datadoghq.com/incident_response/on-call/)
      * [Onboard a Team](https://docs.datadoghq.com/incident_response/on-call/teams/)
      * [Trigger a Page](https://docs.datadoghq.com/incident_response/on-call/triggering_pages/)
        * [Live Call Routing](https://docs.datadoghq.com/incident_response/on-call/triggering_pages/live_call_routing)
      * [Routing Rules](https://docs.datadoghq.com/incident_response/on-call/routing_rules/)
      * [Escalation Policies](https://docs.datadoghq.com/incident_response/on-call/escalation_policies/)
      * [Schedules](https://docs.datadoghq.com/incident_response/on-call/schedules/)
      * [Automations](https://docs.datadoghq.com/incident_response/on-call/automations)
      * [Profile Settings](https://docs.datadoghq.com/incident_response/on-call/profile_settings/)
      * [Guides](https://docs.datadoghq.com/incident_response/on-call/guides/)
    * [Status Pages](https://docs.datadoghq.com/incident_response/status_pages/)
    * [Case Management](https://docs.datadoghq.com/incident_response/case_management/)
      * [Projects](https://docs.datadoghq.com/incident_response/case_management/projects)
        * [Settings](https://docs.datadoghq.com/incident_response/case_management/settings)
      * [Create a Case](https://docs.datadoghq.com/incident_response/case_management/create_case)
      * [Customization](https://docs.datadoghq.com/incident_response/case_management/customization)
      * [View and Manage Cases](https://docs.datadoghq.com/incident_response/case_management/view_and_manage)
      * [Notifications and Integrations](https://docs.datadoghq.com/incident_response/case_management/notifications_integrations)
      * [Case Automation Rules](https://docs.datadoghq.com/incident_response/case_management/automation_rules)
      * [Troubleshooting](https://docs.datadoghq.com/incident_response/case_management/troubleshooting)
  * [Actions & Remediations ](https://docs.datadoghq.com/api/latest/observability-pipelines/)
    * [Agents](https://docs.datadoghq.com/actions/agents/)
    * [Workflow Automation](https://docs.datadoghq.com/actions/workflows/)
      * [Build Workflows](https://docs.datadoghq.com/actions/workflows/build/)
      * [Access and Authentication](https://docs.datadoghq.com/actions/workflows/access_and_auth/)
      * [Trigger Workflows](https://docs.datadoghq.com/actions/workflows/trigger/)
      * [Variables and parameters](https://docs.datadoghq.com/actions/workflows/variables/)
      * [Actions](https://docs.datadoghq.com/actions/workflows/actions/)
        * [Workflow Logic](https://docs.datadoghq.com/actions/workflows/actions/flow_control/)
      * [Save and Reuse Actions](https://docs.datadoghq.com/actions/workflows/saved_actions/)
      * [Test and Debug](https://docs.datadoghq.com/actions/workflows/test_and_debug/)
      * [JavaScript Expressions](https://docs.datadoghq.com/actions/workflows/expressions/)
      * [Track Workflows](https://docs.datadoghq.com/actions/workflows/track)
      * [Limits](https://docs.datadoghq.com/actions/workflows/limits/)
    * [App Builder](https://docs.datadoghq.com/actions/app_builder/)
      * [Build Apps](https://docs.datadoghq.com/actions/app_builder/build/)
      * [Access and Authentication](https://docs.datadoghq.com/actions/app_builder/access_and_auth/)
      * [Queries](https://docs.datadoghq.com/actions/app_builder/queries/)
      * [Variables](https://docs.datadoghq.com/actions/app_builder/variables/)
      * [Events](https://docs.datadoghq.com/actions/app_builder/events/)
      * [Components](https://docs.datadoghq.com/actions/app_builder/components/)
        * [Custom Charts](https://docs.datadoghq.com/actions/app_builder/components/custom_charts/)
        * [React Renderer](https://docs.datadoghq.com/actions/app_builder/components/react_renderer/)
        * [Tables](https://docs.datadoghq.com/actions/app_builder/components/tables/)
        * [Reusable Modules](https://docs.datadoghq.com/actions/app_builder/components/reusable_modules/)
      * [JavaScript Expressions](https://docs.datadoghq.com/actions/app_builder/expressions/)
      * [Embedded Apps](https://docs.datadoghq.com/actions/app_builder/embedded_apps/)
        * [Input Parameters](https://docs.datadoghq.com/actions/app_builder/embedded_apps/input_parameters/)
      * [Save and Reuse Actions](https://docs.datadoghq.com/actions/app_builder/saved_actions/)
    * [Datastores](https://docs.datadoghq.com/actions/datastores/)
      * [Create and Manage Datastores](https://docs.datadoghq.com/actions/datastores/create/)
      * [Use Datastores with Apps and Workflows](https://docs.datadoghq.com/actions/datastores/use)
      * [Automation Rules](https://docs.datadoghq.com/actions/datastores/trigger)
      * [Access and Authentication](https://docs.datadoghq.com/actions/datastores/auth)
    * [Forms](https://docs.datadoghq.com/actions/forms/)
    * [Action Catalog](https://docs.datadoghq.com/actions/actions_catalog/)
      * [Connections](https://docs.datadoghq.com/actions/connections/)
        * [AWS Integration](https://docs.datadoghq.com/actions/connections/aws_integration/)
        * [HTTP Request](https://docs.datadoghq.com/actions/connections/http/)
      * [Private Actions](https://docs.datadoghq.com/actions/private_actions/)
        * [Use Private Actions](https://docs.datadoghq.com/actions/private_actions/use_private_actions/)
        * [Run a Script](https://docs.datadoghq.com/actions/private_actions/run_script/)
        * [Update the Private Action Runner](https://docs.datadoghq.com/actions/private_actions/update_private_action_runner/)
        * [Private Action Credentials](https://docs.datadoghq.com/actions/private_actions/private_action_credentials/)
  * [Infrastructure ](https://docs.datadoghq.com/api/latest/observability-pipelines/)
    * [Cloudcraft](https://docs.datadoghq.com/datadog_cloudcraft/)
      * [Overlays](https://docs.datadoghq.com/datadog_cloudcraft/overlays/)
        * [Infrastructure](https://docs.datadoghq.com/datadog_cloudcraft/overlays/infrastructure/)
        * [Observability](https://docs.datadoghq.com/datadog_cloudcraft/overlays/observability/)
        * [Security](https://docs.datadoghq.com/datadog_cloudcraft/overlays/security/)
        * [Cloud Cost Management](https://docs.datadoghq.com/datadog_cloudcraft/overlays/ccm/)
    * [Resource Catalog](https://docs.datadoghq.com/infrastructure/resource_catalog/)
      * [Cloud Resources Schema](https://docs.datadoghq.com/infrastructure/resource_catalog/schema/)
      * [Policies](https://docs.datadoghq.com/infrastructure/resource_catalog/policies/)
      * [Resource Changes](https://docs.datadoghq.com/infrastructure/resource_catalog/resource_changes/)
    * [Universal Service Monitoring](https://docs.datadoghq.com/universal_service_monitoring/)
      * [Setup](https://docs.datadoghq.com/universal_service_monitoring/setup/)
      * [Guides](https://docs.datadoghq.com/universal_service_monitoring/guide/)
    * [End User Device Monitoring](https://docs.datadoghq.com/infrastructure/end_user_device_monitoring/)
      * [Setup](https://docs.datadoghq.com/infrastructure/end_user_device_monitoring/setup/)
    * [Hosts](https://docs.datadoghq.com/infrastructure/hostmap/)
      * [Host List](https://docs.datadoghq.com/infrastructure/list/)
    * [Containers](https://docs.datadoghq.com/containers/)
      * [Monitoring Containers](https://docs.datadoghq.com/infrastructure/containers/)
        * [Configuration](https://docs.datadoghq.com/infrastructure/containers/configuration)
        * [Container Images View](https://docs.datadoghq.com/infrastructure/containers/container_images)
        * [Orchestrator Explorer](https://docs.datadoghq.com/infrastructure/containers/orchestrator_explorer)
        * [Kubernetes Resource Utilization](https://docs.datadoghq.com/infrastructure/containers/kubernetes_resource_utilization)
        * [Kubernetes Autoscaling](https://docs.datadoghq.com/containers/monitoring/autoscaling)
        * [Amazon Elastic Container Explorer](https://docs.datadoghq.com/infrastructure/containers/amazon_elastic_container_explorer)
      * [Autoscaling](https://docs.datadoghq.com/containers/autoscaling)
      * [Docker and other runtimes](https://docs.datadoghq.com/containers/docker/)
        * [APM](https://docs.datadoghq.com/containers/docker/apm/)
        * [Log collection](https://docs.datadoghq.com/containers/docker/log/)
        * [Tag extraction](https://docs.datadoghq.com/containers/docker/tag/)
        * [Integrations](https://docs.datadoghq.com/containers/docker/integrations/)
        * [Prometheus](https://docs.datadoghq.com/containers/docker/prometheus/)
        * [Data Collected](https://docs.datadoghq.com/containers/docker/data_collected/)
      * [Kubernetes](https://docs.datadoghq.com/containers/kubernetes/)
        * [Installation](https://docs.datadoghq.com/containers/kubernetes/installation)
        * [Further Configuration](https://docs.datadoghq.com/containers/kubernetes/configuration)
        * [Distributions](https://docs.datadoghq.com/containers/kubernetes/distributions)
        * [APM](https://docs.datadoghq.com/containers/kubernetes/apm/)
        * [Log collection](https://docs.datadoghq.com/containers/kubernetes/log/)
        * [Tag extraction](https://docs.datadoghq.com/containers/kubernetes/tag/)
        * [Integrations](https://docs.datadoghq.com/containers/kubernetes/integrations/)
        * [Prometheus & OpenMetrics](https://docs.datadoghq.com/containers/kubernetes/prometheus/)
        * [Control plane monitoring](https://docs.datadoghq.com/containers/kubernetes/control_plane/)
        * [Data collected](https://docs.datadoghq.com/containers/kubernetes/data_collected/)
        * [kubectl Plugin](https://docs.datadoghq.com/containers/kubernetes/kubectl_plugin)
        * [Datadog CSI Driver](https://docs.datadoghq.com/containers/kubernetes/csi_driver)
        * [Data security](https://docs.datadoghq.com/data_security/kubernetes)
      * [Cluster Agent](https://docs.datadoghq.com/containers/cluster_agent/)
        * [Setup](https://docs.datadoghq.com/containers/cluster_agent/setup/)
        * [Commands & Options](https://docs.datadoghq.com/containers/cluster_agent/commands/)
        * [Cluster Checks](https://docs.datadoghq.com/containers/cluster_agent/clusterchecks/)
        * [Endpoint Checks](https://docs.datadoghq.com/containers/cluster_agent/endpointschecks/)
        * [Admission Controller](https://docs.datadoghq.com/containers/cluster_agent/admission_controller/)
      * [Amazon ECS](https://docs.datadoghq.com/containers/amazon_ecs/)
        * [APM](https://docs.datadoghq.com/containers/amazon_ecs/apm/)
        * [Log collection](https://docs.datadoghq.com/containers/amazon_ecs/logs/)
        * [Tag extraction](https://docs.datadoghq.com/containers/amazon_ecs/tags/)
        * [Data collected](https://docs.datadoghq.com/containers/amazon_ecs/data_collected/)
        * [Managed Instances](https://docs.datadoghq.com/containers/amazon_ecs/managed_instances/)
        * [AWS Fargate with ECS](https://docs.datadoghq.com/integrations/ecs_fargate/)
      * [Datadog Operator](https://docs.datadoghq.com/containers/datadog_operator)
        * [Advanced Install](https://docs.datadoghq.com/containers/datadog_operator/advanced_install)
        * [Configuration](https://docs.datadoghq.com/containers/datadog_operator/config)
        * [Custom Checks](https://docs.datadoghq.com/containers/datadog_operator/custom_check)
        * [Data Collected](https://docs.datadoghq.com/containers/datadog_operator/data_collected)
        * [Secret Management](https://docs.datadoghq.com/containers/datadog_operator/secret_management)
        * [DatadogDashboard CRD](https://docs.datadoghq.com/containers/datadog_operator/crd_dashboard)
        * [DatadogMonitor CRD](https://docs.datadoghq.com/containers/datadog_operator/crd_monitor)
        * [DatadogSLO CRD](https://docs.datadoghq.com/containers/datadog_operator/crd_slo)
      * [Troubleshooting](https://docs.datadoghq.com/containers/troubleshooting/)
        * [Duplicate hosts](https://docs.datadoghq.com/containers/troubleshooting/duplicate_hosts)
        * [Cluster Agent](https://docs.datadoghq.com/containers/troubleshooting/cluster-agent)
        * [Cluster Checks](https://docs.datadoghq.com/containers/troubleshooting/cluster-and-endpoint-checks)
        * [HPA and Metrics Provider](https://docs.datadoghq.com/containers/troubleshooting/hpa)
        * [Admission Controller](https://docs.datadoghq.com/containers/troubleshooting/admission-controller)
        * [Log Collection](https://docs.datadoghq.com/containers/troubleshooting/log-collection)
      * [Guides](https://docs.datadoghq.com/containers/guide)
    * [Processes](https://docs.datadoghq.com/infrastructure/process)
      * [Increase Process Retention](https://docs.datadoghq.com/infrastructure/process/increase_process_retention/)
    * [Serverless](https://docs.datadoghq.com/serverless)
      * [AWS Lambda](https://docs.datadoghq.com/serverless/aws_lambda)
        * [Instrumentation](https://docs.datadoghq.com/serverless/aws_lambda/instrumentation)
        * [Managed Instances](https://docs.datadoghq.com/serverless/aws_lambda/managed_instances)
        * [Lambda Metrics](https://docs.datadoghq.com/serverless/aws_lambda/metrics)
        * [Distributed Tracing](https://docs.datadoghq.com/serverless/aws_lambda/distributed_tracing)
        * [Log Collection](https://docs.datadoghq.com/serverless/aws_lambda/logs)
        * [Remote Instrumentation](https://docs.datadoghq.com/serverless/aws_lambda/remote_instrumentation)
        * [Advanced Configuration](https://docs.datadoghq.com/serverless/aws_lambda/configuration)
        * [Continuous Profiler](https://docs.datadoghq.com/serverless/aws_lambda/profiling)
        * [Securing Functions](https://docs.datadoghq.com/serverless/aws_lambda/securing_functions)
        * [Deployment Tracking](https://docs.datadoghq.com/serverless/aws_lambda/deployment_tracking)
        * [OpenTelemetry](https://docs.datadoghq.com/serverless/aws_lambda/opentelemetry)
        * [Troubleshooting](https://docs.datadoghq.com/serverless/aws_lambda/troubleshooting)
        * [Lambda Web Adapter](https://docs.datadoghq.com/serverless/aws_lambda/lwa)
        * [FIPS Compliance](https://docs.datadoghq.com/serverless/aws_lambda/fips-compliance)
      * [AWS Step Functions](https://docs.datadoghq.com/serverless/step_functions)
        * [Installation](https://docs.datadoghq.com/serverless/step_functions/installation)
        * [Merge Step Functions and Lambda Traces](https://docs.datadoghq.com/serverless/step_functions/merge-step-functions-lambda)
        * [Enhanced Metrics](https://docs.datadoghq.com/serverless/step_functions/enhanced-metrics)
        * [Redrive Executions](https://docs.datadoghq.com/serverless/step_functions/redrive)
        * [Distributed Map States](https://docs.datadoghq.com/serverless/step_functions/distributed-maps)
        * [Troubleshooting](https://docs.datadoghq.com/serverless/step_functions/troubleshooting)
      * [AWS Fargate](https://docs.datadoghq.com/integrations/ecs_fargate)
      * [Azure App Service](https://docs.datadoghq.com/serverless/azure_app_service)
        * [Linux - Code](https://docs.datadoghq.com/serverless/azure_app_service/linux_code)
        * [Linux - Container](https://docs.datadoghq.com/serverless/azure_app_service/linux_container)
        * [Windows - Code](https://docs.datadoghq.com/serverless/azure_app_service/windows_code)
      * [Azure Container Apps](https://docs.datadoghq.com/serverless/azure_container_apps)
        * [In-Container](https://docs.datadoghq.com/serverless/azure_container_apps/in_container)
        * [Sidecar](https://docs.datadoghq.com/serverless/azure_container_apps/sidecar)
      * [Azure Functions](https://docs.datadoghq.com/serverless/azure_functions)
      * [Google Cloud Run](https://docs.datadoghq.com/serverless/google_cloud_run)
        * [Containers](https://docs.datadoghq.com/serverless/google_cloud_run/containers)
        * [Functions](https://docs.datadoghq.com/serverless/google_cloud_run/functions)
        * [Functions (1st generation)](https://docs.datadoghq.com/serverless/google_cloud_run/functions_1st_gen)
      * [Libraries & Integrations](https://docs.datadoghq.com/serverless/libraries_integrations)
      * [Glossary](https://docs.datadoghq.com/serverless/glossary)
      * [Guides](https://docs.datadoghq.com/serverless/guide/)
    * [Network Monitoring](https://docs.datadoghq.com/network_monitoring/)
      * [Cloud Network Monitoring](https://docs.datadoghq.com/network_monitoring/cloud_network_monitoring/)
        * [Setup](https://docs.datadoghq.com/network_monitoring/cloud_network_monitoring/setup/)
        * [Network Health](https://docs.datadoghq.com/network_monitoring/cloud_network_monitoring/network_health/)
        * [Network Analytics](https://docs.datadoghq.com/network_monitoring/cloud_network_monitoring/network_analytics/)
        * [Network Map](https://docs.datadoghq.com/network_monitoring/cloud_network_monitoring/network_map/)
        * [Guides](https://docs.datadoghq.com/network_monitoring/cloud_network_monitoring/guide/)
        * [Supported Cloud Services](https://docs.datadoghq.com/network_monitoring/cloud_network_monitoring/supported_cloud_services/)
        * [Terms and Concepts](https://docs.datadoghq.com/network_monitoring/cloud_network_monitoring/glossary)
      * [DNS Monitoring](https://docs.datadoghq.com/network_monitoring/dns/)
      * [Network Device Monitoring](https://docs.datadoghq.com/network_monitoring/devices)
        * [Setup](https://docs.datadoghq.com/network_monitoring/devices/setup)
        * [Integrations](https://docs.datadoghq.com/network_monitoring/devices/integrations)
        * [Profiles](https://docs.datadoghq.com/network_monitoring/devices/profiles)
        * [Configuration Management](https://docs.datadoghq.com/network_monitoring/devices/config_management)
        * [Maps](https://docs.datadoghq.com/network_monitoring/devices/topology)
        * [SNMP Metrics Reference](https://docs.datadoghq.com/network_monitoring/devices/data)
        * [Troubleshooting](https://docs.datadoghq.com/network_monitoring/devices/troubleshooting)
        * [Guides](https://docs.datadoghq.com/network_monitoring/devices/guide/)
        * [Terms and Concepts](https://docs.datadoghq.com/network_monitoring/devices/glossary)
      * [NetFlow Monitoring](https://docs.datadoghq.com/network_monitoring/netflow/)
        * [Monitors](https://docs.datadoghq.com/monitors/types/netflow)
      * [Network Path](https://docs.datadoghq.com/network_monitoring/network_path/)
        * [Setup](https://docs.datadoghq.com/network_monitoring/network_path/setup/)
        * [List View](https://docs.datadoghq.com/network_monitoring/network_path/list_view/)
        * [Path View](https://docs.datadoghq.com/network_monitoring/network_path/path_view/)
        * [Guides](https://docs.datadoghq.com/network_monitoring/network_path/guide)
        * [Terms and Concepts](https://docs.datadoghq.com/network_monitoring/network_path/glossary)
    * [Storage Management](https://docs.datadoghq.com/infrastructure/storage_management/)
      * [Amazon S3](https://docs.datadoghq.com/infrastructure/storage_management/amazon_s3/)
      * [Google Cloud Storage](https://docs.datadoghq.com/infrastructure/storage_management/google_cloud_storage/)
      * [Azure Blob Storage](https://docs.datadoghq.com/infrastructure/storage_management/azure_blob_storage/)
  * [Cloud Cost ](https://docs.datadoghq.com/api/latest/observability-pipelines/)
    * [Cloud Cost](https://docs.datadoghq.com/cloud_cost_management/)
      * [Datadog Costs](https://docs.datadoghq.com/cloud_cost_management/datadog_costs)
      * [Setup](https://docs.datadoghq.com/cloud_cost_management/setup/)
        * [AWS](https://docs.datadoghq.com/cloud_cost_management/setup/aws/)
        * [Azure](https://docs.datadoghq.com/cloud_cost_management/setup/azure/)
        * [Google Cloud](https://docs.datadoghq.com/cloud_cost_management/setup/google_cloud/)
        * [Oracle](https://docs.datadoghq.com/cloud_cost_management/setup/oracle/)
        * [SaaS Integrations](https://docs.datadoghq.com/cloud_cost_management/setup/saas_costs/)
        * [Custom](https://docs.datadoghq.com/cloud_cost_management/setup/custom)
      * [Tags](https://docs.datadoghq.com/cloud_cost_management/tags)
        * [Tag Explorer](https://docs.datadoghq.com/cloud_cost_management/tags/tag_explorer)
        * [Multisource Querying](https://docs.datadoghq.com/cloud_cost_management/tags/multisource_querying)
      * [Allocation](https://docs.datadoghq.com/cloud_cost_management/allocation)
        * [Tag Pipelines](https://docs.datadoghq.com/cloud_cost_management/tags/tag_pipelines)
        * [Container Cost Allocation](https://docs.datadoghq.com/cloud_cost_management/allocation/container_cost_allocation)
        * [BigQuery Costs](https://docs.datadoghq.com/cloud_cost_management/allocation/bigquery)
        * [Custom Allocation Rules](https://docs.datadoghq.com/cloud_cost_management/allocation/custom_allocation_rules)
      * [Reporting](https://docs.datadoghq.com/cloud_cost_management/reporting)
        * [Explorer](https://docs.datadoghq.com/cloud_cost_management/reporting/explorer)
        * [Scheduled Reports](https://docs.datadoghq.com/cloud_cost_management/reporting/scheduled_reports)
      * [Recommendations](https://docs.datadoghq.com/cloud_cost_management/recommendations)
        * [Custom Recommendations](https://docs.datadoghq.com/cloud_cost_management/recommendations/custom_recommendations)
      * [Planning](https://docs.datadoghq.com/cloud_cost_management/planning)
        * [Budgets](https://docs.datadoghq.com/cloud_cost_management/planning/budgets)
        * [Commitment Programs](https://docs.datadoghq.com/cloud_cost_management/planning/commitment_programs)
      * [Cost Changes](https://docs.datadoghq.com/cloud_cost_management/cost_changes)
        * [Monitors](https://docs.datadoghq.com/cloud_cost_management/cost_changes/monitors)
        * [Anomalies](https://docs.datadoghq.com/cloud_cost_management/cost_changes/anomalies)
        * [Real-Time Costs](https://docs.datadoghq.com/cloud_cost_management/cost_changes/real_time_costs)
  * [Application Performance ](https://docs.datadoghq.com/api/latest/observability-pipelines/)
    * [APM](https://docs.datadoghq.com/tracing/)
      * [APM Terms and Concepts](https://docs.datadoghq.com/tracing/glossary/)
      * [Application Instrumentation](https://docs.datadoghq.com/tracing/trace_collection/)
        * [Single Step Instrumentation](https://docs.datadoghq.com/tracing/trace_collection/single-step-apm/)
        * [Manually managed SDKs](https://docs.datadoghq.com/tracing/trace_collection/dd_libraries/)
        * [Code-based Custom Instrumentation](https://docs.datadoghq.com/tracing/trace_collection/custom_instrumentation/)
        * [Dynamic Instrumentation](https://docs.datadoghq.com/tracing/trace_collection/dynamic_instrumentation/)
        * [Library Compatibility](https://docs.datadoghq.com/tracing/trace_collection/compatibility/)
        * [Library Configuration](https://docs.datadoghq.com/tracing/trace_collection/library_config/)
        * [Configuration at Runtime](https://docs.datadoghq.com/tracing/trace_collection/runtime_config/)
        * [Trace Context Propagation](https://docs.datadoghq.com/tracing/trace_collection/trace_context_propagation/)
        * [Serverless Application Tracing](https://docs.datadoghq.com/serverless/distributed_tracing/)
        * [Proxy Tracing](https://docs.datadoghq.com/tracing/trace_collection/proxy_setup/)
        * [Span Tag Semantics](https://docs.datadoghq.com/tracing/trace_collection/tracing_naming_convention)
        * [Span Links](https://docs.datadoghq.com/tracing/trace_collection/span_links)
      * [APM Metrics Collection](https://docs.datadoghq.com/tracing/metrics/)
        * [Trace Metrics](https://docs.datadoghq.com/tracing/metrics/metrics_namespace/)
        * [Runtime Metrics](https://docs.datadoghq.com/tracing/metrics/runtime_metrics/)
      * [Trace Pipeline Configuration](https://docs.datadoghq.com/tracing/trace_pipeline/)
        * [Ingestion Mechanisms](https://docs.datadoghq.com/tracing/trace_pipeline/ingestion_mechanisms/)
        * [Ingestion Controls](https://docs.datadoghq.com/tracing/trace_pipeline/ingestion_controls/)
        * [Adaptive Sampling](https://docs.datadoghq.com/tracing/trace_pipeline/adaptive_sampling/)
        * [Generate Metrics](https://docs.datadoghq.com/tracing/trace_pipeline/generate_metrics/)
        * [Trace Retention](https://docs.datadoghq.com/tracing/trace_pipeline/trace_retention/)
        * [Usage Metrics](https://docs.datadoghq.com/tracing/trace_pipeline/metrics/)
      * [Correlate Traces with Other Telemetry](https://docs.datadoghq.com/tracing/other_telemetry/)
        * [Correlate DBM and Traces](https://docs.datadoghq.com/database_monitoring/connect_dbm_and_apm/)
        * [Correlate Logs and Traces](https://docs.datadoghq.com/tracing/other_telemetry/connect_logs_and_traces/)
        * [Correlate RUM and Traces](https://docs.datadoghq.com/tracing/other_telemetry/rum)
        * [Correlate Synthetics and Traces](https://docs.datadoghq.com/tracing/other_telemetry/synthetics/)
        * [Correlate Profiles and Traces](https://docs.datadoghq.com/profiler/connect_traces_and_profiles/)
      * [Trace Explorer](https://docs.datadoghq.com/tracing/trace_explorer/)
        * [Search Spans](https://docs.datadoghq.com/tracing/trace_explorer/search/)
        * [Query Syntax](https://docs.datadoghq.com/tracing/trace_explorer/query_syntax/)
        * [Trace Queries](https://docs.datadoghq.com/tracing/trace_explorer/trace_queries/)
        * [Span Tags and Attributes](https://docs.datadoghq.com/tracing/trace_explorer/span_tags_attributes/)
        * [Span Visualizations](https://docs.datadoghq.com/tracing/trace_explorer/visualize/)
        * [Trace View](https://docs.datadoghq.com/tracing/trace_explorer/trace_view/)
        * [Tag Analysis](https://docs.datadoghq.com/tracing/trace_explorer/tag_analysis/)
      * [Recommendations](https://docs.datadoghq.com/tracing/recommendations/)
      * [Code Origin for Spans](https://docs.datadoghq.com/tracing/code_origin)
      * [Service Observability](https://docs.datadoghq.com/tracing/services/)
        * [Software Catalog](https://docs.datadoghq.com/internal_developer_portal/software_catalog/)
        * [Service Page](https://docs.datadoghq.com/tracing/services/service_page/)
        * [Resource Page](https://docs.datadoghq.com/tracing/services/resource_page/)
        * [Deployment Tracking](https://docs.datadoghq.com/tracing/services/deployment_tracking/)
        * [Service Map](https://docs.datadoghq.com/tracing/services/services_map/)
        * [Inferred Services](https://docs.datadoghq.com/tracing/services/inferred_services)
        * [Remapping Rules for Inferred Entities](https://docs.datadoghq.com/tracing/services/inferred_entity_remapping_rules/)
        * [Service Remapping Rules](https://docs.datadoghq.com/tracing/services/service_remapping_rules)
        * [Service Override Removal](https://docs.datadoghq.com/tracing/services/service_override_removal)
        * [APM Monitors](https://docs.datadoghq.com/monitors/create/types/apm/)
      * [Endpoint Observability](https://docs.datadoghq.com/internal_developer_portal/software_catalog/endpoints/)
        * [Explore Endpoints](https://docs.datadoghq.com/internal_developer_portal/software_catalog/endpoints/explore_endpoints)
        * [Monitor Endpoints](https://docs.datadoghq.com/internal_developer_portal/software_catalog/endpoints/monitor_endpoints)
      * [Live Debugger](https://docs.datadoghq.com/tracing/live_debugger/)
      * [Error Tracking](https://docs.datadoghq.com/tracing/error_tracking/)
        * [Issue States](https://docs.datadoghq.com/tracing/error_tracking/issue_states)
        * [Error Tracking Explorer](https://docs.datadoghq.com/tracing/error_tracking/explorer)
        * [Error Grouping](https://docs.datadoghq.com/tracing/error_tracking/error_grouping)
        * [Monitors](https://docs.datadoghq.com/tracing/error_tracking/monitors)
        * [Identify Suspect Commits](https://docs.datadoghq.com/tracing/error_tracking/suspect_commits)
        * [Exception Replay](https://docs.datadoghq.com/tracing/error_tracking/exception_replay)
        * [Troubleshooting](https://docs.datadoghq.com/error_tracking/troubleshooting)
      * [Data Security](https://docs.datadoghq.com/tracing/configure_data_security/)
      * [Guides](https://docs.datadoghq.com/tracing/guide/)
      * [Troubleshooting](https://docs.datadoghq.com/tracing/troubleshooting/)
        * [Agent Rate Limits](https://docs.datadoghq.com/tracing/troubleshooting/agent_rate_limits)
        * [Agent APM metrics](https://docs.datadoghq.com/tracing/troubleshooting/agent_apm_metrics)
        * [Agent Resource Usage](https://docs.datadoghq.com/tracing/troubleshooting/agent_apm_resource_usage)
        * [Correlated Logs](https://docs.datadoghq.com/tracing/troubleshooting/correlated-logs-not-showing-up-in-the-trace-id-panel)
        * [PHP 5 Deep Call Stacks](https://docs.datadoghq.com/tracing/troubleshooting/php_5_deep_call_stacks)
        * [.NET diagnostic tool](https://docs.datadoghq.com/tracing/troubleshooting/dotnet_diagnostic_tool)
        * [APM Quantization](https://docs.datadoghq.com/tracing/troubleshooting/quantization)
        * [Go Compile-Time Instrumentation](https://docs.datadoghq.com/tracing/troubleshooting/go_compile_time)
        * [Tracer Startup Logs](https://docs.datadoghq.com/tracing/troubleshooting/tracer_startup_logs)
        * [Tracer Debug Logs](https://docs.datadoghq.com/tracing/troubleshooting/tracer_debug_logs)
        * [Connection Errors](https://docs.datadoghq.com/tracing/troubleshooting/connection_errors)
    * [Continuous Profiler](https://docs.datadoghq.com/profiler/)
      * [Enabling the Profiler](https://docs.datadoghq.com/profiler/enabling/)
        * [Supported Language and Tracer Versions](https://docs.datadoghq.com/profiler/enabling/supported_versions/)
        * [Java](https://docs.datadoghq.com/profiler/enabling/java/)
        * [Python](https://docs.datadoghq.com/profiler/enabling/python/)
        * [Go](https://docs.datadoghq.com/profiler/enabling/go/)
        * [Ruby](https://docs.datadoghq.com/profiler/enabling/ruby/)
        * [Node.js](https://docs.datadoghq.com/profiler/enabling/nodejs/)
        * [.NET](https://docs.datadoghq.com/profiler/enabling/dotnet/)
        * [PHP](https://docs.datadoghq.com/profiler/enabling/php/)
        * [C/C++/Rust](https://docs.datadoghq.com/profiler/enabling/ddprof/)
      * [Profile Types](https://docs.datadoghq.com/profiler/profile_types/)
      * [Profile Visualizations](https://docs.datadoghq.com/profiler/profile_visualizations/)
      * [Investigate Slow Traces or Endpoints](https://docs.datadoghq.com/profiler/connect_traces_and_profiles/)
      * [Compare Profiles](https://docs.datadoghq.com/profiler/compare_profiles)
      * [Automated Analysis](https://docs.datadoghq.com/profiler/automated_analysis)
      * [Profiler Troubleshooting](https://docs.datadoghq.com/profiler/profiler_troubleshooting/)
        * [Java](https://docs.datadoghq.com/profiler/profiler_troubleshooting/java/)
        * [Python](https://docs.datadoghq.com/profiler/profiler_troubleshooting/python/)
        * [Go](https://docs.datadoghq.com/profiler/profiler_troubleshooting/go/)
        * [Ruby](https://docs.datadoghq.com/profiler/profiler_troubleshooting/ruby/)
        * [Node.js](https://docs.datadoghq.com/profiler/profiler_troubleshooting/nodejs/)
        * [.NET](https://docs.datadoghq.com/profiler/profiler_troubleshooting/dotnet/)
        * [PHP](https://docs.datadoghq.com/profiler/profiler_troubleshooting/php/)
        * [C/C++/Rust](https://docs.datadoghq.com/profiler/profiler_troubleshooting/ddprof/)
      * [Guides](https://docs.datadoghq.com/profiler/guide/)
    * [Database Monitoring](https://docs.datadoghq.com/database_monitoring/)
      * [Agent Integration Overhead](https://docs.datadoghq.com/database_monitoring/agent_integration_overhead)
      * [Setup Architectures](https://docs.datadoghq.com/database_monitoring/architecture/)
      * [Setting Up Postgres](https://docs.datadoghq.com/database_monitoring/setup_postgres/)
        * [Self-hosted](https://docs.datadoghq.com/database_monitoring/setup_postgres/selfhosted)
        * [RDS](https://docs.datadoghq.com/database_monitoring/setup_postgres/rds)
        * [Aurora](https://docs.datadoghq.com/database_monitoring/setup_postgres/aurora)
        * [Google Cloud SQL](https://docs.datadoghq.com/database_monitoring/setup_postgres/gcsql)
        * [AlloyDB](https://docs.datadoghq.com/database_monitoring/setup_postgres/alloydb)
        * [Azure](https://docs.datadoghq.com/database_monitoring/setup_postgres/azure)
        * [Supabase](https://docs.datadoghq.com/database_monitoring/setup_postgres/supabase)
        * [Heroku](https://docs.datadoghq.com/database_monitoring/setup_postgres/heroku)
        * [Advanced Configuration](https://docs.datadoghq.com/database_monitoring/setup_postgres/advanced_configuration)
        * [Troubleshooting](https://docs.datadoghq.com/database_monitoring/setup_postgres/troubleshooting/)
      * [Setting Up MySQL](https://docs.datadoghq.com/database_monitoring/setup_mysql/)
        * [Self-hosted](https://docs.datadoghq.com/database_monitoring/setup_mysql/selfhosted)
        * [RDS](https://docs.datadoghq.com/database_monitoring/setup_mysql/rds)
        * [Aurora](https://docs.datadoghq.com/database_monitoring/setup_mysql/aurora)
        * [Google Cloud SQL](https://docs.datadoghq.com/database_monitoring/setup_mysql/gcsql)
        * [Azure](https://docs.datadoghq.com/database_monitoring/setup_mysql/azure)
        * [Advanced Configuration](https://docs.datadoghq.com/database_monitoring/setup_mysql/advanced_configuration)
        * [Troubleshooting](https://docs.datadoghq.com/database_monitoring/setup_mysql/troubleshooting/)
      * [Setting Up SQL Server](https://docs.datadoghq.com/database_monitoring/setup_sql_server/)
        * [Self-hosted](https://docs.datadoghq.com/database_monitoring/setup_sql_server/selfhosted/)
        * [RDS](https://docs.datadoghq.com/database_monitoring/setup_sql_server/rds/)
        * [Azure](https://docs.datadoghq.com/database_monitoring/setup_sql_server/azure/)
        * [Google Cloud SQL](https://docs.datadoghq.com/database_monitoring/setup_sql_server/gcsql/)
        * [Troubleshooting](https://docs.datadoghq.com/database_monitoring/setup_sql_server/troubleshooting/)
      * [Setting Up Oracle](https://docs.datadoghq.com/database_monitoring/setup_oracle/)
        * [Self-hosted](https://docs.datadoghq.com/database_monitoring/setup_oracle/selfhosted/)
        * [RDS](https://docs.datadoghq.com/database_monitoring/setup_oracle/rds/)
        * [RAC](https://docs.datadoghq.com/database_monitoring/setup_oracle/rac/)
        * [Exadata](https://docs.datadoghq.com/database_monitoring/setup_oracle/exadata/)
        * [Autonomous Database](https://docs.datadoghq.com/database_monitoring/setup_oracle/autonomous_database/)
        * [Troubleshooting](https://docs.datadoghq.com/database_monitoring/setup_oracle/troubleshooting/)
      * [Setting Up Amazon DocumentDB](https://docs.datadoghq.com/database_monitoring/setup_documentdb/)
        * [Amazon DocumentDB](https://docs.datadoghq.com/database_monitoring/setup_documentdb/amazon_documentdb)
      * [Setting Up MongoDB](https://docs.datadoghq.com/database_monitoring/setup_mongodb/)
        * [Self-hosted](https://docs.datadoghq.com/database_monitoring/setup_mongodb/selfhosted)
        * [MongoDB Atlas](https://docs.datadoghq.com/database_monitoring/setup_mongodb/mongodbatlas)
        * [Troubleshooting](https://docs.datadoghq.com/database_monitoring/setup_mongodb/troubleshooting/)
      * [Connecting DBM and Traces](https://docs.datadoghq.com/database_monitoring/connect_dbm_and_apm/)
      * [Data Collected](https://docs.datadoghq.com/database_monitoring/data_collected)
      * [Exploring Database Hosts](https://docs.datadoghq.com/database_monitoring/database_hosts/)
      * [Exploring Query Metrics](https://docs.datadoghq.com/database_monitoring/query_metrics/)
      * [Exploring Query Samples](https://docs.datadoghq.com/database_monitoring/query_samples/)
      * [Exploring Database Schemas](https://docs.datadoghq.com/database_monitoring/schema_explorer)
      * [Exploring Recommendations](https://docs.datadoghq.com/database_monitoring/recommendations/)
      * [Troubleshooting](https://docs.datadoghq.com/database_monitoring/troubleshooting/)
      * [Guides](https://docs.datadoghq.com/database_monitoring/guide/)
    * [Data Streams Monitoring](https://docs.datadoghq.com/data_streams/)
      * [Setup](https://docs.datadoghq.com/data_streams/setup)
      * [Kafka Messages](https://docs.datadoghq.com/data_streams/messages)
      * [Schema Tracking](https://docs.datadoghq.com/data_streams/schema_tracking)
      * [Dead Letter Queues](https://docs.datadoghq.com/data_streams/dead_letter_queues)
      * [Metrics and Tags](https://docs.datadoghq.com/data_streams/metrics_and_tags)
  * [Data Observability ](https://docs.datadoghq.com/api/latest/observability-pipelines/)
    * [Data Observability](https://docs.datadoghq.com/data_observability/)
    * [Quality Monitoring](https://docs.datadoghq.com/data_observability/quality_monitoring/)
      * [Data Warehouses](https://docs.datadoghq.com/data_observability/quality_monitoring/data_warehouses)
        * [Snowflake](https://docs.datadoghq.com/data_observability/quality_monitoring/data_warehouses/snowflake)
        * [Databricks](https://docs.datadoghq.com/data_observability/quality_monitoring/data_warehouses/databricks)
        * [BigQuery](https://docs.datadoghq.com/data_observability/quality_monitoring/data_warehouses/bigquery)
      * [Business Intelligence Integrations](https://docs.datadoghq.com/data_observability/quality_monitoring/business_intelligence)
        * [Tableau](https://docs.datadoghq.com/data_observability/quality_monitoring/business_intelligence/tableau)
        * [Sigma](https://docs.datadoghq.com/data_observability/quality_monitoring/business_intelligence/sigma)
        * [Metabase](https://docs.datadoghq.com/data_observability/quality_monitoring/business_intelligence/metabase)
        * [Power BI](https://docs.datadoghq.com/data_observability/quality_monitoring/business_intelligence/powerbi)
    * [Jobs Monitoring](https://docs.datadoghq.com/data_observability/jobs_monitoring/)
      * [Databricks](https://docs.datadoghq.com/data_observability/jobs_monitoring/databricks)
      * [Airflow](https://docs.datadoghq.com/data_observability/jobs_monitoring/airflow)
      * [dbt](https://docs.datadoghq.com/data_observability/jobs_monitoring/dbt)
      * [Spark on Kubernetes](https://docs.datadoghq.com/data_observability/jobs_monitoring/kubernetes)
      * [Spark on Amazon EMR](https://docs.datadoghq.com/data_observability/jobs_monitoring/emr)
      * [Spark on Google Dataproc](https://docs.datadoghq.com/data_observability/jobs_monitoring/dataproc)
      * [Custom Jobs (OpenLineage)](https://docs.datadoghq.com/data_observability/jobs_monitoring/openlineage)
        * [Datadog Agent for OpenLineage Proxy](https://docs.datadoghq.com/data_observability/jobs_monitoring/openlineage/datadog_agent_for_openlineage)
  * [Digital Experience ](https://docs.datadoghq.com/api/latest/observability-pipelines/)
    * [Real User Monitoring](https://docs.datadoghq.com/real_user_monitoring/)
      * [Application Monitoring](https://docs.datadoghq.com/real_user_monitoring/application_monitoring/)
        * [Browser](https://docs.datadoghq.com/real_user_monitoring/application_monitoring/browser/)
        * [Android and Android TV](https://docs.datadoghq.com/real_user_monitoring/application_monitoring/android)
        * [iOS and tvOS](https://docs.datadoghq.com/real_user_monitoring/application_monitoring/ios)
        * [Flutter](https://docs.datadoghq.com/real_user_monitoring/application_monitoring/flutter)
        * [Kotlin Multiplatform](https://docs.datadoghq.com/real_user_monitoring/application_monitoring/kotlin_multiplatform)
        * [React Native](https://docs.datadoghq.com/real_user_monitoring/application_monitoring/react_native)
        * [Roku](https://docs.datadoghq.com/real_user_monitoring/application_monitoring/roku)
        * [Unity](https://docs.datadoghq.com/real_user_monitoring/application_monitoring/unity)
      * [Platform](https://docs.datadoghq.com/real_user_monitoring/platform)
        * [Dashboards](https://docs.datadoghq.com/real_user_monitoring/platform/dashboards/)
        * [Monitors](https://docs.datadoghq.com/monitors/types/real_user_monitoring/)
        * [Generate Custom Metrics](https://docs.datadoghq.com/real_user_monitoring/platform/generate_metrics)
      * [Exploring RUM Data](https://docs.datadoghq.com/real_user_monitoring/explorer/)
        * [Search RUM Events](https://docs.datadoghq.com/real_user_monitoring/explorer/search/)
        * [Search Syntax](https://docs.datadoghq.com/real_user_monitoring/explorer/search_syntax/)
        * [Group](https://docs.datadoghq.com/real_user_monitoring/explorer/group/)
        * [Visualize](https://docs.datadoghq.com/real_user_monitoring/explorer/visualize/)
        * [Events](https://docs.datadoghq.com/real_user_monitoring/explorer/events/)
        * [Export](https://docs.datadoghq.com/real_user_monitoring/explorer/export/)
        * [Saved Views](https://docs.datadoghq.com/real_user_monitoring/explorer/saved_views/)
        * [Watchdog Insights for RUM](https://docs.datadoghq.com/real_user_monitoring/explorer/watchdog_insights/)
      * [Correlate RUM with Other Telemetry](https://docs.datadoghq.com/real_user_monitoring/correlate_with_other_telemetry/)
        * [Correlate LLM with RUM](https://docs.datadoghq.com/real_user_monitoring/correlate_with_other_telemetry/llm_observability)
        * [Correlate Logs with RUM](https://docs.datadoghq.com/real_user_monitoring/correlate_with_other_telemetry/logs)
        * [Correlate Profiling with RUM](https://docs.datadoghq.com/real_user_monitoring/correlate_with_other_telemetry/profiling/)
        * [Correlate Synthetics with RUM](https://docs.datadoghq.com/real_user_monitoring/correlate_with_other_telemetry/synthetics/)
        * [Correlate Traces with RUM](https://docs.datadoghq.com/real_user_monitoring/correlate_with_other_telemetry/apm)
      * [Feature Flag Tracking](https://docs.datadoghq.com/real_user_monitoring/feature_flag_tracking)
        * [Setup](https://docs.datadoghq.com/real_user_monitoring/feature_flag_tracking/setup)
        * [Using Feature Flags](https://docs.datadoghq.com/real_user_monitoring/feature_flag_tracking/using_feature_flags)
      * [Error Tracking](https://docs.datadoghq.com/real_user_monitoring/error_tracking/)
        * [Explorer](https://docs.datadoghq.com/real_user_monitoring/error_tracking/explorer/)
        * [Issue States](https://docs.datadoghq.com/real_user_monitoring/error_tracking/issue_states)
        * [Track Browser Errors](https://docs.datadoghq.com/real_user_monitoring/error_tracking/browser/)
        * [Track Mobile Errors](https://docs.datadoghq.com/real_user_monitoring/error_tracking/mobile/)
        * [Error Grouping](https://docs.datadoghq.com/real_user_monitoring/error_tracking/error_grouping)
        * [Monitors](https://docs.datadoghq.com/real_user_monitoring/error_tracking/monitors)
        * [Identify Suspect Commits](https://docs.datadoghq.com/real_user_monitoring/error_tracking/suspect_commits)
        * [Troubleshooting](https://docs.datadoghq.com/real_user_monitoring/error_tracking/troubleshooting)
      * [RUM Without Limits](https://docs.datadoghq.com/real_user_monitoring/rum_without_limits/)
        * [Metrics](https://docs.datadoghq.com/real_user_monitoring/rum_without_limits/metrics)
        * [Retention Filters](https://docs.datadoghq.com/real_user_monitoring/rum_without_limits/retention_filters)
      * [Operations Monitoring](https://docs.datadoghq.com/real_user_monitoring/operations_monitoring/)
      * [Ownership of Views](https://docs.datadoghq.com/real_user_monitoring/ownership_of_views/)
      * [Guides](https://docs.datadoghq.com/real_user_monitoring/guide/)
      * [Data Security](https://docs.datadoghq.com/data_security/real_user_monitoring/)
    * [Synthetic Testing and Monitoring](https://docs.datadoghq.com/synthetics/)
      * [API Testing](https://docs.datadoghq.com/synthetics/api_tests/)
        * [HTTP](https://docs.datadoghq.com/synthetics/api_tests/http_tests)
        * [SSL](https://docs.datadoghq.com/synthetics/api_tests/ssl_tests)
        * [DNS](https://docs.datadoghq.com/synthetics/api_tests/dns_tests)
        * [WebSocket](https://docs.datadoghq.com/synthetics/api_tests/websocket_tests)
        * [TCP](https://docs.datadoghq.com/synthetics/api_tests/tcp_tests)
        * [UDP](https://docs.datadoghq.com/synthetics/api_tests/udp_tests)
        * [ICMP](https://docs.datadoghq.com/synthetics/api_tests/icmp_tests)
        * [GRPC](https://docs.datadoghq.com/synthetics/api_tests/grpc_tests)
        * [Error codes](https://docs.datadoghq.com/synthetics/api_tests/errors)
      * [Multistep API Testing](https://docs.datadoghq.com/synthetics/multistep)
      * [Browser Testing](https://docs.datadoghq.com/synthetics/browser_tests/)
        * [Recording Steps](https://docs.datadoghq.com/synthetics/browser_tests/test_steps)
        * [Browser Testing Results](https://docs.datadoghq.com/synthetics/browser_tests/test_results)
        * [Advanced Options for Steps](https://docs.datadoghq.com/synthetics/browser_tests/advanced_options)
        * [Authentication in Browser Testing](https://docs.datadoghq.com/synthetics/browser_tests/app-that-requires-login)
      * [Network Path Testing](https://docs.datadoghq.com/synthetics/network_path_tests/)
        * [Terms and Concepts](https://docs.datadoghq.com/synthetics/network_path_tests/glossary)
      * [Mobile Application Testing](https://docs.datadoghq.com/synthetics/mobile_app_testing/)
        * [Testing Steps](https://docs.datadoghq.com/synthetics/mobile_app_testing/mobile_app_tests/steps)
        * [Testing Results](https://docs.datadoghq.com/synthetics/mobile_app_testing/mobile_app_tests/results)
        * [Advanced Options for Steps](https://docs.datadoghq.com/synthetics/mobile_app_testing/mobile_app_tests/advanced_options)
        * [Supported Devices](https://docs.datadoghq.com/synthetics/mobile_app_testing/devices/)
        * [Restricted Networks](https://docs.datadoghq.com/synthetics/mobile_app_testing/mobile_app_tests/restricted_networks)
        * [Settings](https://docs.datadoghq.com/synthetics/mobile_app_testing/settings)
      * [Test Suites](https://docs.datadoghq.com/synthetics/test_suites/)
      * [Platform](https://docs.datadoghq.com/synthetics/platform/)
        * [Dashboards](https://docs.datadoghq.com/synthetics/platform/dashboards)
        * [Metrics](https://docs.datadoghq.com/synthetics/platform/metrics/)
        * [Test Coverage](https://docs.datadoghq.com/synthetics/platform/test_coverage)
        * [Private Locations](https://docs.datadoghq.com/synthetics/platform/private_locations)
        * [Connect APM](https://docs.datadoghq.com/synthetics/platform/apm/)
        * [Settings](https://docs.datadoghq.com/synthetics/platform/settings)
      * [Exploring Synthetics Data](https://docs.datadoghq.com/synthetics/explore/)
        * [Saved Views](https://docs.datadoghq.com/synthetics/explore/saved_views)
        * [Results Explorer](https://docs.datadoghq.com/synthetics/explore/results_explorer)
      * [Guides](https://docs.datadoghq.com/synthetics/guide/)
      * [Notifications](https://docs.datadoghq.com/synthetics/notifications/)
        * [Template Variables](https://docs.datadoghq.com/synthetics/notifications/template_variables)
        * [Conditional Alerting](https://docs.datadoghq.com/synthetics/notifications/conditional_alerting)
        * [Advanced Notifications](https://docs.datadoghq.com/synthetics/notifications/advanced_notifications)
        * [Integrate with Statuspage](https://docs.datadoghq.com/synthetics/notifications/statuspage)
      * [Troubleshooting](https://docs.datadoghq.com/synthetics/troubleshooting/)
      * [Data Security](https://docs.datadoghq.com/data_security/synthetics/)
    * [Continuous Testing](https://docs.datadoghq.com/continuous_testing/)
      * [Local and Staging Environments](https://docs.datadoghq.com/continuous_testing/environments)
        * [Testing Multiple Environments](https://docs.datadoghq.com/continuous_testing/environments/multiple_env)
        * [Testing With Proxy, Firewall, or VPN](https://docs.datadoghq.com/continuous_testing/environments/proxy_firewall_vpn)
      * [CI/CD Integrations](https://docs.datadoghq.com/continuous_testing/cicd_integrations)
        * [Configuration](https://docs.datadoghq.com/continuous_testing/cicd_integrations/configuration)
        * [Azure DevOps Extension](https://docs.datadoghq.com/continuous_testing/cicd_integrations/azure_devops_extension)
        * [CircleCI Orb](https://docs.datadoghq.com/continuous_testing/cicd_integrations/circleci_orb)
        * [GitHub Actions](https://docs.datadoghq.com/continuous_testing/cicd_integrations/github_actions)
        * [GitLab](https://docs.datadoghq.com/continuous_testing/cicd_integrations/gitlab)
        * [Jenkins](https://docs.datadoghq.com/continuous_testing/cicd_integrations/jenkins)
        * [Bitrise (Upload Application)](https://docs.datadoghq.com/continuous_testing/cicd_integrations/bitrise_upload)
        * [Bitrise (Run Tests)](https://docs.datadoghq.com/continuous_testing/cicd_integrations/bitrise_run)
      * [Settings](https://docs.datadoghq.com/continuous_testing/settings)
      * [Results Explorer](https://docs.datadoghq.com/continuous_testing/results_explorer/)
      * [Metrics](https://docs.datadoghq.com/continuous_testing/metrics/)
      * [Guides](https://docs.datadoghq.com/continuous_testing/guide/)
      * [Troubleshooting](https://docs.datadoghq.com/continuous_testing/troubleshooting/)
    * [Product Analytics](https://docs.datadoghq.com/product_analytics)
      * [Vizualizing with Charts](https://docs.datadoghq.com/product_analytics/charts)
        * [Chart Basics](https://docs.datadoghq.com/product_analytics/charts/chart_basics)
        * [Pathways Diagram](https://docs.datadoghq.com/product_analytics/charts/pathways)
        * [Funnel Analysis](https://docs.datadoghq.com/product_analytics/charts/funnel_analysis)
        * [Retention Analysis](https://docs.datadoghq.com/product_analytics/charts/retention_analysis)
        * [Analytics Explorer](https://docs.datadoghq.com/product_analytics/charts/analytics_explorer)
      * [Dashboards](https://docs.datadoghq.com/product_analytics/dashboards)
      * [Segments](https://docs.datadoghq.com/product_analytics/segmentation/)
      * [Managing Profiles](https://docs.datadoghq.com/product_analytics/profiles)
      * [Experiments](https://docs.datadoghq.com/product_analytics/experimentation/)
        * [Define Metrics](https://docs.datadoghq.com/product_analytics/experimentation/defining_metrics)
        * [Reading Experiment Results](https://docs.datadoghq.com/product_analytics/experimentation/reading_results)
        * [Minimum Detectable Effects](https://docs.datadoghq.com/product_analytics/experimentation/minimum_detectable_effect)
      * [Guides](https://docs.datadoghq.com/product_analytics/guide/)
      * [Troubleshooting](https://docs.datadoghq.com/product_analytics/troubleshooting/)
    * [Session Replay](https://docs.datadoghq.com/session_replay/)
      * [Browser](https://docs.datadoghq.com/session_replay/browser)
        * [Setup](https://docs.datadoghq.com/session_replay/browser/setup_and_configuration)
        * [Privacy Options](https://docs.datadoghq.com/session_replay/browser/privacy_options)
        * [Developer Tools](https://docs.datadoghq.com/session_replay/browser/dev_tools)
        * [Troubleshooting](https://docs.datadoghq.com/session_replay/browser/troubleshooting)
      * [Mobile](https://docs.datadoghq.com/session_replay/mobile)
        * [Setup and Configuration](https://docs.datadoghq.com/session_replay/mobile/setup_and_configuration)
        * [Privacy Options](https://docs.datadoghq.com/session_replay/mobile/privacy_options)
        * [Developer Tools](https://docs.datadoghq.com/session_replay/mobile/dev_tools)
        * [Impact on App Performance](https://docs.datadoghq.com/session_replay/mobile/app_performance)
        * [Troubleshooting](https://docs.datadoghq.com/session_replay/mobile/troubleshooting)
      * [Playlists](https://docs.datadoghq.com/session_replay/playlists)
      * [Heatmaps](https://docs.datadoghq.com/session_replay/heatmaps)
  * [Software Delivery ](https://docs.datadoghq.com/api/latest/observability-pipelines/)
    * [CI Visibility](https://docs.datadoghq.com/continuous_integration/)
      * [Pipeline Visibility](https://docs.datadoghq.com/continuous_integration/pipelines/)
        * [AWS CodePipeline](https://docs.datadoghq.com/continuous_integration/pipelines/awscodepipeline/)
        * [Azure Pipelines](https://docs.datadoghq.com/continuous_integration/pipelines/azure/)
        * [Buildkite](https://docs.datadoghq.com/continuous_integration/pipelines/buildkite/)
        * [CircleCI](https://docs.datadoghq.com/continuous_integration/pipelines/circleci/)
        * [Codefresh](https://docs.datadoghq.com/continuous_integration/pipelines/codefresh/)
        * [GitHub Actions](https://docs.datadoghq.com/continuous_integration/pipelines/github/)
        * [GitLab](https://docs.datadoghq.com/continuous_integration/pipelines/gitlab/)
        * [Jenkins](https://docs.datadoghq.com/continuous_integration/pipelines/jenkins/)
        * [TeamCity](https://docs.datadoghq.com/continuous_integration/pipelines/teamcity/)
        * [Other CI Providers](https://docs.datadoghq.com/continuous_integration/pipelines/custom/)
        * [Custom Commands](https://docs.datadoghq.com/continuous_integration/pipelines/custom_commands/)
        * [Custom Tags and Measures](https://docs.datadoghq.com/continuous_integration/pipelines/custom_tags_and_measures/)
      * [Search and Manage](https://docs.datadoghq.com/continuous_integration/search/)
      * [Explorer](https://docs.datadoghq.com/continuous_integration/explorer)
        * [Search Syntax](https://docs.datadoghq.com/continuous_integration/explorer/search_syntax/)
        * [Search Pipeline Executions](https://docs.datadoghq.com/continuous_integration/explorer/facets/)
        * [Export](https://docs.datadoghq.com/continuous_integration/explorer/export/)
        * [Saved Views](https://docs.datadoghq.com/continuous_integration/explorer/saved_views/)
      * [Monitors](https://docs.datadoghq.com/monitors/types/ci/?tab=pipelines)
      * [Guides](https://docs.datadoghq.com/continuous_integration/guides/)
      * [Troubleshooting](https://docs.datadoghq.com/continuous_integration/troubleshooting/)
    * [CD Visibility](https://docs.datadoghq.com/continuous_delivery/)
      * [Deployment Visibility](https://docs.datadoghq.com/continuous_delivery/deployments)
        * [Argo CD](https://docs.datadoghq.com/continuous_delivery/deployments/argocd)
        * [CI Providers](https://docs.datadoghq.com/continuous_delivery/deployments/ciproviders)
      * [Explore Deployments](https://docs.datadoghq.com/continuous_delivery/explorer)
        * [Search Syntax](https://docs.datadoghq.com/continuous_delivery/explorer/search_syntax)
        * [Facets](https://docs.datadoghq.com/continuous_delivery/explorer/facets)
        * [Saved Views](https://docs.datadoghq.com/continuous_delivery/explorer/saved_views)
      * [Features](https://docs.datadoghq.com/continuous_delivery/features)
        * [Code Changes Detection](https://docs.datadoghq.com/continuous_delivery/features/code_changes_detection)
        * [Rollback Detection](https://docs.datadoghq.com/continuous_delivery/features/rollbacks_detection)
      * [Monitors](https://docs.datadoghq.com/monitors/types/ci/?tab=deployments)
    * [Deployment Gates](https://docs.datadoghq.com/deployment_gates/)
      * [Setup](https://docs.datadoghq.com/deployment_gates/setup/)
      * [Explore](https://docs.datadoghq.com/deployment_gates/explore/)
    * [Test Optimization](https://docs.datadoghq.com/tests/)
      * [Setup](https://docs.datadoghq.com/tests/setup/)
        * [.NET](https://docs.datadoghq.com/tests/setup/dotnet/)
        * [Java and JVM Languages](https://docs.datadoghq.com/tests/setup/java/)
        * [JavaScript and TypeScript](https://docs.datadoghq.com/tests/setup/javascript/)
        * [Python](https://docs.datadoghq.com/tests/setup/python/)
        * [Ruby](https://docs.datadoghq.com/tests/setup/ruby/)
        * [Swift](https://docs.datadoghq.com/tests/setup/swift/)
        * [Go](https://docs.datadoghq.com/tests/setup/go/)
        * [JUnit Report Uploads](https://docs.datadoghq.com/tests/setup/junit_xml/)
      * [Network Settings](https://docs.datadoghq.com/tests/network/)
      * [Tests in Containers](https://docs.datadoghq.com/tests/containers/)
      * [Repositories](https://docs.datadoghq.com/tests/repositories)
      * [Explorer](https://docs.datadoghq.com/tests/explorer/)
        * [Search Syntax](https://docs.datadoghq.com/tests/explorer/search_syntax)
        * [Search Test Runs](https://docs.datadoghq.com/tests/explorer/facets/)
        * [Export](https://docs.datadoghq.com/tests/explorer/export/)
        * [Saved Views](https://docs.datadoghq.com/tests/explorer/saved_views/)
      * [Monitors](https://docs.datadoghq.com/monitors/types/ci/?tab=tests)
      * [Test Health](https://docs.datadoghq.com/tests/test_health)
      * [Flaky Test Management](https://docs.datadoghq.com/tests/flaky_management)
      * [Working with Flaky Tests](https://docs.datadoghq.com/tests/flaky_tests)
        * [Early Flake Detection](https://docs.datadoghq.com/tests/flaky_tests/early_flake_detection)
        * [Auto Test Retries](https://docs.datadoghq.com/tests/flaky_tests/auto_test_retries)
      * [Test Impact Analysis](https://docs.datadoghq.com/tests/test_impact_analysis)
        * [Setup](https://docs.datadoghq.com/tests/test_impact_analysis/setup/)
        * [How It Works](https://docs.datadoghq.com/tests/test_impact_analysis/how_it_works/)
        * [Troubleshooting](https://docs.datadoghq.com/tests/test_impact_analysis/troubleshooting/)
      * [Developer Workflows](https://docs.datadoghq.com/tests/developer_workflows)
      * [Code Coverage](https://docs.datadoghq.com/tests/code_coverage)
      * [Instrument Browser Tests with RUM](https://docs.datadoghq.com/tests/browser_tests)
      * [Instrument Swift Tests with RUM](https://docs.datadoghq.com/tests/swift_tests)
      * [Correlate Logs and Tests](https://docs.datadoghq.com/tests/correlate_logs_and_tests)
      * [Guides](https://docs.datadoghq.com/tests/guides/)
      * [Troubleshooting](https://docs.datadoghq.com/tests/troubleshooting/)
    * [Code Coverage](https://docs.datadoghq.com/code_coverage/)
      * [Setup](https://docs.datadoghq.com/code_coverage/setup/)
      * [Data Collected](https://docs.datadoghq.com/code_coverage/data_collected/)
    * [PR Gates](https://docs.datadoghq.com/pr_gates/)
      * [Setup](https://docs.datadoghq.com/pr_gates/setup)
    * [DORA Metrics](https://docs.datadoghq.com/dora_metrics/)
      * [Setup](https://docs.datadoghq.com/dora_metrics/setup)
        * [Deployment Data Sources](https://docs.datadoghq.com/dora_metrics/setup/deployments)
        * [Failure Data Sources](https://docs.datadoghq.com/dora_metrics/setup/failures/)
      * [Change Failure Detection](https://docs.datadoghq.com/dora_metrics/change_failure_detection/)
      * [Data Collected](https://docs.datadoghq.com/dora_metrics/data_collected/)
    * [Feature Flags](https://docs.datadoghq.com/feature_flags/)
      * [Client SDKs](https://docs.datadoghq.com/feature_flags/client)
        * [Android and Android TV](https://docs.datadoghq.com/feature_flags/client/android)
        * [iOS and tvOS](https://docs.datadoghq.com/feature_flags/client/ios)
        * [JavaScript](https://docs.datadoghq.com/feature_flags/client/javascript)
        * [React](https://docs.datadoghq.com/feature_flags/client/react)
      * [Server SDKs](https://docs.datadoghq.com/feature_flags/server)
        * [Go](https://docs.datadoghq.com/feature_flags/server/go)
        * [Java](https://docs.datadoghq.com/feature_flags/server/java)
        * [Node.js](https://docs.datadoghq.com/feature_flags/server/nodejs)
        * [Python](https://docs.datadoghq.com/feature_flags/server/python)
        * [Ruby](https://docs.datadoghq.com/feature_flags/server/ruby)
      * [MCP Server](https://docs.datadoghq.com/feature_flags/feature_flag_mcp_server)
      * [Guides](https://docs.datadoghq.com/feature_flags/guide)
  * [Security ](https://docs.datadoghq.com/api/latest/observability-pipelines/)
    * [Security Overview](https://docs.datadoghq.com/security/)
      * [Detection Rules](https://docs.datadoghq.com/security/detection_rules/)
        * [OOTB Rules](https://docs.datadoghq.com/security/default_rules/#all)
      * [Notifications](https://docs.datadoghq.com/security/notifications/)
        * [Rules](https://docs.datadoghq.com/security/notifications/rules/)
        * [Variables](https://docs.datadoghq.com/security/notifications/variables/)
      * [Suppressions](https://docs.datadoghq.com/security/suppressions/)
      * [Automation Pipelines](https://docs.datadoghq.com/security/automation_pipelines/)
        * [Mute](https://docs.datadoghq.com/security/automation_pipelines/mute)
        * [Add to Security Inbox](https://docs.datadoghq.com/security/automation_pipelines/security_inbox)
        * [Set Due Date Rules](https://docs.datadoghq.com/security/automation_pipelines/set_due_date)
      * [Security Inbox](https://docs.datadoghq.com/security/security_inbox)
      * [Threat Intelligence](https://docs.datadoghq.com/security/threat_intelligence)
      * [Audit Trail](https://docs.datadoghq.com/security/audit_trail)
      * [Access Control](https://docs.datadoghq.com/security/access_control)
      * [Account Takeover Protection](https://docs.datadoghq.com/security/account_takeover_protection)
      * [Ticketing Integrations](https://docs.datadoghq.com/security/ticketing_integrations)
      * [Research Feed](https://docs.datadoghq.com/security/research_feed)
      * [Guides](https://docs.datadoghq.com/security/guide)
    * [Cloud SIEM](https://docs.datadoghq.com/security/cloud_siem/)
      * [Ingest and Enrich](https://docs.datadoghq.com/security/cloud_siem/ingest_and_enrich/)
        * [Content Packs](https://docs.datadoghq.com/security/cloud_siem/ingest_and_enrich/content_packs)
        * [Bring Your Own Threat Intelligence](https://docs.datadoghq.com/security/cloud_siem/ingest_and_enrich/threat_intelligence)
        * [Open Cybersecurity Schema Framework](https://docs.datadoghq.com/security/cloud_siem/ingest_and_enrich/open_cybersecurity_schema_framework)
      * [Detect and Monitor](https://docs.datadoghq.com/security/cloud_siem/detect_and_monitor/)
        * [OOTB Rules](https://docs.datadoghq.com/security/default_rules/#cat-cloud-siem-log-detection)
        * [Custom Detection Rules](https://docs.datadoghq.com/security/cloud_siem/detect_and_monitor/custom_detection_rules)
        * [Version History](https://docs.datadoghq.com/security/cloud_siem/detect_and_monitor/version_history)
        * [Suppressions](https://docs.datadoghq.com/security/cloud_siem/detect_and_monitor/suppressions)
        * [Historical Jobs](https://docs.datadoghq.com/security/cloud_siem/detect_and_monitor/historical_jobs)
        * [MITRE ATT&CK Map](https://docs.datadoghq.com/security/cloud_siem/detect_and_monitor/mitre_attack_map)
      * [Triage and Investigate](https://docs.datadoghq.com/security/cloud_siem/triage_and_investigate)
        * [Investigate Security Signals](https://docs.datadoghq.com/security/cloud_siem/triage_and_investigate/investigate_security_signals)
        * [Risk Insights](https://docs.datadoghq.com/security/cloud_siem/triage_and_investigate/entities_and_risk_scoring)
        * [IOC Explorer](https://docs.datadoghq.com/security/cloud_siem/triage_and_investigate/ioc_explorer)
        * [Investigator](https://docs.datadoghq.com/security/cloud_siem/triage_and_investigate/investigator)
      * [Respond and Report](https://docs.datadoghq.com/security/cloud_siem/respond_and_report)
        * [Security Operational Metrics](https://docs.datadoghq.com/security/cloud_siem/respond_and_report/security_operational_metrics)
      * [Guides](https://docs.datadoghq.com/security/cloud_siem/guide/)
      * [Data Security](https://docs.datadoghq.com/data_security/cloud_siem/)
    * [Code Security](https://docs.datadoghq.com/security/code_security/)
      * [Static Code Analysis (SAST)](https://docs.datadoghq.com/security/code_security/static_analysis/)
        * [Setup](https://docs.datadoghq.com/security/code_security/static_analysis/setup/)
        * [GitHub Actions](https://docs.datadoghq.com/security/code_security/static_analysis/github_actions/)
        * [Generic CI Providers](https://docs.datadoghq.com/security/code_security/static_analysis/generic_ci_providers/)
        * [AI-Enhanced Static Code Analysis](https://docs.datadoghq.com/security/code_security/static_analysis/ai_enhanced_sast/)
        * [SAST Custom Rule Creation Tutorial](https://docs.datadoghq.com/security/code_security/static_analysis/custom_rules/tutorial/)
        * [SAST Custom Rules](https://docs.datadoghq.com/security/code_security/static_analysis/custom_rules/)
        * [SAST Custom Rules Guide](https://docs.datadoghq.com/security/code_security/static_analysis/custom_rules/guide/)
        * [Static Code Analysis (SAST) rules](https://docs.datadoghq.com/security/code_security/static_analysis/static_analysis_rules/)
      * [Software Composition Analysis (SCA)](https://docs.datadoghq.com/security/code_security/software_composition_analysis/)
        * [Static Setup](https://docs.datadoghq.com/security/code_security/software_composition_analysis/setup_static/)
        * [Runtime Setup](https://docs.datadoghq.com/security/code_security/software_composition_analysis/setup_runtime/)
        * [Library Inventory](https://docs.datadoghq.com/security/code_security/software_composition_analysis/library_inventory/)
      * [Secret Scanning](https://docs.datadoghq.com/security/code_security/secret_scanning/)
        * [GitHub Actions](https://docs.datadoghq.com/security/code_security/secret_scanning/github_actions/)
        * [Generic CI Providers](https://docs.datadoghq.com/security/code_security/secret_scanning/generic_ci_providers/)
        * [Secret Validation](https://docs.datadoghq.com/security/code_security/secret_scanning/secret_validation/)
      * [Runtime Code Analysis (IAST)](https://docs.datadoghq.com/security/code_security/iast/)
        * [Setup](https://docs.datadoghq.com/security/code_security/iast/setup/)
        * [Security Controls](https://docs.datadoghq.com/security/code_security/iast/security_controls/)
      * [Infrastructure as Code (IaC) Security](https://docs.datadoghq.com/security/code_security/iac_security/)
        * [Setup](https://docs.datadoghq.com/security/code_security/iac_security/setup/)
        * [Exclusions](https://docs.datadoghq.com/security/code_security/iac_security/exclusions/)
        * [Rules](https://docs.datadoghq.com/security/code_security/iac_security/iac_rules/)
      * [Developer Tool Integrations](https://docs.datadoghq.com/security/code_security/dev_tool_int/)
        * [Pull Request Comments](https://docs.datadoghq.com/security/code_security/dev_tool_int/pull_request_comments/)
        * [PR Gates](https://docs.datadoghq.com/pr_gates/)
        * [IDE Plugins](https://docs.datadoghq.com/security/code_security/dev_tool_int/ide_plugins/)
        * [Git Hooks](https://docs.datadoghq.com/security/code_security/dev_tool_int/git_hooks/)
      * [Troubleshooting](https://docs.datadoghq.com/security/code_security/troubleshooting/)
      * [Guides](https://docs.datadoghq.com/security/code_security/guides/)
    * [Cloud Security](https://docs.datadoghq.com/security/cloud_security_management)
      * [Setup](https://docs.datadoghq.com/security/cloud_security_management/setup)
        * [Supported Deployment Types](https://docs.datadoghq.com/security/cloud_security_management/setup/supported_deployment_types)
        * [Agentless Scanning](https://docs.datadoghq.com/security/cloud_security_management/setup/agentless_scanning)
        * [Deploy the Agent](https://docs.datadoghq.com/security/cloud_security_management/setup/agent)
        * [Set Up CloudTrail Logs](https://docs.datadoghq.com/security/cloud_security_management/setup/cloudtrail_logs)
        * [Set Up without Infrastructure Monitoring](https://docs.datadoghq.com/security/cloud_security_management/setup/without_infrastructure_monitoring)
        * [Deploy via Cloud Integrations](https://docs.datadoghq.com/security/cloud_security_management/setup/cloud_integrations)
      * [Security Graph](https://docs.datadoghq.com/security/cloud_security_management/security_graph/)
      * [Misconfigurations](https://docs.datadoghq.com/security/cloud_security_management/misconfigurations/)
        * [Manage Compliance Rules](https://docs.datadoghq.com/security/cloud_security_management/misconfigurations/compliance_rules)
        * [Create Custom Rules](https://docs.datadoghq.com/security/cloud_security_management/misconfigurations/custom_rules)
        * [Manage Compliance Posture](https://docs.datadoghq.com/security/cloud_security_management/misconfigurations/frameworks_and_benchmarks)
        * [Explore Misconfigurations](https://docs.datadoghq.com/security/cloud_security_management/misconfigurations/findings)
        * [Kubernetes Security Posture Management](https://docs.datadoghq.com/security/cloud_security_management/misconfigurations/kspm)
      * [Identity Risks](https://docs.datadoghq.com/security/cloud_security_management/identity_risks/)
      * [Vulnerabilities](https://docs.datadoghq.com/security/cloud_security_management/vulnerabilities)
        * [Hosts and Containers Compatibility](https://docs.datadoghq.com/security/cloud_security_management/vulnerabilities/hosts_containers_compatibility)
      * [OOTB Rules](https://docs.datadoghq.com/security/default_rules/#cat-cloud-security-management)
      * [Review and Remediate](https://docs.datadoghq.com/security/cloud_security_management/review_remediate)
        * [Mute Issues](https://docs.datadoghq.com/security/cloud_security_management/review_remediate/mute_issues)
        * [Automate Security Workflows](https://docs.datadoghq.com/security/cloud_security_management/review_remediate/workflows)
        * [Create Jira Issues](https://docs.datadoghq.com/security/cloud_security_management/review_remediate/jira)
      * [Severity Scoring](https://docs.datadoghq.com/security/cloud_security_management/severity_scoring/)
      * [Guides](https://docs.datadoghq.com/security/cloud_security_management/guide/)
      * [Troubleshooting](https://docs.datadoghq.com/security/cloud_security_management/troubleshooting/)
        * [Vulnerabilities](https://docs.datadoghq.com/security/cloud_security_management/troubleshooting/vulnerabilities/)
    * [App and API Protection](https://docs.datadoghq.com/security/application_security/)
      * [Terms and Concepts](https://docs.datadoghq.com/security/application_security/terms/)
      * [How It Works](https://docs.datadoghq.com/security/application_security/how-it-works/)
        * [Threat Intelligence](https://docs.datadoghq.com/security/application_security/how-it-works/threat-intelligence)
        * [Trace Qualification](https://docs.datadoghq.com/security/application_security/how-it-works/trace_qualification)
        * [User Monitoring and Protection](https://docs.datadoghq.com/security/application_security/how-it-works/add-user-info)
      * [Setup](https://docs.datadoghq.com/security/application_security/setup/)
      * [Overview](https://docs.datadoghq.com/security/application_security/overview)
      * [Security Signals](https://docs.datadoghq.com/security/application_security/security_signals)
        * [Attackers Explorer](https://docs.datadoghq.com/security/application_security/security_signals/attacker-explorer/)
        * [Attacker Fingerprint](https://docs.datadoghq.com/security/application_security/security_signals/attacker_fingerprint/)
        * [Attacker Clustering](https://docs.datadoghq.com/security/application_security/security_signals/attacker_clustering/)
        * [Users Explorer](https://docs.datadoghq.com/security/application_security/security_signals/users_explorer/)
      * [Policies](https://docs.datadoghq.com/security/application_security/policies/)
        * [Custom Rules](https://docs.datadoghq.com/security/application_security/policies/custom_rules/)
        * [OOTB Rules](https://docs.datadoghq.com/security/default_rules/)
        * [In-App WAF Rules](https://docs.datadoghq.com/security/application_security/policies/inapp_waf_rules/)
        * [Tracing Library Configuration](https://docs.datadoghq.com/security/application_security/policies/library_configuration/)
      * [Exploit Prevention](https://docs.datadoghq.com/security/application_security/exploit-prevention/)
      * [WAF Integrations](https://docs.datadoghq.com/security/application_security/waf-integration/)
      * [API Security Inventory](https://docs.datadoghq.com/security/application_security/api-inventory/)
      * [Guides](https://docs.datadoghq.com/security/application_security/guide/)
      * [Troubleshooting](https://docs.datadoghq.com/security/application_security/troubleshooting/)
    * [Workload Protection](https://docs.datadoghq.com/security/workload_protection/)
      * [Setup](https://docs.datadoghq.com/security/workload_protection/setup)
        * [Deploy the Agent](https://docs.datadoghq.com/security/workload_protection/setup/agent)
        * [Workload Protection Agent Variables](https://docs.datadoghq.com/security/workload_protection/setup/agent_variables)
      * [Detection Rules](https://docs.datadoghq.com/security/workload_protection/workload_security_rules)
        * [OOTB Rules](https://docs.datadoghq.com/security/workload_protection/setup/ootb_rules)
        * [Custom Rules](https://docs.datadoghq.com/security/workload_protection/workload_security_rules/custom_rules)
      * [Investigate Security Signals](https://docs.datadoghq.com/security/workload_protection/security_signals)
      * [Investigate Agent Events](https://docs.datadoghq.com/security/workload_protection/investigate_agent_events)
      * [Creating Agent Rule Expressions](https://docs.datadoghq.com/security/workload_protection/agent_expressions)
        * [Writing Custom Rule Expressions](https://docs.datadoghq.com/security/workload_protection/secl_auth_guide)
        * [Linux Syntax](https://docs.datadoghq.com/security/workload_protection/linux_expressions)
        * [Windows Syntax](https://docs.datadoghq.com/security/workload_protection/windows_expressions)
      * [Coverage and Posture Management](https://docs.datadoghq.com/security/workload_protection/inventory)
        * [Hosts and Containers](https://docs.datadoghq.com/security/workload_protection/inventory/hosts_and_containers)
        * [Serverless](https://docs.datadoghq.com/security/workload_protection/inventory/serverless)
        * [Coverage](https://docs.datadoghq.com/security/workload_protection/inventory/coverage_map)
      * [Guides](https://docs.datadoghq.com/security/workload_protection/guide)
      * [Troubleshooting](https://docs.datadoghq.com/security/workload_protection/troubleshooting/threats)
    * [Sensitive Data Scanner](https://docs.datadoghq.com/security/sensitive_data_scanner/)
      * [Setup](https://docs.datadoghq.com/security/sensitive_data_scanner/setup/)
        * [Telemetry Data](https://docs.datadoghq.com/security/sensitive_data_scanner/setup/telemetry_data/)
        * [Cloud Storage](https://docs.datadoghq.com/security/sensitive_data_scanner/setup/cloud_storage/)
      * [Scanning Rules](https://docs.datadoghq.com/security/sensitive_data_scanner/scanning_rules/)
        * [Library Rules](https://docs.datadoghq.com/security/sensitive_data_scanner/scanning_rules/library_rules/)
        * [Custom Rules](https://docs.datadoghq.com/security/sensitive_data_scanner/scanning_rules/custom_rules/)
      * [Guides](https://docs.datadoghq.com/security/sensitive_data_scanner/guide/)
  * [AI Observability ](https://docs.datadoghq.com/api/latest/observability-pipelines/)
    * [LLM Observability](https://docs.datadoghq.com/llm_observability/)
      * [Quickstart](https://docs.datadoghq.com/llm_observability/quickstart/)
      * [Instrumentation](https://docs.datadoghq.com/llm_observability/instrumentation/)
        * [Automatic](https://docs.datadoghq.com/llm_observability/instrumentation/auto_instrumentation)
        * [SDK Reference](https://docs.datadoghq.com/llm_observability/instrumentation/sdk)
        * [HTTP API](https://docs.datadoghq.com/llm_observability/instrumentation/api)
        * [OpenTelemetry](https://docs.datadoghq.com/llm_observability/instrumentation/otel_instrumentation)
      * [Monitoring](https://docs.datadoghq.com/llm_observability/monitoring)
        * [Querying spans and traces](https://docs.datadoghq.com/llm_observability/monitoring/querying)
        * [Correlate with APM](https://docs.datadoghq.com/llm_observability/monitoring/llm_observability_and_apm)
        * [Cluster Map](https://docs.datadoghq.com/llm_observability/monitoring/cluster_map/)
        * [Agent Monitoring](https://docs.datadoghq.com/llm_observability/monitoring/agent_monitoring)
        * [MCP Clients](https://docs.datadoghq.com/llm_observability/monitoring/mcp_client)
        * [Prompt Tracking](https://docs.datadoghq.com/llm_observability/monitoring/prompt_tracking)
        * [Metrics](https://docs.datadoghq.com/llm_observability/monitoring/metrics)
        * [Cost](https://docs.datadoghq.com/llm_observability/monitoring/cost)
      * [Evaluations](https://docs.datadoghq.com/llm_observability/evaluations/)
        * [Managed Evaluations](https://docs.datadoghq.com/llm_observability/evaluations/managed_evaluations)
        * [Custom LLM-as-a-Judge](https://docs.datadoghq.com/llm_observability/evaluations/custom_llm_as_a_judge_evaluations)
        * [External Evaluations](https://docs.datadoghq.com/llm_observability/evaluations/external_evaluations)
        * [Compatibility](https://docs.datadoghq.com/llm_observability/evaluations/evaluation_compatibility)
        * [Export API](https://docs.datadoghq.com/llm_observability/evaluations/export_api)
      * [Experiments](https://docs.datadoghq.com/llm_observability/experiments)
      * [Data Security and RBAC](https://docs.datadoghq.com/llm_observability/data_security_and_rbac)
      * [Terms and Concepts](https://docs.datadoghq.com/llm_observability/terms/)
      * [Guides](https://docs.datadoghq.com/llm_observability/guide/)
  * [Log Management ](https://docs.datadoghq.com/api/latest/observability-pipelines/)
    * [Observability Pipelines](https://docs.datadoghq.com/observability_pipelines/)
      * [Configuration](https://docs.datadoghq.com/observability_pipelines/configuration/)
        * [Explore Templates](https://docs.datadoghq.com/observability_pipelines/configuration/explore_templates/)
        * [Set Up Pipelines](https://docs.datadoghq.com/observability_pipelines/configuration/set_up_pipelines/)
        * [Install the Worker](https://docs.datadoghq.com/observability_pipelines/configuration/install_the_worker/)
        * [Live Capture](https://docs.datadoghq.com/observability_pipelines/configuration/live_capture/)
        * [Update Existing Pipelines](https://docs.datadoghq.com/observability_pipelines/configuration/update_existing_pipelines)
        * [Access Control](https://docs.datadoghq.com/observability_pipelines/configuration/access_control)
      * [Sources](https://docs.datadoghq.com/observability_pipelines/sources/)
        * [Akamai DataStream](https://docs.datadoghq.com/observability_pipelines/sources/akamai_datastream/)
        * [Amazon Data Firehose](https://docs.datadoghq.com/observability_pipelines/sources/amazon_data_firehose/)
        * [Amazon S3](https://docs.datadoghq.com/observability_pipelines/sources/amazon_s3/)
        * [Azure Event Hubs](https://docs.datadoghq.com/observability_pipelines/sources/azure_event_hubs/)
        * [Datadog Agent](https://docs.datadoghq.com/observability_pipelines/sources/datadog_agent/)
        * [Datadog Lambda Extension](https://docs.datadoghq.com/observability_pipelines/sources/lambda_extension/)
        * [Datadog Lambda Forwarder](https://docs.datadoghq.com/observability_pipelines/sources/lambda_forwarder/)
        * [Filebeat](https://docs.datadoghq.com/observability_pipelines/sources/filebeat/)
        * [Fluent](https://docs.datadoghq.com/observability_pipelines/sources/fluent/)
        * [Google Pub/Sub](https://docs.datadoghq.com/observability_pipelines/sources/google_pubsub/)
        * [HTTP Client](https://docs.datadoghq.com/observability_pipelines/sources/http_client/)
        * [HTTP Server](https://docs.datadoghq.com/observability_pipelines/sources/http_server/)
        * [OpenTelemetry](https://docs.datadoghq.com/observability_pipelines/sources/opentelemetry/)
        * [Kafka](https://docs.datadoghq.com/observability_pipelines/sources/kafka/)
        * [Logstash](https://docs.datadoghq.com/observability_pipelines/sources/logstash/)
        * [Socket](https://docs.datadoghq.com/observability_pipelines/sources/socket/)
        * [Splunk HEC](https://docs.datadoghq.com/observability_pipelines/sources/splunk_hec/)
        * [Splunk TCP](https://docs.datadoghq.com/observability_pipelines/sources/splunk_tcp/)
        * [Sumo Logic Hosted Collector](https://docs.datadoghq.com/observability_pipelines/sources/sumo_logic/)
        * [Syslog](https://docs.datadoghq.com/observability_pipelines/sources/syslog/)
      * [Processors](https://docs.datadoghq.com/observability_pipelines/processors/)
        * [Add Environment Variables](https://docs.datadoghq.com/observability_pipelines/processors/add_environment_variables)
        * [Add hostname](https://docs.datadoghq.com/observability_pipelines/processors/add_hostname)
        * [Custom Processor](https://docs.datadoghq.com/observability_pipelines/processors/custom_processor)
        * [Deduplicate](https://docs.datadoghq.com/observability_pipelines/processors/dedupe)
        * [Edit fields](https://docs.datadoghq.com/observability_pipelines/processors/edit_fields)
        * [Enrichment Table](https://docs.datadoghq.com/observability_pipelines/processors/enrichment_table)
        * [Filter](https://docs.datadoghq.com/observability_pipelines/processors/filter)
        * [Generate Metrics](https://docs.datadoghq.com/observability_pipelines/processors/generate_metrics)
        * [Grok Parser](https://docs.datadoghq.com/observability_pipelines/processors/grok_parser)
        * [Parse JSON](https://docs.datadoghq.com/observability_pipelines/processors/parse_json)
        * [Parse XML](https://docs.datadoghq.com/observability_pipelines/processors/parse_xml)
        * [Quota](https://docs.datadoghq.com/observability_pipelines/processors/quota)
        * [Reduce](https://docs.datadoghq.com/observability_pipelines/processors/reduce)
        * [Remap to OCSF](https://docs.datadoghq.com/observability_pipelines/processors/remap_ocsf)
        * [Sample](https://docs.datadoghq.com/observability_pipelines/processors/sample)
        * [Sensitive Data Scanner](https://docs.datadoghq.com/observability_pipelines/processors/sensitive_data_scanner)
        * [Split Array](https://docs.datadoghq.com/observability_pipelines/processors/split_array)
        * [Tag Control](https://docs.datadoghq.com/observability_pipelines/processors/tag_control/logs/)
        * [Throttle](https://docs.datadoghq.com/observability_pipelines/processors/throttle)
      * [Destinations](https://docs.datadoghq.com/observability_pipelines/destinations/)
        * [Amazon OpenSearch](https://docs.datadoghq.com/observability_pipelines/destinations/amazon_opensearch/)
        * [Amazon S3](https://docs.datadoghq.com/observability_pipelines/destinations/amazon_s3/)
        * [Amazon Security Lake](https://docs.datadoghq.com/observability_pipelines/destinations/amazon_security_lake/)
        * [Azure Storage](https://docs.datadoghq.com/observability_pipelines/destinations/azure_storage/)
        * [CrowdStrike NG-SIEM](https://docs.datadoghq.com/observability_pipelines/destinations/crowdstrike_ng_siem/)
        * [Datadog CloudPrem](https://docs.datadoghq.com/observability_pipelines/destinations/cloudprem/)
        * [Datadog Logs](https://docs.datadoghq.com/observability_pipelines/destinations/datadog_logs/)
        * [Datadog Metrics](https://docs.datadoghq.com/observability_pipelines/destinations/datadog_metrics/)
        * [Elasticsearch](https://docs.datadoghq.com/observability_pipelines/destinations/elasticsearch/)
        * [Google Cloud Storage](https://docs.datadoghq.com/observability_pipelines/destinations/google_cloud_storage/)
        * [Google Pub/Sub](https://docs.datadoghq.com/observability_pipelines/destinations/google_pubsub/)
        * [Google SecOps](https://docs.datadoghq.com/observability_pipelines/destinations/google_secops/)
        * [HTTP Client](https://docs.datadoghq.com/observability_pipelines/destinations/http_client/)
        * [Kafka](https://docs.datadoghq.com/observability_pipelines/destinations/kafka/)
        * [Microsoft Sentinel](https://docs.datadoghq.com/observability_pipelines/destinations/microsoft_sentinel/)
        * [New Relic](https://docs.datadoghq.com/observability_pipelines/destinations/new_relic/)
        * [OpenSearch](https://docs.datadoghq.com/observability_pipelines/destinations/opensearch)
        * [SentinelOne](https://docs.datadoghq.com/observability_pipelines/destinations/sentinelone)
        * [Socket](https://docs.datadoghq.com/observability_pipelines/destinations/socket)
        * [Splunk HEC](https://docs.datadoghq.com/observability_pipelines/destinations/splunk_hec)
        * [Sumo Logic Hosted Collector](https://docs.datadoghq.com/observability_pipelines/destinations/sumo_logic_hosted_collector)
        * [Syslog](https://docs.datadoghq.com/observability_pipelines/destinations/syslog)
      * [Packs](https://docs.datadoghq.com/observability_pipelines/packs/)
        * [Akamai CDN](https://docs.datadoghq.com/observability_pipelines/packs/akamai_cdn/)
        * [Amazon CloudFront](https://docs.datadoghq.com/observability_pipelines/packs/amazon_cloudfront/)
        * [Amazon VPC Flow Logs](https://docs.datadoghq.com/observability_pipelines/packs/amazon_vpc_flow_logs/)
        * [AWS CloudTrail](https://docs.datadoghq.com/observability_pipelines/packs/aws_cloudtrail/)
        * [Cisco ASA](https://docs.datadoghq.com/observability_pipelines/packs/cisco_asa/)
        * [Cloudflare](https://docs.datadoghq.com/observability_pipelines/packs/cloudflare/)
        * [F5](https://docs.datadoghq.com/observability_pipelines/packs/f5/)
        * [Fastly](https://docs.datadoghq.com/observability_pipelines/packs/fastly/)
        * [Fortinet Firewall](https://docs.datadoghq.com/observability_pipelines/packs/fortinet_firewall/)
        * [HAProxy Ingress](https://docs.datadoghq.com/observability_pipelines/packs/haproxy_ingress/)
        * [Istio Proxy](https://docs.datadoghq.com/observability_pipelines/packs/istio_proxy/)
        * [Netskope](https://docs.datadoghq.com/observability_pipelines/packs/netskope/)
        * [NGINX](https://docs.datadoghq.com/observability_pipelines/packs/nginx/)
        * [Okta](https://docs.datadoghq.com/observability_pipelines/packs/okta/)
        * [Palo Alto Firewall](https://docs.datadoghq.com/observability_pipelines/packs/palo_alto_firewall/)
        * [Windows XML](https://docs.datadoghq.com/observability_pipelines/packs/windows_xml/)
        * [ZScaler ZIA DNS](https://docs.datadoghq.com/observability_pipelines/packs/zscaler_zia_dns/)
        * [Zscaler ZIA Firewall](https://docs.datadoghq.com/observability_pipelines/packs/zscaler_zia_firewall/)
        * [Zscaler ZIA Tunnel](https://docs.datadoghq.com/observability_pipelines/packs/zscaler_zia_tunnel/)
        * [Zscaler ZIA Web Logs](https://docs.datadoghq.com/observability_pipelines/packs/zscaler_zia_web_logs/)
      * [Search Syntax](https://docs.datadoghq.com/observability_pipelines/search_syntax/logs/)
      * [Scaling and Performance](https://docs.datadoghq.com/observability_pipelines/scaling_and_performance/)
        * [Handling Load and Backpressure](https://docs.datadoghq.com/observability_pipelines/scaling_and_performance/handling_load_and_backpressure/)
        * [Scaling Best Practices](https://docs.datadoghq.com/observability_pipelines/scaling_and_performance/best_practices_for_scaling_observability_pipelines/)
      * [Monitoring and Troubleshooting](https://docs.datadoghq.com/observability_pipelines/monitoring_and_troubleshooting/)
        * [Worker CLI Commands](https://docs.datadoghq.com/observability_pipelines/monitoring_and_troubleshooting/worker_cli_commands/)
        * [Monitoring Pipelines](https://docs.datadoghq.com/observability_pipelines/monitoring_and_troubleshooting/monitoring_pipelines/)
        * [Pipeline Usage Metrics](https://docs.datadoghq.com/observability_pipelines/monitoring_and_troubleshooting/pipeline_usage_metrics/)
        * [Troubleshooting](https://docs.datadoghq.com/observability_pipelines/monitoring_and_troubleshooting/troubleshooting/)
      * [Guides and Resources](https://docs.datadoghq.com/observability_pipelines/guide/)
        * [Upgrade Worker Guide](https://docs.datadoghq.com/observability_pipelines/guide/upgrade_worker/)
    * [Log Management](https://docs.datadoghq.com/logs/)
      * [Log Collection & Integrations](https://docs.datadoghq.com/logs/log_collection/)
        * [Browser](https://docs.datadoghq.com/logs/log_collection/javascript/)
        * [Android](https://docs.datadoghq.com/logs/log_collection/android/)
        * [iOS](https://docs.datadoghq.com/logs/log_collection/ios/)
        * [Flutter](https://docs.datadoghq.com/logs/log_collection/flutter/)
        * [React Native](https://docs.datadoghq.com/logs/log_collection/reactnative/)
        * [Roku](https://docs.datadoghq.com/logs/log_collection/roku/)
        * [Kotlin Multiplatform](https://docs.datadoghq.com/logs/log_collection/kotlin_multiplatform/)
        * [C#](https://docs.datadoghq.com/logs/log_collection/csharp/)
        * [Go](https://docs.datadoghq.com/logs/log_collection/go/)
        * [Java](https://docs.datadoghq.com/logs/log_collection/java/)
        * [Node.js](https://docs.datadoghq.com/logs/log_collection/nodejs/)
        * [PHP](https://docs.datadoghq.com/logs/log_collection/php/)
        * [Python](https://docs.datadoghq.com/logs/log_collection/python/)
        * [Ruby](https://docs.datadoghq.com/logs/log_collection/ruby/)
        * [OpenTelemetry](https://docs.datadoghq.com/opentelemetry/otel_logs/)
        * [Agent Integrations](https://docs.datadoghq.com/logs/log_collection/agent_checks/)
        * [Other Integrations](https://docs.datadoghq.com/integrations/#cat-log-collection)
      * [Log Configuration](https://docs.datadoghq.com/logs/log_configuration/)
        * [Pipelines](https://docs.datadoghq.com/logs/log_configuration/pipelines/)
        * [Processors](https://docs.datadoghq.com/logs/log_configuration/processors/)
        * [Parsing](https://docs.datadoghq.com/logs/log_configuration/parsing/)
        * [Pipeline Scanner](https://docs.datadoghq.com/logs/log_configuration/pipeline_scanner/)
        * [Attributes and Aliasing](https://docs.datadoghq.com/logs/log_configuration/attributes_naming_convention/)
        * [Generate Metrics](https://docs.datadoghq.com/logs/log_configuration/logs_to_metrics/)
        * [Indexes](https://docs.datadoghq.com/logs/log_configuration/indexes)
        * [Flex Logs](https://docs.datadoghq.com/logs/log_configuration/flex_logs/)
        * [Archives](https://docs.datadoghq.com/logs/log_configuration/archives/)
        * [Rehydrate from Archives](https://docs.datadoghq.com/logs/log_configuration/rehydrating)
        * [Archive Search](https://docs.datadoghq.com/logs/log_configuration/archive_search)
        * [Forwarding](https://docs.datadoghq.com/logs/log_configuration/forwarding_custom_destinations/)
      * [Log Explorer](https://docs.datadoghq.com/logs/explorer/)
        * [Live Tail](https://docs.datadoghq.com/logs/explorer/live_tail/)
        * [Search Logs](https://docs.datadoghq.com/logs/explorer/search/)
        * [Search Syntax](https://docs.datadoghq.com/logs/explorer/search_syntax/)
        * [Advanced Search](https://docs.datadoghq.com/logs/explorer/advanced_search)
        * [Facets](https://docs.datadoghq.com/logs/explorer/facets/)
        * [Calculated Fields](https://docs.datadoghq.com/logs/explorer/calculated_fields/)
        * [Analytics](https://docs.datadoghq.com/logs/explorer/analytics/)
        * [Patterns](https://docs.datadoghq.com/logs/explorer/analytics/patterns/)
        * [Transactions](https://docs.datadoghq.com/logs/explorer/analytics/transactions/)
        * [Visualize](https://docs.datadoghq.com/logs/explorer/visualize/)
        * [Log Side Panel](https://docs.datadoghq.com/logs/explorer/side_panel/)
        * [Export](https://docs.datadoghq.com/logs/explorer/export/)
        * [Watchdog Insights for Logs](https://docs.datadoghq.com/logs/explorer/watchdog_insights/)
        * [Saved Views](https://docs.datadoghq.com/logs/explorer/saved_views/)
      * [Error Tracking](https://docs.datadoghq.com/logs/error_tracking/)
        * [Error Tracking Explorer](https://docs.datadoghq.com/logs/error_tracking/explorer)
        * [Issue States](https://docs.datadoghq.com/logs/error_tracking/issue_states)
        * [Track Browser and Mobile Errors](https://docs.datadoghq.com/logs/error_tracking/browser_and_mobile)
        * [Track Backend Errors](https://docs.datadoghq.com/logs/error_tracking/backend)
        * [Error Grouping](https://docs.datadoghq.com/logs/error_tracking/error_grouping)
        * [Manage Data Collection](https://docs.datadoghq.com/logs/error_tracking/manage_data_collection)
        * [Dynamic Sampling](https://docs.datadoghq.com/logs/error_tracking/dynamic_sampling)
        * [Monitors](https://docs.datadoghq.com/logs/error_tracking/monitors)
        * [Identify Suspect Commits](https://docs.datadoghq.com/logs/error_tracking/suspect_commits)
        * [Troubleshooting](https://docs.datadoghq.com/error_tracking/troubleshooting)
      * [Reports](https://docs.datadoghq.com/logs/reports/)
      * [Guides](https://docs.datadoghq.com/logs/guide/)
      * [Data Security](https://docs.datadoghq.com/data_security/logs/)
      * [Troubleshooting](https://docs.datadoghq.com/logs/troubleshooting)
        * [Live Tail](https://docs.datadoghq.com/logs/troubleshooting/live_tail)
    * [CloudPrem](https://docs.datadoghq.com/cloudprem/)
      * [Quickstart](https://docs.datadoghq.com/cloudprem/quickstart)
      * [Architecture](https://docs.datadoghq.com/cloudprem/architecture)
      * [Installation](https://docs.datadoghq.com/cloudprem/install)
        * [AWS EKS](https://docs.datadoghq.com/cloudprem/install/aws_eks)
        * [Azure AKS](https://docs.datadoghq.com/cloudprem/install/azure_aks)
        * [Install Locally with Docker](https://docs.datadoghq.com/cloudprem/install/docker)
      * [Log Ingestion](https://docs.datadoghq.com/cloudprem/ingest_logs/)
        * [Datadog Agent](https://docs.datadoghq.com/cloudprem/ingest_logs/datadog_agent)
        * [Observability Pipelines](https://docs.datadoghq.com/cloudprem/ingest_logs/observability_pipelines)
        * [REST API](https://docs.datadoghq.com/cloudprem/ingest_logs/rest_api/)
      * [Configuration](https://docs.datadoghq.com/cloudprem/configure/)
        * [Datadog Account](https://docs.datadoghq.com/cloudprem/configure/datadog_account)
        * [AWS Configuration](https://docs.datadoghq.com/cloudprem/configure/aws_config)
        * [Azure Configuration](https://docs.datadoghq.com/cloudprem/configure/azure_config)
        * [Cluster Sizing](https://docs.datadoghq.com/cloudprem/configure/cluster_sizing)
        * [Ingress](https://docs.datadoghq.com/cloudprem/configure/ingress)
        * [Processing](https://docs.datadoghq.com/cloudprem/configure/processing)
        * [Reverse Connection](https://docs.datadoghq.com/cloudprem/configure/reverse_connection)
      * [Management](https://docs.datadoghq.com/cloudprem/manage/)
      * [Supported Features](https://docs.datadoghq.com/cloudprem/supported_features/)
      * [Troubleshooting](https://docs.datadoghq.com/cloudprem/troubleshooting/)
  * [Administration ](https://docs.datadoghq.com/api/latest/observability-pipelines/)
    * [Account Management](https://docs.datadoghq.com/account_management/)
      * [Switching Between Orgs](https://docs.datadoghq.com/account_management/org_switching/)
      * [Organization Settings](https://docs.datadoghq.com/account_management/org_settings/)
        * [User Management](https://docs.datadoghq.com/account_management/users/)
        * [Login Methods](https://docs.datadoghq.com/account_management/login_methods/)
        * [OAuth Apps](https://docs.datadoghq.com/account_management/org_settings/oauth_apps)
        * [Custom Organization Landing Page](https://docs.datadoghq.com/account_management/org_settings/custom_landing)
        * [Service Accounts](https://docs.datadoghq.com/account_management/org_settings/service_accounts)
        * [IP Allowlist](https://docs.datadoghq.com/account_management/org_settings/ip_allowlist)
        * [Domain Allowlist](https://docs.datadoghq.com/account_management/org_settings/domain_allowlist)
        * [Cross-Organization Visibility](https://docs.datadoghq.com/account_management/org_settings/cross_org_visibility)
      * [Access Control](https://docs.datadoghq.com/account_management/rbac/)
        * [Granular Access](https://docs.datadoghq.com/account_management/rbac/granular_access)
        * [Permissions](https://docs.datadoghq.com/account_management/rbac/permissions)
        * [Data Access](https://docs.datadoghq.com/account_management/rbac/data_access)
      * [SSO with SAML](https://docs.datadoghq.com/account_management/saml/)
        * [Configuring SAML](https://docs.datadoghq.com/account_management/saml/configuration/)
        * [User Group Mapping](https://docs.datadoghq.com/account_management/saml/mapping/)
        * [Active Directory](https://docs.datadoghq.com/account_management/saml/activedirectory/)
        * [Auth0](https://docs.datadoghq.com/account_management/saml/auth0/)
        * [Entra ID](https://docs.datadoghq.com/account_management/saml/entra/)
        * [Google](https://docs.datadoghq.com/account_management/saml/google/)
        * [LastPass](https://docs.datadoghq.com/account_management/saml/lastpass/)
        * [Okta](https://docs.datadoghq.com/account_management/saml/okta/)
        * [SafeNet](https://docs.datadoghq.com/account_management/saml/safenet/)
        * [Troubleshooting](https://docs.datadoghq.com/account_management/saml/troubleshooting/)
      * [SCIM](https://docs.datadoghq.com/account_management/scim/)
        * [Okta](https://docs.datadoghq.com/account_management/scim/okta)
        * [Microsoft Entra ID](https://docs.datadoghq.com/account_management/scim/entra)
      * [API and Application Keys](https://docs.datadoghq.com/account_management/api-app-keys/)
      * [Teams](https://docs.datadoghq.com/account_management/teams/)
        * [Team Management](https://docs.datadoghq.com/account_management/teams/manage/)
        * [Provision with GitHub](https://docs.datadoghq.com/account_management/teams/github/)
      * [Governance Console](https://docs.datadoghq.com/account_management/governance_console/)
      * [Multi-Factor Authentication](https://docs.datadoghq.com/account_management/multi-factor_authentication/)
      * [Audit Trail](https://docs.datadoghq.com/account_management/audit_trail/)
        * [Events](https://docs.datadoghq.com/account_management/audit_trail/events/)
        * [Forwarding](https://docs.datadoghq.com/account_management/audit_trail/forwarding_audit_events/)
        * [Guides](https://docs.datadoghq.com/account_management/audit_trail/guides/)
      * [Safety Center](https://docs.datadoghq.com/account_management/safety_center/)
      * [Plan and Usage](https://docs.datadoghq.com/account_management/plan_and_usage/)
        * [Cost Details](https://docs.datadoghq.com/account_management/plan_and_usage/cost_details/)
        * [Usage Details](https://docs.datadoghq.com/account_management/plan_and_usage/usage_details/)
      * [Billing](https://docs.datadoghq.com/account_management/billing/)
        * [Pricing](https://docs.datadoghq.com/account_management/billing/pricing)
        * [Credit Card](https://docs.datadoghq.com/account_management/billing/credit_card/)
        * [Product Allotments](https://docs.datadoghq.com/account_management/billing/product_allotments)
        * [Usage Metrics](https://docs.datadoghq.com/account_management/billing/usage_metrics/)
        * [Usage Attribution](https://docs.datadoghq.com/account_management/billing/usage_attribution/)
        * [Custom Metrics](https://docs.datadoghq.com/account_management/billing/custom_metrics/)
        * [Containers](https://docs.datadoghq.com/account_management/billing/containers)
        * [Log Management](https://docs.datadoghq.com/account_management/billing/log_management/)
        * [APM](https://docs.datadoghq.com/account_management/billing/apm_tracing_profiler/)
        * [Serverless](https://docs.datadoghq.com/account_management/billing/serverless/)
        * [Real User Monitoring](https://docs.datadoghq.com/account_management/billing/rum/)
        * [CI Visibility](https://docs.datadoghq.com/account_management/billing/ci_visibility/)
        * [Incident Response](https://docs.datadoghq.com/account_management/billing/incident_response/)
        * [AWS Integration](https://docs.datadoghq.com/account_management/billing/aws/)
        * [Azure Integration](https://docs.datadoghq.com/account_management/billing/azure/)
        * [Google Cloud Integration](https://docs.datadoghq.com/account_management/billing/google_cloud/)
        * [Alibaba Integration](https://docs.datadoghq.com/account_management/billing/alibaba/)
        * [vSphere Integration](https://docs.datadoghq.com/account_management/billing/vsphere/)
        * [Workflow Automation](https://docs.datadoghq.com/account_management/billing/workflow_automation/)
      * [Multi-org Accounts](https://docs.datadoghq.com/account_management/multi_organization/)
      * [Guides](https://docs.datadoghq.com/account_management/guide/)
      * [Cloud-based Authentication](https://docs.datadoghq.com/account_management/cloud_provider_authentication/)
    * [Data Security](https://docs.datadoghq.com/data_security/)
      * [Agent](https://docs.datadoghq.com/data_security/agent/)
      * [Cloud SIEM](https://docs.datadoghq.com/data_security/cloud_siem/)
      * [Kubernetes](https://docs.datadoghq.com/data_security/kubernetes)
      * [Log Management](https://docs.datadoghq.com/data_security/logs/)
      * [Real User Monitoring](https://docs.datadoghq.com/data_security/real_user_monitoring/)
      * [Synthetic Monitoring](https://docs.datadoghq.com/data_security/synthetics/)
      * [Tracing](https://docs.datadoghq.com/tracing/configure_data_security/)
      * [PCI Compliance](https://docs.datadoghq.com/data_security/pci_compliance/)
      * [HIPAA Compliance](https://docs.datadoghq.com/data_security/hipaa_compliance/)
      * [Data Retention Periods](https://docs.datadoghq.com/data_security/data_retention_periods/)
      * [Guides](https://docs.datadoghq.com/data_security/guide/)
    * [Help](https://docs.datadoghq.com/help/)


[Datadog Docs API](https://docs.datadoghq.com/)
search
  * [Overview](https://docs.datadoghq.com/api/latest/)
    * [Using the API](https://docs.datadoghq.com/api/latest/using-the-api/)
    * [Authorization Scopes](https://docs.datadoghq.com/api/latest/scopes/)
    * [Rate Limits](https://docs.datadoghq.com/api/latest/rate-limits/)
  * [Action Connection](https://docs.datadoghq.com/api/latest/action-connection/)
    * [Get an existing Action Connection](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-an-existing-action-connection)
    * [Create a new Action Connection](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-new-action-connection)
    * [Update an existing Action Connection](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-an-existing-action-connection)
    * [Delete an existing Action Connection](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-existing-action-connection)
    * [Register a new App Key](https://docs.datadoghq.com/api/latest/observability-pipelines/#register-a-new-app-key)
    * [List App Key Registrations](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-app-key-registrations)
    * [Unregister an App Key](https://docs.datadoghq.com/api/latest/observability-pipelines/#unregister-an-app-key)
    * [Get an existing App Key Registration](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-an-existing-app-key-registration)
  * [Actions Datastores](https://docs.datadoghq.com/api/latest/actions-datastores/)
    * [List datastores](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-datastores)
    * [Create datastore](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-datastore)
    * [Get datastore](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-datastore)
    * [Update datastore](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-datastore)
    * [Delete datastore](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-datastore)
    * [List datastore items](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-datastore-items)
    * [Delete datastore item](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-datastore-item)
    * [Update datastore item](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-datastore-item)
    * [Bulk write datastore items](https://docs.datadoghq.com/api/latest/observability-pipelines/#bulk-write-datastore-items)
    * [Bulk delete datastore items](https://docs.datadoghq.com/api/latest/observability-pipelines/#bulk-delete-datastore-items)
  * [Agentless Scanning](https://docs.datadoghq.com/api/latest/agentless-scanning/)
    * [List AWS scan options](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-aws-scan-options)
    * [Create AWS scan options](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-aws-scan-options)
    * [Get AWS scan options](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-aws-scan-options)
    * [Update AWS scan options](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-aws-scan-options)
    * [Delete AWS scan options](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-aws-scan-options)
    * [List Azure scan options](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-azure-scan-options)
    * [Create Azure scan options](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-azure-scan-options)
    * [Get Azure scan options](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-azure-scan-options)
    * [Update Azure scan options](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-azure-scan-options)
    * [Delete Azure scan options](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-azure-scan-options)
    * [List GCP scan options](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-gcp-scan-options)
    * [Create GCP scan options](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-gcp-scan-options)
    * [Get GCP scan options](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-gcp-scan-options)
    * [Update GCP scan options](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-gcp-scan-options)
    * [Delete GCP scan options](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-gcp-scan-options)
    * [List AWS on demand tasks](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-aws-on-demand-tasks)
    * [Create AWS on demand task](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-aws-on-demand-task)
    * [Get AWS on demand task](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-aws-on-demand-task)
  * [API Management](https://docs.datadoghq.com/api/latest/api-management/)
    * [Create a new API](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-new-api)
    * [Update an API](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-an-api)
    * [Get an API](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-an-api)
    * [List APIs](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-apis)
    * [Delete an API](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-api)
  * [APM](https://docs.datadoghq.com/api/latest/apm/)
    * [Get service list](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-service-list)
  * [APM Retention Filters](https://docs.datadoghq.com/api/latest/apm-retention-filters/)
    * [List all APM retention filters](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-all-apm-retention-filters)
    * [Create a retention filter](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-retention-filter)
    * [Get a given APM retention filter](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-given-apm-retention-filter)
    * [Update a retention filter](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-retention-filter)
    * [Delete a retention filter](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-retention-filter)
    * [Re-order retention filters](https://docs.datadoghq.com/api/latest/observability-pipelines/#re-order-retention-filters)
  * [App Builder](https://docs.datadoghq.com/api/latest/app-builder/)
    * [List Apps](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-apps)
    * [Create App](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-app)
    * [Delete Multiple Apps](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-multiple-apps)
    * [Get App](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-app)
    * [Update App](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-app)
    * [Delete App](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-app)
    * [Publish App](https://docs.datadoghq.com/api/latest/observability-pipelines/#publish-app)
    * [Unpublish App](https://docs.datadoghq.com/api/latest/observability-pipelines/#unpublish-app)
  * [Application Security](https://docs.datadoghq.com/api/latest/application-security/)
    * [Get a WAF exclusion filter](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-waf-exclusion-filter)
    * [Create a WAF exclusion filter](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-waf-exclusion-filter)
    * [List all WAF exclusion filters](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-all-waf-exclusion-filters)
    * [Update a WAF exclusion filter](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-waf-exclusion-filter)
    * [Delete a WAF exclusion filter](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-waf-exclusion-filter)
    * [Get a WAF custom rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-waf-custom-rule)
    * [Create a WAF custom rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-waf-custom-rule)
    * [List all WAF custom rules](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-all-waf-custom-rules)
    * [Update a WAF Custom Rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-waf-custom-rule)
    * [Delete a WAF Custom Rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-waf-custom-rule)
  * [Audit](https://docs.datadoghq.com/api/latest/audit/)
    * [Search Audit Logs events](https://docs.datadoghq.com/api/latest/observability-pipelines/#search-audit-logs-events)
    * [Get a list of Audit Logs events](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-list-of-audit-logs-events)
  * [Authentication](https://docs.datadoghq.com/api/latest/authentication/)
    * [Validate API key](https://docs.datadoghq.com/api/latest/observability-pipelines/#validate-api-key)
  * [AuthN Mappings](https://docs.datadoghq.com/api/latest/authn-mappings/)
    * [Get an AuthN Mapping by UUID](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-an-authn-mapping-by-uuid)
    * [Edit an AuthN Mapping](https://docs.datadoghq.com/api/latest/observability-pipelines/#edit-an-authn-mapping)
    * [Delete an AuthN Mapping](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-authn-mapping)
    * [List all AuthN Mappings](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-all-authn-mappings)
    * [Create an AuthN Mapping](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-an-authn-mapping)
  * [AWS Integration](https://docs.datadoghq.com/api/latest/aws-integration/)
    * [Get all AWS tag filters](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-aws-tag-filters)
    * [List available namespaces](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-available-namespaces)
    * [Set an AWS tag filter](https://docs.datadoghq.com/api/latest/observability-pipelines/#set-an-aws-tag-filter)
    * [Delete a tag filtering entry](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-tag-filtering-entry)
    * [Get an AWS integration by config ID](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-an-aws-integration-by-config-id)
    * [Generate a new external ID](https://docs.datadoghq.com/api/latest/observability-pipelines/#generate-a-new-external-id)
    * [List namespace rules](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-namespace-rules)
    * [Get AWS integration IAM permissions](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-aws-integration-iam-permissions)
    * [List all AWS integrations](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-all-aws-integrations)
    * [Delete an AWS integration](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-aws-integration)
    * [Get AWS integration standard IAM permissions](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-aws-integration-standard-iam-permissions)
    * [Create an AWS integration](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-an-aws-integration)
    * [Get resource collection IAM permissions](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-resource-collection-iam-permissions)
    * [Update an AWS integration](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-an-aws-integration)
    * [Get all Amazon EventBridge sources](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-amazon-eventbridge-sources)
    * [Create an Amazon EventBridge source](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-an-amazon-eventbridge-source)
    * [Delete an Amazon EventBridge source](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-amazon-eventbridge-source)
  * [AWS Logs Integration](https://docs.datadoghq.com/api/latest/aws-logs-integration/)
    * [List all AWS Logs integrations](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-all-aws-logs-integrations)
    * [Add AWS Log Lambda ARN](https://docs.datadoghq.com/api/latest/observability-pipelines/#add-aws-log-lambda-arn)
    * [Delete an AWS Logs integration](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-aws-logs-integration)
    * [Get list of AWS log ready services](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-list-of-aws-log-ready-services)
    * [Enable an AWS Logs integration](https://docs.datadoghq.com/api/latest/observability-pipelines/#enable-an-aws-logs-integration)
    * [Check permissions for log services](https://docs.datadoghq.com/api/latest/observability-pipelines/#check-permissions-for-log-services)
    * [Check that an AWS Lambda Function exists](https://docs.datadoghq.com/api/latest/observability-pipelines/#check-that-an-aws-lambda-function-exists)
  * [Azure Integration](https://docs.datadoghq.com/api/latest/azure-integration/)
    * [List all Azure integrations](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-all-azure-integrations)
    * [Create an Azure integration](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-an-azure-integration)
    * [Delete an Azure integration](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-azure-integration)
    * [Update an Azure integration](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-an-azure-integration)
    * [Update Azure integration host filters](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-azure-integration-host-filters)
  * [Case Management](https://docs.datadoghq.com/api/latest/case-management/)
    * [Create a project](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-project)
    * [Search cases](https://docs.datadoghq.com/api/latest/observability-pipelines/#search-cases)
    * [Create a case](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-case)
    * [Get all projects](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-projects)
    * [Get the details of a case](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-the-details-of-a-case)
    * [Get the details of a project](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-the-details-of-a-project)
    * [Remove a project](https://docs.datadoghq.com/api/latest/observability-pipelines/#remove-a-project)
    * [Update case description](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-case-description)
    * [Update case status](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-case-status)
    * [Update case title](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-case-title)
    * [Update case priority](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-case-priority)
    * [Assign case](https://docs.datadoghq.com/api/latest/observability-pipelines/#assign-case)
    * [Unassign case](https://docs.datadoghq.com/api/latest/observability-pipelines/#unassign-case)
    * [Archive case](https://docs.datadoghq.com/api/latest/observability-pipelines/#archive-case)
    * [Unarchive case](https://docs.datadoghq.com/api/latest/observability-pipelines/#unarchive-case)
    * [Update case attributes](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-case-attributes)
    * [Comment case](https://docs.datadoghq.com/api/latest/observability-pipelines/#comment-case)
    * [Delete case comment](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-case-comment)
    * [Update case custom attribute](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-case-custom-attribute)
    * [Delete custom attribute from case](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-custom-attribute-from-case)
  * [Case Management Attribute](https://docs.datadoghq.com/api/latest/case-management-attribute/)
    * [Get all custom attributes](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-custom-attributes)
    * [Get all custom attributes config of case type](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-custom-attributes-config-of-case-type)
    * [Create custom attribute config for a case type](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-custom-attribute-config-for-a-case-type)
    * [Delete custom attributes config](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-custom-attributes-config)
  * [Case Management Type](https://docs.datadoghq.com/api/latest/case-management-type/)
    * [Get all case types](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-case-types)
    * [Create a case type](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-case-type)
    * [Delete a case type](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-case-type)
  * [CI Visibility Pipelines](https://docs.datadoghq.com/api/latest/ci-visibility-pipelines/)
    * [Send pipeline event](https://docs.datadoghq.com/api/latest/observability-pipelines/#send-pipeline-event)
    * [Get a list of pipelines events](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-list-of-pipelines-events)
    * [Search pipelines events](https://docs.datadoghq.com/api/latest/observability-pipelines/#search-pipelines-events)
    * [Aggregate pipelines events](https://docs.datadoghq.com/api/latest/observability-pipelines/#aggregate-pipelines-events)
  * [CI Visibility Tests](https://docs.datadoghq.com/api/latest/ci-visibility-tests/)
    * [Get a list of tests events](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-list-of-tests-events)
    * [Search tests events](https://docs.datadoghq.com/api/latest/observability-pipelines/#search-tests-events)
    * [Aggregate tests events](https://docs.datadoghq.com/api/latest/observability-pipelines/#aggregate-tests-events)
  * [Cloud Cost Management](https://docs.datadoghq.com/api/latest/cloud-cost-management/)
    * [List Cloud Cost Management AWS CUR configs](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-cloud-cost-management-aws-cur-configs)
    * [Create Cloud Cost Management AWS CUR config](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-cloud-cost-management-aws-cur-config)
    * [Update Cloud Cost Management AWS CUR config](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-cloud-cost-management-aws-cur-config)
    * [Delete Cloud Cost Management AWS CUR config](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-cloud-cost-management-aws-cur-config)
    * [Get cost AWS CUR config](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-cost-aws-cur-config)
    * [List Cloud Cost Management Azure configs](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-cloud-cost-management-azure-configs)
    * [Create Cloud Cost Management Azure configs](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-cloud-cost-management-azure-configs)
    * [Update Cloud Cost Management Azure config](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-cloud-cost-management-azure-config)
    * [Delete Cloud Cost Management Azure config](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-cloud-cost-management-azure-config)
    * [Get cost Azure UC config](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-cost-azure-uc-config)
    * [List Google Cloud Usage Cost configs](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-google-cloud-usage-cost-configs)
    * [Create Google Cloud Usage Cost config](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-google-cloud-usage-cost-config)
    * [Update Google Cloud Usage Cost config](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-google-cloud-usage-cost-config)
    * [Delete Google Cloud Usage Cost config](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-google-cloud-usage-cost-config)
    * [Get Google Cloud Usage Cost config](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-google-cloud-usage-cost-config)
    * [List tag pipeline rulesets](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-tag-pipeline-rulesets)
    * [Create tag pipeline ruleset](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-tag-pipeline-ruleset)
    * [Update tag pipeline ruleset](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-tag-pipeline-ruleset)
    * [Delete tag pipeline ruleset](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-tag-pipeline-ruleset)
    * [Get a tag pipeline ruleset](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-tag-pipeline-ruleset)
    * [Reorder tag pipeline rulesets](https://docs.datadoghq.com/api/latest/observability-pipelines/#reorder-tag-pipeline-rulesets)
    * [Validate query](https://docs.datadoghq.com/api/latest/observability-pipelines/#validate-query)
    * [List custom allocation rules](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-custom-allocation-rules)
    * [Create custom allocation rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-custom-allocation-rule)
    * [Update custom allocation rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-custom-allocation-rule)
    * [Delete custom allocation rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-custom-allocation-rule)
    * [Get custom allocation rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-custom-allocation-rule)
    * [Reorder custom allocation rules](https://docs.datadoghq.com/api/latest/observability-pipelines/#reorder-custom-allocation-rules)
    * [List Custom Costs files](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-custom-costs-files)
    * [Upload Custom Costs file](https://docs.datadoghq.com/api/latest/observability-pipelines/#upload-custom-costs-file)
    * [Delete Custom Costs file](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-custom-costs-file)
    * [Get Custom Costs file](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-custom-costs-file)
    * [List budgets](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-budgets)
    * [Create or update a budget](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-or-update-a-budget)
    * [Delete a budget](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-budget)
    * [Get a budget](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-budget)
  * [Cloud Network Monitoring](https://docs.datadoghq.com/api/latest/cloud-network-monitoring/)
    * [Get all aggregated connections](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-aggregated-connections)
    * [Get all aggregated DNS traffic](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-aggregated-dns-traffic)
  * [Cloudflare Integration](https://docs.datadoghq.com/api/latest/cloudflare-integration/)
    * [List Cloudflare accounts](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-cloudflare-accounts)
    * [Add Cloudflare account](https://docs.datadoghq.com/api/latest/observability-pipelines/#add-cloudflare-account)
    * [Get Cloudflare account](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-cloudflare-account)
    * [Update Cloudflare account](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-cloudflare-account)
    * [Delete Cloudflare account](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-cloudflare-account)
  * [Confluent Cloud](https://docs.datadoghq.com/api/latest/confluent-cloud/)
    * [Update resource in Confluent account](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-resource-in-confluent-account)
    * [Get resource from Confluent account](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-resource-from-confluent-account)
    * [Delete resource from Confluent account](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-resource-from-confluent-account)
    * [Add resource to Confluent account](https://docs.datadoghq.com/api/latest/observability-pipelines/#add-resource-to-confluent-account)
    * [List Confluent Account resources](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-confluent-account-resources)
    * [Update Confluent account](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-confluent-account)
    * [Get Confluent account](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-confluent-account)
    * [Delete Confluent account](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-confluent-account)
    * [Add Confluent account](https://docs.datadoghq.com/api/latest/observability-pipelines/#add-confluent-account)
    * [List Confluent accounts](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-confluent-accounts)
  * [Container Images](https://docs.datadoghq.com/api/latest/container-images/)
    * [Get all Container Images](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-container-images)
  * [Containers](https://docs.datadoghq.com/api/latest/containers/)
    * [Get All Containers](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-containers)
  * [CSM Agents](https://docs.datadoghq.com/api/latest/csm-agents/)
    * [Get all CSM Agents](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-csm-agents)
    * [Get all CSM Serverless Agents](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-csm-serverless-agents)
  * [CSM Coverage Analysis](https://docs.datadoghq.com/api/latest/csm-coverage-analysis/)
    * [Get the CSM Cloud Accounts Coverage Analysis](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-the-csm-cloud-accounts-coverage-analysis)
    * [Get the CSM Hosts and Containers Coverage Analysis](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-the-csm-hosts-and-containers-coverage-analysis)
    * [Get the CSM Serverless Coverage Analysis](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-the-csm-serverless-coverage-analysis)
  * [CSM Threats](https://docs.datadoghq.com/api/latest/csm-threats/)
    * [Get all Workload Protection agent rules](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-workload-protection-agent-rules)
    * [Get a Workload Protection agent rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-workload-protection-agent-rule)
    * [Create a Workload Protection agent rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-workload-protection-agent-rule)
    * [Update a Workload Protection agent rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-workload-protection-agent-rule)
    * [Delete a Workload Protection agent rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-workload-protection-agent-rule)
    * [Get all Workload Protection policies](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-workload-protection-policies)
    * [Get a Workload Protection policy](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-workload-protection-policy)
    * [Create a Workload Protection policy](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-workload-protection-policy)
    * [Update a Workload Protection policy](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-workload-protection-policy)
    * [Delete a Workload Protection policy](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-workload-protection-policy)
    * [Download the Workload Protection policy](https://docs.datadoghq.com/api/latest/observability-pipelines/#download-the-workload-protection-policy)
    * [Get all Workload Protection agent rules (US1-FED)](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-workload-protection-agent-rules-us1-fed)
    * [Get a Workload Protection agent rule (US1-FED)](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-workload-protection-agent-rule-us1-fed)
    * [Create a Workload Protection agent rule (US1-FED)](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-workload-protection-agent-rule-us1-fed)
    * [Update a Workload Protection agent rule (US1-FED)](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-workload-protection-agent-rule-us1-fed)
    * [Delete a Workload Protection agent rule (US1-FED)](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-workload-protection-agent-rule-us1-fed)
    * [Download the Workload Protection policy (US1-FED)](https://docs.datadoghq.com/api/latest/observability-pipelines/#download-the-workload-protection-policy-us1-fed)
  * [Dashboard Lists](https://docs.datadoghq.com/api/latest/dashboard-lists/)
    * [Get all dashboard lists](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-dashboard-lists)
    * [Get items of a Dashboard List](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-items-of-a-dashboard-list)
    * [Add Items to a Dashboard List](https://docs.datadoghq.com/api/latest/observability-pipelines/#add-items-to-a-dashboard-list)
    * [Create a dashboard list](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-dashboard-list)
    * [Get a dashboard list](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-dashboard-list)
    * [Update items of a dashboard list](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-items-of-a-dashboard-list)
    * [Delete items from a dashboard list](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-items-from-a-dashboard-list)
    * [Update a dashboard list](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-dashboard-list)
    * [Delete a dashboard list](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-dashboard-list)
  * [Dashboards](https://docs.datadoghq.com/api/latest/dashboards/)
    * [Create a new dashboard](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-new-dashboard)
    * [Get a dashboard](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-dashboard)
    * [Get all dashboards](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-dashboards)
    * [Update a dashboard](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-dashboard)
    * [Delete a dashboard](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-dashboard)
    * [Delete dashboards](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-dashboards)
    * [Restore deleted dashboards](https://docs.datadoghq.com/api/latest/observability-pipelines/#restore-deleted-dashboards)
    * [Create a shared dashboard](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-shared-dashboard)
    * [Get a shared dashboard](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-shared-dashboard)
    * [Update a shared dashboard](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-shared-dashboard)
    * [Send shared dashboard invitation email](https://docs.datadoghq.com/api/latest/observability-pipelines/#send-shared-dashboard-invitation-email)
    * [Get all invitations for a shared dashboard](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-invitations-for-a-shared-dashboard)
    * [Revoke a shared dashboard URL](https://docs.datadoghq.com/api/latest/observability-pipelines/#revoke-a-shared-dashboard-url)
    * [Revoke shared dashboard invitations](https://docs.datadoghq.com/api/latest/observability-pipelines/#revoke-shared-dashboard-invitations)
  * [Datasets](https://docs.datadoghq.com/api/latest/datasets/)
    * [Create a dataset](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-dataset)
    * [Get a single dataset by ID](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-single-dataset-by-id)
    * [Get all datasets](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-datasets)
    * [Edit a dataset](https://docs.datadoghq.com/api/latest/observability-pipelines/#edit-a-dataset)
    * [Delete a dataset](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-dataset)
  * [Deployment Gates](https://docs.datadoghq.com/api/latest/deployment-gates/)
    * [Create deployment gate](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-deployment-gate)
    * [Get deployment gate](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-deployment-gate)
    * [Update deployment gate](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-deployment-gate)
    * [Delete deployment gate](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-deployment-gate)
    * [Create deployment rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-deployment-rule)
    * [Get deployment rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-deployment-rule)
    * [Update deployment rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-deployment-rule)
    * [Delete deployment rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-deployment-rule)
    * [Get rules for a deployment gate](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-rules-for-a-deployment-gate)
  * [Domain Allowlist](https://docs.datadoghq.com/api/latest/domain-allowlist/)
    * [Get Domain Allowlist](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-domain-allowlist)
    * [Sets Domain Allowlist](https://docs.datadoghq.com/api/latest/observability-pipelines/#sets-domain-allowlist)
  * [DORA Metrics](https://docs.datadoghq.com/api/latest/dora-metrics/)
    * [Send a deployment event](https://docs.datadoghq.com/api/latest/observability-pipelines/#send-a-deployment-event)
    * [Send a failure event](https://docs.datadoghq.com/api/latest/observability-pipelines/#send-a-failure-event)
    * [Send an incident event](https://docs.datadoghq.com/api/latest/observability-pipelines/#send-an-incident-event)
    * [Get a list of deployment events](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-list-of-deployment-events)
    * [Get a list of failure events](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-list-of-failure-events)
    * [Get a deployment event](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-deployment-event)
    * [Get a failure event](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-failure-event)
    * [Delete a failure event](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-failure-event)
    * [Delete a deployment event](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-deployment-event)
  * [Downtimes](https://docs.datadoghq.com/api/latest/downtimes/)
    * [Get all downtimes](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-downtimes)
    * [Schedule a downtime](https://docs.datadoghq.com/api/latest/observability-pipelines/#schedule-a-downtime)
    * [Cancel downtimes by scope](https://docs.datadoghq.com/api/latest/observability-pipelines/#cancel-downtimes-by-scope)
    * [Cancel a downtime](https://docs.datadoghq.com/api/latest/observability-pipelines/#cancel-a-downtime)
    * [Get a downtime](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-downtime)
    * [Update a downtime](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-downtime)
    * [Get active downtimes for a monitor](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-active-downtimes-for-a-monitor)
  * [Embeddable Graphs](https://docs.datadoghq.com/api/latest/embeddable-graphs/)
    * [Revoke embed](https://docs.datadoghq.com/api/latest/observability-pipelines/#revoke-embed)
    * [Enable embed](https://docs.datadoghq.com/api/latest/observability-pipelines/#enable-embed)
    * [Get specific embed](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-specific-embed)
    * [Create embed](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-embed)
    * [Get all embeds](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-embeds)
  * [Error Tracking](https://docs.datadoghq.com/api/latest/error-tracking/)
    * [Search error tracking issues](https://docs.datadoghq.com/api/latest/observability-pipelines/#search-error-tracking-issues)
    * [Get the details of an error tracking issue](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-the-details-of-an-error-tracking-issue)
    * [Update the state of an issue](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-the-state-of-an-issue)
    * [Update the assignee of an issue](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-the-assignee-of-an-issue)
    * [Remove the assignee of an issue](https://docs.datadoghq.com/api/latest/observability-pipelines/#remove-the-assignee-of-an-issue)
  * [Events](https://docs.datadoghq.com/api/latest/events/)
    * [Get a list of events](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-list-of-events)
    * [Post an event](https://docs.datadoghq.com/api/latest/observability-pipelines/#post-an-event)
    * [Get an event](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-an-event)
    * [Search events](https://docs.datadoghq.com/api/latest/observability-pipelines/#search-events)
  * [Fastly Integration](https://docs.datadoghq.com/api/latest/fastly-integration/)
    * [List Fastly accounts](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-fastly-accounts)
    * [Add Fastly account](https://docs.datadoghq.com/api/latest/observability-pipelines/#add-fastly-account)
    * [Get Fastly account](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-fastly-account)
    * [Update Fastly account](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-fastly-account)
    * [Delete Fastly account](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-fastly-account)
    * [List Fastly services](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-fastly-services)
    * [Add Fastly service](https://docs.datadoghq.com/api/latest/observability-pipelines/#add-fastly-service)
    * [Get Fastly service](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-fastly-service)
    * [Update Fastly service](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-fastly-service)
    * [Delete Fastly service](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-fastly-service)
  * [Fleet Automation](https://docs.datadoghq.com/api/latest/fleet-automation/)
    * [List all available Agent versions](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-all-available-agent-versions)
    * [List all Datadog Agents](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-all-datadog-agents)
    * [Get detailed information about an agent](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-detailed-information-about-an-agent)
    * [List all deployments](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-all-deployments)
    * [Create a configuration deployment](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-configuration-deployment)
    * [Upgrade hosts](https://docs.datadoghq.com/api/latest/observability-pipelines/#upgrade-hosts)
    * [Get a configuration deployment by ID](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-configuration-deployment-by-id)
    * [Cancel a deployment](https://docs.datadoghq.com/api/latest/observability-pipelines/#cancel-a-deployment)
    * [List all schedules](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-all-schedules)
    * [Create a schedule](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-schedule)
    * [Get a schedule by ID](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-schedule-by-id)
    * [Update a schedule](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-schedule)
    * [Delete a schedule](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-schedule)
    * [Trigger a schedule deployment](https://docs.datadoghq.com/api/latest/observability-pipelines/#trigger-a-schedule-deployment)
  * [GCP Integration](https://docs.datadoghq.com/api/latest/gcp-integration/)
    * [List all GCP integrations](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-all-gcp-integrations)
    * [List all GCP STS-enabled service accounts](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-all-gcp-sts-enabled-service-accounts)
    * [Create a GCP integration](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-gcp-integration)
    * [Create a new entry for your service account](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-new-entry-for-your-service-account)
    * [Delete a GCP integration](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-gcp-integration)
    * [Delete an STS enabled GCP Account](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-sts-enabled-gcp-account)
    * [Update a GCP integration](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-gcp-integration)
    * [Update STS Service Account](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-sts-service-account)
    * [Create a Datadog GCP principal](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-datadog-gcp-principal)
    * [List delegate account](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-delegate-account)
  * [Hosts](https://docs.datadoghq.com/api/latest/hosts/)
    * [Get all hosts for your organization](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-hosts-for-your-organization)
    * [Get the total number of active hosts](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-the-total-number-of-active-hosts)
    * [Mute a host](https://docs.datadoghq.com/api/latest/observability-pipelines/#mute-a-host)
    * [Unmute a host](https://docs.datadoghq.com/api/latest/observability-pipelines/#unmute-a-host)
  * [Incident Services](https://docs.datadoghq.com/api/latest/incident-services/)
    * [Get details of an incident service](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-details-of-an-incident-service)
    * [Delete an existing incident service](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-existing-incident-service)
    * [Update an existing incident service](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-an-existing-incident-service)
    * [Get a list of all incident services](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-list-of-all-incident-services)
    * [Create a new incident service](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-new-incident-service)
  * [Incident Teams](https://docs.datadoghq.com/api/latest/incident-teams/)
    * [Get details of an incident team](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-details-of-an-incident-team)
    * [Delete an existing incident team](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-existing-incident-team)
    * [Update an existing incident team](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-an-existing-incident-team)
    * [Get a list of all incident teams](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-list-of-all-incident-teams)
    * [Create a new incident team](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-new-incident-team)
  * [Incidents](https://docs.datadoghq.com/api/latest/incidents/)
    * [Create an incident](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-an-incident)
    * [Get the details of an incident](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-the-details-of-an-incident)
    * [Update an existing incident](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-an-existing-incident)
    * [Delete an existing incident](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-existing-incident)
    * [Get a list of incidents](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-list-of-incidents)
    * [Search for incidents](https://docs.datadoghq.com/api/latest/observability-pipelines/#search-for-incidents)
    * [List an incident's impacts](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-an-incidents-impacts)
    * [Create an incident impact](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-an-incident-impact)
    * [Delete an incident impact](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-incident-impact)
    * [Get a list of an incident's integration metadata](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-list-of-an-incidents-integration-metadata)
    * [Create an incident integration metadata](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-an-incident-integration-metadata)
    * [Get incident integration metadata details](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-incident-integration-metadata-details)
    * [Update an existing incident integration metadata](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-an-existing-incident-integration-metadata)
    * [Delete an incident integration metadata](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-incident-integration-metadata)
    * [Get a list of an incident's todos](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-list-of-an-incidents-todos)
    * [Create an incident todo](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-an-incident-todo)
    * [Get incident todo details](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-incident-todo-details)
    * [Update an incident todo](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-an-incident-todo)
    * [Delete an incident todo](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-incident-todo)
    * [Create an incident type](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-an-incident-type)
    * [Get a list of incident types](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-list-of-incident-types)
    * [Get incident type details](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-incident-type-details)
    * [Update an incident type](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-an-incident-type)
    * [Delete an incident type](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-incident-type)
    * [List incident notification templates](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-incident-notification-templates)
    * [Create incident notification template](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-incident-notification-template)
    * [Get incident notification template](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-incident-notification-template)
    * [Update incident notification template](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-incident-notification-template)
    * [Delete a notification template](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-notification-template)
    * [List incident notification rules](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-incident-notification-rules)
    * [Create an incident notification rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-an-incident-notification-rule)
    * [Get an incident notification rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-an-incident-notification-rule)
    * [Update an incident notification rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-an-incident-notification-rule)
    * [Delete an incident notification rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-incident-notification-rule)
    * [List incident attachments](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-incident-attachments)
    * [Create incident attachment](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-incident-attachment)
    * [Delete incident attachment](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-incident-attachment)
    * [Update incident attachment](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-incident-attachment)
  * [IP Allowlist](https://docs.datadoghq.com/api/latest/ip-allowlist/)
    * [Get IP Allowlist](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-ip-allowlist)
    * [Update IP Allowlist](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-ip-allowlist)
  * [IP Ranges](https://docs.datadoghq.com/api/latest/ip-ranges/)
    * [List IP Ranges](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-ip-ranges)
  * [Key Management](https://docs.datadoghq.com/api/latest/key-management/)
    * [Delete an application key owned by current user](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-application-key-owned-by-current-user)
    * [Get all API keys](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-api-keys)
    * [Create an API key](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-an-api-key)
    * [Edit an application key owned by current user](https://docs.datadoghq.com/api/latest/observability-pipelines/#edit-an-application-key-owned-by-current-user)
    * [Get API key](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-api-key)
    * [Get one application key owned by current user](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-one-application-key-owned-by-current-user)
    * [Create an application key for current user](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-an-application-key-for-current-user)
    * [Edit an API key](https://docs.datadoghq.com/api/latest/observability-pipelines/#edit-an-api-key)
    * [Delete an API key](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-api-key)
    * [Get all application keys owned by current user](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-application-keys-owned-by-current-user)
    * [Get all application keys](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-application-keys)
    * [Create an application key](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-an-application-key)
    * [Get an application key](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-an-application-key)
    * [Edit an application key](https://docs.datadoghq.com/api/latest/observability-pipelines/#edit-an-application-key)
    * [Delete an application key](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-application-key)
  * [Logs](https://docs.datadoghq.com/api/latest/logs/)
    * [Send logs](https://docs.datadoghq.com/api/latest/observability-pipelines/#send-logs)
    * [Aggregate events](https://docs.datadoghq.com/api/latest/observability-pipelines/#aggregate-events)
    * [Search logs](https://docs.datadoghq.com/api/latest/observability-pipelines/#search-logs)
    * [Search logs (POST)](https://docs.datadoghq.com/api/latest/observability-pipelines/#search-logs-post)
    * [Search logs (GET)](https://docs.datadoghq.com/api/latest/observability-pipelines/#search-logs-get)
  * [Logs Archives](https://docs.datadoghq.com/api/latest/logs-archives/)
    * [Get all archives](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-archives)
    * [Create an archive](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-an-archive)
    * [Get an archive](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-an-archive)
    * [Update an archive](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-an-archive)
    * [Delete an archive](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-archive)
    * [List read roles for an archive](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-read-roles-for-an-archive)
    * [Grant role to an archive](https://docs.datadoghq.com/api/latest/observability-pipelines/#grant-role-to-an-archive)
    * [Revoke role from an archive](https://docs.datadoghq.com/api/latest/observability-pipelines/#revoke-role-from-an-archive)
    * [Get archive order](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-archive-order)
    * [Update archive order](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-archive-order)
  * [Logs Custom Destinations](https://docs.datadoghq.com/api/latest/logs-custom-destinations/)
    * [Get all custom destinations](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-custom-destinations)
    * [Create a custom destination](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-custom-destination)
    * [Get a custom destination](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-custom-destination)
    * [Update a custom destination](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-custom-destination)
    * [Delete a custom destination](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-custom-destination)
  * [Logs Indexes](https://docs.datadoghq.com/api/latest/logs-indexes/)
    * [Get all indexes](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-indexes)
    * [Get an index](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-an-index)
    * [Create an index](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-an-index)
    * [Update an index](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-an-index)
    * [Delete an index](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-index)
    * [Get indexes order](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-indexes-order)
    * [Update indexes order](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-indexes-order)
  * [Logs Metrics](https://docs.datadoghq.com/api/latest/logs-metrics/)
    * [Get all log-based metrics](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-log-based-metrics)
    * [Create a log-based metric](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-log-based-metric)
    * [Get a log-based metric](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-log-based-metric)
    * [Update a log-based metric](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-log-based-metric)
    * [Delete a log-based metric](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-log-based-metric)
  * [Logs Pipelines](https://docs.datadoghq.com/api/latest/logs-pipelines/)
    * [Get pipeline order](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-pipeline-order)
    * [Update pipeline order](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-pipeline-order)
    * [Get all pipelines](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-pipelines)
    * [Create a pipeline](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-pipeline)
    * [Get a pipeline](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-pipeline)
    * [Delete a pipeline](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-pipeline)
    * [Update a pipeline](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-pipeline)
  * [Logs Restriction Queries](https://docs.datadoghq.com/api/latest/logs-restriction-queries/)
    * [List restriction queries](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-restriction-queries)
    * [Create a restriction query](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-restriction-query)
    * [Get a restriction query](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-restriction-query)
    * [Replace a restriction query](https://docs.datadoghq.com/api/latest/observability-pipelines/#replace-a-restriction-query)
    * [Update a restriction query](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-restriction-query)
    * [Delete a restriction query](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-restriction-query)
    * [List roles for a restriction query](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-roles-for-a-restriction-query)
    * [Grant role to a restriction query](https://docs.datadoghq.com/api/latest/observability-pipelines/#grant-role-to-a-restriction-query)
    * [Revoke role from a restriction query](https://docs.datadoghq.com/api/latest/observability-pipelines/#revoke-role-from-a-restriction-query)
    * [Get all restriction queries for a given user](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-restriction-queries-for-a-given-user)
    * [Get restriction query for a given role](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-restriction-query-for-a-given-role)
  * [Metrics](https://docs.datadoghq.com/api/latest/metrics/)
    * [Create a tag configuration](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-tag-configuration)
    * [Get active metrics list](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-active-metrics-list)
    * [Query timeseries data across multiple products](https://docs.datadoghq.com/api/latest/observability-pipelines/#query-timeseries-data-across-multiple-products)
    * [Submit distribution points](https://docs.datadoghq.com/api/latest/observability-pipelines/#submit-distribution-points)
    * [Submit metrics](https://docs.datadoghq.com/api/latest/observability-pipelines/#submit-metrics)
    * [Get metric metadata](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-metric-metadata)
    * [List tag configuration by name](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-tag-configuration-by-name)
    * [Query scalar data across multiple products](https://docs.datadoghq.com/api/latest/observability-pipelines/#query-scalar-data-across-multiple-products)
    * [Edit metric metadata](https://docs.datadoghq.com/api/latest/observability-pipelines/#edit-metric-metadata)
    * [Update a tag configuration](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-tag-configuration)
    * [Delete a tag configuration](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-tag-configuration)
    * [Search metrics](https://docs.datadoghq.com/api/latest/observability-pipelines/#search-metrics)
    * [Get a list of metrics](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-list-of-metrics)
    * [Query timeseries points](https://docs.datadoghq.com/api/latest/observability-pipelines/#query-timeseries-points)
    * [List tags by metric name](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-tags-by-metric-name)
    * [List active tags and aggregations](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-active-tags-and-aggregations)
    * [List distinct metric volumes by metric name](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-distinct-metric-volumes-by-metric-name)
    * [Configure tags for multiple metrics](https://docs.datadoghq.com/api/latest/observability-pipelines/#configure-tags-for-multiple-metrics)
    * [Delete tags for multiple metrics](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-tags-for-multiple-metrics)
    * [Tag Configuration Cardinality Estimator](https://docs.datadoghq.com/api/latest/observability-pipelines/#tag-configuration-cardinality-estimator)
    * [Related Assets to a Metric](https://docs.datadoghq.com/api/latest/observability-pipelines/#related-assets-to-a-metric)
    * [Get tag key cardinality details](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-tag-key-cardinality-details)
  * [Microsoft Teams Integration](https://docs.datadoghq.com/api/latest/microsoft-teams-integration/)
    * [Create tenant-based handle](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-tenant-based-handle)
    * [Create Workflows webhook handle](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-workflows-webhook-handle)
    * [Delete tenant-based handle](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-tenant-based-handle)
    * [Delete Workflows webhook handle](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-workflows-webhook-handle)
    * [Get all tenant-based handles](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-tenant-based-handles)
    * [Get all Workflows webhook handles](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-workflows-webhook-handles)
    * [Get channel information by name](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-channel-information-by-name)
    * [Get tenant-based handle information](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-tenant-based-handle-information)
    * [Get Workflows webhook handle information](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-workflows-webhook-handle-information)
    * [Update tenant-based handle](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-tenant-based-handle)
    * [Update Workflows webhook handle](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-workflows-webhook-handle)
  * [Monitors](https://docs.datadoghq.com/api/latest/monitors/)
    * [Create a monitor](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-monitor)
    * [Monitors search](https://docs.datadoghq.com/api/latest/observability-pipelines/#monitors-search)
    * [Unmute a monitor](https://docs.datadoghq.com/api/latest/observability-pipelines/#unmute-a-monitor)
    * [Get all monitors](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-monitors)
    * [Monitors group search](https://docs.datadoghq.com/api/latest/observability-pipelines/#monitors-group-search)
    * [Mute a monitor](https://docs.datadoghq.com/api/latest/observability-pipelines/#mute-a-monitor)
    * [Edit a monitor](https://docs.datadoghq.com/api/latest/observability-pipelines/#edit-a-monitor)
    * [Unmute all monitors](https://docs.datadoghq.com/api/latest/observability-pipelines/#unmute-all-monitors)
    * [Get a monitor's details](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-monitors-details)
    * [Mute all monitors](https://docs.datadoghq.com/api/latest/observability-pipelines/#mute-all-monitors)
    * [Delete a monitor](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-monitor)
    * [Check if a monitor can be deleted](https://docs.datadoghq.com/api/latest/observability-pipelines/#check-if-a-monitor-can-be-deleted)
    * [Validate a monitor](https://docs.datadoghq.com/api/latest/observability-pipelines/#validate-a-monitor)
    * [Validate an existing monitor](https://docs.datadoghq.com/api/latest/observability-pipelines/#validate-an-existing-monitor)
    * [Get a monitor configuration policy](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-monitor-configuration-policy)
    * [Get all monitor configuration policies](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-monitor-configuration-policies)
    * [Create a monitor configuration policy](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-monitor-configuration-policy)
    * [Edit a monitor configuration policy](https://docs.datadoghq.com/api/latest/observability-pipelines/#edit-a-monitor-configuration-policy)
    * [Delete a monitor configuration policy](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-monitor-configuration-policy)
    * [Get a monitor notification rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-monitor-notification-rule)
    * [Get all monitor notification rules](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-monitor-notification-rules)
    * [Create a monitor notification rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-monitor-notification-rule)
    * [Update a monitor notification rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-monitor-notification-rule)
    * [Delete a monitor notification rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-monitor-notification-rule)
    * [Get a monitor user template](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-monitor-user-template)
    * [Get all monitor user templates](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-monitor-user-templates)
    * [Create a monitor user template](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-monitor-user-template)
    * [Update a monitor user template to a new version](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-monitor-user-template-to-a-new-version)
    * [Delete a monitor user template](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-monitor-user-template)
    * [Validate a monitor user template](https://docs.datadoghq.com/api/latest/observability-pipelines/#validate-a-monitor-user-template)
    * [Validate an existing monitor user template](https://docs.datadoghq.com/api/latest/observability-pipelines/#validate-an-existing-monitor-user-template)
  * [Network Device Monitoring](https://docs.datadoghq.com/api/latest/network-device-monitoring/)
    * [Get the list of devices](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-the-list-of-devices)
    * [Get the device details](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-the-device-details)
    * [Get the list of interfaces of the device](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-the-list-of-interfaces-of-the-device)
    * [Get the list of tags for a device](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-the-list-of-tags-for-a-device)
    * [Update the tags for a device](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-the-tags-for-a-device)
  * [Notebooks](https://docs.datadoghq.com/api/latest/notebooks/)
    * [Create a notebook](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-notebook)
    * [Get all notebooks](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-notebooks)
    * [Delete a notebook](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-notebook)
    * [Update a notebook](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-notebook)
    * [Get a notebook](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-notebook)
  * [Observability Pipelines](https://docs.datadoghq.com/api/latest/observability-pipelines/)
    * [List pipelines](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-pipelines)
    * [Create a new pipeline](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-new-pipeline)
    * [Get a specific pipeline](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-specific-pipeline)
    * [Update a pipeline](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-pipeline)
    * [Delete a pipeline](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-pipeline)
    * [Validate an observability pipeline](https://docs.datadoghq.com/api/latest/observability-pipelines/#validate-an-observability-pipeline)
  * [Okta Integration](https://docs.datadoghq.com/api/latest/okta-integration/)
    * [List Okta accounts](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-okta-accounts)
    * [Add Okta account](https://docs.datadoghq.com/api/latest/observability-pipelines/#add-okta-account)
    * [Get Okta account](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-okta-account)
    * [Update Okta account](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-okta-account)
    * [Delete Okta account](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-okta-account)
  * [On-Call](https://docs.datadoghq.com/api/latest/on-call/)
    * [Create On-Call schedule](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-on-call-schedule)
    * [Get On-Call schedule](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-on-call-schedule)
    * [Delete On-Call schedule](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-on-call-schedule)
    * [Update On-Call schedule](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-on-call-schedule)
    * [Create On-Call escalation policy](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-on-call-escalation-policy)
    * [Update On-Call escalation policy](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-on-call-escalation-policy)
    * [Get On-Call escalation policy](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-on-call-escalation-policy)
    * [Delete On-Call escalation policy](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-on-call-escalation-policy)
    * [Get On-Call team routing rules](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-on-call-team-routing-rules)
    * [Set On-Call team routing rules](https://docs.datadoghq.com/api/latest/observability-pipelines/#set-on-call-team-routing-rules)
    * [Get scheduled on-call user](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-scheduled-on-call-user)
    * [Get team on-call users](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-team-on-call-users)
    * [Delete an On-Call notification channel for a user](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-on-call-notification-channel-for-a-user)
    * [Get an On-Call notification channel for a user](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-an-on-call-notification-channel-for-a-user)
    * [List On-Call notification channels for a user](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-on-call-notification-channels-for-a-user)
    * [Create an On-Call notification channel for a user](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-an-on-call-notification-channel-for-a-user)
    * [Create an On-Call notification rule for a user](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-an-on-call-notification-rule-for-a-user)
    * [List On-Call notification rules for a user](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-on-call-notification-rules-for-a-user)
    * [Delete an On-Call notification rule for a user](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-on-call-notification-rule-for-a-user)
    * [Get an On-Call notification rule for a user](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-an-on-call-notification-rule-for-a-user)
    * [Update an On-Call notification rule for a user](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-an-on-call-notification-rule-for-a-user)
  * [On-Call Paging](https://docs.datadoghq.com/api/latest/on-call-paging/)
    * [Create On-Call Page](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-on-call-page)
    * [Acknowledge On-Call Page](https://docs.datadoghq.com/api/latest/observability-pipelines/#acknowledge-on-call-page)
    * [Escalate On-Call Page](https://docs.datadoghq.com/api/latest/observability-pipelines/#escalate-on-call-page)
    * [Resolve On-Call Page](https://docs.datadoghq.com/api/latest/observability-pipelines/#resolve-on-call-page)
  * [Opsgenie Integration](https://docs.datadoghq.com/api/latest/opsgenie-integration/)
    * [Get all service objects](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-service-objects)
    * [Create a new service object](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-new-service-object)
    * [Get a single service object](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-single-service-object)
    * [Update a single service object](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-single-service-object)
    * [Delete a single service object](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-single-service-object)
  * [Org Connections](https://docs.datadoghq.com/api/latest/org-connections/)
    * [List Org Connections](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-org-connections)
    * [Create Org Connection](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-org-connection)
    * [Update Org Connection](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-org-connection)
    * [Delete Org Connection](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-org-connection)
  * [Organizations](https://docs.datadoghq.com/api/latest/organizations/)
    * [Create a child organization](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-child-organization)
    * [List your managed organizations](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-your-managed-organizations)
    * [Get organization information](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-organization-information)
    * [Update your organization](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-your-organization)
    * [Upload IdP metadata](https://docs.datadoghq.com/api/latest/observability-pipelines/#upload-idp-metadata)
    * [Spin-off Child Organization](https://docs.datadoghq.com/api/latest/observability-pipelines/#spin-off-child-organization)
    * [List Org Configs](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-org-configs)
    * [Get a specific Org Config value](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-specific-org-config-value)
    * [Update a specific Org Config](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-specific-org-config)
  * [PagerDuty Integration](https://docs.datadoghq.com/api/latest/pagerduty-integration/)
    * [Create a new service object](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-new-service-object)
    * [Get a single service object](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-single-service-object)
    * [Update a single service object](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-single-service-object)
    * [Delete a single service object](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-single-service-object)
  * [Powerpack](https://docs.datadoghq.com/api/latest/powerpack/)
    * [Get all powerpacks](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-powerpacks)
    * [Create a new powerpack](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-new-powerpack)
    * [Delete a powerpack](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-powerpack)
    * [Get a Powerpack](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-powerpack)
    * [Update a powerpack](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-powerpack)
  * [Processes](https://docs.datadoghq.com/api/latest/processes/)
    * [Get all processes](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-processes)
  * [Product Analytics](https://docs.datadoghq.com/api/latest/product-analytics/)
    * [Send server-side events](https://docs.datadoghq.com/api/latest/observability-pipelines/#send-server-side-events)
  * [Reference Tables](https://docs.datadoghq.com/api/latest/reference-tables/)
    * [Create reference table upload](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-reference-table-upload)
    * [Create reference table](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-reference-table)
    * [List tables](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-tables)
    * [Get table](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-table)
    * [Get rows by id](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-rows-by-id)
    * [Update reference table](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-reference-table)
    * [Delete table](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-table)
    * [Upsert rows](https://docs.datadoghq.com/api/latest/observability-pipelines/#upsert-rows)
    * [Delete rows](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-rows)
  * [Restriction Policies](https://docs.datadoghq.com/api/latest/restriction-policies/)
    * [Update a restriction policy](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-restriction-policy)
    * [Get a restriction policy](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-restriction-policy)
    * [Delete a restriction policy](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-restriction-policy)
  * [Roles](https://docs.datadoghq.com/api/latest/roles/)
    * [List permissions](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-permissions)
    * [List roles](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-roles)
    * [Create role](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-role)
    * [Get a role](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-role)
    * [Update a role](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-role)
    * [Delete role](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-role)
    * [List permissions for a role](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-permissions-for-a-role)
    * [Grant permission to a role](https://docs.datadoghq.com/api/latest/observability-pipelines/#grant-permission-to-a-role)
    * [Revoke permission](https://docs.datadoghq.com/api/latest/observability-pipelines/#revoke-permission)
    * [Get all users of a role](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-users-of-a-role)
    * [Add a user to a role](https://docs.datadoghq.com/api/latest/observability-pipelines/#add-a-user-to-a-role)
    * [Remove a user from a role](https://docs.datadoghq.com/api/latest/observability-pipelines/#remove-a-user-from-a-role)
    * [Create a new role by cloning an existing role](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-new-role-by-cloning-an-existing-role)
    * [List role templates](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-role-templates)
  * [RUM](https://docs.datadoghq.com/api/latest/rum/)
    * [Search RUM events](https://docs.datadoghq.com/api/latest/observability-pipelines/#search-rum-events)
    * [Get a list of RUM events](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-list-of-rum-events)
    * [Aggregate RUM events](https://docs.datadoghq.com/api/latest/observability-pipelines/#aggregate-rum-events)
    * [Update a RUM application](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-rum-application)
    * [Get a RUM application](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-rum-application)
    * [Delete a RUM application](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-rum-application)
    * [Create a new RUM application](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-new-rum-application)
    * [List all the RUM applications](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-all-the-rum-applications)
  * [Rum Audience Management](https://docs.datadoghq.com/api/latest/rum-audience-management/)
    * [Query accounts](https://docs.datadoghq.com/api/latest/observability-pipelines/#query-accounts)
    * [Create connection](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-connection)
    * [Update connection](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-connection)
    * [Query event filtered users](https://docs.datadoghq.com/api/latest/observability-pipelines/#query-event-filtered-users)
    * [Get account facet info](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-account-facet-info)
    * [Delete connection](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-connection)
    * [List connections](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-connections)
    * [Query users](https://docs.datadoghq.com/api/latest/observability-pipelines/#query-users)
    * [Get user facet info](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-user-facet-info)
    * [Get mapping](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-mapping)
  * [Rum Metrics](https://docs.datadoghq.com/api/latest/rum-metrics/)
    * [Get all rum-based metrics](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-rum-based-metrics)
    * [Create a rum-based metric](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-rum-based-metric)
    * [Get a rum-based metric](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-rum-based-metric)
    * [Update a rum-based metric](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-rum-based-metric)
    * [Delete a rum-based metric](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-rum-based-metric)
  * [Rum Retention Filters](https://docs.datadoghq.com/api/latest/rum-retention-filters/)
    * [Get all RUM retention filters](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-rum-retention-filters)
    * [Get a RUM retention filter](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-rum-retention-filter)
    * [Create a RUM retention filter](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-rum-retention-filter)
    * [Update a RUM retention filter](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-rum-retention-filter)
    * [Delete a RUM retention filter](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-rum-retention-filter)
    * [Order RUM retention filters](https://docs.datadoghq.com/api/latest/observability-pipelines/#order-rum-retention-filters)
  * [SCIM](https://docs.datadoghq.com/api/latest/scim/)
    * [List users](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-users)
    * [Create user](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-user)
    * [Get user](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-user)
    * [Update user](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-user)
    * [Patch user](https://docs.datadoghq.com/api/latest/observability-pipelines/#patch-user)
    * [Delete user](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-user)
    * [List groups](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-groups)
    * [Create group](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-group)
    * [Get group](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-group)
    * [Update group](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-group)
    * [Patch group](https://docs.datadoghq.com/api/latest/observability-pipelines/#patch-group)
    * [Delete group](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-group)
  * [Screenboards](https://docs.datadoghq.com/api/latest/screenboards/)
  * [Security Monitoring](https://docs.datadoghq.com/api/latest/security-monitoring/)
    * [Create a suppression rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-suppression-rule)
    * [Delete a suppression rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-suppression-rule)
    * [Get a suppression rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-suppression-rule)
    * [Get a suppression's version history](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-suppressions-version-history)
    * [Get all suppression rules](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-suppression-rules)
    * [Get suppressions affecting a specific rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-suppressions-affecting-a-specific-rule)
    * [Get suppressions affecting future rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-suppressions-affecting-future-rule)
    * [Update a suppression rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-suppression-rule)
    * [Validate a suppression rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#validate-a-suppression-rule)
    * [List findings](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-findings)
    * [List security findings](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-security-findings)
    * [Get a finding](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-finding)
    * [Mute or unmute a batch of findings](https://docs.datadoghq.com/api/latest/observability-pipelines/#mute-or-unmute-a-batch-of-findings)
    * [Search security findings](https://docs.datadoghq.com/api/latest/observability-pipelines/#search-security-findings)
    * [Add a security signal to an incident](https://docs.datadoghq.com/api/latest/observability-pipelines/#add-a-security-signal-to-an-incident)
    * [Change the triage state of a security signal](https://docs.datadoghq.com/api/latest/observability-pipelines/#change-the-triage-state-of-a-security-signal)
    * [Create a custom framework](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-custom-framework)
    * [Create a detection rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-detection-rule)
    * [Delete a custom framework](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-custom-framework)
    * [Get a custom framework](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-custom-framework)
    * [List rules](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-rules)
    * [Update a custom framework](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-custom-framework)
    * [Get a rule's details](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-rules-details)
    * [Modify the triage assignee of a security signal](https://docs.datadoghq.com/api/latest/observability-pipelines/#modify-the-triage-assignee-of-a-security-signal)
    * [Update an existing rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-an-existing-rule)
    * [Delete an existing rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-existing-rule)
    * [Test an existing rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#test-an-existing-rule)
    * [Test a rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#test-a-rule)
    * [Validate a detection rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#validate-a-detection-rule)
    * [Change the related incidents of a security signal](https://docs.datadoghq.com/api/latest/observability-pipelines/#change-the-related-incidents-of-a-security-signal)
    * [Convert an existing rule from JSON to Terraform](https://docs.datadoghq.com/api/latest/observability-pipelines/#convert-an-existing-rule-from-json-to-terraform)
    * [Convert a rule from JSON to Terraform](https://docs.datadoghq.com/api/latest/observability-pipelines/#convert-a-rule-from-json-to-terraform)
    * [Get a list of security signals](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-list-of-security-signals)
    * [Get a signal's details](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-signals-details)
    * [Delete a security filter](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-security-filter)
    * [Get a quick list of security signals](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-quick-list-of-security-signals)
    * [Get the list of signal-based notification rules](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-the-list-of-signal-based-notification-rules)
    * [Get the list of vulnerability notification rules](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-the-list-of-vulnerability-notification-rules)
    * [Update a security filter](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-security-filter)
    * [Create a new signal-based notification rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-new-signal-based-notification-rule)
    * [Create a new vulnerability-based notification rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-new-vulnerability-based-notification-rule)
    * [Get a security filter](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-security-filter)
    * [Create a security filter](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-security-filter)
    * [Get all security filters](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-security-filters)
    * [Get details of a signal-based notification rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-details-of-a-signal-based-notification-rule)
    * [Get details of a vulnerability notification rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-details-of-a-vulnerability-notification-rule)
    * [Run a threat hunting job](https://docs.datadoghq.com/api/latest/observability-pipelines/#run-a-threat-hunting-job)
    * [List threat hunting jobs](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-threat-hunting-jobs)
    * [Patch a signal-based notification rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#patch-a-signal-based-notification-rule)
    * [Patch a vulnerability-based notification rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#patch-a-vulnerability-based-notification-rule)
    * [Delete a signal-based notification rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-signal-based-notification-rule)
    * [Delete a vulnerability-based notification rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-vulnerability-based-notification-rule)
    * [Get a job's details](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-jobs-details)
    * [Cancel a threat hunting job](https://docs.datadoghq.com/api/latest/observability-pipelines/#cancel-a-threat-hunting-job)
    * [Delete an existing job](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-existing-job)
    * [Convert a job result to a signal](https://docs.datadoghq.com/api/latest/observability-pipelines/#convert-a-job-result-to-a-signal)
    * [Get a rule's version history](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-rules-version-history)
    * [List vulnerabilities](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-vulnerabilities)
    * [List resource filters](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-resource-filters)
    * [List vulnerable assets](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-vulnerable-assets)
    * [Get SBOM](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-sbom)
    * [Update resource filters](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-resource-filters)
    * [List assets SBOMs](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-assets-sboms)
    * [List scanned assets metadata](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-scanned-assets-metadata)
    * [Returns a list of Secrets rules](https://docs.datadoghq.com/api/latest/observability-pipelines/#returns-a-list-of-secrets-rules)
    * [Ruleset get multiple](https://docs.datadoghq.com/api/latest/observability-pipelines/#ruleset-get-multiple)
    * [Search hist signals](https://docs.datadoghq.com/api/latest/observability-pipelines/#search-hist-signals)
    * [Get a hist signal's details](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-hist-signals-details)
    * [List hist signals](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-hist-signals)
    * [Get a job's hist signals](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-jobs-hist-signals)
    * [Create cases for security findings](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-cases-for-security-findings)
    * [Attach security findings to a case](https://docs.datadoghq.com/api/latest/observability-pipelines/#attach-security-findings-to-a-case)
    * [Detach security findings from their case](https://docs.datadoghq.com/api/latest/observability-pipelines/#detach-security-findings-from-their-case)
    * [Create Jira issues for security findings](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-jira-issues-for-security-findings)
    * [Attach security findings to a Jira issue](https://docs.datadoghq.com/api/latest/observability-pipelines/#attach-security-findings-to-a-jira-issue)
  * [Sensitive Data Scanner](https://docs.datadoghq.com/api/latest/sensitive-data-scanner/)
    * [List Scanning Groups](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-scanning-groups)
    * [Reorder Groups](https://docs.datadoghq.com/api/latest/observability-pipelines/#reorder-groups)
    * [List standard patterns](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-standard-patterns)
    * [Create Scanning Group](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-scanning-group)
    * [Update Scanning Group](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-scanning-group)
    * [Delete Scanning Group](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-scanning-group)
    * [Create Scanning Rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-scanning-rule)
    * [Update Scanning Rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-scanning-rule)
    * [Delete Scanning Rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-scanning-rule)
  * [Service Accounts](https://docs.datadoghq.com/api/latest/service-accounts/)
    * [Create a service account](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-service-account)
    * [Get one application key for this service account](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-one-application-key-for-this-service-account)
    * [Edit an application key for this service account](https://docs.datadoghq.com/api/latest/observability-pipelines/#edit-an-application-key-for-this-service-account)
    * [Delete an application key for this service account](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-application-key-for-this-service-account)
    * [Create an application key for this service account](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-an-application-key-for-this-service-account)
    * [List application keys for this service account](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-application-keys-for-this-service-account)
  * [Service Checks](https://docs.datadoghq.com/api/latest/service-checks/)
    * [Submit a Service Check](https://docs.datadoghq.com/api/latest/observability-pipelines/#submit-a-service-check)
  * [Service Definition](https://docs.datadoghq.com/api/latest/service-definition/)
    * [Get all service definitions](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-service-definitions)
    * [Create or update service definition](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-or-update-service-definition)
    * [Get a single service definition](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-single-service-definition)
    * [Delete a single service definition](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-single-service-definition)
  * [Service Dependencies](https://docs.datadoghq.com/api/latest/service-dependencies/)
    * [Get all APM service dependencies](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-apm-service-dependencies)
    * [Get one APM service's dependencies](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-one-apm-services-dependencies)
  * [Service Level Objective Corrections](https://docs.datadoghq.com/api/latest/service-level-objective-corrections/)
    * [Create an SLO correction](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-an-slo-correction)
    * [Get all SLO corrections](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-slo-corrections)
    * [Get an SLO correction for an SLO](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-an-slo-correction-for-an-slo)
    * [Update an SLO correction](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-an-slo-correction)
    * [Delete an SLO correction](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-slo-correction)
  * [Service Level Objectives](https://docs.datadoghq.com/api/latest/service-level-objectives/)
    * [Create an SLO object](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-an-slo-object)
    * [Search for SLOs](https://docs.datadoghq.com/api/latest/observability-pipelines/#search-for-slos)
    * [Get all SLOs](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-slos)
    * [Update an SLO](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-an-slo)
    * [Get an SLO's details](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-an-slos-details)
    * [Delete an SLO](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-slo)
    * [Get an SLO's history](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-an-slos-history)
    * [Get Corrections For an SLO](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-corrections-for-an-slo)
    * [Check if SLOs can be safely deleted](https://docs.datadoghq.com/api/latest/observability-pipelines/#check-if-slos-can-be-safely-deleted)
    * [Bulk Delete SLO Timeframes](https://docs.datadoghq.com/api/latest/observability-pipelines/#bulk-delete-slo-timeframes)
    * [Create a new SLO report](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-new-slo-report)
    * [Get SLO report status](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-slo-report-status)
    * [Get SLO report](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-slo-report)
  * [Service Scorecards](https://docs.datadoghq.com/api/latest/service-scorecards/)
    * [Create a new rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-new-rule)
    * [Create outcomes batch](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-outcomes-batch)
    * [Update Scorecard outcomes asynchronously](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-scorecard-outcomes-asynchronously)
    * [List all rule outcomes](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-all-rule-outcomes)
    * [List all rules](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-all-rules)
    * [Delete a rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-rule)
    * [Update an existing rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-an-existing-rule)
  * [Slack Integration](https://docs.datadoghq.com/api/latest/slack-integration/)
    * [Delete a Slack integration](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-slack-integration)
    * [Get all channels in a Slack integration](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-channels-in-a-slack-integration)
    * [Add channels to Slack integration](https://docs.datadoghq.com/api/latest/observability-pipelines/#add-channels-to-slack-integration)
    * [Create a Slack integration channel](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-slack-integration-channel)
    * [Create a Slack integration](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-slack-integration)
    * [Get a Slack integration channel](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-slack-integration-channel)
    * [Get info about a Slack integration](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-info-about-a-slack-integration)
    * [Update a Slack integration channel](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-slack-integration-channel)
    * [Remove a Slack integration channel](https://docs.datadoghq.com/api/latest/observability-pipelines/#remove-a-slack-integration-channel)
  * [Snapshots](https://docs.datadoghq.com/api/latest/snapshots/)
    * [Take graph snapshots](https://docs.datadoghq.com/api/latest/observability-pipelines/#take-graph-snapshots)
  * [Software Catalog](https://docs.datadoghq.com/api/latest/software-catalog/)
    * [Get a list of entities](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-list-of-entities)
    * [Create or update entities](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-or-update-entities)
    * [Delete a single entity](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-single-entity)
    * [Get a list of entity relations](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-list-of-entity-relations)
    * [Preview catalog entities](https://docs.datadoghq.com/api/latest/observability-pipelines/#preview-catalog-entities)
    * [Get a list of entity kinds](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-list-of-entity-kinds)
    * [Create or update kinds](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-or-update-kinds)
    * [Delete a single kind](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-single-kind)
  * [Spa](https://docs.datadoghq.com/api/latest/spa/)
    * [Get SPA Recommendations with a shard parameter](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-spa-recommendations-with-a-shard-parameter)
    * [Get SPA Recommendations](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-spa-recommendations)
  * [Spans](https://docs.datadoghq.com/api/latest/spans/)
    * [Get a list of spans](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-list-of-spans)
    * [Search spans](https://docs.datadoghq.com/api/latest/observability-pipelines/#search-spans)
    * [Aggregate spans](https://docs.datadoghq.com/api/latest/observability-pipelines/#aggregate-spans)
  * [Spans Metrics](https://docs.datadoghq.com/api/latest/spans-metrics/)
    * [Get all span-based metrics](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-span-based-metrics)
    * [Create a span-based metric](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-span-based-metric)
    * [Get a span-based metric](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-span-based-metric)
    * [Update a span-based metric](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-span-based-metric)
    * [Delete a span-based metric](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-span-based-metric)
  * [Static Analysis](https://docs.datadoghq.com/api/latest/static-analysis/)
    * [POST request to resolve vulnerable symbols](https://docs.datadoghq.com/api/latest/observability-pipelines/#post-request-to-resolve-vulnerable-symbols)
    * [Post dependencies for analysis](https://docs.datadoghq.com/api/latest/observability-pipelines/#post-dependencies-for-analysis)
  * [Synthetics](https://docs.datadoghq.com/api/latest/synthetics/)
    * [Create an API test](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-an-api-test)
    * [Create a browser test](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-browser-test)
    * [Create a mobile test](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-mobile-test)
    * [Edit a Mobile test](https://docs.datadoghq.com/api/latest/observability-pipelines/#edit-a-mobile-test)
    * [Edit an API test](https://docs.datadoghq.com/api/latest/observability-pipelines/#edit-an-api-test)
    * [Edit a browser test](https://docs.datadoghq.com/api/latest/observability-pipelines/#edit-a-browser-test)
    * [Patch a Synthetic test](https://docs.datadoghq.com/api/latest/observability-pipelines/#patch-a-synthetic-test)
    * [Pause or start a test](https://docs.datadoghq.com/api/latest/observability-pipelines/#pause-or-start-a-test)
    * [Trigger tests from CI/CD pipelines](https://docs.datadoghq.com/api/latest/observability-pipelines/#trigger-tests-from-cicd-pipelines)
    * [Trigger Synthetic tests](https://docs.datadoghq.com/api/latest/observability-pipelines/#trigger-synthetic-tests)
    * [Get a Mobile test](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-mobile-test)
    * [Get an API test](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-an-api-test)
    * [Get a browser test](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-browser-test)
    * [Get the on-demand concurrency cap](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-the-on-demand-concurrency-cap)
    * [Get the list of all Synthetic tests](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-the-list-of-all-synthetic-tests)
    * [Save new value for on-demand concurrency cap](https://docs.datadoghq.com/api/latest/observability-pipelines/#save-new-value-for-on-demand-concurrency-cap)
    * [Get an API test result](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-an-api-test-result)
    * [Get a browser test result](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-browser-test-result)
    * [Get an API test's latest results summaries](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-an-api-tests-latest-results-summaries)
    * [Get a browser test's latest results summaries](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-browser-tests-latest-results-summaries)
    * [Get details of batch](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-details-of-batch)
    * [Delete tests](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-tests)
    * [Get all global variables](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-global-variables)
    * [Create a global variable](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-global-variable)
    * [Get a global variable](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-global-variable)
    * [Patch a global variable](https://docs.datadoghq.com/api/latest/observability-pipelines/#patch-a-global-variable)
    * [Edit a global variable](https://docs.datadoghq.com/api/latest/observability-pipelines/#edit-a-global-variable)
    * [Delete a global variable](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-global-variable)
    * [Create a private location](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-private-location)
    * [Get a private location](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-private-location)
    * [Edit a private location](https://docs.datadoghq.com/api/latest/observability-pipelines/#edit-a-private-location)
    * [Get all locations (public and private)](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-locations-public-and-private)
    * [Delete a private location](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-private-location)
    * [Get a test configuration](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-test-configuration)
    * [Get the default locations](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-the-default-locations)
    * [Edit a test](https://docs.datadoghq.com/api/latest/observability-pipelines/#edit-a-test)
    * [Fetch uptime for multiple tests](https://docs.datadoghq.com/api/latest/observability-pipelines/#fetch-uptime-for-multiple-tests)
    * [Create a test](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-test)
    * [Search Synthetic tests](https://docs.datadoghq.com/api/latest/observability-pipelines/#search-synthetic-tests)
  * [Tags](https://docs.datadoghq.com/api/latest/tags/)
    * [Get Tags](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-tags)
    * [Get host tags](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-host-tags)
    * [Add tags to a host](https://docs.datadoghq.com/api/latest/observability-pipelines/#add-tags-to-a-host)
    * [Update host tags](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-host-tags)
    * [Remove host tags](https://docs.datadoghq.com/api/latest/observability-pipelines/#remove-host-tags)
  * [Teams](https://docs.datadoghq.com/api/latest/teams/)
    * [Get all teams](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-teams)
    * [Create a team](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-team)
    * [Get a team](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-team)
    * [Update a team](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-team)
    * [Remove a team](https://docs.datadoghq.com/api/latest/observability-pipelines/#remove-a-team)
    * [Get team memberships](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-team-memberships)
    * [Add a user to a team](https://docs.datadoghq.com/api/latest/observability-pipelines/#add-a-user-to-a-team)
    * [Remove a user from a team](https://docs.datadoghq.com/api/latest/observability-pipelines/#remove-a-user-from-a-team)
    * [Update a user's membership attributes on a team](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-users-membership-attributes-on-a-team)
    * [Get links for a team](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-links-for-a-team)
    * [Create a team link](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-team-link)
    * [Get a team link](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-team-link)
    * [Update a team link](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-team-link)
    * [Remove a team link](https://docs.datadoghq.com/api/latest/observability-pipelines/#remove-a-team-link)
    * [Get permission settings for a team](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-permission-settings-for-a-team)
    * [Get team sync configurations](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-team-sync-configurations)
    * [Update permission setting for team](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-permission-setting-for-team)
    * [Get user memberships](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-user-memberships)
    * [Link Teams with GitHub Teams](https://docs.datadoghq.com/api/latest/observability-pipelines/#link-teams-with-github-teams)
    * [Add a member team](https://docs.datadoghq.com/api/latest/observability-pipelines/#add-a-member-team)
    * [Get team hierarchy links](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-team-hierarchy-links)
    * [Get a team hierarchy link](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-team-hierarchy-link)
    * [Get all member teams](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-member-teams)
    * [Create a team hierarchy link](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-team-hierarchy-link)
    * [Remove a member team](https://docs.datadoghq.com/api/latest/observability-pipelines/#remove-a-member-team)
    * [Remove a team hierarchy link](https://docs.datadoghq.com/api/latest/observability-pipelines/#remove-a-team-hierarchy-link)
    * [List team connections](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-team-connections)
    * [Create team connections](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-team-connections)
    * [Delete team connections](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-team-connections)
    * [Get team notification rules](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-team-notification-rules)
    * [Create team notification rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-team-notification-rule)
    * [Get team notification rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-team-notification-rule)
    * [Update team notification rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-team-notification-rule)
    * [Delete team notification rule](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-team-notification-rule)
  * [Test Optimization](https://docs.datadoghq.com/api/latest/test-optimization/)
    * [Search flaky tests](https://docs.datadoghq.com/api/latest/observability-pipelines/#search-flaky-tests)
  * [Timeboards](https://docs.datadoghq.com/api/latest/timeboards/)
  * [Usage Metering](https://docs.datadoghq.com/api/latest/usage-metering/)
    * [Get billing dimension mapping for usage endpoints](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-billing-dimension-mapping-for-usage-endpoints)
    * [Get hourly usage by product family](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-by-product-family)
    * [Get hourly usage attribution](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-attribution)
    * [Get monthly usage attribution](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-monthly-usage-attribution)
    * [Get active billing dimensions for cost attribution](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-active-billing-dimensions-for-cost-attribution)
    * [Get billable usage across your account](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-billable-usage-across-your-account)
    * [Get historical cost across your account](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-historical-cost-across-your-account)
    * [Get Monthly Cost Attribution](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-monthly-cost-attribution)
    * [Get estimated cost across your account](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-estimated-cost-across-your-account)
    * [Get all custom metrics by hourly average](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-all-custom-metrics-by-hourly-average)
    * [Get projected cost across your account](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-projected-cost-across-your-account)
    * [Get usage across your account](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-usage-across-your-account)
    * [Get hourly usage for logs by index](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-logs-by-index)
    * [Get hourly logs usage by retention](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-logs-usage-by-retention)
    * [Get hourly usage for hosts and containers](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-hosts-and-containers)
    * [Get hourly usage for logs](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-logs)
    * [Get hourly usage for custom metrics](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-custom-metrics)
    * [Get hourly usage for indexed spans](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-indexed-spans)
    * [Get hourly usage for synthetics checks](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-synthetics-checks)
    * [Get hourly usage for synthetics API checks](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-synthetics-api-checks)
    * [Get hourly usage for synthetics browser checks](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-synthetics-browser-checks)
    * [Get hourly usage for Fargate](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-fargate)
    * [Get hourly usage for Lambda](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-lambda)
    * [Get hourly usage for RUM sessions](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-rum-sessions)
    * [Get hourly usage for network hosts](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-network-hosts)
    * [get hourly usage for network flows](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-network-flows)
    * [Get hourly usage for analyzed logs](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-analyzed-logs)
    * [Get hourly usage for SNMP devices](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-snmp-devices)
    * [Get hourly usage for ingested spans](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-ingested-spans)
    * [Get hourly usage for incident management](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-incident-management)
    * [Get hourly usage for IoT](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-iot)
    * [Get hourly usage for CSM Pro](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-csm-pro)
    * [Get hourly usage for cloud workload security](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-cloud-workload-security)
    * [Get hourly usage for database monitoring](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-database-monitoring)
    * [Get hourly usage for sensitive data scanner](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-sensitive-data-scanner)
    * [Get hourly usage for RUM units](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-rum-units)
    * [Get hourly usage for profiled hosts](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-profiled-hosts)
    * [Get hourly usage for CI visibility](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-ci-visibility)
    * [Get hourly usage for online archive](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-online-archive)
    * [Get hourly usage for Lambda traced invocations](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-lambda-traced-invocations)
    * [Get hourly usage for application security](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-application-security)
    * [Get hourly usage for observability pipelines](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-observability-pipelines)
    * [Get hourly usage for audit logs](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-hourly-usage-for-audit-logs)
    * [Get the list of available daily custom reports](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-the-list-of-available-daily-custom-reports)
    * [Get specified daily custom reports](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-specified-daily-custom-reports)
    * [Get the list of available monthly custom reports](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-the-list-of-available-monthly-custom-reports)
    * [Get specified monthly custom reports](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-specified-monthly-custom-reports)
    * [Get cost across multi-org account](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-cost-across-multi-org-account)
  * [Users](https://docs.datadoghq.com/api/latest/users/)
    * [Create a user](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-user)
    * [List all users](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-all-users)
    * [Get user details](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-user-details)
    * [Update a user](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-user)
    * [Disable a user](https://docs.datadoghq.com/api/latest/observability-pipelines/#disable-a-user)
    * [Get a user organization](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-user-organization)
    * [Get a user permissions](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-user-permissions)
    * [Send invitation emails](https://docs.datadoghq.com/api/latest/observability-pipelines/#send-invitation-emails)
    * [Get a user invitation](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-user-invitation)
  * [Webhooks Integration](https://docs.datadoghq.com/api/latest/webhooks-integration/)
    * [Create a webhooks integration](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-webhooks-integration)
    * [Get a webhook integration](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-webhook-integration)
    * [Update a webhook](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-webhook)
    * [Delete a webhook](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-webhook)
    * [Create a custom variable](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-custom-variable)
    * [Get a custom variable](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-custom-variable)
    * [Update a custom variable](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-custom-variable)
    * [Delete a custom variable](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-custom-variable)
  * [Workflow Automation](https://docs.datadoghq.com/api/latest/workflow-automation/)
    * [Get an existing Workflow](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-an-existing-workflow)
    * [Create a Workflow](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-workflow)
    * [Update an existing Workflow](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-an-existing-workflow)
    * [Delete an existing Workflow](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-an-existing-workflow)
    * [List workflow instances](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-workflow-instances)
    * [Execute a workflow](https://docs.datadoghq.com/api/latest/observability-pipelines/#execute-a-workflow)
    * [Get a workflow instance](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-workflow-instance)
    * [Cancel a workflow instance](https://docs.datadoghq.com/api/latest/observability-pipelines/#cancel-a-workflow-instance)


[Docs](https://docs.datadoghq.com/) > [API Reference](https://docs.datadoghq.com/api/latest/) > [Observability Pipelines](https://docs.datadoghq.com/api/latest/observability-pipelines/)
Language
English
[English](https://docs.datadoghq.com/api/latest/observability-pipelines/?lang_pref=en) [Français](https://docs.datadoghq.com/fr/api/latest/observability-pipelines/?lang_pref=fr) [日本語](https://docs.datadoghq.com/ja/api/latest/observability-pipelines/?lang_pref=ja) [한국어](https://docs.datadoghq.com/ko/api/latest/observability-pipelines/?lang_pref=ko) [Español](https://docs.datadoghq.com/es/api/latest/observability-pipelines/?lang_pref=es)
Datadog Site
[![Site help](https://datadog-docs.imgix.net/images/icons/help-druids.svg)](https://docs.datadoghq.com/getting_started/site/)
US1
US1 US3 US5 EU AP1 AP2 US1-FED
# Observability Pipelines
Observability Pipelines allows you to collect and process logs within your own infrastructure, and then route them to downstream integrations.
## [List pipelines](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-pipelines)
  * [v2 (latest)](https://docs.datadoghq.com/api/latest/observability-pipelines/#list-pipelines-v2)


**Note** : This endpoint is in Preview. Fill out this [form](https://www.datadoghq.com/product-preview/observability-pipelines-api-and-terraform-support/) to request access.
GET https://api.ap1.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelineshttps://api.ap2.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelineshttps://api.datadoghq.eu/api/v2/remote_config/products/obs_pipelines/pipelineshttps://api.ddog-gov.com/api/v2/remote_config/products/obs_pipelines/pipelineshttps://api.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelineshttps://api.us3.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelineshttps://api.us5.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelines
### Overview
Retrieve a list of pipelines. This endpoint requires the `observability_pipelines_read` permission.
### Arguments
#### Query Strings
Name
Type
Description
page[size]
integer
Size for a given page. The maximum allowed value is 100.
page[number]
integer
Specific page number to return.
### Response
  * [200](https://docs.datadoghq.com/api/latest/observability-pipelines/#ListPipelines-200-v2)
  * [400](https://docs.datadoghq.com/api/latest/observability-pipelines/#ListPipelines-400-v2)
  * [403](https://docs.datadoghq.com/api/latest/observability-pipelines/#ListPipelines-403-v2)
  * [429](https://docs.datadoghq.com/api/latest/observability-pipelines/#ListPipelines-429-v2)


OK
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


Represents the response payload containing a list of pipelines and associated metadata.
Expand All
Field
Type
Description
_required_]
[object]
The `schema` `data`.
_required_]
object
Defines the pipeline’s name and its components (sources, processors, and destinations).
_required_]
object
Specifies the pipeline's configuration, including its sources, processors, and destinations.
_required_]
[ <oneOf>]
A list of destination components where processed logs are sent.
object
The `datadog_logs` destination forwards logs to Datadog Log Management.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `datadog_logs`. Allowed enum values: `datadog_logs`
default: `datadog_logs`
object
The `amazon_s3` destination sends your logs in Datadog-rehydratable format to an Amazon S3 bucket for archiving.
object
AWS authentication credentials used for accessing AWS services such as S3. If omitted, the system’s default credentials are used (for example, the IAM role and environment variables).
assume_role
string
The Amazon Resource Name (ARN) of the role to assume.
external_id
string
A unique identifier for cross-account role assumption.
session_name
string
A session identifier used for logging and tracing the assumed role session.
bucket [_required_]
string
S3 bucket name.
id [_required_]
string
Unique identifier for the destination component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
key_prefix
string
Optional prefix for object keys.
region [_required_]
string
AWS region of the S3 bucket.
storage_class [_required_]
enum
S3 storage class. Allowed enum values: `STANDARD,REDUCED_REDUNDANCY,INTELLIGENT_TIERING,STANDARD_IA,EXPRESS_ONEZONE,ONEZONE_IA,GLACIER,GLACIER_IR,DEEP_ARCHIVE`
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. Always `amazon_s3`. Allowed enum values: `amazon_s3`
default: `amazon_s3`
object
The `google_cloud_storage` destination stores logs in a Google Cloud Storage (GCS) bucket. It requires a bucket name, GCP authentication, and metadata fields.
acl
enum
Access control list setting for objects written to the bucket. Allowed enum values: `private,project-private,public-read,authenticated-read,bucket-owner-read,bucket-owner-full-control`
object
GCP credentials used to authenticate with Google Cloud Storage.
credentials_file [_required_]
string
Path to the GCP service account key file.
bucket [_required_]
string
Name of the GCS bucket.
id [_required_]
string
Unique identifier for the destination component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
key_prefix
string
Optional prefix for object keys within the GCS bucket.
[object]
Custom metadata to attach to each object uploaded to the GCS bucket.
name [_required_]
string
The metadata key.
value [_required_]
string
The metadata value.
storage_class [_required_]
enum
Storage class used for objects stored in GCS. Allowed enum values: `STANDARD,NEARLINE,COLDLINE,ARCHIVE`
type [_required_]
enum
The destination type. Always `google_cloud_storage`. Allowed enum values: `google_cloud_storage`
default: `google_cloud_storage`
object
The `splunk_hec` destination forwards logs to Splunk using the HTTP Event Collector (HEC).
auto_extract_timestamp
boolean
If `true`, Splunk tries to extract timestamps from incoming log events. If `false`, Splunk assigns the time the event was received.
encoding
enum
Encoding format for log events. Allowed enum values: `json,raw_message`
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
index
string
Optional name of the Splunk index where logs are written.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
sourcetype
string
The Splunk sourcetype to assign to log events.
type [_required_]
enum
The destination type. Always `splunk_hec`. Allowed enum values: `splunk_hec`
default: `splunk_hec`
object
The `sumo_logic` destination forwards logs to Sumo Logic.
encoding
enum
The output encoding format. Allowed enum values: `json,raw_message,logfmt`
[object]
A list of custom headers to include in the request to Sumo Logic.
name [_required_]
string
The header field name.
value [_required_]
string
The header field value.
header_host_name
string
Optional override for the host name header.
header_source_category
string
Optional override for the source category header.
header_source_name
string
Optional override for the source name header.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `sumo_logic`. Allowed enum values: `sumo_logic`
default: `sumo_logic`
object
The `elasticsearch` destination writes logs to an Elasticsearch cluster.
api_version
enum
The Elasticsearch API version to use. Set to `auto` to auto-detect. Allowed enum values: `auto,v6,v7,v8`
bulk_index
string
The index to write logs to in Elasticsearch.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `elasticsearch`. Allowed enum values: `elasticsearch`
default: `elasticsearch`
object
The `rsyslog` destination forwards logs to an external `rsyslog` server over TCP or UDP using the syslog protocol.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
keepalive
int64
Optional socket keepalive duration in milliseconds.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. The value should always be `rsyslog`. Allowed enum values: `rsyslog`
default: `rsyslog`
object
The `syslog_ng` destination forwards logs to an external `syslog-ng` server over TCP or UDP using the syslog protocol.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
keepalive
int64
Optional socket keepalive duration in milliseconds.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. The value should always be `syslog_ng`. Allowed enum values: `syslog_ng`
default: `syslog_ng`
object
The `azure_storage` destination forwards logs to an Azure Blob Storage container.
blob_prefix
string
Optional prefix for blobs written to the container.
container_name [_required_]
string
The name of the Azure Blob Storage container to store logs in.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `azure_storage`. Allowed enum values: `azure_storage`
default: `azure_storage`
object
The `microsoft_sentinel` destination forwards logs to Microsoft Sentinel.
client_id [_required_]
string
Azure AD client ID used for authentication.
dcr_immutable_id [_required_]
string
The immutable ID of the Data Collection Rule (DCR).
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
table [_required_]
string
The name of the Log Analytics table where logs are sent.
tenant_id [_required_]
string
Azure AD tenant ID.
type [_required_]
enum
The destination type. The value should always be `microsoft_sentinel`. Allowed enum values: `microsoft_sentinel`
default: `microsoft_sentinel`
object
The `google_chronicle` destination sends logs to Google Chronicle.
object
GCP credentials used to authenticate with Google Cloud Storage.
credentials_file [_required_]
string
Path to the GCP service account key file.
customer_id [_required_]
string
The Google Chronicle customer ID.
encoding
enum
The encoding format for the logs sent to Chronicle. Allowed enum values: `json,raw_message`
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
log_type
string
The log type metadata associated with the Chronicle destination.
type [_required_]
enum
The destination type. The value should always be `google_chronicle`. Allowed enum values: `google_chronicle`
default: `google_chronicle`
object
The `new_relic` destination sends logs to the New Relic platform.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
region [_required_]
enum
The New Relic region. Allowed enum values: `us,eu`
type [_required_]
enum
The destination type. The value should always be `new_relic`. Allowed enum values: `new_relic`
default: `new_relic`
object
The `sentinel_one` destination sends logs to SentinelOne.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
region [_required_]
enum
The SentinelOne region to send logs to. Allowed enum values: `us,eu,ca,data_set_us`
type [_required_]
enum
The destination type. The value should always be `sentinel_one`. Allowed enum values: `sentinel_one`
default: `sentinel_one`
object
The `opensearch` destination writes logs to an OpenSearch cluster.
bulk_index
string
The index to write logs to.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `opensearch`. Allowed enum values: `opensearch`
default: `opensearch`
object
The `amazon_opensearch` destination writes logs to Amazon OpenSearch.
_required_]
object
Authentication settings for the Amazon OpenSearch destination. The `strategy` field determines whether basic or AWS-based authentication is used.
assume_role
string
The ARN of the role to assume (used with `aws` strategy).
aws_region
string
AWS region
external_id
string
External ID for the assumed role (used with `aws` strategy).
session_name
string
Session name for the assumed role (used with `aws` strategy).
strategy [_required_]
enum
The authentication strategy to use. Allowed enum values: `basic,aws`
bulk_index
string
The index to write logs to.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `amazon_opensearch`. Allowed enum values: `amazon_opensearch`
default: `amazon_opensearch`
object
The `socket` destination sends logs over TCP or UDP to a remote server.
encoding [_required_]
enum
Encoding format for log events. Allowed enum values: `json,raw_message`
_required_]
<oneOf>
Framing method configuration.
object
Each log event is delimited by a newline character.
method [_required_]
enum
The definition of `ObservabilityPipelineSocketDestinationFramingNewlineDelimitedMethod` object. Allowed enum values: `newline_delimited`
object
Event data is not delimited at all.
method [_required_]
enum
The definition of `ObservabilityPipelineSocketDestinationFramingBytesMethod` object. Allowed enum values: `bytes`
object
Each log event is separated using the specified delimiter character.
delimiter [_required_]
string
A single ASCII character used as a delimiter.
method [_required_]
enum
The definition of `ObservabilityPipelineSocketDestinationFramingCharacterDelimitedMethod` object. Allowed enum values: `character_delimited`
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
mode [_required_]
enum
Protocol used to send logs. Allowed enum values: `tcp,udp`
object
TLS configuration. Relevant only when `mode` is `tcp`.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. The value should always be `socket`. Allowed enum values: `socket`
default: `socket`
object
The `amazon_security_lake` destination sends your logs to Amazon Security Lake.
object
AWS authentication credentials used for accessing AWS services such as S3. If omitted, the system’s default credentials are used (for example, the IAM role and environment variables).
assume_role
string
The Amazon Resource Name (ARN) of the role to assume.
external_id
string
A unique identifier for cross-account role assumption.
session_name
string
A session identifier used for logging and tracing the assumed role session.
bucket [_required_]
string
Name of the Amazon S3 bucket in Security Lake (3-63 characters).
custom_source_name [_required_]
string
Custom source name for the logs in Security Lake.
id [_required_]
string
Unique identifier for the destination component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
region [_required_]
string
AWS region of the S3 bucket.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. Always `amazon_security_lake`. Allowed enum values: `amazon_security_lake`
default: `amazon_security_lake`
object
The `crowdstrike_next_gen_siem` destination forwards logs to CrowdStrike Next Gen SIEM.
object
Compression configuration for log events.
algorithm [_required_]
enum
Compression algorithm for log events. Allowed enum values: `gzip,zlib`
level
int64
Compression level.
encoding [_required_]
enum
Encoding format for log events. Allowed enum values: `json,raw_message`
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. The value should always be `crowdstrike_next_gen_siem`. Allowed enum values: `crowdstrike_next_gen_siem`
default: `crowdstrike_next_gen_siem`
object
The `google_pubsub` destination publishes logs to a Google Cloud Pub/Sub topic.
object
GCP credentials used to authenticate with Google Cloud Storage.
credentials_file [_required_]
string
Path to the GCP service account key file.
encoding [_required_]
enum
Encoding format for log events. Allowed enum values: `json,raw_message`
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
project [_required_]
string
The GCP project ID that owns the Pub/Sub topic.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
topic [_required_]
string
The Pub/Sub topic name to publish logs to.
type [_required_]
enum
The destination type. The value should always be `google_pubsub`. Allowed enum values: `google_pubsub`
default: `google_pubsub`
[object]
A list of processor groups that transform or enrich log data.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor group is enabled.
id [_required_]
string
The unique identifier for the processor group.
include [_required_]
string
Conditional expression for when this processor group should execute.
inputs [_required_]
[string]
A list of IDs for components whose output is used as the input for this processor group.
_required_]
[ <oneOf>]
Processors applied sequentially within this group. Events flow through each processor in order.
object
The `filter` processor allows conditional processing of logs based on a Datadog search query. Logs that match the `include` query are passed through; others are discarded.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs should pass through the filter. Logs that match this query continue to downstream components; others are dropped.
type [_required_]
enum
The processor type. The value should always be `filter`. Allowed enum values: `filter`
default: `filter`
object
The `parse_json` processor extracts JSON from a specified field and flattens it into the event. This is useful when logs contain embedded JSON as a string.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
field [_required_]
string
The name of the log field that contains a JSON string.
id [_required_]
string
A unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `parse_json`. Allowed enum values: `parse_json`
default: `parse_json`
object
The Quota Processor measures logging traffic for logs that match a specified filter. When the configured daily quota is met, the processor can drop or alert.
display_name
string
The display name for a component.
drop_events
boolean
If set to `true`, logs that matched the quota filter and sent after the quota has been met are dropped; only logs that did not match the filter query continue through the pipeline.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
ignore_when_missing_partitions
boolean
If `true`, the processor skips quota checks when partition fields are missing from the logs.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
object
The maximum amount of data or number of events allowed before the quota is enforced. Can be specified in bytes or events.
enforce [_required_]
enum
Unit for quota enforcement in bytes for data size or events for count. Allowed enum values: `bytes,events`
limit [_required_]
int64
The limit for quota enforcement.
name [_required_]
string
Name of the quota.
overflow_action
enum
The action to take when the quota is exceeded. Options:
  * `drop`: Drop the event.
  * `no_action`: Let the event pass through.
  * `overflow_routing`: Route to an overflow destination. Allowed enum values: `drop,no_action,overflow_routing`


[object]
A list of alternate quota rules that apply to specific sets of events, identified by matching field values. Each override can define a custom limit.
_required_]
[object]
A list of field matchers used to apply a specific override. If an event matches all listed key-value pairs, the corresponding override limit is enforced.
name [_required_]
string
The field name.
value [_required_]
string
The field value.
_required_]
object
The maximum amount of data or number of events allowed before the quota is enforced. Can be specified in bytes or events.
enforce [_required_]
enum
Unit for quota enforcement in bytes for data size or events for count. Allowed enum values: `bytes,events`
limit [_required_]
int64
The limit for quota enforcement.
partition_fields
[string]
A list of fields used to segment log traffic for quota enforcement. Quotas are tracked independently by unique combinations of these field values.
type [_required_]
enum
The processor type. The value should always be `quota`. Allowed enum values: `quota`
default: `quota`
object
The `add_fields` processor adds static key-value fields to logs.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
_required_]
[object]
A list of static fields (key-value pairs) that is added to each log event processed by this component.
name [_required_]
string
The field name.
value [_required_]
string
The field value.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `add_fields`. Allowed enum values: `add_fields`
default: `add_fields`
object
The `remove_fields` processor deletes specified fields from logs.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
fields [_required_]
[string]
A list of field names to be removed from each log event.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `remove_fields`. Allowed enum values: `remove_fields`
default: `remove_fields`
object
The `rename_fields` processor changes field names.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
_required_]
[object]
A list of rename rules specifying which fields to rename in the event, what to rename them to, and whether to preserve the original fields.
destination [_required_]
string
The field name to assign the renamed value to.
preserve_source [_required_]
boolean
Indicates whether the original field, that is received from the source, should be kept (`true`) or removed (`false`) after renaming.
source [_required_]
string
The original field name in the log event that should be renamed.
id [_required_]
string
A unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `rename_fields`. Allowed enum values: `rename_fields`
default: `rename_fields`
object
The `generate_datadog_metrics` processor creates custom metrics from logs and sends them to Datadog. Metrics can be counters, gauges, or distributions and optionally grouped by log fields.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline.
include
string
A Datadog search query used to determine which logs this processor targets.
[object]
Configuration for generating individual metrics.
group_by
[string]
Optional fields used to group the metric series.
include [_required_]
string
Datadog filter query to match logs for metric generation.
metric_type [_required_]
enum
Type of metric to create. Allowed enum values: `count,gauge,distribution`
name [_required_]
string
Name of the custom metric to be created.
_required_]
<oneOf>
Specifies how the value of the generated metric is computed.
object
Strategy that increments a generated metric by one for each matching event.
strategy [_required_]
enum
Increments the metric by 1 for each matching event. Allowed enum values: `increment_by_one`
object
Strategy that increments a generated metric based on the value of a log field.
field [_required_]
string
Name of the log field containing the numeric value to increment the metric by.
strategy [_required_]
enum
Uses a numeric field in the log event as the metric increment. Allowed enum values: `increment_by_field`
type [_required_]
enum
The processor type. Always `generate_datadog_metrics`. Allowed enum values: `generate_datadog_metrics`
default: `generate_datadog_metrics`
object
The `sample` processor allows probabilistic sampling of logs at a fixed rate.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
percentage
double
The percentage of logs to sample.
rate
int64
Number of events to sample (1 in N).
type [_required_]
enum
The processor type. The value should always be `sample`. Allowed enum values: `sample`
default: `sample`
object
The `parse_grok` processor extracts structured fields from unstructured log messages using Grok patterns.
disable_library_rules
boolean
If set to `true`, disables the default Grok rules provided by Datadog.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
A unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
[object]
The list of Grok parsing rules. If multiple matching rules are provided, they are evaluated in order. The first successful match is applied.
_required_]
[object]
A list of Grok parsing rules that define how to extract fields from the source field. Each rule must contain a name and a valid Grok pattern.
name [_required_]
string
The name of the rule.
rule [_required_]
string
The definition of the Grok rule.
source [_required_]
string
The name of the field in the log event to apply the Grok rules to.
[object]
A list of Grok helper rules that can be referenced by the parsing rules.
name [_required_]
string
The name of the Grok helper rule.
rule [_required_]
string
The definition of the Grok helper rule.
type [_required_]
enum
The processor type. The value should always be `parse_grok`. Allowed enum values: `parse_grok`
default: `parse_grok`
object
The `sensitive_data_scanner` processor detects and optionally redacts sensitive data in log events.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
[object]
A list of rules for identifying and acting on sensitive data patterns.
object
Configuration for keywords used to reinforce sensitive data pattern detection.
keywords [_required_]
[string]
A list of keywords to match near the sensitive pattern.
proximity [_required_]
int64
Maximum number of tokens between a keyword and a sensitive value match.
name [_required_]
string
A name identifying the rule.
_required_]
<oneOf>
Defines what action to take when sensitive data is matched.
object
Configuration for completely redacting matched sensitive data.
action [_required_]
enum
Action type that completely replaces the matched sensitive data with a fixed replacement string to remove all visibility. Allowed enum values: `redact`
_required_]
object
Configuration for fully redacting sensitive data.
replace [_required_]
string
The `ObservabilityPipelineSensitiveDataScannerProcessorActionRedactOptions` `replace`.
object
Configuration for hashing matched sensitive values.
action [_required_]
enum
Action type that replaces the matched sensitive data with a hashed representation, preserving structure while securing content. Allowed enum values: `hash`
options
object
The `ObservabilityPipelineSensitiveDataScannerProcessorActionHash` `options`.
object
Configuration for partially redacting matched sensitive data.
action [_required_]
enum
Action type that redacts part of the sensitive data while preserving a configurable number of characters, typically used for masking purposes (e.g., show last 4 digits of a credit card). Allowed enum values: `partial_redact`
_required_]
object
Controls how partial redaction is applied, including character count and direction.
characters [_required_]
int64
The `ObservabilityPipelineSensitiveDataScannerProcessorActionPartialRedactOptions` `characters`.
direction [_required_]
enum
Indicates whether to redact characters from the first or last part of the matched value. Allowed enum values: `first,last`
_required_]
<oneOf>
Pattern detection configuration for identifying sensitive data using either a custom regex or a library reference.
object
Defines a custom regex-based pattern for identifying sensitive data in logs.
_required_]
object
Options for defining a custom regex pattern.
rule [_required_]
string
A regular expression used to detect sensitive values. Must be a valid regex.
type [_required_]
enum
Indicates a custom regular expression is used for matching. Allowed enum values: `custom`
object
Specifies a pattern from Datadog’s sensitive data detection library to match known sensitive data types.
_required_]
object
Options for selecting a predefined library pattern and enabling keyword support.
id [_required_]
string
Identifier for a predefined pattern from the sensitive data scanner pattern library.
use_recommended_keywords
boolean
Whether to augment the pattern with recommended keywords (optional).
type [_required_]
enum
Indicates that a predefined library pattern is used. Allowed enum values: `library`
_required_]
<oneOf>
Determines which parts of the log the pattern-matching rule should be applied to.
object
Includes only specific fields for sensitive data scanning.
_required_]
object
Fields to which the scope rule applies.
fields [_required_]
[string]
The `ObservabilityPipelineSensitiveDataScannerProcessorScopeOptions` `fields`.
target [_required_]
enum
Applies the rule only to included fields. Allowed enum values: `include`
object
Excludes specific fields from sensitive data scanning.
_required_]
object
Fields to which the scope rule applies.
fields [_required_]
[string]
The `ObservabilityPipelineSensitiveDataScannerProcessorScopeOptions` `fields`.
target [_required_]
enum
Excludes specific fields from processing. Allowed enum values: `exclude`
object
Applies scanning across all available fields.
target [_required_]
enum
Applies the rule to all fields. Allowed enum values: `all`
tags [_required_]
[string]
Tags assigned to this rule for filtering and classification.
type [_required_]
enum
The processor type. The value should always be `sensitive_data_scanner`. Allowed enum values: `sensitive_data_scanner`
default: `sensitive_data_scanner`
object
The `ocsf_mapper` processor transforms logs into the OCSF schema using a predefined mapping configuration.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
[object]
A list of mapping rules to convert events to the OCSF format.
include [_required_]
string
A Datadog search query used to select the logs that this mapping should apply to.
_required_]
<oneOf>
Defines a single mapping rule for transforming logs into the OCSF schema.
Option 1
enum
Predefined library mappings for common log formats. Allowed enum values: `CloudTrail Account Change,GCP Cloud Audit CreateBucket,GCP Cloud Audit CreateSink,GCP Cloud Audit SetIamPolicy,GCP Cloud Audit UpdateSink,Github Audit Log API Activity,Google Workspace Admin Audit addPrivilege,Microsoft 365 Defender Incident,Microsoft 365 Defender UserLoggedIn,Okta System Log Authentication,Palo Alto Networks Firewall Traffic`
type [_required_]
enum
The processor type. The value should always be `ocsf_mapper`. Allowed enum values: `ocsf_mapper`
default: `ocsf_mapper`
object
The `add_env_vars` processor adds environment variable values to log events.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this processor in the pipeline.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `add_env_vars`. Allowed enum values: `add_env_vars`
default: `add_env_vars`
_required_]
[object]
A list of environment variable mappings to apply to log fields.
field [_required_]
string
The target field in the log event.
name [_required_]
string
The name of the environment variable to read.
object
The `dedupe` processor removes duplicate fields in log events.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
fields [_required_]
[string]
A list of log field paths to check for duplicates.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
mode [_required_]
enum
The deduplication mode to apply to the fields. Allowed enum values: `match,ignore`
type [_required_]
enum
The processor type. The value should always be `dedupe`. Allowed enum values: `dedupe`
default: `dedupe`
object
The `enrichment_table` processor enriches logs using a static CSV file or GeoIP database.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
object
Defines a static enrichment table loaded from a CSV file.
_required_]
object
File encoding format.
delimiter [_required_]
string
The `encoding` `delimiter`.
includes_headers [_required_]
boolean
The `encoding` `includes_headers`.
type [_required_]
enum
Specifies the encoding format (e.g., CSV) used for enrichment tables. Allowed enum values: `csv`
_required_]
[object]
Key fields used to look up enrichment values.
column [_required_]
string
The `items` `column`.
comparison [_required_]
enum
Defines how to compare key fields for enrichment table lookups. Allowed enum values: `equals`
field [_required_]
string
The `items` `field`.
path [_required_]
string
Path to the CSV file.
_required_]
[object]
Schema defining column names and their types.
column [_required_]
string
The `items` `column`.
type [_required_]
enum
Declares allowed data types for enrichment table columns. Allowed enum values: `string,boolean,integer,float,date,timestamp`
object
Uses a GeoIP database to enrich logs based on an IP field.
key_field [_required_]
string
Path to the IP field in the log.
locale [_required_]
string
Locale used to resolve geographical names.
path [_required_]
string
Path to the GeoIP database file.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
target [_required_]
string
Path where enrichment results should be stored in the log.
type [_required_]
enum
The processor type. The value should always be `enrichment_table`. Allowed enum values: `enrichment_table`
default: `enrichment_table`
object
The `reduce` processor aggregates and merges logs based on matching keys and merge strategies.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
group_by [_required_]
[string]
A list of fields used to group log events for merging.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
[object]
List of merge strategies defining how values from grouped events should be combined.
path [_required_]
string
The field path in the log event.
strategy [_required_]
enum
The merge strategy to apply. Allowed enum values: `discard,retain,sum,max,min,array,concat,concat_newline,concat_raw,shortest_array,longest_array,flat_unique`
type [_required_]
enum
The processor type. The value should always be `reduce`. Allowed enum values: `reduce`
default: `reduce`
object
The `throttle` processor limits the number of events that pass through over a given time window.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
group_by
[string]
Optional list of fields used to group events before the threshold has been reached.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
threshold [_required_]
int64
the number of events allowed in a given time window. Events sent after the threshold has been reached, are dropped.
type [_required_]
enum
The processor type. The value should always be `throttle`. Allowed enum values: `throttle`
default: `throttle`
window [_required_]
double
The time window in seconds over which the threshold applies.
object
The `custom_processor` processor transforms events using [Vector Remap Language (VRL)](https://vector.dev/docs/reference/vrl/) scripts with advanced filtering capabilities.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets. This field should always be set to `*` for the custom_processor processor.
default: `*`
_required_]
[object]
Array of VRL remap rules.
drop_on_error [_required_]
boolean
Whether to drop events that caused errors during processing.
enabled
boolean
Whether this remap rule is enabled.
include [_required_]
string
A Datadog search query used to filter events for this specific remap rule.
name [_required_]
string
A descriptive name for this remap rule.
source [_required_]
string
The VRL script source code that defines the processing logic.
type [_required_]
enum
The processor type. The value should always be `custom_processor`. Allowed enum values: `custom_processor`
default: `custom_processor`
object
The `datadog_tags` processor includes or excludes specific Datadog tags in your logs.
action [_required_]
enum
The action to take on tags with matching keys. Allowed enum values: `include,exclude`
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
keys [_required_]
[string]
A list of tag keys.
mode [_required_]
enum
The processing mode. Allowed enum values: `filter`
type [_required_]
enum
The processor type. The value should always be `datadog_tags`. Allowed enum values: `datadog_tags`
default: `datadog_tags`
_required_]
[ <oneOf>]
A list of configured data sources for the pipeline.
object
The `kafka` source ingests data from Apache Kafka topics.
group_id [_required_]
string
Consumer group ID used by the Kafka client.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
[object]
Optional list of advanced Kafka client configuration options, defined as key-value pairs.
name [_required_]
string
The name of the `librdkafka` configuration option to set.
value [_required_]
string
The value assigned to the specified `librdkafka` configuration option.
object
Specifies the SASL mechanism for authenticating with a Kafka cluster.
mechanism
enum
SASL mechanism used for Kafka authentication. Allowed enum values: `PLAIN,SCRAM-SHA-256,SCRAM-SHA-512`
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
topics [_required_]
[string]
A list of Kafka topic names to subscribe to. The source ingests messages from each topic specified.
type [_required_]
enum
The source type. The value should always be `kafka`. Allowed enum values: `kafka`
default: `kafka`
object
The `datadog_agent` source collects logs from the Datadog Agent.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `datadog_agent`. Allowed enum values: `datadog_agent`
default: `datadog_agent`
object
The `splunk_tcp` source receives logs from a Splunk Universal Forwarder over TCP. TLS is supported for secure transmission.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. Always `splunk_tcp`. Allowed enum values: `splunk_tcp`
default: `splunk_tcp`
object
The `splunk_hec` source implements the Splunk HTTP Event Collector (HEC) API.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. Always `splunk_hec`. Allowed enum values: `splunk_hec`
default: `splunk_hec`
object
The `amazon_s3` source ingests logs from an Amazon S3 bucket. It supports AWS authentication and TLS encryption.
object
AWS authentication credentials used for accessing AWS services such as S3. If omitted, the system’s default credentials are used (for example, the IAM role and environment variables).
assume_role
string
The Amazon Resource Name (ARN) of the role to assume.
external_id
string
A unique identifier for cross-account role assumption.
session_name
string
A session identifier used for logging and tracing the assumed role session.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
region [_required_]
string
AWS region where the S3 bucket resides.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. Always `amazon_s3`. Allowed enum values: `amazon_s3`
default: `amazon_s3`
object
The `fluentd` source ingests logs from a Fluentd-compatible service.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `fluentd. Allowed enum values: `fluentd`
default: `fluentd`
object
The `fluent_bit` source ingests logs from Fluent Bit.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `fluent_bit`. Allowed enum values: `fluent_bit`
default: `fluent_bit`
object
The `http_server` source collects logs over HTTP POST from external services.
auth_strategy [_required_]
enum
HTTP authentication method. Allowed enum values: `none,plain`
decoding [_required_]
enum
The decoding format used to interpret incoming logs. Allowed enum values: `bytes,gelf,json,syslog`
id [_required_]
string
Unique ID for the HTTP server source.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `http_server`. Allowed enum values: `http_server`
default: `http_server`
object
The `sumo_logic` source receives logs from Sumo Logic collectors.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
type [_required_]
enum
The source type. The value should always be `sumo_logic`. Allowed enum values: `sumo_logic`
default: `sumo_logic`
object
The `rsyslog` source listens for logs over TCP or UDP from an `rsyslog` server using the syslog protocol.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
mode [_required_]
enum
Protocol used by the syslog source to receive messages. Allowed enum values: `tcp,udp`
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `rsyslog`. Allowed enum values: `rsyslog`
default: `rsyslog`
object
The `syslog_ng` source listens for logs over TCP or UDP from a `syslog-ng` server using the syslog protocol.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
mode [_required_]
enum
Protocol used by the syslog source to receive messages. Allowed enum values: `tcp,udp`
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `syslog_ng`. Allowed enum values: `syslog_ng`
default: `syslog_ng`
object
The `amazon_data_firehose` source ingests logs from AWS Data Firehose.
object
AWS authentication credentials used for accessing AWS services such as S3. If omitted, the system’s default credentials are used (for example, the IAM role and environment variables).
assume_role
string
The Amazon Resource Name (ARN) of the role to assume.
external_id
string
A unique identifier for cross-account role assumption.
session_name
string
A session identifier used for logging and tracing the assumed role session.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `amazon_data_firehose`. Allowed enum values: `amazon_data_firehose`
default: `amazon_data_firehose`
object
The `google_pubsub` source ingests logs from a Google Cloud Pub/Sub subscription.
object
GCP credentials used to authenticate with Google Cloud Storage.
credentials_file [_required_]
string
Path to the GCP service account key file.
decoding [_required_]
enum
The decoding format used to interpret incoming logs. Allowed enum values: `bytes,gelf,json,syslog`
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
project [_required_]
string
The GCP project ID that owns the Pub/Sub subscription.
subscription [_required_]
string
The Pub/Sub subscription name from which messages are consumed.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `google_pubsub`. Allowed enum values: `google_pubsub`
default: `google_pubsub`
object
The `http_client` source scrapes logs from HTTP endpoints at regular intervals.
auth_strategy
enum
Optional authentication strategy for HTTP requests. Allowed enum values: `basic,bearer`
decoding [_required_]
enum
The decoding format used to interpret incoming logs. Allowed enum values: `bytes,gelf,json,syslog`
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
scrape_interval_secs
int64
The interval (in seconds) between HTTP scrape requests.
scrape_timeout_secs
int64
The timeout (in seconds) for each scrape request.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `http_client`. Allowed enum values: `http_client`
default: `http_client`
object
The `logstash` source ingests logs from a Logstash forwarder.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `logstash`. Allowed enum values: `logstash`
default: `logstash`
object
The `socket` source ingests logs over TCP or UDP.
_required_]
<oneOf>
Framing method configuration for the socket source.
object
Byte frames which are delimited by a newline character.
method [_required_]
enum
Byte frames which are delimited by a newline character. Allowed enum values: `newline_delimited`
object
Byte frames are passed through as-is according to the underlying I/O boundaries (for example, split between messages or stream segments).
method [_required_]
enum
Byte frames are passed through as-is according to the underlying I/O boundaries (for example, split between messages or stream segments). Allowed enum values: `bytes`
object
Byte frames which are delimited by a chosen character.
delimiter [_required_]
string
A single ASCII character used to delimit events.
method [_required_]
enum
Byte frames which are delimited by a chosen character. Allowed enum values: `character_delimited`
object
Byte frames according to the octet counting format as per RFC6587.
method [_required_]
enum
Byte frames according to the octet counting format as per RFC6587. Allowed enum values: `octet_counting`
object
Byte frames which are chunked GELF messages.
method [_required_]
enum
Byte frames which are chunked GELF messages. Allowed enum values: `chunked_gelf`
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
mode [_required_]
enum
Protocol used to receive logs. Allowed enum values: `tcp,udp`
object
TLS configuration. Relevant only when `mode` is `tcp`.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `socket`. Allowed enum values: `socket`
default: `socket`
name [_required_]
string
Name of the pipeline.
id [_required_]
string
Unique identifier for the pipeline.
type [_required_]
string
The resource type identifier. For pipeline resources, this should always be set to `pipelines`.
default: `pipelines`
object
Metadata about the response.
totalCount
int64
The total number of pipelines.
```
{
  "data": [
    {
      "attributes": {
        "config": {
          "destinations": [
            {
              "id": "datadog-logs-destination",
              "inputs": [
                "filter-processor"
              ],
              "type": "datadog_logs"
            }
          ],
          "processors": [
            {
              "display_name": "my component",
              "enabled": true,
              "id": "grouped-processors",
              "include": "service:my-service",
              "inputs": [
                "datadog-agent-source"
              ],
              "processors": [
                []
              ]
            }
          ],
          "sources": [
            {
              "group_id": "consumer-group-0",
              "id": "kafka-source",
              "librdkafka_options": [
                {
                  "name": "fetch.message.max.bytes",
                  "value": "1048576"
                }
              ],
              "sasl": {
                "mechanism": "string"
              },
              "tls": {
                "ca_file": "string",
                "crt_file": "/path/to/cert.crt",
                "key_file": "string"
              },
              "topics": [
                "topic1",
                "topic2"
              ],
              "type": "kafka"
            }
          ]
        },
        "name": "Main Observability Pipeline"
      },
      "id": "3fa85f64-5717-4562-b3fc-2c963f66afa6",
      "type": "pipelines"
    }
  ],
  "meta": {
    "totalCount": 42
  }
}
```

Copy
Bad Request
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


API error response.
Expand All
Field
Type
Description
errors [_required_]
[string]
A list of errors.
```
{
  "errors": [
    "Bad Request"
  ]
}
```

Copy
Not Authorized
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


API error response.
Expand All
Field
Type
Description
errors [_required_]
[string]
A list of errors.
```
{
  "errors": [
    "Bad Request"
  ]
}
```

Copy
Too many requests
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


API error response.
Expand All
Field
Type
Description
errors [_required_]
[string]
A list of errors.
```
{
  "errors": [
    "Bad Request"
  ]
}
```

Copy
### Code Example
  * [Curl](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=curl)
  * [Python](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=python)
  * [Ruby](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=ruby)
  * [Go](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=go)
  * [Java](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=java)
  * [Rust](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=rust)
  * [Typescript](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=typescript)


#####  List pipelines
Copy
```
                  # Curl command  
curl -X GET "https://api.ap1.datadoghq.com"https://api.ap2.datadoghq.com"https://api.datadoghq.eu"https://api.ddog-gov.com"https://api.datadoghq.com"https://api.us3.datadoghq.com"https://api.us5.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelines" \
-H "Accept: application/json" \
-H "DD-API-KEY: ${DD_API_KEY}" \
-H "DD-APPLICATION-KEY: ${DD_APP_KEY}"  

                
```

#####  List pipelines
```
"""
List pipelines returns "OK" response
"""

from datadog_api_client import ApiClient, Configuration
from datadog_api_client.v2.api.observability_pipelines_api import ObservabilityPipelinesApi

configuration = Configuration()
configuration.unstable_operations["list_pipelines"] = True
with ApiClient(configuration) as api_client:
    api_instance = ObservabilityPipelinesApi(api_client)
    response = api_instance.list_pipelines()

    print(response)

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=python) and then save the example to `example.py` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" python3 "example.py"


```

#####  List pipelines
```
# List pipelines returns "OK" response

require "datadog_api_client"
DatadogAPIClient.configure do |config|
  config.unstable_operations["v2.list_pipelines".to_sym] = true
end
api_instance = DatadogAPIClient::V2::ObservabilityPipelinesAPI.new
p api_instance.list_pipelines()

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=ruby) and then save the example to `example.rb` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" rb "example.rb"


```

#####  List pipelines
```
// List pipelines returns "OK" response

package main

import (
	"context"
	"encoding/json"
	"fmt"
	"os"

	"github.com/DataDog/datadog-api-client-go/v2/api/datadog"
	"github.com/DataDog/datadog-api-client-go/v2/api/datadogV2"
)

func main() {
	ctx := datadog.NewDefaultContext(context.Background())
	configuration := datadog.NewConfiguration()
	configuration.SetUnstableOperationEnabled("v2.ListPipelines", true)
	apiClient := datadog.NewAPIClient(configuration)
	api := datadogV2.NewObservabilityPipelinesApi(apiClient)
	resp, r, err := api.ListPipelines(ctx, *datadogV2.NewListPipelinesOptionalParameters())

	if err != nil {
		fmt.Fprintf(os.Stderr, "Error when calling `ObservabilityPipelinesApi.ListPipelines`: %v\n", err)
		fmt.Fprintf(os.Stderr, "Full HTTP response: %v\n", r)
	}

	responseContent, _ := json.MarshalIndent(resp, "", "  ")
	fmt.Fprintf(os.Stdout, "Response from `ObservabilityPipelinesApi.ListPipelines`:\n%s\n", responseContent)
}

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=go) and then save the example to `main.go` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" go run "main.go"


```

#####  List pipelines
```
// List pipelines returns "OK" response

import com.datadog.api.client.ApiClient;
import com.datadog.api.client.ApiException;
import com.datadog.api.client.v2.api.ObservabilityPipelinesApi;
import com.datadog.api.client.v2.model.ListPipelinesResponse;

public class Example {
  public static void main(String[] args) {
    ApiClient defaultClient = ApiClient.getDefaultApiClient();
    defaultClient.setUnstableOperationEnabled("v2.listPipelines", true);
    ObservabilityPipelinesApi apiInstance = new ObservabilityPipelinesApi(defaultClient);

    try {
      ListPipelinesResponse result = apiInstance.listPipelines();
      System.out.println(result);
    } catch (ApiException e) {
      System.err.println("Exception when calling ObservabilityPipelinesApi#listPipelines");
      System.err.println("Status code: " + e.getCode());
      System.err.println("Reason: " + e.getResponseBody());
      System.err.println("Response headers: " + e.getResponseHeaders());
      e.printStackTrace();
    }
  }
}

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=java) and then save the example to `Example.java` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" java "Example.java"


```

#####  List pipelines
```
// List pipelines returns "OK" response
use datadog_api_client::datadog;
use datadog_api_client::datadogV2::api_observability_pipelines::ListPipelinesOptionalParams;
use datadog_api_client::datadogV2::api_observability_pipelines::ObservabilityPipelinesAPI;

#[tokio::main]
async fn main() {
    let mut configuration = datadog::Configuration::new();
    configuration.set_unstable_operation_enabled("v2.ListPipelines", true);
    let api = ObservabilityPipelinesAPI::with_config(configuration);
    let resp = api
        .list_pipelines(ListPipelinesOptionalParams::default())
        .await;
    if let Ok(value) = resp {
        println!("{:#?}", value);
    } else {
        println!("{:#?}", resp.unwrap_err());
    }
}

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=rust) and then save the example to `src/main.rs` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" cargo run


```

#####  List pipelines
```
/**
 * List pipelines returns "OK" response
 */

import { client, v2 } from "@datadog/datadog-api-client";

const configuration = client.createConfiguration();
configuration.unstableOperations["v2.listPipelines"] = true;
const apiInstance = new v2.ObservabilityPipelinesApi(configuration);

apiInstance
  .listPipelines()
  .then((data: v2.ListPipelinesResponse) => {
    console.log(
      "API called successfully. Returned data: " + JSON.stringify(data)
    );
  })
  .catch((error: any) => console.error(error));

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=typescript) and then save the example to `example.ts` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" tsc "example.ts"


```

* * *
## [Create a new pipeline](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-new-pipeline)
  * [v2 (latest)](https://docs.datadoghq.com/api/latest/observability-pipelines/#create-a-new-pipeline-v2)


**Note** : This endpoint is in Preview. Fill out this [form](https://www.datadoghq.com/product-preview/observability-pipelines-api-and-terraform-support/) to request access.
POST https://api.ap1.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelineshttps://api.ap2.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelineshttps://api.datadoghq.eu/api/v2/remote_config/products/obs_pipelines/pipelineshttps://api.ddog-gov.com/api/v2/remote_config/products/obs_pipelines/pipelineshttps://api.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelineshttps://api.us3.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelineshttps://api.us5.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelines
### Overview
Create a new pipeline. This endpoint requires the `observability_pipelines_deploy` permission.
### Request
#### Body Data (required)
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


Expand All
Field
Type
Description
_required_]
object
Contains the the pipeline configuration.
_required_]
object
Defines the pipeline’s name and its components (sources, processors, and destinations).
_required_]
object
Specifies the pipeline's configuration, including its sources, processors, and destinations.
_required_]
[ <oneOf>]
A list of destination components where processed logs are sent.
object
The `datadog_logs` destination forwards logs to Datadog Log Management.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `datadog_logs`. Allowed enum values: `datadog_logs`
default: `datadog_logs`
object
The `amazon_s3` destination sends your logs in Datadog-rehydratable format to an Amazon S3 bucket for archiving.
object
AWS authentication credentials used for accessing AWS services such as S3. If omitted, the system’s default credentials are used (for example, the IAM role and environment variables).
assume_role
string
The Amazon Resource Name (ARN) of the role to assume.
external_id
string
A unique identifier for cross-account role assumption.
session_name
string
A session identifier used for logging and tracing the assumed role session.
bucket [_required_]
string
S3 bucket name.
id [_required_]
string
Unique identifier for the destination component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
key_prefix
string
Optional prefix for object keys.
region [_required_]
string
AWS region of the S3 bucket.
storage_class [_required_]
enum
S3 storage class. Allowed enum values: `STANDARD,REDUCED_REDUNDANCY,INTELLIGENT_TIERING,STANDARD_IA,EXPRESS_ONEZONE,ONEZONE_IA,GLACIER,GLACIER_IR,DEEP_ARCHIVE`
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. Always `amazon_s3`. Allowed enum values: `amazon_s3`
default: `amazon_s3`
object
The `google_cloud_storage` destination stores logs in a Google Cloud Storage (GCS) bucket. It requires a bucket name, GCP authentication, and metadata fields.
acl
enum
Access control list setting for objects written to the bucket. Allowed enum values: `private,project-private,public-read,authenticated-read,bucket-owner-read,bucket-owner-full-control`
object
GCP credentials used to authenticate with Google Cloud Storage.
credentials_file [_required_]
string
Path to the GCP service account key file.
bucket [_required_]
string
Name of the GCS bucket.
id [_required_]
string
Unique identifier for the destination component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
key_prefix
string
Optional prefix for object keys within the GCS bucket.
[object]
Custom metadata to attach to each object uploaded to the GCS bucket.
name [_required_]
string
The metadata key.
value [_required_]
string
The metadata value.
storage_class [_required_]
enum
Storage class used for objects stored in GCS. Allowed enum values: `STANDARD,NEARLINE,COLDLINE,ARCHIVE`
type [_required_]
enum
The destination type. Always `google_cloud_storage`. Allowed enum values: `google_cloud_storage`
default: `google_cloud_storage`
object
The `splunk_hec` destination forwards logs to Splunk using the HTTP Event Collector (HEC).
auto_extract_timestamp
boolean
If `true`, Splunk tries to extract timestamps from incoming log events. If `false`, Splunk assigns the time the event was received.
encoding
enum
Encoding format for log events. Allowed enum values: `json,raw_message`
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
index
string
Optional name of the Splunk index where logs are written.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
sourcetype
string
The Splunk sourcetype to assign to log events.
type [_required_]
enum
The destination type. Always `splunk_hec`. Allowed enum values: `splunk_hec`
default: `splunk_hec`
object
The `sumo_logic` destination forwards logs to Sumo Logic.
encoding
enum
The output encoding format. Allowed enum values: `json,raw_message,logfmt`
[object]
A list of custom headers to include in the request to Sumo Logic.
name [_required_]
string
The header field name.
value [_required_]
string
The header field value.
header_host_name
string
Optional override for the host name header.
header_source_category
string
Optional override for the source category header.
header_source_name
string
Optional override for the source name header.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `sumo_logic`. Allowed enum values: `sumo_logic`
default: `sumo_logic`
object
The `elasticsearch` destination writes logs to an Elasticsearch cluster.
api_version
enum
The Elasticsearch API version to use. Set to `auto` to auto-detect. Allowed enum values: `auto,v6,v7,v8`
bulk_index
string
The index to write logs to in Elasticsearch.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `elasticsearch`. Allowed enum values: `elasticsearch`
default: `elasticsearch`
object
The `rsyslog` destination forwards logs to an external `rsyslog` server over TCP or UDP using the syslog protocol.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
keepalive
int64
Optional socket keepalive duration in milliseconds.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. The value should always be `rsyslog`. Allowed enum values: `rsyslog`
default: `rsyslog`
object
The `syslog_ng` destination forwards logs to an external `syslog-ng` server over TCP or UDP using the syslog protocol.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
keepalive
int64
Optional socket keepalive duration in milliseconds.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. The value should always be `syslog_ng`. Allowed enum values: `syslog_ng`
default: `syslog_ng`
object
The `azure_storage` destination forwards logs to an Azure Blob Storage container.
blob_prefix
string
Optional prefix for blobs written to the container.
container_name [_required_]
string
The name of the Azure Blob Storage container to store logs in.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `azure_storage`. Allowed enum values: `azure_storage`
default: `azure_storage`
object
The `microsoft_sentinel` destination forwards logs to Microsoft Sentinel.
client_id [_required_]
string
Azure AD client ID used for authentication.
dcr_immutable_id [_required_]
string
The immutable ID of the Data Collection Rule (DCR).
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
table [_required_]
string
The name of the Log Analytics table where logs are sent.
tenant_id [_required_]
string
Azure AD tenant ID.
type [_required_]
enum
The destination type. The value should always be `microsoft_sentinel`. Allowed enum values: `microsoft_sentinel`
default: `microsoft_sentinel`
object
The `google_chronicle` destination sends logs to Google Chronicle.
object
GCP credentials used to authenticate with Google Cloud Storage.
credentials_file [_required_]
string
Path to the GCP service account key file.
customer_id [_required_]
string
The Google Chronicle customer ID.
encoding
enum
The encoding format for the logs sent to Chronicle. Allowed enum values: `json,raw_message`
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
log_type
string
The log type metadata associated with the Chronicle destination.
type [_required_]
enum
The destination type. The value should always be `google_chronicle`. Allowed enum values: `google_chronicle`
default: `google_chronicle`
object
The `new_relic` destination sends logs to the New Relic platform.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
region [_required_]
enum
The New Relic region. Allowed enum values: `us,eu`
type [_required_]
enum
The destination type. The value should always be `new_relic`. Allowed enum values: `new_relic`
default: `new_relic`
object
The `sentinel_one` destination sends logs to SentinelOne.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
region [_required_]
enum
The SentinelOne region to send logs to. Allowed enum values: `us,eu,ca,data_set_us`
type [_required_]
enum
The destination type. The value should always be `sentinel_one`. Allowed enum values: `sentinel_one`
default: `sentinel_one`
object
The `opensearch` destination writes logs to an OpenSearch cluster.
bulk_index
string
The index to write logs to.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `opensearch`. Allowed enum values: `opensearch`
default: `opensearch`
object
The `amazon_opensearch` destination writes logs to Amazon OpenSearch.
_required_]
object
Authentication settings for the Amazon OpenSearch destination. The `strategy` field determines whether basic or AWS-based authentication is used.
assume_role
string
The ARN of the role to assume (used with `aws` strategy).
aws_region
string
AWS region
external_id
string
External ID for the assumed role (used with `aws` strategy).
session_name
string
Session name for the assumed role (used with `aws` strategy).
strategy [_required_]
enum
The authentication strategy to use. Allowed enum values: `basic,aws`
bulk_index
string
The index to write logs to.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `amazon_opensearch`. Allowed enum values: `amazon_opensearch`
default: `amazon_opensearch`
object
The `socket` destination sends logs over TCP or UDP to a remote server.
encoding [_required_]
enum
Encoding format for log events. Allowed enum values: `json,raw_message`
_required_]
<oneOf>
Framing method configuration.
object
Each log event is delimited by a newline character.
method [_required_]
enum
The definition of `ObservabilityPipelineSocketDestinationFramingNewlineDelimitedMethod` object. Allowed enum values: `newline_delimited`
object
Event data is not delimited at all.
method [_required_]
enum
The definition of `ObservabilityPipelineSocketDestinationFramingBytesMethod` object. Allowed enum values: `bytes`
object
Each log event is separated using the specified delimiter character.
delimiter [_required_]
string
A single ASCII character used as a delimiter.
method [_required_]
enum
The definition of `ObservabilityPipelineSocketDestinationFramingCharacterDelimitedMethod` object. Allowed enum values: `character_delimited`
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
mode [_required_]
enum
Protocol used to send logs. Allowed enum values: `tcp,udp`
object
TLS configuration. Relevant only when `mode` is `tcp`.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. The value should always be `socket`. Allowed enum values: `socket`
default: `socket`
object
The `amazon_security_lake` destination sends your logs to Amazon Security Lake.
object
AWS authentication credentials used for accessing AWS services such as S3. If omitted, the system’s default credentials are used (for example, the IAM role and environment variables).
assume_role
string
The Amazon Resource Name (ARN) of the role to assume.
external_id
string
A unique identifier for cross-account role assumption.
session_name
string
A session identifier used for logging and tracing the assumed role session.
bucket [_required_]
string
Name of the Amazon S3 bucket in Security Lake (3-63 characters).
custom_source_name [_required_]
string
Custom source name for the logs in Security Lake.
id [_required_]
string
Unique identifier for the destination component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
region [_required_]
string
AWS region of the S3 bucket.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. Always `amazon_security_lake`. Allowed enum values: `amazon_security_lake`
default: `amazon_security_lake`
object
The `crowdstrike_next_gen_siem` destination forwards logs to CrowdStrike Next Gen SIEM.
object
Compression configuration for log events.
algorithm [_required_]
enum
Compression algorithm for log events. Allowed enum values: `gzip,zlib`
level
int64
Compression level.
encoding [_required_]
enum
Encoding format for log events. Allowed enum values: `json,raw_message`
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. The value should always be `crowdstrike_next_gen_siem`. Allowed enum values: `crowdstrike_next_gen_siem`
default: `crowdstrike_next_gen_siem`
object
The `google_pubsub` destination publishes logs to a Google Cloud Pub/Sub topic.
object
GCP credentials used to authenticate with Google Cloud Storage.
credentials_file [_required_]
string
Path to the GCP service account key file.
encoding [_required_]
enum
Encoding format for log events. Allowed enum values: `json,raw_message`
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
project [_required_]
string
The GCP project ID that owns the Pub/Sub topic.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
topic [_required_]
string
The Pub/Sub topic name to publish logs to.
type [_required_]
enum
The destination type. The value should always be `google_pubsub`. Allowed enum values: `google_pubsub`
default: `google_pubsub`
[object]
A list of processor groups that transform or enrich log data.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor group is enabled.
id [_required_]
string
The unique identifier for the processor group.
include [_required_]
string
Conditional expression for when this processor group should execute.
inputs [_required_]
[string]
A list of IDs for components whose output is used as the input for this processor group.
_required_]
[ <oneOf>]
Processors applied sequentially within this group. Events flow through each processor in order.
object
The `filter` processor allows conditional processing of logs based on a Datadog search query. Logs that match the `include` query are passed through; others are discarded.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs should pass through the filter. Logs that match this query continue to downstream components; others are dropped.
type [_required_]
enum
The processor type. The value should always be `filter`. Allowed enum values: `filter`
default: `filter`
object
The `parse_json` processor extracts JSON from a specified field and flattens it into the event. This is useful when logs contain embedded JSON as a string.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
field [_required_]
string
The name of the log field that contains a JSON string.
id [_required_]
string
A unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `parse_json`. Allowed enum values: `parse_json`
default: `parse_json`
object
The Quota Processor measures logging traffic for logs that match a specified filter. When the configured daily quota is met, the processor can drop or alert.
display_name
string
The display name for a component.
drop_events
boolean
If set to `true`, logs that matched the quota filter and sent after the quota has been met are dropped; only logs that did not match the filter query continue through the pipeline.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
ignore_when_missing_partitions
boolean
If `true`, the processor skips quota checks when partition fields are missing from the logs.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
object
The maximum amount of data or number of events allowed before the quota is enforced. Can be specified in bytes or events.
enforce [_required_]
enum
Unit for quota enforcement in bytes for data size or events for count. Allowed enum values: `bytes,events`
limit [_required_]
int64
The limit for quota enforcement.
name [_required_]
string
Name of the quota.
overflow_action
enum
The action to take when the quota is exceeded. Options:
  * `drop`: Drop the event.
  * `no_action`: Let the event pass through.
  * `overflow_routing`: Route to an overflow destination. Allowed enum values: `drop,no_action,overflow_routing`


[object]
A list of alternate quota rules that apply to specific sets of events, identified by matching field values. Each override can define a custom limit.
_required_]
[object]
A list of field matchers used to apply a specific override. If an event matches all listed key-value pairs, the corresponding override limit is enforced.
name [_required_]
string
The field name.
value [_required_]
string
The field value.
_required_]
object
The maximum amount of data or number of events allowed before the quota is enforced. Can be specified in bytes or events.
enforce [_required_]
enum
Unit for quota enforcement in bytes for data size or events for count. Allowed enum values: `bytes,events`
limit [_required_]
int64
The limit for quota enforcement.
partition_fields
[string]
A list of fields used to segment log traffic for quota enforcement. Quotas are tracked independently by unique combinations of these field values.
type [_required_]
enum
The processor type. The value should always be `quota`. Allowed enum values: `quota`
default: `quota`
object
The `add_fields` processor adds static key-value fields to logs.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
_required_]
[object]
A list of static fields (key-value pairs) that is added to each log event processed by this component.
name [_required_]
string
The field name.
value [_required_]
string
The field value.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `add_fields`. Allowed enum values: `add_fields`
default: `add_fields`
object
The `remove_fields` processor deletes specified fields from logs.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
fields [_required_]
[string]
A list of field names to be removed from each log event.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `remove_fields`. Allowed enum values: `remove_fields`
default: `remove_fields`
object
The `rename_fields` processor changes field names.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
_required_]
[object]
A list of rename rules specifying which fields to rename in the event, what to rename them to, and whether to preserve the original fields.
destination [_required_]
string
The field name to assign the renamed value to.
preserve_source [_required_]
boolean
Indicates whether the original field, that is received from the source, should be kept (`true`) or removed (`false`) after renaming.
source [_required_]
string
The original field name in the log event that should be renamed.
id [_required_]
string
A unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `rename_fields`. Allowed enum values: `rename_fields`
default: `rename_fields`
object
The `generate_datadog_metrics` processor creates custom metrics from logs and sends them to Datadog. Metrics can be counters, gauges, or distributions and optionally grouped by log fields.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline.
include
string
A Datadog search query used to determine which logs this processor targets.
[object]
Configuration for generating individual metrics.
group_by
[string]
Optional fields used to group the metric series.
include [_required_]
string
Datadog filter query to match logs for metric generation.
metric_type [_required_]
enum
Type of metric to create. Allowed enum values: `count,gauge,distribution`
name [_required_]
string
Name of the custom metric to be created.
_required_]
<oneOf>
Specifies how the value of the generated metric is computed.
object
Strategy that increments a generated metric by one for each matching event.
strategy [_required_]
enum
Increments the metric by 1 for each matching event. Allowed enum values: `increment_by_one`
object
Strategy that increments a generated metric based on the value of a log field.
field [_required_]
string
Name of the log field containing the numeric value to increment the metric by.
strategy [_required_]
enum
Uses a numeric field in the log event as the metric increment. Allowed enum values: `increment_by_field`
type [_required_]
enum
The processor type. Always `generate_datadog_metrics`. Allowed enum values: `generate_datadog_metrics`
default: `generate_datadog_metrics`
object
The `sample` processor allows probabilistic sampling of logs at a fixed rate.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
percentage
double
The percentage of logs to sample.
rate
int64
Number of events to sample (1 in N).
type [_required_]
enum
The processor type. The value should always be `sample`. Allowed enum values: `sample`
default: `sample`
object
The `parse_grok` processor extracts structured fields from unstructured log messages using Grok patterns.
disable_library_rules
boolean
If set to `true`, disables the default Grok rules provided by Datadog.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
A unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
[object]
The list of Grok parsing rules. If multiple matching rules are provided, they are evaluated in order. The first successful match is applied.
_required_]
[object]
A list of Grok parsing rules that define how to extract fields from the source field. Each rule must contain a name and a valid Grok pattern.
name [_required_]
string
The name of the rule.
rule [_required_]
string
The definition of the Grok rule.
source [_required_]
string
The name of the field in the log event to apply the Grok rules to.
[object]
A list of Grok helper rules that can be referenced by the parsing rules.
name [_required_]
string
The name of the Grok helper rule.
rule [_required_]
string
The definition of the Grok helper rule.
type [_required_]
enum
The processor type. The value should always be `parse_grok`. Allowed enum values: `parse_grok`
default: `parse_grok`
object
The `sensitive_data_scanner` processor detects and optionally redacts sensitive data in log events.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
[object]
A list of rules for identifying and acting on sensitive data patterns.
object
Configuration for keywords used to reinforce sensitive data pattern detection.
keywords [_required_]
[string]
A list of keywords to match near the sensitive pattern.
proximity [_required_]
int64
Maximum number of tokens between a keyword and a sensitive value match.
name [_required_]
string
A name identifying the rule.
_required_]
<oneOf>
Defines what action to take when sensitive data is matched.
object
Configuration for completely redacting matched sensitive data.
action [_required_]
enum
Action type that completely replaces the matched sensitive data with a fixed replacement string to remove all visibility. Allowed enum values: `redact`
_required_]
object
Configuration for fully redacting sensitive data.
replace [_required_]
string
The `ObservabilityPipelineSensitiveDataScannerProcessorActionRedactOptions` `replace`.
object
Configuration for hashing matched sensitive values.
action [_required_]
enum
Action type that replaces the matched sensitive data with a hashed representation, preserving structure while securing content. Allowed enum values: `hash`
options
object
The `ObservabilityPipelineSensitiveDataScannerProcessorActionHash` `options`.
object
Configuration for partially redacting matched sensitive data.
action [_required_]
enum
Action type that redacts part of the sensitive data while preserving a configurable number of characters, typically used for masking purposes (e.g., show last 4 digits of a credit card). Allowed enum values: `partial_redact`
_required_]
object
Controls how partial redaction is applied, including character count and direction.
characters [_required_]
int64
The `ObservabilityPipelineSensitiveDataScannerProcessorActionPartialRedactOptions` `characters`.
direction [_required_]
enum
Indicates whether to redact characters from the first or last part of the matched value. Allowed enum values: `first,last`
_required_]
<oneOf>
Pattern detection configuration for identifying sensitive data using either a custom regex or a library reference.
object
Defines a custom regex-based pattern for identifying sensitive data in logs.
_required_]
object
Options for defining a custom regex pattern.
rule [_required_]
string
A regular expression used to detect sensitive values. Must be a valid regex.
type [_required_]
enum
Indicates a custom regular expression is used for matching. Allowed enum values: `custom`
object
Specifies a pattern from Datadog’s sensitive data detection library to match known sensitive data types.
_required_]
object
Options for selecting a predefined library pattern and enabling keyword support.
id [_required_]
string
Identifier for a predefined pattern from the sensitive data scanner pattern library.
use_recommended_keywords
boolean
Whether to augment the pattern with recommended keywords (optional).
type [_required_]
enum
Indicates that a predefined library pattern is used. Allowed enum values: `library`
_required_]
<oneOf>
Determines which parts of the log the pattern-matching rule should be applied to.
object
Includes only specific fields for sensitive data scanning.
_required_]
object
Fields to which the scope rule applies.
fields [_required_]
[string]
The `ObservabilityPipelineSensitiveDataScannerProcessorScopeOptions` `fields`.
target [_required_]
enum
Applies the rule only to included fields. Allowed enum values: `include`
object
Excludes specific fields from sensitive data scanning.
_required_]
object
Fields to which the scope rule applies.
fields [_required_]
[string]
The `ObservabilityPipelineSensitiveDataScannerProcessorScopeOptions` `fields`.
target [_required_]
enum
Excludes specific fields from processing. Allowed enum values: `exclude`
object
Applies scanning across all available fields.
target [_required_]
enum
Applies the rule to all fields. Allowed enum values: `all`
tags [_required_]
[string]
Tags assigned to this rule for filtering and classification.
type [_required_]
enum
The processor type. The value should always be `sensitive_data_scanner`. Allowed enum values: `sensitive_data_scanner`
default: `sensitive_data_scanner`
object
The `ocsf_mapper` processor transforms logs into the OCSF schema using a predefined mapping configuration.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
[object]
A list of mapping rules to convert events to the OCSF format.
include [_required_]
string
A Datadog search query used to select the logs that this mapping should apply to.
_required_]
<oneOf>
Defines a single mapping rule for transforming logs into the OCSF schema.
Option 1
enum
Predefined library mappings for common log formats. Allowed enum values: `CloudTrail Account Change,GCP Cloud Audit CreateBucket,GCP Cloud Audit CreateSink,GCP Cloud Audit SetIamPolicy,GCP Cloud Audit UpdateSink,Github Audit Log API Activity,Google Workspace Admin Audit addPrivilege,Microsoft 365 Defender Incident,Microsoft 365 Defender UserLoggedIn,Okta System Log Authentication,Palo Alto Networks Firewall Traffic`
type [_required_]
enum
The processor type. The value should always be `ocsf_mapper`. Allowed enum values: `ocsf_mapper`
default: `ocsf_mapper`
object
The `add_env_vars` processor adds environment variable values to log events.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this processor in the pipeline.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `add_env_vars`. Allowed enum values: `add_env_vars`
default: `add_env_vars`
_required_]
[object]
A list of environment variable mappings to apply to log fields.
field [_required_]
string
The target field in the log event.
name [_required_]
string
The name of the environment variable to read.
object
The `dedupe` processor removes duplicate fields in log events.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
fields [_required_]
[string]
A list of log field paths to check for duplicates.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
mode [_required_]
enum
The deduplication mode to apply to the fields. Allowed enum values: `match,ignore`
type [_required_]
enum
The processor type. The value should always be `dedupe`. Allowed enum values: `dedupe`
default: `dedupe`
object
The `enrichment_table` processor enriches logs using a static CSV file or GeoIP database.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
object
Defines a static enrichment table loaded from a CSV file.
_required_]
object
File encoding format.
delimiter [_required_]
string
The `encoding` `delimiter`.
includes_headers [_required_]
boolean
The `encoding` `includes_headers`.
type [_required_]
enum
Specifies the encoding format (e.g., CSV) used for enrichment tables. Allowed enum values: `csv`
_required_]
[object]
Key fields used to look up enrichment values.
column [_required_]
string
The `items` `column`.
comparison [_required_]
enum
Defines how to compare key fields for enrichment table lookups. Allowed enum values: `equals`
field [_required_]
string
The `items` `field`.
path [_required_]
string
Path to the CSV file.
_required_]
[object]
Schema defining column names and their types.
column [_required_]
string
The `items` `column`.
type [_required_]
enum
Declares allowed data types for enrichment table columns. Allowed enum values: `string,boolean,integer,float,date,timestamp`
object
Uses a GeoIP database to enrich logs based on an IP field.
key_field [_required_]
string
Path to the IP field in the log.
locale [_required_]
string
Locale used to resolve geographical names.
path [_required_]
string
Path to the GeoIP database file.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
target [_required_]
string
Path where enrichment results should be stored in the log.
type [_required_]
enum
The processor type. The value should always be `enrichment_table`. Allowed enum values: `enrichment_table`
default: `enrichment_table`
object
The `reduce` processor aggregates and merges logs based on matching keys and merge strategies.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
group_by [_required_]
[string]
A list of fields used to group log events for merging.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
[object]
List of merge strategies defining how values from grouped events should be combined.
path [_required_]
string
The field path in the log event.
strategy [_required_]
enum
The merge strategy to apply. Allowed enum values: `discard,retain,sum,max,min,array,concat,concat_newline,concat_raw,shortest_array,longest_array,flat_unique`
type [_required_]
enum
The processor type. The value should always be `reduce`. Allowed enum values: `reduce`
default: `reduce`
object
The `throttle` processor limits the number of events that pass through over a given time window.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
group_by
[string]
Optional list of fields used to group events before the threshold has been reached.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
threshold [_required_]
int64
the number of events allowed in a given time window. Events sent after the threshold has been reached, are dropped.
type [_required_]
enum
The processor type. The value should always be `throttle`. Allowed enum values: `throttle`
default: `throttle`
window [_required_]
double
The time window in seconds over which the threshold applies.
object
The `custom_processor` processor transforms events using [Vector Remap Language (VRL)](https://vector.dev/docs/reference/vrl/) scripts with advanced filtering capabilities.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets. This field should always be set to `*` for the custom_processor processor.
default: `*`
_required_]
[object]
Array of VRL remap rules.
drop_on_error [_required_]
boolean
Whether to drop events that caused errors during processing.
enabled
boolean
Whether this remap rule is enabled.
include [_required_]
string
A Datadog search query used to filter events for this specific remap rule.
name [_required_]
string
A descriptive name for this remap rule.
source [_required_]
string
The VRL script source code that defines the processing logic.
type [_required_]
enum
The processor type. The value should always be `custom_processor`. Allowed enum values: `custom_processor`
default: `custom_processor`
object
The `datadog_tags` processor includes or excludes specific Datadog tags in your logs.
action [_required_]
enum
The action to take on tags with matching keys. Allowed enum values: `include,exclude`
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
keys [_required_]
[string]
A list of tag keys.
mode [_required_]
enum
The processing mode. Allowed enum values: `filter`
type [_required_]
enum
The processor type. The value should always be `datadog_tags`. Allowed enum values: `datadog_tags`
default: `datadog_tags`
_required_]
[ <oneOf>]
A list of configured data sources for the pipeline.
object
The `kafka` source ingests data from Apache Kafka topics.
group_id [_required_]
string
Consumer group ID used by the Kafka client.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
[object]
Optional list of advanced Kafka client configuration options, defined as key-value pairs.
name [_required_]
string
The name of the `librdkafka` configuration option to set.
value [_required_]
string
The value assigned to the specified `librdkafka` configuration option.
object
Specifies the SASL mechanism for authenticating with a Kafka cluster.
mechanism
enum
SASL mechanism used for Kafka authentication. Allowed enum values: `PLAIN,SCRAM-SHA-256,SCRAM-SHA-512`
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
topics [_required_]
[string]
A list of Kafka topic names to subscribe to. The source ingests messages from each topic specified.
type [_required_]
enum
The source type. The value should always be `kafka`. Allowed enum values: `kafka`
default: `kafka`
object
The `datadog_agent` source collects logs from the Datadog Agent.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `datadog_agent`. Allowed enum values: `datadog_agent`
default: `datadog_agent`
object
The `splunk_tcp` source receives logs from a Splunk Universal Forwarder over TCP. TLS is supported for secure transmission.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. Always `splunk_tcp`. Allowed enum values: `splunk_tcp`
default: `splunk_tcp`
object
The `splunk_hec` source implements the Splunk HTTP Event Collector (HEC) API.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. Always `splunk_hec`. Allowed enum values: `splunk_hec`
default: `splunk_hec`
object
The `amazon_s3` source ingests logs from an Amazon S3 bucket. It supports AWS authentication and TLS encryption.
object
AWS authentication credentials used for accessing AWS services such as S3. If omitted, the system’s default credentials are used (for example, the IAM role and environment variables).
assume_role
string
The Amazon Resource Name (ARN) of the role to assume.
external_id
string
A unique identifier for cross-account role assumption.
session_name
string
A session identifier used for logging and tracing the assumed role session.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
region [_required_]
string
AWS region where the S3 bucket resides.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. Always `amazon_s3`. Allowed enum values: `amazon_s3`
default: `amazon_s3`
object
The `fluentd` source ingests logs from a Fluentd-compatible service.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `fluentd. Allowed enum values: `fluentd`
default: `fluentd`
object
The `fluent_bit` source ingests logs from Fluent Bit.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `fluent_bit`. Allowed enum values: `fluent_bit`
default: `fluent_bit`
object
The `http_server` source collects logs over HTTP POST from external services.
auth_strategy [_required_]
enum
HTTP authentication method. Allowed enum values: `none,plain`
decoding [_required_]
enum
The decoding format used to interpret incoming logs. Allowed enum values: `bytes,gelf,json,syslog`
id [_required_]
string
Unique ID for the HTTP server source.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `http_server`. Allowed enum values: `http_server`
default: `http_server`
object
The `sumo_logic` source receives logs from Sumo Logic collectors.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
type [_required_]
enum
The source type. The value should always be `sumo_logic`. Allowed enum values: `sumo_logic`
default: `sumo_logic`
object
The `rsyslog` source listens for logs over TCP or UDP from an `rsyslog` server using the syslog protocol.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
mode [_required_]
enum
Protocol used by the syslog source to receive messages. Allowed enum values: `tcp,udp`
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `rsyslog`. Allowed enum values: `rsyslog`
default: `rsyslog`
object
The `syslog_ng` source listens for logs over TCP or UDP from a `syslog-ng` server using the syslog protocol.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
mode [_required_]
enum
Protocol used by the syslog source to receive messages. Allowed enum values: `tcp,udp`
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `syslog_ng`. Allowed enum values: `syslog_ng`
default: `syslog_ng`
object
The `amazon_data_firehose` source ingests logs from AWS Data Firehose.
object
AWS authentication credentials used for accessing AWS services such as S3. If omitted, the system’s default credentials are used (for example, the IAM role and environment variables).
assume_role
string
The Amazon Resource Name (ARN) of the role to assume.
external_id
string
A unique identifier for cross-account role assumption.
session_name
string
A session identifier used for logging and tracing the assumed role session.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `amazon_data_firehose`. Allowed enum values: `amazon_data_firehose`
default: `amazon_data_firehose`
object
The `google_pubsub` source ingests logs from a Google Cloud Pub/Sub subscription.
object
GCP credentials used to authenticate with Google Cloud Storage.
credentials_file [_required_]
string
Path to the GCP service account key file.
decoding [_required_]
enum
The decoding format used to interpret incoming logs. Allowed enum values: `bytes,gelf,json,syslog`
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
project [_required_]
string
The GCP project ID that owns the Pub/Sub subscription.
subscription [_required_]
string
The Pub/Sub subscription name from which messages are consumed.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `google_pubsub`. Allowed enum values: `google_pubsub`
default: `google_pubsub`
object
The `http_client` source scrapes logs from HTTP endpoints at regular intervals.
auth_strategy
enum
Optional authentication strategy for HTTP requests. Allowed enum values: `basic,bearer`
decoding [_required_]
enum
The decoding format used to interpret incoming logs. Allowed enum values: `bytes,gelf,json,syslog`
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
scrape_interval_secs
int64
The interval (in seconds) between HTTP scrape requests.
scrape_timeout_secs
int64
The timeout (in seconds) for each scrape request.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `http_client`. Allowed enum values: `http_client`
default: `http_client`
object
The `logstash` source ingests logs from a Logstash forwarder.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `logstash`. Allowed enum values: `logstash`
default: `logstash`
object
The `socket` source ingests logs over TCP or UDP.
_required_]
<oneOf>
Framing method configuration for the socket source.
object
Byte frames which are delimited by a newline character.
method [_required_]
enum
Byte frames which are delimited by a newline character. Allowed enum values: `newline_delimited`
object
Byte frames are passed through as-is according to the underlying I/O boundaries (for example, split between messages or stream segments).
method [_required_]
enum
Byte frames are passed through as-is according to the underlying I/O boundaries (for example, split between messages or stream segments). Allowed enum values: `bytes`
object
Byte frames which are delimited by a chosen character.
delimiter [_required_]
string
A single ASCII character used to delimit events.
method [_required_]
enum
Byte frames which are delimited by a chosen character. Allowed enum values: `character_delimited`
object
Byte frames according to the octet counting format as per RFC6587.
method [_required_]
enum
Byte frames according to the octet counting format as per RFC6587. Allowed enum values: `octet_counting`
object
Byte frames which are chunked GELF messages.
method [_required_]
enum
Byte frames which are chunked GELF messages. Allowed enum values: `chunked_gelf`
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
mode [_required_]
enum
Protocol used to receive logs. Allowed enum values: `tcp,udp`
object
TLS configuration. Relevant only when `mode` is `tcp`.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `socket`. Allowed enum values: `socket`
default: `socket`
name [_required_]
string
Name of the pipeline.
type [_required_]
string
The resource type identifier. For pipeline resources, this should always be set to `pipelines`.
default: `pipelines`
```
{
  "data": {
    "attributes": {
      "config": {
        "destinations": [
          {
            "id": "datadog-logs-destination",
            "inputs": [
              "my-processor-group"
            ],
            "type": "datadog_logs"
          }
        ],
        "processors": [
          {
            "enabled": true,
            "id": "my-processor-group",
            "include": "service:my-service",
            "inputs": [
              "datadog-agent-source"
            ],
            "processors": [
              {
                "enabled": true,
                "id": "filter-processor",
                "include": "status:error",
                "type": "filter"
              }
            ]
          }
        ],
        "sources": [
          {
            "id": "datadog-agent-source",
            "type": "datadog_agent"
          }
        ]
      },
      "name": "Main Observability Pipeline"
    },
    "type": "pipelines"
  }
}
```

Copy
### Response
  * [201](https://docs.datadoghq.com/api/latest/observability-pipelines/#CreatePipeline-201-v2)
  * [400](https://docs.datadoghq.com/api/latest/observability-pipelines/#CreatePipeline-400-v2)
  * [403](https://docs.datadoghq.com/api/latest/observability-pipelines/#CreatePipeline-403-v2)
  * [409](https://docs.datadoghq.com/api/latest/observability-pipelines/#CreatePipeline-409-v2)
  * [429](https://docs.datadoghq.com/api/latest/observability-pipelines/#CreatePipeline-429-v2)


OK
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


Top-level schema representing a pipeline.
Expand All
Field
Type
Description
_required_]
object
Contains the pipeline’s ID, type, and configuration attributes.
_required_]
object
Defines the pipeline’s name and its components (sources, processors, and destinations).
_required_]
object
Specifies the pipeline's configuration, including its sources, processors, and destinations.
_required_]
[ <oneOf>]
A list of destination components where processed logs are sent.
object
The `datadog_logs` destination forwards logs to Datadog Log Management.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `datadog_logs`. Allowed enum values: `datadog_logs`
default: `datadog_logs`
object
The `amazon_s3` destination sends your logs in Datadog-rehydratable format to an Amazon S3 bucket for archiving.
object
AWS authentication credentials used for accessing AWS services such as S3. If omitted, the system’s default credentials are used (for example, the IAM role and environment variables).
assume_role
string
The Amazon Resource Name (ARN) of the role to assume.
external_id
string
A unique identifier for cross-account role assumption.
session_name
string
A session identifier used for logging and tracing the assumed role session.
bucket [_required_]
string
S3 bucket name.
id [_required_]
string
Unique identifier for the destination component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
key_prefix
string
Optional prefix for object keys.
region [_required_]
string
AWS region of the S3 bucket.
storage_class [_required_]
enum
S3 storage class. Allowed enum values: `STANDARD,REDUCED_REDUNDANCY,INTELLIGENT_TIERING,STANDARD_IA,EXPRESS_ONEZONE,ONEZONE_IA,GLACIER,GLACIER_IR,DEEP_ARCHIVE`
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. Always `amazon_s3`. Allowed enum values: `amazon_s3`
default: `amazon_s3`
object
The `google_cloud_storage` destination stores logs in a Google Cloud Storage (GCS) bucket. It requires a bucket name, GCP authentication, and metadata fields.
acl
enum
Access control list setting for objects written to the bucket. Allowed enum values: `private,project-private,public-read,authenticated-read,bucket-owner-read,bucket-owner-full-control`
object
GCP credentials used to authenticate with Google Cloud Storage.
credentials_file [_required_]
string
Path to the GCP service account key file.
bucket [_required_]
string
Name of the GCS bucket.
id [_required_]
string
Unique identifier for the destination component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
key_prefix
string
Optional prefix for object keys within the GCS bucket.
[object]
Custom metadata to attach to each object uploaded to the GCS bucket.
name [_required_]
string
The metadata key.
value [_required_]
string
The metadata value.
storage_class [_required_]
enum
Storage class used for objects stored in GCS. Allowed enum values: `STANDARD,NEARLINE,COLDLINE,ARCHIVE`
type [_required_]
enum
The destination type. Always `google_cloud_storage`. Allowed enum values: `google_cloud_storage`
default: `google_cloud_storage`
object
The `splunk_hec` destination forwards logs to Splunk using the HTTP Event Collector (HEC).
auto_extract_timestamp
boolean
If `true`, Splunk tries to extract timestamps from incoming log events. If `false`, Splunk assigns the time the event was received.
encoding
enum
Encoding format for log events. Allowed enum values: `json,raw_message`
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
index
string
Optional name of the Splunk index where logs are written.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
sourcetype
string
The Splunk sourcetype to assign to log events.
type [_required_]
enum
The destination type. Always `splunk_hec`. Allowed enum values: `splunk_hec`
default: `splunk_hec`
object
The `sumo_logic` destination forwards logs to Sumo Logic.
encoding
enum
The output encoding format. Allowed enum values: `json,raw_message,logfmt`
[object]
A list of custom headers to include in the request to Sumo Logic.
name [_required_]
string
The header field name.
value [_required_]
string
The header field value.
header_host_name
string
Optional override for the host name header.
header_source_category
string
Optional override for the source category header.
header_source_name
string
Optional override for the source name header.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `sumo_logic`. Allowed enum values: `sumo_logic`
default: `sumo_logic`
object
The `elasticsearch` destination writes logs to an Elasticsearch cluster.
api_version
enum
The Elasticsearch API version to use. Set to `auto` to auto-detect. Allowed enum values: `auto,v6,v7,v8`
bulk_index
string
The index to write logs to in Elasticsearch.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `elasticsearch`. Allowed enum values: `elasticsearch`
default: `elasticsearch`
object
The `rsyslog` destination forwards logs to an external `rsyslog` server over TCP or UDP using the syslog protocol.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
keepalive
int64
Optional socket keepalive duration in milliseconds.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. The value should always be `rsyslog`. Allowed enum values: `rsyslog`
default: `rsyslog`
object
The `syslog_ng` destination forwards logs to an external `syslog-ng` server over TCP or UDP using the syslog protocol.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
keepalive
int64
Optional socket keepalive duration in milliseconds.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. The value should always be `syslog_ng`. Allowed enum values: `syslog_ng`
default: `syslog_ng`
object
The `azure_storage` destination forwards logs to an Azure Blob Storage container.
blob_prefix
string
Optional prefix for blobs written to the container.
container_name [_required_]
string
The name of the Azure Blob Storage container to store logs in.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `azure_storage`. Allowed enum values: `azure_storage`
default: `azure_storage`
object
The `microsoft_sentinel` destination forwards logs to Microsoft Sentinel.
client_id [_required_]
string
Azure AD client ID used for authentication.
dcr_immutable_id [_required_]
string
The immutable ID of the Data Collection Rule (DCR).
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
table [_required_]
string
The name of the Log Analytics table where logs are sent.
tenant_id [_required_]
string
Azure AD tenant ID.
type [_required_]
enum
The destination type. The value should always be `microsoft_sentinel`. Allowed enum values: `microsoft_sentinel`
default: `microsoft_sentinel`
object
The `google_chronicle` destination sends logs to Google Chronicle.
object
GCP credentials used to authenticate with Google Cloud Storage.
credentials_file [_required_]
string
Path to the GCP service account key file.
customer_id [_required_]
string
The Google Chronicle customer ID.
encoding
enum
The encoding format for the logs sent to Chronicle. Allowed enum values: `json,raw_message`
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
log_type
string
The log type metadata associated with the Chronicle destination.
type [_required_]
enum
The destination type. The value should always be `google_chronicle`. Allowed enum values: `google_chronicle`
default: `google_chronicle`
object
The `new_relic` destination sends logs to the New Relic platform.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
region [_required_]
enum
The New Relic region. Allowed enum values: `us,eu`
type [_required_]
enum
The destination type. The value should always be `new_relic`. Allowed enum values: `new_relic`
default: `new_relic`
object
The `sentinel_one` destination sends logs to SentinelOne.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
region [_required_]
enum
The SentinelOne region to send logs to. Allowed enum values: `us,eu,ca,data_set_us`
type [_required_]
enum
The destination type. The value should always be `sentinel_one`. Allowed enum values: `sentinel_one`
default: `sentinel_one`
object
The `opensearch` destination writes logs to an OpenSearch cluster.
bulk_index
string
The index to write logs to.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `opensearch`. Allowed enum values: `opensearch`
default: `opensearch`
object
The `amazon_opensearch` destination writes logs to Amazon OpenSearch.
_required_]
object
Authentication settings for the Amazon OpenSearch destination. The `strategy` field determines whether basic or AWS-based authentication is used.
assume_role
string
The ARN of the role to assume (used with `aws` strategy).
aws_region
string
AWS region
external_id
string
External ID for the assumed role (used with `aws` strategy).
session_name
string
Session name for the assumed role (used with `aws` strategy).
strategy [_required_]
enum
The authentication strategy to use. Allowed enum values: `basic,aws`
bulk_index
string
The index to write logs to.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `amazon_opensearch`. Allowed enum values: `amazon_opensearch`
default: `amazon_opensearch`
object
The `socket` destination sends logs over TCP or UDP to a remote server.
encoding [_required_]
enum
Encoding format for log events. Allowed enum values: `json,raw_message`
_required_]
<oneOf>
Framing method configuration.
object
Each log event is delimited by a newline character.
method [_required_]
enum
The definition of `ObservabilityPipelineSocketDestinationFramingNewlineDelimitedMethod` object. Allowed enum values: `newline_delimited`
object
Event data is not delimited at all.
method [_required_]
enum
The definition of `ObservabilityPipelineSocketDestinationFramingBytesMethod` object. Allowed enum values: `bytes`
object
Each log event is separated using the specified delimiter character.
delimiter [_required_]
string
A single ASCII character used as a delimiter.
method [_required_]
enum
The definition of `ObservabilityPipelineSocketDestinationFramingCharacterDelimitedMethod` object. Allowed enum values: `character_delimited`
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
mode [_required_]
enum
Protocol used to send logs. Allowed enum values: `tcp,udp`
object
TLS configuration. Relevant only when `mode` is `tcp`.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. The value should always be `socket`. Allowed enum values: `socket`
default: `socket`
object
The `amazon_security_lake` destination sends your logs to Amazon Security Lake.
object
AWS authentication credentials used for accessing AWS services such as S3. If omitted, the system’s default credentials are used (for example, the IAM role and environment variables).
assume_role
string
The Amazon Resource Name (ARN) of the role to assume.
external_id
string
A unique identifier for cross-account role assumption.
session_name
string
A session identifier used for logging and tracing the assumed role session.
bucket [_required_]
string
Name of the Amazon S3 bucket in Security Lake (3-63 characters).
custom_source_name [_required_]
string
Custom source name for the logs in Security Lake.
id [_required_]
string
Unique identifier for the destination component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
region [_required_]
string
AWS region of the S3 bucket.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. Always `amazon_security_lake`. Allowed enum values: `amazon_security_lake`
default: `amazon_security_lake`
object
The `crowdstrike_next_gen_siem` destination forwards logs to CrowdStrike Next Gen SIEM.
object
Compression configuration for log events.
algorithm [_required_]
enum
Compression algorithm for log events. Allowed enum values: `gzip,zlib`
level
int64
Compression level.
encoding [_required_]
enum
Encoding format for log events. Allowed enum values: `json,raw_message`
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. The value should always be `crowdstrike_next_gen_siem`. Allowed enum values: `crowdstrike_next_gen_siem`
default: `crowdstrike_next_gen_siem`
object
The `google_pubsub` destination publishes logs to a Google Cloud Pub/Sub topic.
object
GCP credentials used to authenticate with Google Cloud Storage.
credentials_file [_required_]
string
Path to the GCP service account key file.
encoding [_required_]
enum
Encoding format for log events. Allowed enum values: `json,raw_message`
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
project [_required_]
string
The GCP project ID that owns the Pub/Sub topic.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
topic [_required_]
string
The Pub/Sub topic name to publish logs to.
type [_required_]
enum
The destination type. The value should always be `google_pubsub`. Allowed enum values: `google_pubsub`
default: `google_pubsub`
[object]
A list of processor groups that transform or enrich log data.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor group is enabled.
id [_required_]
string
The unique identifier for the processor group.
include [_required_]
string
Conditional expression for when this processor group should execute.
inputs [_required_]
[string]
A list of IDs for components whose output is used as the input for this processor group.
_required_]
[ <oneOf>]
Processors applied sequentially within this group. Events flow through each processor in order.
object
The `filter` processor allows conditional processing of logs based on a Datadog search query. Logs that match the `include` query are passed through; others are discarded.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs should pass through the filter. Logs that match this query continue to downstream components; others are dropped.
type [_required_]
enum
The processor type. The value should always be `filter`. Allowed enum values: `filter`
default: `filter`
object
The `parse_json` processor extracts JSON from a specified field and flattens it into the event. This is useful when logs contain embedded JSON as a string.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
field [_required_]
string
The name of the log field that contains a JSON string.
id [_required_]
string
A unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `parse_json`. Allowed enum values: `parse_json`
default: `parse_json`
object
The Quota Processor measures logging traffic for logs that match a specified filter. When the configured daily quota is met, the processor can drop or alert.
display_name
string
The display name for a component.
drop_events
boolean
If set to `true`, logs that matched the quota filter and sent after the quota has been met are dropped; only logs that did not match the filter query continue through the pipeline.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
ignore_when_missing_partitions
boolean
If `true`, the processor skips quota checks when partition fields are missing from the logs.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
object
The maximum amount of data or number of events allowed before the quota is enforced. Can be specified in bytes or events.
enforce [_required_]
enum
Unit for quota enforcement in bytes for data size or events for count. Allowed enum values: `bytes,events`
limit [_required_]
int64
The limit for quota enforcement.
name [_required_]
string
Name of the quota.
overflow_action
enum
The action to take when the quota is exceeded. Options:
  * `drop`: Drop the event.
  * `no_action`: Let the event pass through.
  * `overflow_routing`: Route to an overflow destination. Allowed enum values: `drop,no_action,overflow_routing`


[object]
A list of alternate quota rules that apply to specific sets of events, identified by matching field values. Each override can define a custom limit.
_required_]
[object]
A list of field matchers used to apply a specific override. If an event matches all listed key-value pairs, the corresponding override limit is enforced.
name [_required_]
string
The field name.
value [_required_]
string
The field value.
_required_]
object
The maximum amount of data or number of events allowed before the quota is enforced. Can be specified in bytes or events.
enforce [_required_]
enum
Unit for quota enforcement in bytes for data size or events for count. Allowed enum values: `bytes,events`
limit [_required_]
int64
The limit for quota enforcement.
partition_fields
[string]
A list of fields used to segment log traffic for quota enforcement. Quotas are tracked independently by unique combinations of these field values.
type [_required_]
enum
The processor type. The value should always be `quota`. Allowed enum values: `quota`
default: `quota`
object
The `add_fields` processor adds static key-value fields to logs.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
_required_]
[object]
A list of static fields (key-value pairs) that is added to each log event processed by this component.
name [_required_]
string
The field name.
value [_required_]
string
The field value.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `add_fields`. Allowed enum values: `add_fields`
default: `add_fields`
object
The `remove_fields` processor deletes specified fields from logs.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
fields [_required_]
[string]
A list of field names to be removed from each log event.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `remove_fields`. Allowed enum values: `remove_fields`
default: `remove_fields`
object
The `rename_fields` processor changes field names.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
_required_]
[object]
A list of rename rules specifying which fields to rename in the event, what to rename them to, and whether to preserve the original fields.
destination [_required_]
string
The field name to assign the renamed value to.
preserve_source [_required_]
boolean
Indicates whether the original field, that is received from the source, should be kept (`true`) or removed (`false`) after renaming.
source [_required_]
string
The original field name in the log event that should be renamed.
id [_required_]
string
A unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `rename_fields`. Allowed enum values: `rename_fields`
default: `rename_fields`
object
The `generate_datadog_metrics` processor creates custom metrics from logs and sends them to Datadog. Metrics can be counters, gauges, or distributions and optionally grouped by log fields.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline.
include
string
A Datadog search query used to determine which logs this processor targets.
[object]
Configuration for generating individual metrics.
group_by
[string]
Optional fields used to group the metric series.
include [_required_]
string
Datadog filter query to match logs for metric generation.
metric_type [_required_]
enum
Type of metric to create. Allowed enum values: `count,gauge,distribution`
name [_required_]
string
Name of the custom metric to be created.
_required_]
<oneOf>
Specifies how the value of the generated metric is computed.
object
Strategy that increments a generated metric by one for each matching event.
strategy [_required_]
enum
Increments the metric by 1 for each matching event. Allowed enum values: `increment_by_one`
object
Strategy that increments a generated metric based on the value of a log field.
field [_required_]
string
Name of the log field containing the numeric value to increment the metric by.
strategy [_required_]
enum
Uses a numeric field in the log event as the metric increment. Allowed enum values: `increment_by_field`
type [_required_]
enum
The processor type. Always `generate_datadog_metrics`. Allowed enum values: `generate_datadog_metrics`
default: `generate_datadog_metrics`
object
The `sample` processor allows probabilistic sampling of logs at a fixed rate.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
percentage
double
The percentage of logs to sample.
rate
int64
Number of events to sample (1 in N).
type [_required_]
enum
The processor type. The value should always be `sample`. Allowed enum values: `sample`
default: `sample`
object
The `parse_grok` processor extracts structured fields from unstructured log messages using Grok patterns.
disable_library_rules
boolean
If set to `true`, disables the default Grok rules provided by Datadog.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
A unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
[object]
The list of Grok parsing rules. If multiple matching rules are provided, they are evaluated in order. The first successful match is applied.
_required_]
[object]
A list of Grok parsing rules that define how to extract fields from the source field. Each rule must contain a name and a valid Grok pattern.
name [_required_]
string
The name of the rule.
rule [_required_]
string
The definition of the Grok rule.
source [_required_]
string
The name of the field in the log event to apply the Grok rules to.
[object]
A list of Grok helper rules that can be referenced by the parsing rules.
name [_required_]
string
The name of the Grok helper rule.
rule [_required_]
string
The definition of the Grok helper rule.
type [_required_]
enum
The processor type. The value should always be `parse_grok`. Allowed enum values: `parse_grok`
default: `parse_grok`
object
The `sensitive_data_scanner` processor detects and optionally redacts sensitive data in log events.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
[object]
A list of rules for identifying and acting on sensitive data patterns.
object
Configuration for keywords used to reinforce sensitive data pattern detection.
keywords [_required_]
[string]
A list of keywords to match near the sensitive pattern.
proximity [_required_]
int64
Maximum number of tokens between a keyword and a sensitive value match.
name [_required_]
string
A name identifying the rule.
_required_]
<oneOf>
Defines what action to take when sensitive data is matched.
object
Configuration for completely redacting matched sensitive data.
action [_required_]
enum
Action type that completely replaces the matched sensitive data with a fixed replacement string to remove all visibility. Allowed enum values: `redact`
_required_]
object
Configuration for fully redacting sensitive data.
replace [_required_]
string
The `ObservabilityPipelineSensitiveDataScannerProcessorActionRedactOptions` `replace`.
object
Configuration for hashing matched sensitive values.
action [_required_]
enum
Action type that replaces the matched sensitive data with a hashed representation, preserving structure while securing content. Allowed enum values: `hash`
options
object
The `ObservabilityPipelineSensitiveDataScannerProcessorActionHash` `options`.
object
Configuration for partially redacting matched sensitive data.
action [_required_]
enum
Action type that redacts part of the sensitive data while preserving a configurable number of characters, typically used for masking purposes (e.g., show last 4 digits of a credit card). Allowed enum values: `partial_redact`
_required_]
object
Controls how partial redaction is applied, including character count and direction.
characters [_required_]
int64
The `ObservabilityPipelineSensitiveDataScannerProcessorActionPartialRedactOptions` `characters`.
direction [_required_]
enum
Indicates whether to redact characters from the first or last part of the matched value. Allowed enum values: `first,last`
_required_]
<oneOf>
Pattern detection configuration for identifying sensitive data using either a custom regex or a library reference.
object
Defines a custom regex-based pattern for identifying sensitive data in logs.
_required_]
object
Options for defining a custom regex pattern.
rule [_required_]
string
A regular expression used to detect sensitive values. Must be a valid regex.
type [_required_]
enum
Indicates a custom regular expression is used for matching. Allowed enum values: `custom`
object
Specifies a pattern from Datadog’s sensitive data detection library to match known sensitive data types.
_required_]
object
Options for selecting a predefined library pattern and enabling keyword support.
id [_required_]
string
Identifier for a predefined pattern from the sensitive data scanner pattern library.
use_recommended_keywords
boolean
Whether to augment the pattern with recommended keywords (optional).
type [_required_]
enum
Indicates that a predefined library pattern is used. Allowed enum values: `library`
_required_]
<oneOf>
Determines which parts of the log the pattern-matching rule should be applied to.
object
Includes only specific fields for sensitive data scanning.
_required_]
object
Fields to which the scope rule applies.
fields [_required_]
[string]
The `ObservabilityPipelineSensitiveDataScannerProcessorScopeOptions` `fields`.
target [_required_]
enum
Applies the rule only to included fields. Allowed enum values: `include`
object
Excludes specific fields from sensitive data scanning.
_required_]
object
Fields to which the scope rule applies.
fields [_required_]
[string]
The `ObservabilityPipelineSensitiveDataScannerProcessorScopeOptions` `fields`.
target [_required_]
enum
Excludes specific fields from processing. Allowed enum values: `exclude`
object
Applies scanning across all available fields.
target [_required_]
enum
Applies the rule to all fields. Allowed enum values: `all`
tags [_required_]
[string]
Tags assigned to this rule for filtering and classification.
type [_required_]
enum
The processor type. The value should always be `sensitive_data_scanner`. Allowed enum values: `sensitive_data_scanner`
default: `sensitive_data_scanner`
object
The `ocsf_mapper` processor transforms logs into the OCSF schema using a predefined mapping configuration.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
[object]
A list of mapping rules to convert events to the OCSF format.
include [_required_]
string
A Datadog search query used to select the logs that this mapping should apply to.
_required_]
<oneOf>
Defines a single mapping rule for transforming logs into the OCSF schema.
Option 1
enum
Predefined library mappings for common log formats. Allowed enum values: `CloudTrail Account Change,GCP Cloud Audit CreateBucket,GCP Cloud Audit CreateSink,GCP Cloud Audit SetIamPolicy,GCP Cloud Audit UpdateSink,Github Audit Log API Activity,Google Workspace Admin Audit addPrivilege,Microsoft 365 Defender Incident,Microsoft 365 Defender UserLoggedIn,Okta System Log Authentication,Palo Alto Networks Firewall Traffic`
type [_required_]
enum
The processor type. The value should always be `ocsf_mapper`. Allowed enum values: `ocsf_mapper`
default: `ocsf_mapper`
object
The `add_env_vars` processor adds environment variable values to log events.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this processor in the pipeline.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `add_env_vars`. Allowed enum values: `add_env_vars`
default: `add_env_vars`
_required_]
[object]
A list of environment variable mappings to apply to log fields.
field [_required_]
string
The target field in the log event.
name [_required_]
string
The name of the environment variable to read.
object
The `dedupe` processor removes duplicate fields in log events.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
fields [_required_]
[string]
A list of log field paths to check for duplicates.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
mode [_required_]
enum
The deduplication mode to apply to the fields. Allowed enum values: `match,ignore`
type [_required_]
enum
The processor type. The value should always be `dedupe`. Allowed enum values: `dedupe`
default: `dedupe`
object
The `enrichment_table` processor enriches logs using a static CSV file or GeoIP database.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
object
Defines a static enrichment table loaded from a CSV file.
_required_]
object
File encoding format.
delimiter [_required_]
string
The `encoding` `delimiter`.
includes_headers [_required_]
boolean
The `encoding` `includes_headers`.
type [_required_]
enum
Specifies the encoding format (e.g., CSV) used for enrichment tables. Allowed enum values: `csv`
_required_]
[object]
Key fields used to look up enrichment values.
column [_required_]
string
The `items` `column`.
comparison [_required_]
enum
Defines how to compare key fields for enrichment table lookups. Allowed enum values: `equals`
field [_required_]
string
The `items` `field`.
path [_required_]
string
Path to the CSV file.
_required_]
[object]
Schema defining column names and their types.
column [_required_]
string
The `items` `column`.
type [_required_]
enum
Declares allowed data types for enrichment table columns. Allowed enum values: `string,boolean,integer,float,date,timestamp`
object
Uses a GeoIP database to enrich logs based on an IP field.
key_field [_required_]
string
Path to the IP field in the log.
locale [_required_]
string
Locale used to resolve geographical names.
path [_required_]
string
Path to the GeoIP database file.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
target [_required_]
string
Path where enrichment results should be stored in the log.
type [_required_]
enum
The processor type. The value should always be `enrichment_table`. Allowed enum values: `enrichment_table`
default: `enrichment_table`
object
The `reduce` processor aggregates and merges logs based on matching keys and merge strategies.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
group_by [_required_]
[string]
A list of fields used to group log events for merging.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
[object]
List of merge strategies defining how values from grouped events should be combined.
path [_required_]
string
The field path in the log event.
strategy [_required_]
enum
The merge strategy to apply. Allowed enum values: `discard,retain,sum,max,min,array,concat,concat_newline,concat_raw,shortest_array,longest_array,flat_unique`
type [_required_]
enum
The processor type. The value should always be `reduce`. Allowed enum values: `reduce`
default: `reduce`
object
The `throttle` processor limits the number of events that pass through over a given time window.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
group_by
[string]
Optional list of fields used to group events before the threshold has been reached.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
threshold [_required_]
int64
the number of events allowed in a given time window. Events sent after the threshold has been reached, are dropped.
type [_required_]
enum
The processor type. The value should always be `throttle`. Allowed enum values: `throttle`
default: `throttle`
window [_required_]
double
The time window in seconds over which the threshold applies.
object
The `custom_processor` processor transforms events using [Vector Remap Language (VRL)](https://vector.dev/docs/reference/vrl/) scripts with advanced filtering capabilities.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets. This field should always be set to `*` for the custom_processor processor.
default: `*`
_required_]
[object]
Array of VRL remap rules.
drop_on_error [_required_]
boolean
Whether to drop events that caused errors during processing.
enabled
boolean
Whether this remap rule is enabled.
include [_required_]
string
A Datadog search query used to filter events for this specific remap rule.
name [_required_]
string
A descriptive name for this remap rule.
source [_required_]
string
The VRL script source code that defines the processing logic.
type [_required_]
enum
The processor type. The value should always be `custom_processor`. Allowed enum values: `custom_processor`
default: `custom_processor`
object
The `datadog_tags` processor includes or excludes specific Datadog tags in your logs.
action [_required_]
enum
The action to take on tags with matching keys. Allowed enum values: `include,exclude`
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
keys [_required_]
[string]
A list of tag keys.
mode [_required_]
enum
The processing mode. Allowed enum values: `filter`
type [_required_]
enum
The processor type. The value should always be `datadog_tags`. Allowed enum values: `datadog_tags`
default: `datadog_tags`
_required_]
[ <oneOf>]
A list of configured data sources for the pipeline.
object
The `kafka` source ingests data from Apache Kafka topics.
group_id [_required_]
string
Consumer group ID used by the Kafka client.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
[object]
Optional list of advanced Kafka client configuration options, defined as key-value pairs.
name [_required_]
string
The name of the `librdkafka` configuration option to set.
value [_required_]
string
The value assigned to the specified `librdkafka` configuration option.
object
Specifies the SASL mechanism for authenticating with a Kafka cluster.
mechanism
enum
SASL mechanism used for Kafka authentication. Allowed enum values: `PLAIN,SCRAM-SHA-256,SCRAM-SHA-512`
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
topics [_required_]
[string]
A list of Kafka topic names to subscribe to. The source ingests messages from each topic specified.
type [_required_]
enum
The source type. The value should always be `kafka`. Allowed enum values: `kafka`
default: `kafka`
object
The `datadog_agent` source collects logs from the Datadog Agent.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `datadog_agent`. Allowed enum values: `datadog_agent`
default: `datadog_agent`
object
The `splunk_tcp` source receives logs from a Splunk Universal Forwarder over TCP. TLS is supported for secure transmission.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. Always `splunk_tcp`. Allowed enum values: `splunk_tcp`
default: `splunk_tcp`
object
The `splunk_hec` source implements the Splunk HTTP Event Collector (HEC) API.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. Always `splunk_hec`. Allowed enum values: `splunk_hec`
default: `splunk_hec`
object
The `amazon_s3` source ingests logs from an Amazon S3 bucket. It supports AWS authentication and TLS encryption.
object
AWS authentication credentials used for accessing AWS services such as S3. If omitted, the system’s default credentials are used (for example, the IAM role and environment variables).
assume_role
string
The Amazon Resource Name (ARN) of the role to assume.
external_id
string
A unique identifier for cross-account role assumption.
session_name
string
A session identifier used for logging and tracing the assumed role session.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
region [_required_]
string
AWS region where the S3 bucket resides.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. Always `amazon_s3`. Allowed enum values: `amazon_s3`
default: `amazon_s3`
object
The `fluentd` source ingests logs from a Fluentd-compatible service.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `fluentd. Allowed enum values: `fluentd`
default: `fluentd`
object
The `fluent_bit` source ingests logs from Fluent Bit.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `fluent_bit`. Allowed enum values: `fluent_bit`
default: `fluent_bit`
object
The `http_server` source collects logs over HTTP POST from external services.
auth_strategy [_required_]
enum
HTTP authentication method. Allowed enum values: `none,plain`
decoding [_required_]
enum
The decoding format used to interpret incoming logs. Allowed enum values: `bytes,gelf,json,syslog`
id [_required_]
string
Unique ID for the HTTP server source.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `http_server`. Allowed enum values: `http_server`
default: `http_server`
object
The `sumo_logic` source receives logs from Sumo Logic collectors.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
type [_required_]
enum
The source type. The value should always be `sumo_logic`. Allowed enum values: `sumo_logic`
default: `sumo_logic`
object
The `rsyslog` source listens for logs over TCP or UDP from an `rsyslog` server using the syslog protocol.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
mode [_required_]
enum
Protocol used by the syslog source to receive messages. Allowed enum values: `tcp,udp`
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `rsyslog`. Allowed enum values: `rsyslog`
default: `rsyslog`
object
The `syslog_ng` source listens for logs over TCP or UDP from a `syslog-ng` server using the syslog protocol.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
mode [_required_]
enum
Protocol used by the syslog source to receive messages. Allowed enum values: `tcp,udp`
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `syslog_ng`. Allowed enum values: `syslog_ng`
default: `syslog_ng`
object
The `amazon_data_firehose` source ingests logs from AWS Data Firehose.
object
AWS authentication credentials used for accessing AWS services such as S3. If omitted, the system’s default credentials are used (for example, the IAM role and environment variables).
assume_role
string
The Amazon Resource Name (ARN) of the role to assume.
external_id
string
A unique identifier for cross-account role assumption.
session_name
string
A session identifier used for logging and tracing the assumed role session.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `amazon_data_firehose`. Allowed enum values: `amazon_data_firehose`
default: `amazon_data_firehose`
object
The `google_pubsub` source ingests logs from a Google Cloud Pub/Sub subscription.
object
GCP credentials used to authenticate with Google Cloud Storage.
credentials_file [_required_]
string
Path to the GCP service account key file.
decoding [_required_]
enum
The decoding format used to interpret incoming logs. Allowed enum values: `bytes,gelf,json,syslog`
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
project [_required_]
string
The GCP project ID that owns the Pub/Sub subscription.
subscription [_required_]
string
The Pub/Sub subscription name from which messages are consumed.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `google_pubsub`. Allowed enum values: `google_pubsub`
default: `google_pubsub`
object
The `http_client` source scrapes logs from HTTP endpoints at regular intervals.
auth_strategy
enum
Optional authentication strategy for HTTP requests. Allowed enum values: `basic,bearer`
decoding [_required_]
enum
The decoding format used to interpret incoming logs. Allowed enum values: `bytes,gelf,json,syslog`
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
scrape_interval_secs
int64
The interval (in seconds) between HTTP scrape requests.
scrape_timeout_secs
int64
The timeout (in seconds) for each scrape request.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `http_client`. Allowed enum values: `http_client`
default: `http_client`
object
The `logstash` source ingests logs from a Logstash forwarder.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `logstash`. Allowed enum values: `logstash`
default: `logstash`
object
The `socket` source ingests logs over TCP or UDP.
_required_]
<oneOf>
Framing method configuration for the socket source.
object
Byte frames which are delimited by a newline character.
method [_required_]
enum
Byte frames which are delimited by a newline character. Allowed enum values: `newline_delimited`
object
Byte frames are passed through as-is according to the underlying I/O boundaries (for example, split between messages or stream segments).
method [_required_]
enum
Byte frames are passed through as-is according to the underlying I/O boundaries (for example, split between messages or stream segments). Allowed enum values: `bytes`
object
Byte frames which are delimited by a chosen character.
delimiter [_required_]
string
A single ASCII character used to delimit events.
method [_required_]
enum
Byte frames which are delimited by a chosen character. Allowed enum values: `character_delimited`
object
Byte frames according to the octet counting format as per RFC6587.
method [_required_]
enum
Byte frames according to the octet counting format as per RFC6587. Allowed enum values: `octet_counting`
object
Byte frames which are chunked GELF messages.
method [_required_]
enum
Byte frames which are chunked GELF messages. Allowed enum values: `chunked_gelf`
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
mode [_required_]
enum
Protocol used to receive logs. Allowed enum values: `tcp,udp`
object
TLS configuration. Relevant only when `mode` is `tcp`.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `socket`. Allowed enum values: `socket`
default: `socket`
name [_required_]
string
Name of the pipeline.
id [_required_]
string
Unique identifier for the pipeline.
type [_required_]
string
The resource type identifier. For pipeline resources, this should always be set to `pipelines`.
default: `pipelines`
```
{
  "data": {
    "attributes": {
      "config": {
        "destinations": [
          {
            "id": "datadog-logs-destination",
            "inputs": [
              "filter-processor"
            ],
            "type": "datadog_logs"
          }
        ],
        "processors": [
          {
            "display_name": "my component",
            "enabled": true,
            "id": "grouped-processors",
            "include": "service:my-service",
            "inputs": [
              "datadog-agent-source"
            ],
            "processors": [
              []
            ]
          }
        ],
        "sources": [
          {
            "group_id": "consumer-group-0",
            "id": "kafka-source",
            "librdkafka_options": [
              {
                "name": "fetch.message.max.bytes",
                "value": "1048576"
              }
            ],
            "sasl": {
              "mechanism": "string"
            },
            "tls": {
              "ca_file": "string",
              "crt_file": "/path/to/cert.crt",
              "key_file": "string"
            },
            "topics": [
              "topic1",
              "topic2"
            ],
            "type": "kafka"
          }
        ]
      },
      "name": "Main Observability Pipeline"
    },
    "id": "3fa85f64-5717-4562-b3fc-2c963f66afa6",
    "type": "pipelines"
  }
}
```

Copy
Bad Request
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


API error response.
Expand All
Field
Type
Description
errors [_required_]
[string]
A list of errors.
```
{
  "errors": [
    "Bad Request"
  ]
}
```

Copy
Not Authorized
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


API error response.
Expand All
Field
Type
Description
errors [_required_]
[string]
A list of errors.
```
{
  "errors": [
    "Bad Request"
  ]
}
```

Copy
Conflict
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


API error response.
Expand All
Field
Type
Description
errors [_required_]
[string]
A list of errors.
```
{
  "errors": [
    "Bad Request"
  ]
}
```

Copy
Too many requests
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


API error response.
Expand All
Field
Type
Description
errors [_required_]
[string]
A list of errors.
```
{
  "errors": [
    "Bad Request"
  ]
}
```

Copy
### Code Example
  * [Curl](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=curl)
  * [Go](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=go)
  * [Java](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=java)
  * [Python](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=python)
  * [Ruby](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=ruby)
  * [Rust](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=rust)
  * [Typescript](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=typescript)


#####  Create a new pipeline returns "OK" response
Copy
```
                          # Curl command  
curl -X POST "https://api.ap1.datadoghq.com"https://api.ap2.datadoghq.com"https://api.datadoghq.eu"https://api.ddog-gov.com"https://api.datadoghq.com"https://api.us3.datadoghq.com"https://api.us5.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelines" \
-H "Accept: application/json" \
-H "Content-Type: application/json" \
-H "DD-API-KEY: ${DD_API_KEY}" \
-H "DD-APPLICATION-KEY: ${DD_APP_KEY}" \
-d @- << EOF
{
  "data": {
    "attributes": {
      "config": {
        "destinations": [
          {
            "id": "datadog-logs-destination",
            "inputs": [
              "my-processor-group"
            ],
            "type": "datadog_logs"
          }
        ],
        "processors": [
          {
            "enabled": true,
            "id": "my-processor-group",
            "include": "service:my-service",
            "inputs": [
              "datadog-agent-source"
            ],
            "processors": [
              {
                "enabled": true,
                "id": "filter-processor",
                "include": "status:error",
                "type": "filter"
              }
            ]
          }
        ],
        "sources": [
          {
            "id": "datadog-agent-source",
            "type": "datadog_agent"
          }
        ]
      },
      "name": "Main Observability Pipeline"
    },
    "type": "pipelines"
  }
}
EOF  

                        
```

#####  Create a new pipeline returns "OK" response
```
// Create a new pipeline returns "OK" response

package main

import (
	"context"
	"encoding/json"
	"fmt"
	"os"

	"github.com/DataDog/datadog-api-client-go/v2/api/datadog"
	"github.com/DataDog/datadog-api-client-go/v2/api/datadogV2"
)

func main() {
	body := datadogV2.ObservabilityPipelineSpec{
		Data: datadogV2.ObservabilityPipelineSpecData{
			Attributes: datadogV2.ObservabilityPipelineDataAttributes{
				Config: datadogV2.ObservabilityPipelineConfig{
					Destinations: []datadogV2.ObservabilityPipelineConfigDestinationItem{
						datadogV2.ObservabilityPipelineConfigDestinationItem{
							ObservabilityPipelineDatadogLogsDestination: &datadogV2.ObservabilityPipelineDatadogLogsDestination{
								Id: "datadog-logs-destination",
								Inputs: []string{
									"my-processor-group",
								},
								Type: datadogV2.OBSERVABILITYPIPELINEDATADOGLOGSDESTINATIONTYPE_DATADOG_LOGS,
							}},
					},
					Processors: []datadogV2.ObservabilityPipelineConfigProcessorGroup{
						{
							Enabled: true,
							Id:      "my-processor-group",
							Include: "service:my-service",
							Inputs: []string{
								"datadog-agent-source",
							},
							Processors: []datadogV2.ObservabilityPipelineConfigProcessorItem{
								datadogV2.ObservabilityPipelineConfigProcessorItem{
									ObservabilityPipelineFilterProcessor: &datadogV2.ObservabilityPipelineFilterProcessor{
										Enabled: true,
										Id:      "filter-processor",
										Include: "status:error",
										Type:    datadogV2.OBSERVABILITYPIPELINEFILTERPROCESSORTYPE_FILTER,
									}},
							},
						},
					},
					Sources: []datadogV2.ObservabilityPipelineConfigSourceItem{
						datadogV2.ObservabilityPipelineConfigSourceItem{
							ObservabilityPipelineDatadogAgentSource: &datadogV2.ObservabilityPipelineDatadogAgentSource{
								Id:   "datadog-agent-source",
								Type: datadogV2.OBSERVABILITYPIPELINEDATADOGAGENTSOURCETYPE_DATADOG_AGENT,
							}},
					},
				},
				Name: "Main Observability Pipeline",
			},
			Type: "pipelines",
		},
	}
	ctx := datadog.NewDefaultContext(context.Background())
	configuration := datadog.NewConfiguration()
	configuration.SetUnstableOperationEnabled("v2.CreatePipeline", true)
	apiClient := datadog.NewAPIClient(configuration)
	api := datadogV2.NewObservabilityPipelinesApi(apiClient)
	resp, r, err := api.CreatePipeline(ctx, body)

	if err != nil {
		fmt.Fprintf(os.Stderr, "Error when calling `ObservabilityPipelinesApi.CreatePipeline`: %v\n", err)
		fmt.Fprintf(os.Stderr, "Full HTTP response: %v\n", r)
	}

	responseContent, _ := json.MarshalIndent(resp, "", "  ")
	fmt.Fprintf(os.Stdout, "Response from `ObservabilityPipelinesApi.CreatePipeline`:\n%s\n", responseContent)
}

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=go) and then save the example to `main.go` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" go run "main.go"


```

#####  Create a new pipeline returns "OK" response
```
// Create a new pipeline returns "OK" response

import com.datadog.api.client.ApiClient;
import com.datadog.api.client.ApiException;
import com.datadog.api.client.v2.api.ObservabilityPipelinesApi;
import com.datadog.api.client.v2.model.ObservabilityPipeline;
import com.datadog.api.client.v2.model.ObservabilityPipelineConfig;
import com.datadog.api.client.v2.model.ObservabilityPipelineConfigDestinationItem;
import com.datadog.api.client.v2.model.ObservabilityPipelineConfigProcessorGroup;
import com.datadog.api.client.v2.model.ObservabilityPipelineConfigProcessorItem;
import com.datadog.api.client.v2.model.ObservabilityPipelineConfigSourceItem;
import com.datadog.api.client.v2.model.ObservabilityPipelineDataAttributes;
import com.datadog.api.client.v2.model.ObservabilityPipelineDatadogAgentSource;
import com.datadog.api.client.v2.model.ObservabilityPipelineDatadogAgentSourceType;
import com.datadog.api.client.v2.model.ObservabilityPipelineDatadogLogsDestination;
import com.datadog.api.client.v2.model.ObservabilityPipelineDatadogLogsDestinationType;
import com.datadog.api.client.v2.model.ObservabilityPipelineFilterProcessor;
import com.datadog.api.client.v2.model.ObservabilityPipelineFilterProcessorType;
import com.datadog.api.client.v2.model.ObservabilityPipelineSpec;
import com.datadog.api.client.v2.model.ObservabilityPipelineSpecData;
import java.util.Collections;

public class Example {
  public static void main(String[] args) {
    ApiClient defaultClient = ApiClient.getDefaultApiClient();
    defaultClient.setUnstableOperationEnabled("v2.createPipeline", true);
    ObservabilityPipelinesApi apiInstance = new ObservabilityPipelinesApi(defaultClient);

    ObservabilityPipelineSpec body =
        new ObservabilityPipelineSpec()
            .data(
                new ObservabilityPipelineSpecData()
                    .attributes(
                        new ObservabilityPipelineDataAttributes()
                            .config(
                                new ObservabilityPipelineConfig()
                                    .destinations(
                                        Collections.singletonList(
                                            new ObservabilityPipelineConfigDestinationItem(
                                                new ObservabilityPipelineDatadogLogsDestination()
                                                    .id("datadog-logs-destination")
                                                    .inputs(
                                                        Collections.singletonList(
                                                            "my-processor-group"))
                                                    .type(
                                                        ObservabilityPipelineDatadogLogsDestinationType
                                                            .DATADOG_LOGS))))
                                    .processors(
                                        Collections.singletonList(
                                            new ObservabilityPipelineConfigProcessorGroup()
                                                .enabled(true)
                                                .id("my-processor-group")
                                                .include("service:my-service")
                                                .inputs(
                                                    Collections.singletonList(
                                                        "datadog-agent-source"))
                                                .processors(
                                                    Collections.singletonList(
                                                        new ObservabilityPipelineConfigProcessorItem(
                                                            new ObservabilityPipelineFilterProcessor()
                                                                .enabled(true)
                                                                .id("filter-processor")
                                                                .include("status:error")
                                                                .type(
                                                                    ObservabilityPipelineFilterProcessorType
                                                                        .FILTER))))))
                                    .sources(
                                        Collections.singletonList(
                                            new ObservabilityPipelineConfigSourceItem(
                                                new ObservabilityPipelineDatadogAgentSource()
                                                    .id("datadog-agent-source")
                                                    .type(
                                                        ObservabilityPipelineDatadogAgentSourceType
                                                            .DATADOG_AGENT)))))
                            .name("Main Observability Pipeline"))
                    .type("pipelines"));

    try {
      ObservabilityPipeline result = apiInstance.createPipeline(body);
      System.out.println(result);
    } catch (ApiException e) {
      System.err.println("Exception when calling ObservabilityPipelinesApi#createPipeline");
      System.err.println("Status code: " + e.getCode());
      System.err.println("Reason: " + e.getResponseBody());
      System.err.println("Response headers: " + e.getResponseHeaders());
      e.printStackTrace();
    }
  }
}

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=java) and then save the example to `Example.java` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" java "Example.java"


```

#####  Create a new pipeline returns "OK" response
```
"""
Create a new pipeline returns "OK" response
"""

from datadog_api_client import ApiClient, Configuration
from datadog_api_client.v2.api.observability_pipelines_api import ObservabilityPipelinesApi
from datadog_api_client.v2.model.observability_pipeline_config import ObservabilityPipelineConfig
from datadog_api_client.v2.model.observability_pipeline_config_processor_group import (
    ObservabilityPipelineConfigProcessorGroup,
)
from datadog_api_client.v2.model.observability_pipeline_data_attributes import ObservabilityPipelineDataAttributes
from datadog_api_client.v2.model.observability_pipeline_datadog_agent_source import (
    ObservabilityPipelineDatadogAgentSource,
)
from datadog_api_client.v2.model.observability_pipeline_datadog_agent_source_type import (
    ObservabilityPipelineDatadogAgentSourceType,
)
from datadog_api_client.v2.model.observability_pipeline_datadog_logs_destination import (
    ObservabilityPipelineDatadogLogsDestination,
)
from datadog_api_client.v2.model.observability_pipeline_datadog_logs_destination_type import (
    ObservabilityPipelineDatadogLogsDestinationType,
)
from datadog_api_client.v2.model.observability_pipeline_filter_processor import ObservabilityPipelineFilterProcessor
from datadog_api_client.v2.model.observability_pipeline_filter_processor_type import (
    ObservabilityPipelineFilterProcessorType,
)
from datadog_api_client.v2.model.observability_pipeline_spec import ObservabilityPipelineSpec
from datadog_api_client.v2.model.observability_pipeline_spec_data import ObservabilityPipelineSpecData

body = ObservabilityPipelineSpec(
    data=ObservabilityPipelineSpecData(
        attributes=ObservabilityPipelineDataAttributes(
            config=ObservabilityPipelineConfig(
                destinations=[
                    ObservabilityPipelineDatadogLogsDestination(
                        id="datadog-logs-destination",
                        inputs=[
                            "my-processor-group",
                        ],
                        type=ObservabilityPipelineDatadogLogsDestinationType.DATADOG_LOGS,
                    ),
                ],
                processors=[
                    ObservabilityPipelineConfigProcessorGroup(
                        enabled=True,
                        id="my-processor-group",
                        include="service:my-service",
                        inputs=[
                            "datadog-agent-source",
                        ],
                        processors=[
                            ObservabilityPipelineFilterProcessor(
                                enabled=True,
                                id="filter-processor",
                                include="status:error",
                                type=ObservabilityPipelineFilterProcessorType.FILTER,
                            ),
                        ],
                    ),
                ],
                sources=[
                    ObservabilityPipelineDatadogAgentSource(
                        id="datadog-agent-source",
                        type=ObservabilityPipelineDatadogAgentSourceType.DATADOG_AGENT,
                    ),
                ],
            ),
            name="Main Observability Pipeline",
        ),
        type="pipelines",
    ),
)

configuration = Configuration()
configuration.unstable_operations["create_pipeline"] = True
with ApiClient(configuration) as api_client:
    api_instance = ObservabilityPipelinesApi(api_client)
    response = api_instance.create_pipeline(body=body)

    print(response)

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=python) and then save the example to `example.py` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" python3 "example.py"


```

#####  Create a new pipeline returns "OK" response
```
# Create a new pipeline returns "OK" response

require "datadog_api_client"
DatadogAPIClient.configure do |config|
  config.unstable_operations["v2.create_pipeline".to_sym] = true
end
api_instance = DatadogAPIClient::V2::ObservabilityPipelinesAPI.new

body = DatadogAPIClient::V2::ObservabilityPipelineSpec.new({
  data: DatadogAPIClient::V2::ObservabilityPipelineSpecData.new({
    attributes: DatadogAPIClient::V2::ObservabilityPipelineDataAttributes.new({
      config: DatadogAPIClient::V2::ObservabilityPipelineConfig.new({
        destinations: [
          DatadogAPIClient::V2::ObservabilityPipelineDatadogLogsDestination.new({
            id: "datadog-logs-destination",
            inputs: [
              "my-processor-group",
            ],
            type: DatadogAPIClient::V2::ObservabilityPipelineDatadogLogsDestinationType::DATADOG_LOGS,
          }),
        ],
        processors: [
          DatadogAPIClient::V2::ObservabilityPipelineConfigProcessorGroup.new({
            enabled: true,
            id: "my-processor-group",
            include: "service:my-service",
            inputs: [
              "datadog-agent-source",
            ],
            processors: [
              DatadogAPIClient::V2::ObservabilityPipelineFilterProcessor.new({
                enabled: true,
                id: "filter-processor",
                include: "status:error",
                type: DatadogAPIClient::V2::ObservabilityPipelineFilterProcessorType::FILTER,
              }),
            ],
          }),
        ],
        sources: [
          DatadogAPIClient::V2::ObservabilityPipelineDatadogAgentSource.new({
            id: "datadog-agent-source",
            type: DatadogAPIClient::V2::ObservabilityPipelineDatadogAgentSourceType::DATADOG_AGENT,
          }),
        ],
      }),
      name: "Main Observability Pipeline",
    }),
    type: "pipelines",
  }),
})
p api_instance.create_pipeline(body)

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=ruby) and then save the example to `example.rb` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" rb "example.rb"


```

#####  Create a new pipeline returns "OK" response
```
// Create a new pipeline returns "OK" response
use datadog_api_client::datadog;
use datadog_api_client::datadogV2::api_observability_pipelines::ObservabilityPipelinesAPI;
use datadog_api_client::datadogV2::model::ObservabilityPipelineConfig;
use datadog_api_client::datadogV2::model::ObservabilityPipelineConfigDestinationItem;
use datadog_api_client::datadogV2::model::ObservabilityPipelineConfigProcessorGroup;
use datadog_api_client::datadogV2::model::ObservabilityPipelineConfigProcessorItem;
use datadog_api_client::datadogV2::model::ObservabilityPipelineConfigSourceItem;
use datadog_api_client::datadogV2::model::ObservabilityPipelineDataAttributes;
use datadog_api_client::datadogV2::model::ObservabilityPipelineDatadogAgentSource;
use datadog_api_client::datadogV2::model::ObservabilityPipelineDatadogAgentSourceType;
use datadog_api_client::datadogV2::model::ObservabilityPipelineDatadogLogsDestination;
use datadog_api_client::datadogV2::model::ObservabilityPipelineDatadogLogsDestinationType;
use datadog_api_client::datadogV2::model::ObservabilityPipelineFilterProcessor;
use datadog_api_client::datadogV2::model::ObservabilityPipelineFilterProcessorType;
use datadog_api_client::datadogV2::model::ObservabilityPipelineSpec;
use datadog_api_client::datadogV2::model::ObservabilityPipelineSpecData;

#[tokio::main]
async fn main() {
    let body =
        ObservabilityPipelineSpec::new(
            ObservabilityPipelineSpecData::new(
                ObservabilityPipelineDataAttributes::new(
                    ObservabilityPipelineConfig::new(
                        vec![
                            ObservabilityPipelineConfigDestinationItem::ObservabilityPipelineDatadogLogsDestination(
                                Box::new(
                                    ObservabilityPipelineDatadogLogsDestination::new(
                                        "datadog-logs-destination".to_string(),
                                        vec!["my-processor-group".to_string()],
                                        ObservabilityPipelineDatadogLogsDestinationType::DATADOG_LOGS,
                                    ),
                                ),
                            )
                        ],
                        vec![
                            ObservabilityPipelineConfigSourceItem::ObservabilityPipelineDatadogAgentSource(
                                Box::new(
                                    ObservabilityPipelineDatadogAgentSource::new(
                                        "datadog-agent-source".to_string(),
                                        ObservabilityPipelineDatadogAgentSourceType::DATADOG_AGENT,
                                    ),
                                ),
                            )
                        ],
                    ).processors(
                        vec![
                            ObservabilityPipelineConfigProcessorGroup::new(
                                true,
                                "my-processor-group".to_string(),
                                "service:my-service".to_string(),
                                vec!["datadog-agent-source".to_string()],
                                vec![
                                    ObservabilityPipelineConfigProcessorItem::ObservabilityPipelineFilterProcessor(
                                        Box::new(
                                            ObservabilityPipelineFilterProcessor::new(
                                                true,
                                                "filter-processor".to_string(),
                                                "status:error".to_string(),
                                                ObservabilityPipelineFilterProcessorType::FILTER,
                                            ),
                                        ),
                                    )
                                ],
                            )
                        ],
                    ),
                    "Main Observability Pipeline".to_string(),
                ),
                "pipelines".to_string(),
            ),
        );
    let mut configuration = datadog::Configuration::new();
    configuration.set_unstable_operation_enabled("v2.CreatePipeline", true);
    let api = ObservabilityPipelinesAPI::with_config(configuration);
    let resp = api.create_pipeline(body).await;
    if let Ok(value) = resp {
        println!("{:#?}", value);
    } else {
        println!("{:#?}", resp.unwrap_err());
    }
}

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=rust) and then save the example to `src/main.rs` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" cargo run


```

#####  Create a new pipeline returns "OK" response
```
/**
 * Create a new pipeline returns "OK" response
 */

import { client, v2 } from "@datadog/datadog-api-client";

const configuration = client.createConfiguration();
configuration.unstableOperations["v2.createPipeline"] = true;
const apiInstance = new v2.ObservabilityPipelinesApi(configuration);

const params: v2.ObservabilityPipelinesApiCreatePipelineRequest = {
  body: {
    data: {
      attributes: {
        config: {
          destinations: [
            {
              id: "datadog-logs-destination",
              inputs: ["my-processor-group"],
              type: "datadog_logs",
            },
          ],
          processors: [
            {
              enabled: true,
              id: "my-processor-group",
              include: "service:my-service",
              inputs: ["datadog-agent-source"],
              processors: [
                {
                  enabled: true,
                  id: "filter-processor",
                  include: "status:error",
                  type: "filter",
                },
              ],
            },
          ],
          sources: [
            {
              id: "datadog-agent-source",
              type: "datadog_agent",
            },
          ],
        },
        name: "Main Observability Pipeline",
      },
      type: "pipelines",
    },
  },
};

apiInstance
  .createPipeline(params)
  .then((data: v2.ObservabilityPipeline) => {
    console.log(
      "API called successfully. Returned data: " + JSON.stringify(data)
    );
  })
  .catch((error: any) => console.error(error));

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=typescript) and then save the example to `example.ts` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" tsc "example.ts"


```

* * *
## [Get a specific pipeline](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-specific-pipeline)
  * [v2 (latest)](https://docs.datadoghq.com/api/latest/observability-pipelines/#get-a-specific-pipeline-v2)


**Note** : This endpoint is in Preview. Fill out this [form](https://www.datadoghq.com/product-preview/observability-pipelines-api-and-terraform-support/) to request access.
GET https://api.ap1.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelines/{pipeline_id}https://api.ap2.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelines/{pipeline_id}https://api.datadoghq.eu/api/v2/remote_config/products/obs_pipelines/pipelines/{pipeline_id}https://api.ddog-gov.com/api/v2/remote_config/products/obs_pipelines/pipelines/{pipeline_id}https://api.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelines/{pipeline_id}https://api.us3.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelines/{pipeline_id}https://api.us5.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelines/{pipeline_id}
### Overview
Get a specific pipeline by its ID. This endpoint requires the `observability_pipelines_read` permission.
### Arguments
#### Path Parameters
Name
Type
Description
pipeline_id [_required_]
string
The ID of the pipeline to retrieve.
### Response
  * [200](https://docs.datadoghq.com/api/latest/observability-pipelines/#GetPipeline-200-v2)
  * [403](https://docs.datadoghq.com/api/latest/observability-pipelines/#GetPipeline-403-v2)
  * [429](https://docs.datadoghq.com/api/latest/observability-pipelines/#GetPipeline-429-v2)


OK
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


Top-level schema representing a pipeline.
Expand All
Field
Type
Description
_required_]
object
Contains the pipeline’s ID, type, and configuration attributes.
_required_]
object
Defines the pipeline’s name and its components (sources, processors, and destinations).
_required_]
object
Specifies the pipeline's configuration, including its sources, processors, and destinations.
_required_]
[ <oneOf>]
A list of destination components where processed logs are sent.
object
The `datadog_logs` destination forwards logs to Datadog Log Management.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `datadog_logs`. Allowed enum values: `datadog_logs`
default: `datadog_logs`
object
The `amazon_s3` destination sends your logs in Datadog-rehydratable format to an Amazon S3 bucket for archiving.
object
AWS authentication credentials used for accessing AWS services such as S3. If omitted, the system’s default credentials are used (for example, the IAM role and environment variables).
assume_role
string
The Amazon Resource Name (ARN) of the role to assume.
external_id
string
A unique identifier for cross-account role assumption.
session_name
string
A session identifier used for logging and tracing the assumed role session.
bucket [_required_]
string
S3 bucket name.
id [_required_]
string
Unique identifier for the destination component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
key_prefix
string
Optional prefix for object keys.
region [_required_]
string
AWS region of the S3 bucket.
storage_class [_required_]
enum
S3 storage class. Allowed enum values: `STANDARD,REDUCED_REDUNDANCY,INTELLIGENT_TIERING,STANDARD_IA,EXPRESS_ONEZONE,ONEZONE_IA,GLACIER,GLACIER_IR,DEEP_ARCHIVE`
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. Always `amazon_s3`. Allowed enum values: `amazon_s3`
default: `amazon_s3`
object
The `google_cloud_storage` destination stores logs in a Google Cloud Storage (GCS) bucket. It requires a bucket name, GCP authentication, and metadata fields.
acl
enum
Access control list setting for objects written to the bucket. Allowed enum values: `private,project-private,public-read,authenticated-read,bucket-owner-read,bucket-owner-full-control`
object
GCP credentials used to authenticate with Google Cloud Storage.
credentials_file [_required_]
string
Path to the GCP service account key file.
bucket [_required_]
string
Name of the GCS bucket.
id [_required_]
string
Unique identifier for the destination component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
key_prefix
string
Optional prefix for object keys within the GCS bucket.
[object]
Custom metadata to attach to each object uploaded to the GCS bucket.
name [_required_]
string
The metadata key.
value [_required_]
string
The metadata value.
storage_class [_required_]
enum
Storage class used for objects stored in GCS. Allowed enum values: `STANDARD,NEARLINE,COLDLINE,ARCHIVE`
type [_required_]
enum
The destination type. Always `google_cloud_storage`. Allowed enum values: `google_cloud_storage`
default: `google_cloud_storage`
object
The `splunk_hec` destination forwards logs to Splunk using the HTTP Event Collector (HEC).
auto_extract_timestamp
boolean
If `true`, Splunk tries to extract timestamps from incoming log events. If `false`, Splunk assigns the time the event was received.
encoding
enum
Encoding format for log events. Allowed enum values: `json,raw_message`
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
index
string
Optional name of the Splunk index where logs are written.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
sourcetype
string
The Splunk sourcetype to assign to log events.
type [_required_]
enum
The destination type. Always `splunk_hec`. Allowed enum values: `splunk_hec`
default: `splunk_hec`
object
The `sumo_logic` destination forwards logs to Sumo Logic.
encoding
enum
The output encoding format. Allowed enum values: `json,raw_message,logfmt`
[object]
A list of custom headers to include in the request to Sumo Logic.
name [_required_]
string
The header field name.
value [_required_]
string
The header field value.
header_host_name
string
Optional override for the host name header.
header_source_category
string
Optional override for the source category header.
header_source_name
string
Optional override for the source name header.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `sumo_logic`. Allowed enum values: `sumo_logic`
default: `sumo_logic`
object
The `elasticsearch` destination writes logs to an Elasticsearch cluster.
api_version
enum
The Elasticsearch API version to use. Set to `auto` to auto-detect. Allowed enum values: `auto,v6,v7,v8`
bulk_index
string
The index to write logs to in Elasticsearch.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `elasticsearch`. Allowed enum values: `elasticsearch`
default: `elasticsearch`
object
The `rsyslog` destination forwards logs to an external `rsyslog` server over TCP or UDP using the syslog protocol.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
keepalive
int64
Optional socket keepalive duration in milliseconds.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. The value should always be `rsyslog`. Allowed enum values: `rsyslog`
default: `rsyslog`
object
The `syslog_ng` destination forwards logs to an external `syslog-ng` server over TCP or UDP using the syslog protocol.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
keepalive
int64
Optional socket keepalive duration in milliseconds.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. The value should always be `syslog_ng`. Allowed enum values: `syslog_ng`
default: `syslog_ng`
object
The `azure_storage` destination forwards logs to an Azure Blob Storage container.
blob_prefix
string
Optional prefix for blobs written to the container.
container_name [_required_]
string
The name of the Azure Blob Storage container to store logs in.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `azure_storage`. Allowed enum values: `azure_storage`
default: `azure_storage`
object
The `microsoft_sentinel` destination forwards logs to Microsoft Sentinel.
client_id [_required_]
string
Azure AD client ID used for authentication.
dcr_immutable_id [_required_]
string
The immutable ID of the Data Collection Rule (DCR).
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
table [_required_]
string
The name of the Log Analytics table where logs are sent.
tenant_id [_required_]
string
Azure AD tenant ID.
type [_required_]
enum
The destination type. The value should always be `microsoft_sentinel`. Allowed enum values: `microsoft_sentinel`
default: `microsoft_sentinel`
object
The `google_chronicle` destination sends logs to Google Chronicle.
object
GCP credentials used to authenticate with Google Cloud Storage.
credentials_file [_required_]
string
Path to the GCP service account key file.
customer_id [_required_]
string
The Google Chronicle customer ID.
encoding
enum
The encoding format for the logs sent to Chronicle. Allowed enum values: `json,raw_message`
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
log_type
string
The log type metadata associated with the Chronicle destination.
type [_required_]
enum
The destination type. The value should always be `google_chronicle`. Allowed enum values: `google_chronicle`
default: `google_chronicle`
object
The `new_relic` destination sends logs to the New Relic platform.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
region [_required_]
enum
The New Relic region. Allowed enum values: `us,eu`
type [_required_]
enum
The destination type. The value should always be `new_relic`. Allowed enum values: `new_relic`
default: `new_relic`
object
The `sentinel_one` destination sends logs to SentinelOne.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
region [_required_]
enum
The SentinelOne region to send logs to. Allowed enum values: `us,eu,ca,data_set_us`
type [_required_]
enum
The destination type. The value should always be `sentinel_one`. Allowed enum values: `sentinel_one`
default: `sentinel_one`
object
The `opensearch` destination writes logs to an OpenSearch cluster.
bulk_index
string
The index to write logs to.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `opensearch`. Allowed enum values: `opensearch`
default: `opensearch`
object
The `amazon_opensearch` destination writes logs to Amazon OpenSearch.
_required_]
object
Authentication settings for the Amazon OpenSearch destination. The `strategy` field determines whether basic or AWS-based authentication is used.
assume_role
string
The ARN of the role to assume (used with `aws` strategy).
aws_region
string
AWS region
external_id
string
External ID for the assumed role (used with `aws` strategy).
session_name
string
Session name for the assumed role (used with `aws` strategy).
strategy [_required_]
enum
The authentication strategy to use. Allowed enum values: `basic,aws`
bulk_index
string
The index to write logs to.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `amazon_opensearch`. Allowed enum values: `amazon_opensearch`
default: `amazon_opensearch`
object
The `socket` destination sends logs over TCP or UDP to a remote server.
encoding [_required_]
enum
Encoding format for log events. Allowed enum values: `json,raw_message`
_required_]
<oneOf>
Framing method configuration.
object
Each log event is delimited by a newline character.
method [_required_]
enum
The definition of `ObservabilityPipelineSocketDestinationFramingNewlineDelimitedMethod` object. Allowed enum values: `newline_delimited`
object
Event data is not delimited at all.
method [_required_]
enum
The definition of `ObservabilityPipelineSocketDestinationFramingBytesMethod` object. Allowed enum values: `bytes`
object
Each log event is separated using the specified delimiter character.
delimiter [_required_]
string
A single ASCII character used as a delimiter.
method [_required_]
enum
The definition of `ObservabilityPipelineSocketDestinationFramingCharacterDelimitedMethod` object. Allowed enum values: `character_delimited`
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
mode [_required_]
enum
Protocol used to send logs. Allowed enum values: `tcp,udp`
object
TLS configuration. Relevant only when `mode` is `tcp`.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. The value should always be `socket`. Allowed enum values: `socket`
default: `socket`
object
The `amazon_security_lake` destination sends your logs to Amazon Security Lake.
object
AWS authentication credentials used for accessing AWS services such as S3. If omitted, the system’s default credentials are used (for example, the IAM role and environment variables).
assume_role
string
The Amazon Resource Name (ARN) of the role to assume.
external_id
string
A unique identifier for cross-account role assumption.
session_name
string
A session identifier used for logging and tracing the assumed role session.
bucket [_required_]
string
Name of the Amazon S3 bucket in Security Lake (3-63 characters).
custom_source_name [_required_]
string
Custom source name for the logs in Security Lake.
id [_required_]
string
Unique identifier for the destination component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
region [_required_]
string
AWS region of the S3 bucket.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. Always `amazon_security_lake`. Allowed enum values: `amazon_security_lake`
default: `amazon_security_lake`
object
The `crowdstrike_next_gen_siem` destination forwards logs to CrowdStrike Next Gen SIEM.
object
Compression configuration for log events.
algorithm [_required_]
enum
Compression algorithm for log events. Allowed enum values: `gzip,zlib`
level
int64
Compression level.
encoding [_required_]
enum
Encoding format for log events. Allowed enum values: `json,raw_message`
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. The value should always be `crowdstrike_next_gen_siem`. Allowed enum values: `crowdstrike_next_gen_siem`
default: `crowdstrike_next_gen_siem`
object
The `google_pubsub` destination publishes logs to a Google Cloud Pub/Sub topic.
object
GCP credentials used to authenticate with Google Cloud Storage.
credentials_file [_required_]
string
Path to the GCP service account key file.
encoding [_required_]
enum
Encoding format for log events. Allowed enum values: `json,raw_message`
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
project [_required_]
string
The GCP project ID that owns the Pub/Sub topic.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
topic [_required_]
string
The Pub/Sub topic name to publish logs to.
type [_required_]
enum
The destination type. The value should always be `google_pubsub`. Allowed enum values: `google_pubsub`
default: `google_pubsub`
[object]
A list of processor groups that transform or enrich log data.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor group is enabled.
id [_required_]
string
The unique identifier for the processor group.
include [_required_]
string
Conditional expression for when this processor group should execute.
inputs [_required_]
[string]
A list of IDs for components whose output is used as the input for this processor group.
_required_]
[ <oneOf>]
Processors applied sequentially within this group. Events flow through each processor in order.
object
The `filter` processor allows conditional processing of logs based on a Datadog search query. Logs that match the `include` query are passed through; others are discarded.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs should pass through the filter. Logs that match this query continue to downstream components; others are dropped.
type [_required_]
enum
The processor type. The value should always be `filter`. Allowed enum values: `filter`
default: `filter`
object
The `parse_json` processor extracts JSON from a specified field and flattens it into the event. This is useful when logs contain embedded JSON as a string.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
field [_required_]
string
The name of the log field that contains a JSON string.
id [_required_]
string
A unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `parse_json`. Allowed enum values: `parse_json`
default: `parse_json`
object
The Quota Processor measures logging traffic for logs that match a specified filter. When the configured daily quota is met, the processor can drop or alert.
display_name
string
The display name for a component.
drop_events
boolean
If set to `true`, logs that matched the quota filter and sent after the quota has been met are dropped; only logs that did not match the filter query continue through the pipeline.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
ignore_when_missing_partitions
boolean
If `true`, the processor skips quota checks when partition fields are missing from the logs.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
object
The maximum amount of data or number of events allowed before the quota is enforced. Can be specified in bytes or events.
enforce [_required_]
enum
Unit for quota enforcement in bytes for data size or events for count. Allowed enum values: `bytes,events`
limit [_required_]
int64
The limit for quota enforcement.
name [_required_]
string
Name of the quota.
overflow_action
enum
The action to take when the quota is exceeded. Options:
  * `drop`: Drop the event.
  * `no_action`: Let the event pass through.
  * `overflow_routing`: Route to an overflow destination. Allowed enum values: `drop,no_action,overflow_routing`


[object]
A list of alternate quota rules that apply to specific sets of events, identified by matching field values. Each override can define a custom limit.
_required_]
[object]
A list of field matchers used to apply a specific override. If an event matches all listed key-value pairs, the corresponding override limit is enforced.
name [_required_]
string
The field name.
value [_required_]
string
The field value.
_required_]
object
The maximum amount of data or number of events allowed before the quota is enforced. Can be specified in bytes or events.
enforce [_required_]
enum
Unit for quota enforcement in bytes for data size or events for count. Allowed enum values: `bytes,events`
limit [_required_]
int64
The limit for quota enforcement.
partition_fields
[string]
A list of fields used to segment log traffic for quota enforcement. Quotas are tracked independently by unique combinations of these field values.
type [_required_]
enum
The processor type. The value should always be `quota`. Allowed enum values: `quota`
default: `quota`
object
The `add_fields` processor adds static key-value fields to logs.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
_required_]
[object]
A list of static fields (key-value pairs) that is added to each log event processed by this component.
name [_required_]
string
The field name.
value [_required_]
string
The field value.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `add_fields`. Allowed enum values: `add_fields`
default: `add_fields`
object
The `remove_fields` processor deletes specified fields from logs.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
fields [_required_]
[string]
A list of field names to be removed from each log event.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `remove_fields`. Allowed enum values: `remove_fields`
default: `remove_fields`
object
The `rename_fields` processor changes field names.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
_required_]
[object]
A list of rename rules specifying which fields to rename in the event, what to rename them to, and whether to preserve the original fields.
destination [_required_]
string
The field name to assign the renamed value to.
preserve_source [_required_]
boolean
Indicates whether the original field, that is received from the source, should be kept (`true`) or removed (`false`) after renaming.
source [_required_]
string
The original field name in the log event that should be renamed.
id [_required_]
string
A unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `rename_fields`. Allowed enum values: `rename_fields`
default: `rename_fields`
object
The `generate_datadog_metrics` processor creates custom metrics from logs and sends them to Datadog. Metrics can be counters, gauges, or distributions and optionally grouped by log fields.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline.
include
string
A Datadog search query used to determine which logs this processor targets.
[object]
Configuration for generating individual metrics.
group_by
[string]
Optional fields used to group the metric series.
include [_required_]
string
Datadog filter query to match logs for metric generation.
metric_type [_required_]
enum
Type of metric to create. Allowed enum values: `count,gauge,distribution`
name [_required_]
string
Name of the custom metric to be created.
_required_]
<oneOf>
Specifies how the value of the generated metric is computed.
object
Strategy that increments a generated metric by one for each matching event.
strategy [_required_]
enum
Increments the metric by 1 for each matching event. Allowed enum values: `increment_by_one`
object
Strategy that increments a generated metric based on the value of a log field.
field [_required_]
string
Name of the log field containing the numeric value to increment the metric by.
strategy [_required_]
enum
Uses a numeric field in the log event as the metric increment. Allowed enum values: `increment_by_field`
type [_required_]
enum
The processor type. Always `generate_datadog_metrics`. Allowed enum values: `generate_datadog_metrics`
default: `generate_datadog_metrics`
object
The `sample` processor allows probabilistic sampling of logs at a fixed rate.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
percentage
double
The percentage of logs to sample.
rate
int64
Number of events to sample (1 in N).
type [_required_]
enum
The processor type. The value should always be `sample`. Allowed enum values: `sample`
default: `sample`
object
The `parse_grok` processor extracts structured fields from unstructured log messages using Grok patterns.
disable_library_rules
boolean
If set to `true`, disables the default Grok rules provided by Datadog.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
A unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
[object]
The list of Grok parsing rules. If multiple matching rules are provided, they are evaluated in order. The first successful match is applied.
_required_]
[object]
A list of Grok parsing rules that define how to extract fields from the source field. Each rule must contain a name and a valid Grok pattern.
name [_required_]
string
The name of the rule.
rule [_required_]
string
The definition of the Grok rule.
source [_required_]
string
The name of the field in the log event to apply the Grok rules to.
[object]
A list of Grok helper rules that can be referenced by the parsing rules.
name [_required_]
string
The name of the Grok helper rule.
rule [_required_]
string
The definition of the Grok helper rule.
type [_required_]
enum
The processor type. The value should always be `parse_grok`. Allowed enum values: `parse_grok`
default: `parse_grok`
object
The `sensitive_data_scanner` processor detects and optionally redacts sensitive data in log events.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
[object]
A list of rules for identifying and acting on sensitive data patterns.
object
Configuration for keywords used to reinforce sensitive data pattern detection.
keywords [_required_]
[string]
A list of keywords to match near the sensitive pattern.
proximity [_required_]
int64
Maximum number of tokens between a keyword and a sensitive value match.
name [_required_]
string
A name identifying the rule.
_required_]
<oneOf>
Defines what action to take when sensitive data is matched.
object
Configuration for completely redacting matched sensitive data.
action [_required_]
enum
Action type that completely replaces the matched sensitive data with a fixed replacement string to remove all visibility. Allowed enum values: `redact`
_required_]
object
Configuration for fully redacting sensitive data.
replace [_required_]
string
The `ObservabilityPipelineSensitiveDataScannerProcessorActionRedactOptions` `replace`.
object
Configuration for hashing matched sensitive values.
action [_required_]
enum
Action type that replaces the matched sensitive data with a hashed representation, preserving structure while securing content. Allowed enum values: `hash`
options
object
The `ObservabilityPipelineSensitiveDataScannerProcessorActionHash` `options`.
object
Configuration for partially redacting matched sensitive data.
action [_required_]
enum
Action type that redacts part of the sensitive data while preserving a configurable number of characters, typically used for masking purposes (e.g., show last 4 digits of a credit card). Allowed enum values: `partial_redact`
_required_]
object
Controls how partial redaction is applied, including character count and direction.
characters [_required_]
int64
The `ObservabilityPipelineSensitiveDataScannerProcessorActionPartialRedactOptions` `characters`.
direction [_required_]
enum
Indicates whether to redact characters from the first or last part of the matched value. Allowed enum values: `first,last`
_required_]
<oneOf>
Pattern detection configuration for identifying sensitive data using either a custom regex or a library reference.
object
Defines a custom regex-based pattern for identifying sensitive data in logs.
_required_]
object
Options for defining a custom regex pattern.
rule [_required_]
string
A regular expression used to detect sensitive values. Must be a valid regex.
type [_required_]
enum
Indicates a custom regular expression is used for matching. Allowed enum values: `custom`
object
Specifies a pattern from Datadog’s sensitive data detection library to match known sensitive data types.
_required_]
object
Options for selecting a predefined library pattern and enabling keyword support.
id [_required_]
string
Identifier for a predefined pattern from the sensitive data scanner pattern library.
use_recommended_keywords
boolean
Whether to augment the pattern with recommended keywords (optional).
type [_required_]
enum
Indicates that a predefined library pattern is used. Allowed enum values: `library`
_required_]
<oneOf>
Determines which parts of the log the pattern-matching rule should be applied to.
object
Includes only specific fields for sensitive data scanning.
_required_]
object
Fields to which the scope rule applies.
fields [_required_]
[string]
The `ObservabilityPipelineSensitiveDataScannerProcessorScopeOptions` `fields`.
target [_required_]
enum
Applies the rule only to included fields. Allowed enum values: `include`
object
Excludes specific fields from sensitive data scanning.
_required_]
object
Fields to which the scope rule applies.
fields [_required_]
[string]
The `ObservabilityPipelineSensitiveDataScannerProcessorScopeOptions` `fields`.
target [_required_]
enum
Excludes specific fields from processing. Allowed enum values: `exclude`
object
Applies scanning across all available fields.
target [_required_]
enum
Applies the rule to all fields. Allowed enum values: `all`
tags [_required_]
[string]
Tags assigned to this rule for filtering and classification.
type [_required_]
enum
The processor type. The value should always be `sensitive_data_scanner`. Allowed enum values: `sensitive_data_scanner`
default: `sensitive_data_scanner`
object
The `ocsf_mapper` processor transforms logs into the OCSF schema using a predefined mapping configuration.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
[object]
A list of mapping rules to convert events to the OCSF format.
include [_required_]
string
A Datadog search query used to select the logs that this mapping should apply to.
_required_]
<oneOf>
Defines a single mapping rule for transforming logs into the OCSF schema.
Option 1
enum
Predefined library mappings for common log formats. Allowed enum values: `CloudTrail Account Change,GCP Cloud Audit CreateBucket,GCP Cloud Audit CreateSink,GCP Cloud Audit SetIamPolicy,GCP Cloud Audit UpdateSink,Github Audit Log API Activity,Google Workspace Admin Audit addPrivilege,Microsoft 365 Defender Incident,Microsoft 365 Defender UserLoggedIn,Okta System Log Authentication,Palo Alto Networks Firewall Traffic`
type [_required_]
enum
The processor type. The value should always be `ocsf_mapper`. Allowed enum values: `ocsf_mapper`
default: `ocsf_mapper`
object
The `add_env_vars` processor adds environment variable values to log events.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this processor in the pipeline.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `add_env_vars`. Allowed enum values: `add_env_vars`
default: `add_env_vars`
_required_]
[object]
A list of environment variable mappings to apply to log fields.
field [_required_]
string
The target field in the log event.
name [_required_]
string
The name of the environment variable to read.
object
The `dedupe` processor removes duplicate fields in log events.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
fields [_required_]
[string]
A list of log field paths to check for duplicates.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
mode [_required_]
enum
The deduplication mode to apply to the fields. Allowed enum values: `match,ignore`
type [_required_]
enum
The processor type. The value should always be `dedupe`. Allowed enum values: `dedupe`
default: `dedupe`
object
The `enrichment_table` processor enriches logs using a static CSV file or GeoIP database.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
object
Defines a static enrichment table loaded from a CSV file.
_required_]
object
File encoding format.
delimiter [_required_]
string
The `encoding` `delimiter`.
includes_headers [_required_]
boolean
The `encoding` `includes_headers`.
type [_required_]
enum
Specifies the encoding format (e.g., CSV) used for enrichment tables. Allowed enum values: `csv`
_required_]
[object]
Key fields used to look up enrichment values.
column [_required_]
string
The `items` `column`.
comparison [_required_]
enum
Defines how to compare key fields for enrichment table lookups. Allowed enum values: `equals`
field [_required_]
string
The `items` `field`.
path [_required_]
string
Path to the CSV file.
_required_]
[object]
Schema defining column names and their types.
column [_required_]
string
The `items` `column`.
type [_required_]
enum
Declares allowed data types for enrichment table columns. Allowed enum values: `string,boolean,integer,float,date,timestamp`
object
Uses a GeoIP database to enrich logs based on an IP field.
key_field [_required_]
string
Path to the IP field in the log.
locale [_required_]
string
Locale used to resolve geographical names.
path [_required_]
string
Path to the GeoIP database file.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
target [_required_]
string
Path where enrichment results should be stored in the log.
type [_required_]
enum
The processor type. The value should always be `enrichment_table`. Allowed enum values: `enrichment_table`
default: `enrichment_table`
object
The `reduce` processor aggregates and merges logs based on matching keys and merge strategies.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
group_by [_required_]
[string]
A list of fields used to group log events for merging.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
[object]
List of merge strategies defining how values from grouped events should be combined.
path [_required_]
string
The field path in the log event.
strategy [_required_]
enum
The merge strategy to apply. Allowed enum values: `discard,retain,sum,max,min,array,concat,concat_newline,concat_raw,shortest_array,longest_array,flat_unique`
type [_required_]
enum
The processor type. The value should always be `reduce`. Allowed enum values: `reduce`
default: `reduce`
object
The `throttle` processor limits the number of events that pass through over a given time window.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
group_by
[string]
Optional list of fields used to group events before the threshold has been reached.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
threshold [_required_]
int64
the number of events allowed in a given time window. Events sent after the threshold has been reached, are dropped.
type [_required_]
enum
The processor type. The value should always be `throttle`. Allowed enum values: `throttle`
default: `throttle`
window [_required_]
double
The time window in seconds over which the threshold applies.
object
The `custom_processor` processor transforms events using [Vector Remap Language (VRL)](https://vector.dev/docs/reference/vrl/) scripts with advanced filtering capabilities.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets. This field should always be set to `*` for the custom_processor processor.
default: `*`
_required_]
[object]
Array of VRL remap rules.
drop_on_error [_required_]
boolean
Whether to drop events that caused errors during processing.
enabled
boolean
Whether this remap rule is enabled.
include [_required_]
string
A Datadog search query used to filter events for this specific remap rule.
name [_required_]
string
A descriptive name for this remap rule.
source [_required_]
string
The VRL script source code that defines the processing logic.
type [_required_]
enum
The processor type. The value should always be `custom_processor`. Allowed enum values: `custom_processor`
default: `custom_processor`
object
The `datadog_tags` processor includes or excludes specific Datadog tags in your logs.
action [_required_]
enum
The action to take on tags with matching keys. Allowed enum values: `include,exclude`
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
keys [_required_]
[string]
A list of tag keys.
mode [_required_]
enum
The processing mode. Allowed enum values: `filter`
type [_required_]
enum
The processor type. The value should always be `datadog_tags`. Allowed enum values: `datadog_tags`
default: `datadog_tags`
_required_]
[ <oneOf>]
A list of configured data sources for the pipeline.
object
The `kafka` source ingests data from Apache Kafka topics.
group_id [_required_]
string
Consumer group ID used by the Kafka client.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
[object]
Optional list of advanced Kafka client configuration options, defined as key-value pairs.
name [_required_]
string
The name of the `librdkafka` configuration option to set.
value [_required_]
string
The value assigned to the specified `librdkafka` configuration option.
object
Specifies the SASL mechanism for authenticating with a Kafka cluster.
mechanism
enum
SASL mechanism used for Kafka authentication. Allowed enum values: `PLAIN,SCRAM-SHA-256,SCRAM-SHA-512`
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
topics [_required_]
[string]
A list of Kafka topic names to subscribe to. The source ingests messages from each topic specified.
type [_required_]
enum
The source type. The value should always be `kafka`. Allowed enum values: `kafka`
default: `kafka`
object
The `datadog_agent` source collects logs from the Datadog Agent.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `datadog_agent`. Allowed enum values: `datadog_agent`
default: `datadog_agent`
object
The `splunk_tcp` source receives logs from a Splunk Universal Forwarder over TCP. TLS is supported for secure transmission.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. Always `splunk_tcp`. Allowed enum values: `splunk_tcp`
default: `splunk_tcp`
object
The `splunk_hec` source implements the Splunk HTTP Event Collector (HEC) API.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. Always `splunk_hec`. Allowed enum values: `splunk_hec`
default: `splunk_hec`
object
The `amazon_s3` source ingests logs from an Amazon S3 bucket. It supports AWS authentication and TLS encryption.
object
AWS authentication credentials used for accessing AWS services such as S3. If omitted, the system’s default credentials are used (for example, the IAM role and environment variables).
assume_role
string
The Amazon Resource Name (ARN) of the role to assume.
external_id
string
A unique identifier for cross-account role assumption.
session_name
string
A session identifier used for logging and tracing the assumed role session.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
region [_required_]
string
AWS region where the S3 bucket resides.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. Always `amazon_s3`. Allowed enum values: `amazon_s3`
default: `amazon_s3`
object
The `fluentd` source ingests logs from a Fluentd-compatible service.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `fluentd. Allowed enum values: `fluentd`
default: `fluentd`
object
The `fluent_bit` source ingests logs from Fluent Bit.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `fluent_bit`. Allowed enum values: `fluent_bit`
default: `fluent_bit`
object
The `http_server` source collects logs over HTTP POST from external services.
auth_strategy [_required_]
enum
HTTP authentication method. Allowed enum values: `none,plain`
decoding [_required_]
enum
The decoding format used to interpret incoming logs. Allowed enum values: `bytes,gelf,json,syslog`
id [_required_]
string
Unique ID for the HTTP server source.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `http_server`. Allowed enum values: `http_server`
default: `http_server`
object
The `sumo_logic` source receives logs from Sumo Logic collectors.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
type [_required_]
enum
The source type. The value should always be `sumo_logic`. Allowed enum values: `sumo_logic`
default: `sumo_logic`
object
The `rsyslog` source listens for logs over TCP or UDP from an `rsyslog` server using the syslog protocol.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
mode [_required_]
enum
Protocol used by the syslog source to receive messages. Allowed enum values: `tcp,udp`
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `rsyslog`. Allowed enum values: `rsyslog`
default: `rsyslog`
object
The `syslog_ng` source listens for logs over TCP or UDP from a `syslog-ng` server using the syslog protocol.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
mode [_required_]
enum
Protocol used by the syslog source to receive messages. Allowed enum values: `tcp,udp`
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `syslog_ng`. Allowed enum values: `syslog_ng`
default: `syslog_ng`
object
The `amazon_data_firehose` source ingests logs from AWS Data Firehose.
object
AWS authentication credentials used for accessing AWS services such as S3. If omitted, the system’s default credentials are used (for example, the IAM role and environment variables).
assume_role
string
The Amazon Resource Name (ARN) of the role to assume.
external_id
string
A unique identifier for cross-account role assumption.
session_name
string
A session identifier used for logging and tracing the assumed role session.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `amazon_data_firehose`. Allowed enum values: `amazon_data_firehose`
default: `amazon_data_firehose`
object
The `google_pubsub` source ingests logs from a Google Cloud Pub/Sub subscription.
object
GCP credentials used to authenticate with Google Cloud Storage.
credentials_file [_required_]
string
Path to the GCP service account key file.
decoding [_required_]
enum
The decoding format used to interpret incoming logs. Allowed enum values: `bytes,gelf,json,syslog`
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
project [_required_]
string
The GCP project ID that owns the Pub/Sub subscription.
subscription [_required_]
string
The Pub/Sub subscription name from which messages are consumed.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `google_pubsub`. Allowed enum values: `google_pubsub`
default: `google_pubsub`
object
The `http_client` source scrapes logs from HTTP endpoints at regular intervals.
auth_strategy
enum
Optional authentication strategy for HTTP requests. Allowed enum values: `basic,bearer`
decoding [_required_]
enum
The decoding format used to interpret incoming logs. Allowed enum values: `bytes,gelf,json,syslog`
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
scrape_interval_secs
int64
The interval (in seconds) between HTTP scrape requests.
scrape_timeout_secs
int64
The timeout (in seconds) for each scrape request.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `http_client`. Allowed enum values: `http_client`
default: `http_client`
object
The `logstash` source ingests logs from a Logstash forwarder.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `logstash`. Allowed enum values: `logstash`
default: `logstash`
object
The `socket` source ingests logs over TCP or UDP.
_required_]
<oneOf>
Framing method configuration for the socket source.
object
Byte frames which are delimited by a newline character.
method [_required_]
enum
Byte frames which are delimited by a newline character. Allowed enum values: `newline_delimited`
object
Byte frames are passed through as-is according to the underlying I/O boundaries (for example, split between messages or stream segments).
method [_required_]
enum
Byte frames are passed through as-is according to the underlying I/O boundaries (for example, split between messages or stream segments). Allowed enum values: `bytes`
object
Byte frames which are delimited by a chosen character.
delimiter [_required_]
string
A single ASCII character used to delimit events.
method [_required_]
enum
Byte frames which are delimited by a chosen character. Allowed enum values: `character_delimited`
object
Byte frames according to the octet counting format as per RFC6587.
method [_required_]
enum
Byte frames according to the octet counting format as per RFC6587. Allowed enum values: `octet_counting`
object
Byte frames which are chunked GELF messages.
method [_required_]
enum
Byte frames which are chunked GELF messages. Allowed enum values: `chunked_gelf`
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
mode [_required_]
enum
Protocol used to receive logs. Allowed enum values: `tcp,udp`
object
TLS configuration. Relevant only when `mode` is `tcp`.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `socket`. Allowed enum values: `socket`
default: `socket`
name [_required_]
string
Name of the pipeline.
id [_required_]
string
Unique identifier for the pipeline.
type [_required_]
string
The resource type identifier. For pipeline resources, this should always be set to `pipelines`.
default: `pipelines`
```
{
  "data": {
    "attributes": {
      "config": {
        "destinations": [
          {
            "id": "datadog-logs-destination",
            "inputs": [
              "filter-processor"
            ],
            "type": "datadog_logs"
          }
        ],
        "processors": [
          {
            "display_name": "my component",
            "enabled": true,
            "id": "grouped-processors",
            "include": "service:my-service",
            "inputs": [
              "datadog-agent-source"
            ],
            "processors": [
              []
            ]
          }
        ],
        "sources": [
          {
            "group_id": "consumer-group-0",
            "id": "kafka-source",
            "librdkafka_options": [
              {
                "name": "fetch.message.max.bytes",
                "value": "1048576"
              }
            ],
            "sasl": {
              "mechanism": "string"
            },
            "tls": {
              "ca_file": "string",
              "crt_file": "/path/to/cert.crt",
              "key_file": "string"
            },
            "topics": [
              "topic1",
              "topic2"
            ],
            "type": "kafka"
          }
        ]
      },
      "name": "Main Observability Pipeline"
    },
    "id": "3fa85f64-5717-4562-b3fc-2c963f66afa6",
    "type": "pipelines"
  }
}
```

Copy
Forbidden
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


API error response.
Expand All
Field
Type
Description
errors [_required_]
[string]
A list of errors.
```
{
  "errors": [
    "Bad Request"
  ]
}
```

Copy
Too many requests
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


API error response.
Expand All
Field
Type
Description
errors [_required_]
[string]
A list of errors.
```
{
  "errors": [
    "Bad Request"
  ]
}
```

Copy
### Code Example
  * [Curl](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=curl)
  * [Python](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=python)
  * [Ruby](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=ruby)
  * [Go](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=go)
  * [Java](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=java)
  * [Rust](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=rust)
  * [Typescript](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=typescript)


#####  Get a specific pipeline
Copy
```
                  # Path parameters  
export pipeline_id="CHANGE_ME"  
# Curl command  
curl -X GET "https://api.ap1.datadoghq.com"https://api.ap2.datadoghq.com"https://api.datadoghq.eu"https://api.ddog-gov.com"https://api.datadoghq.com"https://api.us3.datadoghq.com"https://api.us5.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelines/${pipeline_id}" \
-H "Accept: application/json" \
-H "DD-API-KEY: ${DD_API_KEY}" \
-H "DD-APPLICATION-KEY: ${DD_APP_KEY}"  

                
```

#####  Get a specific pipeline
```
"""
Get a specific pipeline returns "OK" response
"""

from os import environ
from datadog_api_client import ApiClient, Configuration
from datadog_api_client.v2.api.observability_pipelines_api import ObservabilityPipelinesApi

# there is a valid "pipeline" in the system
PIPELINE_DATA_ID = environ["PIPELINE_DATA_ID"]

configuration = Configuration()
configuration.unstable_operations["get_pipeline"] = True
with ApiClient(configuration) as api_client:
    api_instance = ObservabilityPipelinesApi(api_client)
    response = api_instance.get_pipeline(
        pipeline_id=PIPELINE_DATA_ID,
    )

    print(response)

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=python) and then save the example to `example.py` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" python3 "example.py"


```

#####  Get a specific pipeline
```
# Get a specific pipeline returns "OK" response

require "datadog_api_client"
DatadogAPIClient.configure do |config|
  config.unstable_operations["v2.get_pipeline".to_sym] = true
end
api_instance = DatadogAPIClient::V2::ObservabilityPipelinesAPI.new

# there is a valid "pipeline" in the system
PIPELINE_DATA_ID = ENV["PIPELINE_DATA_ID"]
p api_instance.get_pipeline(PIPELINE_DATA_ID)

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=ruby) and then save the example to `example.rb` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" rb "example.rb"


```

#####  Get a specific pipeline
```
// Get a specific pipeline returns "OK" response

package main

import (
	"context"
	"encoding/json"
	"fmt"
	"os"

	"github.com/DataDog/datadog-api-client-go/v2/api/datadog"
	"github.com/DataDog/datadog-api-client-go/v2/api/datadogV2"
)

func main() {
	// there is a valid "pipeline" in the system
	PipelineDataID := os.Getenv("PIPELINE_DATA_ID")

	ctx := datadog.NewDefaultContext(context.Background())
	configuration := datadog.NewConfiguration()
	configuration.SetUnstableOperationEnabled("v2.GetPipeline", true)
	apiClient := datadog.NewAPIClient(configuration)
	api := datadogV2.NewObservabilityPipelinesApi(apiClient)
	resp, r, err := api.GetPipeline(ctx, PipelineDataID)

	if err != nil {
		fmt.Fprintf(os.Stderr, "Error when calling `ObservabilityPipelinesApi.GetPipeline`: %v\n", err)
		fmt.Fprintf(os.Stderr, "Full HTTP response: %v\n", r)
	}

	responseContent, _ := json.MarshalIndent(resp, "", "  ")
	fmt.Fprintf(os.Stdout, "Response from `ObservabilityPipelinesApi.GetPipeline`:\n%s\n", responseContent)
}

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=go) and then save the example to `main.go` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" go run "main.go"


```

#####  Get a specific pipeline
```
// Get a specific pipeline returns "OK" response

import com.datadog.api.client.ApiClient;
import com.datadog.api.client.ApiException;
import com.datadog.api.client.v2.api.ObservabilityPipelinesApi;
import com.datadog.api.client.v2.model.ObservabilityPipeline;

public class Example {
  public static void main(String[] args) {
    ApiClient defaultClient = ApiClient.getDefaultApiClient();
    defaultClient.setUnstableOperationEnabled("v2.getPipeline", true);
    ObservabilityPipelinesApi apiInstance = new ObservabilityPipelinesApi(defaultClient);

    // there is a valid "pipeline" in the system
    String PIPELINE_DATA_ID = System.getenv("PIPELINE_DATA_ID");

    try {
      ObservabilityPipeline result = apiInstance.getPipeline(PIPELINE_DATA_ID);
      System.out.println(result);
    } catch (ApiException e) {
      System.err.println("Exception when calling ObservabilityPipelinesApi#getPipeline");
      System.err.println("Status code: " + e.getCode());
      System.err.println("Reason: " + e.getResponseBody());
      System.err.println("Response headers: " + e.getResponseHeaders());
      e.printStackTrace();
    }
  }
}

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=java) and then save the example to `Example.java` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" java "Example.java"


```

#####  Get a specific pipeline
```
// Get a specific pipeline returns "OK" response
use datadog_api_client::datadog;
use datadog_api_client::datadogV2::api_observability_pipelines::ObservabilityPipelinesAPI;

#[tokio::main]
async fn main() {
    // there is a valid "pipeline" in the system
    let pipeline_data_id = std::env::var("PIPELINE_DATA_ID").unwrap();
    let mut configuration = datadog::Configuration::new();
    configuration.set_unstable_operation_enabled("v2.GetPipeline", true);
    let api = ObservabilityPipelinesAPI::with_config(configuration);
    let resp = api.get_pipeline(pipeline_data_id.clone()).await;
    if let Ok(value) = resp {
        println!("{:#?}", value);
    } else {
        println!("{:#?}", resp.unwrap_err());
    }
}

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=rust) and then save the example to `src/main.rs` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" cargo run


```

#####  Get a specific pipeline
```
/**
 * Get a specific pipeline returns "OK" response
 */

import { client, v2 } from "@datadog/datadog-api-client";

const configuration = client.createConfiguration();
configuration.unstableOperations["v2.getPipeline"] = true;
const apiInstance = new v2.ObservabilityPipelinesApi(configuration);

// there is a valid "pipeline" in the system
const PIPELINE_DATA_ID = process.env.PIPELINE_DATA_ID as string;

const params: v2.ObservabilityPipelinesApiGetPipelineRequest = {
  pipelineId: PIPELINE_DATA_ID,
};

apiInstance
  .getPipeline(params)
  .then((data: v2.ObservabilityPipeline) => {
    console.log(
      "API called successfully. Returned data: " + JSON.stringify(data)
    );
  })
  .catch((error: any) => console.error(error));

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=typescript) and then save the example to `example.ts` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" tsc "example.ts"


```

* * *
## [Update a pipeline](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-pipeline)
  * [v2 (latest)](https://docs.datadoghq.com/api/latest/observability-pipelines/#update-a-pipeline-v2)


**Note** : This endpoint is in Preview. Fill out this [form](https://www.datadoghq.com/product-preview/observability-pipelines-api-and-terraform-support/) to request access.
PUT https://api.ap1.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelines/{pipeline_id}https://api.ap2.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelines/{pipeline_id}https://api.datadoghq.eu/api/v2/remote_config/products/obs_pipelines/pipelines/{pipeline_id}https://api.ddog-gov.com/api/v2/remote_config/products/obs_pipelines/pipelines/{pipeline_id}https://api.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelines/{pipeline_id}https://api.us3.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelines/{pipeline_id}https://api.us5.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelines/{pipeline_id}
### Overview
Update a pipeline. This endpoint requires the `observability_pipelines_deploy` permission.
### Arguments
#### Path Parameters
Name
Type
Description
pipeline_id [_required_]
string
The ID of the pipeline to update.
### Request
#### Body Data (required)
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


Expand All
Field
Type
Description
_required_]
object
Contains the pipeline’s ID, type, and configuration attributes.
_required_]
object
Defines the pipeline’s name and its components (sources, processors, and destinations).
_required_]
object
Specifies the pipeline's configuration, including its sources, processors, and destinations.
_required_]
[ <oneOf>]
A list of destination components where processed logs are sent.
object
The `datadog_logs` destination forwards logs to Datadog Log Management.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `datadog_logs`. Allowed enum values: `datadog_logs`
default: `datadog_logs`
object
The `amazon_s3` destination sends your logs in Datadog-rehydratable format to an Amazon S3 bucket for archiving.
object
AWS authentication credentials used for accessing AWS services such as S3. If omitted, the system’s default credentials are used (for example, the IAM role and environment variables).
assume_role
string
The Amazon Resource Name (ARN) of the role to assume.
external_id
string
A unique identifier for cross-account role assumption.
session_name
string
A session identifier used for logging and tracing the assumed role session.
bucket [_required_]
string
S3 bucket name.
id [_required_]
string
Unique identifier for the destination component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
key_prefix
string
Optional prefix for object keys.
region [_required_]
string
AWS region of the S3 bucket.
storage_class [_required_]
enum
S3 storage class. Allowed enum values: `STANDARD,REDUCED_REDUNDANCY,INTELLIGENT_TIERING,STANDARD_IA,EXPRESS_ONEZONE,ONEZONE_IA,GLACIER,GLACIER_IR,DEEP_ARCHIVE`
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. Always `amazon_s3`. Allowed enum values: `amazon_s3`
default: `amazon_s3`
object
The `google_cloud_storage` destination stores logs in a Google Cloud Storage (GCS) bucket. It requires a bucket name, GCP authentication, and metadata fields.
acl
enum
Access control list setting for objects written to the bucket. Allowed enum values: `private,project-private,public-read,authenticated-read,bucket-owner-read,bucket-owner-full-control`
object
GCP credentials used to authenticate with Google Cloud Storage.
credentials_file [_required_]
string
Path to the GCP service account key file.
bucket [_required_]
string
Name of the GCS bucket.
id [_required_]
string
Unique identifier for the destination component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
key_prefix
string
Optional prefix for object keys within the GCS bucket.
[object]
Custom metadata to attach to each object uploaded to the GCS bucket.
name [_required_]
string
The metadata key.
value [_required_]
string
The metadata value.
storage_class [_required_]
enum
Storage class used for objects stored in GCS. Allowed enum values: `STANDARD,NEARLINE,COLDLINE,ARCHIVE`
type [_required_]
enum
The destination type. Always `google_cloud_storage`. Allowed enum values: `google_cloud_storage`
default: `google_cloud_storage`
object
The `splunk_hec` destination forwards logs to Splunk using the HTTP Event Collector (HEC).
auto_extract_timestamp
boolean
If `true`, Splunk tries to extract timestamps from incoming log events. If `false`, Splunk assigns the time the event was received.
encoding
enum
Encoding format for log events. Allowed enum values: `json,raw_message`
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
index
string
Optional name of the Splunk index where logs are written.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
sourcetype
string
The Splunk sourcetype to assign to log events.
type [_required_]
enum
The destination type. Always `splunk_hec`. Allowed enum values: `splunk_hec`
default: `splunk_hec`
object
The `sumo_logic` destination forwards logs to Sumo Logic.
encoding
enum
The output encoding format. Allowed enum values: `json,raw_message,logfmt`
[object]
A list of custom headers to include in the request to Sumo Logic.
name [_required_]
string
The header field name.
value [_required_]
string
The header field value.
header_host_name
string
Optional override for the host name header.
header_source_category
string
Optional override for the source category header.
header_source_name
string
Optional override for the source name header.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `sumo_logic`. Allowed enum values: `sumo_logic`
default: `sumo_logic`
object
The `elasticsearch` destination writes logs to an Elasticsearch cluster.
api_version
enum
The Elasticsearch API version to use. Set to `auto` to auto-detect. Allowed enum values: `auto,v6,v7,v8`
bulk_index
string
The index to write logs to in Elasticsearch.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `elasticsearch`. Allowed enum values: `elasticsearch`
default: `elasticsearch`
object
The `rsyslog` destination forwards logs to an external `rsyslog` server over TCP or UDP using the syslog protocol.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
keepalive
int64
Optional socket keepalive duration in milliseconds.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. The value should always be `rsyslog`. Allowed enum values: `rsyslog`
default: `rsyslog`
object
The `syslog_ng` destination forwards logs to an external `syslog-ng` server over TCP or UDP using the syslog protocol.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
keepalive
int64
Optional socket keepalive duration in milliseconds.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. The value should always be `syslog_ng`. Allowed enum values: `syslog_ng`
default: `syslog_ng`
object
The `azure_storage` destination forwards logs to an Azure Blob Storage container.
blob_prefix
string
Optional prefix for blobs written to the container.
container_name [_required_]
string
The name of the Azure Blob Storage container to store logs in.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `azure_storage`. Allowed enum values: `azure_storage`
default: `azure_storage`
object
The `microsoft_sentinel` destination forwards logs to Microsoft Sentinel.
client_id [_required_]
string
Azure AD client ID used for authentication.
dcr_immutable_id [_required_]
string
The immutable ID of the Data Collection Rule (DCR).
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
table [_required_]
string
The name of the Log Analytics table where logs are sent.
tenant_id [_required_]
string
Azure AD tenant ID.
type [_required_]
enum
The destination type. The value should always be `microsoft_sentinel`. Allowed enum values: `microsoft_sentinel`
default: `microsoft_sentinel`
object
The `google_chronicle` destination sends logs to Google Chronicle.
object
GCP credentials used to authenticate with Google Cloud Storage.
credentials_file [_required_]
string
Path to the GCP service account key file.
customer_id [_required_]
string
The Google Chronicle customer ID.
encoding
enum
The encoding format for the logs sent to Chronicle. Allowed enum values: `json,raw_message`
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
log_type
string
The log type metadata associated with the Chronicle destination.
type [_required_]
enum
The destination type. The value should always be `google_chronicle`. Allowed enum values: `google_chronicle`
default: `google_chronicle`
object
The `new_relic` destination sends logs to the New Relic platform.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
region [_required_]
enum
The New Relic region. Allowed enum values: `us,eu`
type [_required_]
enum
The destination type. The value should always be `new_relic`. Allowed enum values: `new_relic`
default: `new_relic`
object
The `sentinel_one` destination sends logs to SentinelOne.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
region [_required_]
enum
The SentinelOne region to send logs to. Allowed enum values: `us,eu,ca,data_set_us`
type [_required_]
enum
The destination type. The value should always be `sentinel_one`. Allowed enum values: `sentinel_one`
default: `sentinel_one`
object
The `opensearch` destination writes logs to an OpenSearch cluster.
bulk_index
string
The index to write logs to.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `opensearch`. Allowed enum values: `opensearch`
default: `opensearch`
object
The `amazon_opensearch` destination writes logs to Amazon OpenSearch.
_required_]
object
Authentication settings for the Amazon OpenSearch destination. The `strategy` field determines whether basic or AWS-based authentication is used.
assume_role
string
The ARN of the role to assume (used with `aws` strategy).
aws_region
string
AWS region
external_id
string
External ID for the assumed role (used with `aws` strategy).
session_name
string
Session name for the assumed role (used with `aws` strategy).
strategy [_required_]
enum
The authentication strategy to use. Allowed enum values: `basic,aws`
bulk_index
string
The index to write logs to.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `amazon_opensearch`. Allowed enum values: `amazon_opensearch`
default: `amazon_opensearch`
object
The `socket` destination sends logs over TCP or UDP to a remote server.
encoding [_required_]
enum
Encoding format for log events. Allowed enum values: `json,raw_message`
_required_]
<oneOf>
Framing method configuration.
object
Each log event is delimited by a newline character.
method [_required_]
enum
The definition of `ObservabilityPipelineSocketDestinationFramingNewlineDelimitedMethod` object. Allowed enum values: `newline_delimited`
object
Event data is not delimited at all.
method [_required_]
enum
The definition of `ObservabilityPipelineSocketDestinationFramingBytesMethod` object. Allowed enum values: `bytes`
object
Each log event is separated using the specified delimiter character.
delimiter [_required_]
string
A single ASCII character used as a delimiter.
method [_required_]
enum
The definition of `ObservabilityPipelineSocketDestinationFramingCharacterDelimitedMethod` object. Allowed enum values: `character_delimited`
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
mode [_required_]
enum
Protocol used to send logs. Allowed enum values: `tcp,udp`
object
TLS configuration. Relevant only when `mode` is `tcp`.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. The value should always be `socket`. Allowed enum values: `socket`
default: `socket`
object
The `amazon_security_lake` destination sends your logs to Amazon Security Lake.
object
AWS authentication credentials used for accessing AWS services such as S3. If omitted, the system’s default credentials are used (for example, the IAM role and environment variables).
assume_role
string
The Amazon Resource Name (ARN) of the role to assume.
external_id
string
A unique identifier for cross-account role assumption.
session_name
string
A session identifier used for logging and tracing the assumed role session.
bucket [_required_]
string
Name of the Amazon S3 bucket in Security Lake (3-63 characters).
custom_source_name [_required_]
string
Custom source name for the logs in Security Lake.
id [_required_]
string
Unique identifier for the destination component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
region [_required_]
string
AWS region of the S3 bucket.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. Always `amazon_security_lake`. Allowed enum values: `amazon_security_lake`
default: `amazon_security_lake`
object
The `crowdstrike_next_gen_siem` destination forwards logs to CrowdStrike Next Gen SIEM.
object
Compression configuration for log events.
algorithm [_required_]
enum
Compression algorithm for log events. Allowed enum values: `gzip,zlib`
level
int64
Compression level.
encoding [_required_]
enum
Encoding format for log events. Allowed enum values: `json,raw_message`
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. The value should always be `crowdstrike_next_gen_siem`. Allowed enum values: `crowdstrike_next_gen_siem`
default: `crowdstrike_next_gen_siem`
object
The `google_pubsub` destination publishes logs to a Google Cloud Pub/Sub topic.
object
GCP credentials used to authenticate with Google Cloud Storage.
credentials_file [_required_]
string
Path to the GCP service account key file.
encoding [_required_]
enum
Encoding format for log events. Allowed enum values: `json,raw_message`
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
project [_required_]
string
The GCP project ID that owns the Pub/Sub topic.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
topic [_required_]
string
The Pub/Sub topic name to publish logs to.
type [_required_]
enum
The destination type. The value should always be `google_pubsub`. Allowed enum values: `google_pubsub`
default: `google_pubsub`
[object]
A list of processor groups that transform or enrich log data.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor group is enabled.
id [_required_]
string
The unique identifier for the processor group.
include [_required_]
string
Conditional expression for when this processor group should execute.
inputs [_required_]
[string]
A list of IDs for components whose output is used as the input for this processor group.
_required_]
[ <oneOf>]
Processors applied sequentially within this group. Events flow through each processor in order.
object
The `filter` processor allows conditional processing of logs based on a Datadog search query. Logs that match the `include` query are passed through; others are discarded.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs should pass through the filter. Logs that match this query continue to downstream components; others are dropped.
type [_required_]
enum
The processor type. The value should always be `filter`. Allowed enum values: `filter`
default: `filter`
object
The `parse_json` processor extracts JSON from a specified field and flattens it into the event. This is useful when logs contain embedded JSON as a string.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
field [_required_]
string
The name of the log field that contains a JSON string.
id [_required_]
string
A unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `parse_json`. Allowed enum values: `parse_json`
default: `parse_json`
object
The Quota Processor measures logging traffic for logs that match a specified filter. When the configured daily quota is met, the processor can drop or alert.
display_name
string
The display name for a component.
drop_events
boolean
If set to `true`, logs that matched the quota filter and sent after the quota has been met are dropped; only logs that did not match the filter query continue through the pipeline.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
ignore_when_missing_partitions
boolean
If `true`, the processor skips quota checks when partition fields are missing from the logs.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
object
The maximum amount of data or number of events allowed before the quota is enforced. Can be specified in bytes or events.
enforce [_required_]
enum
Unit for quota enforcement in bytes for data size or events for count. Allowed enum values: `bytes,events`
limit [_required_]
int64
The limit for quota enforcement.
name [_required_]
string
Name of the quota.
overflow_action
enum
The action to take when the quota is exceeded. Options:
  * `drop`: Drop the event.
  * `no_action`: Let the event pass through.
  * `overflow_routing`: Route to an overflow destination. Allowed enum values: `drop,no_action,overflow_routing`


[object]
A list of alternate quota rules that apply to specific sets of events, identified by matching field values. Each override can define a custom limit.
_required_]
[object]
A list of field matchers used to apply a specific override. If an event matches all listed key-value pairs, the corresponding override limit is enforced.
name [_required_]
string
The field name.
value [_required_]
string
The field value.
_required_]
object
The maximum amount of data or number of events allowed before the quota is enforced. Can be specified in bytes or events.
enforce [_required_]
enum
Unit for quota enforcement in bytes for data size or events for count. Allowed enum values: `bytes,events`
limit [_required_]
int64
The limit for quota enforcement.
partition_fields
[string]
A list of fields used to segment log traffic for quota enforcement. Quotas are tracked independently by unique combinations of these field values.
type [_required_]
enum
The processor type. The value should always be `quota`. Allowed enum values: `quota`
default: `quota`
object
The `add_fields` processor adds static key-value fields to logs.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
_required_]
[object]
A list of static fields (key-value pairs) that is added to each log event processed by this component.
name [_required_]
string
The field name.
value [_required_]
string
The field value.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `add_fields`. Allowed enum values: `add_fields`
default: `add_fields`
object
The `remove_fields` processor deletes specified fields from logs.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
fields [_required_]
[string]
A list of field names to be removed from each log event.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `remove_fields`. Allowed enum values: `remove_fields`
default: `remove_fields`
object
The `rename_fields` processor changes field names.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
_required_]
[object]
A list of rename rules specifying which fields to rename in the event, what to rename them to, and whether to preserve the original fields.
destination [_required_]
string
The field name to assign the renamed value to.
preserve_source [_required_]
boolean
Indicates whether the original field, that is received from the source, should be kept (`true`) or removed (`false`) after renaming.
source [_required_]
string
The original field name in the log event that should be renamed.
id [_required_]
string
A unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `rename_fields`. Allowed enum values: `rename_fields`
default: `rename_fields`
object
The `generate_datadog_metrics` processor creates custom metrics from logs and sends them to Datadog. Metrics can be counters, gauges, or distributions and optionally grouped by log fields.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline.
include
string
A Datadog search query used to determine which logs this processor targets.
[object]
Configuration for generating individual metrics.
group_by
[string]
Optional fields used to group the metric series.
include [_required_]
string
Datadog filter query to match logs for metric generation.
metric_type [_required_]
enum
Type of metric to create. Allowed enum values: `count,gauge,distribution`
name [_required_]
string
Name of the custom metric to be created.
_required_]
<oneOf>
Specifies how the value of the generated metric is computed.
object
Strategy that increments a generated metric by one for each matching event.
strategy [_required_]
enum
Increments the metric by 1 for each matching event. Allowed enum values: `increment_by_one`
object
Strategy that increments a generated metric based on the value of a log field.
field [_required_]
string
Name of the log field containing the numeric value to increment the metric by.
strategy [_required_]
enum
Uses a numeric field in the log event as the metric increment. Allowed enum values: `increment_by_field`
type [_required_]
enum
The processor type. Always `generate_datadog_metrics`. Allowed enum values: `generate_datadog_metrics`
default: `generate_datadog_metrics`
object
The `sample` processor allows probabilistic sampling of logs at a fixed rate.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
percentage
double
The percentage of logs to sample.
rate
int64
Number of events to sample (1 in N).
type [_required_]
enum
The processor type. The value should always be `sample`. Allowed enum values: `sample`
default: `sample`
object
The `parse_grok` processor extracts structured fields from unstructured log messages using Grok patterns.
disable_library_rules
boolean
If set to `true`, disables the default Grok rules provided by Datadog.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
A unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
[object]
The list of Grok parsing rules. If multiple matching rules are provided, they are evaluated in order. The first successful match is applied.
_required_]
[object]
A list of Grok parsing rules that define how to extract fields from the source field. Each rule must contain a name and a valid Grok pattern.
name [_required_]
string
The name of the rule.
rule [_required_]
string
The definition of the Grok rule.
source [_required_]
string
The name of the field in the log event to apply the Grok rules to.
[object]
A list of Grok helper rules that can be referenced by the parsing rules.
name [_required_]
string
The name of the Grok helper rule.
rule [_required_]
string
The definition of the Grok helper rule.
type [_required_]
enum
The processor type. The value should always be `parse_grok`. Allowed enum values: `parse_grok`
default: `parse_grok`
object
The `sensitive_data_scanner` processor detects and optionally redacts sensitive data in log events.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
[object]
A list of rules for identifying and acting on sensitive data patterns.
object
Configuration for keywords used to reinforce sensitive data pattern detection.
keywords [_required_]
[string]
A list of keywords to match near the sensitive pattern.
proximity [_required_]
int64
Maximum number of tokens between a keyword and a sensitive value match.
name [_required_]
string
A name identifying the rule.
_required_]
<oneOf>
Defines what action to take when sensitive data is matched.
object
Configuration for completely redacting matched sensitive data.
action [_required_]
enum
Action type that completely replaces the matched sensitive data with a fixed replacement string to remove all visibility. Allowed enum values: `redact`
_required_]
object
Configuration for fully redacting sensitive data.
replace [_required_]
string
The `ObservabilityPipelineSensitiveDataScannerProcessorActionRedactOptions` `replace`.
object
Configuration for hashing matched sensitive values.
action [_required_]
enum
Action type that replaces the matched sensitive data with a hashed representation, preserving structure while securing content. Allowed enum values: `hash`
options
object
The `ObservabilityPipelineSensitiveDataScannerProcessorActionHash` `options`.
object
Configuration for partially redacting matched sensitive data.
action [_required_]
enum
Action type that redacts part of the sensitive data while preserving a configurable number of characters, typically used for masking purposes (e.g., show last 4 digits of a credit card). Allowed enum values: `partial_redact`
_required_]
object
Controls how partial redaction is applied, including character count and direction.
characters [_required_]
int64
The `ObservabilityPipelineSensitiveDataScannerProcessorActionPartialRedactOptions` `characters`.
direction [_required_]
enum
Indicates whether to redact characters from the first or last part of the matched value. Allowed enum values: `first,last`
_required_]
<oneOf>
Pattern detection configuration for identifying sensitive data using either a custom regex or a library reference.
object
Defines a custom regex-based pattern for identifying sensitive data in logs.
_required_]
object
Options for defining a custom regex pattern.
rule [_required_]
string
A regular expression used to detect sensitive values. Must be a valid regex.
type [_required_]
enum
Indicates a custom regular expression is used for matching. Allowed enum values: `custom`
object
Specifies a pattern from Datadog’s sensitive data detection library to match known sensitive data types.
_required_]
object
Options for selecting a predefined library pattern and enabling keyword support.
id [_required_]
string
Identifier for a predefined pattern from the sensitive data scanner pattern library.
use_recommended_keywords
boolean
Whether to augment the pattern with recommended keywords (optional).
type [_required_]
enum
Indicates that a predefined library pattern is used. Allowed enum values: `library`
_required_]
<oneOf>
Determines which parts of the log the pattern-matching rule should be applied to.
object
Includes only specific fields for sensitive data scanning.
_required_]
object
Fields to which the scope rule applies.
fields [_required_]
[string]
The `ObservabilityPipelineSensitiveDataScannerProcessorScopeOptions` `fields`.
target [_required_]
enum
Applies the rule only to included fields. Allowed enum values: `include`
object
Excludes specific fields from sensitive data scanning.
_required_]
object
Fields to which the scope rule applies.
fields [_required_]
[string]
The `ObservabilityPipelineSensitiveDataScannerProcessorScopeOptions` `fields`.
target [_required_]
enum
Excludes specific fields from processing. Allowed enum values: `exclude`
object
Applies scanning across all available fields.
target [_required_]
enum
Applies the rule to all fields. Allowed enum values: `all`
tags [_required_]
[string]
Tags assigned to this rule for filtering and classification.
type [_required_]
enum
The processor type. The value should always be `sensitive_data_scanner`. Allowed enum values: `sensitive_data_scanner`
default: `sensitive_data_scanner`
object
The `ocsf_mapper` processor transforms logs into the OCSF schema using a predefined mapping configuration.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
[object]
A list of mapping rules to convert events to the OCSF format.
include [_required_]
string
A Datadog search query used to select the logs that this mapping should apply to.
_required_]
<oneOf>
Defines a single mapping rule for transforming logs into the OCSF schema.
Option 1
enum
Predefined library mappings for common log formats. Allowed enum values: `CloudTrail Account Change,GCP Cloud Audit CreateBucket,GCP Cloud Audit CreateSink,GCP Cloud Audit SetIamPolicy,GCP Cloud Audit UpdateSink,Github Audit Log API Activity,Google Workspace Admin Audit addPrivilege,Microsoft 365 Defender Incident,Microsoft 365 Defender UserLoggedIn,Okta System Log Authentication,Palo Alto Networks Firewall Traffic`
type [_required_]
enum
The processor type. The value should always be `ocsf_mapper`. Allowed enum values: `ocsf_mapper`
default: `ocsf_mapper`
object
The `add_env_vars` processor adds environment variable values to log events.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this processor in the pipeline.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `add_env_vars`. Allowed enum values: `add_env_vars`
default: `add_env_vars`
_required_]
[object]
A list of environment variable mappings to apply to log fields.
field [_required_]
string
The target field in the log event.
name [_required_]
string
The name of the environment variable to read.
object
The `dedupe` processor removes duplicate fields in log events.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
fields [_required_]
[string]
A list of log field paths to check for duplicates.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
mode [_required_]
enum
The deduplication mode to apply to the fields. Allowed enum values: `match,ignore`
type [_required_]
enum
The processor type. The value should always be `dedupe`. Allowed enum values: `dedupe`
default: `dedupe`
object
The `enrichment_table` processor enriches logs using a static CSV file or GeoIP database.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
object
Defines a static enrichment table loaded from a CSV file.
_required_]
object
File encoding format.
delimiter [_required_]
string
The `encoding` `delimiter`.
includes_headers [_required_]
boolean
The `encoding` `includes_headers`.
type [_required_]
enum
Specifies the encoding format (e.g., CSV) used for enrichment tables. Allowed enum values: `csv`
_required_]
[object]
Key fields used to look up enrichment values.
column [_required_]
string
The `items` `column`.
comparison [_required_]
enum
Defines how to compare key fields for enrichment table lookups. Allowed enum values: `equals`
field [_required_]
string
The `items` `field`.
path [_required_]
string
Path to the CSV file.
_required_]
[object]
Schema defining column names and their types.
column [_required_]
string
The `items` `column`.
type [_required_]
enum
Declares allowed data types for enrichment table columns. Allowed enum values: `string,boolean,integer,float,date,timestamp`
object
Uses a GeoIP database to enrich logs based on an IP field.
key_field [_required_]
string
Path to the IP field in the log.
locale [_required_]
string
Locale used to resolve geographical names.
path [_required_]
string
Path to the GeoIP database file.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
target [_required_]
string
Path where enrichment results should be stored in the log.
type [_required_]
enum
The processor type. The value should always be `enrichment_table`. Allowed enum values: `enrichment_table`
default: `enrichment_table`
object
The `reduce` processor aggregates and merges logs based on matching keys and merge strategies.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
group_by [_required_]
[string]
A list of fields used to group log events for merging.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
[object]
List of merge strategies defining how values from grouped events should be combined.
path [_required_]
string
The field path in the log event.
strategy [_required_]
enum
The merge strategy to apply. Allowed enum values: `discard,retain,sum,max,min,array,concat,concat_newline,concat_raw,shortest_array,longest_array,flat_unique`
type [_required_]
enum
The processor type. The value should always be `reduce`. Allowed enum values: `reduce`
default: `reduce`
object
The `throttle` processor limits the number of events that pass through over a given time window.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
group_by
[string]
Optional list of fields used to group events before the threshold has been reached.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
threshold [_required_]
int64
the number of events allowed in a given time window. Events sent after the threshold has been reached, are dropped.
type [_required_]
enum
The processor type. The value should always be `throttle`. Allowed enum values: `throttle`
default: `throttle`
window [_required_]
double
The time window in seconds over which the threshold applies.
object
The `custom_processor` processor transforms events using [Vector Remap Language (VRL)](https://vector.dev/docs/reference/vrl/) scripts with advanced filtering capabilities.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets. This field should always be set to `*` for the custom_processor processor.
default: `*`
_required_]
[object]
Array of VRL remap rules.
drop_on_error [_required_]
boolean
Whether to drop events that caused errors during processing.
enabled
boolean
Whether this remap rule is enabled.
include [_required_]
string
A Datadog search query used to filter events for this specific remap rule.
name [_required_]
string
A descriptive name for this remap rule.
source [_required_]
string
The VRL script source code that defines the processing logic.
type [_required_]
enum
The processor type. The value should always be `custom_processor`. Allowed enum values: `custom_processor`
default: `custom_processor`
object
The `datadog_tags` processor includes or excludes specific Datadog tags in your logs.
action [_required_]
enum
The action to take on tags with matching keys. Allowed enum values: `include,exclude`
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
keys [_required_]
[string]
A list of tag keys.
mode [_required_]
enum
The processing mode. Allowed enum values: `filter`
type [_required_]
enum
The processor type. The value should always be `datadog_tags`. Allowed enum values: `datadog_tags`
default: `datadog_tags`
_required_]
[ <oneOf>]
A list of configured data sources for the pipeline.
object
The `kafka` source ingests data from Apache Kafka topics.
group_id [_required_]
string
Consumer group ID used by the Kafka client.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
[object]
Optional list of advanced Kafka client configuration options, defined as key-value pairs.
name [_required_]
string
The name of the `librdkafka` configuration option to set.
value [_required_]
string
The value assigned to the specified `librdkafka` configuration option.
object
Specifies the SASL mechanism for authenticating with a Kafka cluster.
mechanism
enum
SASL mechanism used for Kafka authentication. Allowed enum values: `PLAIN,SCRAM-SHA-256,SCRAM-SHA-512`
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
topics [_required_]
[string]
A list of Kafka topic names to subscribe to. The source ingests messages from each topic specified.
type [_required_]
enum
The source type. The value should always be `kafka`. Allowed enum values: `kafka`
default: `kafka`
object
The `datadog_agent` source collects logs from the Datadog Agent.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `datadog_agent`. Allowed enum values: `datadog_agent`
default: `datadog_agent`
object
The `splunk_tcp` source receives logs from a Splunk Universal Forwarder over TCP. TLS is supported for secure transmission.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. Always `splunk_tcp`. Allowed enum values: `splunk_tcp`
default: `splunk_tcp`
object
The `splunk_hec` source implements the Splunk HTTP Event Collector (HEC) API.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. Always `splunk_hec`. Allowed enum values: `splunk_hec`
default: `splunk_hec`
object
The `amazon_s3` source ingests logs from an Amazon S3 bucket. It supports AWS authentication and TLS encryption.
object
AWS authentication credentials used for accessing AWS services such as S3. If omitted, the system’s default credentials are used (for example, the IAM role and environment variables).
assume_role
string
The Amazon Resource Name (ARN) of the role to assume.
external_id
string
A unique identifier for cross-account role assumption.
session_name
string
A session identifier used for logging and tracing the assumed role session.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
region [_required_]
string
AWS region where the S3 bucket resides.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. Always `amazon_s3`. Allowed enum values: `amazon_s3`
default: `amazon_s3`
object
The `fluentd` source ingests logs from a Fluentd-compatible service.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `fluentd. Allowed enum values: `fluentd`
default: `fluentd`
object
The `fluent_bit` source ingests logs from Fluent Bit.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `fluent_bit`. Allowed enum values: `fluent_bit`
default: `fluent_bit`
object
The `http_server` source collects logs over HTTP POST from external services.
auth_strategy [_required_]
enum
HTTP authentication method. Allowed enum values: `none,plain`
decoding [_required_]
enum
The decoding format used to interpret incoming logs. Allowed enum values: `bytes,gelf,json,syslog`
id [_required_]
string
Unique ID for the HTTP server source.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `http_server`. Allowed enum values: `http_server`
default: `http_server`
object
The `sumo_logic` source receives logs from Sumo Logic collectors.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
type [_required_]
enum
The source type. The value should always be `sumo_logic`. Allowed enum values: `sumo_logic`
default: `sumo_logic`
object
The `rsyslog` source listens for logs over TCP or UDP from an `rsyslog` server using the syslog protocol.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
mode [_required_]
enum
Protocol used by the syslog source to receive messages. Allowed enum values: `tcp,udp`
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `rsyslog`. Allowed enum values: `rsyslog`
default: `rsyslog`
object
The `syslog_ng` source listens for logs over TCP or UDP from a `syslog-ng` server using the syslog protocol.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
mode [_required_]
enum
Protocol used by the syslog source to receive messages. Allowed enum values: `tcp,udp`
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `syslog_ng`. Allowed enum values: `syslog_ng`
default: `syslog_ng`
object
The `amazon_data_firehose` source ingests logs from AWS Data Firehose.
object
AWS authentication credentials used for accessing AWS services such as S3. If omitted, the system’s default credentials are used (for example, the IAM role and environment variables).
assume_role
string
The Amazon Resource Name (ARN) of the role to assume.
external_id
string
A unique identifier for cross-account role assumption.
session_name
string
A session identifier used for logging and tracing the assumed role session.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `amazon_data_firehose`. Allowed enum values: `amazon_data_firehose`
default: `amazon_data_firehose`
object
The `google_pubsub` source ingests logs from a Google Cloud Pub/Sub subscription.
object
GCP credentials used to authenticate with Google Cloud Storage.
credentials_file [_required_]
string
Path to the GCP service account key file.
decoding [_required_]
enum
The decoding format used to interpret incoming logs. Allowed enum values: `bytes,gelf,json,syslog`
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
project [_required_]
string
The GCP project ID that owns the Pub/Sub subscription.
subscription [_required_]
string
The Pub/Sub subscription name from which messages are consumed.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `google_pubsub`. Allowed enum values: `google_pubsub`
default: `google_pubsub`
object
The `http_client` source scrapes logs from HTTP endpoints at regular intervals.
auth_strategy
enum
Optional authentication strategy for HTTP requests. Allowed enum values: `basic,bearer`
decoding [_required_]
enum
The decoding format used to interpret incoming logs. Allowed enum values: `bytes,gelf,json,syslog`
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
scrape_interval_secs
int64
The interval (in seconds) between HTTP scrape requests.
scrape_timeout_secs
int64
The timeout (in seconds) for each scrape request.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `http_client`. Allowed enum values: `http_client`
default: `http_client`
object
The `logstash` source ingests logs from a Logstash forwarder.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `logstash`. Allowed enum values: `logstash`
default: `logstash`
object
The `socket` source ingests logs over TCP or UDP.
_required_]
<oneOf>
Framing method configuration for the socket source.
object
Byte frames which are delimited by a newline character.
method [_required_]
enum
Byte frames which are delimited by a newline character. Allowed enum values: `newline_delimited`
object
Byte frames are passed through as-is according to the underlying I/O boundaries (for example, split between messages or stream segments).
method [_required_]
enum
Byte frames are passed through as-is according to the underlying I/O boundaries (for example, split between messages or stream segments). Allowed enum values: `bytes`
object
Byte frames which are delimited by a chosen character.
delimiter [_required_]
string
A single ASCII character used to delimit events.
method [_required_]
enum
Byte frames which are delimited by a chosen character. Allowed enum values: `character_delimited`
object
Byte frames according to the octet counting format as per RFC6587.
method [_required_]
enum
Byte frames according to the octet counting format as per RFC6587. Allowed enum values: `octet_counting`
object
Byte frames which are chunked GELF messages.
method [_required_]
enum
Byte frames which are chunked GELF messages. Allowed enum values: `chunked_gelf`
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
mode [_required_]
enum
Protocol used to receive logs. Allowed enum values: `tcp,udp`
object
TLS configuration. Relevant only when `mode` is `tcp`.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `socket`. Allowed enum values: `socket`
default: `socket`
name [_required_]
string
Name of the pipeline.
id [_required_]
string
Unique identifier for the pipeline.
type [_required_]
string
The resource type identifier. For pipeline resources, this should always be set to `pipelines`.
default: `pipelines`
```
{
  "data": {
    "attributes": {
      "config": {
        "destinations": [
          {
            "id": "updated-datadog-logs-destination-id",
            "inputs": [
              "my-processor-group"
            ],
            "type": "datadog_logs"
          }
        ],
        "processors": [
          {
            "enabled": true,
            "id": "my-processor-group",
            "include": "service:my-service",
            "inputs": [
              "datadog-agent-source"
            ],
            "processors": [
              {
                "enabled": true,
                "id": "filter-processor",
                "include": "status:error",
                "type": "filter"
              }
            ]
          }
        ],
        "sources": [
          {
            "id": "datadog-agent-source",
            "type": "datadog_agent"
          }
        ]
      },
      "name": "Updated Pipeline Name"
    },
    "id": "3fa85f64-5717-4562-b3fc-2c963f66afa6",
    "type": "pipelines"
  }
}
```

Copy
### Response
  * [200](https://docs.datadoghq.com/api/latest/observability-pipelines/#UpdatePipeline-200-v2)
  * [400](https://docs.datadoghq.com/api/latest/observability-pipelines/#UpdatePipeline-400-v2)
  * [403](https://docs.datadoghq.com/api/latest/observability-pipelines/#UpdatePipeline-403-v2)
  * [404](https://docs.datadoghq.com/api/latest/observability-pipelines/#UpdatePipeline-404-v2)
  * [409](https://docs.datadoghq.com/api/latest/observability-pipelines/#UpdatePipeline-409-v2)
  * [429](https://docs.datadoghq.com/api/latest/observability-pipelines/#UpdatePipeline-429-v2)


OK
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


Top-level schema representing a pipeline.
Expand All
Field
Type
Description
_required_]
object
Contains the pipeline’s ID, type, and configuration attributes.
_required_]
object
Defines the pipeline’s name and its components (sources, processors, and destinations).
_required_]
object
Specifies the pipeline's configuration, including its sources, processors, and destinations.
_required_]
[ <oneOf>]
A list of destination components where processed logs are sent.
object
The `datadog_logs` destination forwards logs to Datadog Log Management.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `datadog_logs`. Allowed enum values: `datadog_logs`
default: `datadog_logs`
object
The `amazon_s3` destination sends your logs in Datadog-rehydratable format to an Amazon S3 bucket for archiving.
object
AWS authentication credentials used for accessing AWS services such as S3. If omitted, the system’s default credentials are used (for example, the IAM role and environment variables).
assume_role
string
The Amazon Resource Name (ARN) of the role to assume.
external_id
string
A unique identifier for cross-account role assumption.
session_name
string
A session identifier used for logging and tracing the assumed role session.
bucket [_required_]
string
S3 bucket name.
id [_required_]
string
Unique identifier for the destination component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
key_prefix
string
Optional prefix for object keys.
region [_required_]
string
AWS region of the S3 bucket.
storage_class [_required_]
enum
S3 storage class. Allowed enum values: `STANDARD,REDUCED_REDUNDANCY,INTELLIGENT_TIERING,STANDARD_IA,EXPRESS_ONEZONE,ONEZONE_IA,GLACIER,GLACIER_IR,DEEP_ARCHIVE`
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. Always `amazon_s3`. Allowed enum values: `amazon_s3`
default: `amazon_s3`
object
The `google_cloud_storage` destination stores logs in a Google Cloud Storage (GCS) bucket. It requires a bucket name, GCP authentication, and metadata fields.
acl
enum
Access control list setting for objects written to the bucket. Allowed enum values: `private,project-private,public-read,authenticated-read,bucket-owner-read,bucket-owner-full-control`
object
GCP credentials used to authenticate with Google Cloud Storage.
credentials_file [_required_]
string
Path to the GCP service account key file.
bucket [_required_]
string
Name of the GCS bucket.
id [_required_]
string
Unique identifier for the destination component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
key_prefix
string
Optional prefix for object keys within the GCS bucket.
[object]
Custom metadata to attach to each object uploaded to the GCS bucket.
name [_required_]
string
The metadata key.
value [_required_]
string
The metadata value.
storage_class [_required_]
enum
Storage class used for objects stored in GCS. Allowed enum values: `STANDARD,NEARLINE,COLDLINE,ARCHIVE`
type [_required_]
enum
The destination type. Always `google_cloud_storage`. Allowed enum values: `google_cloud_storage`
default: `google_cloud_storage`
object
The `splunk_hec` destination forwards logs to Splunk using the HTTP Event Collector (HEC).
auto_extract_timestamp
boolean
If `true`, Splunk tries to extract timestamps from incoming log events. If `false`, Splunk assigns the time the event was received.
encoding
enum
Encoding format for log events. Allowed enum values: `json,raw_message`
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
index
string
Optional name of the Splunk index where logs are written.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
sourcetype
string
The Splunk sourcetype to assign to log events.
type [_required_]
enum
The destination type. Always `splunk_hec`. Allowed enum values: `splunk_hec`
default: `splunk_hec`
object
The `sumo_logic` destination forwards logs to Sumo Logic.
encoding
enum
The output encoding format. Allowed enum values: `json,raw_message,logfmt`
[object]
A list of custom headers to include in the request to Sumo Logic.
name [_required_]
string
The header field name.
value [_required_]
string
The header field value.
header_host_name
string
Optional override for the host name header.
header_source_category
string
Optional override for the source category header.
header_source_name
string
Optional override for the source name header.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `sumo_logic`. Allowed enum values: `sumo_logic`
default: `sumo_logic`
object
The `elasticsearch` destination writes logs to an Elasticsearch cluster.
api_version
enum
The Elasticsearch API version to use. Set to `auto` to auto-detect. Allowed enum values: `auto,v6,v7,v8`
bulk_index
string
The index to write logs to in Elasticsearch.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `elasticsearch`. Allowed enum values: `elasticsearch`
default: `elasticsearch`
object
The `rsyslog` destination forwards logs to an external `rsyslog` server over TCP or UDP using the syslog protocol.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
keepalive
int64
Optional socket keepalive duration in milliseconds.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. The value should always be `rsyslog`. Allowed enum values: `rsyslog`
default: `rsyslog`
object
The `syslog_ng` destination forwards logs to an external `syslog-ng` server over TCP or UDP using the syslog protocol.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
keepalive
int64
Optional socket keepalive duration in milliseconds.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. The value should always be `syslog_ng`. Allowed enum values: `syslog_ng`
default: `syslog_ng`
object
The `azure_storage` destination forwards logs to an Azure Blob Storage container.
blob_prefix
string
Optional prefix for blobs written to the container.
container_name [_required_]
string
The name of the Azure Blob Storage container to store logs in.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `azure_storage`. Allowed enum values: `azure_storage`
default: `azure_storage`
object
The `microsoft_sentinel` destination forwards logs to Microsoft Sentinel.
client_id [_required_]
string
Azure AD client ID used for authentication.
dcr_immutable_id [_required_]
string
The immutable ID of the Data Collection Rule (DCR).
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
table [_required_]
string
The name of the Log Analytics table where logs are sent.
tenant_id [_required_]
string
Azure AD tenant ID.
type [_required_]
enum
The destination type. The value should always be `microsoft_sentinel`. Allowed enum values: `microsoft_sentinel`
default: `microsoft_sentinel`
object
The `google_chronicle` destination sends logs to Google Chronicle.
object
GCP credentials used to authenticate with Google Cloud Storage.
credentials_file [_required_]
string
Path to the GCP service account key file.
customer_id [_required_]
string
The Google Chronicle customer ID.
encoding
enum
The encoding format for the logs sent to Chronicle. Allowed enum values: `json,raw_message`
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
log_type
string
The log type metadata associated with the Chronicle destination.
type [_required_]
enum
The destination type. The value should always be `google_chronicle`. Allowed enum values: `google_chronicle`
default: `google_chronicle`
object
The `new_relic` destination sends logs to the New Relic platform.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
region [_required_]
enum
The New Relic region. Allowed enum values: `us,eu`
type [_required_]
enum
The destination type. The value should always be `new_relic`. Allowed enum values: `new_relic`
default: `new_relic`
object
The `sentinel_one` destination sends logs to SentinelOne.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
region [_required_]
enum
The SentinelOne region to send logs to. Allowed enum values: `us,eu,ca,data_set_us`
type [_required_]
enum
The destination type. The value should always be `sentinel_one`. Allowed enum values: `sentinel_one`
default: `sentinel_one`
object
The `opensearch` destination writes logs to an OpenSearch cluster.
bulk_index
string
The index to write logs to.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `opensearch`. Allowed enum values: `opensearch`
default: `opensearch`
object
The `amazon_opensearch` destination writes logs to Amazon OpenSearch.
_required_]
object
Authentication settings for the Amazon OpenSearch destination. The `strategy` field determines whether basic or AWS-based authentication is used.
assume_role
string
The ARN of the role to assume (used with `aws` strategy).
aws_region
string
AWS region
external_id
string
External ID for the assumed role (used with `aws` strategy).
session_name
string
Session name for the assumed role (used with `aws` strategy).
strategy [_required_]
enum
The authentication strategy to use. Allowed enum values: `basic,aws`
bulk_index
string
The index to write logs to.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `amazon_opensearch`. Allowed enum values: `amazon_opensearch`
default: `amazon_opensearch`
object
The `socket` destination sends logs over TCP or UDP to a remote server.
encoding [_required_]
enum
Encoding format for log events. Allowed enum values: `json,raw_message`
_required_]
<oneOf>
Framing method configuration.
object
Each log event is delimited by a newline character.
method [_required_]
enum
The definition of `ObservabilityPipelineSocketDestinationFramingNewlineDelimitedMethod` object. Allowed enum values: `newline_delimited`
object
Event data is not delimited at all.
method [_required_]
enum
The definition of `ObservabilityPipelineSocketDestinationFramingBytesMethod` object. Allowed enum values: `bytes`
object
Each log event is separated using the specified delimiter character.
delimiter [_required_]
string
A single ASCII character used as a delimiter.
method [_required_]
enum
The definition of `ObservabilityPipelineSocketDestinationFramingCharacterDelimitedMethod` object. Allowed enum values: `character_delimited`
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
mode [_required_]
enum
Protocol used to send logs. Allowed enum values: `tcp,udp`
object
TLS configuration. Relevant only when `mode` is `tcp`.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. The value should always be `socket`. Allowed enum values: `socket`
default: `socket`
object
The `amazon_security_lake` destination sends your logs to Amazon Security Lake.
object
AWS authentication credentials used for accessing AWS services such as S3. If omitted, the system’s default credentials are used (for example, the IAM role and environment variables).
assume_role
string
The Amazon Resource Name (ARN) of the role to assume.
external_id
string
A unique identifier for cross-account role assumption.
session_name
string
A session identifier used for logging and tracing the assumed role session.
bucket [_required_]
string
Name of the Amazon S3 bucket in Security Lake (3-63 characters).
custom_source_name [_required_]
string
Custom source name for the logs in Security Lake.
id [_required_]
string
Unique identifier for the destination component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
region [_required_]
string
AWS region of the S3 bucket.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. Always `amazon_security_lake`. Allowed enum values: `amazon_security_lake`
default: `amazon_security_lake`
object
The `crowdstrike_next_gen_siem` destination forwards logs to CrowdStrike Next Gen SIEM.
object
Compression configuration for log events.
algorithm [_required_]
enum
Compression algorithm for log events. Allowed enum values: `gzip,zlib`
level
int64
Compression level.
encoding [_required_]
enum
Encoding format for log events. Allowed enum values: `json,raw_message`
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. The value should always be `crowdstrike_next_gen_siem`. Allowed enum values: `crowdstrike_next_gen_siem`
default: `crowdstrike_next_gen_siem`
object
The `google_pubsub` destination publishes logs to a Google Cloud Pub/Sub topic.
object
GCP credentials used to authenticate with Google Cloud Storage.
credentials_file [_required_]
string
Path to the GCP service account key file.
encoding [_required_]
enum
Encoding format for log events. Allowed enum values: `json,raw_message`
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
project [_required_]
string
The GCP project ID that owns the Pub/Sub topic.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
topic [_required_]
string
The Pub/Sub topic name to publish logs to.
type [_required_]
enum
The destination type. The value should always be `google_pubsub`. Allowed enum values: `google_pubsub`
default: `google_pubsub`
[object]
A list of processor groups that transform or enrich log data.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor group is enabled.
id [_required_]
string
The unique identifier for the processor group.
include [_required_]
string
Conditional expression for when this processor group should execute.
inputs [_required_]
[string]
A list of IDs for components whose output is used as the input for this processor group.
_required_]
[ <oneOf>]
Processors applied sequentially within this group. Events flow through each processor in order.
object
The `filter` processor allows conditional processing of logs based on a Datadog search query. Logs that match the `include` query are passed through; others are discarded.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs should pass through the filter. Logs that match this query continue to downstream components; others are dropped.
type [_required_]
enum
The processor type. The value should always be `filter`. Allowed enum values: `filter`
default: `filter`
object
The `parse_json` processor extracts JSON from a specified field and flattens it into the event. This is useful when logs contain embedded JSON as a string.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
field [_required_]
string
The name of the log field that contains a JSON string.
id [_required_]
string
A unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `parse_json`. Allowed enum values: `parse_json`
default: `parse_json`
object
The Quota Processor measures logging traffic for logs that match a specified filter. When the configured daily quota is met, the processor can drop or alert.
display_name
string
The display name for a component.
drop_events
boolean
If set to `true`, logs that matched the quota filter and sent after the quota has been met are dropped; only logs that did not match the filter query continue through the pipeline.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
ignore_when_missing_partitions
boolean
If `true`, the processor skips quota checks when partition fields are missing from the logs.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
object
The maximum amount of data or number of events allowed before the quota is enforced. Can be specified in bytes or events.
enforce [_required_]
enum
Unit for quota enforcement in bytes for data size or events for count. Allowed enum values: `bytes,events`
limit [_required_]
int64
The limit for quota enforcement.
name [_required_]
string
Name of the quota.
overflow_action
enum
The action to take when the quota is exceeded. Options:
  * `drop`: Drop the event.
  * `no_action`: Let the event pass through.
  * `overflow_routing`: Route to an overflow destination. Allowed enum values: `drop,no_action,overflow_routing`


[object]
A list of alternate quota rules that apply to specific sets of events, identified by matching field values. Each override can define a custom limit.
_required_]
[object]
A list of field matchers used to apply a specific override. If an event matches all listed key-value pairs, the corresponding override limit is enforced.
name [_required_]
string
The field name.
value [_required_]
string
The field value.
_required_]
object
The maximum amount of data or number of events allowed before the quota is enforced. Can be specified in bytes or events.
enforce [_required_]
enum
Unit for quota enforcement in bytes for data size or events for count. Allowed enum values: `bytes,events`
limit [_required_]
int64
The limit for quota enforcement.
partition_fields
[string]
A list of fields used to segment log traffic for quota enforcement. Quotas are tracked independently by unique combinations of these field values.
type [_required_]
enum
The processor type. The value should always be `quota`. Allowed enum values: `quota`
default: `quota`
object
The `add_fields` processor adds static key-value fields to logs.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
_required_]
[object]
A list of static fields (key-value pairs) that is added to each log event processed by this component.
name [_required_]
string
The field name.
value [_required_]
string
The field value.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `add_fields`. Allowed enum values: `add_fields`
default: `add_fields`
object
The `remove_fields` processor deletes specified fields from logs.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
fields [_required_]
[string]
A list of field names to be removed from each log event.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `remove_fields`. Allowed enum values: `remove_fields`
default: `remove_fields`
object
The `rename_fields` processor changes field names.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
_required_]
[object]
A list of rename rules specifying which fields to rename in the event, what to rename them to, and whether to preserve the original fields.
destination [_required_]
string
The field name to assign the renamed value to.
preserve_source [_required_]
boolean
Indicates whether the original field, that is received from the source, should be kept (`true`) or removed (`false`) after renaming.
source [_required_]
string
The original field name in the log event that should be renamed.
id [_required_]
string
A unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `rename_fields`. Allowed enum values: `rename_fields`
default: `rename_fields`
object
The `generate_datadog_metrics` processor creates custom metrics from logs and sends them to Datadog. Metrics can be counters, gauges, or distributions and optionally grouped by log fields.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline.
include
string
A Datadog search query used to determine which logs this processor targets.
[object]
Configuration for generating individual metrics.
group_by
[string]
Optional fields used to group the metric series.
include [_required_]
string
Datadog filter query to match logs for metric generation.
metric_type [_required_]
enum
Type of metric to create. Allowed enum values: `count,gauge,distribution`
name [_required_]
string
Name of the custom metric to be created.
_required_]
<oneOf>
Specifies how the value of the generated metric is computed.
object
Strategy that increments a generated metric by one for each matching event.
strategy [_required_]
enum
Increments the metric by 1 for each matching event. Allowed enum values: `increment_by_one`
object
Strategy that increments a generated metric based on the value of a log field.
field [_required_]
string
Name of the log field containing the numeric value to increment the metric by.
strategy [_required_]
enum
Uses a numeric field in the log event as the metric increment. Allowed enum values: `increment_by_field`
type [_required_]
enum
The processor type. Always `generate_datadog_metrics`. Allowed enum values: `generate_datadog_metrics`
default: `generate_datadog_metrics`
object
The `sample` processor allows probabilistic sampling of logs at a fixed rate.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
percentage
double
The percentage of logs to sample.
rate
int64
Number of events to sample (1 in N).
type [_required_]
enum
The processor type. The value should always be `sample`. Allowed enum values: `sample`
default: `sample`
object
The `parse_grok` processor extracts structured fields from unstructured log messages using Grok patterns.
disable_library_rules
boolean
If set to `true`, disables the default Grok rules provided by Datadog.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
A unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
[object]
The list of Grok parsing rules. If multiple matching rules are provided, they are evaluated in order. The first successful match is applied.
_required_]
[object]
A list of Grok parsing rules that define how to extract fields from the source field. Each rule must contain a name and a valid Grok pattern.
name [_required_]
string
The name of the rule.
rule [_required_]
string
The definition of the Grok rule.
source [_required_]
string
The name of the field in the log event to apply the Grok rules to.
[object]
A list of Grok helper rules that can be referenced by the parsing rules.
name [_required_]
string
The name of the Grok helper rule.
rule [_required_]
string
The definition of the Grok helper rule.
type [_required_]
enum
The processor type. The value should always be `parse_grok`. Allowed enum values: `parse_grok`
default: `parse_grok`
object
The `sensitive_data_scanner` processor detects and optionally redacts sensitive data in log events.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
[object]
A list of rules for identifying and acting on sensitive data patterns.
object
Configuration for keywords used to reinforce sensitive data pattern detection.
keywords [_required_]
[string]
A list of keywords to match near the sensitive pattern.
proximity [_required_]
int64
Maximum number of tokens between a keyword and a sensitive value match.
name [_required_]
string
A name identifying the rule.
_required_]
<oneOf>
Defines what action to take when sensitive data is matched.
object
Configuration for completely redacting matched sensitive data.
action [_required_]
enum
Action type that completely replaces the matched sensitive data with a fixed replacement string to remove all visibility. Allowed enum values: `redact`
_required_]
object
Configuration for fully redacting sensitive data.
replace [_required_]
string
The `ObservabilityPipelineSensitiveDataScannerProcessorActionRedactOptions` `replace`.
object
Configuration for hashing matched sensitive values.
action [_required_]
enum
Action type that replaces the matched sensitive data with a hashed representation, preserving structure while securing content. Allowed enum values: `hash`
options
object
The `ObservabilityPipelineSensitiveDataScannerProcessorActionHash` `options`.
object
Configuration for partially redacting matched sensitive data.
action [_required_]
enum
Action type that redacts part of the sensitive data while preserving a configurable number of characters, typically used for masking purposes (e.g., show last 4 digits of a credit card). Allowed enum values: `partial_redact`
_required_]
object
Controls how partial redaction is applied, including character count and direction.
characters [_required_]
int64
The `ObservabilityPipelineSensitiveDataScannerProcessorActionPartialRedactOptions` `characters`.
direction [_required_]
enum
Indicates whether to redact characters from the first or last part of the matched value. Allowed enum values: `first,last`
_required_]
<oneOf>
Pattern detection configuration for identifying sensitive data using either a custom regex or a library reference.
object
Defines a custom regex-based pattern for identifying sensitive data in logs.
_required_]
object
Options for defining a custom regex pattern.
rule [_required_]
string
A regular expression used to detect sensitive values. Must be a valid regex.
type [_required_]
enum
Indicates a custom regular expression is used for matching. Allowed enum values: `custom`
object
Specifies a pattern from Datadog’s sensitive data detection library to match known sensitive data types.
_required_]
object
Options for selecting a predefined library pattern and enabling keyword support.
id [_required_]
string
Identifier for a predefined pattern from the sensitive data scanner pattern library.
use_recommended_keywords
boolean
Whether to augment the pattern with recommended keywords (optional).
type [_required_]
enum
Indicates that a predefined library pattern is used. Allowed enum values: `library`
_required_]
<oneOf>
Determines which parts of the log the pattern-matching rule should be applied to.
object
Includes only specific fields for sensitive data scanning.
_required_]
object
Fields to which the scope rule applies.
fields [_required_]
[string]
The `ObservabilityPipelineSensitiveDataScannerProcessorScopeOptions` `fields`.
target [_required_]
enum
Applies the rule only to included fields. Allowed enum values: `include`
object
Excludes specific fields from sensitive data scanning.
_required_]
object
Fields to which the scope rule applies.
fields [_required_]
[string]
The `ObservabilityPipelineSensitiveDataScannerProcessorScopeOptions` `fields`.
target [_required_]
enum
Excludes specific fields from processing. Allowed enum values: `exclude`
object
Applies scanning across all available fields.
target [_required_]
enum
Applies the rule to all fields. Allowed enum values: `all`
tags [_required_]
[string]
Tags assigned to this rule for filtering and classification.
type [_required_]
enum
The processor type. The value should always be `sensitive_data_scanner`. Allowed enum values: `sensitive_data_scanner`
default: `sensitive_data_scanner`
object
The `ocsf_mapper` processor transforms logs into the OCSF schema using a predefined mapping configuration.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
[object]
A list of mapping rules to convert events to the OCSF format.
include [_required_]
string
A Datadog search query used to select the logs that this mapping should apply to.
_required_]
<oneOf>
Defines a single mapping rule for transforming logs into the OCSF schema.
Option 1
enum
Predefined library mappings for common log formats. Allowed enum values: `CloudTrail Account Change,GCP Cloud Audit CreateBucket,GCP Cloud Audit CreateSink,GCP Cloud Audit SetIamPolicy,GCP Cloud Audit UpdateSink,Github Audit Log API Activity,Google Workspace Admin Audit addPrivilege,Microsoft 365 Defender Incident,Microsoft 365 Defender UserLoggedIn,Okta System Log Authentication,Palo Alto Networks Firewall Traffic`
type [_required_]
enum
The processor type. The value should always be `ocsf_mapper`. Allowed enum values: `ocsf_mapper`
default: `ocsf_mapper`
object
The `add_env_vars` processor adds environment variable values to log events.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this processor in the pipeline.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `add_env_vars`. Allowed enum values: `add_env_vars`
default: `add_env_vars`
_required_]
[object]
A list of environment variable mappings to apply to log fields.
field [_required_]
string
The target field in the log event.
name [_required_]
string
The name of the environment variable to read.
object
The `dedupe` processor removes duplicate fields in log events.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
fields [_required_]
[string]
A list of log field paths to check for duplicates.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
mode [_required_]
enum
The deduplication mode to apply to the fields. Allowed enum values: `match,ignore`
type [_required_]
enum
The processor type. The value should always be `dedupe`. Allowed enum values: `dedupe`
default: `dedupe`
object
The `enrichment_table` processor enriches logs using a static CSV file or GeoIP database.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
object
Defines a static enrichment table loaded from a CSV file.
_required_]
object
File encoding format.
delimiter [_required_]
string
The `encoding` `delimiter`.
includes_headers [_required_]
boolean
The `encoding` `includes_headers`.
type [_required_]
enum
Specifies the encoding format (e.g., CSV) used for enrichment tables. Allowed enum values: `csv`
_required_]
[object]
Key fields used to look up enrichment values.
column [_required_]
string
The `items` `column`.
comparison [_required_]
enum
Defines how to compare key fields for enrichment table lookups. Allowed enum values: `equals`
field [_required_]
string
The `items` `field`.
path [_required_]
string
Path to the CSV file.
_required_]
[object]
Schema defining column names and their types.
column [_required_]
string
The `items` `column`.
type [_required_]
enum
Declares allowed data types for enrichment table columns. Allowed enum values: `string,boolean,integer,float,date,timestamp`
object
Uses a GeoIP database to enrich logs based on an IP field.
key_field [_required_]
string
Path to the IP field in the log.
locale [_required_]
string
Locale used to resolve geographical names.
path [_required_]
string
Path to the GeoIP database file.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
target [_required_]
string
Path where enrichment results should be stored in the log.
type [_required_]
enum
The processor type. The value should always be `enrichment_table`. Allowed enum values: `enrichment_table`
default: `enrichment_table`
object
The `reduce` processor aggregates and merges logs based on matching keys and merge strategies.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
group_by [_required_]
[string]
A list of fields used to group log events for merging.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
[object]
List of merge strategies defining how values from grouped events should be combined.
path [_required_]
string
The field path in the log event.
strategy [_required_]
enum
The merge strategy to apply. Allowed enum values: `discard,retain,sum,max,min,array,concat,concat_newline,concat_raw,shortest_array,longest_array,flat_unique`
type [_required_]
enum
The processor type. The value should always be `reduce`. Allowed enum values: `reduce`
default: `reduce`
object
The `throttle` processor limits the number of events that pass through over a given time window.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
group_by
[string]
Optional list of fields used to group events before the threshold has been reached.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
threshold [_required_]
int64
the number of events allowed in a given time window. Events sent after the threshold has been reached, are dropped.
type [_required_]
enum
The processor type. The value should always be `throttle`. Allowed enum values: `throttle`
default: `throttle`
window [_required_]
double
The time window in seconds over which the threshold applies.
object
The `custom_processor` processor transforms events using [Vector Remap Language (VRL)](https://vector.dev/docs/reference/vrl/) scripts with advanced filtering capabilities.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets. This field should always be set to `*` for the custom_processor processor.
default: `*`
_required_]
[object]
Array of VRL remap rules.
drop_on_error [_required_]
boolean
Whether to drop events that caused errors during processing.
enabled
boolean
Whether this remap rule is enabled.
include [_required_]
string
A Datadog search query used to filter events for this specific remap rule.
name [_required_]
string
A descriptive name for this remap rule.
source [_required_]
string
The VRL script source code that defines the processing logic.
type [_required_]
enum
The processor type. The value should always be `custom_processor`. Allowed enum values: `custom_processor`
default: `custom_processor`
object
The `datadog_tags` processor includes or excludes specific Datadog tags in your logs.
action [_required_]
enum
The action to take on tags with matching keys. Allowed enum values: `include,exclude`
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
keys [_required_]
[string]
A list of tag keys.
mode [_required_]
enum
The processing mode. Allowed enum values: `filter`
type [_required_]
enum
The processor type. The value should always be `datadog_tags`. Allowed enum values: `datadog_tags`
default: `datadog_tags`
_required_]
[ <oneOf>]
A list of configured data sources for the pipeline.
object
The `kafka` source ingests data from Apache Kafka topics.
group_id [_required_]
string
Consumer group ID used by the Kafka client.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
[object]
Optional list of advanced Kafka client configuration options, defined as key-value pairs.
name [_required_]
string
The name of the `librdkafka` configuration option to set.
value [_required_]
string
The value assigned to the specified `librdkafka` configuration option.
object
Specifies the SASL mechanism for authenticating with a Kafka cluster.
mechanism
enum
SASL mechanism used for Kafka authentication. Allowed enum values: `PLAIN,SCRAM-SHA-256,SCRAM-SHA-512`
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
topics [_required_]
[string]
A list of Kafka topic names to subscribe to. The source ingests messages from each topic specified.
type [_required_]
enum
The source type. The value should always be `kafka`. Allowed enum values: `kafka`
default: `kafka`
object
The `datadog_agent` source collects logs from the Datadog Agent.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `datadog_agent`. Allowed enum values: `datadog_agent`
default: `datadog_agent`
object
The `splunk_tcp` source receives logs from a Splunk Universal Forwarder over TCP. TLS is supported for secure transmission.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. Always `splunk_tcp`. Allowed enum values: `splunk_tcp`
default: `splunk_tcp`
object
The `splunk_hec` source implements the Splunk HTTP Event Collector (HEC) API.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. Always `splunk_hec`. Allowed enum values: `splunk_hec`
default: `splunk_hec`
object
The `amazon_s3` source ingests logs from an Amazon S3 bucket. It supports AWS authentication and TLS encryption.
object
AWS authentication credentials used for accessing AWS services such as S3. If omitted, the system’s default credentials are used (for example, the IAM role and environment variables).
assume_role
string
The Amazon Resource Name (ARN) of the role to assume.
external_id
string
A unique identifier for cross-account role assumption.
session_name
string
A session identifier used for logging and tracing the assumed role session.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
region [_required_]
string
AWS region where the S3 bucket resides.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. Always `amazon_s3`. Allowed enum values: `amazon_s3`
default: `amazon_s3`
object
The `fluentd` source ingests logs from a Fluentd-compatible service.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `fluentd. Allowed enum values: `fluentd`
default: `fluentd`
object
The `fluent_bit` source ingests logs from Fluent Bit.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `fluent_bit`. Allowed enum values: `fluent_bit`
default: `fluent_bit`
object
The `http_server` source collects logs over HTTP POST from external services.
auth_strategy [_required_]
enum
HTTP authentication method. Allowed enum values: `none,plain`
decoding [_required_]
enum
The decoding format used to interpret incoming logs. Allowed enum values: `bytes,gelf,json,syslog`
id [_required_]
string
Unique ID for the HTTP server source.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `http_server`. Allowed enum values: `http_server`
default: `http_server`
object
The `sumo_logic` source receives logs from Sumo Logic collectors.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
type [_required_]
enum
The source type. The value should always be `sumo_logic`. Allowed enum values: `sumo_logic`
default: `sumo_logic`
object
The `rsyslog` source listens for logs over TCP or UDP from an `rsyslog` server using the syslog protocol.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
mode [_required_]
enum
Protocol used by the syslog source to receive messages. Allowed enum values: `tcp,udp`
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `rsyslog`. Allowed enum values: `rsyslog`
default: `rsyslog`
object
The `syslog_ng` source listens for logs over TCP or UDP from a `syslog-ng` server using the syslog protocol.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
mode [_required_]
enum
Protocol used by the syslog source to receive messages. Allowed enum values: `tcp,udp`
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `syslog_ng`. Allowed enum values: `syslog_ng`
default: `syslog_ng`
object
The `amazon_data_firehose` source ingests logs from AWS Data Firehose.
object
AWS authentication credentials used for accessing AWS services such as S3. If omitted, the system’s default credentials are used (for example, the IAM role and environment variables).
assume_role
string
The Amazon Resource Name (ARN) of the role to assume.
external_id
string
A unique identifier for cross-account role assumption.
session_name
string
A session identifier used for logging and tracing the assumed role session.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `amazon_data_firehose`. Allowed enum values: `amazon_data_firehose`
default: `amazon_data_firehose`
object
The `google_pubsub` source ingests logs from a Google Cloud Pub/Sub subscription.
object
GCP credentials used to authenticate with Google Cloud Storage.
credentials_file [_required_]
string
Path to the GCP service account key file.
decoding [_required_]
enum
The decoding format used to interpret incoming logs. Allowed enum values: `bytes,gelf,json,syslog`
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
project [_required_]
string
The GCP project ID that owns the Pub/Sub subscription.
subscription [_required_]
string
The Pub/Sub subscription name from which messages are consumed.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `google_pubsub`. Allowed enum values: `google_pubsub`
default: `google_pubsub`
object
The `http_client` source scrapes logs from HTTP endpoints at regular intervals.
auth_strategy
enum
Optional authentication strategy for HTTP requests. Allowed enum values: `basic,bearer`
decoding [_required_]
enum
The decoding format used to interpret incoming logs. Allowed enum values: `bytes,gelf,json,syslog`
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
scrape_interval_secs
int64
The interval (in seconds) between HTTP scrape requests.
scrape_timeout_secs
int64
The timeout (in seconds) for each scrape request.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `http_client`. Allowed enum values: `http_client`
default: `http_client`
object
The `logstash` source ingests logs from a Logstash forwarder.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `logstash`. Allowed enum values: `logstash`
default: `logstash`
object
The `socket` source ingests logs over TCP or UDP.
_required_]
<oneOf>
Framing method configuration for the socket source.
object
Byte frames which are delimited by a newline character.
method [_required_]
enum
Byte frames which are delimited by a newline character. Allowed enum values: `newline_delimited`
object
Byte frames are passed through as-is according to the underlying I/O boundaries (for example, split between messages or stream segments).
method [_required_]
enum
Byte frames are passed through as-is according to the underlying I/O boundaries (for example, split between messages or stream segments). Allowed enum values: `bytes`
object
Byte frames which are delimited by a chosen character.
delimiter [_required_]
string
A single ASCII character used to delimit events.
method [_required_]
enum
Byte frames which are delimited by a chosen character. Allowed enum values: `character_delimited`
object
Byte frames according to the octet counting format as per RFC6587.
method [_required_]
enum
Byte frames according to the octet counting format as per RFC6587. Allowed enum values: `octet_counting`
object
Byte frames which are chunked GELF messages.
method [_required_]
enum
Byte frames which are chunked GELF messages. Allowed enum values: `chunked_gelf`
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
mode [_required_]
enum
Protocol used to receive logs. Allowed enum values: `tcp,udp`
object
TLS configuration. Relevant only when `mode` is `tcp`.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `socket`. Allowed enum values: `socket`
default: `socket`
name [_required_]
string
Name of the pipeline.
id [_required_]
string
Unique identifier for the pipeline.
type [_required_]
string
The resource type identifier. For pipeline resources, this should always be set to `pipelines`.
default: `pipelines`
```
{
  "data": {
    "attributes": {
      "config": {
        "destinations": [
          {
            "id": "datadog-logs-destination",
            "inputs": [
              "filter-processor"
            ],
            "type": "datadog_logs"
          }
        ],
        "processors": [
          {
            "display_name": "my component",
            "enabled": true,
            "id": "grouped-processors",
            "include": "service:my-service",
            "inputs": [
              "datadog-agent-source"
            ],
            "processors": [
              []
            ]
          }
        ],
        "sources": [
          {
            "group_id": "consumer-group-0",
            "id": "kafka-source",
            "librdkafka_options": [
              {
                "name": "fetch.message.max.bytes",
                "value": "1048576"
              }
            ],
            "sasl": {
              "mechanism": "string"
            },
            "tls": {
              "ca_file": "string",
              "crt_file": "/path/to/cert.crt",
              "key_file": "string"
            },
            "topics": [
              "topic1",
              "topic2"
            ],
            "type": "kafka"
          }
        ]
      },
      "name": "Main Observability Pipeline"
    },
    "id": "3fa85f64-5717-4562-b3fc-2c963f66afa6",
    "type": "pipelines"
  }
}
```

Copy
Bad Request
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


API error response.
Expand All
Field
Type
Description
errors [_required_]
[string]
A list of errors.
```
{
  "errors": [
    "Bad Request"
  ]
}
```

Copy
Not Authorized
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


API error response.
Expand All
Field
Type
Description
errors [_required_]
[string]
A list of errors.
```
{
  "errors": [
    "Bad Request"
  ]
}
```

Copy
Not Found
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


API error response.
Expand All
Field
Type
Description
errors [_required_]
[string]
A list of errors.
```
{
  "errors": [
    "Bad Request"
  ]
}
```

Copy
Conflict
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


API error response.
Expand All
Field
Type
Description
errors [_required_]
[string]
A list of errors.
```
{
  "errors": [
    "Bad Request"
  ]
}
```

Copy
Too many requests
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


API error response.
Expand All
Field
Type
Description
errors [_required_]
[string]
A list of errors.
```
{
  "errors": [
    "Bad Request"
  ]
}
```

Copy
### Code Example
  * [Curl](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=curl)
  * [Go](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=go)
  * [Java](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=java)
  * [Python](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=python)
  * [Ruby](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=ruby)
  * [Rust](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=rust)
  * [Typescript](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=typescript)


#####  Update a pipeline returns "OK" response
Copy
```
                          # Path parameters  
export pipeline_id="CHANGE_ME"  
# Curl command  
curl -X PUT "https://api.ap1.datadoghq.com"https://api.ap2.datadoghq.com"https://api.datadoghq.eu"https://api.ddog-gov.com"https://api.datadoghq.com"https://api.us3.datadoghq.com"https://api.us5.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelines/${pipeline_id}" \
-H "Accept: application/json" \
-H "Content-Type: application/json" \
-H "DD-API-KEY: ${DD_API_KEY}" \
-H "DD-APPLICATION-KEY: ${DD_APP_KEY}" \
-d @- << EOF
{
  "data": {
    "attributes": {
      "config": {
        "destinations": [
          {
            "id": "updated-datadog-logs-destination-id",
            "inputs": [
              "my-processor-group"
            ],
            "type": "datadog_logs"
          }
        ],
        "processors": [
          {
            "enabled": true,
            "id": "my-processor-group",
            "include": "service:my-service",
            "inputs": [
              "datadog-agent-source"
            ],
            "processors": [
              {
                "enabled": true,
                "id": "filter-processor",
                "include": "status:error",
                "type": "filter"
              }
            ]
          }
        ],
        "sources": [
          {
            "id": "datadog-agent-source",
            "type": "datadog_agent"
          }
        ]
      },
      "name": "Updated Pipeline Name"
    },
    "id": "3fa85f64-5717-4562-b3fc-2c963f66afa6",
    "type": "pipelines"
  }
}
EOF  

                        
```

#####  Update a pipeline returns "OK" response
```
// Update a pipeline returns "OK" response

package main

import (
	"context"
	"encoding/json"
	"fmt"
	"os"

	"github.com/DataDog/datadog-api-client-go/v2/api/datadog"
	"github.com/DataDog/datadog-api-client-go/v2/api/datadogV2"
)

func main() {
	// there is a valid "pipeline" in the system
	PipelineDataID := os.Getenv("PIPELINE_DATA_ID")

	body := datadogV2.ObservabilityPipeline{
		Data: datadogV2.ObservabilityPipelineData{
			Attributes: datadogV2.ObservabilityPipelineDataAttributes{
				Config: datadogV2.ObservabilityPipelineConfig{
					Destinations: []datadogV2.ObservabilityPipelineConfigDestinationItem{
						datadogV2.ObservabilityPipelineConfigDestinationItem{
							ObservabilityPipelineDatadogLogsDestination: &datadogV2.ObservabilityPipelineDatadogLogsDestination{
								Id: "updated-datadog-logs-destination-id",
								Inputs: []string{
									"my-processor-group",
								},
								Type: datadogV2.OBSERVABILITYPIPELINEDATADOGLOGSDESTINATIONTYPE_DATADOG_LOGS,
							}},
					},
					Processors: []datadogV2.ObservabilityPipelineConfigProcessorGroup{
						{
							Enabled: true,
							Id:      "my-processor-group",
							Include: "service:my-service",
							Inputs: []string{
								"datadog-agent-source",
							},
							Processors: []datadogV2.ObservabilityPipelineConfigProcessorItem{
								datadogV2.ObservabilityPipelineConfigProcessorItem{
									ObservabilityPipelineFilterProcessor: &datadogV2.ObservabilityPipelineFilterProcessor{
										Enabled: true,
										Id:      "filter-processor",
										Include: "status:error",
										Type:    datadogV2.OBSERVABILITYPIPELINEFILTERPROCESSORTYPE_FILTER,
									}},
							},
						},
					},
					Sources: []datadogV2.ObservabilityPipelineConfigSourceItem{
						datadogV2.ObservabilityPipelineConfigSourceItem{
							ObservabilityPipelineDatadogAgentSource: &datadogV2.ObservabilityPipelineDatadogAgentSource{
								Id:   "datadog-agent-source",
								Type: datadogV2.OBSERVABILITYPIPELINEDATADOGAGENTSOURCETYPE_DATADOG_AGENT,
							}},
					},
				},
				Name: "Updated Pipeline Name",
			},
			Id:   PipelineDataID,
			Type: "pipelines",
		},
	}
	ctx := datadog.NewDefaultContext(context.Background())
	configuration := datadog.NewConfiguration()
	configuration.SetUnstableOperationEnabled("v2.UpdatePipeline", true)
	apiClient := datadog.NewAPIClient(configuration)
	api := datadogV2.NewObservabilityPipelinesApi(apiClient)
	resp, r, err := api.UpdatePipeline(ctx, PipelineDataID, body)

	if err != nil {
		fmt.Fprintf(os.Stderr, "Error when calling `ObservabilityPipelinesApi.UpdatePipeline`: %v\n", err)
		fmt.Fprintf(os.Stderr, "Full HTTP response: %v\n", r)
	}

	responseContent, _ := json.MarshalIndent(resp, "", "  ")
	fmt.Fprintf(os.Stdout, "Response from `ObservabilityPipelinesApi.UpdatePipeline`:\n%s\n", responseContent)
}

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=go) and then save the example to `main.go` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" go run "main.go"


```

#####  Update a pipeline returns "OK" response
```
// Update a pipeline returns "OK" response

import com.datadog.api.client.ApiClient;
import com.datadog.api.client.ApiException;
import com.datadog.api.client.v2.api.ObservabilityPipelinesApi;
import com.datadog.api.client.v2.model.ObservabilityPipeline;
import com.datadog.api.client.v2.model.ObservabilityPipelineConfig;
import com.datadog.api.client.v2.model.ObservabilityPipelineConfigDestinationItem;
import com.datadog.api.client.v2.model.ObservabilityPipelineConfigProcessorGroup;
import com.datadog.api.client.v2.model.ObservabilityPipelineConfigProcessorItem;
import com.datadog.api.client.v2.model.ObservabilityPipelineConfigSourceItem;
import com.datadog.api.client.v2.model.ObservabilityPipelineData;
import com.datadog.api.client.v2.model.ObservabilityPipelineDataAttributes;
import com.datadog.api.client.v2.model.ObservabilityPipelineDatadogAgentSource;
import com.datadog.api.client.v2.model.ObservabilityPipelineDatadogAgentSourceType;
import com.datadog.api.client.v2.model.ObservabilityPipelineDatadogLogsDestination;
import com.datadog.api.client.v2.model.ObservabilityPipelineDatadogLogsDestinationType;
import com.datadog.api.client.v2.model.ObservabilityPipelineFilterProcessor;
import com.datadog.api.client.v2.model.ObservabilityPipelineFilterProcessorType;
import java.util.Collections;

public class Example {
  public static void main(String[] args) {
    ApiClient defaultClient = ApiClient.getDefaultApiClient();
    defaultClient.setUnstableOperationEnabled("v2.updatePipeline", true);
    ObservabilityPipelinesApi apiInstance = new ObservabilityPipelinesApi(defaultClient);

    // there is a valid "pipeline" in the system
    String PIPELINE_DATA_ID = System.getenv("PIPELINE_DATA_ID");

    ObservabilityPipeline body =
        new ObservabilityPipeline()
            .data(
                new ObservabilityPipelineData()
                    .attributes(
                        new ObservabilityPipelineDataAttributes()
                            .config(
                                new ObservabilityPipelineConfig()
                                    .destinations(
                                        Collections.singletonList(
                                            new ObservabilityPipelineConfigDestinationItem(
                                                new ObservabilityPipelineDatadogLogsDestination()
                                                    .id("updated-datadog-logs-destination-id")
                                                    .inputs(
                                                        Collections.singletonList(
                                                            "my-processor-group"))
                                                    .type(
                                                        ObservabilityPipelineDatadogLogsDestinationType
                                                            .DATADOG_LOGS))))
                                    .processors(
                                        Collections.singletonList(
                                            new ObservabilityPipelineConfigProcessorGroup()
                                                .enabled(true)
                                                .id("my-processor-group")
                                                .include("service:my-service")
                                                .inputs(
                                                    Collections.singletonList(
                                                        "datadog-agent-source"))
                                                .processors(
                                                    Collections.singletonList(
                                                        new ObservabilityPipelineConfigProcessorItem(
                                                            new ObservabilityPipelineFilterProcessor()
                                                                .enabled(true)
                                                                .id("filter-processor")
                                                                .include("status:error")
                                                                .type(
                                                                    ObservabilityPipelineFilterProcessorType
                                                                        .FILTER))))))
                                    .sources(
                                        Collections.singletonList(
                                            new ObservabilityPipelineConfigSourceItem(
                                                new ObservabilityPipelineDatadogAgentSource()
                                                    .id("datadog-agent-source")
                                                    .type(
                                                        ObservabilityPipelineDatadogAgentSourceType
                                                            .DATADOG_AGENT)))))
                            .name("Updated Pipeline Name"))
                    .id(PIPELINE_DATA_ID)
                    .type("pipelines"));

    try {
      ObservabilityPipeline result = apiInstance.updatePipeline(PIPELINE_DATA_ID, body);
      System.out.println(result);
    } catch (ApiException e) {
      System.err.println("Exception when calling ObservabilityPipelinesApi#updatePipeline");
      System.err.println("Status code: " + e.getCode());
      System.err.println("Reason: " + e.getResponseBody());
      System.err.println("Response headers: " + e.getResponseHeaders());
      e.printStackTrace();
    }
  }
}

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=java) and then save the example to `Example.java` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" java "Example.java"


```

#####  Update a pipeline returns "OK" response
```
"""
Update a pipeline returns "OK" response
"""

from os import environ
from datadog_api_client import ApiClient, Configuration
from datadog_api_client.v2.api.observability_pipelines_api import ObservabilityPipelinesApi
from datadog_api_client.v2.model.observability_pipeline import ObservabilityPipeline
from datadog_api_client.v2.model.observability_pipeline_config import ObservabilityPipelineConfig
from datadog_api_client.v2.model.observability_pipeline_config_processor_group import (
    ObservabilityPipelineConfigProcessorGroup,
)
from datadog_api_client.v2.model.observability_pipeline_data import ObservabilityPipelineData
from datadog_api_client.v2.model.observability_pipeline_data_attributes import ObservabilityPipelineDataAttributes
from datadog_api_client.v2.model.observability_pipeline_datadog_agent_source import (
    ObservabilityPipelineDatadogAgentSource,
)
from datadog_api_client.v2.model.observability_pipeline_datadog_agent_source_type import (
    ObservabilityPipelineDatadogAgentSourceType,
)
from datadog_api_client.v2.model.observability_pipeline_datadog_logs_destination import (
    ObservabilityPipelineDatadogLogsDestination,
)
from datadog_api_client.v2.model.observability_pipeline_datadog_logs_destination_type import (
    ObservabilityPipelineDatadogLogsDestinationType,
)
from datadog_api_client.v2.model.observability_pipeline_filter_processor import ObservabilityPipelineFilterProcessor
from datadog_api_client.v2.model.observability_pipeline_filter_processor_type import (
    ObservabilityPipelineFilterProcessorType,
)

# there is a valid "pipeline" in the system
PIPELINE_DATA_ID = environ["PIPELINE_DATA_ID"]

body = ObservabilityPipeline(
    data=ObservabilityPipelineData(
        attributes=ObservabilityPipelineDataAttributes(
            config=ObservabilityPipelineConfig(
                destinations=[
                    ObservabilityPipelineDatadogLogsDestination(
                        id="updated-datadog-logs-destination-id",
                        inputs=[
                            "my-processor-group",
                        ],
                        type=ObservabilityPipelineDatadogLogsDestinationType.DATADOG_LOGS,
                    ),
                ],
                processors=[
                    ObservabilityPipelineConfigProcessorGroup(
                        enabled=True,
                        id="my-processor-group",
                        include="service:my-service",
                        inputs=[
                            "datadog-agent-source",
                        ],
                        processors=[
                            ObservabilityPipelineFilterProcessor(
                                enabled=True,
                                id="filter-processor",
                                include="status:error",
                                type=ObservabilityPipelineFilterProcessorType.FILTER,
                            ),
                        ],
                    ),
                ],
                sources=[
                    ObservabilityPipelineDatadogAgentSource(
                        id="datadog-agent-source",
                        type=ObservabilityPipelineDatadogAgentSourceType.DATADOG_AGENT,
                    ),
                ],
            ),
            name="Updated Pipeline Name",
        ),
        id=PIPELINE_DATA_ID,
        type="pipelines",
    ),
)

configuration = Configuration()
configuration.unstable_operations["update_pipeline"] = True
with ApiClient(configuration) as api_client:
    api_instance = ObservabilityPipelinesApi(api_client)
    response = api_instance.update_pipeline(pipeline_id=PIPELINE_DATA_ID, body=body)

    print(response)

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=python) and then save the example to `example.py` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" python3 "example.py"


```

#####  Update a pipeline returns "OK" response
```
# Update a pipeline returns "OK" response

require "datadog_api_client"
DatadogAPIClient.configure do |config|
  config.unstable_operations["v2.update_pipeline".to_sym] = true
end
api_instance = DatadogAPIClient::V2::ObservabilityPipelinesAPI.new

# there is a valid "pipeline" in the system
PIPELINE_DATA_ID = ENV["PIPELINE_DATA_ID"]

body = DatadogAPIClient::V2::ObservabilityPipeline.new({
  data: DatadogAPIClient::V2::ObservabilityPipelineData.new({
    attributes: DatadogAPIClient::V2::ObservabilityPipelineDataAttributes.new({
      config: DatadogAPIClient::V2::ObservabilityPipelineConfig.new({
        destinations: [
          DatadogAPIClient::V2::ObservabilityPipelineDatadogLogsDestination.new({
            id: "updated-datadog-logs-destination-id",
            inputs: [
              "my-processor-group",
            ],
            type: DatadogAPIClient::V2::ObservabilityPipelineDatadogLogsDestinationType::DATADOG_LOGS,
          }),
        ],
        processors: [
          DatadogAPIClient::V2::ObservabilityPipelineConfigProcessorGroup.new({
            enabled: true,
            id: "my-processor-group",
            include: "service:my-service",
            inputs: [
              "datadog-agent-source",
            ],
            processors: [
              DatadogAPIClient::V2::ObservabilityPipelineFilterProcessor.new({
                enabled: true,
                id: "filter-processor",
                include: "status:error",
                type: DatadogAPIClient::V2::ObservabilityPipelineFilterProcessorType::FILTER,
              }),
            ],
          }),
        ],
        sources: [
          DatadogAPIClient::V2::ObservabilityPipelineDatadogAgentSource.new({
            id: "datadog-agent-source",
            type: DatadogAPIClient::V2::ObservabilityPipelineDatadogAgentSourceType::DATADOG_AGENT,
          }),
        ],
      }),
      name: "Updated Pipeline Name",
    }),
    id: PIPELINE_DATA_ID,
    type: "pipelines",
  }),
})
p api_instance.update_pipeline(PIPELINE_DATA_ID, body)

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=ruby) and then save the example to `example.rb` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" rb "example.rb"


```

#####  Update a pipeline returns "OK" response
```
// Update a pipeline returns "OK" response
use datadog_api_client::datadog;
use datadog_api_client::datadogV2::api_observability_pipelines::ObservabilityPipelinesAPI;
use datadog_api_client::datadogV2::model::ObservabilityPipeline;
use datadog_api_client::datadogV2::model::ObservabilityPipelineConfig;
use datadog_api_client::datadogV2::model::ObservabilityPipelineConfigDestinationItem;
use datadog_api_client::datadogV2::model::ObservabilityPipelineConfigProcessorGroup;
use datadog_api_client::datadogV2::model::ObservabilityPipelineConfigProcessorItem;
use datadog_api_client::datadogV2::model::ObservabilityPipelineConfigSourceItem;
use datadog_api_client::datadogV2::model::ObservabilityPipelineData;
use datadog_api_client::datadogV2::model::ObservabilityPipelineDataAttributes;
use datadog_api_client::datadogV2::model::ObservabilityPipelineDatadogAgentSource;
use datadog_api_client::datadogV2::model::ObservabilityPipelineDatadogAgentSourceType;
use datadog_api_client::datadogV2::model::ObservabilityPipelineDatadogLogsDestination;
use datadog_api_client::datadogV2::model::ObservabilityPipelineDatadogLogsDestinationType;
use datadog_api_client::datadogV2::model::ObservabilityPipelineFilterProcessor;
use datadog_api_client::datadogV2::model::ObservabilityPipelineFilterProcessorType;

#[tokio::main]
async fn main() {
    // there is a valid "pipeline" in the system
    let pipeline_data_id = std::env::var("PIPELINE_DATA_ID").unwrap();
    let body =
        ObservabilityPipeline::new(
            ObservabilityPipelineData::new(
                ObservabilityPipelineDataAttributes::new(
                    ObservabilityPipelineConfig::new(
                        vec![
                            ObservabilityPipelineConfigDestinationItem::ObservabilityPipelineDatadogLogsDestination(
                                Box::new(
                                    ObservabilityPipelineDatadogLogsDestination::new(
                                        "updated-datadog-logs-destination-id".to_string(),
                                        vec!["my-processor-group".to_string()],
                                        ObservabilityPipelineDatadogLogsDestinationType::DATADOG_LOGS,
                                    ),
                                ),
                            )
                        ],
                        vec![
                            ObservabilityPipelineConfigSourceItem::ObservabilityPipelineDatadogAgentSource(
                                Box::new(
                                    ObservabilityPipelineDatadogAgentSource::new(
                                        "datadog-agent-source".to_string(),
                                        ObservabilityPipelineDatadogAgentSourceType::DATADOG_AGENT,
                                    ),
                                ),
                            )
                        ],
                    ).processors(
                        vec![
                            ObservabilityPipelineConfigProcessorGroup::new(
                                true,
                                "my-processor-group".to_string(),
                                "service:my-service".to_string(),
                                vec!["datadog-agent-source".to_string()],
                                vec![
                                    ObservabilityPipelineConfigProcessorItem::ObservabilityPipelineFilterProcessor(
                                        Box::new(
                                            ObservabilityPipelineFilterProcessor::new(
                                                true,
                                                "filter-processor".to_string(),
                                                "status:error".to_string(),
                                                ObservabilityPipelineFilterProcessorType::FILTER,
                                            ),
                                        ),
                                    )
                                ],
                            )
                        ],
                    ),
                    "Updated Pipeline Name".to_string(),
                ),
                pipeline_data_id.clone(),
                "pipelines".to_string(),
            ),
        );
    let mut configuration = datadog::Configuration::new();
    configuration.set_unstable_operation_enabled("v2.UpdatePipeline", true);
    let api = ObservabilityPipelinesAPI::with_config(configuration);
    let resp = api.update_pipeline(pipeline_data_id.clone(), body).await;
    if let Ok(value) = resp {
        println!("{:#?}", value);
    } else {
        println!("{:#?}", resp.unwrap_err());
    }
}

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=rust) and then save the example to `src/main.rs` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" cargo run


```

#####  Update a pipeline returns "OK" response
```
/**
 * Update a pipeline returns "OK" response
 */

import { client, v2 } from "@datadog/datadog-api-client";

const configuration = client.createConfiguration();
configuration.unstableOperations["v2.updatePipeline"] = true;
const apiInstance = new v2.ObservabilityPipelinesApi(configuration);

// there is a valid "pipeline" in the system
const PIPELINE_DATA_ID = process.env.PIPELINE_DATA_ID as string;

const params: v2.ObservabilityPipelinesApiUpdatePipelineRequest = {
  body: {
    data: {
      attributes: {
        config: {
          destinations: [
            {
              id: "updated-datadog-logs-destination-id",
              inputs: ["my-processor-group"],
              type: "datadog_logs",
            },
          ],
          processors: [
            {
              enabled: true,
              id: "my-processor-group",
              include: "service:my-service",
              inputs: ["datadog-agent-source"],
              processors: [
                {
                  enabled: true,
                  id: "filter-processor",
                  include: "status:error",
                  type: "filter",
                },
              ],
            },
          ],
          sources: [
            {
              id: "datadog-agent-source",
              type: "datadog_agent",
            },
          ],
        },
        name: "Updated Pipeline Name",
      },
      id: PIPELINE_DATA_ID,
      type: "pipelines",
    },
  },
  pipelineId: PIPELINE_DATA_ID,
};

apiInstance
  .updatePipeline(params)
  .then((data: v2.ObservabilityPipeline) => {
    console.log(
      "API called successfully. Returned data: " + JSON.stringify(data)
    );
  })
  .catch((error: any) => console.error(error));

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=typescript) and then save the example to `example.ts` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" tsc "example.ts"


```

* * *
## [Delete a pipeline](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-pipeline)
  * [v2 (latest)](https://docs.datadoghq.com/api/latest/observability-pipelines/#delete-a-pipeline-v2)


**Note** : This endpoint is in Preview. Fill out this [form](https://www.datadoghq.com/product-preview/observability-pipelines-api-and-terraform-support/) to request access.
DELETE https://api.ap1.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelines/{pipeline_id}https://api.ap2.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelines/{pipeline_id}https://api.datadoghq.eu/api/v2/remote_config/products/obs_pipelines/pipelines/{pipeline_id}https://api.ddog-gov.com/api/v2/remote_config/products/obs_pipelines/pipelines/{pipeline_id}https://api.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelines/{pipeline_id}https://api.us3.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelines/{pipeline_id}https://api.us5.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelines/{pipeline_id}
### Overview
Delete a pipeline. This endpoint requires the `observability_pipelines_delete` permission.
### Arguments
#### Path Parameters
Name
Type
Description
pipeline_id [_required_]
string
The ID of the pipeline to delete.
### Response
  * [204](https://docs.datadoghq.com/api/latest/observability-pipelines/#DeletePipeline-204-v2)
  * [403](https://docs.datadoghq.com/api/latest/observability-pipelines/#DeletePipeline-403-v2)
  * [404](https://docs.datadoghq.com/api/latest/observability-pipelines/#DeletePipeline-404-v2)
  * [409](https://docs.datadoghq.com/api/latest/observability-pipelines/#DeletePipeline-409-v2)
  * [429](https://docs.datadoghq.com/api/latest/observability-pipelines/#DeletePipeline-429-v2)


OK
Forbidden
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


API error response.
Expand All
Field
Type
Description
errors [_required_]
[string]
A list of errors.
```
{
  "errors": [
    "Bad Request"
  ]
}
```

Copy
Not Found
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


API error response.
Expand All
Field
Type
Description
errors [_required_]
[string]
A list of errors.
```
{
  "errors": [
    "Bad Request"
  ]
}
```

Copy
Conflict
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


API error response.
Expand All
Field
Type
Description
errors [_required_]
[string]
A list of errors.
```
{
  "errors": [
    "Bad Request"
  ]
}
```

Copy
Too many requests
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


API error response.
Expand All
Field
Type
Description
errors [_required_]
[string]
A list of errors.
```
{
  "errors": [
    "Bad Request"
  ]
}
```

Copy
### Code Example
  * [Curl](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=curl)
  * [Python](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=python)
  * [Ruby](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=ruby)
  * [Go](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=go)
  * [Java](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=java)
  * [Rust](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=rust)
  * [Typescript](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=typescript)


#####  Delete a pipeline
Copy
```
                  # Path parameters  
export pipeline_id="CHANGE_ME"  
# Curl command  
curl -X DELETE "https://api.ap1.datadoghq.com"https://api.ap2.datadoghq.com"https://api.datadoghq.eu"https://api.ddog-gov.com"https://api.datadoghq.com"https://api.us3.datadoghq.com"https://api.us5.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelines/${pipeline_id}" \
-H "DD-API-KEY: ${DD_API_KEY}" \
-H "DD-APPLICATION-KEY: ${DD_APP_KEY}"  

                
```

#####  Delete a pipeline
```
"""
Delete a pipeline returns "OK" response
"""

from os import environ
from datadog_api_client import ApiClient, Configuration
from datadog_api_client.v2.api.observability_pipelines_api import ObservabilityPipelinesApi

# there is a valid "pipeline" in the system
PIPELINE_DATA_ID = environ["PIPELINE_DATA_ID"]

configuration = Configuration()
configuration.unstable_operations["delete_pipeline"] = True
with ApiClient(configuration) as api_client:
    api_instance = ObservabilityPipelinesApi(api_client)
    api_instance.delete_pipeline(
        pipeline_id=PIPELINE_DATA_ID,
    )

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=python) and then save the example to `example.py` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" python3 "example.py"


```

#####  Delete a pipeline
```
# Delete a pipeline returns "OK" response

require "datadog_api_client"
DatadogAPIClient.configure do |config|
  config.unstable_operations["v2.delete_pipeline".to_sym] = true
end
api_instance = DatadogAPIClient::V2::ObservabilityPipelinesAPI.new

# there is a valid "pipeline" in the system
PIPELINE_DATA_ID = ENV["PIPELINE_DATA_ID"]
api_instance.delete_pipeline(PIPELINE_DATA_ID)

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=ruby) and then save the example to `example.rb` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" rb "example.rb"


```

#####  Delete a pipeline
```
// Delete a pipeline returns "OK" response

package main

import (
	"context"
	"fmt"
	"os"

	"github.com/DataDog/datadog-api-client-go/v2/api/datadog"
	"github.com/DataDog/datadog-api-client-go/v2/api/datadogV2"
)

func main() {
	// there is a valid "pipeline" in the system
	PipelineDataID := os.Getenv("PIPELINE_DATA_ID")

	ctx := datadog.NewDefaultContext(context.Background())
	configuration := datadog.NewConfiguration()
	configuration.SetUnstableOperationEnabled("v2.DeletePipeline", true)
	apiClient := datadog.NewAPIClient(configuration)
	api := datadogV2.NewObservabilityPipelinesApi(apiClient)
	r, err := api.DeletePipeline(ctx, PipelineDataID)

	if err != nil {
		fmt.Fprintf(os.Stderr, "Error when calling `ObservabilityPipelinesApi.DeletePipeline`: %v\n", err)
		fmt.Fprintf(os.Stderr, "Full HTTP response: %v\n", r)
	}
}

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=go) and then save the example to `main.go` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" go run "main.go"


```

#####  Delete a pipeline
```
// Delete a pipeline returns "OK" response

import com.datadog.api.client.ApiClient;
import com.datadog.api.client.ApiException;
import com.datadog.api.client.v2.api.ObservabilityPipelinesApi;

public class Example {
  public static void main(String[] args) {
    ApiClient defaultClient = ApiClient.getDefaultApiClient();
    defaultClient.setUnstableOperationEnabled("v2.deletePipeline", true);
    ObservabilityPipelinesApi apiInstance = new ObservabilityPipelinesApi(defaultClient);

    // there is a valid "pipeline" in the system
    String PIPELINE_DATA_ID = System.getenv("PIPELINE_DATA_ID");

    try {
      apiInstance.deletePipeline(PIPELINE_DATA_ID);
    } catch (ApiException e) {
      System.err.println("Exception when calling ObservabilityPipelinesApi#deletePipeline");
      System.err.println("Status code: " + e.getCode());
      System.err.println("Reason: " + e.getResponseBody());
      System.err.println("Response headers: " + e.getResponseHeaders());
      e.printStackTrace();
    }
  }
}

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=java) and then save the example to `Example.java` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" java "Example.java"


```

#####  Delete a pipeline
```
// Delete a pipeline returns "OK" response
use datadog_api_client::datadog;
use datadog_api_client::datadogV2::api_observability_pipelines::ObservabilityPipelinesAPI;

#[tokio::main]
async fn main() {
    // there is a valid "pipeline" in the system
    let pipeline_data_id = std::env::var("PIPELINE_DATA_ID").unwrap();
    let mut configuration = datadog::Configuration::new();
    configuration.set_unstable_operation_enabled("v2.DeletePipeline", true);
    let api = ObservabilityPipelinesAPI::with_config(configuration);
    let resp = api.delete_pipeline(pipeline_data_id.clone()).await;
    if let Ok(value) = resp {
        println!("{:#?}", value);
    } else {
        println!("{:#?}", resp.unwrap_err());
    }
}

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=rust) and then save the example to `src/main.rs` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" cargo run


```

#####  Delete a pipeline
```
/**
 * Delete a pipeline returns "OK" response
 */

import { client, v2 } from "@datadog/datadog-api-client";

const configuration = client.createConfiguration();
configuration.unstableOperations["v2.deletePipeline"] = true;
const apiInstance = new v2.ObservabilityPipelinesApi(configuration);

// there is a valid "pipeline" in the system
const PIPELINE_DATA_ID = process.env.PIPELINE_DATA_ID as string;

const params: v2.ObservabilityPipelinesApiDeletePipelineRequest = {
  pipelineId: PIPELINE_DATA_ID,
};

apiInstance
  .deletePipeline(params)
  .then((data: any) => {
    console.log(
      "API called successfully. Returned data: " + JSON.stringify(data)
    );
  })
  .catch((error: any) => console.error(error));

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=typescript) and then save the example to `example.ts` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" tsc "example.ts"


```

* * *
## [Validate an observability pipeline](https://docs.datadoghq.com/api/latest/observability-pipelines/#validate-an-observability-pipeline)
  * [v2 (latest)](https://docs.datadoghq.com/api/latest/observability-pipelines/#validate-an-observability-pipeline-v2)


**Note** : This endpoint is in Preview. Fill out this [form](https://www.datadoghq.com/product-preview/observability-pipelines-api-and-terraform-support/) to request access.
POST https://api.ap1.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelines/validatehttps://api.ap2.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelines/validatehttps://api.datadoghq.eu/api/v2/remote_config/products/obs_pipelines/pipelines/validatehttps://api.ddog-gov.com/api/v2/remote_config/products/obs_pipelines/pipelines/validatehttps://api.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelines/validatehttps://api.us3.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelines/validatehttps://api.us5.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelines/validate
### Overview
Validates a pipeline configuration without creating or updating any resources. Returns a list of validation errors, if any. This endpoint requires the `observability_pipelines_read` permission.
### Request
#### Body Data (required)
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


Expand All
Field
Type
Description
_required_]
object
Contains the the pipeline configuration.
_required_]
object
Defines the pipeline’s name and its components (sources, processors, and destinations).
_required_]
object
Specifies the pipeline's configuration, including its sources, processors, and destinations.
_required_]
[ <oneOf>]
A list of destination components where processed logs are sent.
object
The `datadog_logs` destination forwards logs to Datadog Log Management.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `datadog_logs`. Allowed enum values: `datadog_logs`
default: `datadog_logs`
object
The `amazon_s3` destination sends your logs in Datadog-rehydratable format to an Amazon S3 bucket for archiving.
object
AWS authentication credentials used for accessing AWS services such as S3. If omitted, the system’s default credentials are used (for example, the IAM role and environment variables).
assume_role
string
The Amazon Resource Name (ARN) of the role to assume.
external_id
string
A unique identifier for cross-account role assumption.
session_name
string
A session identifier used for logging and tracing the assumed role session.
bucket [_required_]
string
S3 bucket name.
id [_required_]
string
Unique identifier for the destination component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
key_prefix
string
Optional prefix for object keys.
region [_required_]
string
AWS region of the S3 bucket.
storage_class [_required_]
enum
S3 storage class. Allowed enum values: `STANDARD,REDUCED_REDUNDANCY,INTELLIGENT_TIERING,STANDARD_IA,EXPRESS_ONEZONE,ONEZONE_IA,GLACIER,GLACIER_IR,DEEP_ARCHIVE`
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. Always `amazon_s3`. Allowed enum values: `amazon_s3`
default: `amazon_s3`
object
The `google_cloud_storage` destination stores logs in a Google Cloud Storage (GCS) bucket. It requires a bucket name, GCP authentication, and metadata fields.
acl
enum
Access control list setting for objects written to the bucket. Allowed enum values: `private,project-private,public-read,authenticated-read,bucket-owner-read,bucket-owner-full-control`
object
GCP credentials used to authenticate with Google Cloud Storage.
credentials_file [_required_]
string
Path to the GCP service account key file.
bucket [_required_]
string
Name of the GCS bucket.
id [_required_]
string
Unique identifier for the destination component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
key_prefix
string
Optional prefix for object keys within the GCS bucket.
[object]
Custom metadata to attach to each object uploaded to the GCS bucket.
name [_required_]
string
The metadata key.
value [_required_]
string
The metadata value.
storage_class [_required_]
enum
Storage class used for objects stored in GCS. Allowed enum values: `STANDARD,NEARLINE,COLDLINE,ARCHIVE`
type [_required_]
enum
The destination type. Always `google_cloud_storage`. Allowed enum values: `google_cloud_storage`
default: `google_cloud_storage`
object
The `splunk_hec` destination forwards logs to Splunk using the HTTP Event Collector (HEC).
auto_extract_timestamp
boolean
If `true`, Splunk tries to extract timestamps from incoming log events. If `false`, Splunk assigns the time the event was received.
encoding
enum
Encoding format for log events. Allowed enum values: `json,raw_message`
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
index
string
Optional name of the Splunk index where logs are written.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
sourcetype
string
The Splunk sourcetype to assign to log events.
type [_required_]
enum
The destination type. Always `splunk_hec`. Allowed enum values: `splunk_hec`
default: `splunk_hec`
object
The `sumo_logic` destination forwards logs to Sumo Logic.
encoding
enum
The output encoding format. Allowed enum values: `json,raw_message,logfmt`
[object]
A list of custom headers to include in the request to Sumo Logic.
name [_required_]
string
The header field name.
value [_required_]
string
The header field value.
header_host_name
string
Optional override for the host name header.
header_source_category
string
Optional override for the source category header.
header_source_name
string
Optional override for the source name header.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `sumo_logic`. Allowed enum values: `sumo_logic`
default: `sumo_logic`
object
The `elasticsearch` destination writes logs to an Elasticsearch cluster.
api_version
enum
The Elasticsearch API version to use. Set to `auto` to auto-detect. Allowed enum values: `auto,v6,v7,v8`
bulk_index
string
The index to write logs to in Elasticsearch.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `elasticsearch`. Allowed enum values: `elasticsearch`
default: `elasticsearch`
object
The `rsyslog` destination forwards logs to an external `rsyslog` server over TCP or UDP using the syslog protocol.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
keepalive
int64
Optional socket keepalive duration in milliseconds.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. The value should always be `rsyslog`. Allowed enum values: `rsyslog`
default: `rsyslog`
object
The `syslog_ng` destination forwards logs to an external `syslog-ng` server over TCP or UDP using the syslog protocol.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
keepalive
int64
Optional socket keepalive duration in milliseconds.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. The value should always be `syslog_ng`. Allowed enum values: `syslog_ng`
default: `syslog_ng`
object
The `azure_storage` destination forwards logs to an Azure Blob Storage container.
blob_prefix
string
Optional prefix for blobs written to the container.
container_name [_required_]
string
The name of the Azure Blob Storage container to store logs in.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `azure_storage`. Allowed enum values: `azure_storage`
default: `azure_storage`
object
The `microsoft_sentinel` destination forwards logs to Microsoft Sentinel.
client_id [_required_]
string
Azure AD client ID used for authentication.
dcr_immutable_id [_required_]
string
The immutable ID of the Data Collection Rule (DCR).
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
table [_required_]
string
The name of the Log Analytics table where logs are sent.
tenant_id [_required_]
string
Azure AD tenant ID.
type [_required_]
enum
The destination type. The value should always be `microsoft_sentinel`. Allowed enum values: `microsoft_sentinel`
default: `microsoft_sentinel`
object
The `google_chronicle` destination sends logs to Google Chronicle.
object
GCP credentials used to authenticate with Google Cloud Storage.
credentials_file [_required_]
string
Path to the GCP service account key file.
customer_id [_required_]
string
The Google Chronicle customer ID.
encoding
enum
The encoding format for the logs sent to Chronicle. Allowed enum values: `json,raw_message`
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
log_type
string
The log type metadata associated with the Chronicle destination.
type [_required_]
enum
The destination type. The value should always be `google_chronicle`. Allowed enum values: `google_chronicle`
default: `google_chronicle`
object
The `new_relic` destination sends logs to the New Relic platform.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
region [_required_]
enum
The New Relic region. Allowed enum values: `us,eu`
type [_required_]
enum
The destination type. The value should always be `new_relic`. Allowed enum values: `new_relic`
default: `new_relic`
object
The `sentinel_one` destination sends logs to SentinelOne.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
region [_required_]
enum
The SentinelOne region to send logs to. Allowed enum values: `us,eu,ca,data_set_us`
type [_required_]
enum
The destination type. The value should always be `sentinel_one`. Allowed enum values: `sentinel_one`
default: `sentinel_one`
object
The `opensearch` destination writes logs to an OpenSearch cluster.
bulk_index
string
The index to write logs to.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `opensearch`. Allowed enum values: `opensearch`
default: `opensearch`
object
The `amazon_opensearch` destination writes logs to Amazon OpenSearch.
_required_]
object
Authentication settings for the Amazon OpenSearch destination. The `strategy` field determines whether basic or AWS-based authentication is used.
assume_role
string
The ARN of the role to assume (used with `aws` strategy).
aws_region
string
AWS region
external_id
string
External ID for the assumed role (used with `aws` strategy).
session_name
string
Session name for the assumed role (used with `aws` strategy).
strategy [_required_]
enum
The authentication strategy to use. Allowed enum values: `basic,aws`
bulk_index
string
The index to write logs to.
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
type [_required_]
enum
The destination type. The value should always be `amazon_opensearch`. Allowed enum values: `amazon_opensearch`
default: `amazon_opensearch`
object
The `socket` destination sends logs over TCP or UDP to a remote server.
encoding [_required_]
enum
Encoding format for log events. Allowed enum values: `json,raw_message`
_required_]
<oneOf>
Framing method configuration.
object
Each log event is delimited by a newline character.
method [_required_]
enum
The definition of `ObservabilityPipelineSocketDestinationFramingNewlineDelimitedMethod` object. Allowed enum values: `newline_delimited`
object
Event data is not delimited at all.
method [_required_]
enum
The definition of `ObservabilityPipelineSocketDestinationFramingBytesMethod` object. Allowed enum values: `bytes`
object
Each log event is separated using the specified delimiter character.
delimiter [_required_]
string
A single ASCII character used as a delimiter.
method [_required_]
enum
The definition of `ObservabilityPipelineSocketDestinationFramingCharacterDelimitedMethod` object. Allowed enum values: `character_delimited`
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
mode [_required_]
enum
Protocol used to send logs. Allowed enum values: `tcp,udp`
object
TLS configuration. Relevant only when `mode` is `tcp`.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. The value should always be `socket`. Allowed enum values: `socket`
default: `socket`
object
The `amazon_security_lake` destination sends your logs to Amazon Security Lake.
object
AWS authentication credentials used for accessing AWS services such as S3. If omitted, the system’s default credentials are used (for example, the IAM role and environment variables).
assume_role
string
The Amazon Resource Name (ARN) of the role to assume.
external_id
string
A unique identifier for cross-account role assumption.
session_name
string
A session identifier used for logging and tracing the assumed role session.
bucket [_required_]
string
Name of the Amazon S3 bucket in Security Lake (3-63 characters).
custom_source_name [_required_]
string
Custom source name for the logs in Security Lake.
id [_required_]
string
Unique identifier for the destination component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
region [_required_]
string
AWS region of the S3 bucket.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. Always `amazon_security_lake`. Allowed enum values: `amazon_security_lake`
default: `amazon_security_lake`
object
The `crowdstrike_next_gen_siem` destination forwards logs to CrowdStrike Next Gen SIEM.
object
Compression configuration for log events.
algorithm [_required_]
enum
Compression algorithm for log events. Allowed enum values: `gzip,zlib`
level
int64
Compression level.
encoding [_required_]
enum
Encoding format for log events. Allowed enum values: `json,raw_message`
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The destination type. The value should always be `crowdstrike_next_gen_siem`. Allowed enum values: `crowdstrike_next_gen_siem`
default: `crowdstrike_next_gen_siem`
object
The `google_pubsub` destination publishes logs to a Google Cloud Pub/Sub topic.
object
GCP credentials used to authenticate with Google Cloud Storage.
credentials_file [_required_]
string
Path to the GCP service account key file.
encoding [_required_]
enum
Encoding format for log events. Allowed enum values: `json,raw_message`
id [_required_]
string
The unique identifier for this component.
inputs [_required_]
[string]
A list of component IDs whose output is used as the `input` for this component.
project [_required_]
string
The GCP project ID that owns the Pub/Sub topic.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
topic [_required_]
string
The Pub/Sub topic name to publish logs to.
type [_required_]
enum
The destination type. The value should always be `google_pubsub`. Allowed enum values: `google_pubsub`
default: `google_pubsub`
[object]
A list of processor groups that transform or enrich log data.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor group is enabled.
id [_required_]
string
The unique identifier for the processor group.
include [_required_]
string
Conditional expression for when this processor group should execute.
inputs [_required_]
[string]
A list of IDs for components whose output is used as the input for this processor group.
_required_]
[ <oneOf>]
Processors applied sequentially within this group. Events flow through each processor in order.
object
The `filter` processor allows conditional processing of logs based on a Datadog search query. Logs that match the `include` query are passed through; others are discarded.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs should pass through the filter. Logs that match this query continue to downstream components; others are dropped.
type [_required_]
enum
The processor type. The value should always be `filter`. Allowed enum values: `filter`
default: `filter`
object
The `parse_json` processor extracts JSON from a specified field and flattens it into the event. This is useful when logs contain embedded JSON as a string.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
field [_required_]
string
The name of the log field that contains a JSON string.
id [_required_]
string
A unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `parse_json`. Allowed enum values: `parse_json`
default: `parse_json`
object
The Quota Processor measures logging traffic for logs that match a specified filter. When the configured daily quota is met, the processor can drop or alert.
display_name
string
The display name for a component.
drop_events
boolean
If set to `true`, logs that matched the quota filter and sent after the quota has been met are dropped; only logs that did not match the filter query continue through the pipeline.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
ignore_when_missing_partitions
boolean
If `true`, the processor skips quota checks when partition fields are missing from the logs.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
object
The maximum amount of data or number of events allowed before the quota is enforced. Can be specified in bytes or events.
enforce [_required_]
enum
Unit for quota enforcement in bytes for data size or events for count. Allowed enum values: `bytes,events`
limit [_required_]
int64
The limit for quota enforcement.
name [_required_]
string
Name of the quota.
overflow_action
enum
The action to take when the quota is exceeded. Options:
  * `drop`: Drop the event.
  * `no_action`: Let the event pass through.
  * `overflow_routing`: Route to an overflow destination. Allowed enum values: `drop,no_action,overflow_routing`


[object]
A list of alternate quota rules that apply to specific sets of events, identified by matching field values. Each override can define a custom limit.
_required_]
[object]
A list of field matchers used to apply a specific override. If an event matches all listed key-value pairs, the corresponding override limit is enforced.
name [_required_]
string
The field name.
value [_required_]
string
The field value.
_required_]
object
The maximum amount of data or number of events allowed before the quota is enforced. Can be specified in bytes or events.
enforce [_required_]
enum
Unit for quota enforcement in bytes for data size or events for count. Allowed enum values: `bytes,events`
limit [_required_]
int64
The limit for quota enforcement.
partition_fields
[string]
A list of fields used to segment log traffic for quota enforcement. Quotas are tracked independently by unique combinations of these field values.
type [_required_]
enum
The processor type. The value should always be `quota`. Allowed enum values: `quota`
default: `quota`
object
The `add_fields` processor adds static key-value fields to logs.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
_required_]
[object]
A list of static fields (key-value pairs) that is added to each log event processed by this component.
name [_required_]
string
The field name.
value [_required_]
string
The field value.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `add_fields`. Allowed enum values: `add_fields`
default: `add_fields`
object
The `remove_fields` processor deletes specified fields from logs.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
fields [_required_]
[string]
A list of field names to be removed from each log event.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `remove_fields`. Allowed enum values: `remove_fields`
default: `remove_fields`
object
The `rename_fields` processor changes field names.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
_required_]
[object]
A list of rename rules specifying which fields to rename in the event, what to rename them to, and whether to preserve the original fields.
destination [_required_]
string
The field name to assign the renamed value to.
preserve_source [_required_]
boolean
Indicates whether the original field, that is received from the source, should be kept (`true`) or removed (`false`) after renaming.
source [_required_]
string
The original field name in the log event that should be renamed.
id [_required_]
string
A unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `rename_fields`. Allowed enum values: `rename_fields`
default: `rename_fields`
object
The `generate_datadog_metrics` processor creates custom metrics from logs and sends them to Datadog. Metrics can be counters, gauges, or distributions and optionally grouped by log fields.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline.
include
string
A Datadog search query used to determine which logs this processor targets.
[object]
Configuration for generating individual metrics.
group_by
[string]
Optional fields used to group the metric series.
include [_required_]
string
Datadog filter query to match logs for metric generation.
metric_type [_required_]
enum
Type of metric to create. Allowed enum values: `count,gauge,distribution`
name [_required_]
string
Name of the custom metric to be created.
_required_]
<oneOf>
Specifies how the value of the generated metric is computed.
object
Strategy that increments a generated metric by one for each matching event.
strategy [_required_]
enum
Increments the metric by 1 for each matching event. Allowed enum values: `increment_by_one`
object
Strategy that increments a generated metric based on the value of a log field.
field [_required_]
string
Name of the log field containing the numeric value to increment the metric by.
strategy [_required_]
enum
Uses a numeric field in the log event as the metric increment. Allowed enum values: `increment_by_field`
type [_required_]
enum
The processor type. Always `generate_datadog_metrics`. Allowed enum values: `generate_datadog_metrics`
default: `generate_datadog_metrics`
object
The `sample` processor allows probabilistic sampling of logs at a fixed rate.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
percentage
double
The percentage of logs to sample.
rate
int64
Number of events to sample (1 in N).
type [_required_]
enum
The processor type. The value should always be `sample`. Allowed enum values: `sample`
default: `sample`
object
The `parse_grok` processor extracts structured fields from unstructured log messages using Grok patterns.
disable_library_rules
boolean
If set to `true`, disables the default Grok rules provided by Datadog.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
A unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
[object]
The list of Grok parsing rules. If multiple matching rules are provided, they are evaluated in order. The first successful match is applied.
_required_]
[object]
A list of Grok parsing rules that define how to extract fields from the source field. Each rule must contain a name and a valid Grok pattern.
name [_required_]
string
The name of the rule.
rule [_required_]
string
The definition of the Grok rule.
source [_required_]
string
The name of the field in the log event to apply the Grok rules to.
[object]
A list of Grok helper rules that can be referenced by the parsing rules.
name [_required_]
string
The name of the Grok helper rule.
rule [_required_]
string
The definition of the Grok helper rule.
type [_required_]
enum
The processor type. The value should always be `parse_grok`. Allowed enum values: `parse_grok`
default: `parse_grok`
object
The `sensitive_data_scanner` processor detects and optionally redacts sensitive data in log events.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
[object]
A list of rules for identifying and acting on sensitive data patterns.
object
Configuration for keywords used to reinforce sensitive data pattern detection.
keywords [_required_]
[string]
A list of keywords to match near the sensitive pattern.
proximity [_required_]
int64
Maximum number of tokens between a keyword and a sensitive value match.
name [_required_]
string
A name identifying the rule.
_required_]
<oneOf>
Defines what action to take when sensitive data is matched.
object
Configuration for completely redacting matched sensitive data.
action [_required_]
enum
Action type that completely replaces the matched sensitive data with a fixed replacement string to remove all visibility. Allowed enum values: `redact`
_required_]
object
Configuration for fully redacting sensitive data.
replace [_required_]
string
The `ObservabilityPipelineSensitiveDataScannerProcessorActionRedactOptions` `replace`.
object
Configuration for hashing matched sensitive values.
action [_required_]
enum
Action type that replaces the matched sensitive data with a hashed representation, preserving structure while securing content. Allowed enum values: `hash`
options
object
The `ObservabilityPipelineSensitiveDataScannerProcessorActionHash` `options`.
object
Configuration for partially redacting matched sensitive data.
action [_required_]
enum
Action type that redacts part of the sensitive data while preserving a configurable number of characters, typically used for masking purposes (e.g., show last 4 digits of a credit card). Allowed enum values: `partial_redact`
_required_]
object
Controls how partial redaction is applied, including character count and direction.
characters [_required_]
int64
The `ObservabilityPipelineSensitiveDataScannerProcessorActionPartialRedactOptions` `characters`.
direction [_required_]
enum
Indicates whether to redact characters from the first or last part of the matched value. Allowed enum values: `first,last`
_required_]
<oneOf>
Pattern detection configuration for identifying sensitive data using either a custom regex or a library reference.
object
Defines a custom regex-based pattern for identifying sensitive data in logs.
_required_]
object
Options for defining a custom regex pattern.
rule [_required_]
string
A regular expression used to detect sensitive values. Must be a valid regex.
type [_required_]
enum
Indicates a custom regular expression is used for matching. Allowed enum values: `custom`
object
Specifies a pattern from Datadog’s sensitive data detection library to match known sensitive data types.
_required_]
object
Options for selecting a predefined library pattern and enabling keyword support.
id [_required_]
string
Identifier for a predefined pattern from the sensitive data scanner pattern library.
use_recommended_keywords
boolean
Whether to augment the pattern with recommended keywords (optional).
type [_required_]
enum
Indicates that a predefined library pattern is used. Allowed enum values: `library`
_required_]
<oneOf>
Determines which parts of the log the pattern-matching rule should be applied to.
object
Includes only specific fields for sensitive data scanning.
_required_]
object
Fields to which the scope rule applies.
fields [_required_]
[string]
The `ObservabilityPipelineSensitiveDataScannerProcessorScopeOptions` `fields`.
target [_required_]
enum
Applies the rule only to included fields. Allowed enum values: `include`
object
Excludes specific fields from sensitive data scanning.
_required_]
object
Fields to which the scope rule applies.
fields [_required_]
[string]
The `ObservabilityPipelineSensitiveDataScannerProcessorScopeOptions` `fields`.
target [_required_]
enum
Excludes specific fields from processing. Allowed enum values: `exclude`
object
Applies scanning across all available fields.
target [_required_]
enum
Applies the rule to all fields. Allowed enum values: `all`
tags [_required_]
[string]
Tags assigned to this rule for filtering and classification.
type [_required_]
enum
The processor type. The value should always be `sensitive_data_scanner`. Allowed enum values: `sensitive_data_scanner`
default: `sensitive_data_scanner`
object
The `ocsf_mapper` processor transforms logs into the OCSF schema using a predefined mapping configuration.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
[object]
A list of mapping rules to convert events to the OCSF format.
include [_required_]
string
A Datadog search query used to select the logs that this mapping should apply to.
_required_]
<oneOf>
Defines a single mapping rule for transforming logs into the OCSF schema.
Option 1
enum
Predefined library mappings for common log formats. Allowed enum values: `CloudTrail Account Change,GCP Cloud Audit CreateBucket,GCP Cloud Audit CreateSink,GCP Cloud Audit SetIamPolicy,GCP Cloud Audit UpdateSink,Github Audit Log API Activity,Google Workspace Admin Audit addPrivilege,Microsoft 365 Defender Incident,Microsoft 365 Defender UserLoggedIn,Okta System Log Authentication,Palo Alto Networks Firewall Traffic`
type [_required_]
enum
The processor type. The value should always be `ocsf_mapper`. Allowed enum values: `ocsf_mapper`
default: `ocsf_mapper`
object
The `add_env_vars` processor adds environment variable values to log events.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this processor in the pipeline.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
type [_required_]
enum
The processor type. The value should always be `add_env_vars`. Allowed enum values: `add_env_vars`
default: `add_env_vars`
_required_]
[object]
A list of environment variable mappings to apply to log fields.
field [_required_]
string
The target field in the log event.
name [_required_]
string
The name of the environment variable to read.
object
The `dedupe` processor removes duplicate fields in log events.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
fields [_required_]
[string]
A list of log field paths to check for duplicates.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
mode [_required_]
enum
The deduplication mode to apply to the fields. Allowed enum values: `match,ignore`
type [_required_]
enum
The processor type. The value should always be `dedupe`. Allowed enum values: `dedupe`
default: `dedupe`
object
The `enrichment_table` processor enriches logs using a static CSV file or GeoIP database.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
object
Defines a static enrichment table loaded from a CSV file.
_required_]
object
File encoding format.
delimiter [_required_]
string
The `encoding` `delimiter`.
includes_headers [_required_]
boolean
The `encoding` `includes_headers`.
type [_required_]
enum
Specifies the encoding format (e.g., CSV) used for enrichment tables. Allowed enum values: `csv`
_required_]
[object]
Key fields used to look up enrichment values.
column [_required_]
string
The `items` `column`.
comparison [_required_]
enum
Defines how to compare key fields for enrichment table lookups. Allowed enum values: `equals`
field [_required_]
string
The `items` `field`.
path [_required_]
string
Path to the CSV file.
_required_]
[object]
Schema defining column names and their types.
column [_required_]
string
The `items` `column`.
type [_required_]
enum
Declares allowed data types for enrichment table columns. Allowed enum values: `string,boolean,integer,float,date,timestamp`
object
Uses a GeoIP database to enrich logs based on an IP field.
key_field [_required_]
string
Path to the IP field in the log.
locale [_required_]
string
Locale used to resolve geographical names.
path [_required_]
string
Path to the GeoIP database file.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
target [_required_]
string
Path where enrichment results should be stored in the log.
type [_required_]
enum
The processor type. The value should always be `enrichment_table`. Allowed enum values: `enrichment_table`
default: `enrichment_table`
object
The `reduce` processor aggregates and merges logs based on matching keys and merge strategies.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
group_by [_required_]
[string]
A list of fields used to group log events for merging.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
_required_]
[object]
List of merge strategies defining how values from grouped events should be combined.
path [_required_]
string
The field path in the log event.
strategy [_required_]
enum
The merge strategy to apply. Allowed enum values: `discard,retain,sum,max,min,array,concat,concat_newline,concat_raw,shortest_array,longest_array,flat_unique`
type [_required_]
enum
The processor type. The value should always be `reduce`. Allowed enum values: `reduce`
default: `reduce`
object
The `throttle` processor limits the number of events that pass through over a given time window.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
group_by
[string]
Optional list of fields used to group events before the threshold has been reached.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
threshold [_required_]
int64
the number of events allowed in a given time window. Events sent after the threshold has been reached, are dropped.
type [_required_]
enum
The processor type. The value should always be `throttle`. Allowed enum values: `throttle`
default: `throttle`
window [_required_]
double
The time window in seconds over which the threshold applies.
object
The `custom_processor` processor transforms events using [Vector Remap Language (VRL)](https://vector.dev/docs/reference/vrl/) scripts with advanced filtering capabilities.
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this processor.
include [_required_]
string
A Datadog search query used to determine which logs this processor targets. This field should always be set to `*` for the custom_processor processor.
default: `*`
_required_]
[object]
Array of VRL remap rules.
drop_on_error [_required_]
boolean
Whether to drop events that caused errors during processing.
enabled
boolean
Whether this remap rule is enabled.
include [_required_]
string
A Datadog search query used to filter events for this specific remap rule.
name [_required_]
string
A descriptive name for this remap rule.
source [_required_]
string
The VRL script source code that defines the processing logic.
type [_required_]
enum
The processor type. The value should always be `custom_processor`. Allowed enum values: `custom_processor`
default: `custom_processor`
object
The `datadog_tags` processor includes or excludes specific Datadog tags in your logs.
action [_required_]
enum
The action to take on tags with matching keys. Allowed enum values: `include,exclude`
display_name
string
The display name for a component.
enabled [_required_]
boolean
Whether this processor is enabled.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
include [_required_]
string
A Datadog search query used to determine which logs this processor targets.
keys [_required_]
[string]
A list of tag keys.
mode [_required_]
enum
The processing mode. Allowed enum values: `filter`
type [_required_]
enum
The processor type. The value should always be `datadog_tags`. Allowed enum values: `datadog_tags`
default: `datadog_tags`
_required_]
[ <oneOf>]
A list of configured data sources for the pipeline.
object
The `kafka` source ingests data from Apache Kafka topics.
group_id [_required_]
string
Consumer group ID used by the Kafka client.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
[object]
Optional list of advanced Kafka client configuration options, defined as key-value pairs.
name [_required_]
string
The name of the `librdkafka` configuration option to set.
value [_required_]
string
The value assigned to the specified `librdkafka` configuration option.
object
Specifies the SASL mechanism for authenticating with a Kafka cluster.
mechanism
enum
SASL mechanism used for Kafka authentication. Allowed enum values: `PLAIN,SCRAM-SHA-256,SCRAM-SHA-512`
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
topics [_required_]
[string]
A list of Kafka topic names to subscribe to. The source ingests messages from each topic specified.
type [_required_]
enum
The source type. The value should always be `kafka`. Allowed enum values: `kafka`
default: `kafka`
object
The `datadog_agent` source collects logs from the Datadog Agent.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `datadog_agent`. Allowed enum values: `datadog_agent`
default: `datadog_agent`
object
The `splunk_tcp` source receives logs from a Splunk Universal Forwarder over TCP. TLS is supported for secure transmission.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. Always `splunk_tcp`. Allowed enum values: `splunk_tcp`
default: `splunk_tcp`
object
The `splunk_hec` source implements the Splunk HTTP Event Collector (HEC) API.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. Always `splunk_hec`. Allowed enum values: `splunk_hec`
default: `splunk_hec`
object
The `amazon_s3` source ingests logs from an Amazon S3 bucket. It supports AWS authentication and TLS encryption.
object
AWS authentication credentials used for accessing AWS services such as S3. If omitted, the system’s default credentials are used (for example, the IAM role and environment variables).
assume_role
string
The Amazon Resource Name (ARN) of the role to assume.
external_id
string
A unique identifier for cross-account role assumption.
session_name
string
A session identifier used for logging and tracing the assumed role session.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
region [_required_]
string
AWS region where the S3 bucket resides.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. Always `amazon_s3`. Allowed enum values: `amazon_s3`
default: `amazon_s3`
object
The `fluentd` source ingests logs from a Fluentd-compatible service.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `fluentd. Allowed enum values: `fluentd`
default: `fluentd`
object
The `fluent_bit` source ingests logs from Fluent Bit.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (for example, as the `input` to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `fluent_bit`. Allowed enum values: `fluent_bit`
default: `fluent_bit`
object
The `http_server` source collects logs over HTTP POST from external services.
auth_strategy [_required_]
enum
HTTP authentication method. Allowed enum values: `none,plain`
decoding [_required_]
enum
The decoding format used to interpret incoming logs. Allowed enum values: `bytes,gelf,json,syslog`
id [_required_]
string
Unique ID for the HTTP server source.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `http_server`. Allowed enum values: `http_server`
default: `http_server`
object
The `sumo_logic` source receives logs from Sumo Logic collectors.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
type [_required_]
enum
The source type. The value should always be `sumo_logic`. Allowed enum values: `sumo_logic`
default: `sumo_logic`
object
The `rsyslog` source listens for logs over TCP or UDP from an `rsyslog` server using the syslog protocol.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
mode [_required_]
enum
Protocol used by the syslog source to receive messages. Allowed enum values: `tcp,udp`
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `rsyslog`. Allowed enum values: `rsyslog`
default: `rsyslog`
object
The `syslog_ng` source listens for logs over TCP or UDP from a `syslog-ng` server using the syslog protocol.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
mode [_required_]
enum
Protocol used by the syslog source to receive messages. Allowed enum values: `tcp,udp`
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `syslog_ng`. Allowed enum values: `syslog_ng`
default: `syslog_ng`
object
The `amazon_data_firehose` source ingests logs from AWS Data Firehose.
object
AWS authentication credentials used for accessing AWS services such as S3. If omitted, the system’s default credentials are used (for example, the IAM role and environment variables).
assume_role
string
The Amazon Resource Name (ARN) of the role to assume.
external_id
string
A unique identifier for cross-account role assumption.
session_name
string
A session identifier used for logging and tracing the assumed role session.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `amazon_data_firehose`. Allowed enum values: `amazon_data_firehose`
default: `amazon_data_firehose`
object
The `google_pubsub` source ingests logs from a Google Cloud Pub/Sub subscription.
object
GCP credentials used to authenticate with Google Cloud Storage.
credentials_file [_required_]
string
Path to the GCP service account key file.
decoding [_required_]
enum
The decoding format used to interpret incoming logs. Allowed enum values: `bytes,gelf,json,syslog`
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
project [_required_]
string
The GCP project ID that owns the Pub/Sub subscription.
subscription [_required_]
string
The Pub/Sub subscription name from which messages are consumed.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `google_pubsub`. Allowed enum values: `google_pubsub`
default: `google_pubsub`
object
The `http_client` source scrapes logs from HTTP endpoints at regular intervals.
auth_strategy
enum
Optional authentication strategy for HTTP requests. Allowed enum values: `basic,bearer`
decoding [_required_]
enum
The decoding format used to interpret incoming logs. Allowed enum values: `bytes,gelf,json,syslog`
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
scrape_interval_secs
int64
The interval (in seconds) between HTTP scrape requests.
scrape_timeout_secs
int64
The timeout (in seconds) for each scrape request.
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `http_client`. Allowed enum values: `http_client`
default: `http_client`
object
The `logstash` source ingests logs from a Logstash forwarder.
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
object
Configuration for enabling TLS encryption between the pipeline component and external services.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `logstash`. Allowed enum values: `logstash`
default: `logstash`
object
The `socket` source ingests logs over TCP or UDP.
_required_]
<oneOf>
Framing method configuration for the socket source.
object
Byte frames which are delimited by a newline character.
method [_required_]
enum
Byte frames which are delimited by a newline character. Allowed enum values: `newline_delimited`
object
Byte frames are passed through as-is according to the underlying I/O boundaries (for example, split between messages or stream segments).
method [_required_]
enum
Byte frames are passed through as-is according to the underlying I/O boundaries (for example, split between messages or stream segments). Allowed enum values: `bytes`
object
Byte frames which are delimited by a chosen character.
delimiter [_required_]
string
A single ASCII character used to delimit events.
method [_required_]
enum
Byte frames which are delimited by a chosen character. Allowed enum values: `character_delimited`
object
Byte frames according to the octet counting format as per RFC6587.
method [_required_]
enum
Byte frames according to the octet counting format as per RFC6587. Allowed enum values: `octet_counting`
object
Byte frames which are chunked GELF messages.
method [_required_]
enum
Byte frames which are chunked GELF messages. Allowed enum values: `chunked_gelf`
id [_required_]
string
The unique identifier for this component. Used to reference this component in other parts of the pipeline (e.g., as input to downstream components).
mode [_required_]
enum
Protocol used to receive logs. Allowed enum values: `tcp,udp`
object
TLS configuration. Relevant only when `mode` is `tcp`.
ca_file
string
Path to the Certificate Authority (CA) file used to validate the server’s TLS certificate.
crt_file [_required_]
string
Path to the TLS client certificate file used to authenticate the pipeline component with upstream or downstream services.
key_file
string
Path to the private key file associated with the TLS client certificate. Used for mutual TLS authentication.
type [_required_]
enum
The source type. The value should always be `socket`. Allowed enum values: `socket`
default: `socket`
name [_required_]
string
Name of the pipeline.
type [_required_]
string
The resource type identifier. For pipeline resources, this should always be set to `pipelines`.
default: `pipelines`
```
{
  "data": {
    "attributes": {
      "config": {
        "destinations": [
          {
            "id": "datadog-logs-destination",
            "inputs": [
              "my-processor-group"
            ],
            "type": "datadog_logs"
          }
        ],
        "processors": [
          {
            "enabled": true,
            "id": "my-processor-group",
            "include": "service:my-service",
            "inputs": [
              "datadog-agent-source"
            ],
            "processors": [
              {
                "enabled": true,
                "id": "filter-processor",
                "include": "status:error",
                "type": "filter"
              }
            ]
          }
        ],
        "sources": [
          {
            "id": "datadog-agent-source",
            "type": "datadog_agent"
          }
        ]
      },
      "name": "Main Observability Pipeline"
    },
    "type": "pipelines"
  }
}
```

Copy
### Response
  * [200](https://docs.datadoghq.com/api/latest/observability-pipelines/#ValidatePipeline-200-v2)
  * [400](https://docs.datadoghq.com/api/latest/observability-pipelines/#ValidatePipeline-400-v2)
  * [403](https://docs.datadoghq.com/api/latest/observability-pipelines/#ValidatePipeline-403-v2)
  * [429](https://docs.datadoghq.com/api/latest/observability-pipelines/#ValidatePipeline-429-v2)


OK
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


Response containing validation errors.
Expand All
Field
Type
Description
[object]
The `ValidationResponse` `errors`.
_required_]
object
Describes additional metadata for validation errors, including field names and error messages.
field
string
The field name that caused the error.
id
string
The ID of the component in which the error occurred.
message [_required_]
string
The detailed error message.
title [_required_]
string
A short, human-readable summary of the error.
```
{
  "errors": [
    {
      "meta": {
        "field": "region",
        "id": "datadog-agent-source",
        "message": "Field 'region' is required"
      },
      "title": "Field 'region' is required"
    }
  ]
}
```

Copy
Bad Request
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


API error response.
Expand All
Field
Type
Description
errors [_required_]
[string]
A list of errors.
```
{
  "errors": [
    "Bad Request"
  ]
}
```

Copy
Not Authorized
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


API error response.
Expand All
Field
Type
Description
errors [_required_]
[string]
A list of errors.
```
{
  "errors": [
    "Bad Request"
  ]
}
```

Copy
Too many requests
  * [Model](https://docs.datadoghq.com/api/latest/observability-pipelines/)
  * [Example](https://docs.datadoghq.com/api/latest/observability-pipelines/)


API error response.
Expand All
Field
Type
Description
errors [_required_]
[string]
A list of errors.
```
{
  "errors": [
    "Bad Request"
  ]
}
```

Copy
### Code Example
  * [Curl](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=curl)
  * [Go](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=go)
  * [Java](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=java)
  * [Python](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=python)
  * [Ruby](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=ruby)
  * [Rust](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=rust)
  * [Typescript](https://docs.datadoghq.com/api/latest/observability-pipelines/?code-lang=typescript)


#####  Validate an observability pipeline returns "OK" response
Copy
```
                          # Curl command  
curl -X POST "https://api.ap1.datadoghq.com"https://api.ap2.datadoghq.com"https://api.datadoghq.eu"https://api.ddog-gov.com"https://api.datadoghq.com"https://api.us3.datadoghq.com"https://api.us5.datadoghq.com/api/v2/remote_config/products/obs_pipelines/pipelines/validate" \
-H "Accept: application/json" \
-H "Content-Type: application/json" \
-H "DD-API-KEY: ${DD_API_KEY}" \
-H "DD-APPLICATION-KEY: ${DD_APP_KEY}" \
-d @- << EOF
{
  "data": {
    "attributes": {
      "config": {
        "destinations": [
          {
            "id": "datadog-logs-destination",
            "inputs": [
              "my-processor-group"
            ],
            "type": "datadog_logs"
          }
        ],
        "processors": [
          {
            "enabled": true,
            "id": "my-processor-group",
            "include": "service:my-service",
            "inputs": [
              "datadog-agent-source"
            ],
            "processors": [
              {
                "enabled": true,
                "id": "filter-processor",
                "include": "status:error",
                "type": "filter"
              }
            ]
          }
        ],
        "sources": [
          {
            "id": "datadog-agent-source",
            "type": "datadog_agent"
          }
        ]
      },
      "name": "Main Observability Pipeline"
    },
    "type": "pipelines"
  }
}
EOF  

                        
```

#####  Validate an observability pipeline returns "OK" response
```
// Validate an observability pipeline returns "OK" response

package main

import (
	"context"
	"encoding/json"
	"fmt"
	"os"

	"github.com/DataDog/datadog-api-client-go/v2/api/datadog"
	"github.com/DataDog/datadog-api-client-go/v2/api/datadogV2"
)

func main() {
	body := datadogV2.ObservabilityPipelineSpec{
		Data: datadogV2.ObservabilityPipelineSpecData{
			Attributes: datadogV2.ObservabilityPipelineDataAttributes{
				Config: datadogV2.ObservabilityPipelineConfig{
					Destinations: []datadogV2.ObservabilityPipelineConfigDestinationItem{
						datadogV2.ObservabilityPipelineConfigDestinationItem{
							ObservabilityPipelineDatadogLogsDestination: &datadogV2.ObservabilityPipelineDatadogLogsDestination{
								Id: "datadog-logs-destination",
								Inputs: []string{
									"my-processor-group",
								},
								Type: datadogV2.OBSERVABILITYPIPELINEDATADOGLOGSDESTINATIONTYPE_DATADOG_LOGS,
							}},
					},
					Processors: []datadogV2.ObservabilityPipelineConfigProcessorGroup{
						{
							Enabled: true,
							Id:      "my-processor-group",
							Include: "service:my-service",
							Inputs: []string{
								"datadog-agent-source",
							},
							Processors: []datadogV2.ObservabilityPipelineConfigProcessorItem{
								datadogV2.ObservabilityPipelineConfigProcessorItem{
									ObservabilityPipelineFilterProcessor: &datadogV2.ObservabilityPipelineFilterProcessor{
										Enabled: true,
										Id:      "filter-processor",
										Include: "status:error",
										Type:    datadogV2.OBSERVABILITYPIPELINEFILTERPROCESSORTYPE_FILTER,
									}},
							},
						},
					},
					Sources: []datadogV2.ObservabilityPipelineConfigSourceItem{
						datadogV2.ObservabilityPipelineConfigSourceItem{
							ObservabilityPipelineDatadogAgentSource: &datadogV2.ObservabilityPipelineDatadogAgentSource{
								Id:   "datadog-agent-source",
								Type: datadogV2.OBSERVABILITYPIPELINEDATADOGAGENTSOURCETYPE_DATADOG_AGENT,
							}},
					},
				},
				Name: "Main Observability Pipeline",
			},
			Type: "pipelines",
		},
	}
	ctx := datadog.NewDefaultContext(context.Background())
	configuration := datadog.NewConfiguration()
	configuration.SetUnstableOperationEnabled("v2.ValidatePipeline", true)
	apiClient := datadog.NewAPIClient(configuration)
	api := datadogV2.NewObservabilityPipelinesApi(apiClient)
	resp, r, err := api.ValidatePipeline(ctx, body)

	if err != nil {
		fmt.Fprintf(os.Stderr, "Error when calling `ObservabilityPipelinesApi.ValidatePipeline`: %v\n", err)
		fmt.Fprintf(os.Stderr, "Full HTTP response: %v\n", r)
	}

	responseContent, _ := json.MarshalIndent(resp, "", "  ")
	fmt.Fprintf(os.Stdout, "Response from `ObservabilityPipelinesApi.ValidatePipeline`:\n%s\n", responseContent)
}

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=go) and then save the example to `main.go` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" go run "main.go"


```

#####  Validate an observability pipeline returns "OK" response
```
// Validate an observability pipeline returns "OK" response

import com.datadog.api.client.ApiClient;
import com.datadog.api.client.ApiException;
import com.datadog.api.client.v2.api.ObservabilityPipelinesApi;
import com.datadog.api.client.v2.model.ObservabilityPipelineConfig;
import com.datadog.api.client.v2.model.ObservabilityPipelineConfigDestinationItem;
import com.datadog.api.client.v2.model.ObservabilityPipelineConfigProcessorGroup;
import com.datadog.api.client.v2.model.ObservabilityPipelineConfigProcessorItem;
import com.datadog.api.client.v2.model.ObservabilityPipelineConfigSourceItem;
import com.datadog.api.client.v2.model.ObservabilityPipelineDataAttributes;
import com.datadog.api.client.v2.model.ObservabilityPipelineDatadogAgentSource;
import com.datadog.api.client.v2.model.ObservabilityPipelineDatadogAgentSourceType;
import com.datadog.api.client.v2.model.ObservabilityPipelineDatadogLogsDestination;
import com.datadog.api.client.v2.model.ObservabilityPipelineDatadogLogsDestinationType;
import com.datadog.api.client.v2.model.ObservabilityPipelineFilterProcessor;
import com.datadog.api.client.v2.model.ObservabilityPipelineFilterProcessorType;
import com.datadog.api.client.v2.model.ObservabilityPipelineSpec;
import com.datadog.api.client.v2.model.ObservabilityPipelineSpecData;
import com.datadog.api.client.v2.model.ValidationResponse;
import java.util.Collections;

public class Example {
  public static void main(String[] args) {
    ApiClient defaultClient = ApiClient.getDefaultApiClient();
    defaultClient.setUnstableOperationEnabled("v2.validatePipeline", true);
    ObservabilityPipelinesApi apiInstance = new ObservabilityPipelinesApi(defaultClient);

    ObservabilityPipelineSpec body =
        new ObservabilityPipelineSpec()
            .data(
                new ObservabilityPipelineSpecData()
                    .attributes(
                        new ObservabilityPipelineDataAttributes()
                            .config(
                                new ObservabilityPipelineConfig()
                                    .destinations(
                                        Collections.singletonList(
                                            new ObservabilityPipelineConfigDestinationItem(
                                                new ObservabilityPipelineDatadogLogsDestination()
                                                    .id("datadog-logs-destination")
                                                    .inputs(
                                                        Collections.singletonList(
                                                            "my-processor-group"))
                                                    .type(
                                                        ObservabilityPipelineDatadogLogsDestinationType
                                                            .DATADOG_LOGS))))
                                    .processors(
                                        Collections.singletonList(
                                            new ObservabilityPipelineConfigProcessorGroup()
                                                .enabled(true)
                                                .id("my-processor-group")
                                                .include("service:my-service")
                                                .inputs(
                                                    Collections.singletonList(
                                                        "datadog-agent-source"))
                                                .processors(
                                                    Collections.singletonList(
                                                        new ObservabilityPipelineConfigProcessorItem(
                                                            new ObservabilityPipelineFilterProcessor()
                                                                .enabled(true)
                                                                .id("filter-processor")
                                                                .include("status:error")
                                                                .type(
                                                                    ObservabilityPipelineFilterProcessorType
                                                                        .FILTER))))))
                                    .sources(
                                        Collections.singletonList(
                                            new ObservabilityPipelineConfigSourceItem(
                                                new ObservabilityPipelineDatadogAgentSource()
                                                    .id("datadog-agent-source")
                                                    .type(
                                                        ObservabilityPipelineDatadogAgentSourceType
                                                            .DATADOG_AGENT)))))
                            .name("Main Observability Pipeline"))
                    .type("pipelines"));

    try {
      ValidationResponse result = apiInstance.validatePipeline(body);
      System.out.println(result);
    } catch (ApiException e) {
      System.err.println("Exception when calling ObservabilityPipelinesApi#validatePipeline");
      System.err.println("Status code: " + e.getCode());
      System.err.println("Reason: " + e.getResponseBody());
      System.err.println("Response headers: " + e.getResponseHeaders());
      e.printStackTrace();
    }
  }
}

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=java) and then save the example to `Example.java` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" java "Example.java"


```

#####  Validate an observability pipeline returns "OK" response
```
"""
Validate an observability pipeline returns "OK" response
"""

from datadog_api_client import ApiClient, Configuration
from datadog_api_client.v2.api.observability_pipelines_api import ObservabilityPipelinesApi
from datadog_api_client.v2.model.observability_pipeline_config import ObservabilityPipelineConfig
from datadog_api_client.v2.model.observability_pipeline_config_processor_group import (
    ObservabilityPipelineConfigProcessorGroup,
)
from datadog_api_client.v2.model.observability_pipeline_data_attributes import ObservabilityPipelineDataAttributes
from datadog_api_client.v2.model.observability_pipeline_datadog_agent_source import (
    ObservabilityPipelineDatadogAgentSource,
)
from datadog_api_client.v2.model.observability_pipeline_datadog_agent_source_type import (
    ObservabilityPipelineDatadogAgentSourceType,
)
from datadog_api_client.v2.model.observability_pipeline_datadog_logs_destination import (
    ObservabilityPipelineDatadogLogsDestination,
)
from datadog_api_client.v2.model.observability_pipeline_datadog_logs_destination_type import (
    ObservabilityPipelineDatadogLogsDestinationType,
)
from datadog_api_client.v2.model.observability_pipeline_filter_processor import ObservabilityPipelineFilterProcessor
from datadog_api_client.v2.model.observability_pipeline_filter_processor_type import (
    ObservabilityPipelineFilterProcessorType,
)
from datadog_api_client.v2.model.observability_pipeline_spec import ObservabilityPipelineSpec
from datadog_api_client.v2.model.observability_pipeline_spec_data import ObservabilityPipelineSpecData

body = ObservabilityPipelineSpec(
    data=ObservabilityPipelineSpecData(
        attributes=ObservabilityPipelineDataAttributes(
            config=ObservabilityPipelineConfig(
                destinations=[
                    ObservabilityPipelineDatadogLogsDestination(
                        id="datadog-logs-destination",
                        inputs=[
                            "my-processor-group",
                        ],
                        type=ObservabilityPipelineDatadogLogsDestinationType.DATADOG_LOGS,
                    ),
                ],
                processors=[
                    ObservabilityPipelineConfigProcessorGroup(
                        enabled=True,
                        id="my-processor-group",
                        include="service:my-service",
                        inputs=[
                            "datadog-agent-source",
                        ],
                        processors=[
                            ObservabilityPipelineFilterProcessor(
                                enabled=True,
                                id="filter-processor",
                                include="status:error",
                                type=ObservabilityPipelineFilterProcessorType.FILTER,
                            ),
                        ],
                    ),
                ],
                sources=[
                    ObservabilityPipelineDatadogAgentSource(
                        id="datadog-agent-source",
                        type=ObservabilityPipelineDatadogAgentSourceType.DATADOG_AGENT,
                    ),
                ],
            ),
            name="Main Observability Pipeline",
        ),
        type="pipelines",
    ),
)

configuration = Configuration()
configuration.unstable_operations["validate_pipeline"] = True
with ApiClient(configuration) as api_client:
    api_instance = ObservabilityPipelinesApi(api_client)
    response = api_instance.validate_pipeline(body=body)

    print(response)

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=python) and then save the example to `example.py` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" python3 "example.py"


```

#####  Validate an observability pipeline returns "OK" response
```
# Validate an observability pipeline returns "OK" response

require "datadog_api_client"
DatadogAPIClient.configure do |config|
  config.unstable_operations["v2.validate_pipeline".to_sym] = true
end
api_instance = DatadogAPIClient::V2::ObservabilityPipelinesAPI.new

body = DatadogAPIClient::V2::ObservabilityPipelineSpec.new({
  data: DatadogAPIClient::V2::ObservabilityPipelineSpecData.new({
    attributes: DatadogAPIClient::V2::ObservabilityPipelineDataAttributes.new({
      config: DatadogAPIClient::V2::ObservabilityPipelineConfig.new({
        destinations: [
          DatadogAPIClient::V2::ObservabilityPipelineDatadogLogsDestination.new({
            id: "datadog-logs-destination",
            inputs: [
              "my-processor-group",
            ],
            type: DatadogAPIClient::V2::ObservabilityPipelineDatadogLogsDestinationType::DATADOG_LOGS,
          }),
        ],
        processors: [
          DatadogAPIClient::V2::ObservabilityPipelineConfigProcessorGroup.new({
            enabled: true,
            id: "my-processor-group",
            include: "service:my-service",
            inputs: [
              "datadog-agent-source",
            ],
            processors: [
              DatadogAPIClient::V2::ObservabilityPipelineFilterProcessor.new({
                enabled: true,
                id: "filter-processor",
                include: "status:error",
                type: DatadogAPIClient::V2::ObservabilityPipelineFilterProcessorType::FILTER,
              }),
            ],
          }),
        ],
        sources: [
          DatadogAPIClient::V2::ObservabilityPipelineDatadogAgentSource.new({
            id: "datadog-agent-source",
            type: DatadogAPIClient::V2::ObservabilityPipelineDatadogAgentSourceType::DATADOG_AGENT,
          }),
        ],
      }),
      name: "Main Observability Pipeline",
    }),
    type: "pipelines",
  }),
})
p api_instance.validate_pipeline(body)

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=ruby) and then save the example to `example.rb` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" rb "example.rb"


```

#####  Validate an observability pipeline returns "OK" response
```
// Validate an observability pipeline returns "OK" response
use datadog_api_client::datadog;
use datadog_api_client::datadogV2::api_observability_pipelines::ObservabilityPipelinesAPI;
use datadog_api_client::datadogV2::model::ObservabilityPipelineConfig;
use datadog_api_client::datadogV2::model::ObservabilityPipelineConfigDestinationItem;
use datadog_api_client::datadogV2::model::ObservabilityPipelineConfigProcessorGroup;
use datadog_api_client::datadogV2::model::ObservabilityPipelineConfigProcessorItem;
use datadog_api_client::datadogV2::model::ObservabilityPipelineConfigSourceItem;
use datadog_api_client::datadogV2::model::ObservabilityPipelineDataAttributes;
use datadog_api_client::datadogV2::model::ObservabilityPipelineDatadogAgentSource;
use datadog_api_client::datadogV2::model::ObservabilityPipelineDatadogAgentSourceType;
use datadog_api_client::datadogV2::model::ObservabilityPipelineDatadogLogsDestination;
use datadog_api_client::datadogV2::model::ObservabilityPipelineDatadogLogsDestinationType;
use datadog_api_client::datadogV2::model::ObservabilityPipelineFilterProcessor;
use datadog_api_client::datadogV2::model::ObservabilityPipelineFilterProcessorType;
use datadog_api_client::datadogV2::model::ObservabilityPipelineSpec;
use datadog_api_client::datadogV2::model::ObservabilityPipelineSpecData;

#[tokio::main]
async fn main() {
    let body =
        ObservabilityPipelineSpec::new(
            ObservabilityPipelineSpecData::new(
                ObservabilityPipelineDataAttributes::new(
                    ObservabilityPipelineConfig::new(
                        vec![
                            ObservabilityPipelineConfigDestinationItem::ObservabilityPipelineDatadogLogsDestination(
                                Box::new(
                                    ObservabilityPipelineDatadogLogsDestination::new(
                                        "datadog-logs-destination".to_string(),
                                        vec!["my-processor-group".to_string()],
                                        ObservabilityPipelineDatadogLogsDestinationType::DATADOG_LOGS,
                                    ),
                                ),
                            )
                        ],
                        vec![
                            ObservabilityPipelineConfigSourceItem::ObservabilityPipelineDatadogAgentSource(
                                Box::new(
                                    ObservabilityPipelineDatadogAgentSource::new(
                                        "datadog-agent-source".to_string(),
                                        ObservabilityPipelineDatadogAgentSourceType::DATADOG_AGENT,
                                    ),
                                ),
                            )
                        ],
                    ).processors(
                        vec![
                            ObservabilityPipelineConfigProcessorGroup::new(
                                true,
                                "my-processor-group".to_string(),
                                "service:my-service".to_string(),
                                vec!["datadog-agent-source".to_string()],
                                vec![
                                    ObservabilityPipelineConfigProcessorItem::ObservabilityPipelineFilterProcessor(
                                        Box::new(
                                            ObservabilityPipelineFilterProcessor::new(
                                                true,
                                                "filter-processor".to_string(),
                                                "status:error".to_string(),
                                                ObservabilityPipelineFilterProcessorType::FILTER,
                                            ),
                                        ),
                                    )
                                ],
                            )
                        ],
                    ),
                    "Main Observability Pipeline".to_string(),
                ),
                "pipelines".to_string(),
            ),
        );
    let mut configuration = datadog::Configuration::new();
    configuration.set_unstable_operation_enabled("v2.ValidatePipeline", true);
    let api = ObservabilityPipelinesAPI::with_config(configuration);
    let resp = api.validate_pipeline(body).await;
    if let Ok(value) = resp {
        println!("{:#?}", value);
    } else {
        println!("{:#?}", resp.unwrap_err());
    }
}

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=rust) and then save the example to `src/main.rs` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" cargo run


```

#####  Validate an observability pipeline returns "OK" response
```
/**
 * Validate an observability pipeline returns "OK" response
 */

import { client, v2 } from "@datadog/datadog-api-client";

const configuration = client.createConfiguration();
configuration.unstableOperations["v2.validatePipeline"] = true;
const apiInstance = new v2.ObservabilityPipelinesApi(configuration);

const params: v2.ObservabilityPipelinesApiValidatePipelineRequest = {
  body: {
    data: {
      attributes: {
        config: {
          destinations: [
            {
              id: "datadog-logs-destination",
              inputs: ["my-processor-group"],
              type: "datadog_logs",
            },
          ],
          processors: [
            {
              enabled: true,
              id: "my-processor-group",
              include: "service:my-service",
              inputs: ["datadog-agent-source"],
              processors: [
                {
                  enabled: true,
                  id: "filter-processor",
                  include: "status:error",
                  type: "filter",
                },
              ],
            },
          ],
          sources: [
            {
              id: "datadog-agent-source",
              type: "datadog_agent",
            },
          ],
        },
        name: "Main Observability Pipeline",
      },
      type: "pipelines",
    },
  },
};

apiInstance
  .validatePipeline(params)
  .then((data: v2.ValidationResponse) => {
    console.log(
      "API called successfully. Returned data: " + JSON.stringify(data)
    );
  })
  .catch((error: any) => console.error(error));

```

Copy
#### Instructions
First [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=typescript) and then save the example to `example.ts` and run following commands:
```
    

DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" tsc "example.ts"


```

* * *
## Can't find something?
Our friendly, knowledgeable solutions engineers are here to help!
[Contact Us](https://docs.datadoghq.com/help/)
[Free Trial](https://docs.datadoghq.com/api/latest/observability-pipelines/)
Download mobile app
[](https://apps.apple.com/app/datadog/id1391380318)[](https://play.google.com/store/apps/details?id=com.datadog.app)
Product
[Infrastructure Monitoring](https://www.datadoghq.com/product/infrastructure-monitoring/) [Network Monitoring](https://www.datadoghq.com/product/network-monitoring/) [Container Monitoring](https://www.datadoghq.com/product/container-monitoring/) [Serverless](https://www.datadoghq.com/product/serverless-monitoring/) [Cloud Cost Management](https://www.datadoghq.com/product/cloud-cost-management/) [Cloudcraft](https://www.datadoghq.com/product/cloudcraft/) [Kubernetes Autoscaling](https://www.datadoghq.com/product/kubernetes-autoscaling/) [Application Performance Monitoring](https://www.datadoghq.com/product/apm/) [Software Catalog](https://www.datadoghq.com/product/software-catalog/) [Universal Service Monitoring](https://www.datadoghq.com/product/universal-service-monitoring/) [Data Streams Monitoring](https://www.datadoghq.com/product/data-streams-monitoring/) [Jobs Monitoring](https://www.datadoghq.com/product/data-observability/jobs-monitoring/) [Quality Monitoring](https://www.datadoghq.com/product/data-observability/quality-monitoring/) [Database Monitoring](https://www.datadoghq.com/product/database-monitoring/) [Continuous Profiler](https://www.datadoghq.com/product/code-profiling/) [Dynamic Instrumentation](https://www.datadoghq.com/product/dynamic-instrumentation/) [Log Management](https://www.datadoghq.com/product/log-management/) [Sensitive Data Scanner](https://www.datadoghq.com/product/sensitive-data-scanner/) [Audit Trail](https://www.datadoghq.com/product/audit-trail/) [Observability Pipelines](https://www.datadoghq.com/product/observability-pipelines/) [Cloud Security](https://www.datadoghq.com/product/cloud-security/) [Cloud Security Posture Management](https://www.datadoghq.com/product/cloud-security/#posture-management) [Workload Protection](https://www.datadoghq.com/product/workload-protection/) [Cloud Infrastructure Entitlement Management](https://www.datadoghq.com/product/cloud-security/#entitlement-management) [Vulnerability Management](https://www.datadoghq.com/product/cloud-security/#vulnerability-management) [Compliance](https://www.datadoghq.com/product/cloud-security/#compliance) [App and API Protection](https://www.datadoghq.com/product/app-and-api-protection/) [Software Composition Analysis](https://www.datadoghq.com/product/software-composition-analysis/) [Code Security](https://www.datadoghq.com/product/code-security/) [Static Code Analysis (SAST)](https://www.datadoghq.com/product/sast/) [Runtime Code Analysis (IAST)](https://www.datadoghq.com/product/iast/) [IaC Security](https://www.datadoghq.com/product/iac-security) [Cloud SIEM](https://www.datadoghq.com/product/cloud-siem/) [Browser Real User Monitoring](https://www.datadoghq.com/product/real-user-monitoring/) [Mobile Real User Monitoring](https://www.datadoghq.com/product/real-user-monitoring/mobile-rum/) [Product Analytics](https://www.datadoghq.com/product/product-analytics/) [Session Replay](https://www.datadoghq.com/product/real-user-monitoring/session-replay/) [Synthetic Monitoring](https://www.datadoghq.com/product/synthetic-monitoring/) [Mobile App Testing](https://www.datadoghq.com/product/mobile-app-testing/)
[Continuous Testing](https://www.datadoghq.com/product/continuous-testing/) [Error Tracking](https://www.datadoghq.com/product/error-tracking/) [CloudPrem](https://www.datadoghq.com/product/cloudprem/) [Internal Developer Portal](https://www.datadoghq.com/product/internal-developer-portal/) [CI Visibility](https://www.datadoghq.com/product/ci-cd-monitoring/) [Test Optimization](https://www.datadoghq.com/product/test-optimization/) [Feature Flags](https://www.datadoghq.com/product/feature-flags/) [Code Coverage](https://www.datadoghq.com/product/code-coverage/) [Service Level Objectives](https://www.datadoghq.com/product/service-level-objectives/) [Incident Response](https://www.datadoghq.com/product/incident-response/) [Event Management](https://www.datadoghq.com/product/event-management/) [Case Management](https://www.datadoghq.com/product/case-management/) [Bits AI Agents](https://www.datadoghq.com/product/ai/bits-ai-agents/) [Bits AI SRE](https://www.datadoghq.com/product/ai/bits-ai-sre/) [Metrics](https://www.datadoghq.com/product/metrics/) [Watchdog](https://www.datadoghq.com/product/platform/watchdog/) [LLM Observability](https://www.datadoghq.com/product/llm-observability/) [AI Integrations](https://www.datadoghq.com/product/platform/integrations/#cat-aiml) [Workflow Automation](https://www.datadoghq.com/product/workflow-automation/) [App Builder](https://www.datadoghq.com/product/app-builder/) [CoScreen](https://www.datadoghq.com/product/coscreen/) [Teams](https://docs.datadoghq.com/account_management/teams/) [Dashboards](https://www.datadoghq.com/product/platform/dashboards/) [Notebooks](https://docs.datadoghq.com/notebooks/) [Mobile App](https://docs.datadoghq.com/service_management/mobile/?tab=ios) [Fleet Automation](https://www.datadoghq.com/product/fleet-automation/) [Access Control](https://docs.datadoghq.com/account_management/rbac/?tab=datadogapplication) [OpenTelemetry](https://www.datadoghq.com/solutions/opentelemetry/) [Alerts](https://www.datadoghq.com/product/platform/alerts/) [integrations](https://www.datadoghq.com/product/platform/integrations/) [IDE Plugins](https://www.datadoghq.com/product/platform/ides/) [API](https://docs.datadoghq.com/api/) [Marketplace](https://www.datadoghq.com/marketplacepartners/) [Security Labs Research](https://securitylabs.datadoghq.com/) [Open Source Projects](https://opensource.datadoghq.com/) [Storage Management](https://www.datadoghq.com/product/storage-management/) [DORA Metrics](https://www.datadoghq.com/product/platform/dora-metrics/) [Secret Scanning](https://www.datadoghq.com/product/secret-scanning/)
resources
[Pricing](https://www.datadoghq.com/pricing/) [Documentation](https://docs.datadoghq.com/) [Support](https://www.datadoghq.com/support/) [Services & Enablement](https://www.datadoghq.com/support-services/) [Product Preview Program](https://www.datadoghq.com/product-preview/) [Certification](https://www.datadoghq.com/certification/overview/)
[Open Source](https://opensource.datadoghq.com/) [Events and Webinars](https://www.datadoghq.com/events-webinars/) [Security](https://www.datadoghq.com/security/) [Privacy Center](https://www.datadoghq.com/privacy/) [Knowledge Center](https://www.datadoghq.com/knowledge-center/) [Learning Resources](https://www.datadoghq.com/learn/)
About
[Contact Us](https://www.datadoghq.com/about/contact/) [Partners](https://www.datadoghq.com/partner/network/) [Press](https://www.datadoghq.com/about/latest-news/press-releases/) [Leadership](https://www.datadoghq.com/about/leadership/) [Careers](https://careers.datadoghq.com/) [Legal](https://www.datadoghq.com/legal/)
[Investor Relations](https://investors.datadoghq.com/) [Analyst Reports](https://www.datadoghq.com/about/analyst/) [ESG Report](https://www.datadoghq.com/esg-report/) [Vendor Help](https://www.datadoghq.com/vendor-help/) [Trust Hub](https://www.datadoghq.com/trust/)
Blog
[The Monitor](https://www.datadoghq.com/blog/) [Engineering](https://www.datadoghq.com/blog/engineering/)
[AI](https://www.datadoghq.com/blog/ai/) [Security Labs](https://securitylabs.datadoghq.com/)
Icon/world Created with Sketch. English 
[English ](https://docs.datadoghq.com/?lang_pref=en)[Français ](https://docs.datadoghq.com/fr/?lang_pref=fr)[日本語 ](https://docs.datadoghq.com/ja/?lang_pref=ja)[한국어 ](https://docs.datadoghq.com/ko/?lang_pref=ko)[Español](https://docs.datadoghq.com/es/?lang_pref=es)
[](https://twitter.com/datadoghq)[](https://www.instagram.com/datadoghq/)[](https://www.youtube.com/user/DatadogHQ)[](https://www.LinkedIn.com/company/datadog/)
© Datadog 2026 [Terms](https://www.datadoghq.com/legal/terms/) | [Privacy](https://www.datadoghq.com/legal/privacy/) | [Your Privacy Choices ![](https://imgix.datadoghq.com/img/icons/privacyoptions.svg?w=24&dpr=2)](https://docs.datadoghq.com/api/latest/observability-pipelines/)
###### Request a personalized demo
×
*
First Name*
*
Last Name*
*
Business Email*
*
Company*
*
Job Title*
*
Phone Number
*
How are you currently monitoring your infrastructure and applications?
By submitting this form, you agree to the [Privacy Policy](https://www.datadoghq.com/legal/privacy/) and [Cookie Policy.](https://www.datadoghq.com/legal/cookies/)
Request a Demo
##### Get Started with Datadog
