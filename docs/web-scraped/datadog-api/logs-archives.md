Logs ArchivesRead the 2025 State of Containers and Serverless Report!
Read the State of Containers and Serverless Report!

Home

Docs

API- 
- Agent
- API
- APM Tracing
- ContainersAutodiscovery
- Datadog Operator
- Dashboards
- Database Monitoring
- Datadog
- Datadog Site
- DevSecOps
- Incident Management
- IntegrationsAWS
- Azure
- Google Cloud
- Terraform
- Internal Developer Portal
- Logs
- Monitors
- Notebooks
- OpenTelemetry
- Profiler
- SearchProduct-Specific Search
- Session Replay
- SecurityApp and API Protection
- Cloud Security
- Cloud SIEM
- Code Security
- Serverless for AWS Lambda
- Software DeliveryCI Visibility
- Feature Flags
- Test Optimization
- Test Impact Analysis
- Synthetic Monitoring and TestingAPI Tests
- Browser Tests
- Mobile App Tests
- Continuous Testing
- Private Locations
- TagsAssigning Tags
- Unified Service Tagging
- Using Tags
- Workflow Automation
- Learning Center
- Support
- 
- 
- 
- Architecture
- IoT
- Supported PlatformsAIX
- Linux
- Ansible
- Chef
- Heroku
- MacOS
- Puppet
- SaltStack
- SCCM
- Windows
- From Source
- Log CollectionLog Agent tags
- Advanced Configurations
- Proxy
- Transport
- Multi-Line Detection
- ConfigurationCommands
- Configuration Files
- Log Files
- Status Page
- Network Traffic
- Proxy Configuration
- FIPS Compliance
- Dual Shipping
- Secrets Management
- Fleet AutomationRemote Agent Management
- TroubleshootingContainer Hostname Detection
- Debug Mode
- Agent Flare
- Agent Check Status
- NTP Issues
- Permission Issues
- Integrations Issues
- Site Issues
- Autodiscovery Issues
- Windows Container Issues
- Agent Runtime Configuration
- High CPU or Memory Consumption
- Guides
- Data Security
- Guides
- AuthorizationOAuth2 in Datadog
- Authorization Endpoints
- DogStatsDDatagram Format
- Unix Domain Socket
- High Throughput Data
- Data Aggregation
- DogStatsD Mapper
- Custom ChecksWriting a Custom Agent Check
- Writing a Custom OpenMetrics Check
- IntegrationsBuild an Integration with Datadog
- Create an Agent-based Integration
- Create an API-based Integration
- Create a Log Pipeline
- Integration Assets Reference
- Build a Marketplace Offering
- Create an Integration Dashboard
- Create a Monitor Template
- Create a Cloud SIEM Detection Rule
- Install Agent Integration Developer Tool
- Service ChecksSubmission - Agent Check
- Submission - DogStatsD
- Submission - API
- IDE PluginsJetBrains IDEs
- VS Code & Cursor
- CommunityLibraries
- Guides
- Getting StartedDatadog Example Application
- OpenTelemetry Demo Application
- Feature Compatibility
- Instrument Your ApplicationsOTel SDKs
- OTel APIs with Datadog SDKs
- OTel Instrumentation Libraries
- Configuration
- Send Data to DatadogDDOT Collector (Recommended)
- Other Setup Options
- Semantic MappingResource Attribute Mapping
- Metrics Mapping
- Infrastructure Host Mapping
- Hostname Mapping
- Service-entry Spans Mapping
- Ingestion Sampling
- Correlate DataLogs and Traces
- Metrics and Traces
- RUM and Traces
- DBM and Traces
- IntegrationsApache Metrics
- Apache Spark Metrics
- Collector Health Metrics
- Datadog Extension
- Docker Metrics
- HAProxy Metrics
- Host Metrics
- IIS Metrics
- Kafka Metrics
- Kubernetes Metrics
- MySQL Metrics
- NGINX Metrics
- Podman Metrics
- Runtime Metrics
- Trace Metrics
- Troubleshooting
- Guides and ResourcesProduce Delta Temporality Metrics
- Visualize Histograms as Heatmaps
- Migration Guides
- ReferenceTerms and Concepts
- Trace Context Propagation
- Trace IDs
- OTLP Metric Types
- Getting Started
- Plan
- Build
- Run
- 
- 
- Enterprise Configuration
- Datadog for Intune
- Shortcut Configurations
- Push Notifications
- Widgets
- Guides
- Data Directory
- Troubleshooting
- Install
- Using CoTerm
- Configuration Rules
- 
- Getting Started
- Account Management
- Components: Common
- Components: Azure
- Components: AWS
- Advanced
- FAQ
- APIAWS Accounts
- Azure Accounts
- Blueprints
- Budgets
- Teams
- Users
- Configure
- Dashboard List
- WidgetsConfiguration
- Widget Types
- Querying
- FunctionsAlgorithms
- Arithmetic
- Count
- Exclusion
- Interpolation
- Rank
- Rate
- Regression
- Rollup
- Smoothing
- Timeshift
- Beta
- Graph InsightsMetric Correlations
- Watchdog Explains
- Template Variables
- Overlays
- Annotations
- Guides
- SharingShared Dashboards
- Share Graphs
- Scheduled Reports
- Analysis FeaturesGetting Started
- Guides
- 
- 
- Functions and Operators
- Guides
- Draft Monitors
- Configure Monitors
- Monitor Templates
- Monitor TypesHost
- Metric
- Analysis
- Anomaly
- APM
- Audit Trail
- Change
- CI/CD & Test
- Cloud Cost
- Composite
- Database Monitoring
- Error Tracking
- Event
- Forecast
- Integration
- Live Process
- Logs
- Network
- Cloud Network Monitoring
- NetFlow
- Outlier
- Process Check
- Real User Monitoring
- Service Check
- SLO Alerts
- Synthetic Monitoring
- Watchdog
- NotificationsNotification Rules
- Variables
- DowntimesExamples
- Manage MonitorsSearch Monitors
- Check Summary
- Monitor StatusStatus Graphs
- Status Events
- Monitor Settings
- Monitor Quality
- Guides
- Monitor-based SLOs
- Metric-based SLOs
- Time Slice SLOs
- Error Budget Alerts
- Burn Rate Alerts
- Guides
- Custom MetricsMetric Type Modifiers
- Historical Metrics Ingestion
- Submission - Agent Check
- Submission - DogStatsD
- Submission - Powershell
- Submission - API
- OpenTelemetry MetricsOTLP Metric Types
- Query OpenTelemetry Metrics
- Metrics Types
- Distributions
- Overview
- ExplorerMetrics Units
- Summary
- Volume
- Advanced Filtering
- Nested Queries
- Composite Metrics Queries
- Derived Metrics
- Metrics Without Limits™
- Guides
- Alerts
- Impact Analysis
- RCA
- Insights
- Faulty Deployment Detection
- Faulty Cloud & SaaS API Detection
- Bits AI SREInvestigate issues
- Remediate issues
- Bits AI SRE integrations and settings
- Help Bits learn
- Chat with Bits AI SRE
- Bits AI Dev AgentSetup
- Chat with Bits AI
- MCP Server
- Software CatalogSet Up
- Entity Model
- Troubleshooting
- ScorecardsScorecard Configuration
- Custom Rules
- Using Scorecards
- Self-Service ActionsSoftware Templates
- Engineering ReportsReliability Overview
- Scorecards Performance
- DORA Metrics
- Custom Reports
- Developer Homepage
- Campaigns
- External Provider Status
- Plugins
- Integrations
- Use CasesAPI Management
- Cloud Cost Management
- App and API Protection
- Developer Onboarding
- Dependency Management
- Production Readiness
- Incident Response
- CI Pipeline Visibility
- Onboarding Guide
- Explorer
- Issue States
- Regression Detection
- Suspected Causes
- Error Grouping
- Bits AI Dev Agent
- Monitors
- Issue Correlation
- Identify Suspect Commits
- Auto Assign
- Issue Team Ownership
- Track Browser and Mobile ErrorsBrowser Error Tracking
- Collecting Browser Errors
- Mobile Crash Tracking
- Replay Errors
- Real User Monitoring
- Logs
- Track Backend ErrorsGetting Started
- Exception Replay
- Capturing Handled Errors
- APM
- Logs
- Manage Data Collection
- Troubleshooting
- Guides
- Feature Flags
- Ingest Events
- Pipelines and ProcessorsAggregation Key Processor
- Arithmetic Processor
- Date Remapper
- Category Processor
- Grok Parser
- Lookup Processor
- Remapper
- Service Remapper
- Status Remapper
- String Builder Processor
- ExplorerSearching
- Navigate the Explorer
- Customization
- Facets
- Attributes
- Notifications
- Analytics
- Saved Views
- Triage Inbox
- CorrelationConfiguration
- Triaging & Notifying
- Analytics
- Guides
- Declare an Incident
- Describe an Incident
- Response Team
- Notification
- Investigate an IncidentTimeline
- Follow-ups
- Incident AI
- Incident SettingsInformation
- Property Fields
- Responder Types
- Integrations
- Notification Rules
- Templates
- Incident Analytics
- IntegrationsSlack
- Microsoft Teams
- Jira
- ServiceNow
- Status Pages
- Atlassian Statuspage
- Datadog Clipboard
- Onboard a Team
- Trigger a PageLive Call Routing
- Routing Rules
- Escalation Policies
- Schedules
- Automations
- Profile Settings
- Guides
- 
- ProjectsSettings
- Create a Case
- Customization
- View and Manage Cases
- Notifications and Integrations
- Case Automation Rules
- Troubleshooting
- 
- Build Workflows
- Access and Authentication
- Trigger Workflows
- Variables and parameters
- ActionsWorkflow Logic
- Save and Reuse Actions
- Test and Debug
- JavaScript Expressions
- Track Workflows
- Limits
- Build Apps
- Access and Authentication
- Queries
- Variables
- Events
- ComponentsCustom Charts
- React Renderer
- Tables
- Reusable Modules
- JavaScript Expressions
- Embedded AppsInput Parameters
- Save and Reuse Actions
- Create and Manage Datastores
- Use Datastores with Apps and Workflows
- Automation Rules
- Access and Authentication
- 
- ConnectionsAWS Integration
- HTTP Request
- Private ActionsUse Private Actions
- Run a Script
- Update the Private Action Runner
- Private Action Credentials
- OverlaysInfrastructure
- Observability
- Security
- Cloud Cost Management
- Cloud Resources Schema
- Policies
- Resource Changes
- Setup
- Guides
- Setup
- Host List
- Monitoring ContainersConfiguration
- Container Images View
- Orchestrator Explorer
- Kubernetes Resource Utilization
- Kubernetes Autoscaling
- Amazon Elastic Container Explorer
- Autoscaling
- Docker and other runtimesAPM
- Log collection
- Tag extraction
- Integrations
- Prometheus
- Data Collected
- KubernetesInstallation
- Further Configuration
- Distributions
- APM
- Log collection
- Tag extraction
- Integrations
- Prometheus & OpenMetrics
- Control plane monitoring
- Data collected
- kubectl Plugin
- Datadog CSI Driver
- Data security
- Cluster AgentSetup
- Commands & Options
- Cluster Checks
- Endpoint Checks
- Admission Controller
- Amazon ECSAPM
- Log collection
- Tag extraction
- Data collected
- Managed Instances
- AWS Fargate with ECS
- Datadog OperatorAdvanced Install
- Configuration
- Custom Checks
- Data Collected
- Secret Management
- DatadogDashboard CRD
- DatadogMonitor CRD
- DatadogSLO CRD
- TroubleshootingDuplicate hosts
- Cluster Agent
- Cluster Checks
- HPA and Metrics Provider
- Admission Controller
- Log Collection
- Guides
- Increase Process Retention
- AWS LambdaInstrumentation
- Managed Instances
- Lambda Metrics
- Distributed Tracing
- Log Collection
- Remote Instrumentation
- Advanced Configuration
- Continuous Profiler
- Securing Functions
- Deployment Tracking
- OpenTelemetry
- Troubleshooting
- Lambda Web Adapter
- FIPS Compliance
- AWS Step FunctionsInstallation
- Merge Step Functions and Lambda Traces
- Enhanced Metrics
- Redrive Executions
- Distributed Map States
- Troubleshooting
- AWS Fargate
- Azure App ServiceLinux - Code
- Linux - Container
- Windows - Code
- Azure Container AppsIn-Container
- Sidecar
- Azure Functions
- Google Cloud RunContainers
- Functions
- Functions (1st generation)
- Libraries & Integrations
- Glossary
- Guides
- Cloud Network MonitoringSetup
- Network Health
- Network Analytics
- Network Map
- Guides
- Supported Cloud Services
- Terms and Concepts
- DNS Monitoring
- Network Device MonitoringSetup
- Integrations
- Profiles
- Configuration Management
- Maps
- SNMP Metrics Reference
- Troubleshooting
- Guides
- Terms and Concepts
- NetFlow MonitoringMonitors
- Network PathSetup
- List View
- Path View
- Guides
- Terms and Concepts
- Amazon S3
- Google Cloud Storage
- Azure Blob Storage
- Datadog Costs
- SetupAWS
- Azure
- Google Cloud
- Oracle
- SaaS Integrations
- Custom
- TagsTag Explorer
- Multisource Querying
- AllocationTag Pipelines
- Container Cost Allocation
- BigQuery Costs
- Custom Allocation Rules
- ReportingExplorer
- Scheduled Reports
- RecommendationsCustom Recommendations
- PlanningBudgets
- Commitment Programs
- Cost ChangesMonitors
- Anomalies
- Real-Time Costs
- APM Terms and Concepts
- Application InstrumentationSingle Step Instrumentation
- Manually managed SDKs
- Code-based Custom Instrumentation
- Dynamic Instrumentation
- Library Compatibility
- Library Configuration
- Configuration at Runtime
- Trace Context Propagation
- Serverless Application Tracing
- Proxy Tracing
- Span Tag Semantics
- Span Links
- APM Metrics CollectionTrace Metrics
- Runtime Metrics
- Trace Pipeline ConfigurationIngestion Mechanisms
- Ingestion Controls
- Adaptive Sampling
- Generate Metrics
- Trace Retention
- Usage Metrics
- Correlate Traces with Other TelemetryCorrelate DBM and Traces
- Correlate Logs and Traces
- Correlate RUM and Traces
- Correlate Synthetics and Traces
- Correlate Profiles and Traces
- Trace ExplorerSearch Spans
- Query Syntax
- Trace Queries
- Span Tags and Attributes
- Span Visualizations
- Trace View
- Tag Analysis
- Recommendations
- Code Origin for Spans
- Service ObservabilitySoftware Catalog
- Service Page
- Resource Page
- Deployment Tracking
- Service Map
- Inferred Services
- Remapping Rules for Inferred Entities
- Service Remapping Rules
- Service Override Removal
- APM Monitors
- Endpoint ObservabilityExplore Endpoints
- Monitor Endpoints
- Live Debugger
- Error TrackingIssue States
- Error Tracking Explorer
- Error Grouping
- Monitors
- Identify Suspect Commits
- Exception Replay
- Troubleshooting
- Data Security
- Guides
- TroubleshootingAgent Rate Limits
- Agent APM metrics
- Agent Resource Usage
- Correlated Logs
- PHP 5 Deep Call Stacks
- .NET diagnostic tool
- APM Quantization
- Go Compile-Time Instrumentation
- Tracer Startup Logs
- Tracer Debug Logs
- Connection Errors
- Enabling the ProfilerSupported Language and Tracer Versions
- Java
- Python
- Go
- Ruby
- Node.js
- .NET
- PHP
- C/C++/Rust
- Profile Types
- Profile Visualizations
- Investigate Slow Traces or Endpoints
- Compare Profiles
- Automated Analysis
- Profiler TroubleshootingJava
- Python
- Go
- Ruby
- Node.js
- .NET
- PHP
- C/C++/Rust
- Guides
- Agent Integration Overhead
- Setup Architectures
- Setting Up PostgresSelf-hosted
- RDS
- Aurora
- Google Cloud SQL
- AlloyDB
- Azure
- Supabase
- Heroku
- Advanced Configuration
- Troubleshooting
- Setting Up MySQLSelf-hosted
- RDS
- Aurora
- Google Cloud SQL
- Azure
- Advanced Configuration
- Troubleshooting
- Setting Up SQL ServerSelf-hosted
- RDS
- Azure
- Google Cloud SQL
- Troubleshooting
- Setting Up OracleSelf-hosted
- RDS
- RAC
- Exadata
- Autonomous Database
- Troubleshooting
- Setting Up Amazon DocumentDBAmazon DocumentDB
- Setting Up MongoDBSelf-hosted
- MongoDB Atlas
- Troubleshooting
- Connecting DBM and Traces
- Data Collected
- Exploring Database Hosts
- Exploring Query Metrics
- Exploring Query Samples
- Exploring Database Schemas
- Exploring Recommendations
- Troubleshooting
- Guides
- Setup
- Kafka Messages
- Schema Tracking
- Dead Letter Queues
- Metrics and Tags
- 
- Data WarehousesSnowflake
- Databricks
- BigQuery
- Business Intelligence IntegrationsTableau
- Sigma
- Metabase
- Power BI
- Databricks
- Airflow
- dbt
- Spark on Kubernetes
- Spark on Amazon EMR
- Spark on Google Dataproc
- Custom Jobs (OpenLineage)Datadog Agent for OpenLineage Proxy
- Application MonitoringBrowser
- Android and Android TV
- iOS and tvOS
- Flutter
- Kotlin Multiplatform
- React Native
- Roku
- Unity
- PlatformDashboards
- Monitors
- Generate Custom Metrics
- Exploring RUM DataSearch RUM Events
- Search Syntax
- Group
- Visualize
- Events
- Export
- Saved Views
- Watchdog Insights for RUM
- Correlate RUM with Other TelemetryCorrelate LLM with RUM
- Correlate Logs with RUM
- Correlate Profiling with RUM
- Correlate Synthetics with RUM
- Correlate Traces with RUM
- Feature Flag TrackingSetup
- Using Feature Flags
- Error TrackingExplorer
- Issue States
- Track Browser Errors
- Track Mobile Errors
- Error Grouping
- Monitors
- Identify Suspect Commits
- Troubleshooting
- RUM Without LimitsMetrics
- Retention Filters
- Operations Monitoring
- Ownership of Views
- Guides
- Data Security
- API TestingHTTP
- SSL
- DNS
- WebSocket
- TCP
- UDP
- ICMP
- GRPC
- Error codes
- Multistep API Testing
- Browser TestingRecording Steps
- Browser Testing Results
- Advanced Options for Steps
- Authentication in Browser Testing
- Network Path TestingTerms and Concepts
- Mobile Application TestingTesting Steps
- Testing Results
- Advanced Options for Steps
- Supported Devices
- Restricted Networks
- Settings
- Test Suites
- PlatformDashboards
- Metrics
- Test Coverage
- Private Locations
- Connect APM
- Settings
- Exploring Synthetics DataSaved Views
- Results Explorer
- Guides
- NotificationsTemplate Variables
- Conditional Alerting
- Advanced Notifications
- Integrate with Statuspage
- Troubleshooting
- Data Security
- Local and Staging EnvironmentsTesting Multiple Environments
- Testing With Proxy, Firewall, or VPN
- CI/CD IntegrationsConfiguration
- Azure DevOps Extension
- CircleCI Orb
- GitHub Actions
- GitLab
- Jenkins
- Bitrise (Upload Application)
- Bitrise (Run Tests)
- Settings
- Results Explorer
- Metrics
- Guides
- Troubleshooting
- Vizualizing with ChartsChart Basics
- Pathways Diagram
- Funnel Analysis
- Retention Analysis
- Analytics Explorer
- Dashboards
- Segments
- Managing Profiles
- ExperimentsDefine Metrics
- Reading Experiment Results
- Minimum Detectable Effects
- Guides
- Troubleshooting
- BrowserSetup
- Privacy Options
- Developer Tools
- Troubleshooting
- MobileSetup and Configuration
- Privacy Options
- Developer Tools
- Impact on App Performance
- Troubleshooting
- Playlists
- Heatmaps
- Pipeline VisibilityAWS CodePipeline
- Azure Pipelines
- Buildkite
- CircleCI
- Codefresh
- GitHub Actions
- GitLab
- Jenkins
- TeamCity
- Other CI Providers
- Custom Commands
- Custom Tags and Measures
- Search and Manage
- ExplorerSearch Syntax
- Search Pipeline Executions
- Export
- Saved Views
- [Monitors](https://docs.datadoghq.com/monitors/types/ci/?tab=pipelines)
- Guides
- Troubleshooting
- Deployment VisibilityArgo CD
- CI Providers
- Explore DeploymentsSearch Syntax
- Facets
- Saved Views
- FeaturesCode Changes Detection
- Rollback Detection
- [Monitors](https://docs.datadoghq.com/monitors/types/ci/?tab=deployments)
- Setup
- Explore
- Setup.NET
- Java and JVM Languages
- JavaScript and TypeScript
- Python
- Ruby
- Swift
- Go
- JUnit Report Uploads
- Network Settings
- Tests in Containers
- Repositories
- ExplorerSearch Syntax
- Search Test Runs
- Export
- Saved Views
- [Monitors](https://docs.datadoghq.com/monitors/types/ci/?tab=tests)
- Test Health
- Flaky Test Management
- Working with Flaky TestsEarly Flake Detection
- Auto Test Retries
- Test Impact AnalysisSetup
- How It Works
- Troubleshooting
- Developer Workflows
- Code Coverage
- Instrument Browser Tests with RUM
- Instrument Swift Tests with RUM
- Correlate Logs and Tests
- Guides
- Troubleshooting
- Setup
- Data Collected
- Setup
- SetupDeployment Data Sources
- Failure Data Sources
- Change Failure Detection
- Data Collected
- Client SDKsAndroid and Android TV
- iOS and tvOS
- JavaScript
- React
- Server SDKsGo
- Java
- Node.js
- Python
- Ruby
- MCP Server
- Guides
- Detection RulesOOTB Rules
- NotificationsRules
- Variables
- Suppressions
- Automation PipelinesMute
- Add to Security Inbox
- Set Due Date Rules
- Security Inbox
- Threat Intelligence
- Audit Trail
- Access Control
- Account Takeover Protection
- Ticketing Integrations
- Research Feed
- Guides
- Ingest and EnrichContent Packs
- Bring Your Own Threat Intelligence
- Open Cybersecurity Schema Framework
- Detect and MonitorOOTB Rules
- Custom Detection Rules
- Version History
- Suppressions
- Historical Jobs
- MITRE ATT&CK Map
- Triage and InvestigateInvestigate Security Signals
- Risk Insights
- IOC Explorer
- Investigator
- Respond and ReportSecurity Operational Metrics
- Guides
- Data Security
- Static Code Analysis (SAST)Setup
- GitHub Actions
- Generic CI Providers
- AI-Enhanced Static Code Analysis
- SAST Custom Rule Creation Tutorial
- SAST Custom Rules
- SAST Custom Rules Guide
- Static Code Analysis (SAST) rules
- Software Composition Analysis (SCA)Static Setup
- Runtime Setup
- Library Inventory
- Secret ScanningGitHub Actions
- Generic CI Providers
- Secret Validation
- Runtime Code Analysis (IAST)Setup
- Security Controls
- Infrastructure as Code (IaC) SecuritySetup
- Exclusions
- Rules
- Developer Tool IntegrationsPull Request Comments
- PR Gates
- IDE Plugins
- Git Hooks
- Troubleshooting
- Guides
- SetupSupported Deployment Types
- Agentless Scanning
- Deploy the Agent
- Set Up CloudTrail Logs
- Set Up without Infrastructure Monitoring
- Deploy via Cloud Integrations
- Security Graph
- MisconfigurationsManage Compliance Rules
- Create Custom Rules
- Manage Compliance Posture
- Explore Misconfigurations
- Kubernetes Security Posture Management
- Identity Risks
- VulnerabilitiesHosts and Containers Compatibility
- OOTB Rules
- Review and RemediateMute Issues
- Automate Security Workflows
- Create Jira Issues
- Severity Scoring
- Guides
- TroubleshootingVulnerabilities
- Terms and Concepts
- How It WorksThreat Intelligence
- Trace Qualification
- User Monitoring and Protection
- Setup
- Overview
- Security SignalsAttackers Explorer
- Attacker Fingerprint
- Attacker Clustering
- Users Explorer
- PoliciesCustom Rules
- OOTB Rules
- In-App WAF Rules
- Tracing Library Configuration
- Exploit Prevention
- WAF Integrations
- API Security Inventory
- Guides
- Troubleshooting
- SetupDeploy the Agent
- Workload Protection Agent Variables
- Detection RulesOOTB Rules
- Custom Rules
- Investigate Security Signals
- Investigate Agent Events
- Creating Agent Rule ExpressionsWriting Custom Rule Expressions
- Linux Syntax
- Windows Syntax
- Coverage and Posture ManagementHosts and Containers
- Serverless
- Coverage
- Guides
- Troubleshooting
- SetupTelemetry Data
- Cloud Storage
- Scanning RulesLibrary Rules
- Custom Rules
- Guides
- Quickstart
- InstrumentationAutomatic
- SDK Reference
- HTTP API
- OpenTelemetry
- MonitoringQuerying spans and traces
- Correlate with APM
- Cluster Map
- Agent Monitoring
- MCP Clients
- Prompt Tracking
- Metrics
- Cost
- EvaluationsManaged Evaluations
- Custom LLM-as-a-Judge
- External Evaluations
- Compatibility
- Export API
- Experiments
- Data Security and RBAC
- Terms and Concepts
- Guides
- ConfigurationExplore Templates
- Set Up Pipelines
- Install the Worker
- Live Capture
- Update Existing Pipelines
- Access Control
- SourcesAkamai DataStream
- Amazon Data Firehose
- Amazon S3
- Azure Event Hubs
- Datadog Agent
- Datadog Lambda Extension
- Datadog Lambda Forwarder
- Filebeat
- Fluent
- Google Pub/Sub
- HTTP Client
- HTTP Server
- OpenTelemetry
- Kafka
- Logstash
- Socket
- Splunk HEC
- Splunk TCP
- Sumo Logic Hosted Collector
- Syslog
- ProcessorsAdd Environment Variables
- Add hostname
- Custom Processor
- Deduplicate
- Edit fields
- Enrichment Table
- Filter
- Generate Metrics
- Grok Parser
- Parse JSON
- Parse XML
- Quota
- Reduce
- Remap to OCSF
- Sample
- Sensitive Data Scanner
- Split Array
- Tag Control
- Throttle
- DestinationsAmazon OpenSearch
- Amazon S3
- Amazon Security Lake
- Azure Storage
- CrowdStrike NG-SIEM
- Datadog CloudPrem
- Datadog Logs
- Datadog Metrics
- Elasticsearch
- Google Cloud Storage
- Google Pub/Sub
- Google SecOps
- HTTP Client
- Kafka
- Microsoft Sentinel
- New Relic
- OpenSearch
- SentinelOne
- Socket
- Splunk HEC
- Sumo Logic Hosted Collector
- Syslog
- PacksAkamai CDN
- Amazon CloudFront
- Amazon VPC Flow Logs
- AWS CloudTrail
- Cisco ASA
- Cloudflare
- F5
- Fastly
- Fortinet Firewall
- HAProxy Ingress
- Istio Proxy
- Netskope
- NGINX
- Okta
- Palo Alto Firewall
- Windows XML
- ZScaler ZIA DNS
- Zscaler ZIA Firewall
- Zscaler ZIA Tunnel
- Zscaler ZIA Web Logs
- Search Syntax
- Scaling and PerformanceHandling Load and Backpressure
- Scaling Best Practices
- Monitoring and TroubleshootingWorker CLI Commands
- Monitoring Pipelines
- Pipeline Usage Metrics
- Troubleshooting
- Guides and ResourcesUpgrade Worker Guide
- Log Collection & IntegrationsBrowser
- Android
- iOS
- Flutter
- React Native
- Roku
- Kotlin Multiplatform
- C#
- Go
- Java
- Node.js
- PHP
- Python
- Ruby
- OpenTelemetry
- Agent Integrations
- Other Integrations
- Log ConfigurationPipelines
- Processors
- Parsing
- Pipeline Scanner
- Attributes and Aliasing
- Generate Metrics
- Indexes
- Flex Logs
- Archives
- Rehydrate from Archives
- Archive Search
- Forwarding
- Log ExplorerLive Tail
- Search Logs
- Search Syntax
- Advanced Search
- Facets
- Calculated Fields
- Analytics
- Patterns
- Transactions
- Visualize
- Log Side Panel
- Export
- Watchdog Insights for Logs
- Saved Views
- Error TrackingError Tracking Explorer
- Issue States
- Track Browser and Mobile Errors
- Track Backend Errors
- Error Grouping
- Manage Data Collection
- Dynamic Sampling
- Monitors
- Identify Suspect Commits
- Troubleshooting
- Reports
- Guides
- Data Security
- TroubleshootingLive Tail
- Quickstart
- Architecture
- InstallationAWS EKS
- Azure AKS
- Install Locally with Docker
- Log IngestionDatadog Agent
- Observability Pipelines
- REST API
- ConfigurationDatadog Account
- AWS Configuration
- Azure Configuration
- Cluster Sizing
- Ingress
- Processing
- Reverse Connection
- Management
- Supported Features
- Troubleshooting
- Switching Between Orgs
- Organization SettingsUser Management
- Login Methods
- OAuth Apps
- Custom Organization Landing Page
- Service Accounts
- IP Allowlist
- Domain Allowlist
- Cross-Organization Visibility
- Access ControlGranular Access
- Permissions
- Data Access
- SSO with SAMLConfiguring SAML
- User Group Mapping
- Active Directory
- Auth0
- Entra ID
- Google
- LastPass
- Okta
- SafeNet
- Troubleshooting
- SCIMOkta
- Microsoft Entra ID
- API and Application Keys
- TeamsTeam Management
- Provision with GitHub
- Governance Console
- Multi-Factor Authentication
- Audit TrailEvents
- Forwarding
- Guides
- Safety Center
- Plan and UsageCost Details
- Usage Details
- BillingPricing
- Credit Card
- Product Allotments
- Usage Metrics
- Usage Attribution
- Custom Metrics
- Containers
- Log Management
- APM
- Serverless
- Real User Monitoring
- CI Visibility
- Incident Response
- AWS Integration
- Azure Integration
- Google Cloud Integration
- Alibaba Integration
- vSphere Integration
- Workflow Automation
- Multi-org Accounts
- Guides
- Cloud-based Authentication
- Agent
- Cloud SIEM
- Kubernetes
- Log Management
- Real User Monitoring
- Synthetic Monitoring
- Tracing
- PCI Compliance
- HIPAA Compliance
- Data Retention Periods
- Guides
- 
Docs > 
API Reference > 
Logs ArchivesLanguage

English[English](https://docs.datadoghq.com/api/latest/logs-archives/?lang_pref=en)
[Français](https://docs.datadoghq.com/fr/api/latest/logs-archives/?lang_pref=fr)
[日本語](https://docs.datadoghq.com/ja/api/latest/logs-archives/?lang_pref=ja)
[한국어](https://docs.datadoghq.com/ko/api/latest/logs-archives/?lang_pref=ko)
[Español](https://docs.datadoghq.com/es/api/latest/logs-archives/?lang_pref=es)Datadog Site

US1
US3
US5
EU
AP1
AP2
US1-FED# Logs ArchivesArchives forward all the logs ingested to a cloud storage system.
See the Archives Page
for a list of the archives currently configured in Datadog.## Get all archives- v2 (latest)
GET https://api.ap1.datadoghq.com/api/v2/logs/config/archiveshttps://api.ap2.datadoghq.com/api/v2/logs/config/archiveshttps://api.datadoghq.eu/api/v2/logs/config/archiveshttps://api.ddog-gov.com/api/v2/logs/config/archiveshttps://api.datadoghq.com/api/v2/logs/config/archiveshttps://api.us3.datadoghq.com/api/v2/logs/config/archiveshttps://api.us5.datadoghq.com/api/v2/logs/config/archives
### OverviewGet the list of configured logs archives with their definitions.
This endpoint requires the `logs_read_archives` permission.### Response- 200
- 403
- 429
OK
- Model
- Example
The available archives.
Expand All
Field
Type
Description
data
[object]
A list of archives.
attributes
object
The attributes associated with the archive.
destination [*required*]
object <oneOf>
An archive's destination.
Option 1
object
The Azure archive destination.
container [*required*]
string
The container where the archive will be stored.
integration [*required*]
object
The Azure archive's integration destination.
client_id [*required*]
string
A client ID.
tenant_id [*required*]
string
A tenant ID.
path
string
The archive path.
region
string
The region where the archive will be stored.
storage_account [*required*]
string
The associated storage account.
type [*required*]
enum
Type of the Azure archive destination.
Allowed enum values: `azure`default: `azure`
Option 2
object
The GCS archive destination.
bucket [*required*]
string
The bucket where the archive will be stored.
integration [*required*]
object
The GCS archive's integration destination.
client_email [*required*]
string
A client email.
project_id
string
A project ID.
path
string
The archive path.
type [*required*]
enum
Type of the GCS archive destination.
Allowed enum values: `gcs`default: `gcs`
Option 3
object
The S3 archive destination.
bucket [*required*]
string
The bucket where the archive will be stored.
encryption
object
The S3 encryption settings.
key
string
An Amazon Resource Name (ARN) used to identify an AWS KMS key.
type [*required*]
enum
Type of S3 encryption for a destination.
Allowed enum values: `NO_OVERRIDE,SSE_S3,SSE_KMS` integration [*required*]
object
The S3 Archive's integration destination.
account_id [*required*]
string
The account ID for the integration.
role_name [*required*]
string
The path of the integration.
path
string
The archive path.
storage_class
enum
The storage class where the archive will be stored.
Allowed enum values: `STANDARD,STANDARD_IA,ONEZONE_IA,INTELLIGENT_TIERING,GLACIER_IR`default: `STANDARD`
type [*required*]
enum
Type of the S3 archive destination.
Allowed enum values: `s3`default: `s3`
include_tags
boolean
To store the tags in the archive, set the value "true".
If it is set to "false", the tags will be deleted when the logs are sent to the archive.name [*required*]
string
The archive name.
query [*required*]
string
The archive query/filter. Logs matching this query are included in the archive.
rehydration_max_scan_size_in_gb
int64
Maximum scan size for rehydration from this archive.
rehydration_tags
[string]
An array of tags to add to rehydrated logs from an archive.
state
enum
The state of the archive.
Allowed enum values: `UNKNOWN,WORKING,FAILING,WORKING_AUTH_LEGACY`id
string
The archive ID.
type [*required*]
string
The type of the resource. The value should always be archives.
default: `archives`
```
{
"data": [
{
"attributes": {
"destination": {
"container": "container-name",
"integration": {
"client_id": "aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa",
"tenant_id": "aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa"
},
"path": "string",
"region": "string",
"storage_account": "account-name",
"type": "azure"
},
"include_tags": false,
"name": "Nginx Archive",
"query": "source:nginx",
"rehydration_max_scan_size_in_gb": 100,
"rehydration_tags": [
"team:intake",
"team:app"
],
"state": "WORKING"
},
"id": "a2zcMylnM4OCHpYusxIi3g",
"type": "archives"
}
]
}
```Forbidden
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```Too many requests
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```### Code Example- [Curl](?code-lang=curl#)
- [Python](?code-lang=python#)
- [Ruby](?code-lang=ruby#)
- [Go](?code-lang=go#)
- [Java](?code-lang=java#)
- [Rust](?code-lang=rust#)
- [Typescript](?code-lang=typescript#)

Get all archivesCopy```

# Curl commandcurl -X GET "https://api.ap1.datadoghq.com"https://api.ap2.datadoghq.com"https://api.datadoghq.eu"https://api.ddog-gov.com"https://api.datadoghq.com"https://api.us3.datadoghq.com"https://api.us5.datadoghq.com/api/v2/logs/config/archives" \
-H "Accept: application/json" \
-H "DD-API-KEY: ${DD_API_KEY}" \
-H "DD-APPLICATION-KEY: ${DD_APP_KEY}"

```
Get all archives```
"""
Get all archives returns "OK" response
"""

from datadog_api_client import ApiClient, Configuration
from datadog_api_client.v2.api.logs_archives_api import LogsArchivesApi

configuration = Configuration()
with ApiClient(configuration) as api_client:
api_instance = LogsArchivesApi(api_client)
response = api_instance.list_logs_archives()

print(response)

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=python) and then save the example to `example.py` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" python3 "example.py"`
```
Get all archives```
# Get all archives returns "OK" response

require "datadog_api_client"
api_instance = DatadogAPIClient::V2::LogsArchivesAPI.new
p api_instance.list_logs_archives()

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=ruby) and then save the example to `example.rb` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" rb "example.rb"`
```
Get all archives```
// Get all archives returns "OK" response

package main

import (
	"context"
	"encoding/json"
	"fmt"
	"os"

	"github.com/DataDog/datadog-api-client-go/v2/api/datadog"
	"github.com/DataDog/datadog-api-client-go/v2/api/datadogV2"
)

func main() {
	ctx := datadog.NewDefaultContext(context.Background())
	configuration := datadog.NewConfiguration()
	apiClient := datadog.NewAPIClient(configuration)
	api := datadogV2.NewLogsArchivesApi(apiClient)
	resp, r, err := api.ListLogsArchives(ctx)

	if err != nil {
		fmt.Fprintf(os.Stderr, "Error when calling `LogsArchivesApi.ListLogsArchives`: %v\n", err)
		fmt.Fprintf(os.Stderr, "Full HTTP response: %v\n", r)
	}

	responseContent, _ := json.MarshalIndent(resp, "", " ")
	fmt.Fprintf(os.Stdout, "Response from `LogsArchivesApi.ListLogsArchives`:\n%s\n", responseContent)
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=go) and then save the example to `main.go` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" go run "main.go"`
```
Get all archives```
// Get all archives returns "OK" response

import com.datadog.api.client.ApiClient;
import com.datadog.api.client.ApiException;
import com.datadog.api.client.v2.api.LogsArchivesApi;
import com.datadog.api.client.v2.model.LogsArchives;

public class Example {
public static void main(String[] args) {
ApiClient defaultClient = ApiClient.getDefaultApiClient();
LogsArchivesApi apiInstance = new LogsArchivesApi(defaultClient);

try {
LogsArchives result = apiInstance.listLogsArchives();
System.out.println(result);
} catch (ApiException e) {
System.err.println("Exception when calling LogsArchivesApi#listLogsArchives");
System.err.println("Status code: " + e.getCode());
System.err.println("Reason: " + e.getResponseBody());
System.err.println("Response headers: " + e.getResponseHeaders());
e.printStackTrace();
}
}
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=java) and then save the example to `Example.java` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" java "Example.java"`
```
Get all archives```
// Get all archives returns "OK" response
use datadog_api_client::datadog;
use datadog_api_client::datadogV2::api_logs_archives::LogsArchivesAPI;

#[tokio::main]
async fn main() {
let configuration = datadog::Configuration::new();
let api = LogsArchivesAPI::with_config(configuration);
let resp = api.list_logs_archives().await;
if let Ok(value) = resp {
println!("{:#?}", value);
} else {
println!("{:#?}", resp.unwrap_err());
}
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=rust) and then save the example to `src/main.rs` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" cargo run`
```
Get all archives```
/**
* Get all archives returns "OK" response
*/

import { client, v2 } from "@datadog/datadog-api-client";

const configuration = client.createConfiguration();
const apiInstance = new v2.LogsArchivesApi(configuration);

apiInstance
.listLogsArchives()
.then((data: v2.LogsArchives) => {
console.log(
"API called successfully. Returned data: " + JSON.stringify(data)
);
})
.catch((error: any) => console.error(error));

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=typescript) and then save the example to `example.ts` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" tsc "example.ts"`
```## Create an archive- v2 (latest)
POST https://api.ap1.datadoghq.com/api/v2/logs/config/archiveshttps://api.ap2.datadoghq.com/api/v2/logs/config/archiveshttps://api.datadoghq.eu/api/v2/logs/config/archiveshttps://api.ddog-gov.com/api/v2/logs/config/archiveshttps://api.datadoghq.com/api/v2/logs/config/archiveshttps://api.us3.datadoghq.com/api/v2/logs/config/archiveshttps://api.us5.datadoghq.com/api/v2/logs/config/archives
### OverviewCreate an archive in your organization.
This endpoint requires the `logs_write_archives` permission.### Request#### Body Data (required)The definition of the new archive.
- Model
- Example
Expand All
Field
Type
Description
data
object
The definition of an archive.
attributes
object
The attributes associated with the archive.
destination [*required*]
 <oneOf>
An archive's destination.
Option 1
object
The Azure archive destination.
container [*required*]
string
The container where the archive will be stored.
integration [*required*]
object
The Azure archive's integration destination.
client_id [*required*]
string
A client ID.
tenant_id [*required*]
string
A tenant ID.
path
string
The archive path.
region
string
The region where the archive will be stored.
storage_account [*required*]
string
The associated storage account.
type [*required*]
enum
Type of the Azure archive destination.
Allowed enum values: `azure`default: `azure`
Option 2
object
The GCS archive destination.
bucket [*required*]
string
The bucket where the archive will be stored.
integration [*required*]
object
The GCS archive's integration destination.
client_email [*required*]
string
A client email.
project_id
string
A project ID.
path
string
The archive path.
type [*required*]
enum
Type of the GCS archive destination.
Allowed enum values: `gcs`default: `gcs`
Option 3
object
The S3 archive destination.
bucket [*required*]
string
The bucket where the archive will be stored.
encryption
object
The S3 encryption settings.
key
string
An Amazon Resource Name (ARN) used to identify an AWS KMS key.
type [*required*]
enum
Type of S3 encryption for a destination.
Allowed enum values: `NO_OVERRIDE,SSE_S3,SSE_KMS` integration [*required*]
object
The S3 Archive's integration destination.
account_id [*required*]
string
The account ID for the integration.
role_name [*required*]
string
The path of the integration.
path
string
The archive path.
storage_class
enum
The storage class where the archive will be stored.
Allowed enum values: `STANDARD,STANDARD_IA,ONEZONE_IA,INTELLIGENT_TIERING,GLACIER_IR`default: `STANDARD`
type [*required*]
enum
Type of the S3 archive destination.
Allowed enum values: `s3`default: `s3`
include_tags
boolean
To store the tags in the archive, set the value "true".
If it is set to "false", the tags will be deleted when the logs are sent to the archive.name [*required*]
string
The archive name.
query [*required*]
string
The archive query/filter. Logs matching this query are included in the archive.
rehydration_max_scan_size_in_gb
int64
Maximum scan size for rehydration from this archive.
rehydration_tags
[string]
An array of tags to add to rehydrated logs from an archive.
type [*required*]
string
The type of the resource. The value should always be archives.
default: `archives`
```
{
"data": {
"attributes": {
"destination": {
"container": "container-name",
"integration": {
"client_id": "aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa",
"tenant_id": "aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa"
},
"path": "string",
"region": "string",
"storage_account": "account-name",
"type": "azure"
},
"include_tags": false,
"name": "Nginx Archive",
"query": "source:nginx",
"rehydration_max_scan_size_in_gb": 100,
"rehydration_tags": [
"team:intake",
"team:app"
]
},
"type": "archives"
}
}
```### Response- 200
- 400
- 403
- 429
OK
- Model
- Example
The logs archive.
Expand All
Field
Type
Description
data
object
The definition of an archive.
attributes
object
The attributes associated with the archive.
destination [*required*]
object <oneOf>
An archive's destination.
Option 1
object
The Azure archive destination.
container [*required*]
string
The container where the archive will be stored.
integration [*required*]
object
The Azure archive's integration destination.
client_id [*required*]
string
A client ID.
tenant_id [*required*]
string
A tenant ID.
path
string
The archive path.
region
string
The region where the archive will be stored.
storage_account [*required*]
string
The associated storage account.
type [*required*]
enum
Type of the Azure archive destination.
Allowed enum values: `azure`default: `azure`
Option 2
object
The GCS archive destination.
bucket [*required*]
string
The bucket where the archive will be stored.
integration [*required*]
object
The GCS archive's integration destination.
client_email [*required*]
string
A client email.
project_id
string
A project ID.
path
string
The archive path.
type [*required*]
enum
Type of the GCS archive destination.
Allowed enum values: `gcs`default: `gcs`
Option 3
object
The S3 archive destination.
bucket [*required*]
string
The bucket where the archive will be stored.
encryption
object
The S3 encryption settings.
key
string
An Amazon Resource Name (ARN) used to identify an AWS KMS key.
type [*required*]
enum
Type of S3 encryption for a destination.
Allowed enum values: `NO_OVERRIDE,SSE_S3,SSE_KMS` integration [*required*]
object
The S3 Archive's integration destination.
account_id [*required*]
string
The account ID for the integration.
role_name [*required*]
string
The path of the integration.
path
string
The archive path.
storage_class
enum
The storage class where the archive will be stored.
Allowed enum values: `STANDARD,STANDARD_IA,ONEZONE_IA,INTELLIGENT_TIERING,GLACIER_IR`default: `STANDARD`
type [*required*]
enum
Type of the S3 archive destination.
Allowed enum values: `s3`default: `s3`
include_tags
boolean
To store the tags in the archive, set the value "true".
If it is set to "false", the tags will be deleted when the logs are sent to the archive.name [*required*]
string
The archive name.
query [*required*]
string
The archive query/filter. Logs matching this query are included in the archive.
rehydration_max_scan_size_in_gb
int64
Maximum scan size for rehydration from this archive.
rehydration_tags
[string]
An array of tags to add to rehydrated logs from an archive.
state
enum
The state of the archive.
Allowed enum values: `UNKNOWN,WORKING,FAILING,WORKING_AUTH_LEGACY`id
string
The archive ID.
type [*required*]
string
The type of the resource. The value should always be archives.
default: `archives`
```
{
"data": {
"attributes": {
"destination": {
"container": "container-name",
"integration": {
"client_id": "aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa",
"tenant_id": "aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa"
},
"path": "string",
"region": "string",
"storage_account": "account-name",
"type": "azure"
},
"include_tags": false,
"name": "Nginx Archive",
"query": "source:nginx",
"rehydration_max_scan_size_in_gb": 100,
"rehydration_tags": [
"team:intake",
"team:app"
],
"state": "WORKING"
},
"id": "a2zcMylnM4OCHpYusxIi3g",
"type": "archives"
}
}
```Bad Request
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```Forbidden
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```Too many requests
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```### Code Example- [Curl](?code-lang=curl#)
- [Python](?code-lang=python#)
- [Ruby](?code-lang=ruby#)
- [Go](?code-lang=go#)
- [Java](?code-lang=java#)
- [Rust](?code-lang=rust#)
- [Typescript](?code-lang=typescript#)

Create an archiveCopy```

# Curl commandcurl -X POST "https://api.ap1.datadoghq.com"https://api.ap2.datadoghq.com"https://api.datadoghq.eu"https://api.ddog-gov.com"https://api.datadoghq.com"https://api.us3.datadoghq.com"https://api.us5.datadoghq.com/api/v2/logs/config/archives" \
-H "Accept: application/json" \
-H "Content-Type: application/json" \
-H "DD-API-KEY: ${DD_API_KEY}" \
-H "DD-APPLICATION-KEY: ${DD_APP_KEY}" \
-d @- << EOF
{
"data": {
"attributes": {
"destination": {
"integration": {
"client_id": "aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa",
"tenant_id": "aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa"
}
},
"name": "Nginx Archive",
"query": "source:nginx"
},
"type": "archives"
}
}
EOF

```
Create an archive```
"""
Create an archive returns "OK" response
"""

from datadog_api_client import ApiClient, Configuration
from datadog_api_client.v2.api.logs_archives_api import LogsArchivesApi
from datadog_api_client.v2.model.logs_archive_create_request import LogsArchiveCreateRequest
from datadog_api_client.v2.model.logs_archive_create_request_attributes import LogsArchiveCreateRequestAttributes
from datadog_api_client.v2.model.logs_archive_create_request_definition import LogsArchiveCreateRequestDefinition
from datadog_api_client.v2.model.logs_archive_destination_azure import LogsArchiveDestinationAzure
from datadog_api_client.v2.model.logs_archive_destination_azure_type import LogsArchiveDestinationAzureType
from datadog_api_client.v2.model.logs_archive_integration_azure import LogsArchiveIntegrationAzure

body = LogsArchiveCreateRequest(
data=LogsArchiveCreateRequestDefinition(
attributes=LogsArchiveCreateRequestAttributes(
destination=LogsArchiveDestinationAzure(
container="container-name",
integration=LogsArchiveIntegrationAzure(
client_id="aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa",
tenant_id="aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa",
),
storage_account="account-name",
type=LogsArchiveDestinationAzureType.AZURE,
),
include_tags=False,
name="Nginx Archive",
query="source:nginx",
rehydration_max_scan_size_in_gb=100,
rehydration_tags=[
"team:intake",
"team:app",
],
),
type="archives",
),
)

configuration = Configuration()
with ApiClient(configuration) as api_client:
api_instance = LogsArchivesApi(api_client)
response = api_instance.create_logs_archive(body=body)

print(response)

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=python) and then save the example to `example.py` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" python3 "example.py"`
```
Create an archive```
# Create an archive returns "OK" response

require "datadog_api_client"
api_instance = DatadogAPIClient::V2::LogsArchivesAPI.new

body = DatadogAPIClient::V2::LogsArchiveCreateRequest.new({
data: DatadogAPIClient::V2::LogsArchiveCreateRequestDefinition.new({
attributes: DatadogAPIClient::V2::LogsArchiveCreateRequestAttributes.new({
destination: DatadogAPIClient::V2::LogsArchiveDestinationAzure.new({
container: "container-name",
integration: DatadogAPIClient::V2::LogsArchiveIntegrationAzure.new({
client_id: "aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa",
tenant_id: "aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa",
}),
storage_account: "account-name",
type: DatadogAPIClient::V2::LogsArchiveDestinationAzureType::AZURE,
}),
include_tags: false,
name: "Nginx Archive",
query: "source:nginx",
rehydration_max_scan_size_in_gb: 100,
rehydration_tags: [
"team:intake",
"team:app",
],
}),
type: "archives",
}),
})
p api_instance.create_logs_archive(body)

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=ruby) and then save the example to `example.rb` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" rb "example.rb"`
```
Create an archive```
// Create an archive returns "OK" response

package main

import (
	"context"
	"encoding/json"
	"fmt"
	"os"

	"github.com/DataDog/datadog-api-client-go/v2/api/datadog"
	"github.com/DataDog/datadog-api-client-go/v2/api/datadogV2"
)

func main() {
	body := datadogV2.LogsArchiveCreateRequest{
		Data: &datadogV2.LogsArchiveCreateRequestDefinition{
			Attributes: &datadogV2.LogsArchiveCreateRequestAttributes{
				Destination: datadogV2.LogsArchiveCreateRequestDestination{
					LogsArchiveDestinationAzure: &datadogV2.LogsArchiveDestinationAzure{
						Container: "container-name",
						Integration: datadogV2.LogsArchiveIntegrationAzure{
							ClientId: "aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa",
							TenantId: "aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa",
						},
						StorageAccount: "account-name",
						Type: datadogV2.LOGSARCHIVEDESTINATIONAZURETYPE_AZURE,
					}},
				IncludeTags: datadog.PtrBool(false),
				Name: "Nginx Archive",
				Query: "source:nginx",
				RehydrationMaxScanSizeInGb: *datadog.NewNullableInt64(datadog.PtrInt64(100)),
				RehydrationTags: []string{
					"team:intake",
					"team:app",
				},
			},
			Type: "archives",
		},
	}
	ctx := datadog.NewDefaultContext(context.Background())
	configuration := datadog.NewConfiguration()
	apiClient := datadog.NewAPIClient(configuration)
	api := datadogV2.NewLogsArchivesApi(apiClient)
	resp, r, err := api.CreateLogsArchive(ctx, body)

	if err != nil {
		fmt.Fprintf(os.Stderr, "Error when calling `LogsArchivesApi.CreateLogsArchive`: %v\n", err)
		fmt.Fprintf(os.Stderr, "Full HTTP response: %v\n", r)
	}

	responseContent, _ := json.MarshalIndent(resp, "", " ")
	fmt.Fprintf(os.Stdout, "Response from `LogsArchivesApi.CreateLogsArchive`:\n%s\n", responseContent)
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=go) and then save the example to `main.go` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" go run "main.go"`
```
Create an archive```
// Create an archive returns "OK" response

import com.datadog.api.client.ApiClient;
import com.datadog.api.client.ApiException;
import com.datadog.api.client.v2.api.LogsArchivesApi;
import com.datadog.api.client.v2.model.LogsArchive;
import com.datadog.api.client.v2.model.LogsArchiveCreateRequest;
import com.datadog.api.client.v2.model.LogsArchiveCreateRequestAttributes;
import com.datadog.api.client.v2.model.LogsArchiveCreateRequestDefinition;
import com.datadog.api.client.v2.model.LogsArchiveCreateRequestDestination;
import com.datadog.api.client.v2.model.LogsArchiveDestinationAzure;
import com.datadog.api.client.v2.model.LogsArchiveDestinationAzureType;
import com.datadog.api.client.v2.model.LogsArchiveIntegrationAzure;
import java.util.Arrays;

public class Example {
public static void main(String[] args) {
ApiClient defaultClient = ApiClient.getDefaultApiClient();
LogsArchivesApi apiInstance = new LogsArchivesApi(defaultClient);

LogsArchiveCreateRequest body =
new LogsArchiveCreateRequest()
.data(
new LogsArchiveCreateRequestDefinition()
.attributes(
new LogsArchiveCreateRequestAttributes()
.destination(
new LogsArchiveCreateRequestDestination(
new LogsArchiveDestinationAzure()
.container("container-name")
.integration(
new LogsArchiveIntegrationAzure()
.clientId("aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa")
.tenantId("aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa"))
.storageAccount("account-name")
.type(LogsArchiveDestinationAzureType.AZURE)))
.includeTags(false)
.name("Nginx Archive")
.query("source:nginx")
.rehydrationMaxScanSizeInGb(100L)
.rehydrationTags(Arrays.asList("team:intake", "team:app")))
.type("archives"));

try {
LogsArchive result = apiInstance.createLogsArchive(body);
System.out.println(result);
} catch (ApiException e) {
System.err.println("Exception when calling LogsArchivesApi#createLogsArchive");
System.err.println("Status code: " + e.getCode());
System.err.println("Reason: " + e.getResponseBody());
System.err.println("Response headers: " + e.getResponseHeaders());
e.printStackTrace();
}
}
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=java) and then save the example to `Example.java` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" java "Example.java"`
```
Create an archive```
// Create an archive returns "OK" response
use datadog_api_client::datadog;
use datadog_api_client::datadogV2::api_logs_archives::LogsArchivesAPI;
use datadog_api_client::datadogV2::model::LogsArchiveCreateRequest;
use datadog_api_client::datadogV2::model::LogsArchiveCreateRequestAttributes;
use datadog_api_client::datadogV2::model::LogsArchiveCreateRequestDefinition;
use datadog_api_client::datadogV2::model::LogsArchiveCreateRequestDestination;
use datadog_api_client::datadogV2::model::LogsArchiveDestinationAzure;
use datadog_api_client::datadogV2::model::LogsArchiveDestinationAzureType;
use datadog_api_client::datadogV2::model::LogsArchiveIntegrationAzure;

#[tokio::main]
async fn main() {
let body = LogsArchiveCreateRequest::new().data(
LogsArchiveCreateRequestDefinition::new("archives".to_string()).attributes(
LogsArchiveCreateRequestAttributes::new(
LogsArchiveCreateRequestDestination::LogsArchiveDestinationAzure(Box::new(
LogsArchiveDestinationAzure::new(
"container-name".to_string(),
LogsArchiveIntegrationAzure::new(
"aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa".to_string(),
"aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa".to_string(),
),
"account-name".to_string(),
LogsArchiveDestinationAzureType::AZURE,
),
)),
"Nginx Archive".to_string(),
"source:nginx".to_string(),
)
.include_tags(false)
.rehydration_max_scan_size_in_gb(Some(100))
.rehydration_tags(vec!["team:intake".to_string(), "team:app".to_string()]),
),
);
let configuration = datadog::Configuration::new();
let api = LogsArchivesAPI::with_config(configuration);
let resp = api.create_logs_archive(body).await;
if let Ok(value) = resp {
println!("{:#?}", value);
} else {
println!("{:#?}", resp.unwrap_err());
}
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=rust) and then save the example to `src/main.rs` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" cargo run`
```
Create an archive```
/**
* Create an archive returns "OK" response
*/

import { client, v2 } from "@datadog/datadog-api-client";

const configuration = client.createConfiguration();
const apiInstance = new v2.LogsArchivesApi(configuration);

const params: v2.LogsArchivesApiCreateLogsArchiveRequest = {
body: {
data: {
attributes: {
destination: {
container: "container-name",
integration: {
clientId: "aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa",
tenantId: "aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa",
},
storageAccount: "account-name",
type: "azure",
},
includeTags: false,
name: "Nginx Archive",
query: "source:nginx",
rehydrationMaxScanSizeInGb: 100,
rehydrationTags: ["team:intake", "team:app"],
},
type: "archives",
},
},
};

apiInstance
.createLogsArchive(params)
.then((data: v2.LogsArchive) => {
console.log(
"API called successfully. Returned data: " + JSON.stringify(data)
);
})
.catch((error: any) => console.error(error));

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=typescript) and then save the example to `example.ts` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" tsc "example.ts"`
```## Get an archive- v2 (latest)
GET https://api.ap1.datadoghq.com/api/v2/logs/config/archives/{archive_id}https://api.ap2.datadoghq.com/api/v2/logs/config/archives/{archive_id}https://api.datadoghq.eu/api/v2/logs/config/archives/{archive_id}https://api.ddog-gov.com/api/v2/logs/config/archives/{archive_id}https://api.datadoghq.com/api/v2/logs/config/archives/{archive_id}https://api.us3.datadoghq.com/api/v2/logs/config/archives/{archive_id}https://api.us5.datadoghq.com/api/v2/logs/config/archives/{archive_id}
### OverviewGet a specific archive from your organization.
This endpoint requires the `logs_read_archives` permission.### Arguments#### Path ParametersName
Type
Description
archive_id [*required*]
string
The ID of the archive.
### Response- 200
- 400
- 403
- 404
- 429
OK
- Model
- Example
The logs archive.
Expand All
Field
Type
Description
data
object
The definition of an archive.
attributes
object
The attributes associated with the archive.
destination [*required*]
object <oneOf>
An archive's destination.
Option 1
object
The Azure archive destination.
container [*required*]
string
The container where the archive will be stored.
integration [*required*]
object
The Azure archive's integration destination.
client_id [*required*]
string
A client ID.
tenant_id [*required*]
string
A tenant ID.
path
string
The archive path.
region
string
The region where the archive will be stored.
storage_account [*required*]
string
The associated storage account.
type [*required*]
enum
Type of the Azure archive destination.
Allowed enum values: `azure`default: `azure`
Option 2
object
The GCS archive destination.
bucket [*required*]
string
The bucket where the archive will be stored.
integration [*required*]
object
The GCS archive's integration destination.
client_email [*required*]
string
A client email.
project_id
string
A project ID.
path
string
The archive path.
type [*required*]
enum
Type of the GCS archive destination.
Allowed enum values: `gcs`default: `gcs`
Option 3
object
The S3 archive destination.
bucket [*required*]
string
The bucket where the archive will be stored.
encryption
object
The S3 encryption settings.
key
string
An Amazon Resource Name (ARN) used to identify an AWS KMS key.
type [*required*]
enum
Type of S3 encryption for a destination.
Allowed enum values: `NO_OVERRIDE,SSE_S3,SSE_KMS` integration [*required*]
object
The S3 Archive's integration destination.
account_id [*required*]
string
The account ID for the integration.
role_name [*required*]
string
The path of the integration.
path
string
The archive path.
storage_class
enum
The storage class where the archive will be stored.
Allowed enum values: `STANDARD,STANDARD_IA,ONEZONE_IA,INTELLIGENT_TIERING,GLACIER_IR`default: `STANDARD`
type [*required*]
enum
Type of the S3 archive destination.
Allowed enum values: `s3`default: `s3`
include_tags
boolean
To store the tags in the archive, set the value "true".
If it is set to "false", the tags will be deleted when the logs are sent to the archive.name [*required*]
string
The archive name.
query [*required*]
string
The archive query/filter. Logs matching this query are included in the archive.
rehydration_max_scan_size_in_gb
int64
Maximum scan size for rehydration from this archive.
rehydration_tags
[string]
An array of tags to add to rehydrated logs from an archive.
state
enum
The state of the archive.
Allowed enum values: `UNKNOWN,WORKING,FAILING,WORKING_AUTH_LEGACY`id
string
The archive ID.
type [*required*]
string
The type of the resource. The value should always be archives.
default: `archives`
```
{
"data": {
"attributes": {
"destination": {
"container": "container-name",
"integration": {
"client_id": "aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa",
"tenant_id": "aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa"
},
"path": "string",
"region": "string",
"storage_account": "account-name",
"type": "azure"
},
"include_tags": false,
"name": "Nginx Archive",
"query": "source:nginx",
"rehydration_max_scan_size_in_gb": 100,
"rehydration_tags": [
"team:intake",
"team:app"
],
"state": "WORKING"
},
"id": "a2zcMylnM4OCHpYusxIi3g",
"type": "archives"
}
}
```Bad Request
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```Forbidden
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```Not found
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```Too many requests
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```### Code Example- [Curl](?code-lang=curl#)
- [Python](?code-lang=python#)
- [Ruby](?code-lang=ruby#)
- [Go](?code-lang=go#)
- [Java](?code-lang=java#)
- [Rust](?code-lang=rust#)
- [Typescript](?code-lang=typescript#)

Get an archiveCopy```

# Path parametersexport archive_id="CHANGE_ME"# Curl commandcurl -X GET "https://api.ap1.datadoghq.com"https://api.ap2.datadoghq.com"https://api.datadoghq.eu"https://api.ddog-gov.com"https://api.datadoghq.com"https://api.us3.datadoghq.com"https://api.us5.datadoghq.com/api/v2/logs/config/archives/${archive_id}" \
-H "Accept: application/json" \
-H "DD-API-KEY: ${DD_API_KEY}" \
-H "DD-APPLICATION-KEY: ${DD_APP_KEY}"

```
Get an archive```
"""
Get an archive returns "OK" response
"""

from datadog_api_client import ApiClient, Configuration
from datadog_api_client.v2.api.logs_archives_api import LogsArchivesApi

configuration = Configuration()
with ApiClient(configuration) as api_client:
api_instance = LogsArchivesApi(api_client)
response = api_instance.get_logs_archive(
archive_id="archive_id",
)

print(response)

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=python) and then save the example to `example.py` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" python3 "example.py"`
```
Get an archive```
# Get an archive returns "OK" response

require "datadog_api_client"
api_instance = DatadogAPIClient::V2::LogsArchivesAPI.new
p api_instance.get_logs_archive("archive_id")

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=ruby) and then save the example to `example.rb` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" rb "example.rb"`
```
Get an archive```
// Get an archive returns "OK" response

package main

import (
	"context"
	"encoding/json"
	"fmt"
	"os"

	"github.com/DataDog/datadog-api-client-go/v2/api/datadog"
	"github.com/DataDog/datadog-api-client-go/v2/api/datadogV2"
)

func main() {
	ctx := datadog.NewDefaultContext(context.Background())
	configuration := datadog.NewConfiguration()
	apiClient := datadog.NewAPIClient(configuration)
	api := datadogV2.NewLogsArchivesApi(apiClient)
	resp, r, err := api.GetLogsArchive(ctx, "archive_id")

	if err != nil {
		fmt.Fprintf(os.Stderr, "Error when calling `LogsArchivesApi.GetLogsArchive`: %v\n", err)
		fmt.Fprintf(os.Stderr, "Full HTTP response: %v\n", r)
	}

	responseContent, _ := json.MarshalIndent(resp, "", " ")
	fmt.Fprintf(os.Stdout, "Response from `LogsArchivesApi.GetLogsArchive`:\n%s\n", responseContent)
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=go) and then save the example to `main.go` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" go run "main.go"`
```
Get an archive```
// Get an archive returns "OK" response

import com.datadog.api.client.ApiClient;
import com.datadog.api.client.ApiException;
import com.datadog.api.client.v2.api.LogsArchivesApi;
import com.datadog.api.client.v2.model.LogsArchive;

public class Example {
public static void main(String[] args) {
ApiClient defaultClient = ApiClient.getDefaultApiClient();
LogsArchivesApi apiInstance = new LogsArchivesApi(defaultClient);

try {
LogsArchive result = apiInstance.getLogsArchive("archive_id");
System.out.println(result);
} catch (ApiException e) {
System.err.println("Exception when calling LogsArchivesApi#getLogsArchive");
System.err.println("Status code: " + e.getCode());
System.err.println("Reason: " + e.getResponseBody());
System.err.println("Response headers: " + e.getResponseHeaders());
e.printStackTrace();
}
}
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=java) and then save the example to `Example.java` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" java "Example.java"`
```
Get an archive```
// Get an archive returns "OK" response
use datadog_api_client::datadog;
use datadog_api_client::datadogV2::api_logs_archives::LogsArchivesAPI;

#[tokio::main]
async fn main() {
let configuration = datadog::Configuration::new();
let api = LogsArchivesAPI::with_config(configuration);
let resp = api.get_logs_archive("archive_id".to_string()).await;
if let Ok(value) = resp {
println!("{:#?}", value);
} else {
println!("{:#?}", resp.unwrap_err());
}
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=rust) and then save the example to `src/main.rs` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" cargo run`
```
Get an archive```
/**
* Get an archive returns "OK" response
*/

import { client, v2 } from "@datadog/datadog-api-client";

const configuration = client.createConfiguration();
const apiInstance = new v2.LogsArchivesApi(configuration);

const params: v2.LogsArchivesApiGetLogsArchiveRequest = {
archiveId: "archive_id",
};

apiInstance
.getLogsArchive(params)
.then((data: v2.LogsArchive) => {
console.log(
"API called successfully. Returned data: " + JSON.stringify(data)
);
})
.catch((error: any) => console.error(error));

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=typescript) and then save the example to `example.ts` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" tsc "example.ts"`
```## Update an archive- v2 (latest)
PUT https://api.ap1.datadoghq.com/api/v2/logs/config/archives/{archive_id}https://api.ap2.datadoghq.com/api/v2/logs/config/archives/{archive_id}https://api.datadoghq.eu/api/v2/logs/config/archives/{archive_id}https://api.ddog-gov.com/api/v2/logs/config/archives/{archive_id}https://api.datadoghq.com/api/v2/logs/config/archives/{archive_id}https://api.us3.datadoghq.com/api/v2/logs/config/archives/{archive_id}https://api.us5.datadoghq.com/api/v2/logs/config/archives/{archive_id}
### OverviewUpdate a given archive configuration.
**Note**: Using this method updates your archive configuration by **replacing**
your current configuration with the new one sent to your Datadog organization.This endpoint requires the `logs_write_archives` permission.### Arguments#### Path ParametersName
Type
Description
archive_id [*required*]
string
The ID of the archive.
### Request#### Body Data (required)New definition of the archive.
- Model
- Example
Expand All
Field
Type
Description
data
object
The definition of an archive.
attributes
object
The attributes associated with the archive.
destination [*required*]
 <oneOf>
An archive's destination.
Option 1
object
The Azure archive destination.
container [*required*]
string
The container where the archive will be stored.
integration [*required*]
object
The Azure archive's integration destination.
client_id [*required*]
string
A client ID.
tenant_id [*required*]
string
A tenant ID.
path
string
The archive path.
region
string
The region where the archive will be stored.
storage_account [*required*]
string
The associated storage account.
type [*required*]
enum
Type of the Azure archive destination.
Allowed enum values: `azure`default: `azure`
Option 2
object
The GCS archive destination.
bucket [*required*]
string
The bucket where the archive will be stored.
integration [*required*]
object
The GCS archive's integration destination.
client_email [*required*]
string
A client email.
project_id
string
A project ID.
path
string
The archive path.
type [*required*]
enum
Type of the GCS archive destination.
Allowed enum values: `gcs`default: `gcs`
Option 3
object
The S3 archive destination.
bucket [*required*]
string
The bucket where the archive will be stored.
encryption
object
The S3 encryption settings.
key
string
An Amazon Resource Name (ARN) used to identify an AWS KMS key.
type [*required*]
enum
Type of S3 encryption for a destination.
Allowed enum values: `NO_OVERRIDE,SSE_S3,SSE_KMS` integration [*required*]
object
The S3 Archive's integration destination.
account_id [*required*]
string
The account ID for the integration.
role_name [*required*]
string
The path of the integration.
path
string
The archive path.
storage_class
enum
The storage class where the archive will be stored.
Allowed enum values: `STANDARD,STANDARD_IA,ONEZONE_IA,INTELLIGENT_TIERING,GLACIER_IR`default: `STANDARD`
type [*required*]
enum
Type of the S3 archive destination.
Allowed enum values: `s3`default: `s3`
include_tags
boolean
To store the tags in the archive, set the value "true".
If it is set to "false", the tags will be deleted when the logs are sent to the archive.name [*required*]
string
The archive name.
query [*required*]
string
The archive query/filter. Logs matching this query are included in the archive.
rehydration_max_scan_size_in_gb
int64
Maximum scan size for rehydration from this archive.
rehydration_tags
[string]
An array of tags to add to rehydrated logs from an archive.
type [*required*]
string
The type of the resource. The value should always be archives.
default: `archives`
```
{
"data": {
"attributes": {
"destination": {
"container": "container-name",
"integration": {
"client_id": "aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa",
"tenant_id": "aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa"
},
"path": "string",
"region": "string",
"storage_account": "account-name",
"type": "azure"
},
"include_tags": false,
"name": "Nginx Archive",
"query": "source:nginx",
"rehydration_max_scan_size_in_gb": 100,
"rehydration_tags": [
"team:intake",
"team:app"
]
},
"type": "archives"
}
}
```### Response- 200
- 400
- 403
- 404
- 429
OK
- Model
- Example
The logs archive.
Expand All
Field
Type
Description
data
object
The definition of an archive.
attributes
object
The attributes associated with the archive.
destination [*required*]
object <oneOf>
An archive's destination.
Option 1
object
The Azure archive destination.
container [*required*]
string
The container where the archive will be stored.
integration [*required*]
object
The Azure archive's integration destination.
client_id [*required*]
string
A client ID.
tenant_id [*required*]
string
A tenant ID.
path
string
The archive path.
region
string
The region where the archive will be stored.
storage_account [*required*]
string
The associated storage account.
type [*required*]
enum
Type of the Azure archive destination.
Allowed enum values: `azure`default: `azure`
Option 2
object
The GCS archive destination.
bucket [*required*]
string
The bucket where the archive will be stored.
integration [*required*]
object
The GCS archive's integration destination.
client_email [*required*]
string
A client email.
project_id
string
A project ID.
path
string
The archive path.
type [*required*]
enum
Type of the GCS archive destination.
Allowed enum values: `gcs`default: `gcs`
Option 3
object
The S3 archive destination.
bucket [*required*]
string
The bucket where the archive will be stored.
encryption
object
The S3 encryption settings.
key
string
An Amazon Resource Name (ARN) used to identify an AWS KMS key.
type [*required*]
enum
Type of S3 encryption for a destination.
Allowed enum values: `NO_OVERRIDE,SSE_S3,SSE_KMS` integration [*required*]
object
The S3 Archive's integration destination.
account_id [*required*]
string
The account ID for the integration.
role_name [*required*]
string
The path of the integration.
path
string
The archive path.
storage_class
enum
The storage class where the archive will be stored.
Allowed enum values: `STANDARD,STANDARD_IA,ONEZONE_IA,INTELLIGENT_TIERING,GLACIER_IR`default: `STANDARD`
type [*required*]
enum
Type of the S3 archive destination.
Allowed enum values: `s3`default: `s3`
include_tags
boolean
To store the tags in the archive, set the value "true".
If it is set to "false", the tags will be deleted when the logs are sent to the archive.name [*required*]
string
The archive name.
query [*required*]
string
The archive query/filter. Logs matching this query are included in the archive.
rehydration_max_scan_size_in_gb
int64
Maximum scan size for rehydration from this archive.
rehydration_tags
[string]
An array of tags to add to rehydrated logs from an archive.
state
enum
The state of the archive.
Allowed enum values: `UNKNOWN,WORKING,FAILING,WORKING_AUTH_LEGACY`id
string
The archive ID.
type [*required*]
string
The type of the resource. The value should always be archives.
default: `archives`
```
{
"data": {
"attributes": {
"destination": {
"container": "container-name",
"integration": {
"client_id": "aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa",
"tenant_id": "aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa"
},
"path": "string",
"region": "string",
"storage_account": "account-name",
"type": "azure"
},
"include_tags": false,
"name": "Nginx Archive",
"query": "source:nginx",
"rehydration_max_scan_size_in_gb": 100,
"rehydration_tags": [
"team:intake",
"team:app"
],
"state": "WORKING"
},
"id": "a2zcMylnM4OCHpYusxIi3g",
"type": "archives"
}
}
```Bad Request
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```Forbidden
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```Not found
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```Too many requests
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```### Code Example- [Curl](?code-lang=curl#)
- [Python](?code-lang=python#)
- [Ruby](?code-lang=ruby#)
- [Go](?code-lang=go#)
- [Java](?code-lang=java#)
- [Rust](?code-lang=rust#)
- [Typescript](?code-lang=typescript#)

Update an archiveCopy```

# Path parametersexport archive_id="CHANGE_ME"# Curl commandcurl -X PUT "https://api.ap1.datadoghq.com"https://api.ap2.datadoghq.com"https://api.datadoghq.eu"https://api.ddog-gov.com"https://api.datadoghq.com"https://api.us3.datadoghq.com"https://api.us5.datadoghq.com/api/v2/logs/config/archives/${archive_id}" \
-H "Accept: application/json" \
-H "Content-Type: application/json" \
-H "DD-API-KEY: ${DD_API_KEY}" \
-H "DD-APPLICATION-KEY: ${DD_APP_KEY}" \
-d @- << EOF
{
"data": {
"attributes": {
"destination": {
"integration": {
"client_id": "aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa",
"tenant_id": "aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa"
}
},
"name": "Nginx Archive",
"query": "source:nginx"
},
"type": "archives"
}
}
EOF

```
Update an archive```
"""
Update an archive returns "OK" response
"""

from datadog_api_client import ApiClient, Configuration
from datadog_api_client.v2.api.logs_archives_api import LogsArchivesApi
from datadog_api_client.v2.model.logs_archive_create_request import LogsArchiveCreateRequest
from datadog_api_client.v2.model.logs_archive_create_request_attributes import LogsArchiveCreateRequestAttributes
from datadog_api_client.v2.model.logs_archive_create_request_definition import LogsArchiveCreateRequestDefinition
from datadog_api_client.v2.model.logs_archive_destination_azure import LogsArchiveDestinationAzure
from datadog_api_client.v2.model.logs_archive_destination_azure_type import LogsArchiveDestinationAzureType
from datadog_api_client.v2.model.logs_archive_integration_azure import LogsArchiveIntegrationAzure

body = LogsArchiveCreateRequest(
data=LogsArchiveCreateRequestDefinition(
attributes=LogsArchiveCreateRequestAttributes(
destination=LogsArchiveDestinationAzure(
container="container-name",
integration=LogsArchiveIntegrationAzure(
client_id="aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa",
tenant_id="aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa",
),
storage_account="account-name",
type=LogsArchiveDestinationAzureType.AZURE,
),
include_tags=False,
name="Nginx Archive",
query="source:nginx",
rehydration_max_scan_size_in_gb=100,
rehydration_tags=[
"team:intake",
"team:app",
],
),
type="archives",
),
)

configuration = Configuration()
with ApiClient(configuration) as api_client:
api_instance = LogsArchivesApi(api_client)
response = api_instance.update_logs_archive(archive_id="archive_id", body=body)

print(response)

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=python) and then save the example to `example.py` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" python3 "example.py"`
```
Update an archive```
# Update an archive returns "OK" response

require "datadog_api_client"
api_instance = DatadogAPIClient::V2::LogsArchivesAPI.new

body = DatadogAPIClient::V2::LogsArchiveCreateRequest.new({
data: DatadogAPIClient::V2::LogsArchiveCreateRequestDefinition.new({
attributes: DatadogAPIClient::V2::LogsArchiveCreateRequestAttributes.new({
destination: DatadogAPIClient::V2::LogsArchiveDestinationAzure.new({
container: "container-name",
integration: DatadogAPIClient::V2::LogsArchiveIntegrationAzure.new({
client_id: "aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa",
tenant_id: "aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa",
}),
storage_account: "account-name",
type: DatadogAPIClient::V2::LogsArchiveDestinationAzureType::AZURE,
}),
include_tags: false,
name: "Nginx Archive",
query: "source:nginx",
rehydration_max_scan_size_in_gb: 100,
rehydration_tags: [
"team:intake",
"team:app",
],
}),
type: "archives",
}),
})
p api_instance.update_logs_archive("archive_id", body)

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=ruby) and then save the example to `example.rb` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" rb "example.rb"`
```
Update an archive```
// Update an archive returns "OK" response

package main

import (
	"context"
	"encoding/json"
	"fmt"
	"os"

	"github.com/DataDog/datadog-api-client-go/v2/api/datadog"
	"github.com/DataDog/datadog-api-client-go/v2/api/datadogV2"
)

func main() {
	body := datadogV2.LogsArchiveCreateRequest{
		Data: &datadogV2.LogsArchiveCreateRequestDefinition{
			Attributes: &datadogV2.LogsArchiveCreateRequestAttributes{
				Destination: datadogV2.LogsArchiveCreateRequestDestination{
					LogsArchiveDestinationAzure: &datadogV2.LogsArchiveDestinationAzure{
						Container: "container-name",
						Integration: datadogV2.LogsArchiveIntegrationAzure{
							ClientId: "aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa",
							TenantId: "aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa",
						},
						StorageAccount: "account-name",
						Type: datadogV2.LOGSARCHIVEDESTINATIONAZURETYPE_AZURE,
					}},
				IncludeTags: datadog.PtrBool(false),
				Name: "Nginx Archive",
				Query: "source:nginx",
				RehydrationMaxScanSizeInGb: *datadog.NewNullableInt64(datadog.PtrInt64(100)),
				RehydrationTags: []string{
					"team:intake",
					"team:app",
				},
			},
			Type: "archives",
		},
	}
	ctx := datadog.NewDefaultContext(context.Background())
	configuration := datadog.NewConfiguration()
	apiClient := datadog.NewAPIClient(configuration)
	api := datadogV2.NewLogsArchivesApi(apiClient)
	resp, r, err := api.UpdateLogsArchive(ctx, "archive_id", body)

	if err != nil {
		fmt.Fprintf(os.Stderr, "Error when calling `LogsArchivesApi.UpdateLogsArchive`: %v\n", err)
		fmt.Fprintf(os.Stderr, "Full HTTP response: %v\n", r)
	}

	responseContent, _ := json.MarshalIndent(resp, "", " ")
	fmt.Fprintf(os.Stdout, "Response from `LogsArchivesApi.UpdateLogsArchive`:\n%s\n", responseContent)
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=go) and then save the example to `main.go` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" go run "main.go"`
```
Update an archive```
// Update an archive returns "OK" response

import com.datadog.api.client.ApiClient;
import com.datadog.api.client.ApiException;
import com.datadog.api.client.v2.api.LogsArchivesApi;
import com.datadog.api.client.v2.model.LogsArchive;
import com.datadog.api.client.v2.model.LogsArchiveCreateRequest;
import com.datadog.api.client.v2.model.LogsArchiveCreateRequestAttributes;
import com.datadog.api.client.v2.model.LogsArchiveCreateRequestDefinition;
import com.datadog.api.client.v2.model.LogsArchiveCreateRequestDestination;
import com.datadog.api.client.v2.model.LogsArchiveDestinationAzure;
import com.datadog.api.client.v2.model.LogsArchiveDestinationAzureType;
import com.datadog.api.client.v2.model.LogsArchiveIntegrationAzure;
import java.util.Arrays;

public class Example {
public static void main(String[] args) {
ApiClient defaultClient = ApiClient.getDefaultApiClient();
LogsArchivesApi apiInstance = new LogsArchivesApi(defaultClient);

LogsArchiveCreateRequest body =
new LogsArchiveCreateRequest()
.data(
new LogsArchiveCreateRequestDefinition()
.attributes(
new LogsArchiveCreateRequestAttributes()
.destination(
new LogsArchiveCreateRequestDestination(
new LogsArchiveDestinationAzure()
.container("container-name")
.integration(
new LogsArchiveIntegrationAzure()
.clientId("aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa")
.tenantId("aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa"))
.storageAccount("account-name")
.type(LogsArchiveDestinationAzureType.AZURE)))
.includeTags(false)
.name("Nginx Archive")
.query("source:nginx")
.rehydrationMaxScanSizeInGb(100L)
.rehydrationTags(Arrays.asList("team:intake", "team:app")))
.type("archives"));

try {
LogsArchive result = apiInstance.updateLogsArchive("archive_id", body);
System.out.println(result);
} catch (ApiException e) {
System.err.println("Exception when calling LogsArchivesApi#updateLogsArchive");
System.err.println("Status code: " + e.getCode());
System.err.println("Reason: " + e.getResponseBody());
System.err.println("Response headers: " + e.getResponseHeaders());
e.printStackTrace();
}
}
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=java) and then save the example to `Example.java` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" java "Example.java"`
```
Update an archive```
// Update an archive returns "OK" response
use datadog_api_client::datadog;
use datadog_api_client::datadogV2::api_logs_archives::LogsArchivesAPI;
use datadog_api_client::datadogV2::model::LogsArchiveCreateRequest;
use datadog_api_client::datadogV2::model::LogsArchiveCreateRequestAttributes;
use datadog_api_client::datadogV2::model::LogsArchiveCreateRequestDefinition;
use datadog_api_client::datadogV2::model::LogsArchiveCreateRequestDestination;
use datadog_api_client::datadogV2::model::LogsArchiveDestinationAzure;
use datadog_api_client::datadogV2::model::LogsArchiveDestinationAzureType;
use datadog_api_client::datadogV2::model::LogsArchiveIntegrationAzure;

#[tokio::main]
async fn main() {
let body = LogsArchiveCreateRequest::new().data(
LogsArchiveCreateRequestDefinition::new("archives".to_string()).attributes(
LogsArchiveCreateRequestAttributes::new(
LogsArchiveCreateRequestDestination::LogsArchiveDestinationAzure(Box::new(
LogsArchiveDestinationAzure::new(
"container-name".to_string(),
LogsArchiveIntegrationAzure::new(
"aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa".to_string(),
"aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa".to_string(),
),
"account-name".to_string(),
LogsArchiveDestinationAzureType::AZURE,
),
)),
"Nginx Archive".to_string(),
"source:nginx".to_string(),
)
.include_tags(false)
.rehydration_max_scan_size_in_gb(Some(100))
.rehydration_tags(vec!["team:intake".to_string(), "team:app".to_string()]),
),
);
let configuration = datadog::Configuration::new();
let api = LogsArchivesAPI::with_config(configuration);
let resp = api
.update_logs_archive("archive_id".to_string(), body)
.await;
if let Ok(value) = resp {
println!("{:#?}", value);
} else {
println!("{:#?}", resp.unwrap_err());
}
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=rust) and then save the example to `src/main.rs` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" cargo run`
```
Update an archive```
/**
* Update an archive returns "OK" response
*/

import { client, v2 } from "@datadog/datadog-api-client";

const configuration = client.createConfiguration();
const apiInstance = new v2.LogsArchivesApi(configuration);

const params: v2.LogsArchivesApiUpdateLogsArchiveRequest = {
body: {
data: {
attributes: {
destination: {
container: "container-name",
integration: {
clientId: "aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa",
tenantId: "aaaaaaaa-1a1a-1a1a-1a1a-aaaaaaaaaaaa",
},
storageAccount: "account-name",
type: "azure",
},
includeTags: false,
name: "Nginx Archive",
query: "source:nginx",
rehydrationMaxScanSizeInGb: 100,
rehydrationTags: ["team:intake", "team:app"],
},
type: "archives",
},
},
archiveId: "archive_id",
};

apiInstance
.updateLogsArchive(params)
.then((data: v2.LogsArchive) => {
console.log(
"API called successfully. Returned data: " + JSON.stringify(data)
);
})
.catch((error: any) => console.error(error));

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=typescript) and then save the example to `example.ts` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" tsc "example.ts"`
```## Delete an archive- v2 (latest)
DELETE https://api.ap1.datadoghq.com/api/v2/logs/config/archives/{archive_id}https://api.ap2.datadoghq.com/api/v2/logs/config/archives/{archive_id}https://api.datadoghq.eu/api/v2/logs/config/archives/{archive_id}https://api.ddog-gov.com/api/v2/logs/config/archives/{archive_id}https://api.datadoghq.com/api/v2/logs/config/archives/{archive_id}https://api.us3.datadoghq.com/api/v2/logs/config/archives/{archive_id}https://api.us5.datadoghq.com/api/v2/logs/config/archives/{archive_id}
### OverviewDelete a given archive from your organization.
This endpoint requires the `logs_write_archives` permission.### Arguments#### Path ParametersName
Type
Description
archive_id [*required*]
string
The ID of the archive.
### Response- 204
- 400
- 403
- 404
- 429
OK
Bad Request
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```Forbidden
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```Not found
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```Too many requests
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```### Code Example- [Curl](?code-lang=curl#)
- [Python](?code-lang=python#)
- [Ruby](?code-lang=ruby#)
- [Go](?code-lang=go#)
- [Java](?code-lang=java#)
- [Rust](?code-lang=rust#)
- [Typescript](?code-lang=typescript#)

Delete an archiveCopy```

# Path parametersexport archive_id="CHANGE_ME"# Curl commandcurl -X DELETE "https://api.ap1.datadoghq.com"https://api.ap2.datadoghq.com"https://api.datadoghq.eu"https://api.ddog-gov.com"https://api.datadoghq.com"https://api.us3.datadoghq.com"https://api.us5.datadoghq.com/api/v2/logs/config/archives/${archive_id}" \
-H "DD-API-KEY: ${DD_API_KEY}" \
-H "DD-APPLICATION-KEY: ${DD_APP_KEY}"

```
Delete an archive```
"""
Delete an archive returns "OK" response
"""

from datadog_api_client import ApiClient, Configuration
from datadog_api_client.v2.api.logs_archives_api import LogsArchivesApi

configuration = Configuration()
with ApiClient(configuration) as api_client:
api_instance = LogsArchivesApi(api_client)
api_instance.delete_logs_archive(
archive_id="archive_id",
)

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=python) and then save the example to `example.py` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" python3 "example.py"`
```
Delete an archive```
# Delete an archive returns "OK" response

require "datadog_api_client"
api_instance = DatadogAPIClient::V2::LogsArchivesAPI.new
api_instance.delete_logs_archive("archive_id")

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=ruby) and then save the example to `example.rb` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" rb "example.rb"`
```
Delete an archive```
// Delete an archive returns "OK" response

package main

import (
	"context"
	"fmt"
	"os"

	"github.com/DataDog/datadog-api-client-go/v2/api/datadog"
	"github.com/DataDog/datadog-api-client-go/v2/api/datadogV2"
)

func main() {
	ctx := datadog.NewDefaultContext(context.Background())
	configuration := datadog.NewConfiguration()
	apiClient := datadog.NewAPIClient(configuration)
	api := datadogV2.NewLogsArchivesApi(apiClient)
	r, err := api.DeleteLogsArchive(ctx, "archive_id")

	if err != nil {
		fmt.Fprintf(os.Stderr, "Error when calling `LogsArchivesApi.DeleteLogsArchive`: %v\n", err)
		fmt.Fprintf(os.Stderr, "Full HTTP response: %v\n", r)
	}
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=go) and then save the example to `main.go` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" go run "main.go"`
```
Delete an archive```
// Delete an archive returns "OK" response

import com.datadog.api.client.ApiClient;
import com.datadog.api.client.ApiException;
import com.datadog.api.client.v2.api.LogsArchivesApi;

public class Example {
public static void main(String[] args) {
ApiClient defaultClient = ApiClient.getDefaultApiClient();
LogsArchivesApi apiInstance = new LogsArchivesApi(defaultClient);

try {
apiInstance.deleteLogsArchive("archive_id");
} catch (ApiException e) {
System.err.println("Exception when calling LogsArchivesApi#deleteLogsArchive");
System.err.println("Status code: " + e.getCode());
System.err.println("Reason: " + e.getResponseBody());
System.err.println("Response headers: " + e.getResponseHeaders());
e.printStackTrace();
}
}
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=java) and then save the example to `Example.java` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" java "Example.java"`
```
Delete an archive```
// Delete an archive returns "OK" response
use datadog_api_client::datadog;
use datadog_api_client::datadogV2::api_logs_archives::LogsArchivesAPI;

#[tokio::main]
async fn main() {
let configuration = datadog::Configuration::new();
let api = LogsArchivesAPI::with_config(configuration);
let resp = api.delete_logs_archive("archive_id".to_string()).await;
if let Ok(value) = resp {
println!("{:#?}", value);
} else {
println!("{:#?}", resp.unwrap_err());
}
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=rust) and then save the example to `src/main.rs` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" cargo run`
```
Delete an archive```
/**
* Delete an archive returns "OK" response
*/

import { client, v2 } from "@datadog/datadog-api-client";

const configuration = client.createConfiguration();
const apiInstance = new v2.LogsArchivesApi(configuration);

const params: v2.LogsArchivesApiDeleteLogsArchiveRequest = {
archiveId: "archive_id",
};

apiInstance
.deleteLogsArchive(params)
.then((data: any) => {
console.log(
"API called successfully. Returned data: " + JSON.stringify(data)
);
})
.catch((error: any) => console.error(error));

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=typescript) and then save the example to `example.ts` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" tsc "example.ts"`
```## List read roles for an archive- v2 (latest)
GET https://api.ap1.datadoghq.com/api/v2/logs/config/archives/{archive_id}/readershttps://api.ap2.datadoghq.com/api/v2/logs/config/archives/{archive_id}/readershttps://api.datadoghq.eu/api/v2/logs/config/archives/{archive_id}/readershttps://api.ddog-gov.com/api/v2/logs/config/archives/{archive_id}/readershttps://api.datadoghq.com/api/v2/logs/config/archives/{archive_id}/readershttps://api.us3.datadoghq.com/api/v2/logs/config/archives/{archive_id}/readershttps://api.us5.datadoghq.com/api/v2/logs/config/archives/{archive_id}/readers
### OverviewReturns all read roles a given archive is restricted to.
This endpoint requires the `logs_read_config` permission.### Arguments#### Path ParametersName
Type
Description
archive_id [*required*]
string
The ID of the archive.
### Response- 200
- 400
- 403
- 404
- 429
OK
- Model
- Example
Response containing information about multiple roles.
Expand All
Field
Type
Description
data
[object]
Array of returned roles.
attributes
object
Attributes of the role.
created_at
date-time
Creation time of the role.
modified_at
date-time
Time of last role modification.
name
string
The name of the role. The name is neither unique nor a stable identifier of the role.
user_count
int64
Number of users with that role.
id
string
The unique identifier of the role.
relationships
object
Relationships of the role object returned by the API.
permissions
object
Relationship to multiple permissions objects.
data
[object]
Relationships to permission objects.
id
string
ID of the permission.
type
enum
Permissions resource type.
Allowed enum values: `permissions`default: `permissions`
type [*required*]
enum
Roles type.
Allowed enum values: `roles`default: `roles`
meta
object
Object describing meta attributes of response.
page
object
Pagination object.
total_count
int64
Total count.
total_filtered_count
int64
Total count of elements matched by the filter.
```
{
"data": [
{
"attributes": {
"created_at": "2019-09-19T10:00:00.000Z",
"modified_at": "2019-09-19T10:00:00.000Z",
"name": "string",
"user_count": "integer"
},
"id": "string",
"relationships": {
"permissions": {
"data": [
{
"id": "string",
"type": "permissions"
}
]
}
},
"type": "roles"
}
],
"meta": {
"page": {
"total_count": "integer",
"total_filtered_count": "integer"
}
}
}
```Bad Request
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```Forbidden
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```Not found
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```Too many requests
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```### Code Example- [Curl](?code-lang=curl#)
- [Python](?code-lang=python#)
- [Ruby](?code-lang=ruby#)
- [Go](?code-lang=go#)
- [Java](?code-lang=java#)
- [Rust](?code-lang=rust#)
- [Typescript](?code-lang=typescript#)

List read roles for an archiveCopy```

# Path parametersexport archive_id="CHANGE_ME"# Curl commandcurl -X GET "https://api.ap1.datadoghq.com"https://api.ap2.datadoghq.com"https://api.datadoghq.eu"https://api.ddog-gov.com"https://api.datadoghq.com"https://api.us3.datadoghq.com"https://api.us5.datadoghq.com/api/v2/logs/config/archives/${archive_id}/readers" \
-H "Accept: application/json" \
-H "DD-API-KEY: ${DD_API_KEY}" \
-H "DD-APPLICATION-KEY: ${DD_APP_KEY}"

```
List read roles for an archive```
"""
List read roles for an archive returns "OK" response
"""

from datadog_api_client import ApiClient, Configuration
from datadog_api_client.v2.api.logs_archives_api import LogsArchivesApi

configuration = Configuration()
with ApiClient(configuration) as api_client:
api_instance = LogsArchivesApi(api_client)
response = api_instance.list_archive_read_roles(
archive_id="archive_id",
)

print(response)

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=python) and then save the example to `example.py` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" python3 "example.py"`
```
List read roles for an archive```
# List read roles for an archive returns "OK" response

require "datadog_api_client"
api_instance = DatadogAPIClient::V2::LogsArchivesAPI.new
p api_instance.list_archive_read_roles("archive_id")

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=ruby) and then save the example to `example.rb` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" rb "example.rb"`
```
List read roles for an archive```
// List read roles for an archive returns "OK" response

package main

import (
	"context"
	"encoding/json"
	"fmt"
	"os"

	"github.com/DataDog/datadog-api-client-go/v2/api/datadog"
	"github.com/DataDog/datadog-api-client-go/v2/api/datadogV2"
)

func main() {
	ctx := datadog.NewDefaultContext(context.Background())
	configuration := datadog.NewConfiguration()
	apiClient := datadog.NewAPIClient(configuration)
	api := datadogV2.NewLogsArchivesApi(apiClient)
	resp, r, err := api.ListArchiveReadRoles(ctx, "archive_id")

	if err != nil {
		fmt.Fprintf(os.Stderr, "Error when calling `LogsArchivesApi.ListArchiveReadRoles`: %v\n", err)
		fmt.Fprintf(os.Stderr, "Full HTTP response: %v\n", r)
	}

	responseContent, _ := json.MarshalIndent(resp, "", " ")
	fmt.Fprintf(os.Stdout, "Response from `LogsArchivesApi.ListArchiveReadRoles`:\n%s\n", responseContent)
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=go) and then save the example to `main.go` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" go run "main.go"`
```
List read roles for an archive```
// List read roles for an archive returns "OK" response

import com.datadog.api.client.ApiClient;
import com.datadog.api.client.ApiException;
import com.datadog.api.client.v2.api.LogsArchivesApi;
import com.datadog.api.client.v2.model.RolesResponse;

public class Example {
public static void main(String[] args) {
ApiClient defaultClient = ApiClient.getDefaultApiClient();
LogsArchivesApi apiInstance = new LogsArchivesApi(defaultClient);

try {
RolesResponse result = apiInstance.listArchiveReadRoles("archive_id");
System.out.println(result);
} catch (ApiException e) {
System.err.println("Exception when calling LogsArchivesApi#listArchiveReadRoles");
System.err.println("Status code: " + e.getCode());
System.err.println("Reason: " + e.getResponseBody());
System.err.println("Response headers: " + e.getResponseHeaders());
e.printStackTrace();
}
}
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=java) and then save the example to `Example.java` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" java "Example.java"`
```
List read roles for an archive```
// List read roles for an archive returns "OK" response
use datadog_api_client::datadog;
use datadog_api_client::datadogV2::api_logs_archives::LogsArchivesAPI;

#[tokio::main]
async fn main() {
let configuration = datadog::Configuration::new();
let api = LogsArchivesAPI::with_config(configuration);
let resp = api.list_archive_read_roles("archive_id".to_string()).await;
if let Ok(value) = resp {
println!("{:#?}", value);
} else {
println!("{:#?}", resp.unwrap_err());
}
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=rust) and then save the example to `src/main.rs` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" cargo run`
```
List read roles for an archive```
/**
* List read roles for an archive returns "OK" response
*/

import { client, v2 } from "@datadog/datadog-api-client";

const configuration = client.createConfiguration();
const apiInstance = new v2.LogsArchivesApi(configuration);

const params: v2.LogsArchivesApiListArchiveReadRolesRequest = {
archiveId: "archive_id",
};

apiInstance
.listArchiveReadRoles(params)
.then((data: v2.RolesResponse) => {
console.log(
"API called successfully. Returned data: " + JSON.stringify(data)
);
})
.catch((error: any) => console.error(error));

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=typescript) and then save the example to `example.ts` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" tsc "example.ts"`
```## Grant role to an archive- v2 (latest)
POST https://api.ap1.datadoghq.com/api/v2/logs/config/archives/{archive_id}/readershttps://api.ap2.datadoghq.com/api/v2/logs/config/archives/{archive_id}/readershttps://api.datadoghq.eu/api/v2/logs/config/archives/{archive_id}/readershttps://api.ddog-gov.com/api/v2/logs/config/archives/{archive_id}/readershttps://api.datadoghq.com/api/v2/logs/config/archives/{archive_id}/readershttps://api.us3.datadoghq.com/api/v2/logs/config/archives/{archive_id}/readershttps://api.us5.datadoghq.com/api/v2/logs/config/archives/{archive_id}/readers
### OverviewAdds a read role to an archive. (Roles API)
This endpoint requires the `logs_write_archives` permission.### Arguments#### Path ParametersName
Type
Description
archive_id [*required*]
string
The ID of the archive.
### Request#### Body Data (required)
- Model
- Example
Expand All
Field
Type
Description
data
object
Relationship to role object.
id
string
The unique identifier of the role.
type
enum
Roles type.
Allowed enum values: `roles`default: `roles`
```
{
"data": {
"id": "3653d3c6-0c75-11ea-ad28-fb5701eabc7d",
"type": "roles"
}
}
```### Response- 204
- 400
- 403
- 404
- 429
OK
Bad Request
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```Forbidden
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```Not found
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```Too many requests
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```### Code Example- [Curl](?code-lang=curl#)
- [Python](?code-lang=python#)
- [Ruby](?code-lang=ruby#)
- [Go](?code-lang=go#)
- [Java](?code-lang=java#)
- [Rust](?code-lang=rust#)
- [Typescript](?code-lang=typescript#)

Grant role to an archiveCopy```

# Path parametersexport archive_id="CHANGE_ME"# Curl commandcurl -X POST "https://api.ap1.datadoghq.com"https://api.ap2.datadoghq.com"https://api.datadoghq.eu"https://api.ddog-gov.com"https://api.datadoghq.com"https://api.us3.datadoghq.com"https://api.us5.datadoghq.com/api/v2/logs/config/archives/${archive_id}/readers" \
-H "Content-Type: application/json" \
-H "DD-API-KEY: ${DD_API_KEY}" \
-H "DD-APPLICATION-KEY: ${DD_APP_KEY}" \
-d @- << EOF
{}
EOF

```
Grant role to an archive```
"""
Grant role to an archive returns "OK" response
"""

from datadog_api_client import ApiClient, Configuration
from datadog_api_client.v2.api.logs_archives_api import LogsArchivesApi
from datadog_api_client.v2.model.relationship_to_role import RelationshipToRole
from datadog_api_client.v2.model.relationship_to_role_data import RelationshipToRoleData
from datadog_api_client.v2.model.roles_type import RolesType

body = RelationshipToRole(
data=RelationshipToRoleData(
id="3653d3c6-0c75-11ea-ad28-fb5701eabc7d",
type=RolesType.ROLES,
),
)

configuration = Configuration()
with ApiClient(configuration) as api_client:
api_instance = LogsArchivesApi(api_client)
api_instance.add_read_role_to_archive(archive_id="archive_id", body=body)

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=python) and then save the example to `example.py` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" python3 "example.py"`
```
Grant role to an archive```
# Grant role to an archive returns "OK" response

require "datadog_api_client"
api_instance = DatadogAPIClient::V2::LogsArchivesAPI.new

body = DatadogAPIClient::V2::RelationshipToRole.new({
data: DatadogAPIClient::V2::RelationshipToRoleData.new({
id: "3653d3c6-0c75-11ea-ad28-fb5701eabc7d",
type: DatadogAPIClient::V2::RolesType::ROLES,
}),
})
api_instance.add_read_role_to_archive("archive_id", body)

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=ruby) and then save the example to `example.rb` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" rb "example.rb"`
```
Grant role to an archive```
// Grant role to an archive returns "OK" response

package main

import (
	"context"
	"fmt"
	"os"

	"github.com/DataDog/datadog-api-client-go/v2/api/datadog"
	"github.com/DataDog/datadog-api-client-go/v2/api/datadogV2"
)

func main() {
	body := datadogV2.RelationshipToRole{
		Data: &datadogV2.RelationshipToRoleData{
			Id: datadog.PtrString("3653d3c6-0c75-11ea-ad28-fb5701eabc7d"),
			Type: datadogV2.ROLESTYPE_ROLES.Ptr(),
		},
	}
	ctx := datadog.NewDefaultContext(context.Background())
	configuration := datadog.NewConfiguration()
	apiClient := datadog.NewAPIClient(configuration)
	api := datadogV2.NewLogsArchivesApi(apiClient)
	r, err := api.AddReadRoleToArchive(ctx, "archive_id", body)

	if err != nil {
		fmt.Fprintf(os.Stderr, "Error when calling `LogsArchivesApi.AddReadRoleToArchive`: %v\n", err)
		fmt.Fprintf(os.Stderr, "Full HTTP response: %v\n", r)
	}
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=go) and then save the example to `main.go` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" go run "main.go"`
```
Grant role to an archive```
// Grant role to an archive returns "OK" response

import com.datadog.api.client.ApiClient;
import com.datadog.api.client.ApiException;
import com.datadog.api.client.v2.api.LogsArchivesApi;
import com.datadog.api.client.v2.model.RelationshipToRole;
import com.datadog.api.client.v2.model.RelationshipToRoleData;
import com.datadog.api.client.v2.model.RolesType;

public class Example {
public static void main(String[] args) {
ApiClient defaultClient = ApiClient.getDefaultApiClient();
LogsArchivesApi apiInstance = new LogsArchivesApi(defaultClient);

RelationshipToRole body =
new RelationshipToRole()
.data(
new RelationshipToRoleData()
.id("3653d3c6-0c75-11ea-ad28-fb5701eabc7d")
.type(RolesType.ROLES));

try {
apiInstance.addReadRoleToArchive("archive_id", body);
} catch (ApiException e) {
System.err.println("Exception when calling LogsArchivesApi#addReadRoleToArchive");
System.err.println("Status code: " + e.getCode());
System.err.println("Reason: " + e.getResponseBody());
System.err.println("Response headers: " + e.getResponseHeaders());
e.printStackTrace();
}
}
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=java) and then save the example to `Example.java` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" java "Example.java"`
```
Grant role to an archive```
// Grant role to an archive returns "OK" response
use datadog_api_client::datadog;
use datadog_api_client::datadogV2::api_logs_archives::LogsArchivesAPI;
use datadog_api_client::datadogV2::model::RelationshipToRole;
use datadog_api_client::datadogV2::model::RelationshipToRoleData;
use datadog_api_client::datadogV2::model::RolesType;

#[tokio::main]
async fn main() {
let body = RelationshipToRole::new().data(
RelationshipToRoleData::new()
.id("3653d3c6-0c75-11ea-ad28-fb5701eabc7d".to_string())
.type_(RolesType::ROLES),
);
let configuration = datadog::Configuration::new();
let api = LogsArchivesAPI::with_config(configuration);
let resp = api
.add_read_role_to_archive("archive_id".to_string(), body)
.await;
if let Ok(value) = resp {
println!("{:#?}", value);
} else {
println!("{:#?}", resp.unwrap_err());
}
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=rust) and then save the example to `src/main.rs` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" cargo run`
```
Grant role to an archive```
/**
* Grant role to an archive returns "OK" response
*/

import { client, v2 } from "@datadog/datadog-api-client";

const configuration = client.createConfiguration();
const apiInstance = new v2.LogsArchivesApi(configuration);

const params: v2.LogsArchivesApiAddReadRoleToArchiveRequest = {
body: {
data: {
id: "3653d3c6-0c75-11ea-ad28-fb5701eabc7d",
type: "roles",
},
},
archiveId: "archive_id",
};

apiInstance
.addReadRoleToArchive(params)
.then((data: any) => {
console.log(
"API called successfully. Returned data: " + JSON.stringify(data)
);
})
.catch((error: any) => console.error(error));

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=typescript) and then save the example to `example.ts` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" tsc "example.ts"`
```## Revoke role from an archive- v2 (latest)
DELETE https://api.ap1.datadoghq.com/api/v2/logs/config/archives/{archive_id}/readershttps://api.ap2.datadoghq.com/api/v2/logs/config/archives/{archive_id}/readershttps://api.datadoghq.eu/api/v2/logs/config/archives/{archive_id}/readershttps://api.ddog-gov.com/api/v2/logs/config/archives/{archive_id}/readershttps://api.datadoghq.com/api/v2/logs/config/archives/{archive_id}/readershttps://api.us3.datadoghq.com/api/v2/logs/config/archives/{archive_id}/readershttps://api.us5.datadoghq.com/api/v2/logs/config/archives/{archive_id}/readers
### OverviewRemoves a role from an archive. (Roles API)
This endpoint requires the `logs_write_archives` permission.### Arguments#### Path ParametersName
Type
Description
archive_id [*required*]
string
The ID of the archive.
### Request#### Body Data (required)
- Model
- Example
Expand All
Field
Type
Description
data
object
Relationship to role object.
id
string
The unique identifier of the role.
type
enum
Roles type.
Allowed enum values: `roles`default: `roles`
```
{
"data": {
"id": "3653d3c6-0c75-11ea-ad28-fb5701eabc7d",
"type": "roles"
}
}
```### Response- 204
- 400
- 403
- 404
- 429
OK
Bad Request
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```Forbidden
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```Not found
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```Too many requests
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```### Code Example- [Curl](?code-lang=curl#)
- [Python](?code-lang=python#)
- [Ruby](?code-lang=ruby#)
- [Go](?code-lang=go#)
- [Java](?code-lang=java#)
- [Rust](?code-lang=rust#)
- [Typescript](?code-lang=typescript#)

Revoke role from an archiveCopy```

# Path parametersexport archive_id="CHANGE_ME"# Curl commandcurl -X DELETE "https://api.ap1.datadoghq.com"https://api.ap2.datadoghq.com"https://api.datadoghq.eu"https://api.ddog-gov.com"https://api.datadoghq.com"https://api.us3.datadoghq.com"https://api.us5.datadoghq.com/api/v2/logs/config/archives/${archive_id}/readers" \
-H "Content-Type: application/json" \
-H "DD-API-KEY: ${DD_API_KEY}" \
-H "DD-APPLICATION-KEY: ${DD_APP_KEY}" \
-d @- << EOF
{}
EOF

```
Revoke role from an archive```
"""
Revoke role from an archive returns "OK" response
"""

from datadog_api_client import ApiClient, Configuration
from datadog_api_client.v2.api.logs_archives_api import LogsArchivesApi
from datadog_api_client.v2.model.relationship_to_role import RelationshipToRole
from datadog_api_client.v2.model.relationship_to_role_data import RelationshipToRoleData
from datadog_api_client.v2.model.roles_type import RolesType

body = RelationshipToRole(
data=RelationshipToRoleData(
id="3653d3c6-0c75-11ea-ad28-fb5701eabc7d",
type=RolesType.ROLES,
),
)

configuration = Configuration()
with ApiClient(configuration) as api_client:
api_instance = LogsArchivesApi(api_client)
api_instance.remove_role_from_archive(archive_id="archive_id", body=body)

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=python) and then save the example to `example.py` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" python3 "example.py"`
```
Revoke role from an archive```
# Revoke role from an archive returns "OK" response

require "datadog_api_client"
api_instance = DatadogAPIClient::V2::LogsArchivesAPI.new

body = DatadogAPIClient::V2::RelationshipToRole.new({
data: DatadogAPIClient::V2::RelationshipToRoleData.new({
id: "3653d3c6-0c75-11ea-ad28-fb5701eabc7d",
type: DatadogAPIClient::V2::RolesType::ROLES,
}),
})
api_instance.remove_role_from_archive("archive_id", body)

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=ruby) and then save the example to `example.rb` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" rb "example.rb"`
```
Revoke role from an archive```
// Revoke role from an archive returns "OK" response

package main

import (
	"context"
	"fmt"
	"os"

	"github.com/DataDog/datadog-api-client-go/v2/api/datadog"
	"github.com/DataDog/datadog-api-client-go/v2/api/datadogV2"
)

func main() {
	body := datadogV2.RelationshipToRole{
		Data: &datadogV2.RelationshipToRoleData{
			Id: datadog.PtrString("3653d3c6-0c75-11ea-ad28-fb5701eabc7d"),
			Type: datadogV2.ROLESTYPE_ROLES.Ptr(),
		},
	}
	ctx := datadog.NewDefaultContext(context.Background())
	configuration := datadog.NewConfiguration()
	apiClient := datadog.NewAPIClient(configuration)
	api := datadogV2.NewLogsArchivesApi(apiClient)
	r, err := api.RemoveRoleFromArchive(ctx, "archive_id", body)

	if err != nil {
		fmt.Fprintf(os.Stderr, "Error when calling `LogsArchivesApi.RemoveRoleFromArchive`: %v\n", err)
		fmt.Fprintf(os.Stderr, "Full HTTP response: %v\n", r)
	}
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=go) and then save the example to `main.go` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" go run "main.go"`
```
Revoke role from an archive```
// Revoke role from an archive returns "OK" response

import com.datadog.api.client.ApiClient;
import com.datadog.api.client.ApiException;
import com.datadog.api.client.v2.api.LogsArchivesApi;
import com.datadog.api.client.v2.model.RelationshipToRole;
import com.datadog.api.client.v2.model.RelationshipToRoleData;
import com.datadog.api.client.v2.model.RolesType;

public class Example {
public static void main(String[] args) {
ApiClient defaultClient = ApiClient.getDefaultApiClient();
LogsArchivesApi apiInstance = new LogsArchivesApi(defaultClient);

RelationshipToRole body =
new RelationshipToRole()
.data(
new RelationshipToRoleData()
.id("3653d3c6-0c75-11ea-ad28-fb5701eabc7d")
.type(RolesType.ROLES));

try {
apiInstance.removeRoleFromArchive("archive_id", body);
} catch (ApiException e) {
System.err.println("Exception when calling LogsArchivesApi#removeRoleFromArchive");
System.err.println("Status code: " + e.getCode());
System.err.println("Reason: " + e.getResponseBody());
System.err.println("Response headers: " + e.getResponseHeaders());
e.printStackTrace();
}
}
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=java) and then save the example to `Example.java` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" java "Example.java"`
```
Revoke role from an archive```
// Revoke role from an archive returns "OK" response
use datadog_api_client::datadog;
use datadog_api_client::datadogV2::api_logs_archives::LogsArchivesAPI;
use datadog_api_client::datadogV2::model::RelationshipToRole;
use datadog_api_client::datadogV2::model::RelationshipToRoleData;
use datadog_api_client::datadogV2::model::RolesType;

#[tokio::main]
async fn main() {
let body = RelationshipToRole::new().data(
RelationshipToRoleData::new()
.id("3653d3c6-0c75-11ea-ad28-fb5701eabc7d".to_string())
.type_(RolesType::ROLES),
);
let configuration = datadog::Configuration::new();
let api = LogsArchivesAPI::with_config(configuration);
let resp = api
.remove_role_from_archive("archive_id".to_string(), body)
.await;
if let Ok(value) = resp {
println!("{:#?}", value);
} else {
println!("{:#?}", resp.unwrap_err());
}
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=rust) and then save the example to `src/main.rs` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" cargo run`
```
Revoke role from an archive```
/**
* Revoke role from an archive returns "OK" response
*/

import { client, v2 } from "@datadog/datadog-api-client";

const configuration = client.createConfiguration();
const apiInstance = new v2.LogsArchivesApi(configuration);

const params: v2.LogsArchivesApiRemoveRoleFromArchiveRequest = {
body: {
data: {
id: "3653d3c6-0c75-11ea-ad28-fb5701eabc7d",
type: "roles",
},
},
archiveId: "archive_id",
};

apiInstance
.removeRoleFromArchive(params)
.then((data: any) => {
console.log(
"API called successfully. Returned data: " + JSON.stringify(data)
);
})
.catch((error: any) => console.error(error));

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=typescript) and then save the example to `example.ts` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" tsc "example.ts"`
```## Get archive order- v2 (latest)
GET https://api.ap1.datadoghq.com/api/v2/logs/config/archive-orderhttps://api.ap2.datadoghq.com/api/v2/logs/config/archive-orderhttps://api.datadoghq.eu/api/v2/logs/config/archive-orderhttps://api.ddog-gov.com/api/v2/logs/config/archive-orderhttps://api.datadoghq.com/api/v2/logs/config/archive-orderhttps://api.us3.datadoghq.com/api/v2/logs/config/archive-orderhttps://api.us5.datadoghq.com/api/v2/logs/config/archive-order
### OverviewGet the current order of your archives.
This endpoint takes no JSON arguments.
This endpoint requires the `logs_read_config` permission.### Response- 200
- 403
- 429
OK
- Model
- Example
A ordered list of archive IDs.
Expand All
Field
Type
Description
data
object
The definition of an archive order.
attributes [*required*]
object
The attributes associated with the archive order.
archive_ids [*required*]
[string]
An ordered array of `<ARCHIVE_ID>` strings, the order of archive IDs in the array
define the overall archives order for Datadog.type [*required*]
enum
Type of the archive order definition.
Allowed enum values: `archive_order`default: `archive_order`
```
{
"data": {
"attributes": {
"archive_ids": [
"a2zcMylnM4OCHpYusxIi1g",
"a2zcMylnM4OCHpYusxIi2g",
"a2zcMylnM4OCHpYusxIi3g"
]
},
"type": "archive_order"
}
}
```Forbidden
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```Too many requests
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```### Code Example- [Curl](?code-lang=curl#)
- [Python](?code-lang=python#)
- [Ruby](?code-lang=ruby#)
- [Go](?code-lang=go#)
- [Java](?code-lang=java#)
- [Rust](?code-lang=rust#)
- [Typescript](?code-lang=typescript#)

Get archive orderCopy```

# Curl commandcurl -X GET "https://api.ap1.datadoghq.com"https://api.ap2.datadoghq.com"https://api.datadoghq.eu"https://api.ddog-gov.com"https://api.datadoghq.com"https://api.us3.datadoghq.com"https://api.us5.datadoghq.com/api/v2/logs/config/archive-order" \
-H "Accept: application/json" \
-H "DD-API-KEY: ${DD_API_KEY}" \
-H "DD-APPLICATION-KEY: ${DD_APP_KEY}"

```
Get archive order```
"""
Get archive order returns "OK" response
"""

from datadog_api_client import ApiClient, Configuration
from datadog_api_client.v2.api.logs_archives_api import LogsArchivesApi

configuration = Configuration()
with ApiClient(configuration) as api_client:
api_instance = LogsArchivesApi(api_client)
response = api_instance.get_logs_archive_order()

print(response)

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=python) and then save the example to `example.py` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" python3 "example.py"`
```
Get archive order```
# Get archive order returns "OK" response

require "datadog_api_client"
api_instance = DatadogAPIClient::V2::LogsArchivesAPI.new
p api_instance.get_logs_archive_order()

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=ruby) and then save the example to `example.rb` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" rb "example.rb"`
```
Get archive order```
// Get archive order returns "OK" response

package main

import (
	"context"
	"encoding/json"
	"fmt"
	"os"

	"github.com/DataDog/datadog-api-client-go/v2/api/datadog"
	"github.com/DataDog/datadog-api-client-go/v2/api/datadogV2"
)

func main() {
	ctx := datadog.NewDefaultContext(context.Background())
	configuration := datadog.NewConfiguration()
	apiClient := datadog.NewAPIClient(configuration)
	api := datadogV2.NewLogsArchivesApi(apiClient)
	resp, r, err := api.GetLogsArchiveOrder(ctx)

	if err != nil {
		fmt.Fprintf(os.Stderr, "Error when calling `LogsArchivesApi.GetLogsArchiveOrder`: %v\n", err)
		fmt.Fprintf(os.Stderr, "Full HTTP response: %v\n", r)
	}

	responseContent, _ := json.MarshalIndent(resp, "", " ")
	fmt.Fprintf(os.Stdout, "Response from `LogsArchivesApi.GetLogsArchiveOrder`:\n%s\n", responseContent)
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=go) and then save the example to `main.go` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" go run "main.go"`
```
Get archive order```
// Get archive order returns "OK" response

import com.datadog.api.client.ApiClient;
import com.datadog.api.client.ApiException;
import com.datadog.api.client.v2.api.LogsArchivesApi;
import com.datadog.api.client.v2.model.LogsArchiveOrder;

public class Example {
public static void main(String[] args) {
ApiClient defaultClient = ApiClient.getDefaultApiClient();
LogsArchivesApi apiInstance = new LogsArchivesApi(defaultClient);

try {
LogsArchiveOrder result = apiInstance.getLogsArchiveOrder();
System.out.println(result);
} catch (ApiException e) {
System.err.println("Exception when calling LogsArchivesApi#getLogsArchiveOrder");
System.err.println("Status code: " + e.getCode());
System.err.println("Reason: " + e.getResponseBody());
System.err.println("Response headers: " + e.getResponseHeaders());
e.printStackTrace();
}
}
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=java) and then save the example to `Example.java` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" java "Example.java"`
```
Get archive order```
// Get archive order returns "OK" response
use datadog_api_client::datadog;
use datadog_api_client::datadogV2::api_logs_archives::LogsArchivesAPI;

#[tokio::main]
async fn main() {
let configuration = datadog::Configuration::new();
let api = LogsArchivesAPI::with_config(configuration);
let resp = api.get_logs_archive_order().await;
if let Ok(value) = resp {
println!("{:#?}", value);
} else {
println!("{:#?}", resp.unwrap_err());
}
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=rust) and then save the example to `src/main.rs` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" cargo run`
```
Get archive order```
/**
* Get archive order returns "OK" response
*/

import { client, v2 } from "@datadog/datadog-api-client";

const configuration = client.createConfiguration();
const apiInstance = new v2.LogsArchivesApi(configuration);

apiInstance
.getLogsArchiveOrder()
.then((data: v2.LogsArchiveOrder) => {
console.log(
"API called successfully. Returned data: " + JSON.stringify(data)
);
})
.catch((error: any) => console.error(error));

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=typescript) and then save the example to `example.ts` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" tsc "example.ts"`
```## Update archive order- v2 (latest)
PUT https://api.ap1.datadoghq.com/api/v2/logs/config/archive-orderhttps://api.ap2.datadoghq.com/api/v2/logs/config/archive-orderhttps://api.datadoghq.eu/api/v2/logs/config/archive-orderhttps://api.ddog-gov.com/api/v2/logs/config/archive-orderhttps://api.datadoghq.com/api/v2/logs/config/archive-orderhttps://api.us3.datadoghq.com/api/v2/logs/config/archive-orderhttps://api.us5.datadoghq.com/api/v2/logs/config/archive-order
### OverviewUpdate the order of your archives. Since logs are processed sequentially, reordering an archive may change
the structure and content of the data processed by other archives.**Note**: Using the `PUT` method updates your archive’s order by replacing the current order
with the new one.This endpoint requires the `logs_write_archives` permission.### Request#### Body Data (required)An object containing the new ordered list of archive IDs.
- Model
- Example
Expand All
Field
Type
Description
data
object
The definition of an archive order.
attributes [*required*]
object
The attributes associated with the archive order.
archive_ids [*required*]
[string]
An ordered array of `<ARCHIVE_ID>` strings, the order of archive IDs in the array
define the overall archives order for Datadog.type [*required*]
enum
Type of the archive order definition.
Allowed enum values: `archive_order`default: `archive_order`
```
{
"data": {
"attributes": {
"archive_ids": [
"a2zcMylnM4OCHpYusxIi1g",
"a2zcMylnM4OCHpYusxIi2g",
"a2zcMylnM4OCHpYusxIi3g"
]
},
"type": "archive_order"
}
}
```### Response- 200
- 400
- 403
- 422
- 429
OK
- Model
- Example
A ordered list of archive IDs.
Expand All
Field
Type
Description
data
object
The definition of an archive order.
attributes [*required*]
object
The attributes associated with the archive order.
archive_ids [*required*]
[string]
An ordered array of `<ARCHIVE_ID>` strings, the order of archive IDs in the array
define the overall archives order for Datadog.type [*required*]
enum
Type of the archive order definition.
Allowed enum values: `archive_order`default: `archive_order`
```
{
"data": {
"attributes": {
"archive_ids": [
"a2zcMylnM4OCHpYusxIi1g",
"a2zcMylnM4OCHpYusxIi2g",
"a2zcMylnM4OCHpYusxIi3g"
]
},
"type": "archive_order"
}
}
```Bad Request
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```Forbidden
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```Unprocessable Entity
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```Too many requests
- Model
- Example
API error response.
Expand All
Field
Type
Description
errors [*required*]
[string]
A list of errors.
```
{
"errors": [
"Bad Request"
]
}
```### Code Example- [Curl](?code-lang=curl#)
- [Python](?code-lang=python#)
- [Ruby](?code-lang=ruby#)
- [Go](?code-lang=go#)
- [Java](?code-lang=java#)
- [Rust](?code-lang=rust#)
- [Typescript](?code-lang=typescript#)

Update archive orderCopy```

# Curl commandcurl -X PUT "https://api.ap1.datadoghq.com"https://api.ap2.datadoghq.com"https://api.datadoghq.eu"https://api.ddog-gov.com"https://api.datadoghq.com"https://api.us3.datadoghq.com"https://api.us5.datadoghq.com/api/v2/logs/config/archive-order" \
-H "Accept: application/json" \
-H "Content-Type: application/json" \
-H "DD-API-KEY: ${DD_API_KEY}" \
-H "DD-APPLICATION-KEY: ${DD_APP_KEY}" \
-d @- << EOF
{
"data": {
"attributes": {
"archive_ids": [
"a2zcMylnM4OCHpYusxIi1g",
"a2zcMylnM4OCHpYusxIi2g",
"a2zcMylnM4OCHpYusxIi3g"
]
},
"type": "archive_order"
}
}
EOF

```
Update archive order```
"""
Update archive order returns "OK" response
"""

from datadog_api_client import ApiClient, Configuration
from datadog_api_client.v2.api.logs_archives_api import LogsArchivesApi
from datadog_api_client.v2.model.logs_archive_order import LogsArchiveOrder
from datadog_api_client.v2.model.logs_archive_order_attributes import LogsArchiveOrderAttributes
from datadog_api_client.v2.model.logs_archive_order_definition import LogsArchiveOrderDefinition
from datadog_api_client.v2.model.logs_archive_order_definition_type import LogsArchiveOrderDefinitionType

body = LogsArchiveOrder(
data=LogsArchiveOrderDefinition(
attributes=LogsArchiveOrderAttributes(
archive_ids=[
"a2zcMylnM4OCHpYusxIi1g",
"a2zcMylnM4OCHpYusxIi2g",
"a2zcMylnM4OCHpYusxIi3g",
],
),
type=LogsArchiveOrderDefinitionType.ARCHIVE_ORDER,
),
)

configuration = Configuration()
with ApiClient(configuration) as api_client:
api_instance = LogsArchivesApi(api_client)
response = api_instance.update_logs_archive_order(body=body)

print(response)

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=python) and then save the example to `example.py` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" python3 "example.py"`
```
Update archive order```
# Update archive order returns "OK" response

require "datadog_api_client"
api_instance = DatadogAPIClient::V2::LogsArchivesAPI.new

body = DatadogAPIClient::V2::LogsArchiveOrder.new({
data: DatadogAPIClient::V2::LogsArchiveOrderDefinition.new({
attributes: DatadogAPIClient::V2::LogsArchiveOrderAttributes.new({
archive_ids: [
"a2zcMylnM4OCHpYusxIi1g",
"a2zcMylnM4OCHpYusxIi2g",
"a2zcMylnM4OCHpYusxIi3g",
],
}),
type: DatadogAPIClient::V2::LogsArchiveOrderDefinitionType::ARCHIVE_ORDER,
}),
})
p api_instance.update_logs_archive_order(body)

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=ruby) and then save the example to `example.rb` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" rb "example.rb"`
```
Update archive order```
// Update archive order returns "OK" response

package main

import (
	"context"
	"encoding/json"
	"fmt"
	"os"

	"github.com/DataDog/datadog-api-client-go/v2/api/datadog"
	"github.com/DataDog/datadog-api-client-go/v2/api/datadogV2"
)

func main() {
	body := datadogV2.LogsArchiveOrder{
		Data: &datadogV2.LogsArchiveOrderDefinition{
			Attributes: datadogV2.LogsArchiveOrderAttributes{
				ArchiveIds: []string{
					"a2zcMylnM4OCHpYusxIi1g",
					"a2zcMylnM4OCHpYusxIi2g",
					"a2zcMylnM4OCHpYusxIi3g",
				},
			},
			Type: datadogV2.LOGSARCHIVEORDERDEFINITIONTYPE_ARCHIVE_ORDER,
		},
	}
	ctx := datadog.NewDefaultContext(context.Background())
	configuration := datadog.NewConfiguration()
	apiClient := datadog.NewAPIClient(configuration)
	api := datadogV2.NewLogsArchivesApi(apiClient)
	resp, r, err := api.UpdateLogsArchiveOrder(ctx, body)

	if err != nil {
		fmt.Fprintf(os.Stderr, "Error when calling `LogsArchivesApi.UpdateLogsArchiveOrder`: %v\n", err)
		fmt.Fprintf(os.Stderr, "Full HTTP response: %v\n", r)
	}

	responseContent, _ := json.MarshalIndent(resp, "", " ")
	fmt.Fprintf(os.Stdout, "Response from `LogsArchivesApi.UpdateLogsArchiveOrder`:\n%s\n", responseContent)
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=go) and then save the example to `main.go` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" go run "main.go"`
```
Update archive order```
// Update archive order returns "OK" response

import com.datadog.api.client.ApiClient;
import com.datadog.api.client.ApiException;
import com.datadog.api.client.v2.api.LogsArchivesApi;
import com.datadog.api.client.v2.model.LogsArchiveOrder;
import com.datadog.api.client.v2.model.LogsArchiveOrderAttributes;
import com.datadog.api.client.v2.model.LogsArchiveOrderDefinition;
import com.datadog.api.client.v2.model.LogsArchiveOrderDefinitionType;
import java.util.Arrays;

public class Example {
public static void main(String[] args) {
ApiClient defaultClient = ApiClient.getDefaultApiClient();
LogsArchivesApi apiInstance = new LogsArchivesApi(defaultClient);

LogsArchiveOrder body =
new LogsArchiveOrder()
.data(
new LogsArchiveOrderDefinition()
.attributes(
new LogsArchiveOrderAttributes()
.archiveIds(
Arrays.asList(
"a2zcMylnM4OCHpYusxIi1g",
"a2zcMylnM4OCHpYusxIi2g",
"a2zcMylnM4OCHpYusxIi3g")))
.type(LogsArchiveOrderDefinitionType.ARCHIVE_ORDER));

try {
LogsArchiveOrder result = apiInstance.updateLogsArchiveOrder(body);
System.out.println(result);
} catch (ApiException e) {
System.err.println("Exception when calling LogsArchivesApi#updateLogsArchiveOrder");
System.err.println("Status code: " + e.getCode());
System.err.println("Reason: " + e.getResponseBody());
System.err.println("Response headers: " + e.getResponseHeaders());
e.printStackTrace();
}
}
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=java) and then save the example to `Example.java` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" java "Example.java"`
```
Update archive order```
// Update archive order returns "OK" response
use datadog_api_client::datadog;
use datadog_api_client::datadogV2::api_logs_archives::LogsArchivesAPI;
use datadog_api_client::datadogV2::model::LogsArchiveOrder;
use datadog_api_client::datadogV2::model::LogsArchiveOrderAttributes;
use datadog_api_client::datadogV2::model::LogsArchiveOrderDefinition;
use datadog_api_client::datadogV2::model::LogsArchiveOrderDefinitionType;

#[tokio::main]
async fn main() {
let body = LogsArchiveOrder::new().data(LogsArchiveOrderDefinition::new(
LogsArchiveOrderAttributes::new(vec![
"a2zcMylnM4OCHpYusxIi1g".to_string(),
"a2zcMylnM4OCHpYusxIi2g".to_string(),
"a2zcMylnM4OCHpYusxIi3g".to_string(),
]),
LogsArchiveOrderDefinitionType::ARCHIVE_ORDER,
));
let configuration = datadog::Configuration::new();
let api = LogsArchivesAPI::with_config(configuration);
let resp = api.update_logs_archive_order(body).await;
if let Ok(value) = resp {
println!("{:#?}", value);
} else {
println!("{:#?}", resp.unwrap_err());
}
}

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=rust) and then save the example to `src/main.rs` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" cargo run`
```
Update archive order```
/**
* Update archive order returns "OK" response
*/

import { client, v2 } from "@datadog/datadog-api-client";

const configuration = client.createConfiguration();
const apiInstance = new v2.LogsArchivesApi(configuration);

const params: v2.LogsArchivesApiUpdateLogsArchiveOrderRequest = {
body: {
data: {
attributes: {
archiveIds: [
"a2zcMylnM4OCHpYusxIi1g",
"a2zcMylnM4OCHpYusxIi2g",
"a2zcMylnM4OCHpYusxIi3g",
],
},
type: "archive_order",
},
},
};

apiInstance
.updateLogsArchiveOrder(params)
.then((data: v2.LogsArchiveOrder) => {
console.log(
"API called successfully. Returned data: " + JSON.stringify(data)
);
})
.catch((error: any) => console.error(error));

```#### InstructionsFirst [install the library and its dependencies](https://docs.datadoghq.com/api/latest/?code-lang=typescript) and then save the example to `example.ts` and run following commands:
```

`DD_SITE="datadoghq.comus3.datadoghq.comus5.datadoghq.comdatadoghq.euap1.datadoghq.comap2.datadoghq.comddog-gov.com" DD_API_KEY="<API-KEY>" DD_APP_KEY="<APP-KEY>" tsc "example.ts"`
```###### Request a personalized demo×##### Get Started with Datadog