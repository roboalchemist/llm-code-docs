# Source: https://firebase.google.com/docs/reference/android/com/google/firebase/vertexai/type/ImagenSafetyFilterLevel.md.txt

# Source: https://firebase.google.com/docs/reference/swift/firebaseai/api/reference/Structs/ImagenSafetyFilterLevel.md.txt

# Source: https://firebase.google.com/docs/reference/kotlin/com/google/firebase/vertexai/type/ImagenSafetyFilterLevel.md.txt

# Source: https://firebase.google.com/docs/reference/android/com/google/firebase/ai/type/ImagenSafetyFilterLevel.md.txt

# Source: https://firebase.google.com/docs/reference/swift/firebasevertexai/api/reference/Structs/ImagenSafetyFilterLevel.md.txt

# FirebaseVertexAI Framework Reference

# ImagenSafetyFilterLevel

    @available(iOS 15.0, macOS 12.0, tvOS 15.0, watchOS 8.0, *)
    public struct ImagenSafetyFilterLevel : ProtoEnum, Sendable

A filter level controlling how aggressively to filter sensitive content.

Text prompts provided as inputs and images (generated or uploaded) through Imagen on Vertex AI
are assessed against a list of safety filters, which include 'harmful categories' (for example,
`violence`, `sexual`, `derogatory`, and `toxic`). This filter level controls how aggressively to
filter out potentially harmful content from responses. See the
[`safetySetting`](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/imagen-api#parameter_list)
documentation and the [Responsible AI and usage
guidelines](https://cloud.google.com/vertex-ai/generative-ai/docs/image/responsible-ai-imagen#safety-filters)
for more details.
- `
  ``
  ``
  `

  ### [blockLowAndAbove](https://firebase.google.com/docs/reference/swift/firebasevertexai/api/reference/Structs/ImagenSafetyFilterLevel#/s:16FirebaseVertexAI23ImagenSafetyFilterLevelV16blockLowAndAboveACvpZ)

  `
  `  
  The most aggressive filtering level; most strict blocking.  

  #### Declaration

  Swift  

      public static let blockLowAndAbove: ImagenSafetyFilterLevel

- `
  ``
  ``
  `

  ### [blockMediumAndAbove](https://firebase.google.com/docs/reference/swift/firebasevertexai/api/reference/Structs/ImagenSafetyFilterLevel#/s:16FirebaseVertexAI23ImagenSafetyFilterLevelV19blockMediumAndAboveACvpZ)

  `
  `  
  Blocks some problematic prompts and responses.  

  #### Declaration

  Swift  

      public static let blockMediumAndAbove: ImagenSafetyFilterLevel

- `
  ``
  ``
  `

  ### [blockOnlyHigh](https://firebase.google.com/docs/reference/swift/firebasevertexai/api/reference/Structs/ImagenSafetyFilterLevel#/s:16FirebaseVertexAI23ImagenSafetyFilterLevelV13blockOnlyHighACvpZ)

  `
  `  
  Reduces the number of requests blocked due to safety filters.  
  Important

  This may increase objectionable content generated by Imagen.  

  #### Declaration

  Swift  

      public static let blockOnlyHigh: ImagenSafetyFilterLevel

- `
  ``
  ``
  `

  ### [blockNone](https://firebase.google.com/docs/reference/swift/firebasevertexai/api/reference/Structs/ImagenSafetyFilterLevel#/s:16FirebaseVertexAI23ImagenSafetyFilterLevelV9blockNoneACvpZ)

  `
  `  
  The least aggressive filtering level; blocks very few problematic prompts and responses.  
  Important

  Access to this feature is restricted and may require your use case to be reviewed
  and approved by Cloud support.  

  #### Declaration

  Swift  

      public static let blockNone: ImagenSafetyFilterLevel