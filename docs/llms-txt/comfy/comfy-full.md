# Comfy Documentation

Source: https://docs.comfy.org/llms-full.txt

---

# Custom Nodes
Source: https://docs.comfy.org/development/core-concepts/custom-nodes

Learn about installing, enabling dependencies, updating, disabling, and uninstalling custom nodes in ComfyUI

## About Custom Nodes

After installing ComfyUI, you'll discover that it includes many built-in nodes. These native nodes are called **Comfy Core** nodes, which are officially maintained by ComfyUI.

Additionally, there are numerous [**custom nodes**](https://registry.comfy.org) created by various authors from the ComfyUI community. These custom nodes bring extensive functionality to ComfyUI, greatly expanding its capabilities and feature boundaries.

In this guide, we'll cover various operations related to custom nodes, including installation, updates, disabling, uninstalling, and dependency installation.

Anyone can develop their own custom extensions for ComfyUI and share them with others. You can find many community custom nodes [here](https://registry.comfy.org). If you want to develop your own custom nodes, visit the section below to get started:

<Card title="Start Developing Custom Nodes" icon="link" href="/custom-nodes/overview">
  Learn how to start developing a custom node
</Card>

## Custom Node Management

In this section we will cover:

* Installing custom nodes
* Installing node dependencies
* Custom node version control
* Uninstalling custom nodes
* Temporarily disabling custom nodes
* Handling custom node dependency conflicts

### 1. Installing Custom Nodes

Currently, ComfyUI supports installing custom nodes through multiple methods, including:

* [Install via ComfyUI Manager (Recommended)](#install-via-comfyui-manager)
* Install via Git
* Manual installation

We recommend installing custom nodes through **ComfyUI Manager**, which is a highly significant tool in the ComfyUI custom node ecosystem. It makes custom node management (such as searching, installing, updating, disabling, and uninstalling) simple - you just need to search for the node you want to install in ComfyUI Manager and click install.

However, since all custom nodes are currently stored on GitHub, for regions that cannot access GitHub normally, we have written detailed instructions for different custom node installation methods in this guide.

Additionally, since we recommend using **ComfyUI Manager** for plugin management, we recommend using this tool for plugin management. You can find its source code [here](https://github.com/Comfy-Org/ComfyUI-Manager).
Therefore, in this documentation, we will use installing ComfyUI Manager as a custom node installation example, and supplement how to use it for node management in the relevant introduction sections.

<Tabs>
  <Tab title="Install via ComfyUI Manager">
    Since ComfyUI Manager has very rich functionality, we will use a separate document to introduce the ComfyUI Manager installation chapter. Please visit the link below to learn how to use ComfyUI Manager to install custom nodes.

    <Card title="Install Custom Nodes with ComfyUI Manager" icon="link" href="/installation/install_custom_node#method-1%3A-comfyui-manager-recommended">
      Learn how to use ComfyUI Manager to install custom nodes
    </Card>
  </Tab>

  <Tab title="Install via Git">
    <Steps>
      <Step title="Ensure Git is Installed">
        First, you need to ensure that Git is installed on your system. You can check if Git is installed by entering the following command in your system terminal:

        ```bash  theme={null}
        git --version
        ```

        If Git is installed, you will see output similar to the following:

                <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/custom_nodes/win_terminal.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=c0e90164d3a1c8f9ed0f54164b16443b" alt="Windows Terminal" data-og-width="1114" width="1114" data-og-height="228" height="228" data-path="images/concepts/custom_nodes/win_terminal.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/custom_nodes/win_terminal.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=a357e53c048002cce88022038c5bf828 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/custom_nodes/win_terminal.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=9cb6790f16ae6e6f63235f1e959600d5 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/custom_nodes/win_terminal.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=271a0c821c923b7b2fdc3d5746dfeaa8 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/custom_nodes/win_terminal.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=bc288cf2488e869ae3d2f6a8a56063ed 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/custom_nodes/win_terminal.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=2af5489655f4e5cb48890bf017a7b647 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/custom_nodes/win_terminal.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=808b660d6e7116094aadda28a1dfb2d6 2500w" />

        If not yet installed, please visit [git-scm.com](https://git-scm.com/) to download the corresponding installation package. Linux users please refer to [git-scm.com/downloads/linux](https://git-scm.com/downloads/linux) for installation instructions.

        <Tip>
          For ComfyUI Desktop version, you can use the Desktop terminal as shown below to complete the installation.
          <img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/desktop_terminal.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=946b0d382dc99eba1b7c33072ed1bce2" alt="ComfyUI Desktop Terminal" data-og-width="1800" width="1800" data-og-height="1403" height="1403" data-path="images/concepts/custom_nodes/desktop_terminal.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/desktop_terminal.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=4ce6791b7ace89fa5a1e7fd8438b8dc0 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/desktop_terminal.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=6e9fcf09190d3cfa4ee6ef730d6b9fe6 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/desktop_terminal.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=fc1b03b1b9b43062a5c4bd31dbea62d9 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/desktop_terminal.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=216afbc5d75842f85ebc8f1b76b2d5be 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/desktop_terminal.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=cce1b33c7a01b016a61baa10339c97df 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/desktop_terminal.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=1216d21a3cc886305cc67a8c22edfb91 2500w" />
        </Tip>
      </Step>

      <Step title="Clone Custom Node Code to Directory">
        After completing the Git installation, we need the repository address of the custom node. Here we use the ComfyUI-Manager repository address as an example:

        ```bash  theme={null}
        https://github.com/Comfy-Org/ComfyUI-Manager
        ```

        <Tip>For regions that cannot access GitHub smoothly, you can try using other code hosting service websites to fork the corresponding repository, then use that repository address to complete the node installation, such as gitee, etc.</Tip>

        First, we need to navigate to the ComfyUI custom nodes directory. Using ComfyUI portable version as an example, if the folder location is `D:\ComfyUI_windows_portable`, then you should be able to find the custom nodes folder at `D:\ComfyUI_windows_portable\ComfyUI\custom_nodes`. First, we need to use the `cd` command to enter the corresponding directory:

        ```bash  theme={null}
        cd D:\ComfyUI_windows_portable\ComfyUI\custom_nodes
        ```

        Then we use the `git clone` command to complete the node installation:

        ```bash  theme={null}
        git clone https://github.com/Comfy-Org/ComfyUI-Manager
        ```

        If everything goes smoothly, you will see output similar to the following:

                <img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/install_custom_nodes_by_git.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=80409dceca35c9d58c42dde48c4c53a5" alt="Install Custom Nodes via Git" data-og-width="1116" width="1116" data-og-height="317" height="317" data-path="images/concepts/custom_nodes/install_custom_nodes_by_git.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/install_custom_nodes_by_git.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=11856e33ebf16d403fc1bfd7dea88fa7 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/install_custom_nodes_by_git.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=3fe94004114027d4c921c43f3d42a9d9 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/install_custom_nodes_by_git.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=ac2c83634737859345407522169a2777 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/install_custom_nodes_by_git.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=b1a5d9aecc3d51fcf9cd8a254797f565 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/install_custom_nodes_by_git.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=dd666e0e9c82d0929c1c4e0430003571 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/install_custom_nodes_by_git.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=205985da6950ff470816f40f5fc51500 2500w" />

        This means you have successfully cloned the custom node code. Next, we need to install the corresponding dependencies.
      </Step>

      <Step title="Install Dependencies">
        Please refer to the instructions in the [Installing Node Dependencies](#installing-node-dependencies) section for dependency installation.
      </Step>
    </Steps>
  </Tab>

  <Tab title="Manual Installation">
    Manual installation is not the recommended installation method, but it serves as a backup option when you cannot install smoothly using git.

    <Warning>
      Plugins installed this way will lose the corresponding git version history information and will not be convenient for subsequent version management.
    </Warning>

    <Steps>
      <Step title="Download Custom Node Code ZIP Package">
        For manual installation, we need to first download the corresponding node code and then extract it to the appropriate directory.

                <img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/download_zip.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=34fa06cf4580152428c5d2b55b192911" alt="Download Node Code" data-og-width="1011" width="1011" data-og-height="618" height="618" data-path="images/concepts/custom_nodes/download_zip.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/download_zip.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=abe163da0d4486c9ac953740efa53132 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/download_zip.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=938e6e59ac49b2f6f4d5a6ab8e3fc34b 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/download_zip.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=4d2b335af8cf590678abf746ad2faad0 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/download_zip.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=40a958844a5808923eb70f12ecdca784 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/download_zip.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=ea0a46e7137e32594e7d3525b679865c 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/download_zip.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=b8980eb0deed06232f1081ceb18ed528 2500w" />

        Visit the corresponding custom node repository page:

        1. Click the `Code` button
        2. Then click the `Download ZIP` button to download the ZIP package
        3. Extract the ZIP package
      </Step>

      <Step title="Copy Files to ComfyUI Custom Nodes Directory">
        Copy the extracted code from the above steps to the ComfyUI custom nodes directory. Using ComfyUI portable version as an example, if the folder location is `D:\ComfyUI_windows_portable`, then you should be able to find the custom nodes folder at `D:\ComfyUI_windows_portable\ComfyUI\custom_nodes`. Copy the extracted code from the above steps to the corresponding directory.
      </Step>

      <Step title="Install Dependencies">
        Please refer to the instructions in the [Installing Node Dependencies](#installing-node-dependencies) section for dependency installation.
      </Step>
    </Steps>
  </Tab>
</Tabs>

### 2. Installing Node Dependencies

Custom nodes all require the installation of related dependencies. For example, for ComfyUI-Manager, you can visit the [requirements.txt](https://github.com/Comfy-Org/ComfyUI-Manager/blob/main/requirements.txt) file to view the dependency package requirements.

In the previous steps, we only cloned the custom node code locally and did not install the corresponding dependencies, so next we need to install the corresponding dependencies.

<Note>
  Actually, if you use ComfyUI-Manager to install plugins, ComfyUI Manager will automatically help you complete the dependency installation. You just need to restart ComfyUI after installing the plugin. This is why we strongly recommend using ComfyUI Manager to install custom nodes.

  But perhaps you may not be able to use ComfyUI Manager to install custom nodes smoothly in some situations, so we provide this more detailed dependency installation guide.
</Note>

In the [Dependencies](/development/core-concepts/dependencies) chapter, we introduced the relevant content about dependencies in ComfyUI. ComfyUI is a **Python**-based project, and we built an independent **Python** runtime environment for running ComfyUI. All related dependencies need to be installed in this independent **Python** runtime environment.

If you run `pip install -r requirements.txt` directly in the system-level terminal, the corresponding dependencies may be installed in the system-level **Python** environment, which will cause the dependencies to still be missing in ComfyUI's environment, preventing the corresponding custom nodes from running normally.

So next we need to use ComfyUI's independent Python runtime environment to complete the dependency installation.

Depending on different ComfyUI versions, we will use different methods to install the corresponding dependencies:

<Tabs>
  <Tab title="ComfyUI Portable">
    For ComfyUI Portable version, it uses an embedded Python located in the `\ComfyUI_windows_portable\python_embeded` directory. We need to use this Python to complete the dependency installation.

    First, start the terminal in the portable version directory, or use the `cd` command to navigate to the `\ComfyUI_windows_portable\` directory after starting the terminal.

        <img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/open_terminal.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=ee4569d27b4ef901c5f67d8f795b448c" alt="Start Terminal" data-og-width="1822" width="1822" data-og-height="1187" height="1187" data-path="images/concepts/custom_nodes/open_terminal.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/open_terminal.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=6e1589dff5dd380e4484e6ae44172df4 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/open_terminal.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=107a3e8c8020ddc46b1473004a2e8394 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/open_terminal.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=9eb529d60d9cfb939bed1014c0895939 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/open_terminal.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=f17cb8cb8a0f24461baaa909482591e0 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/open_terminal.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=649ce090ec9804a49662f81b20e192c9 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/open_terminal.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=26d70d0642ffe52424f940463d770a2b 2500w" />

    Ensure that the terminal directory is `\ComfyUI_windows_portable\`, as shown below for `D:\ComfyUI_windows_portable\`

        <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/custom_nodes/terminal.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=f6da2e8c4eef1016e6fde84ee5527ce0" alt="Terminal" data-og-width="2400" width="2400" data-og-height="1147" height="1147" data-path="images/concepts/custom_nodes/terminal.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/custom_nodes/terminal.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=d4c3e07411aed9823f7b52287e8c4d95 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/custom_nodes/terminal.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=5dd7ecf31d1183f950d31c9b9b4b97c9 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/custom_nodes/terminal.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=0bc86f9512c81d0045ec3cc8e3067846 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/custom_nodes/terminal.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=e54eab41295a7fb58c69bca2b1310f73 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/custom_nodes/terminal.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=eef8a3bad5e20eb6bf0b14c756cdef1b 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/custom_nodes/terminal.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=b91b3d6f97318bcae7aa99c6613e42ee 2500w" />

    Then use `python_embeded\python.exe` to complete the dependency installation:

    ```bash  theme={null}
    python_embeded\python.exe -m pip install -r ComfyUI\custom_nodes\ComfyUI-Manager\requirements.txt
    ```

    Of course, you can replace ComfyUI-Manager with the name of the custom node you actually installed, but make sure that a `requirements.txt` file exists in the corresponding node directory.
  </Tab>

  <Tab title="ComfyUI Desktop">
    <Tip>
      Since ComfyUI Desktop already has ComfyUI-Manager and its dependencies installed during the installation process, and this guide uses ComfyUI Manager as an example for custom node installation, you don't actually need to perform ComfyUI Manager dependency installation in the desktop version.
      If there are no unexpected issues, we recommend using ComfyUI Manager to install custom nodes, so you don't need to manually install dependencies.
    </Tip>

        <img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/desktop_terminal.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=946b0d382dc99eba1b7c33072ed1bce2" alt="ComfyUI Desktop Terminal" data-og-width="1800" width="1800" data-og-height="1403" height="1403" data-path="images/concepts/custom_nodes/desktop_terminal.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/desktop_terminal.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=4ce6791b7ace89fa5a1e7fd8438b8dc0 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/desktop_terminal.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=6e9fcf09190d3cfa4ee6ef730d6b9fe6 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/desktop_terminal.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=fc1b03b1b9b43062a5c4bd31dbea62d9 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/desktop_terminal.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=216afbc5d75842f85ebc8f1b76b2d5be 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/desktop_terminal.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=cce1b33c7a01b016a61baa10339c97df 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/desktop_terminal.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=1216d21a3cc886305cc67a8c22edfb91 2500w" />

    Then use the following command to install the dependencies for the corresponding plugin:

    ```bash  theme={null}
    pip install -r .\custom_nodes\<corresponding_custom_node_name>\requirements.txt
    ```

    As shown below, this is the dependency installation for ComfyUI-Hunyuan3Dwrapper:

        <img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/install_dependencies.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=23366292f947d238ea260f07d8529502" alt="ComfyUI Desktop Dependency Installation" data-og-width="2038" width="2038" data-og-height="1472" height="1472" data-path="images/concepts/custom_nodes/install_dependencies.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/install_dependencies.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=8849616b5cbe36efa5096e396ab83795 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/install_dependencies.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=32c343240d23faf1f40d94ba43f9c747 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/install_dependencies.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=859ffceb951a83654e3c363a489c3c7a 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/install_dependencies.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=376dd9c1750dbff7a9a796285ce1704e 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/install_dependencies.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=26af3d5f53099c4b5ebb4c51c13053d8 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/custom_nodes/install_dependencies.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=34db9db7e172a2cd0af03eb08ee1265a 2500w" />
  </Tab>

  <Tab title="Custom Python Environment Users">
    For users with custom Python environments, we recommend using `pip install -r requirements.txt` to complete the dependency installation.
  </Tab>
</Tabs>

### Custom Node Version Control

Custom node version control is actually based on Git version control. You can manage node versions through Git, but ComfyUI Manager has already integrated this version management functionality very well. Many thanks to [@Dr.Lt.Data](https://github.com/ltdrdata) for bringing us such a convenient tool.

In this section, we will still explain these two different plugin version management methods for you, but if you use ZIP packages for manual installation, the corresponding git version history information will be lost, making it impossible to perform version management.

<Tabs>
  <Tab title="Version Management with ComfyUI Manager">
    <Tip>Since we are iterating on ComfyUI Manager, the actual latest interface and steps may change significantly</Tip>

    <Steps>
      <Step title="Enter Node Management Interface">
        Perform the corresponding operations as shown to enter the ComfyUI Manager interface
      </Step>

      <Step title="Find the Corresponding Custom Node Package">
        You can use the corresponding filters to filter out installed node packages and then perform the corresponding node management
      </Step>

      <Step title="Perform Version Switching">
        Switch to the corresponding version. Manager will help you complete the corresponding dependency updates and installation. Usually, you need to restart ComfyUI after switching versions for the changes to take effect.
      </Step>
    </Steps>
  </Tab>

  <Tab title="Version Management with Git">
    <Steps>
      <Step title="Navigate to Directory Using Command Line">
        Find the directory folder where your corresponding node is located, such as `ComfyUI/custom_nodes/ComfyUI-Manager`
        Use the `cd` command to enter the corresponding folder:

        ```bash  theme={null}
        cd <your_installation_directory>/ComfyUI/custom_nodes/ComfyUI-Manager
        ```
      </Step>

      <Step title="View Versions Using Git Commands">
        You can use the following command to view all available tags and releases:

        ```bash  theme={null}
        git tag
        ```

        This will list all version tags, and you can choose the version you want to switch to.
      </Step>

      <Step title="Switch to Specified Version">
        Use the following command to switch to a specified tag or release:

        ```bash  theme={null}
        git checkout <tag_name>
        ```

        Replace `<tag_name>` with the specific version tag you want to switch to.
      </Step>

      <Step title="Switch to Specific Commit Version">
        If you want to switch to a specific commit version, you can use the following command:

        ```bash  theme={null}
        git checkout <commit_hash>
        ```

        Replace `<commit_hash>` with the specific commit hash you want to switch to.
      </Step>

      <Step title="Install Dependencies">
        Since the dependencies of the corresponding custom node package may change after version switching, you need to reinstall the dependencies for the corresponding node. Please refer to the instructions in the [Installing Node Dependencies](#2-installing-node-dependencies) section to enter the corresponding environment for installation.
      </Step>
    </Steps>
  </Tab>
</Tabs>

### Uninstalling Custom Nodes

To be updated

### Temporarily Disabling Custom Nodes

To be updated

### Custom Node Dependency Conflicts

To be updated

## ComfyUI Manager

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_nodes_manager.png?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=cea0de500828224b23b64cef84a31936" alt="ComfyUI Manager Interface" data-og-width="1920" width="1920" data-og-height="1080" height="1080" data-path="images/concepts/core-concepts_nodes_manager.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_nodes_manager.png?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=b66fffe074ba15068c2868e5e127cc41 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_nodes_manager.png?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=951aa7941493ea56018c7aa77d1bc143 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_nodes_manager.png?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=93e156a11636d64258510480fe1c28c2 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_nodes_manager.png?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=352d680f30f32e457031be3c6de987d9 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_nodes_manager.png?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=a034b1663b608f4d64c6a690ea652b88 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_nodes_manager.png?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=4ca79e9090f1ed2513db42fa9838af40 2500w" />

This tool is currently included by default in the [Desktop version](/installation/desktop/windows), while in the [Portable version](/installation/comfyui_portable_windows), you need to refer to the installation instructions in the [Install Manager](#installing-custom-nodes) section of this document.

<Note>
  As ComfyUI continues to develop, ComfyUI Manager plays an increasingly important role in ComfyUI. Currently, ComfyUI-Manager has officially joined the Comfy Org organization, officially becoming part of ComfyUI's core dependencies, and continues to be maintained by the original author [Dr.Lt.Data](https://github.com/ltdrdata). You can read [this blog post](https://blog.comfy.org/p/comfyui-manager-joins-comfy-org) for more information.
  In future iterations, we will greatly optimize the use of ComfyUI Manager, so the interface shown in this documentation may differ from the latest version of ComfyUI Manager.
</Note>

### Installing the Manager

If you are running the ComfyUI server application, you need to install the manager. If ComfyUI is running, please close it before continuing.

The first step is to install Git, which is a command-line application for software version control. Git will download the ComfyUI manager from [github.com](https://github.com). Download and install Git from [git-scm.com](https://git-scm.com/).

After installing Git, navigate to the ComfyUI server program directory and enter the folder labeled **custom\_nodes**. Open a command window or terminal. Make sure the command line shows the current directory path as **custom\_nodes**. Enter the following command. This will download the manager. Technically, this is called *cloning a Git repository*.

### Detecting Missing Nodes

After installing the manager, you can detect missing nodes in the manager.

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_nodes_manager.png?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=cea0de500828224b23b64cef84a31936" alt="ComfyUI Manager Interface" data-og-width="1920" width="1920" data-og-height="1080" height="1080" data-path="images/concepts/core-concepts_nodes_manager.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_nodes_manager.png?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=b66fffe074ba15068c2868e5e127cc41 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_nodes_manager.png?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=951aa7941493ea56018c7aa77d1bc143 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_nodes_manager.png?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=93e156a11636d64258510480fe1c28c2 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_nodes_manager.png?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=352d680f30f32e457031be3c6de987d9 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_nodes_manager.png?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=a034b1663b608f4d64c6a690ea652b88 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_nodes_manager.png?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=4ca79e9090f1ed2513db42fa9838af40 2500w" />

## Developing a Custom Node

If you have some development capabilities, please start with the documentation below to learn how to begin developing a custom node.

<Card title="Start Developing Custom Nodes" icon="link" href="/custom-nodes/overview">
  Learn how to start developing a custom node
</Card>


# Dependencies
Source: https://docs.comfy.org/development/core-concepts/dependencies

Understand dependencies in ComfyUI

## A workflow file depends on other files

We often obtain various workflow files from the community, but frequently find that the workflow cannot run directly after loading. This is because a workflow file depends on other files besides the workflow itself, such as media asset inputs, models, custom nodes, related Python dependencies, etc.
ComfyUI workflows can only run normally when all relevant dependencies are satisfied.

ComfyUI workflow dependencies mainly fall into the following categories:

* Assets (media files including audio, video, images, and other inputs)
* Custom nodes
* Python dependencies
* Models (such as Stable Diffusion models, etc.)

## Assets

An AI model is an example of an ***asset***. In media production, an asset is some media file that supplies input data. For example, a video editing program operates on movie files stored on disk. The editing program's project file holds links to these movie file assets, allowing non-destructive editing that doesn't alter the original movie files.

ComfyUI works the same way. A workflow can only run if all of the required assets are found and loaded. Generative AI models, images, movies, and sounds are some examples of assets that a workflow might depend upon. These are therefore known as ***dependent assets*** or ***asset dependencies***.

## Custom Nodes

Custom nodes are an important component of ComfyUI that extend its functionality. They are created by the community and can be installed to add new capabilities to your workflows.

## Python Dependencies

ComfyUI is a Python-based project. We build a standalone Python environment to run ComfyUI, and all related dependencies are installed in this isolated Python environment.

### ComfyUI Dependencies

You can view ComfyUI's current dependencies in the [requirements.txt](https://github.com/comfyanonymous/ComfyUI/blob/master/requirements.txt) file:

```text  theme={null}
comfyui-frontend-package==1.14.5
torch
torchsde
torchvision
torchaudio
numpy>=1.25.0
einops
transformers>=4.28.1
tokenizers>=0.13.3
sentencepiece
safetensors>=0.4.2
aiohttp>=3.11.8
yarl>=1.18.0
pyyaml
Pillow
scipy
tqdm
psutil

#non essential dependencies:
kornia>=0.7.1
spandrel
soundfile
av
```

As ComfyUI evolves, we may adjust dependencies accordingly, such as adding new dependencies or removing ones that are no longer needed.
So if you use Git to update ComfyUI, you need to run the following command in the corresponding environment after pulling the latest updates:

```bash  theme={null}
pip install -r requirements.txt
```

This ensures that ComfyUI's dependencies are up to date for proper operation. You can also modify specific package dependency versions to upgrade or downgrade certain dependencies.

Additionally, ComfyUI's frontend [ComfyUI\_frontend](https://github.com/Comfy-Org/ComfyUI_frontend) is currently maintained as a separate project. We update the `comfyui-frontend-package` dependency version after the corresponding version stabilizes. If you need to switch to a different frontend version, you can check the version information [here](https://pypi.org/project/comfyui-frontend-package/#history).

### Custom Node Dependencies

Thanks to the efforts of many authors in the ComfyUI community, we can extend ComfyUI's functionality by using different custom nodes, enabling impressive creativity.

Typically, each custom node has its own dependencies and a separate `requirements.txt` file.
If you use [ComfyUI Manager](https://github.com/ltdrdata/ComfyUI-Manager) to install custom nodes, ComfyUI Manager will usually automatically install the corresponding dependencies.

There are also cases where you need to install dependencies manually. Currently, all custom nodes are installed in the `ComfyUI/custom_nodes` directory.

You need to navigate to the corresponding plugin directory in your ComfyUI Python environment and run `pip install -r requirements.txt` to install the dependencies.

If you're using the [Windows Portable version](/installation/comfyui_portable_windows), you can use the following command in the `ComfyUI_windows_portable` directory:

```
python_embeded\python.exe -m pip install -r ComfyUI\custom_nodes\<custom_node_name>\requirements.txt
```

to install the dependencies for the corresponding node.

### Dependency Conflicts

Dependency conflicts are a common issue when using ComfyUI. You might find that after installing or updating a custom node, previously installed custom nodes can no longer be found in ComfyUI's node library, or error pop-ups appear. One possible reason is dependency conflicts.

There can be many reasons for dependency conflicts, such as:

1. Custom node version locking

Some plugins may fix the exact version of a dependency library (e.g., `open_clip_torch==2.26.1`), while other plugins may require a higher version (e.g., `open_clip_torch>=2.29.0`), making it impossible to satisfy both version requirements simultaneously.

**Solution**: You can try changing the fixed version dependency to a range constraint, such as `open_clip_torch>=2.26.1`, and then reinstall the dependencies to resolve these issues.

2. Environment pollution

During the installation of custom node dependencies, it may overwrite versions of libraries already installed by other plugins. For example, multiple plugins may depend on `PyTorch` but require different CUDA versions, and the later installed plugin will break the existing environment.

**Solutions**:

* You can try manually installing specific versions of dependencies in the Python virtual environment to resolve such issues.
* Or create different Python virtual environments for different plugins to resolve these issues.
* Try installing plugins one by one, restarting ComfyUI after each installation to observe if dependency conflicts occur.

3. Custom node dependency versions incompatible with ComfyUI dependency versions

These types of dependency conflicts may be more difficult to resolve, and you may need to upgrade/downgrade ComfyUI or change the dependency versions of custom nodes to resolve these issues.

**Solution**: These types of dependency conflicts may be more difficult to resolve, and you may need to upgrade/downgrade ComfyUI or change the dependency versions of custom nodes to resolve these issues.

## Models

Models are a significant asset dependency for ComfyUI. Various custom nodes and workflows are built around specific models, such as the Stable Diffusion series, Flux series, Ltxv, and others.
These models are an essential foundation for creation with ComfyUI, so we need to ensure that the models we use are properly available. Typically, our models are saved in the corresponding directory under `ComfyUI/models/`. Of course, you can also create an [extra\_model\_paths.yaml](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) by modifying the template to make additional model paths recognized by ComfyUI.
This allows multiple ComfyUI instances to share the same model library, reducing disk usage.

## Software

An advanced application like ComfyUI also has ***software dependencies***. These are libraries of programming code and data that are required for the application to run. Custom nodes are examples of software dependencies. On an even more fundamental level, the Python programming environment is the ultimate dependency for ComfyUI. The correct version of Python is required to run a particular version of ComfyUI. Updates to Python, ComfyUI, and custom nodes can all be handled from the **ComfyUI Manager** window.

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_dependecies_custom-nodes-manager.png?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=835aa06ad002a4231415f525c0a88b7c" alt="ComfyUI Custom Nodes Manager" data-og-width="1920" width="1920" data-og-height="1080" height="1080" data-path="images/concepts/core-concepts_dependecies_custom-nodes-manager.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_dependecies_custom-nodes-manager.png?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=94aa8f793709b16a9083153a0b064fe6 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_dependecies_custom-nodes-manager.png?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=d13dd3882c168ec856634edb17713652 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_dependecies_custom-nodes-manager.png?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=ef9ac02b82e19f1525870606066eb822 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_dependecies_custom-nodes-manager.png?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=4322615010bbb635e4341b466aef1c9e 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_dependecies_custom-nodes-manager.png?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=1bf5c13c5e5691d231f58de5fac94852 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_dependecies_custom-nodes-manager.png?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=d3eeed20aced880d10130d3d3f9ae681 2500w" />


# Links
Source: https://docs.comfy.org/development/core-concepts/links

Understand connection links in ComfyUI

<Note>
  As ComfyUI is still in rapid iteration and development, we are continuously improving it every day. Therefore, some operations mentioned in this article may change or be omitted. Please refer to the actual interface. If you find changes in actual operations, it may be due to our iterative updates. You can also fork [this repo](https://github.com/Comfy-Org/docs) and help us improve this documentation.
</Note>

## Links connect nodes

In the terminology of ComfyUI, the lines or curves between nodes are called ***links***. They're also known as ***connections*** or wires. Links can be displayed in several ways, such as curves, right angles, straight lines, or completely hidden.

<img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/link_styles.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=f8d0e4e3831f8d6ff87f35f0c99ba9cf" alt="Link styles" data-og-width="1589" width="1589" data-og-height="917" height="917" data-path="images/interface/link/link_styles.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/link_styles.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=03c3b63b23f9f66234a8c970b1ac87a4 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/link_styles.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=2ca5e7791ae5cebe760201527d1e721d 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/link_styles.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=5d3a6cdf39918c5ca833750c27d7585c 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/link_styles.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=dd607c8f625230d3e3666eaf52ea9b7d 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/link_styles.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=0e14b4122210e7b2bb8733bb366eb5e0 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/link_styles.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=8db036858fb704e08de8177246e6394b 2500w" />

You can modify the link style in **Setup Menu** --> **Display (Lite Graph)** --> **Graph** --> **Link Render Mode**.

<img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/render_mode.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=881d8628e7cd4fcfa39826136bf3d491" alt="Canvas Menu" data-og-width="1066" width="1066" data-og-height="764" height="764" data-path="images/interface/link/render_mode.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/render_mode.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=133d7ea012cdf26adc96253270007802 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/render_mode.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=212d08c88b939dbda02359f983721011 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/render_mode.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=214999290dcd6699869ab2b0eb1aa448 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/render_mode.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=d8d605cc377a43ad00091da338897387 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/render_mode.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=b0ab105529c0fd9fb92fa4152ae534ba 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/render_mode.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=d5dc6365757c6f1bff7e2bccbe8bf303 2500w" />

You can also temporarily hide links in the **Canvas Menu**.

<img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/canvas_menu.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=85e33ad64f139785144b0e379c793045" alt="Canvas Menu" data-og-width="272" width="272" data-og-height="620" height="620" data-path="images/interface/link/canvas_menu.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/canvas_menu.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=8930a2fa7914e4a3994b14eb2162d90b 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/canvas_menu.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=ee78f1b13a81190bb932f6e9d4e86294 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/canvas_menu.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=dc4a3191147bdaa4c09c97350bfc7b3c 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/canvas_menu.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=9e1c99e6ebd74831df213de4b6c858c4 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/canvas_menu.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=eaf9a88a01b09a62c2c9d87cfa3fbd59 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/canvas_menu.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=ac011e8679bd3f117bcc3b7dbc4e3089 2500w" />

Link display is crucial. Depending on the situation, it may be necessary to see all links. Especially when learning, sharing, or even just understanding workflows, the visibility of links enables users to follow the flow of data through the graph. For packaged workflows that aren't intended to be altered, it might make sense to hide the links to reduce clutter.

### Reroute node

If legibility of the graph structure is important, then link wires can be manually routed in the 2D space of the graph with a tiny node called **Reroute**. Its purpose is to position the beginning and/or end points of link wires to ensure visibility. We can design a workflow so that link wires don't pass behind nodes, don't cross other link wires, and so on.

<img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/reroute.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=473539e846d7bb6b4c3d61c5d78a2c91" alt="ComfyUI Reroute node" data-og-width="900" width="900" data-og-height="365" height="365" data-path="images/interface/link/reroute.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/reroute.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=c89a5d5ba3992b366bd140e40b9e7ec4 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/reroute.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=5b0e9691e87bfdad69570b1a52dd893a 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/reroute.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=96ab5d1fd8c9086f7c0b36ccac7fccf5 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/reroute.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=6bdb8be3153d06a3e5354f2d488922e6 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/reroute.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=9f874a9ab894f51dbe51306bd0c730ea 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/reroute.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=312fda60408cc84e0ac59dd98c5ed807 2500w" />

We are also continuously improving the native reroute functionality in litegraph. We recommend using this feature in the future to reorganize connections.

<img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/native_reroute.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=9f33c8c4d344a3cb2758e7f33ef65a0b" alt="ComfyUI Native Reroute" data-og-width="463" width="463" data-og-height="315" height="315" data-path="images/interface/link/native_reroute.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/native_reroute.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=ed36c01b39b1a51e5d3fd7472431fc83 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/native_reroute.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=7993952e65b9b4d48273a581b44e5560 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/native_reroute.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=e9de8c15d06c7ed73012046d6b98610b 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/native_reroute.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=50c6e6421a8c2310c6815c7ab745553a 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/native_reroute.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=25ab2a58bc0e00f2f4c181cf5c242037 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/link/native_reroute.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=6afda371c5a718f1656e83d5ead5a302 2500w" />

## Color-coding

The data type of node properties is indicated by color coding of input/output ports and link connection wires. We can always tell which inputs and outputs can be connected to one another by their color. Ports can only be connected to other ports of the same color to ensure matching data types.

Common data types:

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/data_type.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=a2f5203bde87f24afff17ef3cf0bcaa5" alt="ComfyUI Node Data Types" data-og-width="685" width="685" data-og-height="356" height="356" data-path="images/concepts/node/data_type.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/data_type.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=3a128b7a0a1e6e93733f2fe649efd330 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/data_type.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=56df7925857870daf0995d69dcca1c24 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/data_type.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=69feb5a2bdde5813c9444c500710e1cb 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/data_type.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=087178033c73065ed5a521cb04f3f5cb 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/data_type.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=389aadb6b698921a58ffb6e3c65ce796 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/data_type.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=9ee271b46d36d4ab90b1688dbcf80c5d 2500w" />

| Data type                 | Color        |
| ------------------------- | ------------ |
| diffusion model           | lavender     |
| CLIP model                | yellow       |
| VAE model                 | rose         |
| conditioning              | orange       |
| latent image              | pink         |
| pixel image               | blue         |
| mask                      | green        |
| number (integer or float) | light green  |
| mesh                      | bright green |


# Models
Source: https://docs.comfy.org/development/core-concepts/models



## Models are essential

Models are essential building blocks for media generation workflows. They can be combined and mixed to achieve different creative effects.

The word ***model*** has many different meanings. Here, it means a data file carrying information that is required for a node graph to do its work. Specifically, its a data structure that *models* some function. As a verb, to model something means to represent it or provide an example.

The primary example of a model data file in ComfyUI is an AI ***diffusion model***. This is a large set of data that represents the complex relationships among text strings and images, making it possible to translate words into pictures or vice versa. Other examples of common models used for image generation are multimodal vision and language models such as CLIP, and upscaling models such as RealESRGAN.

## Model files

Model files are indispensable for generative media production. Without them, workflows cannot proceed effectively. Models are not included in the ComfyUI installation, but ComfyUI can often automatically download and install missing model files. Many models can be downloaded and installed from the **ComfyUI Manager** window. Models can also be found at websites such as [huggingface.co](https://huggingface.co), [civitai.green](https://civitai.green), and [github.com](https://github.com).

### Using Models in ComfyUI

1. Download and place them in the ComfyUI program directory
   1. Within the **models** folder, you'll find subfolders for various types of models, such as **checkpoints**
   2. The **ComfyUI Manager** helps to automate the process of searching, downloading, and installing
   3. Restart ComfyUI if it's running
2. In your workflow, create the node appropriate to the model type, e.g. **Load Checkpoint**, **Load LoRA**, **Load VAE**
3. In the loader node, choose the model you wish to use
4. Connect the loader node to other nodes in your workflow

## Adding Extra Model Paths

If you want to manage your model files outside of `ComfyUI/models`, you may have the following reasons:

* You have multiple ComfyUI instances and want them to share model files to save disk space
* You have different types of GUI programs (such as WebUI) and want them to use the same model files
* Model files cannot be recognized or found

We provide a way to add extra model search paths via the `extra_model_paths.yaml` configuration file

### Open Config File

<Tabs>
  <Tab title="Portable/Manual Install">
    For the ComfyUI version such as [portable](/installation/comfyui_portable_windows) and [manual](/installation/manual_install), you can find an example file named `extra_model_paths.yaml.example` in the root directory of ComfyUI:

    ```
    ComfyUI/extra_model_paths.yaml.example
    ```

    Copy and rename it to `extra_model_paths.yaml` for use. Keep it in ComfyUI's root directory at `ComfyUI/extra_model_paths.yaml`.
    You can also find the config example file [here](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example)
  </Tab>

  <Tab title="ComfyUI Desktop">
    If you are using the [ComfyUI Desktop](/installation/desktop/windows) application, you can follow the image below to open the extra model config file:

        <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=b2a08255f65a32e3da0c018a3cebb6a2" alt="Open Config File" data-og-width="1056" width="1056" data-og-height="1166" height="1166" data-path="images/desktop/extra_model_paths.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=4f4044b7cef96f0f4a70e7d8a0eb007a 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=aa576e31096b7306e810c10c97416ad5 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=ff8461eb13a9cec15293e7c5663bc65c 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=a2bf5a5335edfa58f37652bfed7a7dc9 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=a1a4c2d6fd021f4a348ea95c4abb5871 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=f508c6130dd86ec9adbef19aaf0d2530 2500w" />

    Or open it directly at:

    <Tabs>
      <Tab title="Windows">
        ```
        C:\Users\YourUsername\AppData\Roaming\ComfyUI\extra_models_config.yaml
        ```
      </Tab>

      <Tab title="macOS">
        ```
        ~/Library/Application Support/ComfyUI/extra_models_config.yaml
        ```
      </Tab>
    </Tabs>

    You should keep the file in the same directory, should not move these files to other places.
  </Tab>
</Tabs>

If the file does not exist, you can create it yourself with any text editor.

### Example Structure

Suppose you want to add the following model paths to ComfyUI:

```
 YOUR_PATH/
   models/
  |     lora/
  |       xxxxx.safetensors
  |     checkpoints/
  |       xxxxx.safetensors
  |     vae/
  |       xxxxx.safetensors
  |     controlnet/
  |        xxxxx.safetensors
```

Then you can configure the `extra_model_paths.yaml` file like below to let ComfyUI recognize the model paths on your device:

```
my_custom_config:
    base_path: YOUR_PATH
    loras: models/loras/
    checkpoints: models/checkpoints/
    vae: models/vae/
    controlnet: models/controlnet/
```

or

```
my_custom_config:
    base_path: YOUR_PATH/models/
    loras: loras
    checkpoints: checkpoints
    vae: vae
    controlnet: controlnet
```

<Warning>
  For the desktop version, please add the configuration to the existing configuration path without overwriting the path configuration generated during installation. Please back up the corresponding file before modification, so that you can restore it when you make a mistake.
</Warning>

Or you can refer to the default [extra\_model\_paths.yaml.example](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) for more configuration options. After saving, you need to **restart ComfyUI** for the changes to take effect.

Below is the original config example:

```yaml  theme={null}
#Rename this to extra_model_paths.yaml and ComfyUI will load it


#config for a1111 ui
#all you have to do is change the base_path to where yours is installed
a111:
    base_path: path/to/stable-diffusion-webui/

    checkpoints: models/Stable-diffusion
    configs: models/Stable-diffusion
    vae: models/VAE
    loras: |
         models/Lora
         models/LyCORIS
    upscale_models: |
                  models/ESRGAN
                  models/RealESRGAN
                  models/SwinIR
    embeddings: embeddings
    hypernetworks: models/hypernetworks
    controlnet: models/ControlNet

#config for comfyui
#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.

#comfyui:
#     base_path: path/to/comfyui/
#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads
#     #is_default: true
#     checkpoints: models/checkpoints/
#     clip: models/clip/
#     clip_vision: models/clip_vision/
#     configs: models/configs/
#     controlnet: models/controlnet/
#     diffusion_models: |
#                  models/diffusion_models
#                  models/unet
#     embeddings: models/embeddings/
#     loras: models/loras/
#     upscale_models: models/upscale_models/
#     vae: models/vae/

#other_ui:
#    base_path: path/to/ui
#    checkpoints: models/checkpoints
#    gligen: models/gligen
#    custom_nodes: path/custom_nodes

```

For example, if your WebUI is located at `D:\stable-diffusion-webui\`, you can modify the corresponding configuration to

```yaml  theme={null}
a111:
    base_path: D:\stable-diffusion-webui\
    checkpoints: models/Stable-diffusion
    configs: models/Stable-diffusion
    vae: models/VAE
    loras: |
         models/Lora
         models/LyCORIS
    upscale_models: |
                  models/ESRGAN
                  models/RealESRGAN
                  models/SwinIR
    embeddings: embeddings
    hypernetworks: models/hypernetworks
    controlnet: models/ControlNet
```

### Add Extra Custom Nodes Path

Besides adding external models, you can also add custom nodes paths that are not in the default path of ComfyUI

<Tip>
  Please note that this will not change the default installation path of custom nodes, but will add an extra path search when starting ComfyUI. You still need to complete the installation of custom node dependencies in the corresponding environment to ensure the integrity of the running environment.
</Tip>

Below is a simple configuration example (MacOS), please modify it according to your actual situation and add it to the corresponding configuration file, save it and restart ComfyUI for the changes to take effect:

```yaml  theme={null}
my_custom_nodes:
  custom_nodes: /Users/your_username/Documents/extra_custom_nodes
```

### File size

Models can be extremely large files relative to image files. A typical uncompressed image may require a few megabytes of disk storage. Generative AI models can be tens of thousands of times larger, up to tens of gigabytes per model. They take up a great deal of disk space and take a long time to transfer over a network.

## Model training and refinement

A generative AI model is created by training a machine learning program on a very large set of data, such as pairs of images and text descriptions. An AI model doesnt store the training data explicitly, but rather it stores the correlations that are implicit within the data.

Organizations and companies such as Stability AI and Black Forest Labs release base models that carry large amounts of generic information. These are general purpose generative AI models. Commonly, the base models need to be ***refined*** in order to get high quality generative outputs. A dedicated community of people work to refine the base models. The new, refined models produce better output, provide new or different functionality, and/or use fewer resources. Refined models can usually be run on systems with less computing power and/or memory.

## Auxiliary models

Model functionality can be extended with auxiliary models. For example, art directing a text-to-image workflow to achieve a specific result may be difficult or impossible using a diffusion model alone. Additional models can refine a diffusion model within the workflow graph to produce desired results. Examples include **LoRA** (Low Rank Adaptation), a small model that is trained on a specific subject; **ControlNet**, a model that helps control composition using a guide image; and **Inpainting**, a model that allows certain diffusion models to generate new content within an existing image.

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_auxiliary-model.png?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=0f49a29f61c23ccd32ec5a15207b8f9c" alt="auxiliary models" data-og-width="1920" width="1920" data-og-height="1080" height="1080" data-path="images/concepts/core-concepts_auxiliary-model.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_auxiliary-model.png?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=d34ac90d5809aaff3437a6be6760c350 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_auxiliary-model.png?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=d85c146bbf893f23854b57450b9df3f0 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_auxiliary-model.png?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=d9211a61fb606b80819d1be2a9e212e9 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_auxiliary-model.png?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=d390c38768b75c58e713aaef7a79551e 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_auxiliary-model.png?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=7f515f04cbddc512b6713faffa3291cb 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_auxiliary-model.png?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=f5d56a0a9add6dff1d1dcf2cb106c7ed 2500w" />

## Uninstalling models

ComfyUI does not currently support uninstalling models through the frontend interface. If you want to remove or uninstall models, you need to manually delete the corresponding model files from the `ComfyUI/models/` directory on your system.

## Common issues

<AccordionGroup>
  <Accordion title="Does ComfyUI support GGUF format models?">
    ComfyUI does not natively support GGUF format models. To use GGUF models, you need to install community custom nodes such as [ComfyUI-GGUF](https://github.com/city96/ComfyUI-GGUF).
  </Accordion>

  <Accordion title="Why can't I find my model?">
    If you've installed a model but can't find it in ComfyUI, try these steps:

    * Verify the model is in the correct location:
      * For **ComfyUI Desktop**: Go to **Help** menu  **Open Folder**  **Open Model Folder** to check the model installation path
      * Ensure your model file is placed in the correct subfolder (e.g., `checkpoints`, `loras`, `vae`)
    * Press the `r` key to refresh node definitions so ComfyUI can detect the model
    * Restart ComfyUI
    * Ensure the correct model is selected in the model loader node
  </Accordion>
</AccordionGroup>


# Nodes
Source: https://docs.comfy.org/development/core-concepts/nodes

Understand the concept of a node in ComfyUI.

In ComfyUI, nodes are the fundamental building blocks for executing tasks. Each node is an independently built module, whether it's a **Comfy Core** node or a **Custom Node**, with its own unique functionality. Nodes connect to each other through links, allowing us to build complex functionality like assembling LEGO blocks.
The combinations of different nodes create the unlimited possibilities of ComfyUI.

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/sampling/k_sampler.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=aed5e1f863c37948107cfcb3458955b7" alt="Comfy Core K-Sampler Node" data-og-width="854" width="854" data-og-height="767" height="767" data-path="images/comfy_core/sampling/k_sampler.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/sampling/k_sampler.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=f891fb24b7fcbb6616bad811d797cd10 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/sampling/k_sampler.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=d3597cfd9ef1b324ceb8b8e529246540 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/sampling/k_sampler.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=e714d81c3a3214ef9fe1da89cdc2efd3 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/sampling/k_sampler.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=5001918af1c739866639c03df51a3c19 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/sampling/k_sampler.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=ff62cab54c0c1fa1432d98c69877706d 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/sampling/k_sampler.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=350b8494ce4c1e004ddd94f7977563d7 2500w" />

For example, in the K-Sampler node, you can see it has multiple inputs and outputs, and also includes multiple parameter settings. These parameters determine the logic of node execution. Behind each node is well-written Python logic, allowing you to achieve corresponding functionality without having to write code yourself.

<Note>
  As ComfyUI is still in rapid iteration and development, we are continuously improving it every day. Therefore, some operations mentioned in this article may change or be omitted. Please refer to the actual interface. If you find changes in actual operations, it may be due to our iterative updates. You can also fork [this repo](https://github.com/Comfy-Org/docs) and help us improve this documentation.
</Note>

## Nodes perform operations

In computer science, a ***node*** is a container for information, usually including programmed instructions to perform some task. Nodes almost never exist in isolation, they're almost always connected to other nodes in a networked graph. In ComfyUI, nodes take the visual form of boxes that are connected to each other.

ComfyUI nodes are usually ***function operators***. This means that they operate on some data to perform a function. A function is a process that accepts input data, performs some operation on it, and produces output data. In other words, nodes do some work, contributing to the completion of a task such as generating an image. So ComfyUI nodes almost always have at least one input or output, and usually have multiple inputs and outputs.

## Different Node States

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/status.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=150ab91575a7c9300b7ed20d8026e499" alt="Node States" data-og-width="3167" width="3167" data-og-height="900" height="900" data-path="images/concepts/node/status.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/status.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=72ff0c9fc86e4de4abc041f54e9a76a4 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/status.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=ec6264e55b41fe6f284bb32ccf7e09cb 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/status.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=0964829012a38d6540d89773a163e5eb 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/status.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=0b013434ced278f2029243136f80198d 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/status.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=f4077f9da03b83ae9ee4a4d28cce19e7 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/status.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=68a03e96245d13250e0864a8951c2b7f 2500w" />

In ComfyUI, nodes have multiple states. Here are some common node states:

1. **Normal State**: The default state
2. **Running State**: The running state, typically displayed when a node is executing after you start running the workflow
3. **Error State**: Node error, typically displayed after running the workflow if there's a problem with the node's input, indicated by red marking of the erroneous input node. You need to fix the problematic input to ensure the workflow runs correctly
4. **Missing State**: This state usually appears after importing workflows, with two possibilities:
   * Comfy Core native node missing: This usually happens because ComfyUI has been updated, but you're using an older version of ComfyUI. You need to update ComfyUI to resolve this issue
   * Custom node missing: The workflow uses custom nodes developed by third-party authors, but your local ComfyUI version doesn't have these custom nodes installed. You can use [ComfyUI-Manager](https://github.com/Comfy-Org/ComfyUI-Manager) to find and install the missing custom nodes

## Connections Between Nodes

In ComfyUI, nodes are connected through [links](/development/core-concepts/links), allowing data of the same type to flow between different processing units to achieve the final result.

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/inpaint.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=40a62b27aa2f44cb97eae917c8c1f8da" alt="ComfyUI Node Links" data-og-width="2000" width="2000" data-og-height="1108" height="1108" data-path="images/concepts/node/inpaint.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/inpaint.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=c90df520b2046575f0d0005253cf359e 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/inpaint.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=7850db8dad1f1096fe1e7870b47f772b 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/inpaint.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=613be61533b2737be42795c6162d77b9 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/inpaint.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=4db8d8d46ec77ded6285a4939d8cb1cc 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/inpaint.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=76228d5a4063d7923d02fb718938bd71 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/inpaint.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=49f3f3318cf1e678bec7499f116c8065 2500w" />

Each node receives some input, processes it through its module, and converts it to corresponding output. Connections between different nodes must conform to the data type requirements. In ComfyUI, we use different colors to distinguish node data types. Below are some basic data types:

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/data_type.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=a2f5203bde87f24afff17ef3cf0bcaa5" alt="ComfyUI Node Data Types" data-og-width="685" width="685" data-og-height="356" height="356" data-path="images/concepts/node/data_type.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/data_type.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=3a128b7a0a1e6e93733f2fe649efd330 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/data_type.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=56df7925857870daf0995d69dcca1c24 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/data_type.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=69feb5a2bdde5813c9444c500710e1cb 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/data_type.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=087178033c73065ed5a521cb04f3f5cb 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/data_type.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=389aadb6b698921a58ffb6e3c65ce796 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/data_type.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=9ee271b46d36d4ab90b1688dbcf80c5d 2500w" />

| Data type                 | Color        |
| ------------------------- | ------------ |
| diffusion model           | lavender     |
| CLIP model                | yellow       |
| VAE model                 | rose         |
| conditioning              | orange       |
| latent image              | pink         |
| pixel image               | blue         |
| mask                      | green        |
| number (integer or float) | light green  |
| mesh                      | bright green |

As ComfyUI evolves, we may expand to more data types to meet the needs of more scenarios.

### Connecting and Disconnecting Nodes

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/link_nodes.gif?s=06359a20664f48e43763696313a063f9" alt="ComfyUI Node Connecting" data-og-width="820" width="820" data-og-height="724" height="724" data-path="images/concepts/node/link_nodes.gif" data-optimize="true" data-opv="3" />

**Connecting**: Drag from the output point of one node to the input of the same color on another node to connect them
**Disconnecting**: Click on the input endpoint and drag the mouse left button to disconnect, or cancel the connection through the midpoint menu of the link

## Node Appearance

<img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/index/node.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=e7ed881cf2c91cc9c48892dd491cae7b" alt="Node Appearance" data-og-width="950" width="950" data-og-height="560" height="560" data-path="images/index/node.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/index/node.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=0c7f3950359535f427c0b23ed36b99f7 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/index/node.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=0b937677a863b6fefd2d7a68e60c012f 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/index/node.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=a9c398582e0b5bf75033405e469dda22 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/index/node.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=5f05d24d5e77ccb4d050706792186d25 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/index/node.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=828de922e8e1f8816d3d8a674304abe5 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/index/node.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=33297be56d4722e5b5e8b5462129ec7e 2500w" />

We provide various style settings for you to customize the appearance of nodes:

* Modify styles
* Double-click the node title to modify the node name
* Switch node inputs between input sockets and widgets through the context menu
* Resize the node using the bottom right corner

<video controls className="w-full aspect-video" src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/node_appearance.mp4?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=2fb1ff4aac3396cb3e4ecbebd8db7b14" data-path="images/concepts/node/node_appearance.mp4" />

### Node Badges

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/badge.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=1e22b62c50745c547e2b99c5f13df990" alt="Node Badges" data-og-width="1207" width="1207" data-og-height="606" height="606" data-path="images/concepts/node/badge.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/badge.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=5725996ab758f6ca7476c8bc3c68b58a 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/badge.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=dec77789c181bb56a23c8d8ede919ced 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/badge.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=fc1a4671387cc7ffceb4ae130f543dac 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/badge.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=cb938d3aebd43c9d9db64ec629d07781 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/badge.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=4dfa69ff4270496cacc3a9843d4fc841 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/badge.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=17a8ec9f470311a50cbcd2c0bfa36ac1 2500w" />

We provide multiple node badge display features, such as:

* Node ID
* Node source

Currently, **Comfy Core nodes** use a fox icon for display, while custom nodes use their names. This way you can quickly understand which node package a node comes from.

You can set the corresponding display in the menu:

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/badge_setting.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=f63c9698836a1debc6a1bed84d77fdf2" alt="Badge Settings" data-og-width="1500" width="1500" data-og-height="561" height="561" data-path="images/concepts/node/badge_setting.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/badge_setting.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=2ff04102c6062521c93d30324c921e61 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/badge_setting.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=bb5156ae606e2844b6aa118c09b962aa 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/badge_setting.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=343b00bd89bff5e04ecef00e0f999b36 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/badge_setting.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=090801a432d756d73ece2379d0339f19 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/badge_setting.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=6a7f9adec98fca0dff194b2cc8db9314 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/badge_setting.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=c9f0fc340c48d63b80f7a66d6956a4f1 2500w" />

## Node Context Menus

Node context menus are mainly divided into two types:

* Context menu for the node itself
* Context menu for inputs/outputs

### Node Context Menu

By right-clicking on a node, you can expand the corresponding node context menu:
<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/context_menus_1.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=d7d99664537a4f2b09aaf0acc228da4f" alt="Node Context Menu" data-og-width="2100" width="2100" data-og-height="1832" height="1832" data-path="images/concepts/node/context_menus_1.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/context_menus_1.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=f6d35bba454f50422ed60bd36f7d0ffc 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/context_menus_1.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=961e5d91a9363fe91db3c28196f58052 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/context_menus_1.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=bd3ca784987ef4c4931229c920c0239d 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/context_menus_1.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=a544cec80957d1cb035197c4edebf57e 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/context_menus_1.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=4a8684c82442b8ed435fbbd715c3963e 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/context_menus_1.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=d2ffc3276a6e340a5e92a7d32e697ff2 2500w" />

In the node's right-click context menu, you can:

* Adjust the node's color style
* Modify the title
* Clone, copy, or delete the node
* Set the node's mode

In this menu, besides appearance-related settings, the following menu operations are important:

* **Mode**: Set the node's mode: Always, Never, Bypass
* **Toggle between Widget and Input mode for node inputs**: Switch between widget and input mode for node inputs

#### Mode

For modes, you may notice that we currently provide: Always, Never, On Event, On Trigger - four modes, but actually only **Always** and **Never** are effective. **On Event** and **On Trigger** are currently ineffective as we haven't fully implemented this feature. Additionally, you can understand **Bypass** as a mode. Below is an explanation of the available modes:

* **Always**: The default node mode. The node will execute whenever it runs for the first time or when any of its inputs change since the last execution
* **Never**: The node will never execute under any circumstances, as if it's been deleted. Subsequent nodes cannot read or receive any data from it
* **Bypass**: The node will never execute under any circumstances, but subsequent nodes can still try to obtain data that hasn't been processed by this node

Below is a comparison of the `Never` and `Bypass` modes:

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/never_vs_bypass.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=7024037e0a22285a21484d102dae0f0c" alt="Never vs Bypass Mode" data-og-width="3000" width="3000" data-og-height="1092" height="1092" data-path="images/concepts/node/never_vs_bypass.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/never_vs_bypass.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=6344fa2d9cd016f427b02c74e1cf1740 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/never_vs_bypass.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=ea96796a2e23fef29363b065df5a16a9 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/never_vs_bypass.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=aba3e1a0b57114dd729dc8c96e0adb05 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/never_vs_bypass.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=fb4dd9dd8030ce2f4e789056955beaac 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/never_vs_bypass.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=011c5d6f09f51e031c53488b0580db07 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/never_vs_bypass.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=5a6a4976bfd7ee42816e6d8b852c9274 2500w" />

In this comparison example, you can see that both workflows apply two LoRA models simultaneously, with the difference being that one `Load LoRA` node is set to `Never` mode while the other is set to `Bypass` mode.

* The node set to `Never` mode causes subsequent nodes to show errors because they don't receive any input data
* The node set to `Bypass` mode still allows subsequent nodes to receive unprocessed data, so they load the output data from the first `Load LoRA` node, allowing the subsequent workflow to continue running normally

#### Switching Between Widget and Input Mode for Node Inputs

In some cases, we need to use output results from other nodes as input. In this case, we can switch between widget and input mode for node inputs.

Here's a very simple example:

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/switch_widget.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=de80d70f869e562667135ebc71f2e91c" alt="Switch Widget and Input Mode" data-og-width="1500" width="1500" data-og-height="694" height="694" data-path="images/concepts/node/switch_widget.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/switch_widget.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=5e8061f8deb6078ff5b7f871021752cd 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/switch_widget.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=2a556df1052139a92c4e9bfb431ece85 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/switch_widget.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=f542d9157f6862975fa0900cc9cc6e3f 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/switch_widget.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=e0017f836d9a581ac9dd7c0028cbbc3a 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/switch_widget.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=79bc8b9f5c8ede61d61c896cfbed4950 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/switch_widget.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=139f05101a1b3f0682796d2ed099a194 2500w" />

By switching the K-Sampler's Seed from widget to input mode, multiple nodes can share the same seed, achieving variable uniformity across multiple samplers.
Comparing the first node with the subsequent two nodes, you can see that the seed in the latter two nodes is in input mode. You can also convert it back to widget mode:

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/convert_input.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=4be72fd7768910bfc0c8a76196b1c39b" alt="Convert Input Mode" data-og-width="1000" width="1000" data-og-height="1112" height="1112" data-path="images/concepts/node/convert_input.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/convert_input.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=42ff0a1cb530749966011c75fba0bbe0 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/convert_input.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=f876e44bf7334acbae177a4af9e156ae 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/convert_input.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=8459338ee2f090888f5dbc788303520d 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/convert_input.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=98d5cf040acd8c74303bead8e9a9bb2b 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/convert_input.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=0a4a34447189cac5c03c491e321b17b0 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/convert_input.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=de6c19d1632929a2102cfb87cecf7edb 2500w" />

<Note>
  After frontend version v1.16.0, we improved this feature. Now you only need to directly connect the input line to the corresponding widget to complete this process
  <blockquote class="twitter-tweet"><p lang="en" dir="ltr">Say goodbye to annoying widget \<> socket conversion starting from frontend version v1.16.0! Now each widget just always have an associated input socket by default <a href="https://twitter.com/hashtag/ComfyUI?src=hash&ref_src=twsrc%5Etfw">#ComfyUI</a> <a href="https://t.co/sP9HHKyGYW">pic.twitter.com/sP9HHKyGYW</a></p> Chenlei Hu (@HclHno3) <a href="https://twitter.com/HclHno3/status/1909059259536375961?ref_src=twsrc%5Etfw">April 7, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8" />
</Note>

### Input/Output Context Menu

This context menu is mainly related to the data type of the corresponding input/output:

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/context_menus_2.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=2daf16654cdd151a6b9ace6ef9d9e530" alt="Node Input/Output Context Menu" data-og-width="1774" width="1774" data-og-height="910" height="910" data-path="images/concepts/node/context_menus_2.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/context_menus_2.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=21205850f45887caf0f440de0aef9e19 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/context_menus_2.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=a7037a646b1abadb3a8a03cf99b57f53 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/context_menus_2.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=0cfefc04ce317796d93a1d9c52f00d27 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/context_menus_2.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=0c92cbeef4b1beabd4105cd0872a3ae0 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/context_menus_2.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=34f6d22e3d1dcb60697ccd4b811e3687 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/context_menus_2.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=ece7fca06a344e909a62444d253abfa2 2500w" />

When dragging the input/output of a node, if a connection appears but you haven't connected to another node's input or output, releasing the mouse will pop up a context menu for the input/output, used to quickly add related types of nodes.
You can adjust the number of node suggestions in the settings:

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/node_suggestions.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=7a063583df5825a8cc24dc13a1723761" alt="Node Suggestion Count" data-og-width="1000" width="1000" data-og-height="314" height="314" data-path="images/concepts/node/node_suggestions.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/node_suggestions.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=d407262970bad97799d5105e1bc5cea5 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/node_suggestions.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=7bd3a3e2e2e988603d228393390781b0 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/node_suggestions.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=9bbb3101a2655835a2b26d19295f0e81 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/node_suggestions.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=f40f2b2e28af6a96ead52901a9392ed9 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/node_suggestions.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=7baf1d9a57d19a254b9c0675f6644c3e 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/node_suggestions.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=07d79a1ffdbcf13836aa9c11eafe1942 2500w" />

## Node Selection Toolbox

<video controls className="w-full aspect-video" src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/selection_toolbox.mp4?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=f2a55de5682187a2e9e012120b968c95" data-path="images/concepts/node/selection_toolbox.mp4" />

The **Node Selection Toolbox** is a floating tool that provides quick operations for nodes. When you select a node, it hovers above the selected node. Through this toolbox, you can:

* Change the node's color
* Quickly set the node to Bypass mode (not execute during runtime)
* Lock the node
* Delete the node

Of course, these functions can also be found in the right-click menu of the corresponding node. The node selection toolbox just provides a shortcut operation. If you want to disable this feature, you can turn it off in the settings.

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/setting_selection_toolbox.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=60747b0e83640a2f7a4f1bc66e63a89c" alt="Disable Node Selection Toolbox" data-og-width="1067" width="1067" data-og-height="324" height="324" data-path="images/concepts/node/setting_selection_toolbox.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/setting_selection_toolbox.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=ded2a8d85c75a7b282e344be55a9a975 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/setting_selection_toolbox.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=e862be9b2185ba613587c8f8e3c34104 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/setting_selection_toolbox.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=beed142e0d37962809b27c53f98602bf 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/setting_selection_toolbox.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=6efc4f19b0b54b2daddc9c735d0ec36d 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/setting_selection_toolbox.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=d2733538bfb0d2749c5f0ea5c7ebdba2 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/concepts/node/setting_selection_toolbox.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=2b1db6a68c326e2aa3d069f34b1e0da2 2500w" />

## Node Groups

In ComfyUI, you can select multiple parts of a workflow simultaneously, then use the right-click menu to merge them into a node group, making that part a reusable module that can be repeatedly called in your ComfyUI.

## Custom Nodes

ComfyUI includes many powerful nodes in the base installation package. These are known as **Comfy Core** nodes. Additionally, the ComfyUI community has created an amazing array of [***custom nodes***](https://registry.comfy.org) to perform a wide variety of functions.

## ComfyUI Manager

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_nodes_manager.png?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=cea0de500828224b23b64cef84a31936" alt="ComfyUI Manager interface" data-og-width="1920" width="1920" data-og-height="1080" height="1080" data-path="images/concepts/core-concepts_nodes_manager.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_nodes_manager.png?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=b66fffe074ba15068c2868e5e127cc41 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_nodes_manager.png?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=951aa7941493ea56018c7aa77d1bc143 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_nodes_manager.png?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=93e156a11636d64258510480fe1c28c2 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_nodes_manager.png?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=352d680f30f32e457031be3c6de987d9 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_nodes_manager.png?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=a034b1663b608f4d64c6a690ea652b88 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_nodes_manager.png?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=4ca79e9090f1ed2513db42fa9838af40 2500w" />

The **ComfyUI Manager** window makes it easy to perform custom node management tasks such as search, install, update, disable, and uninstall. The Manager is included in the ComfyUI desktop application, but not in the ComfyUI server application.

### Installing the ComfyUI Manager

If you're running the ComfyUI server application, you need to install the Manager. If ComfyUI is running, shut it down before proceeding.

The first step is to install Git, a command-line application for software version control. Git will download the ComfyUI Manager from [github.com](https://github.com). Download Git from [git-scm.com](https://git-scm.com/) and install it.

Once Git is installed, navigate to the ComfyUI server program directory, to the folder labeled **custom\_nodes**. Open up a command window or terminal. Make sure that the command line displays the current directory path as **custom\_nodes**. Enter the following command. This will download the Manager. Technically, this is known as *cloning a Git repository*.

```bash  theme={null}
git clone https://github.com/ltdrdata/ComfyUI-Manager.git
```

For details or special cases, see [ComfyUI Manager Install](https://github.com/ltdrdata/ComfyUI-Manager?tab=readme-ov-file#installation).


# Properties
Source: https://docs.comfy.org/development/core-concepts/properties



## Nodes are containers for properties

Nodes usually have ***properties***. Also known as ***parameters*** or ***attributes***, node properties are variables that can be changed. Some properties can be adjusted manually by the user, using a data entry field called a ***widget***. Other properties can be driven automatically by other nodes connected to the property ***input slot*** or port. Usually, a property can be converted from widget to input and vice versa, allowing users to control property values manually or automatically.

Properties can take many forms and hold many different types of information. For example, a **Load Checkpoint** node has a single property: the file path to the generative model checkpoint file. A **KSampler** node has multiple properties such as the number of sampling **steps**, **CFG** scale, **sampler\_name**, etc.

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_properties.png?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=2121d646aef4f1386f1f09162312785b" alt="node properties" data-og-width="1920" width="1920" data-og-height="1080" height="1080" data-path="images/concepts/core-concepts_properties.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_properties.png?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=2cefc8007c7898e676ff75bf00c6ba92 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_properties.png?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=14dfbe18d40d717bdd05b708f9a8510a 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_properties.png?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=a1c89860f0b232aea917a1ce47b61d2f 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_properties.png?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=f1ba7ae145baeba3b8bed3b62ba6afa0 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_properties.png?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=bf77af55bac39bb412df4cac93054fd9 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/concepts/core-concepts_properties.png?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=ed41455e1d24854be6db7dcec66f09d2 2500w" />

## Data types

Information can come in many different forms, called ***data types***. For example, alphanumeric text is known as a ***string***, a whole number is an ***integer***, and a number with a decimal point is known as a ***floating point*** number or ***float***. New data types are always being added to ComfyUI.

ComfyUI is written in the Python scripting language, which is very forgiving about data types. By contrast, the ComfyUI environment is very ***strongly typed***. This means that different data types cant be mixed up. For example, we cant connect an image output to an integer input. This is a huge benefit to users, guiding them to proper workflow construction and preventing program errors.


# Workflow
Source: https://docs.comfy.org/development/core-concepts/workflow



## A graph of nodes

ComfyUI is an environment for building and running generative content ***workflows***. In this context, a workflow is defined as a collection of program objects called ***nodes*** that are connected to each other, forming a network. This network is also known as a ***graph***.

A ComfyUI workflow can generate any type of media: image, video, audio, AI model, AI agent, and so on.

## Sample workflows

To get started, try out some of the [official workflows](https://comfyanonymous.github.io/ComfyUI_examples). These use only the Core nodes included in the ComfyUI installation. A thriving community of developers has created a rich [ecosystem](https://registry.comfy.org) of custom nodes to extend the functionality of ComfyUI.

### Simple Example

<img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/simple_workflow.jpeg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=60028deaf5c13f399515ca2027f680e6" alt="simple workflow" data-og-width="1400" width="1400" data-og-height="637" height="637" data-path="images/simple_workflow.jpeg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/simple_workflow.jpeg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=44b4c2822d777030f13340f9076c9e41 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/simple_workflow.jpeg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=17c74acde982dffc3890f4d4d0781bf8 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/simple_workflow.jpeg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=6c44653c323146e9578e277dca28d6bd 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/simple_workflow.jpeg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=867c4d06c3510d67daeddff1251a47ea 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/simple_workflow.jpeg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=3fad15ddc2e7c040f2b444fe2b62849a 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/simple_workflow.jpeg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=a23372305e8c14c8797350f56ef29078 2500w" />

## Visual programming

A node-based computer program like ComfyUI provides a level of power and flexibility that cant be achieved with traditional menu- and button-driven applications. The ComfyUI node graph is not limited by the tools provided in a traditional computer application. Its a high-level ***visual programming environment*** allowing users to design complex systems without needing to write program code or understand advanced mathematics.

Many other computer applications use this same node graph paradigm. Examples include the compositing application called Nuke, the 3D programs Maya and Blender, the Unreal real-time graphics engine, and the interactive media authoring program called Max.

### More Complex Example

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/complex_workflow.jpeg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=72268aab0bf1c70be6388930d50aa983" alt="complex workflow" data-og-width="1000" width="1000" data-og-height="511" height="511" data-path="images/complex_workflow.jpeg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/complex_workflow.jpeg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=9efef3c1cfc3f6987ee96b766e13e5f3 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/complex_workflow.jpeg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=c0b199f01cfd67377ee9b38c361764a3 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/complex_workflow.jpeg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=6a8866c954c07678fc29dbe1748f61cf 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/complex_workflow.jpeg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=a7c7ec3f73860b7d36c41889b65911cb 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/complex_workflow.jpeg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=b800e44f63e64f1346cb72b60b51864b 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/complex_workflow.jpeg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=c5559937ae61c95c9e025977585b8a68 2500w" />

## Procedural framework

Another term used to describe a node-based application is ***procedural framework***. Procedural means generative: some procedure or algorithm is employed to generate content such as a 3D model or a musical composition.

ComfyUI is all of these things: a node graph, a visual programming environment, and a procedural framework. What makes ComfyUI different (and amazing!) is that its radically open structure allows us to generate any type of media asset such as picture, movie, sound, 3D model, AI model, etc.

In the context of ComfyUI, the term ***workflow*** is a synonym for the node network or graph. It corresponds to the ***scene graph*** in a 3D or multimedia program: the network of all of the nodes within a particular disk file. 3D programs call this a ***scene file***. Video editing, compositing, and multimedia programs usually call it a ***project file***.

## Saving workflows

The ComfyUI workflow is automatically saved in the metadata of any generated image, allowing users to open and use the graph that generated the image. A workflow can also be stored in a human-readable text file that follows the JSON data format. This is necessary for media formats that dont support metadata. ComfyUI workflows stored as JSON files are very small, allowing convenient versioning, archiving, and sharing of graphs, independently of any generated media.


# Comfy Cloud
Source: https://docs.comfy.org/get_started/cloud

Get started with Comfy Cloud to run ComfyUI workflows in the cloud without local installation

<Card title="Access Comfy Cloud" icon="cloud" href="https://comfy.org/cloud">
  Click here to access ComfyUI Cloud directly
</Card>

## What is Comfy Cloud?

ComfyUI Cloud is the cloud version of ComfyUI with the same features as the local version. Everything is pre-installed and ready to use.

### Key features

<CardGroup cols={2}>
  <Card title="Zero setup" icon="bolt">
    No installation required. All models and custom nodes are pre-installed and ready to use
  </Card>

  <Card title="Powerful GPUs" icon="microchip">
    Run workflows fast on our powerful server GPUs without needing your own hardware
  </Card>

  <Card title="Always up-to-date" icon="arrows-rotate">
    Automatically stays current with the latest ComfyUI releases and features
  </Card>

  <Card title="Access anywhere" icon="globe">
    Use ComfyUI from any device with an internet connection - no local installation needed
  </Card>
</CardGroup>

## Cloud vs local

ComfyUI offers both an official cloud version, [Comfy Cloud](https://comfy.org/cloud), and an open-source self-hosted version. If you have a powerful GPU, running ComfyUI locally is a great option. The cloud version, on the other hand, is an online service that's ready to use instantlysimply open the URL, no installation or setup required.

| Category                | Comfy Cloud                                                                | Self-hosted (local ComfyUI)                                                                                                                        |
| ----------------------- | -------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Cost**                | Monthly Subscription                                                       | Free                                                                                                                                               |
| **GPU**                 | Powerful A100s with 40GB+ VRAM                                             | Bring your own GPU                                                                                                                                 |
| **Technical Knowledge** | No technical knowledge required.                                           | While desktop and portable give you easy ways to get started, you'll need to troubleshoot custom node installations and local installation issues. |
| **Custom Nodes**        | Use pre-installed custom nodes and never worry about compatibility issues. | Install any custom node you want, but you'll need to manage it yourself.                                                                           |
| **Models**              | Use pre-installed models. Upload your own models (coming soon).            | Use any models you want, but you'll need to download them first.                                                                                   |
| **Notable Differences** | Easy to onboard your team                                                  | Works offline, infinitely customizable                                                                                                             |
| **Get started**         | [Run ComfyUI Cloud](https://comfy.org/cloud)                               | [Install ComfyUI locally](/installation/system_requirements)                                                                                       |

## Pricing and subscription

<Card title="Check pricing" icon="tag" href="https://www.comfy.org/cloud/pricing">
  View pricing and subscription options for Comfy Cloud
</Card>

## How to use ComfyUI Cloud

Using ComfyUI Cloud is essentially the same as using your local ComfyUI. If this is your first time using ComfyUI, here are some quick tips to get started:

<Steps>
  <Step title="Select a template">
    Click the template icon in the left sidebar to browse available workflows.
    <img src="https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/open_template.webp?fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=a32875bb4e6d1c92ea4f893831aff925" alt="Open workflow template" data-og-width="2048" width="2048" data-og-height="1128" height="1128" data-path="images/cloud/open_template.webp" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/open_template.webp?w=280&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=b75636f8fdf033185ad180f9b51b5ec2 280w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/open_template.webp?w=560&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=acb56caaddbd17845f65266d0d3a8e17 560w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/open_template.webp?w=840&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=d0c30a7b485b6d96dc64c32b6d70b7f2 840w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/open_template.webp?w=1100&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=9aea324f8357077c9f0b64919e9172e4 1100w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/open_template.webp?w=1650&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=9c44447bb6c57690bf49b1a2f5d258ae 1650w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/open_template.webp?w=2500&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=f3cf92bb13bc5f400effe170ec118bbc 2500w" />
  </Step>

  <Step title="Select the workflow you want to run">
    Click on the template you want to run.
    <img src="https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/template_library.webp?fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=b16043c4ad9372834eb7fe49c1180eee" alt="Select workflow template" data-og-width="2048" width="2048" data-og-height="1136" height="1136" data-path="images/cloud/template_library.webp" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/template_library.webp?w=280&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=a398d81d88d88f5cd9ebae8919f925e0 280w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/template_library.webp?w=560&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=ca97b03eeb15e84605e74d7716724be4 560w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/template_library.webp?w=840&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=75accea5eb08cd6e8dd56d7c8df473f6 840w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/template_library.webp?w=1100&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=837933b6b1a09f75fbb28b2483173898 1100w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/template_library.webp?w=1650&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=bb02cd10b5a2212349830369205e56c7 1650w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/template_library.webp?w=2500&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=66050ae24580d3190d340aa3a38021bd 2500w" />
  </Step>

  <Step title="Update template inputs">
    1. In the loaded workflow, since we have pre-installed all models in the cloud version, all templates are ready to run immediately after loading.
    2. If you need to make updates, you only need to provide image inputs or text prompts. Check the `Load Image` node or `CLIP Text Encode` node.
       <img src="https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/workflow_input_or_loader.webp?fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=56ca79d24c79e666146c927263c96e99" alt="Check inputs" data-og-width="2048" width="2048" data-og-height="1128" height="1128" data-path="images/cloud/workflow_input_or_loader.webp" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/workflow_input_or_loader.webp?w=280&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=127cf8e3644b55f1849aeac06f1b1400 280w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/workflow_input_or_loader.webp?w=560&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=458b4a5311110ba33ae6006f7dd8ac94 560w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/workflow_input_or_loader.webp?w=840&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=69e518f7eb1fd1370ce3059420e823b1 840w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/workflow_input_or_loader.webp?w=1100&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=32d8e4f15f1e6e9c39692b09ad2e8c16 1100w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/workflow_input_or_loader.webp?w=1650&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=1c916d45a5280a5b682fef62adefc8dd 1650w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/workflow_input_or_loader.webp?w=2500&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=8550b03cc2fdf3f2bde1b922e31d248e 2500w" />
  </Step>

  <Step title="Run your workflow">
    If everything is correct, click the "Run" icon or use the shortcut "Ctrl + Enter" to run the workflow.
    <img src="https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/workflow_run_workflow.webp?fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=546f1e0003941deba2006a36d8f9ed34" alt="Run workflow" data-og-width="2048" width="2048" data-og-height="1441" height="1441" data-path="images/cloud/workflow_run_workflow.webp" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/workflow_run_workflow.webp?w=280&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=b10e443657cecde2e6ccd70e5ffd55fc 280w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/workflow_run_workflow.webp?w=560&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=17fd7d78a06d27eed5ba9a7a2bc703e1 560w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/workflow_run_workflow.webp?w=840&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=6e35df195b3167470f2cf9baa63ea9a5 840w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/workflow_run_workflow.webp?w=1100&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=5d9cee59ce80d28cddbe56b6dcfc999a 1100w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/workflow_run_workflow.webp?w=1650&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=fde6f5f8ea2f5dc7d9d12ee8c1092855 1650w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/workflow_run_workflow.webp?w=2500&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=511cfe76f48a42ab357d5fec2a327b54 2500w" />
  </Step>

  <Step title="View output">
    After clicking run, our service will start allocating a machine for your workflow. You can check the execution status of the corresponding workflow in the queue panel.
    <img src="https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/workflow_check_queue.webp?fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=4ac366407cf13fa84e55ef7948cff36e" alt="View queue" data-og-width="1812" width="1812" data-og-height="1246" height="1246" data-path="images/cloud/workflow_check_queue.webp" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/workflow_check_queue.webp?w=280&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=8b15c9935fbbc177171cccf5f9b34153 280w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/workflow_check_queue.webp?w=560&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=f263581788c40988b061499eff933dc3 560w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/workflow_check_queue.webp?w=840&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=1f4c89d2d7d2b329080ad9e7a63fec03 840w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/workflow_check_queue.webp?w=1100&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=57df498441c1f8a8cd04981740e9429e 1100w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/workflow_check_queue.webp?w=1650&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=395585423ef0ad04a17c905f5747582f 1650w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/workflow_check_queue.webp?w=2500&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=67aba03e595f9badd10b3e3dfbd2d2f9 2500w" />
  </Step>

  <Step title="Save content locally">
    After the workflow execution is complete, you can save the generated content locally. Depending on the asset type, the save method is as follows:

    <Tabs>
      <Tab title="Save images">
        In the queue panel or on the save image node, right-click on the generated image and select "Save image" to save the image locally.

                <img src="https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/download_image_assets.jpg?fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=a15e70a3649537fb8c6adc50689dfbe7" alt="Save image assets" data-og-width="2444" width="2444" data-og-height="1176" height="1176" data-path="images/cloud/download_image_assets.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/download_image_assets.jpg?w=280&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=3d38181af7c253a663272638ceb88fd3 280w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/download_image_assets.jpg?w=560&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=55ebb3096557f7f6f7a15608d970f536 560w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/download_image_assets.jpg?w=840&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=afc78f0861b8fbfadb2a38a6ed06152a 840w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/download_image_assets.jpg?w=1100&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=a7a7674d618a5dad86d174024ad3d5b0 1100w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/download_image_assets.jpg?w=1650&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=7ec781932bde423f3b6933b29c2c4b6e 1650w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/download_image_assets.jpg?w=2500&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=da0cdccadf8614d70fa2c77dbedbd33d 2500w" />
      </Tab>

      <Tab title="Save videos">
        In the queue panel or on the save video node:

        1. Click the three dots on the browser player component to open the menu
        2. Select "Download" to save the video locally.

                <img src="https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/download_video_assets.webp?fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=2c987025c1c25e954a3065775ef5223d" alt="Save video assets" data-og-width="2404" width="2404" data-og-height="1306" height="1306" data-path="images/cloud/download_video_assets.webp" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/download_video_assets.webp?w=280&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=33b5d524c89458932e4eb812fb79b45e 280w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/download_video_assets.webp?w=560&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=f772e685ed03b0de2933c7efc40a4a39 560w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/download_video_assets.webp?w=840&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=54c85dee44a6dcaa1e97e2a027948c11 840w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/download_video_assets.webp?w=1100&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=0740c5be44110eb871a002f774877369 1100w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/download_video_assets.webp?w=1650&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=6139c2dad232b504cb1a3a98dd24cd48 1650w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/download_video_assets.webp?w=2500&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=e0c7c0e5e330a2295c92991214d67b91 2500w" />
      </Tab>

      <Tab title="Save audio">
        In the queue panel or on the save audio node:

        1. Click the three dots on the browser player component to open the menu
        2. Select "Download" to save the audio file locally.

                <img src="https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/download_audio_assets.webp?fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=8d876d9c3b89b2ae9f1e84ba8b3e9887" alt="Save audio assets" data-og-width="2416" width="2416" data-og-height="1308" height="1308" data-path="images/cloud/download_audio_assets.webp" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/download_audio_assets.webp?w=280&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=813ddfd43e25c7636c586e220e60d342 280w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/download_audio_assets.webp?w=560&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=b32f5d499050c6551579a63c1f9924d8 560w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/download_audio_assets.webp?w=840&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=bb56a92a4e287f40a707a6b3f8c75eb4 840w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/download_audio_assets.webp?w=1100&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=1fd00394970dcff4d7a2aa6e965a93a1 1100w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/download_audio_assets.webp?w=1650&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=ca02dfe4f3af5aadff45be0405973564 1650w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/download_audio_assets.webp?w=2500&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=1f884d11d52e14f721189989af4bc467 2500w" />
      </Tab>

      <Tab title="Save 3D assets">
        In the 3D browser node menu, select the "Export" option, choose the format you want to save, and the 3D file will be saved locally.

                <img src="https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/download_3d_assets.webp?fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=5e121107826658a3e1725b8d9840d94b" alt="Save 3D assets" data-og-width="4392" width="4392" data-og-height="2752" height="2752" data-path="images/cloud/download_3d_assets.webp" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/download_3d_assets.webp?w=280&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=e4c87feee21051d4628a277017a47447 280w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/download_3d_assets.webp?w=560&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=6dd17940bde85a544e06f9ce26eacebe 560w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/download_3d_assets.webp?w=840&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=c6fd15ca374ae15f02dac542da09d54d 840w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/download_3d_assets.webp?w=1100&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=29d653a4188a9d6f1c4e9164f21acb1c 1100w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/download_3d_assets.webp?w=1650&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=cc994df8c991db973ff8d1f1a7fb3a00 1650w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/download_3d_assets.webp?w=2500&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=75841e7056186b759422dbb281036c16 2500w" />
      </Tab>
    </Tabs>
  </Step>
</Steps>

## Feedback

If you have any thoughts, suggestions, or run into any issues, simply click the "Feedback" icon. This will directly send your feedback to us.

<img src="https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/feedback.webp?fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=f1306e3587e441fc9964ffc0f9e4cc73" alt="Feedback" data-og-width="1844" width="1844" data-og-height="1340" height="1340" data-path="images/cloud/feedback.webp" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/feedback.webp?w=280&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=7f1e8d9591e347e7cd75af2ea6e0804c 280w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/feedback.webp?w=560&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=19d86ae7c41ed6b8fdf1b1fb44834d9f 560w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/feedback.webp?w=840&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=36aa74fc89209a6413af918b956d505c 840w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/feedback.webp?w=1100&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=ece35a7b92f63b9074848cf1b01e6180 1100w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/feedback.webp?w=1650&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=5d37a906c10197a952f114dda1e2b689 1650w, https://mintcdn.com/dripart/dExtXAUR2XogD5qH/images/cloud/feedback.webp?w=2500&fit=max&auto=format&n=dExtXAUR2XogD5qH&q=85&s=740b8fd5f788a3522d33d57b047e9497 2500w" />

## Frequently Asked Questions

<Card title="View FAQs" icon="circle-question" href="https://comfy.org/cloud">
  View frequently asked questions and answers about Comfy Cloud, including pricing, features, limitations, and more
</Card>

## Next steps

<CardGroup cols={2}>
  <Card title="Tutorials" icon="book" href="/tutorials/basic/text-to-image">
    Explore tutorials to learn ComfyUI workflows
  </Card>

  <Card title="Support" icon="life-ring" href="https://discord.com/invite/comfyorg">
    Join our Discord community for help
  </Card>
</CardGroup>


# Getting Started with AI Image Generation
Source: https://docs.comfy.org/get_started/first_generation

This tutorial will guide you through your first image generation with ComfyUI, covering basic interface operations like workflow loading, model installation, and image generation

This guide aims to help you understand ComfyUI's basic operations and complete your first image generation. We'll cover:

1. Loading example workflows
   * Loading from ComfyUI's workflow templates
   * Loading from images with workflow metadata
2. Model installation guidance
   * Automatic model installation
   * Manual model installation
   * Using ComfyUI Manager for model installation
3. Completing your first text-to-image generation

## About Text-to-Image

Text-to-Image is a fundamental AI drawing feature that generates images from text descriptions. It's one of the most commonly used functions in AI art generation. You can think of the process as telling your requirements (positive and negative prompts) to an artist (the drawing model), who will then create what you want. Detailed explanations about text-to-image will be covered in the [Text to Image](/tutorials/basic/text-to-image) chapter.

## ComfyUI Text-to-Image Workflow Tutorial

### 1. Launch ComfyUI

Make sure you've followed the [installation guide](/installation/system_requirements) to start ComfyUI and can successfully enter the ComfyUI interface. Alternatively, you can use [Comfy Cloud](/get_started/cloud) to use ComfyUI without any installation.

<img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/comfyui-boot-screen.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=8d74bbaaf16e92f0e41fbf446bce8655" alt="ComfyUI Interface" data-og-width="1068" width="1068" data-og-height="819" height="819" data-path="images/interface/comfyui-boot-screen.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/comfyui-boot-screen.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=5cb9199e3321fff7ad0f3d05b98b5fb0 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/comfyui-boot-screen.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=d8079eb6460f02357a19daf916789cca 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/comfyui-boot-screen.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=8f430035873b1af54fbefe18ccb7f89f 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/comfyui-boot-screen.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=901fc46444d049a7c309f3d3dde04168 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/comfyui-boot-screen.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=74ef473885ed787270cdba6b753ec96b 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/comfyui-boot-screen.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=eb273ff76ed49bc91d290de3db733468 2500w" />

If you have not installed ComfyUI, please choose a suitable version to install based on your device.

<AccordionGroup>
  <Accordion title="ComfyUI Desktop">
    ComfyUI Desktop currently supports standalone installation for **Windows and MacOS (ARM)**, currently in Beta

    * Code is open source on [Github](https://github.com/Comfy-Org/desktop)

    <Tip>
      Because Desktop is always built based on the **stable release**, so the latest updates may take some time to experience for Desktop, if you want to always experience the latest version, please use the portable version or manual installation
    </Tip>

    You can choose the appropriate installation for your system and hardware below

    <Tabs>
      <Tab title="Windows">
        <Card title="ComfyUI Desktop (Windows) Installation Guide" icon="link" href="/installation/desktop/windows">
          Suitable for **Windows** version with **Nvidia** GPU
        </Card>
      </Tab>

      <Tab title="MacOS(Apple Silicon)">
        <Card title="ComfyUI Desktop (MacOS) Installation Guide" icon="link" href="/installation/desktop/macos">
          Suitable for MacOS with **Apple Silicon**
        </Card>
      </Tab>

      <Tab title="Linux">
        <Note>ComfyUI Desktop **currently has no Linux prebuilds**, please visit the [Manual Installation](/installation/manual_install) section to install ComfyUI</Note>
      </Tab>
    </Tabs>
  </Accordion>

  <Accordion title="ComfyUI Portable (Windows)">
    Portable version is a ComfyUI version that integrates an independent embedded Python environment, using the portable version you can experience the latest features, currently only supports **Windows** system

    <Card title="ComfyUI Portable (Windows) Installation Guide" icon="link" href="/installation/comfyui_portable_windows">
      Supports **Windows** ComfyUI version running on **Nvidia GPUs** or **CPU-only**, always use the latest commits and completely portable.
    </Card>
  </Accordion>

  <Accordion title="Manual Installation">
    <Card title="ComfyUI Manual Installation Guide" icon="link" href="/installation/manual_install">
      Supports all system types and GPU types (Nvidia, AMD, Intel, Apple Silicon, Ascend NPU, Cambricon MLU)
    </Card>
  </Accordion>
</AccordionGroup>

### 2. Load Default Text-to-Image Workflow

ComfyUI usually loads the default text-to-image workflow automatically when launched. However, you can try different methods to load workflows to familiarize yourself with ComfyUI's basic operations:

<Tabs>
  <Tab title="Load from Workflow Template">
    <img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-1.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=9d27e08e27b8eb3d201b83f9e310fcf1" alt="ComfyUI Interface" data-og-width="973" width="973" data-og-height="696" height="696" data-path="images/tutorial/gettingstarted/first-image-generation-1.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-1.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=b8fcf7b9cf036e830c80d5c7d08277eb 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-1.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=024bfa97dcc43a5544614536571f6cfa 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-1.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=bbf0c3c26374de9f92be8dcf2c010623 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-1.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=135c10c9218d1e205c7d14a0648e20ab 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-1.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=04b75e491e8734279e7e595b8ad8557f 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-1.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=8a40678cb2b4ceb856ddc3d2e867b372 2500w" />
    Follow the numbered steps in the image:

    1. Click the **Fit View** button in the bottom right to ensure any loaded workflow isn't hidden
    2. Click the **folder icon (workflows)** in the sidebar
    3. Click the **Browse example workflows** button at the top of the Workflows panel

    Continue with:
    <img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-2-load-workflow.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=95604069c95b98aaaecfac6141eadadc" alt="Load Workflow" data-og-width="972" width="972" data-og-height="692" height="692" data-path="images/tutorial/gettingstarted/first-image-generation-2-load-workflow.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-2-load-workflow.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=0eb0a67fc3fe9e9cb951fd92fa871e7e 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-2-load-workflow.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=d6180ba2452c01eda59a52e8f1af7446 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-2-load-workflow.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=493e05c8c000d4d44b963afa6c6b968c 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-2-load-workflow.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=fe206a76045f658b209d05463f414363 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-2-load-workflow.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=99d1dd62a0ef8e5296477f8cbecb47e6 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-2-load-workflow.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=6f939111ff708e666db1c331f608c22f 2500w" />

    4. Select the first default workflow **Image Generation** to load it

    Alternatively, you can select **Browse workflow templates** from the workflow menu
    <img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-1-menu.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=bbc70cd0f990a2674b9db9d6f080e026" alt="ComfyUI Menu - Browse Workflow Templates" data-og-width="612" width="612" data-og-height="536" height="536" data-path="images/tutorial/gettingstarted/first-image-generation-1-menu.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-1-menu.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=08dbdd225603ece1d7d92b70c4ca5bd6 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-1-menu.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=94951273209435d7e744f8354364f51d 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-1-menu.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=cae53710d2bcbb862a75bdd5218cfb44 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-1-menu.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=692cf88d6cf8868d898c2d7450e90cf6 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-1-menu.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=8368d39fd5611a52749df6d66260c766 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-1-menu.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=ced69fca7e53b06aa1b36d1067aa75a0 2500w" />
  </Tab>

  <Tab title="Load from Images with Metadata">
    All images generated by ComfyUI contain metadata including workflow information. You can load workflows by:

    * Dragging and dropping a ComfyUI-generated image into the interface
    * Using menu **Workflows** -> **Open** to open an image

    Try loading the workflow using this example image:
    ![ComfyUI-Text to Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/text-to-image-workflow.png)
  </Tab>

  <Tab title="Load from workflow.json">
    ComfyUI workflows can be stored in JSON format. You can export workflows using menu **Workflows** -> **Export**.

    Try downloading and loading this example workflow:

    <a className="prose" href="https://github.com/Comfy-Org/docs/blob/main/public/text-to-image.json" download style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
      <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download text-to-image.json</p>
    </a>

    After downloading, use menu **Workflows** -> **Open** to load the JSON file.
  </Tab>
</Tabs>

### 3. Model Installation

Most ComfyUI installations don't include base models by default. After loading the workflow, if you don't have the [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) model installed, you'll see this prompt:

<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-3-missing-models.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=631b68f7fd227245f032c1837b6e44e7" alt="Missing Models" data-og-width="972" width="972" data-og-height="696" height="696" data-path="images/tutorial/gettingstarted/first-image-generation-3-missing-models.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-3-missing-models.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=bf07d8fa6107520f408460032a331d0a 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-3-missing-models.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=04d7ecb43ff2b94040742a31b9dc621b 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-3-missing-models.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=103f5bf2c1bfaf50c9b4084d84a8e9a7 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-3-missing-models.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=005a3b038883b9c372080cfa0bb770fc 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-3-missing-models.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=11adbcce9945ab8aac33ad7b8c509417 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-3-missing-models.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=c0ec6fab2f0449810fef6a376da685e0 2500w" />

All models are stored in `<your ComfyUI installation>/ComfyUI/models/` with subfolders like `checkpoints`, `embeddings`, `vae`, `lora`, `upscale_model`, etc. ComfyUI detects models in these folders and paths configured in `extra_model_paths.yaml` at startup.

<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-models-folder.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=9715b090919f665331e3500712d5e9e7" alt="ComfyUI Models Folder" data-og-width="1564" width="1564" data-og-height="1228" height="1228" data-path="images/tutorial/gettingstarted/first-image-generation-4-models-folder.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-models-folder.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=657fd94230a1a7796906339c48b8c3b0 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-models-folder.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=7d884de0fc538ab8ef00c14da507b9a8 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-models-folder.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=31938fcc781769c8fabc60ddef58363a 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-models-folder.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=b005cfc130ac988cf28f8ceb7315596b 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-models-folder.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=dc0ae8e2a2f2ffaf7c63c57316b6fde1 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-models-folder.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=d0ee5cb2437d9ea25cab05dca82d81d0 2500w" />

You can install models through:

<Tabs>
  <Tab title="Automatic Download">
    After you click the **Download** button, ComfyUI will execute the download, and different behaviors will be performed depending on the version you are using.

    <Tabs>
      <Tab title="ComfyUI Desktop">
        The desktop version will automatically complete the model download and save it to the `<your ComfyUI installation location>/ComfyUI/models/checkpoints` directory.
        You can wait for the installation to complete or view the installation progress in the model panel on the sidebar.

                <img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-download-status.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=850acf678de82f1358fd230347f41a86" alt="Model Download Progress" data-og-width="974" width="974" data-og-height="697" height="697" data-path="images/tutorial/gettingstarted/first-image-generation-4-download-status.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-download-status.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=54a28a5d0739ccec1b41f6def3f88ddb 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-download-status.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=5f210d81700174972dacef081a56659d 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-download-status.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=c4b78e4ac1b98ebc33d5f3e6285f10bb 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-download-status.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=2b3167deb99fcd5ba3b01dfb25b9ff69 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-download-status.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=9718f43183df17023ccccb41a6fb8ffa 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-download-status.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=6084de5bb78eb5956a497389383512f0 2500w" />

        If everything goes smoothly, the model should be able to download locally. If the download fails for a long time, please try other installation methods.
      </Tab>

      <Tab title="ComfyUI Portable">
        The browser will execute file downloads. Please save the file to the `<your ComfyUI installation location>/ComfyUI_windows_portable/ComfyUI/models/checkpoints` directory after the download is complete.
      </Tab>
    </Tabs>
  </Tab>

  <Tab title="ComfyUI Manager">
    ComfyUI Manager is a tool for managing custom nodes, models, and plugins.

    <Steps>
      <Step title="Open ComfyUI Manager">
                <img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=63bea93b65f4aa69499343eaa8f8fa3d" alt="ComfyUI Manager Installation" data-og-width="492" width="492" data-og-height="124" height="124" data-path="images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=3960c57be9777c85e7704ed7d838f389 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=cf45c060e2e6e2d9c7ac9de6071b3004 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=0fbccad434fdbadf7951372cf2e674db 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=2ca5b357da7a0163b274c3eadddd04d8 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=fd74e46709d4bd2e573aa79ea66e0daf 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=4238a1c13094c4e16f7f976a1375d1bb 2500w" />

        Click the `Manager` button to open ComfyUI Manager
      </Step>

      <Step title="Open Model Manager">
                <img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager-model-manager.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=f263705f288d59e181c4fbd5d2ce8077" alt="ComfyUI Manager Model Management" data-og-width="1200" width="1200" data-og-height="723" height="723" data-path="images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager-model-manager.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager-model-manager.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=1ba12adec58d6664a2222b80db97254b 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager-model-manager.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=d6d2feadaf68a0fcca57333e1dc4cfe2 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager-model-manager.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=a3eae65d43bf4d9e1dc1827f3611fafd 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager-model-manager.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=3d54457cd8213afd45d728a38d665d57 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager-model-manager.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=443471cc0f938f916d96aa1213db34e9 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager-model-manager.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=6e3e020446ebe6a348aec7239811d4e4 2500w" />

        Click `Model Manager`
      </Step>

      <Step title="Search and Install Model">
                <img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager-download-model.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=3af1349a1b3a91b680030d3511a53695" alt="ComfyUI Manager Model Download" data-og-width="1200" width="1200" data-og-height="723" height="723" data-path="images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager-download-model.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager-download-model.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=7963c563d6ac8d95b2c043623dd21660 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager-download-model.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=fa757c67c6889c30975d0611159a9151 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager-download-model.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=d5f1855f2d8dd89899b781fe4116d3c0 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager-download-model.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=397a777a3c91325e5c9ec95a5d268271 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager-download-model.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=18ade34b5f43a854f7296f6a90ed4b65 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-4-comfyui-manager-download-model.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=f209b1b10e09c634cd1d5ffe45ce7cfa 2500w" />

        1. Search for `v1-5-pruned-emaonly.ckpt`
        2. Click `install` on the desired model
      </Step>
    </Steps>
  </Tab>

  <Tab title="Manual Installation">
    Visit [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) and follow this guide:

        <img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-5-hugging-face.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=15c2560c44e7e4276b6a351809f7bf9f" alt="Hugging Face Model Download" data-og-width="769" width="769" data-og-height="727" height="727" data-path="images/tutorial/gettingstarted/first-image-generation-5-hugging-face.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-5-hugging-face.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=8a0a11ce09344e6137d0318c4b3b9719 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-5-hugging-face.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=8089b1a5012ff008f26ebec351eab046 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-5-hugging-face.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=c7d903b4cfd6ac8e1104f577d865f8b1 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-5-hugging-face.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=e712597b7d24bbb7564c0b50e7a073de 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-5-hugging-face.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=a96dc866b2123b98a2addbe33920543b 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-5-hugging-face.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=31971d485c7495f4f71baf350f1b83cc 2500w" />

    Save the downloaded file to:

    <Tabs>
      <Tab title="ComfyUI Desktop">
        Save to `<your ComfyUI installation>/ComfyUI/models/checkpoints`

                <img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-6-2-desktop.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=275add7233b1f048455cff0f162847b9" alt="ComfyUI Desktop Model Save Location" data-og-width="911" width="911" data-og-height="710" height="710" data-path="images/tutorial/gettingstarted/first-image-generation-6-2-desktop.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-6-2-desktop.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=9bde4750c8ff36b3c64bc610ebb05b57 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-6-2-desktop.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=e3340ae124d7239717afe84315d6079d 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-6-2-desktop.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=27da936e2e5be7f615411a89244732bc 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-6-2-desktop.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=cd6a3d90f78767506c30deb0c36e8170 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-6-2-desktop.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=390a871fda8ffe2debbe8da45f6d93ab 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-6-2-desktop.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=fe78a0f9c474924a916a57c8a02348e7 2500w" />
      </Tab>

      <Tab title="ComfyUI Portable">
        Save to `ComfyUI_windows_portable/ComfyUI/models/checkpoints`

                <img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-6-1-portable.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=48976b87bcfdba962b119d901c43b286" alt="ComfyUI Portable Model Save Location" data-og-width="1081" width="1081" data-og-height="709" height="709" data-path="images/tutorial/gettingstarted/first-image-generation-6-1-portable.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-6-1-portable.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=e24f9a271a0aea30a444c519a4991c48 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-6-1-portable.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=d2a219d501d98acbc88eb4ef0a3da736 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-6-1-portable.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=c49908bc31bf6c82e47138199558e74e 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-6-1-portable.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=b66628ad620d7ba1a77d824804d68660 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-6-1-portable.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=e41817dee653258bf714a83e29edcede 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-6-1-portable.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=49379cc7f2f8c0c359a6e97a93d091ad 2500w" />
      </Tab>
    </Tabs>

    Refresh or restart ComfyUI after saving.
  </Tab>
</Tabs>

### 4. Load Model and Generate Your First Image

After installing the model:

<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-7-queue.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=e0c8f5d7af48afa25076309392f02129" alt="Image Generation" data-og-width="2640" width="2640" data-og-height="1384" height="1384" data-path="images/tutorial/gettingstarted/first-image-generation-7-queue.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-7-queue.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=a2c75afbbad046faedacdc0d49847785 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-7-queue.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=51baad1b8a08a3d07956f62cf18b11b5 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-7-queue.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=b414fc5ca9f56a55bc317bfc9a6bc445 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-7-queue.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=16fff107aa1c88d7772c01adae99430f 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-7-queue.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=3e6a2c94b9760ea8e4ca615a4d2be760 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-7-queue.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=d8b83b791ae3a2cc979326454d6cc654 2500w" />

1. In the **Load Checkpoint** node, ensure **v1-5-pruned-emaonly-fp16.safetensors** is selected
2. Click `Queue` or press `Ctrl + Enter` to generate

The result will appear in the **Save Image** node. Right-click to save locally.

<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-8-result.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=92b100190dd8f91abb37b01cc2e09a59" alt="ComfyUI First Image Generation Result" data-og-width="2642" width="2642" data-og-height="1390" height="1390" data-path="images/tutorial/gettingstarted/first-image-generation-8-result.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-8-result.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=f0c2219d45e70c2f21ebd64032fddb50 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-8-result.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=9391713ee886380f8302e396975a94aa 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-8-result.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=ace21872a174725c45788b7ce5ce8a34 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-8-result.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=a579037cd0b65cd2115280f8eee6e80b 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-8-result.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=8dba16d2de187f3f6356af67ee1040ef 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-8-result.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=adfc35ce5d546ca79f140b45dee3fe2d 2500w" />

For detailed text-to-image instructions, see our comprehensive guide:

<Card title="ComfyUI Text-to-Image Workflow Guide" icon="link" href="/tutorials/basic/text-to-image">
  Click here for detailed text-to-image workflow instructions
</Card>

## Troubleshooting

### Model Loading Issues

If the `Load Checkpoint` node shows no models or displays "null", verify your model installation location and try refreshing or restarting ComfyUI.


# ComfyUI Official Documentation
Source: https://docs.comfy.org/index

ComfyUI Official Documentation

<div className="relative">
  <div className="w-full text-center mt-16 pb-8">
    <div className="mt-24 text-center">
      <img src="https://mintcdn.com/dripart/m_j0YaT73n4HzDG4/logo/light.svg?fit=max&auto=format&n=m_j0YaT73n4HzDG4&q=85&s=3b44926bd1e2c43ccc2ca318c9aa15a8" className="block dark:hidden pointer-events-none mx-auto" data-og-width="302" width="302" data-og-height="78" height="78" data-path="logo/light.svg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/m_j0YaT73n4HzDG4/logo/light.svg?w=280&fit=max&auto=format&n=m_j0YaT73n4HzDG4&q=85&s=b48d8e2ab40d75dae3a67b7e5ffb828a 280w, https://mintcdn.com/dripart/m_j0YaT73n4HzDG4/logo/light.svg?w=560&fit=max&auto=format&n=m_j0YaT73n4HzDG4&q=85&s=f2303de381057f7463714540aa029b7a 560w, https://mintcdn.com/dripart/m_j0YaT73n4HzDG4/logo/light.svg?w=840&fit=max&auto=format&n=m_j0YaT73n4HzDG4&q=85&s=1a4e17344a5d95f23288696fc0a7e9fc 840w, https://mintcdn.com/dripart/m_j0YaT73n4HzDG4/logo/light.svg?w=1100&fit=max&auto=format&n=m_j0YaT73n4HzDG4&q=85&s=242c50b41aecebe4ac2813499aa1be7f 1100w, https://mintcdn.com/dripart/m_j0YaT73n4HzDG4/logo/light.svg?w=1650&fit=max&auto=format&n=m_j0YaT73n4HzDG4&q=85&s=543a7c093998c00597965566366a6582 1650w, https://mintcdn.com/dripart/m_j0YaT73n4HzDG4/logo/light.svg?w=2500&fit=max&auto=format&n=m_j0YaT73n4HzDG4&q=85&s=96d7d87851c962ab07b96bed59a161d2 2500w" />

      <img src="https://mintcdn.com/dripart/m_j0YaT73n4HzDG4/logo/dark.svg?fit=max&auto=format&n=m_j0YaT73n4HzDG4&q=85&s=6e497cdb7c0cd9847a30359f6607bc25" className="hidden dark:block pointer-events-none mx-auto" data-og-width="302" width="302" data-og-height="78" height="78" data-path="logo/dark.svg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/m_j0YaT73n4HzDG4/logo/dark.svg?w=280&fit=max&auto=format&n=m_j0YaT73n4HzDG4&q=85&s=e74e41eb8dd226289eec7152773ac041 280w, https://mintcdn.com/dripart/m_j0YaT73n4HzDG4/logo/dark.svg?w=560&fit=max&auto=format&n=m_j0YaT73n4HzDG4&q=85&s=ea7a04ffab74488fb0ad96d61961bef5 560w, https://mintcdn.com/dripart/m_j0YaT73n4HzDG4/logo/dark.svg?w=840&fit=max&auto=format&n=m_j0YaT73n4HzDG4&q=85&s=46e4cb58de3a5f279e5bffc12efff328 840w, https://mintcdn.com/dripart/m_j0YaT73n4HzDG4/logo/dark.svg?w=1100&fit=max&auto=format&n=m_j0YaT73n4HzDG4&q=85&s=bf2e8221993c3a0e6cf680f16b43cb68 1100w, https://mintcdn.com/dripart/m_j0YaT73n4HzDG4/logo/dark.svg?w=1650&fit=max&auto=format&n=m_j0YaT73n4HzDG4&q=85&s=1faf9394a068b896ad4ce569aa4b673d 1650w, https://mintcdn.com/dripart/m_j0YaT73n4HzDG4/logo/dark.svg?w=2500&fit=max&auto=format&n=m_j0YaT73n4HzDG4&q=85&s=b81ac2049ff88019a54c1382aa9d6c2a 2500w" />
    </div>

    <div className="flex justify-center w-full">
      <h1
        style={{
      fontSize: "18px",
      fontWeight: "bold",
      marginTop: "10px",
      textAlign: "center"
    }}
        className="text-center"
      >
        ComfyUI Official Documentation
      </h1>
    </div>
  </div>

  {/* social start */}

  <div className="text-center pb-8">
    <div className="flex gap-4 justify-center items-center">
      <a href="https://github.com/comfyanonymous/ComfyUI/" target="_blank" className="hover:opacity-80">
        <Icon icon="github" iconType="solid" size={32} />
      </a>

      <a href="https://x.com/ComfyUI" target="_blank" className="hover:opacity-80">
        <Icon icon="x-twitter" iconType="solid" size={32} />
      </a>

      <a href="https://discord.com/invite/comfyorg" target="_blank" className="hover:opacity-80">
        <Icon icon="discord" iconType="solid" size={32} />
      </a>

      <a href="https://www.youtube.com/@comfyorg" target="_blank" className="hover:opacity-80">
        <Icon icon="youtube" iconType="solid" size={32} />
      </a>
    </div>
  </div>

  <div className="px-4 max-w-5xl mx-auto">
    <p className="max-w-xl mx-auto px-4 mb-12 text-lg text-center text-gray-500 dark:text-zinc-500">
      The most powerful open source node-based application for generative AI
    </p>

    {/* Getting Started Section */}

    <div className="mb-12">
      <h2 className="text-2xl font-bold mb-6 text-center">Getting Started</h2>

      <CardGroup cols={3}>
        <Card title="Download & Install" icon="download" href="/installation/system_requirements">
          Install ComfyUI on Windows, macOS, or Linux
        </Card>

        <Card title="First Generation" icon="rocket" href="/get_started/first_generation">
          Create your first AI-generated image
        </Card>

        <Card title="Basic Concepts" icon="lightbulb" href="/development/core-concepts/workflow">
          Understand workflows, nodes, and links
        </Card>
      </CardGroup>
    </div>

    {/* Learn & Tutorials Section */}

    <div className="mb-12">
      <h2 className="text-2xl font-bold mb-6 text-center">Learn & Tutorials</h2>

      <CardGroup cols={3}>
        <Card title="Interface Guide" icon="window-maximize" href="/interface/overview">
          Navigate the ComfyUI interface
        </Card>

        <Card title="Tutorials" icon="book-open" href="/tutorials/basic/text-to-image">
          Step-by-step guides for common tasks
        </Card>

        <Card title="Built-in Nodes" icon="diagram-project" href="/built-in-nodes/overview">
          Learn about each node in ComfyUI
        </Card>
      </CardGroup>
    </div>

    {/* Development Section */}

    <div className="mb-12">
      <h2 className="text-2xl font-bold mb-6 text-center">Development & Extension</h2>

      <CardGroup cols={3}>
        <Card title="Development Guide" icon="code" href="/development/overview">
          Contribute to ComfyUI development
        </Card>

        <Card title="Custom Nodes" icon="puzzle-piece" href="/custom-nodes/overview">
          Create and publish custom nodes
        </Card>

        <Card title="API Documentation" icon="terminal" href="/development/comfyui-server/comms_overview">
          Integrate ComfyUI with your applications
        </Card>
      </CardGroup>
    </div>

    {/* Get Help Section */}

    <div className="mb-12">
      <h2 className="text-2xl font-bold mb-6 text-center">Get Help</h2>

      <CardGroup cols={3}>
        <Card title="Contact Support" icon="headset" href="/support/contact-support">
          Get help from our support team
        </Card>

        <Card title="Account Management" icon="user-gear" href="/account/create-account">
          Create, login, and manage your account
        </Card>

        <Card title="Billing Support" icon="credit-card" href="/support/subscription/subscribing">
          Manage subscriptions and payments
        </Card>

        <Card title="Troubleshooting" icon="circle-question" href="/troubleshooting/overview">
          Resolve common issues and errors
        </Card>

        <Card title="Community" icon="users" href="/community/links">
          Join the ComfyUI community
        </Card>
      </CardGroup>
    </div>

    {/* About ComfyUI Section */}

    <div className="text-center mt-16 mb-8">
      <h2 className="text-2xl font-bold mb-4">About ComfyUI</h2>

      <p className="mb-4">
        Written by <a href="https://github.com/comfyanonymous" className="text-blue-500 dark:text-blue-400 underline hover:text-blue-600 dark:hover:text-blue-500">comfyanonymous</a> and other <a href="https://github.com/comfyanonymous/ComfyUI/graphs/contributors" className="text-blue-500 dark:text-blue-400 underline hover:text-blue-600 dark:hover:text-blue-500">contributors</a>.
      </p>

      <ul className="list-none space-y-2 max-w-2xl mx-auto">
        <li><strong>ComfyUI</strong> is a node-based interface and inference engine for generative AI</li>
        <li>Users can combine various AI models and operations through nodes to achieve highly customizable and controllable content generation</li>
        <li>ComfyUI is completely open source and can run on your local device</li>
      </ul>
    </div>
  </div>
</div>


# ComfyUI(portable) Windows
Source: https://docs.comfy.org/installation/comfyui_portable_windows

This tutorial will guide you on how to download and start using ComfyUI Portable and run the corresponding programs

**ComfyUI Portable** is a standalone packaged complete ComfyUI Windows version that has integrated an independent **Python (python\_embeded)** required for ComfyUI to run. You only need to extract it to use it. Currently, the portable version supports running through **Nvidia GPU** or **CPU**.

This guide section will walk you through installing ComfyUI Portable.

## Download ComfyUI Portable

You can get the latest ComfyUI Portable download link by clicking the link below

<a className="prose" href="https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia.7z" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download ComfyUI Portable</p>
</a>

After downloading, you can use decompression software like [7-ZIP](https://7-zip.org/) to extract the compressed package

The file structure and description after extracting the portable version are as follows:

```
ComfyUI_windows_portable
 ComfyUI                   // ComfyUI main program
 python_embeded            // Independent Python environment
 update                    // Batch scripts for upgrading portable version
 README_VERY_IMPORTANT.txt   // ComfyUI Portable usage instructions in English
 run_cpu.bat                 // Double click to start ComfyUI (CPU only)
 run_nvidia_gpu.bat          // Double click to start ComfyUI (Nvidia GPU)
```

## How to Launch ComfyUI

Double click either `run_nvidia_gpu.bat` or `run_cpu.bat` depending on your computer's configuration to launch ComfyUI.
You will see the command running as shown in the image below

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfyui-portable-cmd.png?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=27ead27f4bf5f09cabffc238e5a5890a" alt="ComfyUI Portable Command Prompt" data-og-width="1145" width="1145" data-og-height="648" height="648" data-path="images/comfyui-portable-cmd.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfyui-portable-cmd.png?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=2ee9a9bb89a49939af0aa62a3e5d9621 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfyui-portable-cmd.png?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=69bfce9759f4de69fb6382e873a9ad16 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfyui-portable-cmd.png?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=929494225b52b341b9f68b3042120963 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfyui-portable-cmd.png?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=e568e501c7bce40d696f07861b56f6f2 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfyui-portable-cmd.png?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=fa7adcdf557b4c7d398f386cf8f6cf23 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfyui-portable-cmd.png?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=c44563098a54d52ee935b219284e9014 2500w" />

When you see something similar to the image

```
To see the GUI go to: http://127.0.0.1:8188
```

At this point, your ComfyUI service has started. Normally, ComfyUI will automatically open your default browser and navigate to `http://127.0.0.1:8188`. If it doesn't open automatically, please manually open your browser and visit this address.

<Warning>During use, please do not close the corresponding command line window, otherwise ComfyUI will stop running</Warning>

## Adding Extra Model Paths

If you want to manage your model files outside of `ComfyUI/models`, you may have the following reasons:

* You have multiple ComfyUI instances and want them to share model files to save disk space
* You have different types of GUI programs (such as WebUI) and want them to use the same model files
* Model files cannot be recognized or found

We provide a way to add extra model search paths via the `extra_model_paths.yaml` configuration file

### Open Config File

<Tabs>
  <Tab title="Portable/Manual Install">
    For the ComfyUI version such as [portable](/installation/comfyui_portable_windows) and [manual](/installation/manual_install), you can find an example file named `extra_model_paths.yaml.example` in the root directory of ComfyUI:

    ```
    ComfyUI/extra_model_paths.yaml.example
    ```

    Copy and rename it to `extra_model_paths.yaml` for use. Keep it in ComfyUI's root directory at `ComfyUI/extra_model_paths.yaml`.
    You can also find the config example file [here](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example)
  </Tab>

  <Tab title="ComfyUI Desktop">
    If you are using the [ComfyUI Desktop](/installation/desktop/windows) application, you can follow the image below to open the extra model config file:

        <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=b2a08255f65a32e3da0c018a3cebb6a2" alt="Open Config File" data-og-width="1056" width="1056" data-og-height="1166" height="1166" data-path="images/desktop/extra_model_paths.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=4f4044b7cef96f0f4a70e7d8a0eb007a 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=aa576e31096b7306e810c10c97416ad5 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=ff8461eb13a9cec15293e7c5663bc65c 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=a2bf5a5335edfa58f37652bfed7a7dc9 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=a1a4c2d6fd021f4a348ea95c4abb5871 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=f508c6130dd86ec9adbef19aaf0d2530 2500w" />

    Or open it directly at:

    <Tabs>
      <Tab title="Windows">
        ```
        C:\Users\YourUsername\AppData\Roaming\ComfyUI\extra_models_config.yaml
        ```
      </Tab>

      <Tab title="macOS">
        ```
        ~/Library/Application Support/ComfyUI/extra_models_config.yaml
        ```
      </Tab>
    </Tabs>

    You should keep the file in the same directory, should not move these files to other places.
  </Tab>
</Tabs>

If the file does not exist, you can create it yourself with any text editor.

### Example Structure

Suppose you want to add the following model paths to ComfyUI:

```
 YOUR_PATH/
   models/
  |     lora/
  |       xxxxx.safetensors
  |     checkpoints/
  |       xxxxx.safetensors
  |     vae/
  |       xxxxx.safetensors
  |     controlnet/
  |        xxxxx.safetensors
```

Then you can configure the `extra_model_paths.yaml` file like below to let ComfyUI recognize the model paths on your device:

```
my_custom_config:
    base_path: YOUR_PATH
    loras: models/loras/
    checkpoints: models/checkpoints/
    vae: models/vae/
    controlnet: models/controlnet/
```

or

```
my_custom_config:
    base_path: YOUR_PATH/models/
    loras: loras
    checkpoints: checkpoints
    vae: vae
    controlnet: controlnet
```

<Warning>
  For the desktop version, please add the configuration to the existing configuration path without overwriting the path configuration generated during installation. Please back up the corresponding file before modification, so that you can restore it when you make a mistake.
</Warning>

Or you can refer to the default [extra\_model\_paths.yaml.example](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) for more configuration options. After saving, you need to **restart ComfyUI** for the changes to take effect.

Below is the original config example:

```yaml  theme={null}
#Rename this to extra_model_paths.yaml and ComfyUI will load it


#config for a1111 ui
#all you have to do is change the base_path to where yours is installed
a111:
    base_path: path/to/stable-diffusion-webui/

    checkpoints: models/Stable-diffusion
    configs: models/Stable-diffusion
    vae: models/VAE
    loras: |
         models/Lora
         models/LyCORIS
    upscale_models: |
                  models/ESRGAN
                  models/RealESRGAN
                  models/SwinIR
    embeddings: embeddings
    hypernetworks: models/hypernetworks
    controlnet: models/ControlNet

#config for comfyui
#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.

#comfyui:
#     base_path: path/to/comfyui/
#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads
#     #is_default: true
#     checkpoints: models/checkpoints/
#     clip: models/clip/
#     clip_vision: models/clip_vision/
#     configs: models/configs/
#     controlnet: models/controlnet/
#     diffusion_models: |
#                  models/diffusion_models
#                  models/unet
#     embeddings: models/embeddings/
#     loras: models/loras/
#     upscale_models: models/upscale_models/
#     vae: models/vae/

#other_ui:
#    base_path: path/to/ui
#    checkpoints: models/checkpoints
#    gligen: models/gligen
#    custom_nodes: path/custom_nodes

```

For example, if your WebUI is located at `D:\stable-diffusion-webui\`, you can modify the corresponding configuration to

```yaml  theme={null}
a111:
    base_path: D:\stable-diffusion-webui\
    checkpoints: models/Stable-diffusion
    configs: models/Stable-diffusion
    vae: models/VAE
    loras: |
         models/Lora
         models/LyCORIS
    upscale_models: |
                  models/ESRGAN
                  models/RealESRGAN
                  models/SwinIR
    embeddings: embeddings
    hypernetworks: models/hypernetworks
    controlnet: models/ControlNet
```

### Add Extra Custom Nodes Path

Besides adding external models, you can also add custom nodes paths that are not in the default path of ComfyUI

<Tip>
  Please note that this will not change the default installation path of custom nodes, but will add an extra path search when starting ComfyUI. You still need to complete the installation of custom node dependencies in the corresponding environment to ensure the integrity of the running environment.
</Tip>

Below is a simple configuration example (MacOS), please modify it according to your actual situation and add it to the corresponding configuration file, save it and restart ComfyUI for the changes to take effect:

```yaml  theme={null}
my_custom_nodes:
  custom_nodes: /Users/your_username/Documents/extra_custom_nodes
```

## First Image Generation

After successful installation, you can refer to the section below to start your ComfyUI journey\~

<Card title="First Image Generation" icon="link" href="/get_started/first_generation">
  This tutorial will guide you through your first model installation and text-to-image generation
</Card>

## Additional ComfyUI Portable Instructions

### 1. Upgrading ComfyUI Portable

You can use the batch commands in the update folder to upgrade your ComfyUI Portable version

```
ComfyUI_windows_portable
 update
    update.py
    update_comfyui.bat                          // Update ComfyUI to the latest commit version
    update_comfyui_and_python_dependencies.bat  // Only use when you have issues with your runtime environment
    update_comfyui_stable.bat                   // Update ComfyUI to the latest stable version
```

### 2. Setting Up LAN Access for ComfyUI Portable

If your ComfyUI is running on a local network and you want other devices to access ComfyUI, you can modify the `run_nvidia_gpu.bat` or `run_cpu.bat` file using Notepad to complete the configuration. This is mainly done by adding `--listen` to specify the listening address.
Below is an example of the `run_nvidia_gpu.bat` file command with the `--listen` parameter added

```bat  theme={null}
.\python_embeded\python.exe -s ComfyUI\main.py --listen --windows-standalone-build
pause
```

After enabling ComfyUI, you will notice the final running address will become

```
Starting server

To see the GUI go to: http://0.0.0.0:8188
To see the GUI go to: http://[::]:8188
```

You can press `WIN + R` and type `cmd` to open the command prompt, then enter `ipconfig` to view your local IP address. Other devices can then access ComfyUI by entering `http://your-local-IP:8188` in their browser.


# Linux Desktop Version
Source: https://docs.comfy.org/installation/desktop/linux

This article introduces how to download, install and use ComfyUI Desktop for Linux

<Warning>Linux pre-built packages are not yet available. Please try [manual installation.](/installation/manual_install)</Warning>

When Linux desktop packages become available, you can configure external model paths:

## Adding External Model Paths

If you have models stored in other locations on your computer outside the ComfyUI installation directory, you can add them to ComfyUI by configuring the `extra_models_config.yaml` file.

For ComfyUI Desktop, this file is located at:

* On Windows: `C:\Users\<YOUR_USERNAME>\AppData\Roaming\ComfyUI\extra_models_config.yaml`
* On macOS: `~/Library/Application Support/ComfyUI/extra_models_config.yaml`
* On Linux: `~/.config/ComfyUI/extra_models_config.yaml`

For detailed instructions, see [Models documentation](/development/core-concepts/models#adding-external-model-paths)


# MacOS Desktop Version
Source: https://docs.comfy.org/installation/desktop/macos

This article introduces how to download, install and use ComfyUI Desktop for MacOS

export const log_path_0 = "~/Library/Logs/ComfyUI"

export const config_path_0 = "~/Library/Application Support/ComfyUI"

**ComfyUI Desktop** is a standalone installation version that can be installed like regular software.
It supports quick installation and automatic configuration of the **Python environment and dependencies**, and supports one-click import of existing ComfyUI settings, models, workflows, and files.

ComfyUI Desktop is an open source project, please visit the full code [here](https://github.com/Comfy-Org/desktop).

<Note>ComfyUI Desktop (MacOS) only supports Apple Silicon</Note>

This tutorial will guide you through the software installation process and explain related configuration details.

<Warning>As **ComfyUI Desktop** is still in **Beta** status, the actual installation process may change</Warning>

## ComfyUI Desktop (MacOS) Download

Please click the button below to download the installation package for MacOS **ComfyUI Desktop**

<a className="prose" href="https://download.comfy.org/mac/dmg/arm64" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download for MacOS</p>
</a>

## Install via Homebrew

ComfyUI Desktop can also be installed via [Homebrew](https://brew.sh/):

```
brew install comfyui
```

## ComfyUI Desktop Installation Steps

Double-click the downloaded installation package file. As shown in the image, drag the **ComfyUI** application into the **Applications** folder following the arrow

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-0.png?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=fa83e96d6f26f6b34a71cc103eb38108" alt="ComfyUI Installation Package" data-og-width="883" width="883" data-og-height="698" height="698" data-path="images/desktop/mac-comfyui-desktop-0.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-0.png?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=fa13c212b5933b36385bb6da53d70a61 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-0.png?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=46b7c62de74bb178a0153ab028f7b268 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-0.png?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=108cb198b2970fd1745a65163178c246 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-0.png?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=70f9b144f45fb6cf10826834d6daa0f6 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-0.png?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=0330ac481b350cd691482040000a98b9 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-0.png?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=080c2f8dc4a84bd44b11f79e8dcd3008 2500w" />

If your folder shows as below with a prohibition sign on the icon after opening the installation package, it means your current system version is not compatible with **ComfyUI Desktop**
<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-0-1.png?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=fe9e4f24ccff7e80553955b6c944d6be" alt="ComfyUI logo" data-og-width="883" width="883" data-og-height="622" height="622" data-path="images/desktop/mac-comfyui-desktop-0-1.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-0-1.png?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=3477e8f38233451c9a7df68d0093f024 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-0-1.png?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=82d77610a192557cf1fee352deaf9639 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-0-1.png?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=cc4b9f5bb63072b70779b7ce3d1822a7 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-0-1.png?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=887e714dfa49c83db3fde090359d5884 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-0-1.png?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=d42ee07a848d3bd61bc405a9c63fdb48 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-0-1.png?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=6896aa942c971fa98062f8239410cfc1 2500w" />

Then find the **ComfyUI icon** in **Launchpad** and click it to enter ComfyUI initialization settings
<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-1.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=a315ed6d739f9a0590a93da64981b32b" alt="ComfyUI Lanchpad" data-og-width="2880" width="2880" data-og-height="1620" height="1620" data-path="images/desktop/mac-comfyui-desktop-1.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-1.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=4c2285474e2708685f01d74ebdbcf27d 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-1.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=8c500b8fca9d8450a88e74b26a5b5ee4 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-1.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=344b1b0a40ace630b6dba8a0abbcc049 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-1.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=9877f40a7db4dc573f9f7ece9cdb7f3c 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-1.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=cbfe213d92634c15455eb7715b6fe18f 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-1.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=897e72547a9a9429970b3f31ec7456ea 2500w" />

## ComfyUI Desktop Initialization Process

<Steps>
  <Step title="Start Screen">
    <Tabs>
      <Tab title="Normal Start">
                <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-2.png?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=e9795a772f7cb2d26ddf2de4a111048d" alt="ComfyUI Installation Steps - Start" data-og-width="1024" width="1024" data-og-height="768" height="768" data-path="images/desktop/mac-comfyui-desktop-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-2.png?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=16930b1bf7346da56bbe2387667d8181 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-2.png?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=1745ef37e70f4fae1516a605c250fe2e 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-2.png?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=213e53f6e58625fe8a7eab82b90f640c 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-2.png?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=c1e2b9e34fc572a00e08d8b912aadb31 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-2.png?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=00a1747c5b38d51a5b5d34b4517874f3 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-2.png?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=aa9cbfaa0071e65b8cc6e0569a361762 2500w" />

        Click **Get Started** to begin initialization
      </Tab>

      <Tab title="Maintenance Page">
        There are many reasons you might have issues installing ComfyUI. Maybe a network connection failed when installing pytorch (15 GB). Or you dont have git installed. The maintenance page automatically opens when it detects an issue and provides a way to resolve the issue.

        You can use it to resolve most issues:

        * Create a python virtual environment
        * Reinstall all missing core dependencies to your Python virtual environment thats managed by Desktop
        * Install git, VC redis
        * Choose a new install location

        The default maintenance page displays the current error content

                <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/maintenance-1.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=2a01daff7808d4d342aeb423a072308f" alt="ComfyUI Maintenance Page" data-og-width="971" width="971" data-og-height="715" height="715" data-path="images/desktop/maintenance-1.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/maintenance-1.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=6e97afdce4ad87e3a8d0c9149cb346f6 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/maintenance-1.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=83696424cc1d361d118feab76c7daeea 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/maintenance-1.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=140c31b7000ab635c147d40899a1477b 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/maintenance-1.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=b6298c53bd547097ea9813245138f6e5 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/maintenance-1.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=8c822e6822e839ca25df3d15646e4f02 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/maintenance-1.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=bd4e3689bf4b0bcf01a7870457f151b2 2500w" />

        Clicking `All` allows you to view all the content that can be operated on currently

                <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/maintenance-2.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=b987fe536f542650e7bd16bbb81a9b1a" alt="ComfyUI Maintenance Page" data-og-width="1213" width="1213" data-og-height="1028" height="1028" data-path="images/desktop/maintenance-2.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/maintenance-2.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=e7ab140496bcd19ccc0b42d881ddfeb2 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/maintenance-2.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=d713f7abe3e3f0e803014e23f8007c81 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/maintenance-2.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=c7889c0277f2c02c46849515686a09d0 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/maintenance-2.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=e5c2fcaddfe6e1f58e22f19342bd3cc8 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/maintenance-2.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=e9b8aee2ac65b14724dcfddb210d6cc7 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/maintenance-2.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=429efbb180c2ce8101751b0a27327bcd 2500w" />
      </Tab>
    </Tabs>
  </Step>

  <Step title="Select GPU">
        <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-3.png?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=145a5d5a12bebb15a09f6fda3bca6565" alt="ComfyUI Installation Steps - GPU Selection" data-og-width="1024" width="1024" data-og-height="768" height="768" data-path="images/desktop/mac-comfyui-desktop-3.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-3.png?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=ffc04c74298a3d9be74786bc32771fed 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-3.png?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=8f9825571425df2e1679da68dda2e57c 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-3.png?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=b489b601596da8ff6f496366109e8fce 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-3.png?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=76008549ff4e093ee6a20d041f5d6588 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-3.png?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=a1d0189f1fdc62b4b1fe339602904d44 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-3.png?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=965f050a4dcb8bf6d77e5bcac6a2bf53 2500w" />

    The three options are:

    1. **MPS (Recommended):** Metal Performance Shaders (MPS) is an Apple framework that uses GPUs to accelerate computing and machine learning tasks on Apple devices, supporting frameworks like PyTorch.
    2. **Manual Configuration:** You need to manually install and configure the python runtime environment. Don't select this unless you know how to configure
    3. **Enable CPU Mode:** For developers and special cases only. Don't select this unless you're sure you need it

    Unless there are special circumstances, please select **MPS** as shown and click **Next** to proceed
  </Step>

  <Step title="Install location">
        <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-4.png?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=29b0e4ee60b6991d5fff478c2f4028bf" alt="ComfyUI Installation Steps - Installation Location" data-og-width="1024" width="1024" data-og-height="768" height="768" data-path="images/desktop/mac-comfyui-desktop-4.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-4.png?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=3d73b9e85f245091387b95ff1afe29fb 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-4.png?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=b2cadffd88abc71d4472c4460b1142fe 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-4.png?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=c7a0ce651e528d1adc144994813bacf5 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-4.png?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=f5e767661e569c98fea58dba785fac69 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-4.png?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=1b43dcfdbf61d62779943b3a05560e02 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-4.png?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=3eaf8a9217a4e2723a6526f0a854f20b 2500w" />

    In this step, you will select the installation location for the following related content of ComfyUI:

    * **Python Environment**
    * **Models Model Files**
    * **Custom Nodes Custom Nodes**

    Recommendations:

    * Please create a separate empty folder as the installation directory for ComfyUI
    * Please ensure that the disk has at least **5G** of disk space to ensure the normal installation of **ComfyUI Desktop**

    <Note>Not all files are installed in this directory, some files will be located in the MacOS system directory, you can refer to the uninstallation section of this guide to complete the uninstallation of the ComfyUI desktop version</Note>
  </Step>

  <Step title="Migrate from Existing Installation (Optional)">
        <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-5.png?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=ebd173fbc75978d91f05cab02823fd22" alt="ComfyUI Installation Steps - File Migration" data-og-width="1024" width="1024" data-og-height="768" height="768" data-path="images/desktop/mac-comfyui-desktop-5.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-5.png?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=d6b54ec77697e1da0b456a22a3affcd9 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-5.png?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=c44898a8b41e63d5f6d69d363d6a6564 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-5.png?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=6a9a4a799e132a2406eb075b6d435451 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-5.png?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=20d689d3d5016d8ee24834766a514d5a 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-5.png?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=69e19a8beb0daceed5560180543cf2dd 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-5.png?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=af0c6afcedbd7bd41bb3fc7b23865239 2500w" />

    In this step you can migrate your existing ComfyUI installation content to ComfyUI Desktop. Select your existing ComfyUI installation directory, and the installer will automatically recognize:

    * **User Files**
    * **Models:** Will not be copied, only linked with desktop version
    * **Custom Nodes:** Nodes will be reinstalled

    Don't worry, this step won't copy model files. You can check or uncheck options as needed. Click **Next** to continue
  </Step>

  <Step title="Desktop Settings">
        <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-6.png?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=5b0980bf4e43f4b21c4bae613935f59a" alt="ComfyUI Installation Steps - Desktop Settings" data-og-width="1024" width="1024" data-og-height="768" height="768" data-path="images/desktop/mac-comfyui-desktop-6.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-6.png?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=8479828672aa40c53a242c988a28b67d 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-6.png?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=34cdd9999fb5fbca40055d03c0a46920 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-6.png?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=b0041376f45c6ea72fb1d529f93107ee 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-6.png?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=ced87efe8dedea7e5ed1e70ec7d46a31 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-6.png?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=af40e071cfd4c7e68800526d6b1eb48f 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-6.png?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=b5c515f256bf1733652b550f4c62df8d 2500w" />

    These are preference settings:

    1. **Automatic Updates:** Whether to set automatic updates when ComfyUI updates are available
    2. **Usage Metrics:** If enabled, we will collect **anonymous usage data** to help improve ComfyUI
    3. **Mirror Settings:** Since the program needs internet access to download Python and complete environment installation, if you see a red  during installation indicating this may cause installation failure, please follow the steps below

    <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-7.png?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=5ade3ebbc9231bf38c886abaa26c3e92" alt="ComfyUI Installation Steps - Mirror Settings" data-og-width="1024" width="1024" data-og-height="768" height="768" data-path="images/desktop/mac-comfyui-desktop-7.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-7.png?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=21a5375021f1666eb5f9ce3635296b59 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-7.png?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=c8666bf4827f5c69deb2317c879424a4 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-7.png?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=4ba0b3e9aa45c78c8cc71d6ac2992dba 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-7.png?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=c8136cef6df6c6d59bc7bc70c45d0f41 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-7.png?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=65f2f0bd058e434d07ab700bfe4a11e8 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/mac-comfyui-desktop-7.png?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=4a9febf4d9f6b59a596c265b0d99253b 2500w" />
    Expand the mirror settings to find the specific failing mirror. In this screenshot the error is **Python Install Mirror** failure.

    For different mirror errors, you can refer to the following content to try to manually find different mirrors and replace them

    The following cases mainly apply to users in China.

    #### Python Installation Mirror

    If the default mirror is unavailable, please try using the mirror below.

    ```
    https://python-standalone.org/mirror/astral-sh/python-build-standalone
    ```

    If you need to find other alternative GitHub mirror addresses, please look for and construct a mirror address pointing to the releases of the `python-build-standalone` repository.

    ```
    https://github.com/astral-sh/python-build-standalone/releases/download
    ```

    Build a link in the following pattern

    ```
    https://xxx/astral-sh/python-build-standalone/releases/download
    ```

    <info>Since most of the Github mirror services are provided by third parties, please pay attention to the security during use.</info>

    #### PyPI Mirror

    * Alibaba Cloud: [https://mirrors.aliyun.com/pypi/simple/](https://mirrors.aliyun.com/pypi/simple/)
    * Tencent Cloud: [https://mirrors.cloud.tencent.com/pypi/simple/](https://mirrors.cloud.tencent.com/pypi/simple/)
    * University of Science and Technology of China: [https://pypi.mirrors.ustc.edu.cn/simple/](https://pypi.mirrors.ustc.edu.cn/simple/)
    * Shanghai Jiao Tong University: [https://pypi.sjtu.edu.cn/simple/](https://pypi.sjtu.edu.cn/simple/)

    #### Torch Mirror

    * Aliyun: [https://mirrors.aliyun.com/pytorch-wheels/cu121/](https://mirrors.aliyun.com/pytorch-wheels/cu121/)
  </Step>

  <Step title="Complete the installation">
    If everything is correct, the installer will complete and automatically enter the ComfyUI Desktop interface, then the installation is successful
    <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-interface.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=8a86c18801fbede950872e2a26dd4381" alt="ComfyUI Desktop Interface" data-og-width="1678" width="1678" data-og-height="980" height="980" data-path="images/desktop/comfyui-interface.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-interface.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=efd4befd3f4105de49196f4e568f9dc5 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-interface.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=8d3655f1c724ddfd458d0bc325134399 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-interface.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=4c4bd4537cb38b3de785da3fc375b9da 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-interface.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=ed366119da482d7b4c855cea65f0b0d8 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-interface.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=751c5c07a0a4c01dc96cfce7356b5673 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-interface.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=4e86835b96ef55a16595ffdcd6b5cf3e 2500w" />
  </Step>
</Steps>

## First Image Generation

After successful installation, you can refer to the section below to start your ComfyUI journey\~

<Card title="First Image Generation" icon="link" href="/get_started/first_generation">
  This tutorial will guide you through your first model installation and text-to-image generation
</Card>

## How to Update ComfyUI Desktop

Currently, ComfyUI Desktop updates use automatic detection updates, please ensure that automatic updates are enabled in the settings

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-desktop-update-setting.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=5006c9c0e4ad3e4f6ca0cef51132fbf3" alt="ComfyUI Desktop Settings" data-og-width="1065" width="1065" data-og-height="815" height="815" data-path="images/desktop/comfyui-desktop-update-setting.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-desktop-update-setting.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=25f0e90e8df014d0640205bb73e4253c 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-desktop-update-setting.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=0137407c528ac79439d4320701e4c0b1 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-desktop-update-setting.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=73f85bd7cbbeeeaeabad54154b60af7e 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-desktop-update-setting.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=4f880973b712a83849b16a78ce5868f5 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-desktop-update-setting.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=174dc6b670118f452834cece0df52dbd 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-desktop-update-setting.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=1b72afe13029997c797c16772daeff98 2500w" />

You can also choose to manually check for available updates in the `Menu` --> `Help` --> `Check for Updates`

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_check_for_updates.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=0f2bf5d6ad204e762ed858f29df4282c" alt="ComfyUI Desktop Check for Updates" data-og-width="415" width="415" data-og-height="477" height="477" data-path="images/desktop/desktop_check_for_updates.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_check_for_updates.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=c8f28a7f5ef143b0ca31e839187a5a65 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_check_for_updates.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=ffef0bd3b488b1545c0390d65ffbd445 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_check_for_updates.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=e46efe596b9b5b7f38da68489c245e4c 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_check_for_updates.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=9dd9460b2daafd8b398145e1961d668c 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_check_for_updates.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=218f1e5e502286b7a90e21d5f60100fc 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_check_for_updates.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=f7e03d947a487ad2a5624f8d14c68e40 2500w" />

## Adding Extra Model Paths

If you want to manage your model files outside of `ComfyUI/models`, you may have the following reasons:

* You have multiple ComfyUI instances and want them to share model files to save disk space
* You have different types of GUI programs (such as WebUI) and want them to use the same model files
* Model files cannot be recognized or found

We provide a way to add extra model search paths via the `extra_model_paths.yaml` configuration file

### Open Config File

<Tabs>
  <Tab title="Portable/Manual Install">
    For the ComfyUI version such as [portable](/installation/comfyui_portable_windows) and [manual](/installation/manual_install), you can find an example file named `extra_model_paths.yaml.example` in the root directory of ComfyUI:

    ```
    ComfyUI/extra_model_paths.yaml.example
    ```

    Copy and rename it to `extra_model_paths.yaml` for use. Keep it in ComfyUI's root directory at `ComfyUI/extra_model_paths.yaml`.
    You can also find the config example file [here](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example)
  </Tab>

  <Tab title="ComfyUI Desktop">
    If you are using the [ComfyUI Desktop](/installation/desktop/windows) application, you can follow the image below to open the extra model config file:

        <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=b2a08255f65a32e3da0c018a3cebb6a2" alt="Open Config File" data-og-width="1056" width="1056" data-og-height="1166" height="1166" data-path="images/desktop/extra_model_paths.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=4f4044b7cef96f0f4a70e7d8a0eb007a 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=aa576e31096b7306e810c10c97416ad5 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=ff8461eb13a9cec15293e7c5663bc65c 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=a2bf5a5335edfa58f37652bfed7a7dc9 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=a1a4c2d6fd021f4a348ea95c4abb5871 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=f508c6130dd86ec9adbef19aaf0d2530 2500w" />

    Or open it directly at:

    <Tabs>
      <Tab title="Windows">
        ```
        C:\Users\YourUsername\AppData\Roaming\ComfyUI\extra_models_config.yaml
        ```
      </Tab>

      <Tab title="macOS">
        ```
        ~/Library/Application Support/ComfyUI/extra_models_config.yaml
        ```
      </Tab>
    </Tabs>

    You should keep the file in the same directory, should not move these files to other places.
  </Tab>
</Tabs>

If the file does not exist, you can create it yourself with any text editor.

### Example Structure

Suppose you want to add the following model paths to ComfyUI:

```
 YOUR_PATH/
   models/
  |     lora/
  |       xxxxx.safetensors
  |     checkpoints/
  |       xxxxx.safetensors
  |     vae/
  |       xxxxx.safetensors
  |     controlnet/
  |        xxxxx.safetensors
```

Then you can configure the `extra_model_paths.yaml` file like below to let ComfyUI recognize the model paths on your device:

```
my_custom_config:
    base_path: YOUR_PATH
    loras: models/loras/
    checkpoints: models/checkpoints/
    vae: models/vae/
    controlnet: models/controlnet/
```

or

```
my_custom_config:
    base_path: YOUR_PATH/models/
    loras: loras
    checkpoints: checkpoints
    vae: vae
    controlnet: controlnet
```

<Warning>
  For the desktop version, please add the configuration to the existing configuration path without overwriting the path configuration generated during installation. Please back up the corresponding file before modification, so that you can restore it when you make a mistake.
</Warning>

Or you can refer to the default [extra\_model\_paths.yaml.example](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) for more configuration options. After saving, you need to **restart ComfyUI** for the changes to take effect.

Below is the original config example:

```yaml  theme={null}
#Rename this to extra_model_paths.yaml and ComfyUI will load it


#config for a1111 ui
#all you have to do is change the base_path to where yours is installed
a111:
    base_path: path/to/stable-diffusion-webui/

    checkpoints: models/Stable-diffusion
    configs: models/Stable-diffusion
    vae: models/VAE
    loras: |
         models/Lora
         models/LyCORIS
    upscale_models: |
                  models/ESRGAN
                  models/RealESRGAN
                  models/SwinIR
    embeddings: embeddings
    hypernetworks: models/hypernetworks
    controlnet: models/ControlNet

#config for comfyui
#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.

#comfyui:
#     base_path: path/to/comfyui/
#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads
#     #is_default: true
#     checkpoints: models/checkpoints/
#     clip: models/clip/
#     clip_vision: models/clip_vision/
#     configs: models/configs/
#     controlnet: models/controlnet/
#     diffusion_models: |
#                  models/diffusion_models
#                  models/unet
#     embeddings: models/embeddings/
#     loras: models/loras/
#     upscale_models: models/upscale_models/
#     vae: models/vae/

#other_ui:
#    base_path: path/to/ui
#    checkpoints: models/checkpoints
#    gligen: models/gligen
#    custom_nodes: path/custom_nodes

```

For example, if your WebUI is located at `D:\stable-diffusion-webui\`, you can modify the corresponding configuration to

```yaml  theme={null}
a111:
    base_path: D:\stable-diffusion-webui\
    checkpoints: models/Stable-diffusion
    configs: models/Stable-diffusion
    vae: models/VAE
    loras: |
         models/Lora
         models/LyCORIS
    upscale_models: |
                  models/ESRGAN
                  models/RealESRGAN
                  models/SwinIR
    embeddings: embeddings
    hypernetworks: models/hypernetworks
    controlnet: models/ControlNet
```

### Add Extra Custom Nodes Path

Besides adding external models, you can also add custom nodes paths that are not in the default path of ComfyUI

<Tip>
  Please note that this will not change the default installation path of custom nodes, but will add an extra path search when starting ComfyUI. You still need to complete the installation of custom node dependencies in the corresponding environment to ensure the integrity of the running environment.
</Tip>

Below is a simple configuration example (MacOS), please modify it according to your actual situation and add it to the corresponding configuration file, save it and restart ComfyUI for the changes to take effect:

```yaml  theme={null}
my_custom_nodes:
  custom_nodes: /Users/your_username/Documents/extra_custom_nodes
```

## Desktop Python Environment

The desktop installation will create a Python virtual environment in your chosen installation directory, typically a hidden `.venv` folder.

If you need to handle dependencies for ComfyUI plugins, you'll need to do so within this environment. Using the system command line directly risks installing dependencies to the system environment, so please follow the instructions below to activate the appropriate environment.

### How to use the Desktop Python environment?

<Tabs>
  <Tab title="Desktop (Recommended)">
    You can use the built-in terminal in the desktop app to access the Python environment.
    <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_terminal.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=e82c9f778818d0d33ef7518c536917b6" alt="ComfyUI Desktop Terminal" data-og-width="2782" width="2782" data-og-height="1676" height="1676" data-path="images/desktop/desktop_terminal.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_terminal.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=dd8c4c7645b64b2ac22c7f8af2e62c82 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_terminal.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=eac1e917bb83fc7f0e62d6c207d303ba 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_terminal.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=0ff3ee1451664c14e847068b9441a3cd 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_terminal.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=2a5c7e52869e83da9fa03d5744a01965 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_terminal.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=ee349a694537302ed8ca2baef5918b8e 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_terminal.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=c92df81e233c9a35e996e6384d172c89 2500w" />

    1. Click the icon in the menu bar to open the bottom panel
    2. Click `Terminal` to open the terminal
    3. If you want to check the Python installation location for the corresponding environment, you can use the following command

    <Tabs>
      <Tab title="Windows">
        ```
          python -c "import sys; print(sys.executable)"
        ```
      </Tab>

      <Tab title="macOS">
        ```
        which python
        ```
      </Tab>
    </Tabs>

    <Warning>
      Unless you understand the meaning of your current operations, your actions may cause dependency issues in the corresponding environment. Please use this method with caution.
    </Warning>
  </Tab>

  <Tab title="Terminal">
    You can also use your preferred terminal to access the Python environment, but you'll need to activate the virtual environment first.

    <Warning>
      When using other terminals, if you're not familiar with the operations, you may accidentally install dependencies to the system environment. Please use this method with caution.
    </Warning>

        <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/terminal.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=97cbadfa7c28f672be4eba46297a6022" alt="Using system to activate environment" data-og-width="2620" width="2620" data-og-height="812" height="812" data-path="images/desktop/terminal.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/terminal.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=adaa18b30de61dcf3890bc357e040c56 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/terminal.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=7fc0e0be45991b3d40c0d80aa117ebbf 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/terminal.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=38ebbe268f7aec62d1897f473be52b94 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/terminal.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=f92065af206e8e4bc75e87873c31e5ab 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/terminal.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=65ff8e5ce0748cf1af9a2ccdcf8e9b37 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/terminal.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=c6f7db7fc2ba3497637e39450107b595 2500w" />

    <Warning>
      In the screenshot, it shows a macOS terminal. If you are using Windows, please refer to the following steps to activate the virtual environment on your system.
    </Warning>

    <Steps>
      <Step title="Open Terminal">
        Open your preferred terminal and use the `cd` command to navigate to your ComfyUI installation directory, for example:

        ```
        cd <your comfyui install directory>/ComfyUI
        ```
      </Step>

      <Step title="Activate Virtual Environment">
        Type the following command in the terminal to activate the virtual environment

        <Tabs>
          <Tab title="Windows">
            ```
            .venv/Scripts/activate
            ```
          </Tab>

          <Tab title="macOS">
            ```
            source .venv/bin/activate
            ```
          </Tab>
        </Tabs>

        After activation, you should see a prompt like `(ComfyUI)` in the terminal, indicating that you've activated the virtual environment.
      </Step>

      <Step title="Confirm Current Python Environment">
        Use `which python` to check the current Python installation location and confirm it's not the system environment.
      </Step>

      After completing these steps, you've activated the corresponding Python environment, and you can continue to perform dependency installation operations in this environment.
    </Steps>
  </Tab>
</Tabs>

## How to Uninstall ComfyUI Desktop

For **ComfyUI Desktop**, you can directly delete **ComfyUI** from the **Applications** folder

If you want to completely remove all **ComfyUI Desktop** files, you can manually delete these folders:

```
~/Library/Application Support/ComfyUI
```

The above operations will not delete your following folders. If you need to delete corresponding files, please delete manually:

* models files
* custom nodes
* input/output directories

## Troubleshooting

### Error identification

If installation fails, you should see the following screen

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-7.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=1b5506d072a12fec06e4e8109c07a81f" alt="ComfyUI Installation Failed" data-og-width="1514" width="1514" data-og-height="1103" height="1103" data-path="images/desktop/win-comfyui-desktop-7.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-7.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=4a3b8bc500b1469c23b4e30fcfbd36af 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-7.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=7c5bae6de3fea5d44ebb66d0ad281cb4 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-7.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=49ae6c1de01b136e0da09938f24e4f4a 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-7.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=0d7c698905502cfd34d159947c177959 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-7.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=2a1e733756e94307541eee35a9f1eb48 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-7.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=b4c1159b58a121228ac08e07eb0d2ca0 2500w" />

It is recommended to take these steps to find the error cause:

1. Click `Show Terminal` to view error output
2. Click `Open Logs` to view installation logs
3. Visit official forum to search for error reports
4. Click `Reinstall` to try reinstalling

Before submitting feedback, it's recommended to provide the **error output** and **log files** to tools like **GPT**

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-8.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=8775711eafb69c06fd88c068d457d621" alt="ComfyUI Installation Failed - Error Log" data-og-width="1514" width="1514" data-og-height="1142" height="1142" data-path="images/desktop/win-comfyui-desktop-8.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-8.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=b3853195bb8599f8e620dba4b7114adb 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-8.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=a8d720eea46760fadc849f19b6c2512c 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-8.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=81c7f420f2882f76da456df6ba22aaa5 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-8.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=b2a08cedae959954fa063649352ef00e 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-8.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=b64d43de6e3f62ea1f26da5e16e8b596 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-8.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=d27bd134c83d62629986a9cbde62b707 2500w" />
<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-9.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=cac5f2c54db6f17fde50029a8fcfb315" alt="ComfyUI Installation Failed - GPT Feedback" data-og-width="1514" width="1514" data-og-height="1649" height="1649" data-path="images/desktop/win-comfyui-desktop-9.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-9.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=16ecdec9dee3d9d55167ae26c7e8683b 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-9.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=67525d548b448d8b01d7cffe0ccf1a82 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-9.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=2a722ef5654dcd3b9dc2f5da36077121 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-9.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=c6c9aa98d5c10df15723b87e52826dc1 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-9.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=51ea56d56fc0e764a443515739d59c8d 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-9.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=5c0d9cd9cf21c16c5d0caa84bdefebd6 2500w" />

As shown above, ask the GPT for the cause of the corresponding error, or remove ComfyUI completely and retry the installation.

### Feedback Installation Failure

If you encounter any errors during installation, please check if there are similar error reports or submit errors to us through:

* Github Issues: [https://github.com/Comfy-Org/desktop/issues](https://github.com/Comfy-Org/desktop/issues)
* Comfy Official Forum: [https://forum.comfy.org/](https://forum.comfy.org/)

When submitting error reports, please ensure you include the following logs and configuration files to help us locate and investigate the issue:

1. Log Files

| Filename    | Description                                                                                     | Location     |
| ----------- | ----------------------------------------------------------------------------------------------- | ------------ |
| main.log    | Contains logs related to desktop application and server startup from the Electron process       | {log_path_0} |
| comfyui.log | Contains logs related to ComfyUI normal operation, such as core ComfyUI process terminal output | {log_path_0} |

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-10-logs.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=b3d3056cd0203c90712562126a5c204d" alt="ComfyUI Log Files Location" data-og-width="1527" width="1527" data-og-height="987" height="987" data-path="images/desktop/win-comfyui-desktop-10-logs.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-10-logs.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=aa2a640d0250582bbf4f5c34fe8ad8c0 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-10-logs.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=26647edc8867c1690b6b0d3f3c0c03ce 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-10-logs.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=891c9b6f0ec0bc7ee9147da24710ce1c 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-10-logs.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=2e74c76124371b826f0c8187cdbb2643 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-10-logs.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=bd30a2c10ead50f539c4c31f7316e306 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-10-logs.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=45c116fa61178ae841afe93dd94a9c72 2500w" />

2. Configuration Files

| Filename                 | Description                                                                     | Location        |
| ------------------------ | ------------------------------------------------------------------------------- | --------------- |
| extra\_model\_paths.yaml | Contains additional paths where ComfyUI will search for models and custom nodes | {config_path_0} |
| config.json              | Contains application configuration. This file should not be edited directly     | {config_path_0} |

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-11-config.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=82fac333832e9222b4534b35131f856d" alt="ComfyUI Config Files Location" data-og-width="1527" width="1527" data-og-height="987" height="987" data-path="images/desktop/win-comfyui-desktop-11-config.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-11-config.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=4391bf98168ae8cc7ba4e11b6d5f6b86 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-11-config.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=35a70d7105699657f74ef7797b520947 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-11-config.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=0a21a2c57d59b28159d0f43982badc77 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-11-config.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=e23b1a2612b2eb552eb6e1941ce446b2 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-11-config.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=3bc2df5c167269238b188f5fd186740d 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-11-config.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=643f71bd078606d366b131255ac6b9a4 2500w" />


# Windows Desktop Version
Source: https://docs.comfy.org/installation/desktop/windows

This article introduces how to download, install and use ComfyUI Desktop for Windows

export const log_path_0 = "C:\\Users\\<YOUR_USERNAME>\\AppData\\Roaming\\ComfyUI\\logs"

export const config_path_0 = "C:\\Users\\<YOUR_USERNAME>\\AppData\\Roaming\\ComfyUI"

**ComfyUI Desktop** is a standalone installation version that can be installed like regular software. It supports quick installation and automatic configuration of the **Python environment and dependencies**, and supports one-click import of existing ComfyUI settings, models, workflows, and files. You can quickly migrate from an existing [ComfyUI Portable version](/installation/comfyui_portable_windows) to the Desktop version.

ComfyUI Desktop is an open source project, please visit the full code [here](https://github.com/Comfy-Org/desktop)

ComfyUI Desktop hardware requirements:

* NVIDIA GPU

This tutorial will guide you through the software installation process and explain related configuration details.

<Warning>As **ComfyUI Desktop** is still in **Beta** status, the actual installation process may change</Warning>

## ComfyUI Desktop (Windows) Download

Please click the button below to download the installation package for Windows **ComfyUI Desktop**

<a className="prose" href="https://download.comfy.org/windows/nsis/x64" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download for Windows (NVIDIA)</p>
</a>

## ComfyUI Desktop Installation Steps

Double-click the downloaded installation package file, which will first perform an automatic installation and create a **ComfyUI Desktop** shortcut on the desktop

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-shortcut.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=91d39867b16a2c5eb803fe2f26cee5bf" alt="ComfyUI logo" data-og-width="731" width="731" data-og-height="486" height="486" data-path="images/desktop/win-comfyui-desktop-shortcut.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-shortcut.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=9875d7185aebb25f478f1cb94571577c 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-shortcut.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=01c8b6ba2b24ae54115a3b92667b479a 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-shortcut.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=04f19ebfbab1e9726de3bad4e31897ec 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-shortcut.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=619e79aab66bd8480a6afe25f37f9350 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-shortcut.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=031f8da7cb83b0feca9318fd457f4fd7 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-shortcut.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=8083e0b55c8c313c4b26b311f868ae6d 2500w" />

Double-click the corresponding shortcut to enter ComfyUI initialization settings

### ComfyUI Desktop Initialization Process

<Steps>
  <Step title="Start Screen">
    <Tabs>
      <Tab title="Normal Start">
                <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-1.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=778c2d6e0d08a961278a5d0f917221d4" alt="ComfyUI Installation Steps - Start" data-og-width="1514" width="1514" data-og-height="1139" height="1139" data-path="images/desktop/win-comfyui-desktop-1.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-1.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=5ce5b27d7ef8c915cf92fa225b1744f4 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-1.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=0c30ea772deb01623140ad0471567b2b 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-1.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=ba471670151c04fb3fbbbcd087946a6b 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-1.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=59102ff039ce2a93492b4de598de3025 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-1.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=0fe3497ddf26ef941457be73f7de7c6c 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-1.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=532407419f6d37ce88536749c29321af 2500w" />

        Click **Get Started** to begin initialization
      </Tab>

      <Tab title="Maintenance Page">
        There are many reasons you might have issues installing ComfyUI. Maybe a network connection failed when installing pytorch (15 GB). Or you dont have git installed. The maintenance page automatically opens when it detects an issue and provides a way to resolve the issue.

        You can use it to resolve most issues:

        * Create a python virtual environment
        * Reinstall all missing core dependencies to your Python virtual environment thats managed by Desktop
        * Install git, VC redis
        * Choose a new install location

        The default maintenance page displays the current error content

                <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/maintenance-1.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=2a01daff7808d4d342aeb423a072308f" alt="ComfyUI Maintenance Page" data-og-width="971" width="971" data-og-height="715" height="715" data-path="images/desktop/maintenance-1.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/maintenance-1.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=6e97afdce4ad87e3a8d0c9149cb346f6 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/maintenance-1.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=83696424cc1d361d118feab76c7daeea 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/maintenance-1.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=140c31b7000ab635c147d40899a1477b 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/maintenance-1.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=b6298c53bd547097ea9813245138f6e5 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/maintenance-1.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=8c822e6822e839ca25df3d15646e4f02 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/maintenance-1.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=bd4e3689bf4b0bcf01a7870457f151b2 2500w" />

        Clicking `All` allows you to view all the content that can be operated on currently

                <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/maintenance-2.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=b987fe536f542650e7bd16bbb81a9b1a" alt="ComfyUI Maintenance Page" data-og-width="1213" width="1213" data-og-height="1028" height="1028" data-path="images/desktop/maintenance-2.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/maintenance-2.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=e7ab140496bcd19ccc0b42d881ddfeb2 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/maintenance-2.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=d713f7abe3e3f0e803014e23f8007c81 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/maintenance-2.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=c7889c0277f2c02c46849515686a09d0 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/maintenance-2.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=e5c2fcaddfe6e1f58e22f19342bd3cc8 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/maintenance-2.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=e9b8aee2ac65b14724dcfddb210d6cc7 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/maintenance-2.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=429efbb180c2ce8101751b0a27327bcd 2500w" />
      </Tab>
    </Tabs>
  </Step>

  <Step title="Select GPU">
        <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-2.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=a815bed3616548ac875a6ad832b0d69b" alt="ComfyUI Installation Steps - GPU Selection" data-og-width="1514" width="1514" data-og-height="1139" height="1139" data-path="images/desktop/win-comfyui-desktop-2.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-2.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=7d72433d5f0ea8f617d2c14ec70e0291 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-2.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=a4607a964fcc3fb923d13f8c65080b11 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-2.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=64796c4b32b560385161a5bda8764165 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-2.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=36e0b916207c332d7585673506ab132b 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-2.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=7fd7b5977c69486c65da5d58d577f14a 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-2.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=00ee3e6fbf9d7768cdd0531f438438ee 2500w" />

    The three options are:

    1. **Nvidia GPU (Recommended):** Direct support for pytorch and CUDA
    2. **Manual Configuration:** You need to manually install and configure the python runtime environment. Don't select this unless you know how to configure
    3. **Enable CPU Mode:** For developers and special cases only. Don't select this unless you're sure you need it

    Unless there are special circumstances, please select **NVIDIA** as shown and click **Next** to proceed
  </Step>

  <Step title="Install location">
        <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-3.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=040fa19f9a519d4b96d23b494f2ff8c3" alt="ComfyUI Installation Steps - Installation Location" data-og-width="1514" width="1514" data-og-height="1103" height="1103" data-path="images/desktop/win-comfyui-desktop-3.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-3.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=2afa0a87a9afa73f172963aa9c09aa51 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-3.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=cf2bf3c3412a5f8dd66125d25ce818a7 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-3.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=565f8fbd682c8985af3665e0dcc01504 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-3.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=9e54a85c095c9f5ebebddb8d52b2872a 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-3.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=8f41c3c89edb4b428c349e51084e7131 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-3.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=53ffc127a3f90cdab84bcfd657b6e81e 2500w" />

    In this step, you will select the installation location for the following ComfyUI content:

    * **Python Environment**
    * **Models Model Files**
    * **Custom Nodes Custom Nodes**

    Recommendations:

    * Please select a **solid-state drive** as the installation location, which will increase ComfyUI's performance when accessing models.
    * Please create a separate empty folder as the ComfyUI installation directory
    * Please ensure that the corresponding disk has at least around **15G** of disk space to ensure the installation of ComfyUI Desktop

    <Note>Not all files are installed in this directory, some files will still be installed on the C drive, and if you need to uninstall in the future, you can refer to the uninstallation section of this guide to complete the full uninstallation of ComfyUI Desktop</Note>

    After completing this step, click **Next** to proceed to the next step
  </Step>

  <Step title="Migrate from Existing Installation (Optional)">
        <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-4.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=d86e70017f3266e2d8ab290f50814967" alt="ComfyUI Installation Steps - File Migration" data-og-width="1514" width="1514" data-og-height="1103" height="1103" data-path="images/desktop/win-comfyui-desktop-4.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-4.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=9ee204a05ed48a0bdd2b91c34e0549c5 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-4.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=bca0e1f733cc9f1f3645264974699ce0 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-4.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=9db8c811a3610b50d221c9dc827dd260 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-4.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=c5d8407e6eca0f2557dceddc8ef6f388 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-4.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=16880b50c70fd120210951b1cc3fc2b1 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-4.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=e488e02c4b2c099589d1782393af0d10 2500w" />

    In this step you can migrate your existing ComfyUI installation content to ComfyUI Desktop. As shown, I selected my original **D:\ComfyUI\_windows\_portable\ComfyUI** installation directory. The installer will automatically recognize:

    * **User Files**
    * **Models:** Will not be copied, only linked with desktop version
    * **Custom Nodes:** Nodes will be reinstalled

    Don't worry, this step won't copy model files. You can check or uncheck options as needed. Click **Next** to continue
  </Step>

  <Step title="Desktop Settings">
        <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-5.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=daa7e4b3ea6d11d6a157538d3827f34f" alt="ComfyUI Installation Steps - Desktop Settings" data-og-width="1514" width="1514" data-og-height="1103" height="1103" data-path="images/desktop/win-comfyui-desktop-5.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-5.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=fe9d3907ee9e8b498913391b293f87c8 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-5.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=8f7ea827443427b622e692730ecc2b74 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-5.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=de05dc4878a81d7a1697cef7b7b34cb1 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-5.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=ce780d5f2776d759518b5bd7e78b6929 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-5.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=bcf6d7f123edf6e6b2c81707171c7579 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-5.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=135d0a9b4a86d366ec0bc8343ce2aa3d 2500w" />

    These are preference settings:

    1. **Automatic Updates:** Whether to set automatic updates when ComfyUI updates are available
    2. **Usage Metrics:** If enabled, we will collect **anonymous usage data** to help improve ComfyUI
    3. **Mirror Settings:** Since the program needs internet access to download Python and complete environment installation, if you see a red  during installation indicating this may cause installation failure, please follow the steps below

    <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-6.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=a32fdb0f21efef4aeca85590d019ee38" alt="ComfyUI Installation Steps - Mirror Settings" data-og-width="1514" width="1514" data-og-height="1103" height="1103" data-path="images/desktop/win-comfyui-desktop-6.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-6.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=7e3f90c9fe0b52d33df0e3f1529f0b8c 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-6.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=54975797e051816991290d2e8c232177 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-6.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=00d77be238cd352fbbeefe9181ded69d 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-6.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=731c509d1e8cc4873f53a5321b005434 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-6.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=5ebdd372eef75c4a9dbc805d69cde27f 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-6.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=158391fa1775c73e2fcfa8c6d7bbf3e4 2500w" />
    Expand the mirror settings to find the specific failing mirror. In this screenshot the error is **Python Install Mirror** failure.

    For different mirror errors, you can refer to the following content to try to manually find different mirrors and replace them

    The following cases mainly apply to users in China.

    #### Python Installation Mirror

    If the default mirror is unavailable, please try using the mirror below.

    ```
    https://python-standalone.org/mirror/astral-sh/python-build-standalone
    ```

    If you need to find other alternative GitHub mirror addresses, please look for and construct a mirror address pointing to the releases of the `python-build-standalone` repository.

    ```
    https://github.com/astral-sh/python-build-standalone/releases/download
    ```

    Build a link in the following pattern

    ```
    https://xxx/astral-sh/python-build-standalone/releases/download
    ```

    <info>Since most of the Github mirror services are provided by third parties, please pay attention to the security during use.</info>

    #### PyPI Mirror

    * Alibaba Cloud: [https://mirrors.aliyun.com/pypi/simple/](https://mirrors.aliyun.com/pypi/simple/)
    * Tencent Cloud: [https://mirrors.cloud.tencent.com/pypi/simple/](https://mirrors.cloud.tencent.com/pypi/simple/)
    * University of Science and Technology of China: [https://pypi.mirrors.ustc.edu.cn/simple/](https://pypi.mirrors.ustc.edu.cn/simple/)
    * Shanghai Jiao Tong University: [https://pypi.sjtu.edu.cn/simple/](https://pypi.sjtu.edu.cn/simple/)

    #### Torch Mirror

    * Aliyun: [https://mirrors.aliyun.com/pytorch-wheels/cu121/](https://mirrors.aliyun.com/pytorch-wheels/cu121/)
  </Step>

  <Step title="Complete the installation">
    If everything is correct, the installer will complete and automatically enter the ComfyUI Desktop interface, then the installation is successful
    <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-interface.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=8a86c18801fbede950872e2a26dd4381" alt="ComfyUI Desktop Interface" data-og-width="1678" width="1678" data-og-height="980" height="980" data-path="images/desktop/comfyui-interface.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-interface.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=efd4befd3f4105de49196f4e568f9dc5 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-interface.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=8d3655f1c724ddfd458d0bc325134399 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-interface.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=4c4bd4537cb38b3de785da3fc375b9da 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-interface.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=ed366119da482d7b4c855cea65f0b0d8 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-interface.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=751c5c07a0a4c01dc96cfce7356b5673 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-interface.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=4e86835b96ef55a16595ffdcd6b5cf3e 2500w" />
  </Step>
</Steps>

## First Image Generation

After successful installation, you can refer to the section below to start your ComfyUI journey\~

<Card title="First Image Generation" icon="link" href="/get_started/first_generation">
  This tutorial will guide you through your first model installation and text-to-image generation
</Card>

## How to Update ComfyUI Desktop

Currently, ComfyUI Desktop updates use automatic detection updates, please ensure that automatic updates are enabled in the settings

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-desktop-update-setting.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=5006c9c0e4ad3e4f6ca0cef51132fbf3" alt="ComfyUI Desktop Settings" data-og-width="1065" width="1065" data-og-height="815" height="815" data-path="images/desktop/comfyui-desktop-update-setting.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-desktop-update-setting.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=25f0e90e8df014d0640205bb73e4253c 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-desktop-update-setting.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=0137407c528ac79439d4320701e4c0b1 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-desktop-update-setting.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=73f85bd7cbbeeeaeabad54154b60af7e 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-desktop-update-setting.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=4f880973b712a83849b16a78ce5868f5 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-desktop-update-setting.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=174dc6b670118f452834cece0df52dbd 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-desktop-update-setting.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=1b72afe13029997c797c16772daeff98 2500w" />

You can also choose to manually check for available updates in the `Menu` --> `Help` --> `Check for Updates`

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_check_for_updates.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=0f2bf5d6ad204e762ed858f29df4282c" alt="ComfyUI Desktop Check for Updates" data-og-width="415" width="415" data-og-height="477" height="477" data-path="images/desktop/desktop_check_for_updates.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_check_for_updates.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=c8f28a7f5ef143b0ca31e839187a5a65 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_check_for_updates.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=ffef0bd3b488b1545c0390d65ffbd445 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_check_for_updates.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=e46efe596b9b5b7f38da68489c245e4c 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_check_for_updates.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=9dd9460b2daafd8b398145e1961d668c 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_check_for_updates.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=218f1e5e502286b7a90e21d5f60100fc 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_check_for_updates.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=f7e03d947a487ad2a5624f8d14c68e40 2500w" />

## Adding Extra Model Paths

If you want to manage your model files outside of `ComfyUI/models`, you may have the following reasons:

* You have multiple ComfyUI instances and want them to share model files to save disk space
* You have different types of GUI programs (such as WebUI) and want them to use the same model files
* Model files cannot be recognized or found

We provide a way to add extra model search paths via the `extra_model_paths.yaml` configuration file

### Open Config File

<Tabs>
  <Tab title="Portable/Manual Install">
    For the ComfyUI version such as [portable](/installation/comfyui_portable_windows) and [manual](/installation/manual_install), you can find an example file named `extra_model_paths.yaml.example` in the root directory of ComfyUI:

    ```
    ComfyUI/extra_model_paths.yaml.example
    ```

    Copy and rename it to `extra_model_paths.yaml` for use. Keep it in ComfyUI's root directory at `ComfyUI/extra_model_paths.yaml`.
    You can also find the config example file [here](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example)
  </Tab>

  <Tab title="ComfyUI Desktop">
    If you are using the [ComfyUI Desktop](/installation/desktop/windows) application, you can follow the image below to open the extra model config file:

        <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=b2a08255f65a32e3da0c018a3cebb6a2" alt="Open Config File" data-og-width="1056" width="1056" data-og-height="1166" height="1166" data-path="images/desktop/extra_model_paths.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=4f4044b7cef96f0f4a70e7d8a0eb007a 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=aa576e31096b7306e810c10c97416ad5 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=ff8461eb13a9cec15293e7c5663bc65c 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=a2bf5a5335edfa58f37652bfed7a7dc9 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=a1a4c2d6fd021f4a348ea95c4abb5871 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=f508c6130dd86ec9adbef19aaf0d2530 2500w" />

    Or open it directly at:

    <Tabs>
      <Tab title="Windows">
        ```
        C:\Users\YourUsername\AppData\Roaming\ComfyUI\extra_models_config.yaml
        ```
      </Tab>

      <Tab title="macOS">
        ```
        ~/Library/Application Support/ComfyUI/extra_models_config.yaml
        ```
      </Tab>
    </Tabs>

    You should keep the file in the same directory, should not move these files to other places.
  </Tab>
</Tabs>

If the file does not exist, you can create it yourself with any text editor.

### Example Structure

Suppose you want to add the following model paths to ComfyUI:

```
 YOUR_PATH/
   models/
  |     lora/
  |       xxxxx.safetensors
  |     checkpoints/
  |       xxxxx.safetensors
  |     vae/
  |       xxxxx.safetensors
  |     controlnet/
  |        xxxxx.safetensors
```

Then you can configure the `extra_model_paths.yaml` file like below to let ComfyUI recognize the model paths on your device:

```
my_custom_config:
    base_path: YOUR_PATH
    loras: models/loras/
    checkpoints: models/checkpoints/
    vae: models/vae/
    controlnet: models/controlnet/
```

or

```
my_custom_config:
    base_path: YOUR_PATH/models/
    loras: loras
    checkpoints: checkpoints
    vae: vae
    controlnet: controlnet
```

<Warning>
  For the desktop version, please add the configuration to the existing configuration path without overwriting the path configuration generated during installation. Please back up the corresponding file before modification, so that you can restore it when you make a mistake.
</Warning>

Or you can refer to the default [extra\_model\_paths.yaml.example](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) for more configuration options. After saving, you need to **restart ComfyUI** for the changes to take effect.

Below is the original config example:

```yaml  theme={null}
#Rename this to extra_model_paths.yaml and ComfyUI will load it


#config for a1111 ui
#all you have to do is change the base_path to where yours is installed
a111:
    base_path: path/to/stable-diffusion-webui/

    checkpoints: models/Stable-diffusion
    configs: models/Stable-diffusion
    vae: models/VAE
    loras: |
         models/Lora
         models/LyCORIS
    upscale_models: |
                  models/ESRGAN
                  models/RealESRGAN
                  models/SwinIR
    embeddings: embeddings
    hypernetworks: models/hypernetworks
    controlnet: models/ControlNet

#config for comfyui
#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.

#comfyui:
#     base_path: path/to/comfyui/
#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads
#     #is_default: true
#     checkpoints: models/checkpoints/
#     clip: models/clip/
#     clip_vision: models/clip_vision/
#     configs: models/configs/
#     controlnet: models/controlnet/
#     diffusion_models: |
#                  models/diffusion_models
#                  models/unet
#     embeddings: models/embeddings/
#     loras: models/loras/
#     upscale_models: models/upscale_models/
#     vae: models/vae/

#other_ui:
#    base_path: path/to/ui
#    checkpoints: models/checkpoints
#    gligen: models/gligen
#    custom_nodes: path/custom_nodes

```

For example, if your WebUI is located at `D:\stable-diffusion-webui\`, you can modify the corresponding configuration to

```yaml  theme={null}
a111:
    base_path: D:\stable-diffusion-webui\
    checkpoints: models/Stable-diffusion
    configs: models/Stable-diffusion
    vae: models/VAE
    loras: |
         models/Lora
         models/LyCORIS
    upscale_models: |
                  models/ESRGAN
                  models/RealESRGAN
                  models/SwinIR
    embeddings: embeddings
    hypernetworks: models/hypernetworks
    controlnet: models/ControlNet
```

### Add Extra Custom Nodes Path

Besides adding external models, you can also add custom nodes paths that are not in the default path of ComfyUI

<Tip>
  Please note that this will not change the default installation path of custom nodes, but will add an extra path search when starting ComfyUI. You still need to complete the installation of custom node dependencies in the corresponding environment to ensure the integrity of the running environment.
</Tip>

Below is a simple configuration example (MacOS), please modify it according to your actual situation and add it to the corresponding configuration file, save it and restart ComfyUI for the changes to take effect:

```yaml  theme={null}
my_custom_nodes:
  custom_nodes: /Users/your_username/Documents/extra_custom_nodes
```

## Desktop Python Environment

The desktop installation will create a Python virtual environment in your chosen installation directory, typically a hidden `.venv` folder.

If you need to handle dependencies for ComfyUI plugins, you'll need to do so within this environment. Using the system command line directly risks installing dependencies to the system environment, so please follow the instructions below to activate the appropriate environment.

### How to use the Desktop Python environment?

<Tabs>
  <Tab title="Desktop (Recommended)">
    You can use the built-in terminal in the desktop app to access the Python environment.
    <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_terminal.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=e82c9f778818d0d33ef7518c536917b6" alt="ComfyUI Desktop Terminal" data-og-width="2782" width="2782" data-og-height="1676" height="1676" data-path="images/desktop/desktop_terminal.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_terminal.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=dd8c4c7645b64b2ac22c7f8af2e62c82 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_terminal.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=eac1e917bb83fc7f0e62d6c207d303ba 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_terminal.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=0ff3ee1451664c14e847068b9441a3cd 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_terminal.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=2a5c7e52869e83da9fa03d5744a01965 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_terminal.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=ee349a694537302ed8ca2baef5918b8e 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_terminal.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=c92df81e233c9a35e996e6384d172c89 2500w" />

    1. Click the icon in the menu bar to open the bottom panel
    2. Click `Terminal` to open the terminal
    3. If you want to check the Python installation location for the corresponding environment, you can use the following command

    <Tabs>
      <Tab title="Windows">
        ```
          python -c "import sys; print(sys.executable)"
        ```
      </Tab>

      <Tab title="macOS">
        ```
        which python
        ```
      </Tab>
    </Tabs>

    <Warning>
      Unless you understand the meaning of your current operations, your actions may cause dependency issues in the corresponding environment. Please use this method with caution.
    </Warning>
  </Tab>

  <Tab title="Terminal">
    You can also use your preferred terminal to access the Python environment, but you'll need to activate the virtual environment first.

    <Warning>
      When using other terminals, if you're not familiar with the operations, you may accidentally install dependencies to the system environment. Please use this method with caution.
    </Warning>

        <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/terminal.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=97cbadfa7c28f672be4eba46297a6022" alt="Using system to activate environment" data-og-width="2620" width="2620" data-og-height="812" height="812" data-path="images/desktop/terminal.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/terminal.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=adaa18b30de61dcf3890bc357e040c56 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/terminal.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=7fc0e0be45991b3d40c0d80aa117ebbf 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/terminal.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=38ebbe268f7aec62d1897f473be52b94 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/terminal.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=f92065af206e8e4bc75e87873c31e5ab 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/terminal.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=65ff8e5ce0748cf1af9a2ccdcf8e9b37 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/terminal.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=c6f7db7fc2ba3497637e39450107b595 2500w" />

    <Warning>
      In the screenshot, it shows a macOS terminal. If you are using Windows, please refer to the following steps to activate the virtual environment on your system.
    </Warning>

    <Steps>
      <Step title="Open Terminal">
        Open your preferred terminal and use the `cd` command to navigate to your ComfyUI installation directory, for example:

        ```
        cd <your comfyui install directory>/ComfyUI
        ```
      </Step>

      <Step title="Activate Virtual Environment">
        Type the following command in the terminal to activate the virtual environment

        <Tabs>
          <Tab title="Windows">
            ```
            .venv/Scripts/activate
            ```
          </Tab>

          <Tab title="macOS">
            ```
            source .venv/bin/activate
            ```
          </Tab>
        </Tabs>

        After activation, you should see a prompt like `(ComfyUI)` in the terminal, indicating that you've activated the virtual environment.
      </Step>

      <Step title="Confirm Current Python Environment">
        Use `which python` to check the current Python installation location and confirm it's not the system environment.
      </Step>

      After completing these steps, you've activated the corresponding Python environment, and you can continue to perform dependency installation operations in this environment.
    </Steps>
  </Tab>
</Tabs>

## How to Uninstall ComfyUI Desktop

For **ComfyUI Desktop** you can use the system uninstall function in Windows Settings to complete software uninstallation

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-uninstall-comfyui.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=a73f165286a128bb0caac36396a68732" alt="ComfyUI Desktop Uninstallation" data-og-width="1389" width="1389" data-og-height="1008" height="1008" data-path="images/desktop/win-uninstall-comfyui.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-uninstall-comfyui.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=5aaa0b596eb6f6a89046d79013258c8c 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-uninstall-comfyui.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=5a30e2117bea46c000d98d02f4da3bfe 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-uninstall-comfyui.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=d9154a7e4cfadc2b04ef83146b163c2f 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-uninstall-comfyui.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=4c662269a76cbb5a14fa4edc25bc9f12 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-uninstall-comfyui.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=627a8eccb0bc0c71b4e1a1fd633a9a6d 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-uninstall-comfyui.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=3496b1cf52896dcd082de390880435ba 2500w" />

If you want to completely remove all **ComfyUI Desktop** files, you can manually delete these folders:

* C:\Users\<YOUR\_USERNAME>\AppData\Local\@comfyorgcomfyui-electron-updater
* C:\Users\<YOUR\_USERNAME>\AppData\Local\Programs\@comfyorgcomfyui-electron
* C:\Users\<YOUR\_USERNAME>\AppData\Roaming\ComfyUI

The above operations will not delete your following folders. If you need to delete corresponding files, please delete manually:

* models files
* custom nodes
* input/output directories

## Troubleshooting

### Display unsupported devices

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-0.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=b698b91cdff0cca7232fa501f02d0cec" alt="ComfyUI Installation Steps - Unsupported Device" data-og-width="1646" width="1646" data-og-height="1070" height="1070" data-path="images/desktop/win-comfyui-desktop-0.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-0.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=1ca5cf563dc51b4c4e394bebfdfe008a 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-0.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=1e2d44b417e0b06763b23b42629efd9c 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-0.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=677fcc1cf15e7ff2035fe02f021a9a76 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-0.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=084e4529992f5e5cddb6373176a32594 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-0.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=63c01acbc8db2cb8191c2a0f5d9131df 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-0.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=1344ae04a083b4f841bdf62debe5862b 2500w" />

Since ComfyUI Desktop (Windows) only supports **NVIDIA GPUs with CUDA**, you may see this screen if your device is not supported

* Please switch to a supported device
* Or consider using [ComfyUI Portable](/installation/comfyui_portable_windows) or through [manual installation](/installation/manual_install) to use ComfyUI

### Error identification

If installation fails, you should see the following screen

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-7.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=1b5506d072a12fec06e4e8109c07a81f" alt="ComfyUI Installation Failed" data-og-width="1514" width="1514" data-og-height="1103" height="1103" data-path="images/desktop/win-comfyui-desktop-7.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-7.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=4a3b8bc500b1469c23b4e30fcfbd36af 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-7.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=7c5bae6de3fea5d44ebb66d0ad281cb4 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-7.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=49ae6c1de01b136e0da09938f24e4f4a 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-7.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=0d7c698905502cfd34d159947c177959 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-7.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=2a1e733756e94307541eee35a9f1eb48 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-7.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=b4c1159b58a121228ac08e07eb0d2ca0 2500w" />

It is recommended to take these steps to find the error cause:

1. Click `Show Terminal` to view error output
2. Click `Open Logs` to view installation logs
3. Visit official forum to search for error reports
4. Click `Reinstall` to try reinstalling

Before submitting feedback, it's recommended to provide the **error output** and **log files** to tools like **GPT**

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-8.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=8775711eafb69c06fd88c068d457d621" alt="ComfyUI Installation Failed - Error Log" data-og-width="1514" width="1514" data-og-height="1142" height="1142" data-path="images/desktop/win-comfyui-desktop-8.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-8.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=b3853195bb8599f8e620dba4b7114adb 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-8.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=a8d720eea46760fadc849f19b6c2512c 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-8.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=81c7f420f2882f76da456df6ba22aaa5 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-8.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=b2a08cedae959954fa063649352ef00e 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-8.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=b64d43de6e3f62ea1f26da5e16e8b596 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-8.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=d27bd134c83d62629986a9cbde62b707 2500w" />
<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-9.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=cac5f2c54db6f17fde50029a8fcfb315" alt="ComfyUI Installation Failed - GPT Feedback" data-og-width="1514" width="1514" data-og-height="1649" height="1649" data-path="images/desktop/win-comfyui-desktop-9.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-9.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=16ecdec9dee3d9d55167ae26c7e8683b 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-9.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=67525d548b448d8b01d7cffe0ccf1a82 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-9.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=2a722ef5654dcd3b9dc2f5da36077121 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-9.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=c6c9aa98d5c10df15723b87e52826dc1 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-9.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=51ea56d56fc0e764a443515739d59c8d 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-9.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=5c0d9cd9cf21c16c5d0caa84bdefebd6 2500w" />

As shown above, ask the GPT for the cause of the corresponding error, or remove ComfyUI completely and retry the installation.

### Feedback Installation Failure

If you encounter any errors during installation, please check if there are similar error reports or submit errors to us through:

* Github Issues: [https://github.com/Comfy-Org/desktop/issues](https://github.com/Comfy-Org/desktop/issues)
* Comfy Official Forum: [https://forum.comfy.org/](https://forum.comfy.org/)

When submitting error reports, please ensure you include the following logs and configuration files to help us locate and investigate the issue:

1. Log Files

| Filename    | Description                                                                                     | Location     |
| ----------- | ----------------------------------------------------------------------------------------------- | ------------ |
| main.log    | Contains logs related to desktop application and server startup from the Electron process       | {log_path_0} |
| comfyui.log | Contains logs related to ComfyUI normal operation, such as core ComfyUI process terminal output | {log_path_0} |

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-10-logs.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=b3d3056cd0203c90712562126a5c204d" alt="ComfyUI Log Files Location" data-og-width="1527" width="1527" data-og-height="987" height="987" data-path="images/desktop/win-comfyui-desktop-10-logs.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-10-logs.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=aa2a640d0250582bbf4f5c34fe8ad8c0 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-10-logs.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=26647edc8867c1690b6b0d3f3c0c03ce 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-10-logs.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=891c9b6f0ec0bc7ee9147da24710ce1c 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-10-logs.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=2e74c76124371b826f0c8187cdbb2643 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-10-logs.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=bd30a2c10ead50f539c4c31f7316e306 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-10-logs.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=45c116fa61178ae841afe93dd94a9c72 2500w" />

2. Configuration Files

| Filename                 | Description                                                                     | Location        |
| ------------------------ | ------------------------------------------------------------------------------- | --------------- |
| extra\_model\_paths.yaml | Contains additional paths where ComfyUI will search for models and custom nodes | {config_path_0} |
| config.json              | Contains application configuration. This file should not be edited directly     | {config_path_0} |

<img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-11-config.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=82fac333832e9222b4534b35131f856d" alt="ComfyUI Config Files Location" data-og-width="1527" width="1527" data-og-height="987" height="987" data-path="images/desktop/win-comfyui-desktop-11-config.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-11-config.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=4391bf98168ae8cc7ba4e11b6d5f6b86 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-11-config.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=35a70d7105699657f74ef7797b520947 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-11-config.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=0a21a2c57d59b28159d0f43982badc77 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-11-config.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=e23b1a2612b2eb552eb6e1941ce446b2 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-11-config.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=3bc2df5c167269238b188f5fd186740d 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/win-comfyui-desktop-11-config.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=643f71bd078606d366b131255ac6b9a4 2500w" />


# How to Install Custom Nodes in ComfyUI
Source: https://docs.comfy.org/installation/install_custom_node

This guide will show you different methods to install custom nodes in ComfyUI

## What are Custom Nodes ?

Custom nodes are extensions for ComfyUI that add new functionality like advanced image processing, machine learning fine-tuning, color adjustments, and more. These community-developed nodes can significantly expand ComfyUI's core capabilities.

<Warning>
  Before installing custom nodes, it's important to review them carefully. Since ComfyUI is an open-source project, malicious plugins could potentially exploit custom nodes:

  1. Only install custom nodes from trusted authors and those commonly used by the community
  2. Understand the plugin's functionality before installing and avoid unknown sources to ensure system security
  3. Avoid installing obscure or suspicious plugins - unverified plugins may pose security risks that could lead to system compromise
</Warning>

All custom node installations require completing these two steps:

1. Clone the node code to the `ComfyUI/custom_nodes` directory
2. Install the required Python dependencies

This guide covers three installation methods. Here's a comparison of their pros and cons. While [ComfyUI Manager](https://github.com/Comfy-Org/ComfyUI-Manager) isn't yet part of the core dependencies, it will be in the future. We still provide other installation guides to meet different needs.

| Method                            | Advantages                                                                  | Disadvantages                                                                           |
| --------------------------------- | --------------------------------------------------------------------------- | --------------------------------------------------------------------------------------- |
| **ComfyUI Manager** (Recommended) | 1. Automated installation<br />2. Dependency handling<br />3. GUI interface | Cannot directly search for nodes not registered in the registry                         |
| **Git Clone**                     | Can install nodes not registered in the registry                            | 1. Requires Git knowledge<br />2. Manual dependency handling<br />3. Installation risks |
| **Repository ZIP Download**       | 1. No Git required<br />2. Manual control                                   | 1. Manual dependency handling<br />2. No version control<br />3. Installation risks     |

Tip: Before installing custom nodes, check the plugin's README file to understand installation methods, usage, and requirements like specific models, dependency versions, and common issue solutions.

## Method 1: ComfyUI Manager (Recommended)

<Steps>
  <Step title="Click the `Manager` button in ComfyUI interface">
        <img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-1.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=c9ea2c5459becf2073564c99d5918ebd" alt="Click ComfyUI Manager" data-og-width="2000" width="2000" data-og-height="1250" height="1250" data-path="images/installation/custom_nodes/install-custom-nodes-by-manager-1.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-1.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=7973b4e0e017ffb16d264d4231536aee 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-1.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=86d4657014cdf4b1cfbce57188ce5575 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-1.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=7788731e9e63f200a9f95aee0019ffb8 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-1.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=5f747b98ad3ca8a8c1a2d04cde58bd33 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-1.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=6ad5eacec17533604fa9150ada34d639 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-1.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=caacb306d394a006d78fd81cf2c0d231 2500w" />
  </Step>

  <Step title="Select `Install Custom Nodes`">
        <img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-2.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=51a2fb20415d6fc9a2efae30b5801e4f" alt="Select Install Custom Nodes" data-og-width="2000" width="2000" data-og-height="1250" height="1250" data-path="images/installation/custom_nodes/install-custom-nodes-by-manager-2.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-2.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=cc2be0d13ff5c434f3a52d5387f9e276 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-2.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=6b1d53ffaed9f5ff3f97060c03de113f 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-2.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=ff47226ca76ea50b41cc8b171875355b 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-2.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=74a3ce868907f913b2b4fec034d59752 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-2.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=d596a8b3a9913edd910657e82208fecb 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-2.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=c46471859d7b8f02c1c154e1411cdf6b 2500w" />
  </Step>

  <Step title="Browse the custom nodes list">
    <Warning>
      Custom nodes listed in ComfyUI Manager aren't necessarily safe. Understand their functionality before installing and ensure you only install trusted plugins and those from popular authors to avoid potential device risks.
    </Warning>

    <img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-3.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=3bf04620a7efb24a3559116c8b3e2c1c" alt="Enter node name in search box" data-og-width="2000" width="2000" data-og-height="1250" height="1250" data-path="images/installation/custom_nodes/install-custom-nodes-by-manager-3.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-3.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=2c306cff6c8576ea6f2485633aa55b14 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-3.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=ca91d3a1610076758cafac993e735a8b 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-3.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=eda0750485ba62521d0999b2b769d9bb 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-3.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=4aabed0ad5c07b93d2dfbf50c8be53a4 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-3.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=2c5d76bb1df8f33797517854389087bf 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-3.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=797bf30cc03e28f867266a9072336818 2500w" />
    <img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-4.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=61bc2c04fad5a918678885c156c7aadc" alt="Enter node name in search box" data-og-width="2000" width="2000" data-og-height="1250" height="1250" data-path="images/installation/custom_nodes/install-custom-nodes-by-manager-4.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-4.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=4ac1a60bb52edb26c0fe3784aedfd38d 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-4.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=646ac9473cb0be4e3c4579ba0ec07cb2 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-4.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=815f9b634b2843044bf32b9414b5024e 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-4.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=8e6e3308fd597a943890fa6f36b6dcd6 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-4.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=dea18ed9554ac7a83d72016f028ad502 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-4.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=ff321f2dcf95634ded67bda9f20dd01b 2500w" />

    1. Nodes marked with `` may have dependency conflicts with other plugins
    2. Author names marked with `` indicate their activity level on Github
    3. Potential plugin risks are highlighted in red - ensure plugin safety before installing
  </Step>

  <Step title="Click the `Install` button for the desired node">
    <img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-5.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=2ab49bbc1cbc8afa55b3fc422827944e" alt="Click Install button for the node" data-og-width="2000" width="2000" data-og-height="1250" height="1250" data-path="images/installation/custom_nodes/install-custom-nodes-by-manager-5.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-5.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=8f09eb71acbc2e828d90380272ecdbfa 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-5.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=daeee4413d5d46d6c7b6039bee0c3d40 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-5.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=757702fffa5d8d24b407ddb49f7dada8 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-5.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=3410efa332227612ce41abb59b51eb08 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-5.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=38f5cd193a5c609bf316ba8e99747ce6 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-5.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=6a13aabe76c4e9a3b2a92750b7f5e1eb 2500w" />
    Find the node you want to install and click the `Install` button.
    <img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-6.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=79795d6c93adadebc01ee7c96f51b1ec" alt="Click Install button for the node" data-og-width="2000" width="2000" data-og-height="1250" height="1250" data-path="images/installation/custom_nodes/install-custom-nodes-by-manager-6.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-6.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=356cb779b432f7f223b4ae5b87a09b74 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-6.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=e49670f42e4a9d28fb86b1618d6f5a22 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-6.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=0127ddc9a66b8cc898e8b88ea59106e5 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-6.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=4333ddc04205376867748812a67ec894 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-6.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=6302c261b567801b4aa0e83afca878bf 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-6.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=855555e64d068cb33da523c1798fd8e2 2500w" />

    * At this step, if you select the `nightly` version, it will directly download the latest source code of the plugin from Github. However, if your Manager's `security_level` is set to `normal`, you will not be allowed to download the source code directly from Github because the code has not been scanned.
    * If you select other versions such as `latest` or a stable version with a number, the code will be downloaded from [https://registry.comfy.org/](https://registry.comfy.org/), which means the code has been reviewed and will not trigger a security check.

    <Tip>
      The `nightly` version is usually the latest version, but since it is downloaded directly from Github and has not been reviewed, there is a certain code risk. If you really need to install the `nightly` version, please set the Manager's `security_level` to `weak`.
      The configuration file path is `ComfyUI/user/default/ComfyUI-Manager/config.ini`. Please note that this is not our recommended configuration and should only be used temporarily.
    </Tip>
  </Step>

  <Step title="Wait for dependencies to install and restart ComfyUI">
    Manager will automatically install dependencies and prompt you to restart ComfyUI when complete
    <img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-7.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=0d0a41967984241d0cd9f91718142abc" alt="Restart ComfyUI and refresh browser after installation" data-og-width="2000" width="2000" data-og-height="1254" height="1254" data-path="images/installation/custom_nodes/install-custom-nodes-by-manager-7.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-7.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=5ae436ee875468dc5f1d4727b66f70bd 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-7.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=5a198fbad24a062929a2cc1a7b924596 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-7.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=b6c1a6fc5dd012db17980a281ca58c0f 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-7.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=a2bc2a9c52f1ac9ddb409bd387d66e34 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-7.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=ee878686926d5099aef5727fa34e95ab 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-7.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=951407671177ea4bf96d7824a455a562 2500w" />
    <img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-8.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=4fabdae56e5d3dc110b9bc69a0dc78f8" alt="Restart ComfyUI and refresh browser after installation" data-og-width="2000" width="2000" data-og-height="1250" height="1250" data-path="images/installation/custom_nodes/install-custom-nodes-by-manager-8.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-8.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=d0f0c66addb3f7de935c87d8aeb1628f 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-8.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=9b2e3cc35941dcc21dd553c24214a86d 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-8.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=6b8efb4032e1f39facc208b46938eeca 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-8.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=28d897239a624c71b9e54fb4198f8642 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-8.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=5e244ed42d1f9085ae8e78949f88560f 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-8.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=9050141993949c95bf9b2c9ee69db2a9 2500w" />
  </Step>

  <Step title="Verify successful installation">
    Check ComfyUI Manager after restart to confirm the plugin installed successfully and there are no `import failed` errors
    <img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-9.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=c1d9fa38bbfe862ae661177e2948599c" alt="Restart ComfyUI and refresh browser after installation" data-og-width="2000" width="2000" data-og-height="1250" height="1250" data-path="images/installation/custom_nodes/install-custom-nodes-by-manager-9.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-9.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=c6b7124e5fd11e68b1c95b37c1100de9 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-9.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=cb41de584815df2118164653fd0504b9 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-9.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=ca524bc5f88a01c26d4c6829304c7001 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-9.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=96e49901b2fb537edd6dd882e3a97d8c 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-9.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=dae8ce344a3ffea691fa672188b0515b 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-manager-9.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=ae8c5e0c1e02413a587b1bb042815c4f 2500w" />
  </Step>
</Steps>

## Method 2: Manual Installation Using Git

Suitable for new nodes not found in Manager or when specific versions are needed. Requires [Git](https://git-scm.com/) installed on your system.

<Steps>
  <Step title="Get the repository URL">
    Click the "Code" button on GitHub and copy the HTTPS link
  </Step>

  <Step title="Navigate to custom_nodes directory">
    ```bash  theme={null}
    cd /path/to/ComfyUI/custom_nodes
    ```
  </Step>

  <Step title="Clone the repository">
    ```bash  theme={null}
    git clone [repository URL]
    ```
  </Step>

  <Step title="Install dependencies">
    Dependencies must be installed in your ComfyUI environment - be careful not to mix with your system environment to avoid contamination

    <Tabs>
      <Tab title="Windows Portable">
        For Windows portable version, install dependencies in the embedded Python environment

        ```bash  theme={null}
        python_embeded\python.exe -m pip install -r ComfyUI\custom_nodes\[node directory]\requirements.txt
        ```
      </Tab>

      <Tab title="Manual Install">
        Install dependencies in your ComfyUI environment

        ```bash  theme={null}
        cd [node directory]
        pip install -r requirements.txt
        ```
      </Tab>
    </Tabs>
  </Step>

  <Step title="Restart ComfyUI and refresh browser">
    Restart ComfyUI and refresh your browser. Check startup logs for any `import failed` errors
  </Step>
</Steps>

## Method 3: ZIP Download Installation

Suitable for users who cannot use Git or Manager

<Warning>
  We don't recommend this installation method as it loses version control capabilities
</Warning>

<Steps>
  <Step title="Click `Code`  `Download ZIP` on GitHub">
    Click `Code`  `Download ZIP` on the GitHub page
    <img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-zip.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=442d68eaf9a445b1d3d62f046e9f7b69" alt="Click Code  Download ZIP on GitHub" data-og-width="2000" width="2000" data-og-height="1115" height="1115" data-path="images/installation/custom_nodes/install-custom-nodes-by-zip.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-zip.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=a234780546a81960e49c21322f8d9278 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-zip.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=facd00051fefee30ab4bd93ce51c9070 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-zip.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=becba0c76aecd5376ca1b09884bf19ee 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-zip.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=c8fea29fdcbcd9e0c942ecc83bedb78f 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-zip.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=9afb8609ce2965097cb3fb34472319f8 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/installation/custom_nodes/install-custom-nodes-by-zip.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=52b81f5d9d26c13d8cbc2e8d02cf6e50 2500w" />
  </Step>

  <Step title="Extract the ZIP file">
    Extract the downloaded ZIP file
  </Step>

  <Step title="Copy extracted folder to `ComfyUI/custom_nodes/` directory">
    Copy the extracted folder to `ComfyUI/custom_nodes/` directory
  </Step>

  <Step title="Install dependencies manually (same as Git method step 4)">
    Restart ComfyUI and refresh browser
  </Step>

  <Step title="Verify successful installation">
    Check ComfyUI Manager after restart to confirm the plugin installed successfully and there are no `import failed` errors
  </Step>
</Steps>

## Custom Node Resources

In ComfyUI, besides the basic node extension functionality, custom nodes can also include the following additional resources:

* [Node Documentation](/custom-nodes/help_page): This feature supports all custom and basic nodes. You can use it to view node documentation, understand the purpose and usage of nodes, and contribute documentation via PRs to the author.
* [Custom Node Workflow Templates](/custom-nodes/workflow_templates): Workflow templates provided by node authors as example workflows, which can be browsed and loaded from the ComfyUI templates.
* [Multi-language Support](/custom-nodes/i18n)

If you are a custom node developer, you can add these resources to make your custom node more user-friendly.


# How to install ComfyUI manually in different systems
Source: https://docs.comfy.org/installation/manual_install



For the installation of ComfyUI, it is mainly divided into several steps:

1. Create a virtual environment(avoid polluting the system-level Python environment)
2. Clone the ComfyUI code repository
3. Install dependencies
4. Start ComfyUI

You can also refer to [ComfyUI CLI](/comfy-cli/getting-started) to install ComfyUI, it is a command line tool that can easily install ComfyUI and manage its dependencies.

## Create a virtual environment

<Tip>
  Independent virtual environments are necessary because ComfyUI's dependencies may conflict with other dependencies on the system, and it can also avoid polluting the system-level Python environment.
</Tip>

[Install Miniconda](https://docs.anaconda.com/free/miniconda/index.html#latest-miniconda-installer-links). This will help you install the correct versions of Python and other libraries needed by ComfyUI.

Create an environment with Conda.

```
conda create -n comfyenv
conda activate comfyenv
```

## Clone the ComfyUI code repository

You need to ensure that you have installed [Git](https://git-scm.com/downloads) on your system. First, you need to open the terminal (command line), then clone the code repository.

<Tabs>
  <Tab title="Windows">
    <Warning>If you have not installed Microsoft Visual C++ Redistributable, please install it [here.](https://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist?view=msvc-170)</Warning>
  </Tab>

  <Tab title="Linux">
    Open Terminal application.
  </Tab>

  <Tab title="MacOS">
    Open [Terminal application](https://support.apple.com/guide/terminal/open-or-quit-terminal-apd5265185d-f365-44cb-8b09-71a064a42125/mac).
  </Tab>
</Tabs>

```bash  theme={null}
git clone git@github.com:comfyanonymous/ComfyUI.git
```

## Install GPU and ComfyUI dependencies

<Steps>
  <Step title="Install GPU dependencies">
    Install GPU Dependencies

    <Accordion title="Nvidia">
      ```
      conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
      ```

      Alternatively, you can install the nightly version of PyTorch.

      <Accordion title="Install Nightly">
        <Warning>Install Nightly version (might be more risky)</Warning>

        ```
        conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch-nightly -c nvidia
        ```
      </Accordion>
    </Accordion>

    <Accordion title="AMD">
      ```
      pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0
      ```

      Alternatively, you can install the nightly version of PyTorch.

      <Accordion title="Install Nightly">
        <Warning>Install Nightly version (might be more risky)</Warning>

        ```
        pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.0
        ```
      </Accordion>
    </Accordion>

    <Accordion title="Mac ARM Silicon">
      ```bash  theme={null}
      conda install pytorch-nightly::pytorch torchvision torchaudio -c pytorch-nightly
      ```
    </Accordion>
  </Step>

  <Step title="Install ComfyUI dependencies">
    ```bash  theme={null}
    cd ComfyUI
    pip install -r requirements.txt
    ```
  </Step>

  <Step title="Start ComfyUI">
    Start the application

    ```
    cd ComfyUI
    python main.py
    ```
  </Step>
</Steps>

## How to update ComfyUI

<Steps>
  <Step title="pull the latest code">
    Use the command line to enter the installation path of ComfyUI, then pull the latest code.

    ```bash  theme={null}
    cd <installation path>/ComfyUI
    git pull
    ```
  </Step>

  <Step title="install the dependencies">
    Use the command line to enter the installation path of ComfyUI, then install the dependencies.

    <Warning>
      You need to ensure that the current Python environment is the ComfyUI virtual environment, otherwise the dependencies will be installed to the system-level Python environment, polluting the system-level Python environment.
    </Warning>

    ```bash  theme={null}
        pip install -r requirements.txt
    ```
  </Step>
</Steps>

## Adding Extra Model Paths

If you want to manage your model files outside of `ComfyUI/models`, you may have the following reasons:

* You have multiple ComfyUI instances and want them to share model files to save disk space
* You have different types of GUI programs (such as WebUI) and want them to use the same model files
* Model files cannot be recognized or found

We provide a way to add extra model search paths via the `extra_model_paths.yaml` configuration file

### Open Config File

<Tabs>
  <Tab title="Portable/Manual Install">
    For the ComfyUI version such as [portable](/installation/comfyui_portable_windows) and [manual](/installation/manual_install), you can find an example file named `extra_model_paths.yaml.example` in the root directory of ComfyUI:

    ```
    ComfyUI/extra_model_paths.yaml.example
    ```

    Copy and rename it to `extra_model_paths.yaml` for use. Keep it in ComfyUI's root directory at `ComfyUI/extra_model_paths.yaml`.
    You can also find the config example file [here](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example)
  </Tab>

  <Tab title="ComfyUI Desktop">
    If you are using the [ComfyUI Desktop](/installation/desktop/windows) application, you can follow the image below to open the extra model config file:

        <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=b2a08255f65a32e3da0c018a3cebb6a2" alt="Open Config File" data-og-width="1056" width="1056" data-og-height="1166" height="1166" data-path="images/desktop/extra_model_paths.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=4f4044b7cef96f0f4a70e7d8a0eb007a 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=aa576e31096b7306e810c10c97416ad5 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=ff8461eb13a9cec15293e7c5663bc65c 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=a2bf5a5335edfa58f37652bfed7a7dc9 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=a1a4c2d6fd021f4a348ea95c4abb5871 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/extra_model_paths.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=f508c6130dd86ec9adbef19aaf0d2530 2500w" />

    Or open it directly at:

    <Tabs>
      <Tab title="Windows">
        ```
        C:\Users\YourUsername\AppData\Roaming\ComfyUI\extra_models_config.yaml
        ```
      </Tab>

      <Tab title="macOS">
        ```
        ~/Library/Application Support/ComfyUI/extra_models_config.yaml
        ```
      </Tab>
    </Tabs>

    You should keep the file in the same directory, should not move these files to other places.
  </Tab>
</Tabs>

If the file does not exist, you can create it yourself with any text editor.

### Example Structure

Suppose you want to add the following model paths to ComfyUI:

```
 YOUR_PATH/
   models/
  |     lora/
  |       xxxxx.safetensors
  |     checkpoints/
  |       xxxxx.safetensors
  |     vae/
  |       xxxxx.safetensors
  |     controlnet/
  |        xxxxx.safetensors
```

Then you can configure the `extra_model_paths.yaml` file like below to let ComfyUI recognize the model paths on your device:

```
my_custom_config:
    base_path: YOUR_PATH
    loras: models/loras/
    checkpoints: models/checkpoints/
    vae: models/vae/
    controlnet: models/controlnet/
```

or

```
my_custom_config:
    base_path: YOUR_PATH/models/
    loras: loras
    checkpoints: checkpoints
    vae: vae
    controlnet: controlnet
```

<Warning>
  For the desktop version, please add the configuration to the existing configuration path without overwriting the path configuration generated during installation. Please back up the corresponding file before modification, so that you can restore it when you make a mistake.
</Warning>

Or you can refer to the default [extra\_model\_paths.yaml.example](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) for more configuration options. After saving, you need to **restart ComfyUI** for the changes to take effect.

Below is the original config example:

```yaml  theme={null}
#Rename this to extra_model_paths.yaml and ComfyUI will load it


#config for a1111 ui
#all you have to do is change the base_path to where yours is installed
a111:
    base_path: path/to/stable-diffusion-webui/

    checkpoints: models/Stable-diffusion
    configs: models/Stable-diffusion
    vae: models/VAE
    loras: |
         models/Lora
         models/LyCORIS
    upscale_models: |
                  models/ESRGAN
                  models/RealESRGAN
                  models/SwinIR
    embeddings: embeddings
    hypernetworks: models/hypernetworks
    controlnet: models/ControlNet

#config for comfyui
#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.

#comfyui:
#     base_path: path/to/comfyui/
#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads
#     #is_default: true
#     checkpoints: models/checkpoints/
#     clip: models/clip/
#     clip_vision: models/clip_vision/
#     configs: models/configs/
#     controlnet: models/controlnet/
#     diffusion_models: |
#                  models/diffusion_models
#                  models/unet
#     embeddings: models/embeddings/
#     loras: models/loras/
#     upscale_models: models/upscale_models/
#     vae: models/vae/

#other_ui:
#    base_path: path/to/ui
#    checkpoints: models/checkpoints
#    gligen: models/gligen
#    custom_nodes: path/custom_nodes

```

For example, if your WebUI is located at `D:\stable-diffusion-webui\`, you can modify the corresponding configuration to

```yaml  theme={null}
a111:
    base_path: D:\stable-diffusion-webui\
    checkpoints: models/Stable-diffusion
    configs: models/Stable-diffusion
    vae: models/VAE
    loras: |
         models/Lora
         models/LyCORIS
    upscale_models: |
                  models/ESRGAN
                  models/RealESRGAN
                  models/SwinIR
    embeddings: embeddings
    hypernetworks: models/hypernetworks
    controlnet: models/ControlNet
```

### Add Extra Custom Nodes Path

Besides adding external models, you can also add custom nodes paths that are not in the default path of ComfyUI

<Tip>
  Please note that this will not change the default installation path of custom nodes, but will add an extra path search when starting ComfyUI. You still need to complete the installation of custom node dependencies in the corresponding environment to ensure the integrity of the running environment.
</Tip>

Below is a simple configuration example (MacOS), please modify it according to your actual situation and add it to the corresponding configuration file, save it and restart ComfyUI for the changes to take effect:

```yaml  theme={null}
my_custom_nodes:
  custom_nodes: /Users/your_username/Documents/extra_custom_nodes
```


# System Requirements
Source: https://docs.comfy.org/installation/system_requirements

This guide introduces some system requirements for ComfyUI, including hardware and software requirements

In this guide, we will introduce the system requirements for installing ComfyUI. Due to frequent updates of ComfyUI, this document may not be updated in a timely manner. Please refer to the relevant instructions in [ComfyUI](https://github.com/comfyanonymous/ComfyUI).

Regardless of which version of ComfyUI you use, it runs in a separate Python environment.

### System Requirements

Currently, we support the following operating systems:

* Windows
* Linux
* macOS (supports Apple Silicon, such as the M series)

You can refer to the following sections to learn about the installation methods for different systems and versions of ComfyUI. In the installation of different versions, we have simply described the system requirements.

<AccordionGroup>
  <Accordion title="ComfyUI Desktop">
    ComfyUI Desktop currently supports standalone installation for **Windows and MacOS (ARM)**, currently in Beta

    * Code is open source on [Github](https://github.com/Comfy-Org/desktop)

    <Tip>
      Because Desktop is always built based on the **stable release**, so the latest updates may take some time to experience for Desktop, if you want to always experience the latest version, please use the portable version or manual installation
    </Tip>

    You can choose the appropriate installation for your system and hardware below

    <Tabs>
      <Tab title="Windows">
        <Card title="ComfyUI Desktop (Windows) Installation Guide" icon="link" href="/installation/desktop/windows">
          Suitable for **Windows** version with **Nvidia** GPU
        </Card>
      </Tab>

      <Tab title="MacOS(Apple Silicon)">
        <Card title="ComfyUI Desktop (MacOS) Installation Guide" icon="link" href="/installation/desktop/macos">
          Suitable for MacOS with **Apple Silicon**
        </Card>
      </Tab>

      <Tab title="Linux">
        <Note>ComfyUI Desktop **currently has no Linux prebuilds**, please visit the [Manual Installation](/installation/manual_install) section to install ComfyUI</Note>
      </Tab>
    </Tabs>
  </Accordion>

  <Accordion title="ComfyUI Portable (Windows)">
    Portable version is a ComfyUI version that integrates an independent embedded Python environment, using the portable version you can experience the latest features, currently only supports **Windows** system

    <Card title="ComfyUI Portable (Windows) Installation Guide" icon="link" href="/installation/comfyui_portable_windows">
      Supports **Windows** ComfyUI version running on **Nvidia GPUs** or **CPU-only**, always use the latest commits and completely portable.
    </Card>
  </Accordion>

  <Accordion title="Manual Installation">
    <Card title="ComfyUI Manual Installation Guide" icon="link" href="/installation/manual_install">
      Supports all system types and GPU types (Nvidia, AMD, Intel, Apple Silicon, Ascend NPU, Cambricon MLU)
    </Card>
  </Accordion>
</AccordionGroup>

### Python Version

* Recommended Python 3.12
* Supports Python 3.13 (some custom nodes may not be compatible)

### Supported Hardware

* NVIDIA GPU
* AMD GPU
* Intel GPU (includes Arc series, supports IPEX)
* Apple Silicon (M1/M2)
* Ascend NPU
* Cambricon MLU
* CPU (can use the --cpu parameter, slower)

Please refer to the [ComfyUI Windows and Linux manual installation section](https://github.com/comfyanonymous/ComfyUI?tab=readme-ov-file#manual-install-windows-linux) for detailed installation steps.

<Note>
  The stable version of PyTorch 2.7 now supports the Blackwell architecture (CUDA 12.8), and the core and desktop versions of ComfyUI have adopted this version.
</Note>

### Dependencies

* Install PyTorch
* Install all dependencies in the requirements.txt of ComfyUI

<Card title="Manual Installation" icon="book" href="/installation/manual_install">
  Please refer to the manual installation section for detailed installation steps.
</Card>


# How to Update ComfyUI
Source: https://docs.comfy.org/installation/update_comfyui

This section provides a comprehensive guide to updating ComfyUI

While we've covered ComfyUI updates across different installation methods in various sections, this comprehensive guide consolidates all update procedures to help users clearly understand how to update ComfyUI.

## How to Update ComfyUI?

<Tabs>
  <Tab title="Portable">
    ComfyUI Portable provides convenient batch scripts for easy updates.

    ### Update Script Location

    In the `update` folder within your portable installation directory, you'll find the following update scripts:

    ```
    ComfyUI_windows_portable
     update
        update.py
        update_comfyui.bat                           // Update to latest development version
        update_comfyui_stable.bat                    // Update to latest stable version
        update_comfyui_and_python_dependencies.bat   //  DANGER: Reinstalls dependencies - may cause conflicts
    ```

    <Warning>Ensure a stable internet connection during updates.</Warning>

    <Warning>
      ** DANGER: Use `update_comfyui_and_python_dependencies.bat` with extreme caution!**

      This script is more thorough than regular updates and will:

      *  Update ComfyUI code itself
      *  Update PyTorch (for NVIDIA GPU CUDA 12.9)
      *  Reinstall ALL Python dependencies

      **Risks:**

      * May cause dependency conflicts with your existing setup
      * Can break custom nodes that rely on specific package versions
      * May overwrite manually configured packages

      ** Only use this script when:**

      * You need to fix dependency issues
      * Performing major version updates
      * For daily updates, use `update_comfyui.bat` instead

      **Before running this script:**

      1. **Back up your entire ComfyUI installation**
      2. Document any custom package versions you've installed
      3. Be prepared to reinstall custom nodes if needed
    </Warning>
  </Tab>

  <Tab title="Desktop">
    ComfyUI Desktop features automatic updates to ensure you're always running the latest version. However, since the Desktop version is built on stable releases, feature updates may lag behind. You can also visit the [download page](https://www.comfy.org/download) to get the latest version.

    ### Auto-Update Settings

    Ensure automatic updates are enabled in your settings:
    <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-desktop-update-setting.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=5006c9c0e4ad3e4f6ca0cef51132fbf3" alt="ComfyUI Desktop Settings" data-og-width="1065" width="1065" data-og-height="815" height="815" data-path="images/desktop/comfyui-desktop-update-setting.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-desktop-update-setting.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=25f0e90e8df014d0640205bb73e4253c 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-desktop-update-setting.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=0137407c528ac79439d4320701e4c0b1 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-desktop-update-setting.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=73f85bd7cbbeeeaeabad54154b60af7e 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-desktop-update-setting.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=4f880973b712a83849b16a78ce5868f5 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-desktop-update-setting.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=174dc6b670118f452834cece0df52dbd 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/comfyui-desktop-update-setting.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=1b72afe13029997c797c16772daeff98 2500w" />

    ### Manual Update Check

    You can also manually check for available updates:

    1. Click `Menu` in the menu bar
    2. Select `Help`
    3. Click `Check for Updates`
       <img src="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_check_for_updates.jpg?fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=0f2bf5d6ad204e762ed858f29df4282c" alt="ComfyUI Desktop Check Updates" data-og-width="415" width="415" data-og-height="477" height="477" data-path="images/desktop/desktop_check_for_updates.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_check_for_updates.jpg?w=280&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=c8f28a7f5ef143b0ca31e839187a5a65 280w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_check_for_updates.jpg?w=560&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=ffef0bd3b488b1545c0390d65ffbd445 560w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_check_for_updates.jpg?w=840&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=e46efe596b9b5b7f38da68489c245e4c 840w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_check_for_updates.jpg?w=1100&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=9dd9460b2daafd8b398145e1961d668c 1100w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_check_for_updates.jpg?w=1650&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=218f1e5e502286b7a90e21d5f60100fc 1650w, https://mintcdn.com/dripart/CGWmMjlFmU7msQ5S/images/desktop/desktop_check_for_updates.jpg?w=2500&fit=max&auto=format&n=CGWmMjlFmU7msQ5S&q=85&s=f7e03d947a487ad2a5624f8d14c68e40 2500w" />

    <Note>Desktop version automatically handles all update processes, including ComfyUI core code and dependency updates</Note>
  </Tab>

  <Tab title="Manual Installation">
    Manually installed ComfyUI requires Git commands to complete updates.

    ### Pre-Update Requirements

    Ensure your system has [Git](https://git-scm.com/downloads) installed and ComfyUI was installed via Git clone.

    ### Update Steps

    <Steps>
      <Step title="Activate Virtual Environment">
        First, activate ComfyUI's Python virtual environment (if using one):

        ```bash  theme={null}
        # For conda environment
        conda activate comfyui

        # For venv environment
        # Windows
        venv\Scripts\activate
        # macOS/Linux  
        source venv/bin/activate
        ```
      </Step>

      <Step title="Pull Latest Code">
        Navigate to your ComfyUI installation directory and pull the latest code:

        ```bash  theme={null}
        cd <ComfyUI-installation-path>
        git pull
        ```
      </Step>

      <Step title="Update Dependencies">
        Install or update ComfyUI's dependency packages:

        ```bash  theme={null}
        pip install -r requirements.txt
        ```

        <Warning>
          Ensure you're in ComfyUI's virtual environment to avoid contaminating the system-level Python environment
        </Warning>
      </Step>

      <Step title="Restart ComfyUI">
        After updating, restart ComfyUI:

        ```bash  theme={null}
        python main.py
        ```
      </Step>
    </Steps>

    ### Version Switching (Optional)

    To switch to a specific version, use these commands:

    ```bash  theme={null}
    # View commit history
    git log --oneline

    # Switch to specific commit
    git checkout <commit-hash>

    # Return to latest version
    git checkout master
    ```

    <Tip>Regular updates are recommended for the latest features and security fixes, while stable versions are recommended for system stability</Tip>
  </Tab>
</Tabs>

## ComfyUI Version Types

Depending on your installation method, ComfyUI offers several installation versions. The links below contain update instructions for each version.

<AccordionGroup>
  <Accordion title="ComfyUI Desktop">
    ComfyUI Desktop currently supports standalone installation for **Windows and MacOS (ARM)**, currently in Beta

    * Code is open source on [Github](https://github.com/Comfy-Org/desktop)

    <Tip>
      Because Desktop is always built based on the **stable release**, so the latest updates may take some time to experience for Desktop, if you want to always experience the latest version, please use the portable version or manual installation
    </Tip>

    You can choose the appropriate installation for your system and hardware below

    <Tabs>
      <Tab title="Windows">
        <Card title="ComfyUI Desktop (Windows) Installation Guide" icon="link" href="/installation/desktop/windows">
          Suitable for **Windows** version with **Nvidia** GPU
        </Card>
      </Tab>

      <Tab title="MacOS(Apple Silicon)">
        <Card title="ComfyUI Desktop (MacOS) Installation Guide" icon="link" href="/installation/desktop/macos">
          Suitable for MacOS with **Apple Silicon**
        </Card>
      </Tab>

      <Tab title="Linux">
        <Note>ComfyUI Desktop **currently has no Linux prebuilds**, please visit the [Manual Installation](/installation/manual_install) section to install ComfyUI</Note>
      </Tab>
    </Tabs>
  </Accordion>

  <Accordion title="ComfyUI Portable (Windows)">
    Portable version is a ComfyUI version that integrates an independent embedded Python environment, using the portable version you can experience the latest features, currently only supports **Windows** system

    <Card title="ComfyUI Portable (Windows) Installation Guide" icon="link" href="/installation/comfyui_portable_windows">
      Supports **Windows** ComfyUI version running on **Nvidia GPUs** or **CPU-only**, always use the latest commits and completely portable.
    </Card>
  </Accordion>

  <Accordion title="Manual Installation">
    <Card title="ComfyUI Manual Installation Guide" icon="link" href="/installation/manual_install">
      Supports all system types and GPU types (Nvidia, AMD, Intel, Apple Silicon, Ascend NPU, Cambricon MLU)
    </Card>
  </Accordion>
</AccordionGroup>

## What Needs to Be Updated When Updating ComfyUI?

ComfyUI updates primarily consist of two components:

1. Update ComfyUI's core code
2. Update ComfyUI's core dependencies, including necessary Python dependencies and ComfyUI functional dependency packages

**Core Code**: New nodes, new model support, new features, etc.
**Core Dependencies**: Mainly includes ComfyUI's frontend functionality, workflow templates, node help documentation, etc.

```
comfyui-frontend-package   # ComfyUI frontend functionality
comfyui-workflow-templates # ComfyUI workflow templates  
comfyui-embedded-docs      # ComfyUI node help documentation
```

These three core dependencies are maintained in separate repositories:

* [ComfyUI\_frontend](https://github.com/Comfy-Org/ComfyUI_frontend/) - Frontend interface and interactive features
* [workflow\_templates](https://github.com/Comfy-Org/workflow_templates) - Pre-built workflow templates
* [comfyui-embedded-docs](https://github.com/Comfy-Org/embedded-docs) - Node help documentation

It's important to understand the difference between development (nightly) and stable (release) versions:

* **Development version (nightly)**: Latest commit code, giving you access to the newest features, but may contain potential issues
* **Stable version (release)**: Built on stable releases, usually lags behind development versions but offers higher stability. We support stable versions after features are tested and stabilized

Many users often find themselves on release versions or desktop versions during updates, but discover that needed features are only available in development versions. In such cases, check if your local `ComfyUI/requirements.txt` matches the [nightly version dependencies](https://github.com/comfyanonymous/ComfyUI/blob/master/requirements.txt) to determine if all dependencies support the latest features.

## Common Update Issues

### Missing or Outdated Frontend, Workflow Templates, Node  After Updates

<Tabs>
  <Tab title="Dependencies Not Properly Updated">
    Users often only use the `git pull` command to update ComfyUI code but **neglect core dependency updates**, leading to the following issues:

    * Missing or abnormal frontend functionality
    * Cannot find newly added workflow templates
    * Outdated or missing node help documentation
    * New features lack corresponding frontend support

    After using the `git pull` command, use the corresponding ComfyUI environment to use `pip install -r requirements.txt` to update dependencies.
  </Tab>

  <Tab title="Dependency Update Failures">
    If dependency updates fail, it's commonly due to network or computer permission issues. When core dependency failures occur during updates, the system falls back to older versions. You'll typically see logs like this during startup:

    ```
    Falling back to the default frontend.
    ComfyUI frontend version: xxx
    ```

    Follow these troubleshooting steps:

    1. Use `pip list` in the appropriate environment to view currently installed dependency packages. If you find version inconsistencies, use `pip install -r requirements.txt` in the ComfyUI environment to attempt dependency updates again.
    2. If issues persist after updating, check your network connection. Users in mainland China may need to configure a proxy to reliably access GitHub repositories.
    3. If problems continue, check computer permissions. If administrator privileges are required, run the command line with administrator rights.
  </Tab>
</Tabs>

### How to Properly Update Core Dependencies

<Tabs>
  <Tab title="Portable">
    **Recommended Method**: Use the `ComfyUI_windows_portable\update\update_comfyui.bat` batch script, which will update both ComfyUI code and all Python dependencies.

    **Manual Dependency Update**:
    If you need to manually update dependencies, use the following command:

    ```bash  theme={null}
    # Open command line in portable version directory
    .\python_embeded\python.exe -m pip install -r ComfyUI\requirements.txt
    ```
  </Tab>

  <Tab title="Manual Installation">
    <Steps>
      <Step title="Activate Python Environment">
        If you use Conda to manage Python environments, activate your environment first:

        ```bash  theme={null}
        conda activate comfyui  # or other environment name
        ```
      </Step>

      <Step title="Update Code">
        Navigate to the ComfyUI root directory and update the code using Git:

        ```
        cd <ComfyUI_ROOT_PATH>
        git pull
        ```
      </Step>

      <Step title="Update Dependencies">
        Update ComfyUI dependencies - this step is crucial, especially for the `comfyui-frontend-package`:

        ```
        pip install -r requirements.txt
        ```
      </Step>
    </Steps>
  </Tab>

  <Tab title="Desktop">
    Desktop version usually handles dependency updates automatically. If you encounter issues:

    1. **Check if auto-update** is enabled
    2. **Manually trigger update**: Menu  Help  Check for Updates
    3. **Reinstall desktop version** (in extreme cases)
  </Tab>
</Tabs>

### Core Dependency Update Troubleshooting

If core dependency updates fail, follow these troubleshooting steps:

<Steps>
  <Step title="Check Network Connection">
    If located in mainland China, ensure you can access PyPI or configure a domestic mirror:

    ```bash  theme={null}
    # Using Tsinghua University mirror
    pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple
    ```
  </Step>

  <Step title="Install Core Packages Individually">
    If batch installation fails, try installing packages individually. **First check version requirements in `ComfyUI/requirements.txt`**:

    **Then install according to specified versions:**

    ```bash  theme={null}
    pip install comfyui-frontend-package==1.17.11 
    pip install comfyui-workflow-templates==1.0.0
    pip install comfyui-embedded-docs==1.0.0
    ```

    <Warning>
      It's recommended to use the exact version numbers specified in `ComfyUI/requirements.txt`. Don't upgrade to the latest versions independently, as this may cause compatibility issues.
    </Warning>
  </Step>
</Steps>

### Why Can't I Find New Features After Updating?

This is one of the most common issues:

* If you're using the **Desktop version**, features may lag behind because the desktop version is built on stable releases
* Ensure you're using the **development version (nightly)**, not the **stable version (release)**

Additionally, ensure that corresponding dependencies have been successfully updated during the update process. If issues persist after updating, refer to the [Dependency Update Troubleshooting](#dependency-update-troubleshooting) section to diagnose problems.

### How to Switch Between Development (Nightly) and Stable (Release) Versions?

Differences between versions:

<Tabs>
  <Tab title="Development Version (Nightly)">
    * **Characteristics**: Contains the latest commit code
    * **Advantages**: Experience the latest features and improvements first
    * **Risks**: May contain undiscovered bugs or unstable factors
    * **Suitable for**: Developers, testers, users wanting to experience the latest features
  </Tab>

  <Tab title="Stable Version (Release)">
    * **Characteristics**: Tested and verified stable code
    * **Advantages**: High stability, suitable for production environments
    * **Disadvantages**: Feature updates have delays, may lag behind development versions by weeks or months
    * **Suitable for**: Users requiring stability, production environment users
  </Tab>
</Tabs>

<Tabs>
  <Tab title="Portable">
    Use `update_comfyui.bat` instead of `update_comfyui_stable.bat`:

    ```
    # Development version (latest features)
    double-click: update_comfyui.bat

    # Stable version
    double-click: update_comfyui_stable.bat
    ```
  </Tab>

  <Tab title="Manual Installation">
    ```bash  theme={null}
    # Switch to development version
    git checkout master
    git pull

    # Switch to latest stable version
    git fetch --tags
    git checkout $(git describe --tags `git rev-list --tags --max-count=1`)
    ```
  </Tab>

  <Tab title="Desktop">
    Desktop version is typically built on stable releases and doesn't currently support version switching. If you need the latest features, we recommend:

    1. Wait for desktop version updates
    2. Or use portable/manual installation to experience the latest features
  </Tab>
</Tabs>

### What to Do When Errors Occur After Updates?

1. **Check Dependencies**: Run `pip install -r requirements.txt` to ensure all dependencies are updated
2. **Check Custom Nodes**: Some custom nodes may be incompatible with new versions
3. **Roll Back Version**: If issues are severe, you can roll back to a previous stable version

If problems occur, refer to our troubleshooting page for solutions.

<Card title="Troubleshooting" icon="bug" href="/troubleshooting/overview">
  Learn how to troubleshoot ComfyUI issues
</Card>

### How to Stay Updated on New Features?

* **GitHub Releases**: Check [ComfyUI Releases](https://github.com/comfyanonymous/ComfyUI/releases) for stable version updates
* **GitHub Commits**: View [latest commits](https://github.com/comfyanonymous/ComfyUI/commits/master) to understand development progress
* **Community Discussion**: Follow our [blog](https://blog.comfy.org) and [Twitter](https://x.com/comfyui) for the latest updates


# Customizing ComfyUI Appearance
Source: https://docs.comfy.org/interface/appearance

Learn how to customize the appearance of ComfyUI using color palettes and advanced CSS options

ComfyUI offers flexible appearance customization options that allow you to personalize the interface to your preferences.

## Color Palette System

The primary way to customize ComfyUI's appearance is through the built-in color palette system.

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/color-palette.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=206dbb5bcf19ba1923bd722e20ba868e" alt="Color Palette" data-og-width="1180" width="1180" data-og-height="174" height="174" data-path="images/interface/setting/appearance/color-palette.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/color-palette.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=c9a9689322656132fe0cc942ded1b06e 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/color-palette.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=0fafde84e9684d46ce7818b09ffa3a59 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/color-palette.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=0d114e91564c4155e8667de6aa562976 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/color-palette.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=806613c3883910fb1d6ae75e7a85e601 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/color-palette.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=168dfc4fdd621f263fa7ad9432f08a6c 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/color-palette.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=bf2a301c7a1fb756bf9ba53d3ce96906 2500w" />

This allows you to:

1. Switch ComfyUI themes
2. Export the currently selected theme as JSON format
3. Load custom theme configuration from JSON file
4. Delete custom theme configuration

<Note>
  For appearance needs that cannot be satisfied by the color palette, you can perform advanced appearance customization through the [user.css](#advanced-customization-with-user-css) file
</Note>

### How to Customize Color Themes

The color palette allows you to modify many specific properties. Here are some of the most commonly customized elements, with colors represented in hexadecimal format:

<Tip>
  1. The JSON comments below are for illustration only. Do not copy the content below for modification as it will cause the theme to malfunction.
  2. Since we are still iterating frequently, the content below may change with ComfyUI frontend updates. If you need to modify, please export the theme configuration from settings and then modify it.
</Tip>

```json  theme={null}
{
  "id": "dark",                     // Must be unique, cannot be the same as other theme IDs
  "name": "Dark (Default)",         // Theme name, displayed in theme selector
  "colors": {
    "node_slot": {                  // Node connection slot color configuration
      "CLIP": "#FFD500",            // CLIP model connection slot color
      "CLIP_VISION": "#A8DADC",     // CLIP Vision model connection slot color
      "CLIP_VISION_OUTPUT": "#ad7452", // CLIP Vision output connection slot color
      "CONDITIONING": "#FFA931",     // Conditioning control connection slot color
      "CONTROL_NET": "#6EE7B7",     // ControlNet model connection slot color
      "IMAGE": "#64B5F6",           // Image data connection slot color
      "LATENT": "#FF9CF9",          // Latent space connection slot color
      "MASK": "#81C784",            // Mask data connection slot color
      "MODEL": "#B39DDB",           // Model connection slot color
      "STYLE_MODEL": "#C2FFAE",     // Style model connection slot color
      "VAE": "#FF6E6E",             // VAE model connection slot color
      "NOISE": "#B0B0B0",           // Noise data connection slot color
      "GUIDER": "#66FFFF",          // Guider connection slot color
      "SAMPLER": "#ECB4B4",         // Sampler connection slot color
      "SIGMAS": "#CDFFCD",          // Sigmas data connection slot color
      "TAESD": "#DCC274"            // TAESD model connection slot color
    },
    "litegraph_base": {             // LiteGraph base interface configuration
      "BACKGROUND_IMAGE": "",        // Background image, default is empty
      "CLEAR_BACKGROUND_COLOR": "#222", // Main canvas background color
      "NODE_TITLE_COLOR": "#999",    // Node title text color
      "NODE_SELECTED_TITLE_COLOR": "#FFF", // Selected node title color
      "NODE_TEXT_SIZE": 14,          // Node text size
      "NODE_TEXT_COLOR": "#AAA",     // Node text color
      "NODE_TEXT_HIGHLIGHT_COLOR": "#FFF", // Node text highlight color
      "NODE_SUBTEXT_SIZE": 12,       // Node subtext size
      "NODE_DEFAULT_COLOR": "#333",   // Node default color
      "NODE_DEFAULT_BGCOLOR": "#353535", // Node default background color
      "NODE_DEFAULT_BOXCOLOR": "#666", // Node default border color
      "NODE_DEFAULT_SHAPE": 2,        // Node default shape
      "NODE_BOX_OUTLINE_COLOR": "#FFF", // Node border outline color
      "NODE_BYPASS_BGCOLOR": "#FF00FF", // Node bypass background color
      "NODE_ERROR_COLOUR": "#E00",    // Node error state color
      "DEFAULT_SHADOW_COLOR": "rgba(0,0,0,0.5)", // Default shadow color
      "DEFAULT_GROUP_FONT": 24,       // Group default font size
      "WIDGET_BGCOLOR": "#222",       // Widget background color
      "WIDGET_OUTLINE_COLOR": "#666", // Widget outline color
      "WIDGET_TEXT_COLOR": "#DDD",    // Widget text color
      "WIDGET_SECONDARY_TEXT_COLOR": "#999", // Widget secondary text color
      "WIDGET_DISABLED_TEXT_COLOR": "#666", // Widget disabled state text color
      "LINK_COLOR": "#9A9",          // Connection line color
      "EVENT_LINK_COLOR": "#A86",    // Event connection line color
      "CONNECTING_LINK_COLOR": "#AFA", // Connecting line color
      "BADGE_FG_COLOR": "#FFF",      // Badge foreground color
      "BADGE_BG_COLOR": "#0F1F0F"    // Badge background color
    },
    "comfy_base": {                  // ComfyUI base interface configuration
      "fg-color": "#fff",            // Foreground color
      "bg-color": "#202020",         // Background color
      "comfy-menu-bg": "#353535",    // Menu background color
      "comfy-menu-secondary-bg": "#303030", // Secondary menu background color
      "comfy-input-bg": "#222",      // Input field background color
      "input-text": "#ddd",          // Input text color
      "descrip-text": "#999",        // Description text color
      "drag-text": "#ccc",           // Drag text color
      "error-text": "#ff4444",       // Error text color
      "border-color": "#4e4e4e",     // Border color
      "tr-even-bg-color": "#222",    // Table even row background color
      "tr-odd-bg-color": "#353535",  // Table odd row background color
      "content-bg": "#4e4e4e",       // Content area background color
      "content-fg": "#fff",          // Content area foreground color
      "content-hover-bg": "#222",    // Content area hover background color
      "content-hover-fg": "#fff",    // Content area hover foreground color
      "bar-shadow": "rgba(16, 16, 16, 0.5) 0 0 0.5rem" // Bar shadow effect
    }
  }
}
```

## Canvas

### Background Image

* **Requirements**: ComfyUI frontend version 1.20.5 or newer
* **Function**: Set a custom background image for the canvas to provide a more personalized workspace. You can upload images or use web images to set the background for the canvas.

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/set-as-bg.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=f393d494dfdc8b7b84997d58735d2f86" alt="Set Background Image" data-og-width="1768" width="1768" data-og-height="1524" height="1524" data-path="images/interface/setting/appearance/set-as-bg.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/set-as-bg.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=dacd49bd08e941c09260bb001b26c550 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/set-as-bg.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=8ec0fa6b65c0a433e89c0c045fa9e3ea 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/set-as-bg.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=3fbacb928e83bc685e3a4f605bfe53a2 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/set-as-bg.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=cdae7e2939628dd6fe287d0047f6e0a5 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/set-as-bg.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=da1e7e8201256f1366cfe5a8be6e0f21 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/set-as-bg.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=bb134d3708f6686aaf75574993159389 2500w" />

## Node

### Node Opacity

* **Function**: Set the opacity of nodes, where 0 represents completely transparent and 1 represents completely opaque.

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/node-opacity.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=43e2a908124028996938fedd96869b25" alt="Node Opacity" data-og-width="956" width="956" data-og-height="594" height="594" data-path="images/interface/setting/appearance/node-opacity.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/node-opacity.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=73e58f2a90eb80dbdbb6d88b640313e7 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/node-opacity.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=5f19cd8413e15ada7f0edb8879fbad67 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/node-opacity.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=48b086d7d3915e66759f570816d4aec2 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/node-opacity.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=9d1dea85356595670653779100d27dc1 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/node-opacity.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=10ebc6b715728a77bdd47d252c31fa12 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/node-opacity.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=623b9f92d33cd47608e41d886ca089ad 2500w" />

## Node Widget

### Textarea Widget Font Size

* **Range**: 8 - 24
* **Function**: Set the font size in textarea widgets. Adjusts the text display size in text input boxes to improve readability.

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/textarea-font-size.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=2406917ddcec26ddb4e882202d433d0a" alt="Textarea Widget Font Size" data-og-width="1206" width="1206" data-og-height="650" height="650" data-path="images/interface/setting/appearance/textarea-font-size.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/textarea-font-size.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=748bc7d69219db388d4e496c6407ac99 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/textarea-font-size.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=b6afebc4fc8ccdc5a25ee657455bccf0 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/textarea-font-size.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=1b4fbbeb51d63345733f67171f080d12 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/textarea-font-size.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=e0c1551690d00900d20bd90ffce4e24c 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/textarea-font-size.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=cff057ffc84f96a9bbc8ad0a3517c1dc 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/textarea-font-size.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=bec2e089f63f4dbbd58e81ce9c535ea0 2500w" />

## Sidebar

### Unified Sidebar Width

* **Function**: When enabled, the sidebar width will be unified to a consistent width when switching between different sidebars. If disabled, different sidebars can maintain their custom widths when switching.

### Sidebar Size

* **Function**: Control the size of the sidebar, can be set to normal or small.

### Sidebar Location

* **Function**: Control whether the sidebar is displayed on the left or right side of the interface, allowing users to adjust the sidebar position according to their usage habits.

## Tree Explorer

### Tree Explorer Item Padding

* **Function**: Set the padding of items in the tree explorer (sidebar panel), adjusting the spacing between items in the tree structure.

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/padding.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=9fb818218a4713f0a88700de25633a8c" alt="Tree Explorer Item Padding" data-og-width="1254" width="1254" data-og-height="650" height="650" data-path="images/interface/setting/appearance/padding.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/padding.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=fac0725c9684e0bd4385aff8276b11ee 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/padding.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=4c41178c5e11f3c6365307177bf5417b 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/padding.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=c8de042df86c3c648b36a7096fc5503a 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/padding.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=1e4e5e8b6bdfa9d6fdf9ce9101a4cf26 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/padding.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=4c83f423b1b54e709a77874c402218a8 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/appearance/padding.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=dc01878bc2e7ad3ce2b33214c506feab 2500w" />

## Advanced Customization with user.css

For cases where the color palette doesn't provide enough control, you can use custom CSS via a user.css file. This method is recommended for advanced users who need to customize elements that aren't available in the color palette system.

### Requirements

* ComfyUI frontend version 1.20.5 or newer

### Setting Up user.css

1. Create a file named `user.css` in your ComfyUI user directory (same location as your workflows and settings - see location details below)
2. Add your custom CSS rules to this file
3. Restart ComfyUI or refresh the page to apply changes

### User Directory Location

The ComfyUI user directory is where your personal settings, workflows, and customizations are stored. The location depends on your installation type:

<Tabs>
  <Tab title="Desktop - Windows">
    ```
    C:\Users\<your username>\AppData\Roaming\ComfyUI\user
    ```
  </Tab>

  <Tab title="Desktop - macOS">
    ```
    ~/<ComfyUI installation path>/ComfyUI/user
    ```
  </Tab>

  <Tab title="Desktop - Linux">
    ```
    ~/.config/ComfyUI/user
    ```
  </Tab>

  <Tab title="Manual Install">
    The user directory is located in your ComfyUI installation folder:

    ```
    <ComfyUI installation path>/user
    ```

    For example, if you cloned ComfyUI to `C:\ComfyUI`, your user directory would be `C:\ComfyUI\user\default` (or `C:\ComfyUI\user\john` if you've set up a custom username).

    <Note>
      ComfyUI supports multiple users per installation. If you haven't configured a custom username, it defaults to "default". Each user gets their own subdirectory within the `user` folder.
    </Note>
  </Tab>

  <Tab title="Portable">
    The user directory is located in your ComfyUI portable folder:

    ```
    <ComfyUI_windows_portable>/ComfyUI/user
    ```

    For example: `ComfyUI_windows_portable/ComfyUI/user/default`

    <Note>
      ComfyUI supports multiple users per installation. If you haven't configured a custom username, it defaults to "default". Each user gets their own subdirectory within the `user` folder.
    </Note>
  </Tab>
</Tabs>

This location contains your workflows, settings, and other user-specific files.

After finding the above folder location, please copy the corresponding CSS file to the corresponding user directory, such as the default user folder being `ComfyUI/user/default`, then restart ComfyUI or refresh the page to apply changes.

### user.css Examples and Related Instructions

The `user.css` file is loaded early in the application startup process. So you may need to use `!important` in your CSS rules to ensure they override the default styles.

**user.css Customization Examples**

```css  theme={null}
/* Increase font size in inputs and menus for better readability */
.comfy-multiline-input, .litecontextmenu .litemenu-entry {
    font-size: 20px !important;
}

/* Make context menu entries larger for easier selection */
.litegraph .litemenu-entry,
.litemenu-title {
  font-size: 24px !important; 
}

/* Custom styling for specific elements not available in the color palette */
.comfy-menu {
    border: 1px solid rgb(126, 179, 189) !important;
    border-radius: 0px 0px 0px 10px !important;
    backdrop-filter: blur(2px);
}
```

**Best Practices**

1. **Start with the color palette** for most customizations
2. Use **user.css only when necessary** for elements not covered by the color palette
3. **Export your theme** before making significant changes so you can revert if needed
4. **Share your themes** with the community to inspire others

**Troubleshooting**

* If your color palette changes don't appear, try refreshing the page
* If CSS customizations don't work, check that you're using frontend version 1.20.5+
* Try adding `!important` to user.css rules that aren't being applied
* Keep backups of your customizations to easily restore them


# Credits Management
Source: https://docs.comfy.org/interface/credits

In this article, we will introduce ComfyUI's credit management features, including how to obtain, use, and view credits.

The credit system was added to support the `API Nodes`, as calling closed-source AI models requires token consumption, so proper credit management is necessary. By default, the credits interface is not displayed. Please first log in to your ComfyUI account in `Settings` -> `User`, and then you can view your associated account's credit information in `Settings` -> `Credits`.

<img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/menu-credits.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=23c731ee1aca4095c148b557d21211a1" alt="ComfyUI Credits Interface" data-og-width="3358" width="3358" data-og-height="1820" height="1820" data-path="images/interface/setting/menu-credits.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/menu-credits.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=336c1785fbafbfa259e2a990ba0f0137 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/menu-credits.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=2be585162b11f5200d0df5a1bb008ae8 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/menu-credits.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=0048dc13fdba2649100698c54f0d5ea5 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/menu-credits.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=254e3d38a542a261a993031e8312e8a1 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/menu-credits.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=61ec275a6a88b1b488515f5d266df29e 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/menu-credits.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=a97b297afef95f9d651bc8993fbd0c40 2500w" />

<Note>
  ComfyUI will always remain fully open-source and free for local users.
</Note>

## How to Purchase Credits?

Below is a demonstration video for purchasing credits:

<video controls className="w-full aspect-video" src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/Buy_Credits_Flow.mp4?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=a339505a6c77545fbb91e03a7a627c8b" data-path="images/interface/setting/Buy_Credits_Flow.mp4" />

Detailed steps are as follows:

<Steps>
  <Step title="Log in to your ComfyUI account">
    Log in to your ComfyUI account in `Settings` -> `User`
    <img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=066170b38e0b9ead026029685e00fa65" alt="Login Interface" data-og-width="3358" width="3358" data-og-height="1828" height="1828" data-path="images/interface/setting/user.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=c7f1a017e9b00c6224a440f83d121a59 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=92b830f85f4393d802f7f33bdce81634 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=e294573cc054158fb3a108d10bc67087 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=5160ea18d19dfdde201bbf41dbb1af0b 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=463f4645799b84ea5dcd4879ed3b87ca 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=e52e827f35eddf444e91e5ed4f11b331 2500w" />
  </Step>

  <Step title="Go to `Settings` -> `Credits` to purchase credits">
    After logging in, you should see the `Credits` option added to the menu

    Go to `Settings` -> `Credits` to purchase credits
    <img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/purchase-1.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=a324b33ce2d6dce5bf2d0e895ac0d1eb" alt="Credits Interface" data-og-width="3358" width="3358" data-og-height="1828" height="1828" data-path="images/interface/setting/purchase-1.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/purchase-1.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=3be3740eb5ebef1f96177e261d356394 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/purchase-1.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=639eca3dce7544af869871fbc50187a8 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/purchase-1.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=e36ad4124f9eaeeb5ac3c83481c55eef 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/purchase-1.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=05042870ef1f4b2270c6053b3903018e 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/purchase-1.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=06ae7e8245f405be7e80c80808a50dbe 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/purchase-1.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=3544af87748d5ef4b2a4966d08a2bcdd 2500w" />
  </Step>

  <Step title="Set the amount of credits to purchase">
    <img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/buy.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=86ecf76f53edc3d3dc42d96b3886d96e" alt="Set Purchase Amount" data-og-width="3346" width="3346" data-og-height="1830" height="1830" data-path="images/interface/setting/buy.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/buy.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=621220cf01b796d67173a2132a4cbb34 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/buy.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=18138b43a180b39bc3733fb1468828c0 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/buy.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=c35baab028e9bc67a760ebc5b7ecb7dd 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/buy.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=8e228484e085177df2daa361559b325f 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/buy.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=ba22618a739ba2241507244c7c525ad4 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/buy.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=7d119d97bd4e2b6f5ef326f21c0bb27c 2500w" />
    In the popup, set the purchase amount and click the `Buy` button
  </Step>

  <Step title="Make payment through Stripe">
    <img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/stripe_payment.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=2f50a9b77c2e20986184a934e664881d" alt="Stripe Payment Page" data-og-width="4000" width="4000" data-og-height="2203" height="2203" data-path="images/interface/setting/stripe_payment.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/stripe_payment.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=809687e3675146d778f288a67d59007e 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/stripe_payment.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=dbe0830f4fe51572189fdf285cfb6f5c 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/stripe_payment.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=eaa1e0ae4bddcfc7826bdbd33363e2bb 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/stripe_payment.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=b59d0398e3b84686dc6a7cd28c1855d5 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/stripe_payment.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=86926a7fc4f302e016ce96041856bf20 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/stripe_payment.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=e02dd6e259df06d387ed680874f3b8e4 2500w" />
    On the payment page, please follow these steps:

    1. Select the currency for payment
    2. Confirm that the email is the same as your ComfyUI registration email
    3. Choose your payment method

    * Credit Card
    * WeChat (only supported when paying in USD)
    * Alipay (only supported when paying in USD)

    4. Click the `Pay` button or the `Generate QR Code` button to complete the payment process
  </Step>

  <Step title="Complete payment and check your credit balance">
    <img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/purchase-2.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=72a186b7778a5623ba1f3798606c7035" alt="Payment Successful" data-og-width="3310" width="3310" data-og-height="1828" height="1828" data-path="images/interface/setting/purchase-2.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/purchase-2.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=6d325e2f6e3e0d80f29e85605e841653 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/purchase-2.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=ff5671da00b37988bf5869b27529b2bd 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/purchase-2.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=a8f76852a92c067b9fef57633c3de12e 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/purchase-2.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=02943a3d7d5b5c56800cef4cdd56eb00 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/purchase-2.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=5738899d6f145c98b15b799b564ca0e5 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/purchase-2.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=63fbfb592f3b5c7858c3573bab416820 2500w" />
    After completing the payment, please return to `Menu` -> `Credits` to check if your balance has been updated. Try refreshing the interface or restarting if necessary
  </Step>
</Steps>

## Frequently Asked Questions

<AccordionGroup>
  <Accordion title="Can credits go negative?">
    No, when your credit balance is negative, you will not be able to run API Nodes
  </Accordion>

  <Accordion title="Can I get a refund for unused credits?">
    Currently, we do not support refunds
  </Accordion>

  <Accordion title="How do I check my current balance and usage?">
    Click on `Settings` -> `Credits` to see your current balance and access the `Credit History` entry
  </Accordion>

  <Accordion title="Can I share my credits with other users?">
    You can log into the same account on multiple devices, but we do not support sharing credits with other users
  </Accordion>

  <Accordion title="How do I know how many credits I've consumed each time?">
    Due to different image sizes and generation quantities, the `Tokens` and `Credits` consumed each time vary. In `Settings` -> `Credits`, you can see the credits consumed each time and the corresponding credit history
  </Accordion>

  <Accordion title="Why don't I see WeChat or Alipay payment options?">
    Please ensure you are paying in USD, as WeChat and Alipay are only supported when paying in USD
  </Accordion>

  <Accordion title="Do credits expire?">
    Yes, the expiration depends on the type of credits:

    * **Monthly credits**: Expire at the end of your billing period
    * **Top-up credits**: Expire 1 year from the date of purchase
  </Accordion>
</AccordionGroup>


# Node docs - View documentation for any node
Source: https://docs.comfy.org/interface/features/node-docs

Learn how to access built-in documentation for nodes in ComfyUI, including both built-in and custom nodes.

Node docs provide quick access to documentation for any node in your workflow. You can view detailed information about a node's functionality, parameters, and usage examples directly within ComfyUI.

## How to view node docs

<img src="https://mintcdn.com/dripart/dqTrh-Pf-n3XjOH5/images/interface/features/nod-docs/node-docs-1.jpg?fit=max&auto=format&n=dqTrh-Pf-n3XjOH5&q=85&s=f0f84fbc3d0b13d7aea79e61c83a7369" alt="Node docs" data-og-width="2004" width="2004" data-og-height="1182" height="1182" data-path="images/interface/features/nod-docs/node-docs-1.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/dqTrh-Pf-n3XjOH5/images/interface/features/nod-docs/node-docs-1.jpg?w=280&fit=max&auto=format&n=dqTrh-Pf-n3XjOH5&q=85&s=11f5369b0a41c66bfce672adb0734971 280w, https://mintcdn.com/dripart/dqTrh-Pf-n3XjOH5/images/interface/features/nod-docs/node-docs-1.jpg?w=560&fit=max&auto=format&n=dqTrh-Pf-n3XjOH5&q=85&s=73de7505a2910b38a7ebc3057ba73a88 560w, https://mintcdn.com/dripart/dqTrh-Pf-n3XjOH5/images/interface/features/nod-docs/node-docs-1.jpg?w=840&fit=max&auto=format&n=dqTrh-Pf-n3XjOH5&q=85&s=0bf4bae0fc3d26ad3c359a542b0755f8 840w, https://mintcdn.com/dripart/dqTrh-Pf-n3XjOH5/images/interface/features/nod-docs/node-docs-1.jpg?w=1100&fit=max&auto=format&n=dqTrh-Pf-n3XjOH5&q=85&s=5a0a7bc1b150a6a9f0c6b017551b4b09 1100w, https://mintcdn.com/dripart/dqTrh-Pf-n3XjOH5/images/interface/features/nod-docs/node-docs-1.jpg?w=1650&fit=max&auto=format&n=dqTrh-Pf-n3XjOH5&q=85&s=80f852bd83a5fc0d6cc943fd7e4ba6fa 1650w, https://mintcdn.com/dripart/dqTrh-Pf-n3XjOH5/images/interface/features/nod-docs/node-docs-1.jpg?w=2500&fit=max&auto=format&n=dqTrh-Pf-n3XjOH5&q=85&s=dede06d1cd98aade8deec603217584c3 2500w" />

1. **Select a node:**\
   Click on any node in your workflow to select it.

2. **Open node docs:**\
   Click the node info icon in the selection toolbox to open the node documentation page.

3. **Exit node docs:**\
   If you want to exit the node docs, you can click the arrow icon in the top left corner to exit the node docs.

You can also access the node docs via the node library, hover on a node then click the "?" icon to open the node docs.

<img src="https://mintcdn.com/dripart/dqTrh-Pf-n3XjOH5/images/interface/features/nod-docs/node-docs-2.jpg?fit=max&auto=format&n=dqTrh-Pf-n3XjOH5&q=85&s=d1703627c0f200a24ac58c1366a3f51a" alt="Node docs" data-og-width="2000" width="2000" data-og-height="1180" height="1180" data-path="images/interface/features/nod-docs/node-docs-2.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/dqTrh-Pf-n3XjOH5/images/interface/features/nod-docs/node-docs-2.jpg?w=280&fit=max&auto=format&n=dqTrh-Pf-n3XjOH5&q=85&s=30b204a01dfc4182c3c604b0d2f43888 280w, https://mintcdn.com/dripart/dqTrh-Pf-n3XjOH5/images/interface/features/nod-docs/node-docs-2.jpg?w=560&fit=max&auto=format&n=dqTrh-Pf-n3XjOH5&q=85&s=00cd8c3f44bb78f0dfacfddf52cfc67a 560w, https://mintcdn.com/dripart/dqTrh-Pf-n3XjOH5/images/interface/features/nod-docs/node-docs-2.jpg?w=840&fit=max&auto=format&n=dqTrh-Pf-n3XjOH5&q=85&s=dbbcff713f1a63b266edbc302f91b945 840w, https://mintcdn.com/dripart/dqTrh-Pf-n3XjOH5/images/interface/features/nod-docs/node-docs-2.jpg?w=1100&fit=max&auto=format&n=dqTrh-Pf-n3XjOH5&q=85&s=9e0e32dcb3a6253941c55fc247f5baec 1100w, https://mintcdn.com/dripart/dqTrh-Pf-n3XjOH5/images/interface/features/nod-docs/node-docs-2.jpg?w=1650&fit=max&auto=format&n=dqTrh-Pf-n3XjOH5&q=85&s=70981aa1ed82a43cc813af16c24227b2 1650w, https://mintcdn.com/dripart/dqTrh-Pf-n3XjOH5/images/interface/features/nod-docs/node-docs-2.jpg?w=2500&fit=max&auto=format&n=dqTrh-Pf-n3XjOH5&q=85&s=1b655877037204e7152afc473da92faf 2500w" />

## Built-in node documentation

Documentation for ComfyUI's built-in nodes is maintained in the [embedded-docs repository](https://github.com/Comfy-Org/embedded-docs).

**Contributing to built-in node docs:**

* Users can contribute improvements and additions to built-in node documentation
* Submit pull requests to the [embedded-docs repository](https://github.com/Comfy-Org/embedded-docs)
* Help improve documentation quality for the entire ComfyUI community

## Custom node documentation

Custom nodes can also include their own documentation. When custom node authors provide documentation, it will automatically appear in the node docs panel when you select their nodes.

<Card title="Developer guide: Adding node docs" icon="code" href="/custom-nodes/help_page">
  Learn how to create rich documentation for your custom nodes
</Card>


# Partial Execution - Run Only Part of Your workflow in ComfyUI
Source: https://docs.comfy.org/interface/features/partial-execution

How to use and the requirements for the Partial Execution feature in ComfyUI

<img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/partial-execution/partial-execution-icon.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=790f2410f7764701d6569a0bcdb8848a" alt="Partial Execution Feature" data-og-width="1254" width="1254" data-og-height="530" height="530" data-path="images/interface/features/partial-execution/partial-execution-icon.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/partial-execution/partial-execution-icon.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=a15bf6730203c7f1ae919d827d8b532a 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/partial-execution/partial-execution-icon.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=98753703cd874327dc14a0e042684fe2 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/partial-execution/partial-execution-icon.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=b6533929a7339b99b8120b2edf3eef3f 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/partial-execution/partial-execution-icon.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=8a89d4d2e27bc35be992cbd0b044a50f 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/partial-execution/partial-execution-icon.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=d16057a8ac6becff9ea767f94160c804 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/partial-execution/partial-execution-icon.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=2a5749cb3ce13e6128f06c6ccf40eeb4 2500w" />

The Partial Execution feature is located in the ComfyUI node selection toolbox. It allows you to run only a part of your workflow instead of executing all nodes. This feature is only available when the selected node is an output node, and when available, it appears as a green triangle icon.

## What is Partial Execution?

Partial Execution, as the name suggests, lets you run only a part of your workflow instead of executing all nodes in the workflow.

<img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/partial-execution/partial-execution-vs-run-workflow.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=5e7946c8ee4a7ad0b649a4704c53ff20" alt="Partial Execution" data-og-width="1775" width="1775" data-og-height="834" height="834" data-path="images/interface/features/partial-execution/partial-execution-vs-run-workflow.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/partial-execution/partial-execution-vs-run-workflow.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=d6ba18df7c50e5c8df85e275073679c4 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/partial-execution/partial-execution-vs-run-workflow.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=1b5c07cf73137e9f603da8badad607f5 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/partial-execution/partial-execution-vs-run-workflow.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=aec697713a449c9abbe3c772eb871d7d 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/partial-execution/partial-execution-vs-run-workflow.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=420613fd2964d45baf1a53b3d22996eb 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/partial-execution/partial-execution-vs-run-workflow.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=02a9c63a0357ac77fe591da731d27a99 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/partial-execution/partial-execution-vs-run-workflow.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=25b0dee989ff504b5f4e769224c8e8af 2500w" />

The diagram above compares Partial Execution and running the entire workflow:

1. Partial Execution (left): Only runs the branch of the workflow from the starting node to the output node.
2. Run Workflow (right): Runs all nodes in the workflow.

This feature allows you to flexibly execute specific parts of your workflow, rather than running the entire workflow every time.

## How to Use the Partial Execution Feature?

<img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/partial-execution/requirement.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=fa00fd6867f29fdecfb6da4caa9a9c85" alt="Partial Execution Requirements" data-og-width="1311" width="1311" data-og-height="422" height="422" data-path="images/interface/features/partial-execution/requirement.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/partial-execution/requirement.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=c1aaaab45238408187aebc6866235a26 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/partial-execution/requirement.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=62e8baac8cc30674f8d8b4ba71cc3ceb 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/partial-execution/requirement.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=f11d3aaabd7638311b8384c7476c0257 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/partial-execution/requirement.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=5bdf23e88670e1328ecae65c15bfa330 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/partial-execution/requirement.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=0f28c61141927e77e2d1a7528748993c 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/partial-execution/requirement.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=eccc42d70db6ba778720ec1115b6c7de 2500w" />

To use the Partial Execution feature, the currently selected node must be an output node, such as preview or save nodes. When the corresponding node meets the criteria, after selecting the node, the button in the toolbox will display as a green triangle icon. Click this icon to run the partial workflow.

## Common Issues

Q: Why do all nodes run when I use this feature?
A: Please ensure your ComfyUI frontend version is after v1.23.4, or possibly requires v1.24.x version. The corresponding bug was fixed around version 1.24.x, so please update your ComfyUI to the latest version to ensure the frontend version meets the requirements.


# Subgraph - Simplify your workflow
Source: https://docs.comfy.org/interface/features/subgraph

An introduction to the Subgraph feature in ComfyUI, including how to create, navigate, and manage subgraphs.

<Note>
  The subgraph feature requires ComfyUI frontend version 1.24.3 or later. If you don't see this feature, please refer to: [How to Update ComfyUI](/installation/update_comfyui)

  * Images in this document are made with nightly version frontend, please refer to the actual interface
  * Some features like converting subgraph back to nodes will be supported in the future
</Note>

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/xgQoGT-VpxE?si=hD5196gcX0RW-0Ko" title="ComfyUI Selection Toolbox New Features" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

A subgraph is a powerful ComfyUI feature that lets you package complex workflows into a single reusable subgraph node, making them easier to manage and share.

<img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=931b7a8e3c9c167864a963b46602208f" alt="Subgraph" data-og-width="3080" width="3080" data-og-height="814" height="814" data-path="images/interface/features/subgraph/subgraph.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=7484ecff612a8790621622f2291818ca 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=ffb9eb854478f05648f36d6551dba497 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=510c48cb75c3de020a318ccec4cd1fe5 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=f3619aed6ae3b013a2265deac64ddd52 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=5e36e319fa749583004ee266d53ef793 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=7202b1be7d6f81497bc8b358608d91f5 2500w" />

Think of a subgraph as a "folder" for your workflow  you can group related nodes together and use the entire collection as one unified subgraph node.

**Use subgraphs to:**

* Simplify complex workflows
* Reuse common node combinations
* Build more efficient workflows with modular components

## Creating a Subgraph

<Steps>
  <Step title="Select nodes">
    Select the nodes you want to group in ComfyUI
  </Step>

  <Step title="Click the subgraph icon">
    <img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_icon.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=03070a0d4e9ea4289c32d63d0c99ae8d" alt="Subgraph icon" data-og-width="1517" width="1517" data-og-height="700" height="700" data-path="images/interface/features/subgraph/subgraph_icon.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_icon.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=0dd55888b474edadae51bee82901d398 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_icon.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=0cd57cd019aa94a24145809c89fcd8bb 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_icon.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=529ff40b5d7a0b888e367ff49b0b7bfe 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_icon.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=d09520a5f62043b0aadd2af169e3cd64 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_icon.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=0bcbb82ff01f07841bf18ce3c77b2fb2 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_icon.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=e5db376d8c58dd8224557070b36dcd31 2500w" />
    Find the subgraph icon in the toolbar
  </Step>

  <Step title="Subgraph created">
    <img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/workflow_using_subgraph.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=66f5a81ffad4b33458c9f3c273601caa" alt="Workflow using subgraph" data-og-width="1820" width="1820" data-og-height="884" height="884" data-path="images/interface/features/subgraph/workflow_using_subgraph.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/workflow_using_subgraph.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=c05457df039bdf78c996cc0b62d21afa 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/workflow_using_subgraph.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=4c31a704ed275c11cb87d4a0278872da 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/workflow_using_subgraph.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=2bfbacd9216b472a907a49fa0831df00 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/workflow_using_subgraph.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=cb5b011244fb07b78a2711c2331fcb6b 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/workflow_using_subgraph.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=901d398b4feb0293090047763852d066 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/workflow_using_subgraph.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=037d19249e1f20481c2ea133d315b510 2500w" />
    ComfyUI automatically creates a subgraph based on your selected nodes' inputs and outputs
  </Step>

  <Step title="Customize your subgraph">
    Refer to [Editing Subgraphs](#editing-subgraphs), you can edit and organize the subgraph to create a fully functional node
    <img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_after_edited.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=091d4223d6fb2cd2d44009cfee191bab" alt="Workflow using subgraph" data-og-width="1820" width="1820" data-og-height="884" height="884" data-path="images/interface/features/subgraph/subgraph_after_edited.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_after_edited.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=139a8e65714a299ad5dc49bac126e031 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_after_edited.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=626c34ba0e4de784d8cfcae4566970a0 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_after_edited.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=cf7a4be96f187e7d8939834a33af3744 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_after_edited.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=ce5f3c3b7bee31df3dadbfd87d00645f 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_after_edited.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=e206f63587afa90fee39dd611f0de73e 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_after_edited.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=43cb284ca2fd2eca53ee9581563dbc44 2500w" />
  </Step>
</Steps>

## Working with Subgraphs

### Basic Operations

Subgraphs work just like regular nodes. You can:

* Change colors and names
* Use bypass to disable
* Apply all standard node operations

### Editing Subgraphs

**To enter edit mode:**

* Double-click the empty area inside the subgraph (not on widgets), or
* Click the subgraph edit button

<img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/editing_subgraph.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=4802435f0ede07e1172b9f7fbd459399" alt="Subgraph editing mode" data-og-width="1820" width="1820" data-og-height="1203" height="1203" data-path="images/interface/features/subgraph/editing_subgraph.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/editing_subgraph.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=c5004638bdaae3d2edcd8aa9e87e1a22 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/editing_subgraph.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=5784b8ab234eade79fd01dc289ccc16f 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/editing_subgraph.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=fdde61548dc9421b794ee6ad221e09c7 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/editing_subgraph.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=b4e60fafda5a9d0ae693c34d64bc6fc5 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/editing_subgraph.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=d68f2a0aefa79203affffdfdb6ae4750 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/editing_subgraph.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=c09e3b4e183792d54740d945eac2f5fb 2500w" />

**In edit mode you'll see:**

1. **Navigation bar**: Exit the current subgraph and return to the parent level
2. **Input slots**: Internal node inputs exposed to the outside
   * Connect outputs to slots like normal nodes
   * **Right-click** connection points to rename/delete exposed slots
3. **Output slots**: Outputs exposed to the outside (same functionality as input slots)

<img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_slot.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=5f4cdd87ea46007d9bf35921576f12cb" alt="Subgraph slots" data-og-width="1736" width="1736" data-og-height="921" height="921" data-path="images/interface/features/subgraph/subgraph_slot.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_slot.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=a9a3a3a362787bd4ec8baf0b81d7f3be 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_slot.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=cad68d957f2a21f92df351efaf61a722 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_slot.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=0f807ae47f75ca5fb378eba4db01a836 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_slot.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=0c36bf5ddf538699ee69823b93ce1d5f 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_slot.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=4df72f73a01f53cd944bb50a067c8266 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_slot.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=27ad35bf488e79c92e25efd4132c4e0a 2500w" />

**Working with slots:**

1. **Default slot** (labeled 1): Use this to add new input/output connections
2. **Right-click** existing slots to rename, delete, or disconnect from original nodes

> Note: Slot connections follow standard data type validation rules

### Parameters Panel

With ComfyUI v0.3.66 or later, you can edit the subgraph parameters directly from the parameters panel without entering the subgraph.

You can select any subgraph and use the "Edit Subgraph Widgets" button to open the parameters panel.
<img src="https://mintcdn.com/dripart/l9YPgCbipT00aLbn/images/interface/features/subgraph/parameters_panel_open.jpg?fit=max&auto=format&n=l9YPgCbipT00aLbn&q=85&s=efe2c0c6b281f2fe19b2e814454c2f82" alt="Open Parameters Panel" data-og-width="1330" width="1330" data-og-height="1176" height="1176" data-path="images/interface/features/subgraph/parameters_panel_open.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/l9YPgCbipT00aLbn/images/interface/features/subgraph/parameters_panel_open.jpg?w=280&fit=max&auto=format&n=l9YPgCbipT00aLbn&q=85&s=e1bc5a95dc4ebdfcfdfcf52f8ddc4413 280w, https://mintcdn.com/dripart/l9YPgCbipT00aLbn/images/interface/features/subgraph/parameters_panel_open.jpg?w=560&fit=max&auto=format&n=l9YPgCbipT00aLbn&q=85&s=7f6bb92fa13d3bf2391ad78655cd911c 560w, https://mintcdn.com/dripart/l9YPgCbipT00aLbn/images/interface/features/subgraph/parameters_panel_open.jpg?w=840&fit=max&auto=format&n=l9YPgCbipT00aLbn&q=85&s=b49651ee712172a415be39bc2a02a4a3 840w, https://mintcdn.com/dripart/l9YPgCbipT00aLbn/images/interface/features/subgraph/parameters_panel_open.jpg?w=1100&fit=max&auto=format&n=l9YPgCbipT00aLbn&q=85&s=badede49baa2a56d4443713d8ae34818 1100w, https://mintcdn.com/dripart/l9YPgCbipT00aLbn/images/interface/features/subgraph/parameters_panel_open.jpg?w=1650&fit=max&auto=format&n=l9YPgCbipT00aLbn&q=85&s=4a524b24649b4f98dbaa1ad7f14c9ee6 1650w, https://mintcdn.com/dripart/l9YPgCbipT00aLbn/images/interface/features/subgraph/parameters_panel_open.jpg?w=2500&fit=max&auto=format&n=l9YPgCbipT00aLbn&q=85&s=71f5cfae2c659d52a59a9279b293a935 2500w" />

After it is opened, you can edit the order and visibility of the subgraph widgets directly in the parameters panel.

<img src="https://mintcdn.com/dripart/l9YPgCbipT00aLbn/images/interface/features/subgraph/parameters_panel_edit.jpg?fit=max&auto=format&n=l9YPgCbipT00aLbn&q=85&s=4c77e4e50454e6ccd15c2d02a64feb0e" alt="Open Parameters Panel" data-og-width="1456" width="1456" data-og-height="1492" height="1492" data-path="images/interface/features/subgraph/parameters_panel_edit.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/l9YPgCbipT00aLbn/images/interface/features/subgraph/parameters_panel_edit.jpg?w=280&fit=max&auto=format&n=l9YPgCbipT00aLbn&q=85&s=b693731c43937fbaa374fe94be350fae 280w, https://mintcdn.com/dripart/l9YPgCbipT00aLbn/images/interface/features/subgraph/parameters_panel_edit.jpg?w=560&fit=max&auto=format&n=l9YPgCbipT00aLbn&q=85&s=72695443ec942f5eeda51af85acbda41 560w, https://mintcdn.com/dripart/l9YPgCbipT00aLbn/images/interface/features/subgraph/parameters_panel_edit.jpg?w=840&fit=max&auto=format&n=l9YPgCbipT00aLbn&q=85&s=6a73b1061c2ea3d40ac6ef49b3016bcb 840w, https://mintcdn.com/dripart/l9YPgCbipT00aLbn/images/interface/features/subgraph/parameters_panel_edit.jpg?w=1100&fit=max&auto=format&n=l9YPgCbipT00aLbn&q=85&s=1eab5b592ecbab71f5a2b22c96926431 1100w, https://mintcdn.com/dripart/l9YPgCbipT00aLbn/images/interface/features/subgraph/parameters_panel_edit.jpg?w=1650&fit=max&auto=format&n=l9YPgCbipT00aLbn&q=85&s=b99457333b5d92aaa197bf376ead1717 1650w, https://mintcdn.com/dripart/l9YPgCbipT00aLbn/images/interface/features/subgraph/parameters_panel_edit.jpg?w=2500&fit=max&auto=format&n=l9YPgCbipT00aLbn&q=85&s=94460ffb4449c56e91eea6aacbde7fe8 2500w" />

1. Reordering: you use right-click and hold the widget to drag it to the desired position
2. Visibility: you can check the visibility of the widget by clicking the eye icon

### Nested Subgraphs

Create even more complex workflows by nesting subgraphs within subgraphs.

<img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_nested.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=7e92ac585c91b067ab2527da1b5c521c" alt="Nested subgraph" data-og-width="1820" width="1820" data-og-height="1203" height="1203" data-path="images/interface/features/subgraph/subgraph_nested.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_nested.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=a98bb3ff8b829e604fa4bb30ec9e13c2 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_nested.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=7def59b7271a33c9d9580d9124672f60 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_nested.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=54bb00b8554c5467fc22d6e2ec4cb4ed 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_nested.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=a604574cadfd063eff7194cef285b62c 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_nested.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=a4d75bab687f549c3479e2cf6a391e96 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_nested.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=1a8838a69eadae908cb1ef65185d73fb 2500w" />

The navigation bar shows your current level and lets you easily move between nested subgraphs.

<img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_navigation.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=81b0b1bdb46f54e856600de892cf7089" alt="Nested subgraph navigation" data-og-width="1820" width="1820" data-og-height="1203" height="1203" data-path="images/interface/features/subgraph/subgraph_navigation.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_navigation.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=b286efcf22cd5369ecb2574ee6b7c936 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_navigation.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=6be4422e80fed2df3695b27d0793fe85 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_navigation.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=11cf00c50eb0b4e40ea66f9c703b7212 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_navigation.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=8505431b1f4457f9279ae49cde0e7700 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_navigation.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=f6f0268fb51976095cf28f8428ab56df 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_navigation.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=0f55b6380a1981b1770194ec4b849619 2500w" />

### Exit subgraph

To exit a subgraph and return to the parent level, use the **Navigation bar** (labeled 1 in the image) at the top of the canvas.

<img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/editing_subgraph.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=4802435f0ede07e1172b9f7fbd459399" alt="Subgraph editing mode" data-og-width="1820" width="1820" data-og-height="1203" height="1203" data-path="images/interface/features/subgraph/editing_subgraph.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/editing_subgraph.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=c5004638bdaae3d2edcd8aa9e87e1a22 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/editing_subgraph.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=5784b8ab234eade79fd01dc289ccc16f 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/editing_subgraph.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=fdde61548dc9421b794ee6ad221e09c7 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/editing_subgraph.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=b4e60fafda5a9d0ae693c34d64bc6fc5 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/editing_subgraph.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=d68f2a0aefa79203affffdfdb6ae4750 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/editing_subgraph.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=c09e3b4e183792d54740d945eac2f5fb 2500w" />

Click on the navigation bar to exit the current subgraph and return to the parent workflow.

## Unpack Subgraphs to Nodes

When you're done creating a subgraph, you can convert it back to nodes if you need to.

<img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_to_nodes.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=3f6dfede84ad760f7775846259e0b3e6" alt="Subgraph to node" data-og-width="912" width="912" data-og-height="548" height="548" data-path="images/interface/features/subgraph/subgraph_to_nodes.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_to_nodes.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=addf861fcb10c6ffb94dc3467730618c 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_to_nodes.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=4afb9253344041b4196f0d405943e506 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_to_nodes.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=9b11700c233cda409b0deb8b673dda55 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_to_nodes.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=f6194c67fe2ecfc9e41c1e6f14ae495f 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_to_nodes.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=251cde5945070473fb15fa76b11668c5 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/subgraph/subgraph_to_nodes.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=e335ec3ffc26e4ef6f03c283adba1bb6 2500w" />

1. You can select the subgraph node then use right-click menu "Unpack subgraph" to convert it back to nodes.
2. Click the "Unpack subgraph" button in the select toolbox to convert it back to nodes.

## Subgraph Blueprint

With ComfyUI frontend version 1.27.7 or later, you can publish your subgraph to the node library.

This feature allows you to convert a subgraph to a `Subgraph Blueprints` node, which means it's a reusable subgraph node.

### Publish Subgraph to Node Library

<img src="https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/subgraph_publishing.jpg?fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=5079af05b2211b0547a3cc3b999c3c1a" alt="Publish Subgraph" data-og-width="1286" width="1286" data-og-height="1214" height="1214" data-path="images/interface/features/subgraph/subgraph_publishing.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/subgraph_publishing.jpg?w=280&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=29e2581b133c92737c301b43e0a20df1 280w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/subgraph_publishing.jpg?w=560&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=3bcc71ef0d24637239023e3cdbab56df 560w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/subgraph_publishing.jpg?w=840&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=301c24eb26a93299e2cc4b4fae8eba5c 840w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/subgraph_publishing.jpg?w=1100&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=a339604f87d7d63c891543ee3607c1ad 1100w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/subgraph_publishing.jpg?w=1650&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=e27886b5e462b7ae4fd837b376daab4a 1650w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/subgraph_publishing.jpg?w=2500&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=9970571e2c50bbe63b52cfde34468f48 2500w" />

Currently, you have two ways to publish subgraph to node library, both are in the selection toolbox:

1. Click the `book(publish)` icon on the selection toolbox
2. Open the selection toolbox menu, use the `Add Subgraph to Library` menu to publish subgraph

After you click on the `book(publish)` icon or `Add Subgraph to Library` menu, you will see the following dialog:

<img src="https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/subgraph_naming.jpg?fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=0b20fc138a6e4e8ac826176067b5848e" alt="Subgraph naming" data-og-width="1286" width="1286" data-og-height="840" height="840" data-path="images/interface/features/subgraph/subgraph_naming.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/subgraph_naming.jpg?w=280&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=5083e28f19b2931c6aae05d7f5c5b1d3 280w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/subgraph_naming.jpg?w=560&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=63d78999de119d283d542805eafcfa58 560w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/subgraph_naming.jpg?w=840&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=719ddf790a00869d79e82a6a4a0f4514 840w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/subgraph_naming.jpg?w=1100&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=ab19c52163f56fcf90b232a443275632 1100w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/subgraph_naming.jpg?w=1650&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=2ee570463b0291e86e2afd7c269bc143 1650w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/subgraph_naming.jpg?w=2500&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=3068efa382497f73ba39a8ccfda0003a 2500w" />

By default, the subgraph will use the name of the subgraph node as the name of the subgraph blueprint.

After publishing, you will see the subgraph blueprint node in the node library.

<img src="https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/subgraph_blueprints.jpg?fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=90242f76da15ce0de949692114757aeb" alt="Subgraph blueprint node" data-og-width="2003" width="2003" data-og-height="1242" height="1242" data-path="images/interface/features/subgraph/subgraph_blueprints.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/subgraph_blueprints.jpg?w=280&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=82212b77bae2a2e794e5a9fbb01573ab 280w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/subgraph_blueprints.jpg?w=560&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=d61c4a3c34f82da681455976f6867b64 560w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/subgraph_blueprints.jpg?w=840&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=1560181fb4b2ab798eea193703df68ef 840w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/subgraph_blueprints.jpg?w=1100&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=e8f7ed0bf63d0d432434981fb70e26af 1100w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/subgraph_blueprints.jpg?w=1650&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=e6e0d590a98c979171a0a1e0b1ac37d6 1650w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/subgraph_blueprints.jpg?w=2500&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=887b43b903c9e3f76db59702ff2801a4 2500w" />

Now, you can drag or search the subgraph just like a normal node. The new subgraph node that has been added from Subgraph Blueprints is still isolated, which means after adding it to the workflow, it can be edited independently, they will not affect each other.

### Edit Subgraph Blueprint

If you want to edit the subgraph blueprint, you can click the edit button just like the image below, you can delete it as well.

<img src="https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/edit_subgraph_blueprints.jpg?fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=d5d63e62403ea8cf4b95274c968ab835" alt="Edit Subgraph Blueprint" data-og-width="2000" width="2000" data-og-height="1245" height="1245" data-path="images/interface/features/subgraph/edit_subgraph_blueprints.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/edit_subgraph_blueprints.jpg?w=280&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=a7ed619282d9cbe8eb5162d4f02dd4ad 280w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/edit_subgraph_blueprints.jpg?w=560&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=a0730a03c3ff65d1b8e90e5cc6f684b3 560w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/edit_subgraph_blueprints.jpg?w=840&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=99c07e5d038060c1b480ec3572a83dc9 840w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/edit_subgraph_blueprints.jpg?w=1100&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=e75f6845171ab376132850d2e9545a89 1100w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/edit_subgraph_blueprints.jpg?w=1650&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=34e43eb8c09e1fe9db2db904d9c401a2 1650w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/edit_subgraph_blueprints.jpg?w=2500&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=d1289aa590a56b59d3f25cfc7ae8058b 2500w" />

This will enable the subgraph editing mode

<img src="https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/subgraph_editing_mode.jpg?fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=de6bfcfe9561003dc7fe738eff6acdc2" alt="Subgraph editing mode" data-og-width="2000" width="2000" data-og-height="1359" height="1359" data-path="images/interface/features/subgraph/subgraph_editing_mode.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/subgraph_editing_mode.jpg?w=280&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=93f1eae882fe5fc32f81e0489b67481c 280w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/subgraph_editing_mode.jpg?w=560&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=30fb6521e389de81c556100261194549 560w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/subgraph_editing_mode.jpg?w=840&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=9457940713df4050a7966ed11fbf6fe7 840w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/subgraph_editing_mode.jpg?w=1100&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=a13f8aba451eebcd18e40663f05f5568 1100w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/subgraph_editing_mode.jpg?w=1650&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=6c5182b163ff7860acfe8caf4d452e52 1650w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/subgraph_editing_mode.jpg?w=2500&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=20f0905025a4ea70273de710b5efeb16 2500w" />

After editing the subgraph blueprint, you can go to the parent level to preview the subgraph.

<img src="https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/save_updated_subgraph.jpg?fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=976ddb2fc548798253b795d6f19aa044" alt="Update Subgraph Blueprint" data-og-width="2000" width="2000" data-og-height="1359" height="1359" data-path="images/interface/features/subgraph/save_updated_subgraph.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/save_updated_subgraph.jpg?w=280&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=e2054a8c9ff84f911d7d2a634f7c3cfe 280w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/save_updated_subgraph.jpg?w=560&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=612d8ceffaf348a65f9e86baf271f8a8 560w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/save_updated_subgraph.jpg?w=840&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=374109865fd37381fcbcaa2519d132c8 840w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/save_updated_subgraph.jpg?w=1100&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=b8368191351b9489bf6061970b2fbe0e 1100w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/save_updated_subgraph.jpg?w=1650&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=eea83242299e62247324d63926a6f2f9 1650w, https://mintcdn.com/dripart/fLKXN8EDUY0BDXQk/images/interface/features/subgraph/save_updated_subgraph.jpg?w=2500&fit=max&auto=format&n=fLKXN8EDUY0BDXQk&q=85&s=68f2d9dc08241dc8bf608dcfa354ae02 2500w" />

If you want to save the updated subgraph blueprint, you can click the save button or use the shortcut key Ctrl + S.


# Templates - ComfyUI Built-in Workflow Templates
Source: https://docs.comfy.org/interface/features/template

Templates provide model workflows natively supported by ComfyUI and example workflows from custom nodes. You can find and use workflows for currently supported models here.

Workflow Templates is the browser for ComfyUI's natively supported model workflows and also some example workflows from custom nodes.

In ComfyUIs Workflow Templates, you can find:

* Natively supported model workflows
* Example workflows from custom nodes

<img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/template/template-dialog.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=d10f19d6800c660cf0c0bd07753efaa6" alt="Template" data-og-width="2000" width="2000" data-og-height="1219" height="1219" data-path="images/interface/features/template/template-dialog.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/template/template-dialog.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=88d64800a602e930b48c4ee0694a5c2d 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/template/template-dialog.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=9cc0d1b58ac943a62799785ed885184a 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/template/template-dialog.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=a1e3158b06c011eeb07f25e2aed9882e 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/template/template-dialog.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=3bbd163b05ebb332b457a78658944037 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/template/template-dialog.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=6a8db554e87a9227e0a72ae821cec406 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/template/template-dialog.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=6375f5f1d3118f08bad0d63ef1441f36 2500w" />

## How to open Templates in ComfyUI

Open via the menu `Workflow` --> `Browse Workflow Templates`.
<img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/template/template.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=a6d3448741e4e62e32f84778c33f03b0" alt="Template" data-og-width="781" width="781" data-og-height="744" height="744" data-path="images/interface/features/template/template.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/template/template.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=595e4f5b4c7074316b28854dbb956c2d 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/template/template.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=729b659e3660b9e49b0e069e6c7efbc7 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/template/template.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=eefafed40f4778bfe8ed8b45542a5f45 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/template/template.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=8d564ad9ac0d5c608c990f635197b4cd 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/template/template.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=81971a2f3c52c839ae22690fdfb92c7c 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/template/template.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=c7b0c1cd536f6c60abc3504949dabbe3 2500w" />

## How to use templates

1. Load a template: Click any template you want to load its workflow.
2. Download models: When loading a template, ComfyUI automatically checks whether all required model files exist. If anything is missing, it will prompt you to download the models.
3. Run the workflow: Once all requirementssuch as models, input images, and promptsare ready, click the Run button to start using the workflow.

### Model storage location

Each workflow template embeds links to the required models. On first use, if the corresponding model files are not detected, you will see a download prompt.

<img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/template/missing_models.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=8c1d9ac6dd0acf6d220eda2842a41434" alt="Missing Models" data-og-width="1458" width="1458" data-og-height="1102" height="1102" data-path="images/interface/features/template/missing_models.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/template/missing_models.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=1397f3d56c99a7518797cc8aff843c3f 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/template/missing_models.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=055255bd3fe829d45b27ea3e23664712 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/template/missing_models.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=d50a3fff5710b2ceadfddb537894a290 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/template/missing_models.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=a28f6f79541751745e4fcdca1135efaa 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/template/missing_models.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=58e53a6d2149648866a188474895d252 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/features/template/missing_models.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=e6628a148f02dcb622c1827ac339fb97 2500w" />

1. For desktop version, when you click the `Download` button, the desktop program will automatically download the model files for you.
2. For other versions, the browser will be used to download the corresponding model. You need to download the model and save it to the corresponding folder under `ComfyUI/models`. For example, the model in the screenshot should be saved in the following location:

```
 ComfyUI/
  models/
     diffusion_models/
       qwen_image_fp8_e4m3fn.safetensors
     vae/
       qwen_image_vae.safetensors
     text_encoders/
        qwen_2.5_vl_7b_fp8_scaled.safetensors
```

In the current version, missing-file detection only checks whether there is a file with the same name in the corresponding top-level directory. For example, the file must exist directly under `ComfyUI/models/diffusion_models`.
If you have already downloaded the model into a subfolder such as `ComfyUI/models/diffusion_models/wan_video`, you can ignore the popup and simply ensure the correct model is selected in the corresponding model loader node.

If you're curious how model links are embedded, we add a `models` field under the nodes `properties`. Below is a complete snippet of a `DualCLIPLoader` node with embedded model information:

```json  theme={null}
    {
      "id": 40,
      "type": "DualCLIPLoader",
      "pos": [
        -320,
        290
      ],
      "size": [
        270,
        130
      ],
      "flags": {},
      "order": 0,
      "mode": 0,
      "inputs": [],
      "outputs": [
        {
          "name": "CLIP",
          "type": "CLIP",
          "links": [
            64
          ]
        }
      ],
      "properties": {
        "Node name for S&R": "DualCLIPLoader",
        "cnr_id": "comfy-core",
        "ver": "0.3.40",
        "models": [
          {
            "name": "clip_l.safetensors",
            "url": "https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors",
            "directory": "text_encoders"
          },
          {
            "name": "t5xxl_fp16.safetensors",
            "url": "https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors",
            "directory": "text_encoders"
          }
        ]
      },
      "widgets_values": [
        "clip_l.safetensors",
        "t5xxl_fp16.safetensors",
        "flux",
        "default"
      ]
    }
```

The `models` field inside `properties` includes `name`, `url`, and `directory`.

* `name`: the model file name
* `url`: a direct download link to the file (not a repository page)
* `directory`: which subfolder under `ComfyUI/models` to store the file, e.g. `vae` means `ComfyUI/models/vae`

Currently, only links from Hugging Face and Civitai are supported. The model format must be a safe format such as `.safetensors` or `.sft`. Formats like `.gguf` are considered unsafe; when embedded they will be flagged as unsafe and the link will not be shown.

You can use [this tool](https://comfyui-wiki.github.io/ComfyUI-Workflow-JSON-Editor/) to edit the model information in workflow templates. At the moment, [@ComfyUI-Wiki](https://github.com/ComfyUI-Wiki) only adds support for native nodes.

## How to update templates?

Templates are managed and updated as a separate dependency: [`comfyui-workflow-templates`](https://pypi.org/project/comfyui-workflow-templates/).

If, after updating ComfyUI, you dont see the documentation or newly announced templates, you may need to update the corresponding dependency. You can check versions in [`ComfyUI/requirements.txt`](https://github.com/comfyanonymous/ComfyUI/blob/master/requirements.txt).

Typically, the following three dependencies may be upgraded together when ComfyUI is updated:

```
comfyui-frontend-package==1.24.4
comfyui-workflow-templates==0.1.52
comfyui-embedded-docs==0.2.4
```

If youre not sure how to update correctly, see [Update ComfyUI](/installation/update_comfyui) for how to update ComfyUI and its dependencies.

## How to contribute templates to the ComfyUI repository?

All templates are hosted in the [workflow\_templates](https://github.com/Comfy-Org/workflow_templates/) repository. You can contribute templates by submitting a PR. For official templates, we require the following:

1. Do not use any third-party nodes (to avoid extra installations for users who lack those nodes).
2. The template must not duplicate existing ones and should target supported model capabilities.
3. You may open an issue in the repository to ask questions.

## Custom node templates

If a custom node author provides templates and example workflows, you can also find them in the Templates browser. Usually, you can locate all templates by finding the category named after the node.

If you are a custom node author, note that we currently only support a single directory level under the `templates` folder (no nested subdirectories), and only JSON-format templates are supported.

## How to add templates for custom nodes?

See [Custom Node Templates](/custom-nodes/workflow_templates) for how to add workflow templates for your custom nodes.


# Mask Editor - Create and Edit Masks in ComfyUI
Source: https://docs.comfy.org/interface/maskeditor

Learn how to use the Mask Editor in ComfyUI, including settings and usage instructions

The Mask Editor is a very useful feature in ComfyUI that allows users to create and edit masks within images without needing to use other applications.

The Mask Editor is currently triggered through the `Load Image` node. After uploading an image, you can right-click on the node and select `Open in MaskEditor` from the menu to open the Mask Editor.

<img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/maskeditor/maskeditor.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=6c44553609b2b549c343953f385b8cdd" alt="ComfyUI Mask Editor" data-og-width="1082" width="1082" data-og-height="1344" height="1344" data-path="images/interface/maskeditor/maskeditor.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/maskeditor/maskeditor.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=b7a2ade26914299fde3f421992fa025e 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/maskeditor/maskeditor.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=072c660a1bbe4281601134eea338b020 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/maskeditor/maskeditor.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=ef3be676f03b0cf1d61aa34db0dbad82 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/maskeditor/maskeditor.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=bb36cbf201a785197c0e74dd7b1b6376 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/maskeditor/maskeditor.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=32af9df2f2b4bdc5a6f5ed7395a3e789 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/maskeditor/maskeditor.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=d15a19e80d97f1b3be68ce9fb5e29752 2500w" />

<img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/maskeditor/maskeditor_ui.jpg?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=1327b9215d2c5772abebcdfd3e9840aa" alt="ComfyUI Mask Editor" data-og-width="1500" width="1500" data-og-height="1099" height="1099" data-path="images/interface/maskeditor/maskeditor_ui.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/maskeditor/maskeditor_ui.jpg?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=961bf01136c869ca408f8022b9aa9aa4 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/maskeditor/maskeditor_ui.jpg?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=35969fb3d0d3e904efebb89d836b606b 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/maskeditor/maskeditor_ui.jpg?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=4fbe1c6649a551caf87f046093c78ca0 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/maskeditor/maskeditor_ui.jpg?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=353eb8b8cd54cfe6a2fb6863995b2123 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/maskeditor/maskeditor_ui.jpg?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=86bec6fe11b95d0a6b3771b496487135 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/maskeditor/maskeditor_ui.jpg?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=3ff864180319d3a3f2d9896b88676353 2500w" />

You can then click with your mouse on the image to create and edit masks.

## Video Tutorial

<video controls>
  <source src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/interface/maskeditor/maskeditor.mp4?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=f2f905b202074a6f51d1021e799d894d" type="video/mp4" data-path="images/interface/maskeditor/maskeditor.mp4" />

  Your browser does not support the video tag.
</video>


# ComfyUI Interface Overview
Source: https://docs.comfy.org/interface/overview

In this article, we will briefly introduce the basic user interface of ComfyUI, familiarizing you with the various parts of the ComfyUI interface.

The visual interface is currently the way most users utilize ComfyUI to call the [ComfyUI Server](/development/comfyui-server/comms_overview) to generate corresponding media resources. It provides a visual interface for users to operate and organize workflows, debug workflows, and create amazing works.

Typically, when you start the ComfyUI server, you will see an interface like this:

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui_new_interface.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=034e80afc6ca940a6f0dbdf18fa1cd7a" alt="ComfyUI Basic Interface" data-og-width="2000" width="2000" data-og-height="1334" height="1334" data-path="images/interface/overview/comfyui_new_interface.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui_new_interface.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=0beeca566a6bbab9eea4281073fcfd59 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui_new_interface.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=206017b2fdd64f404911be0ff3d9c614 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui_new_interface.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=33963cbf04870a8f23fb73b0256c711e 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui_new_interface.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=d671b98279f44563d949793b1ce86897 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui_new_interface.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=6e26d3c246a91a061789157046a672a3 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui_new_interface.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=223f4f4514acb247db84ff9e42e65b02 2500w" />

If you are an earlier user, you may have seen the previous menu interface like this:

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui_old_interface.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=267da568996a69c02267d78196edece2" alt="ComfyUI Old Interface" data-og-width="2000" width="2000" data-og-height="1334" height="1334" data-path="images/interface/overview/comfyui_old_interface.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui_old_interface.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=fbf58825e4cd1829fea5ddc6a6789c2b 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui_old_interface.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=c549131132fe63f05b5374cfe3cce891 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui_old_interface.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=d446ac63ff6b27df9946cd038bafdaf4 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui_old_interface.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=6b4692234bc0f0d860db61ac5611abd8 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui_old_interface.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=12557d1109eea53aba7edb85b02d047e 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui_old_interface.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=4aab864badc28f87abcc0fd73fe42957 2500w" />

Currently, the [ComfyUI frontend](https://github.com/Comfy-Org/ComfyUI_frontend) is a separate project, released and maintained as an independent pip package. If you want to contribute, you can fork this [repository](https://github.com/Comfy-Org/ComfyUI_frontend) and submit a pull request.

## Localization Support

Currently, ComfyUI supports: English, Chinese, Russian, French, Japanese, and Korean.
If you need to switch the interface language to your preferred language, you can click the **Settings gear icon** and then select your desired language under `Comfy` --> `Locale`.

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/locale.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=e5f9d37eb8ea17027846273e69bf292e" alt="ComfyUI Localization Support" data-og-width="1508" width="1508" data-og-height="936" height="936" data-path="images/interface/overview/locale.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/locale.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=aa873b328567709a7cbfc9234a5b3caf 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/locale.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=d8405253fb460123648042ba60a6679e 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/locale.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=9b890f66294afa8110f84161be072420 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/locale.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=694161d0af0a1d589e3a4eeec4bdc021 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/locale.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=cd18d49dbc8ec8ef3d5cb53d480af73f 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/locale.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=490fd56e79b7e5bf5d6d4f31729697f3 2500w" />

## New Menu Interface

### Workspace Areas

Below are the main interface areas of ComfyUI and a brief introduction to each part.

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui-new-interface-main.png?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=24c16f1242f71e5bcf11a965fc3307da" alt="ComfyUI Workspace" data-og-width="2660" width="2660" data-og-height="1580" height="1580" data-path="images/interface/overview/comfyui-new-interface-main.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui-new-interface-main.png?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=b443b8ba0c23c22354a8f8b3827798e7 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui-new-interface-main.png?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=5bbbe4e81184a1002f058e40609cc8db 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui-new-interface-main.png?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=43b9ce574d162c68abfcd492f14ca2c1 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui-new-interface-main.png?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=c74805970c2d260bd6a8d13354d64118 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui-new-interface-main.png?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=fa39aa2113f18310438364463f931153 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui-new-interface-main.png?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=04838786fff63d394f6e112c74d8e516 2500w" />

Currently, apart from the main workflow interface, the ComfyUI interface is mainly divided into the following parts:

1. Menu Bar: Provides workflow, editing, help menus, workflow execution, ComfyUI Manager entry, etc.
2. Sidebar Panel Switch Buttons: Used to switch between workflow history queue, node library, model library, local user workflow browsing, etc.
3. Theme Switch Button: Quickly switch between ComfyUI's default dark theme and light theme
4. Settings: Click to open the settings button
5. Canvas Menu: Provides zoom in, zoom out, and auto-fit operations for the ComfyUI canvas

### Menu Bar Functions

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui-new-interface-menu-bar.png?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=088e0a362fcda0b8e9eca48b6fa2763a" alt="ComfyUI Workspace" data-og-width="2734" width="2734" data-og-height="1126" height="1126" data-path="images/interface/overview/comfyui-new-interface-menu-bar.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui-new-interface-menu-bar.png?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=010e6b3431cf3ba93f9a6ff2941b3dd3 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui-new-interface-menu-bar.png?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=b605712908ff77c7cc66e8b7cdefe720 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui-new-interface-menu-bar.png?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=088c0aa9fafe9a616f4a35d88fbd0d04 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui-new-interface-menu-bar.png?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=5e916f438e9bf7267b9e10f4e8c186d1 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui-new-interface-menu-bar.png?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=a48cb6dd9321beea5eeb47085329972f 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui-new-interface-menu-bar.png?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=6232de43916a0dc2903a6cf302092a49 2500w" />

The image above shows the corresponding functions of the top menu bar, including common features, which we will explain in detail in the specific function usage section.

### Sidebar Panel Buttons

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/side-panel.png?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=138181422006bac20c5d092f729f00b3" alt="ComfyUI Sidebar Panel" data-og-width="1909" width="1909" data-og-height="1007" height="1007" data-path="images/interface/overview/side-panel.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/side-panel.png?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=d155573e5ba3006af39d10523fc5ed3c 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/side-panel.png?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=f23e97db967f522b0609f262294c0fc7 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/side-panel.png?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=40e1ecac4167eb739881c7a9db2edc53 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/side-panel.png?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=d7af26e3bb5bffcca92e68d0c9a73f15 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/side-panel.png?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=9921051b06c31e065979b569b4ec3dd5 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/side-panel.png?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=473efa4c4d5cb05cef2de7e51a06d8e1 2500w" />

In the current ComfyUI, we provide four side panels with the following functions:

1. Workflow History Queue (Queue): All queue information for ComfyUI executing media content generation
2. Node Library: All nodes in ComfyUI, including `Comfy Core` and your installed custom nodes, can be found here
3. Model Library: Models in your local `ComfyUI/models` directory can be found here
4. Local User Workflows (Workflows): Your locally saved workflows can be found here

## Old Menu Version

Currently, ComfyUI enables the new interface by default. If you prefer to use the old interface, you can click the **Settings gear icon** and then set `Use new menu` to `disabled` under `Comfy` --> `Menu` to switch to the old menu version.

<Note>
  The old menu interface only supports English.
</Note>

The function annotations for the old menu interface are explained below:

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui-old-menu.png?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=2f18e63bae1182c4a7a826d1f9385786" alt="ComfyUI Old Menu" data-og-width="928" width="928" data-og-height="1126" height="1126" data-path="images/interface/overview/comfyui-old-menu.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui-old-menu.png?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=338269cf53536041718a60f5acb77ad2 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui-old-menu.png?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=72a8cef25664abc0f6c9329dc444e30b 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui-old-menu.png?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=db4885dca2bacd1ba583c88227fc5bfc 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui-old-menu.png?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=2b8fa3683811237b47fe6f1815d3f4f0 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui-old-menu.png?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=4eb3611d79e1ecf3900c39cce8d3cbd2 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/overview/comfyui-old-menu.png?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=bf8bac2b01c3bc8b57d29ad8154bae60 2500w" />


# ComfyUI 3D Settings
Source: https://docs.comfy.org/interface/settings/3d

Detailed description of ComfyUI 3D setting options

This section of settings is mainly used to control the initialization settings of 3D-related components in ComfyUI, including camera, lighting, scene, etc. When creating new 3D components, they will be initialized according to these settings. After creation, these settings can still be adjusted individually.

## Camera

### Initial Camera Type

* **Options**:
  * `perspective`
  * `orthographic`
* **Function**: Controls whether the default camera is perspective or orthographic when creating new 3D components. This default setting can still be switched individually for each component after creation

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/camera_type.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=ec34a4175737a848fb1d7d8f94a1725a" alt="" data-og-width="2090" width="2090" data-og-height="1138" height="1138" data-path="images/interface/setting/3d/camera_type.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/camera_type.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=ac3524a333c1e9533d01c8df0bc51ec8 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/camera_type.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=b44eb8e19d64e742dfebbd503a6dd6b2 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/camera_type.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=e812bf46d121d502f05baf6921591c08 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/camera_type.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=b913709de64532fb6d590e99ef1d31d1 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/camera_type.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=b438729a1032bf9f2e88d6b991adb05f 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/camera_type.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=72ff2d41a90d789b771caf08788fadc1 2500w" />

## Light

The light settings in this section are used to set the default lighting settings for 3D components. The corresponding settings in the 3D settings in ComfyUI can also be modified.

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/light.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=ef6b3c98d829e8190f4e2c9e876b527d" alt="light" data-og-width="2090" width="2090" data-og-height="1138" height="1138" data-path="images/interface/setting/3d/light.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/light.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=b779b346bbb85c6500a68b8a38e6f090 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/light.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=af2b04b72d724dba8460a651b637e808 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/light.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=d145761b50f104a4e1394b57d6dfeceb 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/light.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=301644389c9ba00c3d7135c100a440be 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/light.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=c684dba9179a09795449064dbfd329a2 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/light.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=a886f847000eedff782e93112b6b911c 2500w" />

### Light Adjustment Increment

* **Default Value**: 0.5
* **Function**: Controls the step size when adjusting light intensity in 3D scenes. Smaller step values allow for finer light adjustments, while larger values make each adjustment more noticeable

### Light Intensity Minimum

* **Default Value**: 1
* **Function**: Sets the minimum light intensity value allowed in 3D scenes. This defines the lowest brightness that can be set when adjusting the lighting of any 3D control

### Light Intensity Maximum

* **Default Value**: 10
* **Function**: Sets the maximum light intensity value allowed in 3D scenes. This defines the upper limit of brightness that can be set when adjusting the lighting of any 3D control

### Initial Light Intensity

* **Default Value**: 3
* **Function**: Sets the default brightness level of lights in 3D scenes. This value determines the intensity with which lights illuminate objects when creating new 3D controls, but each control can be adjusted individually after creation

## Scene

### Initial Background Color

* **Function**: Controls the default background color of 3D scenes. This setting determines the background appearance when creating new 3D components, but each component can be adjusted individually after creation
* **Default Value**: `282828` (dark gray)

Change the background color, which can also be adjusted in the canvas.
<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/background_color.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=f2f7d9069a14370dc135be10ba2ed70b" alt="Initial Background Color vs Modified" data-og-width="1769" width="1769" data-og-height="992" height="992" data-path="images/interface/setting/3d/background_color.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/background_color.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=79685947a99a7da7f37442ce805db576 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/background_color.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=bb738266527fcf47d8bf06b7cea24780 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/background_color.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=3500b1218e94bb09d4bee17b38b22927 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/background_color.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=1e513f697503e6a5130eefe592438d60 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/background_color.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=e0ae82b5821967144a23026a8166cdfc 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/background_color.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=8d24fb6e9c2670e7b08bfaebcbb7ff24 2500w" />

### Initial Preview Visibility

* **Function**: Controls whether the preview screen is displayed by default when creating new 3D components. This default setting can still be toggled individually for each component after creation
* **Default Value**: true (enabled)

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/hide_preview.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=863f91df39c2019e76cc8d42b561526c" alt="Preview vs hide preview" data-og-width="1769" width="1769" data-og-height="992" height="992" data-path="images/interface/setting/3d/hide_preview.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/hide_preview.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=b455fb65845f190d1244251c0942ac2a 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/hide_preview.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=5d1933ce7f67870f71e0886c7feba840 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/hide_preview.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=2af7cbbdfaab396e86c01ef71f218926 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/hide_preview.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=0e4dc2b4b5ca08c5a937c62582981c52 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/hide_preview.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=e26b8382bc9415d63f9116b641aac177 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/hide_preview.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=fc51dc45c3fe69286cfa0f6808bce774 2500w" />

### Initial Grid Visibility

* **Function**: Controls whether the grid is displayed by default when creating new 3D components. This default setting can still be toggled individually for each component after creation
* **Default Value**: true (enabled)

Hide or show the grid on initialization

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/hide_grid.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=7e21944da6f30f3a667adc56a644fc9e" alt="Show grid vs hide grid" data-og-width="1769" width="1769" data-og-height="992" height="992" data-path="images/interface/setting/3d/hide_grid.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/hide_grid.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=ec61a7365cac3e2413faa60264db1f7c 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/hide_grid.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=550d67ce701a064775c87b32e5eddd3a 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/hide_grid.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=8b5f99d2251c7a7228bd528e89f574bf 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/hide_grid.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=9bd97aee44d42cfc811dd70a7d5fb356 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/hide_grid.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=ec357f97d3129ee39fae90f4c952e915 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/3d/hide_grid.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=66c72c61588f446d1117b0e7263a1d89 2500w" />


# About Page
Source: https://docs.comfy.org/interface/settings/about

Detailed description of ComfyUI About settings page

The About page is an information display panel in the ComfyUI settings system, used to show application version information, related links, and system statistics. These settings can provide us with critical information when you submit feedback or report issues.

<img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/settings-about.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=6fdb66bb60546e4a894ea10c7d46340e" alt="about" data-og-width="2130" width="2130" data-og-height="1530" height="1530" data-path="images/interface/setting/settings-about.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/settings-about.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=a414ed9196a5440614e81b70f99d22af 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/settings-about.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=8000c5d757bad85921092c07ce294f8c 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/settings-about.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=7cf5d70ae1569afeff6a1bd7f663956d 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/settings-about.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=3e56877d6e2f9f978efbae9662acf9c4 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/settings-about.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=95497cee7748c0bcb8fc99d85ab4ade9 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/settings-about.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=fb20ec9f1d1477d715a8e7e464d49c66 2500w" />

### Version Information Badges

The About page displays the following core version information:

* **ComfyUI Version**: Shows the backend ComfyUI version number, linked to the official GitHub repository
* **ComfyUI\_frontend Version**: Shows the frontend interface version number, linked to the frontend GitHub repository
* **Discord Community**: Provides a link to the ComfyOrg Discord server
* **Official Website**: Links to the ComfyOrg official website

<Tip>
  Since the version information here mainly corresponds to stable version information, if you are using the nightly version, the corresponding commit hash will not be displayed here. If you are using the nightly version, you can use the `git log` command in the corresponding ComfyUI main directory to view the corresponding commit hash and other information.
  Another common issue is that different dependency packages may fail and rollback during updates.
</Tip>

### Custom Node Badges

If custom nodes are installed, the About page will also display additional badge information provided by custom nodes. These badges are registered by each custom node through the `aboutPageBadges` property.

### System Info

The bottom of the page displays detailed system statistics, including:

* Hardware configuration information
* Software environment information
* System performance data

## Extension Developer Guide

Extension developers can add custom badges to the About page by adding the `aboutPageBadges` property to their extension configuration:

```javascript  theme={null}
app.registerExtension({
  name: 'MyExtension',
  aboutPageBadges: [
    {
      label: 'My Extension v1.0.0',
      url: 'https://github.com/myuser/myextension',
      icon: 'pi pi-github'
    }
  ]
})
```


# Comfy Settings
Source: https://docs.comfy.org/interface/settings/comfy

Detailed description of ComfyUI core setting options

## API Nodes

### Show API node pricing badge

* **Default Value**: Enabled
* **Function**: Controls whether to display pricing badges on API nodes, helping users identify the usage cost of API nodes

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/api_node_pricing_badge.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=d930d2fdabb096a91d6df9bb8c2b26bb" alt="Show API node pricing badge" data-og-width="1315" width="1315" data-og-height="586" height="586" data-path="images/interface/setting/comfy/api_node_pricing_badge.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/api_node_pricing_badge.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=caf82538a992164f198ee9e1171880a6 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/api_node_pricing_badge.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=66f0004d36afc6de14554ef347a7bf96 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/api_node_pricing_badge.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=97aeca8b75dbcc3189a35e6a0a19c8c1 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/api_node_pricing_badge.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=24eb24aaea600e09dbae602ea68fc754 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/api_node_pricing_badge.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=b5be25bb48bdb7a18106f2bb15c4495b 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/api_node_pricing_badge.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=bfb1be1b459d6407cabb89726ac3d364 2500w" />

> For more information about API nodes, please refer to [API Nodes](/tutorials/partner-nodes/overview)

## Dev Mode

### Enable dev mode options (API save, etc.)

* **Default Value**: Disabled
* **Function**: Enables development mode options (such as API save, etc.)

## Edit Token Weight

### Ctrl+up/down precision

* **Default Value**: 0.01
* **Function**: When using CLIPTextEncode type nodes or text input node widgets, use Ctrl+up/down to quickly adjust weights. This option changes the weight value for each adjustment

<video controls>
  <source src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/token_weight.mp4?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=d65dd660f6b706e509bab75387c0b1bc" type="video/mp4" data-path="images/interface/setting/comfy/token_weight.mp4" />
</video>

## Locale

### Language

* **Options**: English,  (Chinese), (Japanese),  (Korean),  (Russian), Espaol (Spanish), Franais (French)
* **Default Value**: Auto-detect browser language
* **Function**: Modify the display language of ComfyUI interface

## Menu

### Use new menu

* **Default Value**: Top
* **Function**: Select menu interface and position, currently only supports Top, Bottom, Disabled

<Tabs>
  <Tab title="Top">
    The menu interface will be displayed at the top of the workspace
    <img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/UseNewMenu_top.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=b370775415cd976a1d4bb3dd1034789f" alt="Top Menu" data-og-width="2684" width="2684" data-og-height="1436" height="1436" data-path="images/interface/setting/comfy/UseNewMenu_top.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/UseNewMenu_top.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=6fd6ce3e0c40bba9ec8ba9570942a327 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/UseNewMenu_top.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=99928114888dff85b5dec7da1057b1da 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/UseNewMenu_top.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=1fee0f5f32ed40a69511cc5c6f0a865d 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/UseNewMenu_top.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=e9a27b09b1b350ddd6ccfdade905fa2f 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/UseNewMenu_top.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=70d830fa569a986ed57acfed71bf0fc5 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/UseNewMenu_top.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=3ff76378d84728bf5205b97849a988f3 2500w" />
  </Tab>

  <Tab title="Bottom">
    The menu bar interface will be displayed at the bottom of the workspace
    <img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/UseNewMenu_bottom.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=f32277b0016b128cf57783fc09b2ec18" alt="Bottom Menu" data-og-width="2700" width="2700" data-og-height="1436" height="1436" data-path="images/interface/setting/comfy/UseNewMenu_bottom.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/UseNewMenu_bottom.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=954dd5b530323e87bb5b1e9b8dfc9533 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/UseNewMenu_bottom.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=1855e382263451dddacf6cd03927898d 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/UseNewMenu_bottom.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=55eb334456f0f3c3d2453cdc191005dd 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/UseNewMenu_bottom.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=1353a11de61e5647c7827d115bc29e06 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/UseNewMenu_bottom.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=f28f0f28ed5f3cfd01683ea792f12d61 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/UseNewMenu_bottom.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=ee12a2609c0185b4b060d3d90d9261e8 2500w" />
  </Tab>

  <Tab title="Disabled">
    If you prefer the early legacy menu, you can try this option.

    <Tip>
      Since we are constantly updating, some new features will not be synchronized and supported in the legacy menu.
    </Tip>

        <img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/UseNewMenu_disable.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=516d891c3cda836135b996b7b5a23b69" alt="Legacy Menu" data-og-width="2680" width="2680" data-og-height="1444" height="1444" data-path="images/interface/setting/comfy/UseNewMenu_disable.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/UseNewMenu_disable.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=61902564f6f31658a361916b0db85ba4 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/UseNewMenu_disable.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=18ad89be18598c928fdccb8771a4d0fb 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/UseNewMenu_disable.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=65889bc9e1ee49f65e207835615d8435 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/UseNewMenu_disable.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=41b36e352a1ddfac0300da48f0788e23 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/UseNewMenu_disable.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=b398acef9061882d44398e699d1322a2 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/UseNewMenu_disable.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=214745ce7a8c3ec68c2a3664f37ff550 2500w" />
  </Tab>
</Tabs>

## Model Library

Model Library refers to the model management function in the ComfyUI sidebar menu. You can use this function to view models in your `ComfyUI/models` and additionally configured model folders.

<img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/sidepanel/model_library.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=f501edd3ce0fa0b80ba641efafdb5998" alt="Model Library" data-og-width="1050" width="1050" data-og-height="1401" height="1401" data-path="images/interface/sidepanel/model_library.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/sidepanel/model_library.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=e386317fb0156bf63e6d86334e41343e 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/sidepanel/model_library.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=9d6c12a5c7ab88813527ea0812d8c3b0 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/sidepanel/model_library.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=6f261d361591d3c3eb04660728341d94 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/sidepanel/model_library.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=b347a52b379cc293fc0e258053201e8a 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/sidepanel/model_library.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=6535b82f38b2b13dd1357cd124004f78 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/sidepanel/model_library.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=8129b8dbc464b8db13826862999e2539 2500w" />

### What name to display in the model library tree view

* **Default Value**: title
* **Function**: Select the name format to display in the model library tree view, currently only supports filename and title

### Automatically load all model folders

* **Default Value**: Disabled
* **Function**: Whether to automatically detect model files in all folders when clicking the model library. Enabling may cause loading delays (requires traversing all folders). When disabled, files in the corresponding folder will only be loaded when clicking the folder name.

## Node

During the iteration process of ComfyUI, we will adjust some nodes and enable some nodes. These nodes may undergo major changes or be removed in future versions. However, to ensure compatibility, deprecated nodes have not been removed. You can use the settings below to enable whether to display **experimental nodes** and **deprecated nodes**.

### Show deprecated nodes in search

* **Default Value**: Disabled
* **Function**: Controls whether to display deprecated nodes in search. Deprecated nodes are hidden by default in the UI, but remain effective in existing workflows.

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/depr_node.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=be33ca36eb10dc84a8f83aa8955738e4" alt="Show deprecated nodes in search" data-og-width="1582" width="1582" data-og-height="1056" height="1056" data-path="images/interface/setting/comfy/depr_node.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/depr_node.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=647fba7fbfd11d67e6180377a52f9347 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/depr_node.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=ad5aad5be0e93a53db24d6b3112c1e47 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/depr_node.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=c6db711a92c15d94c9f62bee6b1c88be 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/depr_node.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=607001989ace093fcb8ed0e2b9fb9953 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/depr_node.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=2fcc37c212a35e15e716f8023d6a7dc0 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/depr_node.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=a01a62fa452ecb05e2a537f6bd746345 2500w" />

### Show experimental nodes in search

* **Default Value**: Enabled
* **Function**: Controls whether to display experimental nodes in search. Experimental nodes are some new feature support, but are not fully stable and may change or be removed in future versions.

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/beta_node.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=df6bf1a7220de526ab8f115744dcfde4" alt="Show experimental nodes in search" data-og-width="1582" width="1582" data-og-height="1314" height="1314" data-path="images/interface/setting/comfy/beta_node.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/beta_node.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=fd51fa8c1077e9dc6b2a4325374734df 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/beta_node.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=723a9914e06d48f254b126c95ae31998 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/beta_node.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=b217ef096e1b9334d67d505a237adb27 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/beta_node.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=0cc1165e666573568ff69712b173b4f9 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/beta_node.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=9a29ab7726a39a149e20887996ce384f 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/beta_node.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=e25fd667f6dc9cb4c01d61733f4e20b7 2500w" />

## Node Search Box

### Number of nodes suggestions

* **Default Value**: 5
* **Function**: Used to modify the number of recommended nodes in the related node context menu. The larger the value, the more related recommended nodes are displayed.

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/node_suggestions.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=5dc8c7cc67b5ac49fd5a0d733cb73c04" alt="Number of nodes suggestions" data-og-width="2046" width="2046" data-og-height="924" height="924" data-path="images/interface/setting/comfy/node_suggestions.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/node_suggestions.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=24a75018cd61004e5e3c6b559c3d23a6 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/node_suggestions.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=41fdec448e9ca6130178f8d7edac576d 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/node_suggestions.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=df2384aec720c2bfbf63a153c19b2550 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/node_suggestions.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=395f3e52e6604278902840451c013f46 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/node_suggestions.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=6c135b082a8fca9b288955b0dc27e21a 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/node_suggestions.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=9b0605e1cbaf63457b8adc17c0912371 2500w" />

### Show node frequency in search results

* **Default Value**: Disabled
* **Function**: Controls whether to display node usage frequency in search results

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/node_frequency.png?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=fd5b96762d9d576791799c7c2752f6ab" alt="Show node frequency in search results" data-og-width="1364" width="1364" data-og-height="790" height="790" data-path="images/interface/setting/comfy/node_frequency.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/node_frequency.png?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=baa9c2732191d25f40117a217de5b55a 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/node_frequency.png?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=1b65b915ceeecc572b6c9a84015fc488 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/node_frequency.png?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=842240be7b9b31106627a6d293a79503 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/node_frequency.png?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=068367977e45c11e3121e7efc4875c78 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/node_frequency.png?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=84414f150ffdcd434dced56695ae7f07 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/node_frequency.png?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=38dc4b26a72ccfeb3d687c77cd532956 2500w" />

### Show node id name in search results

* **Default Value**: Disabled
* **Function**: Controls whether to display node ID names in search results

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/node_id_name.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=e199bad550b5b9cb99454e2722ce3bf0" alt="Show node id name in search results" data-og-width="1364" width="1364" data-og-height="790" height="790" data-path="images/interface/setting/comfy/node_id_name.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/node_id_name.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=9f48b20e9c22a74f8bc529c93e93494f 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/node_id_name.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=c07850bbe2f2630a0e683b9b40dcc07d 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/node_id_name.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=650e968b26a645362c648b63b3e01b9f 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/node_id_name.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=e3c6410de21486db2dc5465f054eef13 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/node_id_name.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=57a0b8d4fcd7f202369fed36376e60cf 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/comfy/node_id_name.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=fe89f25dc27bd90d821e109311f9c0a9 2500w" />

### Show node category in search results

* **Default Value**: Enabled
* **Function**: Controls whether to display node categories in search results, helping users understand node classification information

### Node preview

* **Default Value**: Enabled
* **Function**: Controls whether to display node previews in search results, making it convenient for you to quickly preview nodes

### Node search box implementation

* **Default Value**: default
* **Function**: Select the implementation method of the node search box (experimental feature). If you select `litegraph (legacy)`, it will switch to the early ComfyUI search box

## Node Widget

### Widget control mode

* **Options**: before, after
* **Function**: Controls whether the timing of node widget value updates is before or after workflow execution, such as updating seed values

### Textarea widget spellcheck

* **Default Value**: Disabled
* **Function**: Controls whether text area widgets enable spellcheck, providing spellcheck functionality during text input. This functionality is implemented through the browser's spellcheck attribute

## Queue

### Queue history size

* **Default Value**: 100
* **Function**: Controls the queue history size recorded in the sidebar queue history panel. The larger the value, the more queue history is recorded. When the number is large, loading the page will also consume more memory

## Queue Button

### Batch count limit

* **Default Value**: 100
* **Function**: Sets the maximum number of tasks added to the queue in a single click, preventing accidentally adding too many tasks to the queue

## Validation

### Validate node definitions (slow)

* **Default Value**: Disabled
* **Function**: Controls whether to validate all node definitions at startup (slow). Only recommended for node developers. When enabled, the system will use Zod schemas to strictly validate each node definition. This functionality will consume more memory and time
* **Error Handling**: Failed node definitions will be skipped and warning information will be output to the console

<Tip>
  Since detailed schema validation needs to be performed on all node definitions, this feature will significantly increase startup time, so it is disabled by default and only recommended for node developers
</Tip>

### Validate workflows

* **Default Value**: Enabled
* **Function**: Ensures the structural and connection correctness of workflows. If enabled, the system will call `useWorkflowValidation().validateWorkflow()` to validate workflow data
* **Validation Process**: The validation process includes two steps:
  * Schema validation: Use Zod schemas to validate workflow structure
  * Link repair: Check and repair connection issues between nodes
* **Error Handling**: When validation fails, error prompts will be displayed, but workflow loading will not be blocked

## Window

### Show confirmation when closing window

* **Default Value**: Enabled
* **Function**: When there are modified but unsaved workflows, controls whether to display confirmation when closing the browser window or tab, preventing accidental window closure that leads to loss of unsaved workflows

## Workflow

### Persist workflow state and restore on page (re)load

* **Default Value**: Enabled
* **Function**: Controls whether to restore workflow state on page (re)load, maintaining workflow content after page refresh

### Auto Save

* **Default Value**: off
* **Function**: Controls the auto-save behavior of workflows, automatically saving workflow changes to avoid data loss

### Auto Save Delay (ms)

* **Default Value**: 1000
* **Function**: Sets the delay time for auto-save, only effective when auto-save is set to "after delay"

### Show confirmation when deleting workflows

* **Default Value**: Enabled
* **Function**: Controls whether to display a confirmation dialog when deleting workflows in the sidebar, preventing accidental deletion of important workflows

### Save and restore canvas position and zoom level in workflows

* **Default Value**: Enabled
* **Function**: Controls whether to save and restore canvas position and zoom level in workflows, restoring the previous view state when reopening workflows

### Opened workflows position

* **Options**: Sidebar, Topbar, Topbar (Second Row)
* **Default Value**: Topbar
* **Function**: Controls the display position of opened workflow tabs, currently only supports Sidebar, Topbar, Topbar (Second Row)

### Prompt for filename when saving workflow

* **Default Value**: Enabled
* **Function**: Controls whether to prompt for filename input when saving workflows, allowing users to customize workflow filenames

### Sort node IDs when saving workflow

* **Default Value**: Disabled
* **Function**: Determines whether to sort node IDs when saving workflows, making workflow file format more standardized and convenient for version control

### Show missing nodes warning

* **Default Value**: Enabled
* **Function**: Controls whether to display warnings for missing nodes in workflows, helping users identify unavailable nodes in workflows

### Show missing models warning

* **Default Value**: Enabled
* **Function**: We support adding model link information to widget values in workflow files for prompts when loading model files. When enabled, if you don't have the corresponding model files locally, warnings for missing models in workflows will be displayed

### Require confirmation when clearing workflow

* **Default Value**: Enabled
* **Function**: Controls whether to display a confirmation dialog when clearing workflows, preventing accidental clearing of workflow content

### Save node IDs to workflow

* **Default Value**: Enabled
* **Function**: Controls whether to save node IDs when saving workflows, making workflow file format more standardized and convenient for version control


# Comfy Desktop General Settings
Source: https://docs.comfy.org/interface/settings/comfy-desktop

Detailed description of ComfyUI Desktop general setting options

## General

### Window Style

**Function**: Controls the title bar style of the application window

### Automatically check for updates

**Function**: Automatically checks for ComfyUI Desktop updates and will remind you to update when updates are available

### Send anonymous usage metrics

**Function**: Sends anonymous usage statistics to help improve the software. Changes to this setting require a restart to take effect

## UV

This section is mainly for users in China, because many of the original mirrors used by Desktop are outside of China, so access may not be friendly for domestic users. You can set your own mirror sources here to improve access speed and ensure that corresponding packages can be accessed and downloaded normally.

### Python Install Mirror

**Function**:

* Managed Python installation packages are downloaded from the Astral python-build-standalone project
* Can set mirror URL to use different Python installation sources
* The provided URL will replace the default GitHub download address
* Supports using file:// protocol to read distribution packages from local directories\
  **Validation**: Automatically checks mirror reachability

### Pypi Install Mirror

**Function**: Default pip package installation mirror source

### Torch Install Mirror

**Function**: PyTorch-specific pip installation mirror source


# Extension Settings
Source: https://docs.comfy.org/interface/settings/extension

Detailed description of ComfyUI extension management and setting options

The Extension settings panel is a special management panel in the ComfyUI frontend settings system, specifically used to manage the enable/disable status of frontend extension plugins. Unlike Custom Nodes, this panel is only used to manage frontend extensions registered by custom nodes, not to disable custom nodes themselves.

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/extension/extension.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=351dc6d25afbe287ed94c4bf39db91a1" alt="extension" data-og-width="1910" width="1910" data-og-height="1248" height="1248" data-path="images/interface/setting/extension/extension.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/extension/extension.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=a34f411c79daef19b975794c98e095fc 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/extension/extension.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=12e4f6f41641a0e9a7600c55a9364fde 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/extension/extension.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=0ee3665ed976865e6315f14b151282d5 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/extension/extension.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=98fb5076a9297011e4ab0993327a06da 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/extension/extension.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=9ef00ebcc6e38e996ffa577618bfcb82 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/extension/extension.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=c2e919422f54efaa23027110369dc014 2500w" />

These frontend extension plugins are used to enhance the ComfyUI experience, such as providing shortcuts, settings, UI components, menu items, and other features.

Extension status changes require a page reload to take effect:

## Extension Settings Panel Features

### 1. Extension List Management

Displays all registered extensions, including:

* Extension Name
* Core extension identification (displays "Core" label)
* Enable/disable status

### 2. Search Functionality

Provides a search box to quickly find specific extensions:

### 3. Enable/Disable Control

Each extension has an independent toggle switch:

### 4. Batch Operations

Provides right-click menu for batch operations:

* Enable All extensions
* Disable All extensions
* Disable 3rd Party extensions (keep core extensions)

## Notes

* Extension status changes require a page reload to take effect
* Some core extensions cannot be disabled
* The system will automatically disable known problematic extensions
* Extension settings are automatically saved to the user configuration file

This Extension settings panel is essentially a "frontend plugin manager" that allows users to flexibly control ComfyUI's functional modules.


# ComfyUI LiteGraph (Canvas) Settings
Source: https://docs.comfy.org/interface/settings/lite-graph

Detailed description of ComfyUI graphics rendering engine LiteGraph setting options

LiteGraph is the underlying graphics rendering engine of ComfyUI. The settings in this category mainly control the behavior and appearance of graphical interfaces such as canvas, nodes, and links.

## Canvas

### Show selection toolbox

* **Default Value**: Enabled
* **Function**: The selection toolbox is a floating quick action toolbar that appears on nodes after they are selected, providing common quick operations such as partial execution, pinning, deletion, color modification, etc.

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/selection-toolbox.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=58f8da6ea3f276272d9be4c4e1e5ebb1" alt="Show selection toolbox" data-og-width="1316" width="1316" data-og-height="756" height="756" data-path="images/interface/setting/lite-graph/selection-toolbox.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/selection-toolbox.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=0852d125213f53175878df6fa7ca6a0b 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/selection-toolbox.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=387feb2717783c88f242df53ba34585c 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/selection-toolbox.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=abdd92feb502d99b79e866826806a3c8 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/selection-toolbox.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=bafd39d46c38f51e73882f7a1edc2b6c 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/selection-toolbox.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=650035cfd4c313de8f498ae453099761 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/selection-toolbox.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=d9083ca10bbeaee4286ad04b2a99d72c 2500w" />

### Low quality rendering zoom threshold

* **Default Value**: 0.6
* **Range**: 0.1 - 1.0
* **Function**: When rendering the interface, especially when the workflow is particularly complex and the entire canvas is particularly large, the frontend rendering of corresponding elements will consume a lot of memory and cause lag. By lowering this threshold, you can control elements to enter low quality rendering mode when scaled to a specific percentage, thereby reducing memory consumption. The corresponding different rendering modes are shown below

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/render-mode.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=e6481f3430f77f8312d78b853d307774" alt="Low quality rendering" data-og-width="1536" width="1536" data-og-height="1008" height="1008" data-path="images/interface/setting/lite-graph/render-mode.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/render-mode.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=9ef378358eb4456f90b98919e1869573 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/render-mode.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=a1187ef33614dc82ab68badc0e511716 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/render-mode.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=2342fc9d728bbb28074f821d164491d1 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/render-mode.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=b9e7dd615fa32ad7f6b2237c9f7d9bcb 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/render-mode.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=cf24d96b6acdc6755988b308baddaece 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/render-mode.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=6d920a9762db0305aac10ff12f950c41 2500w" />

### Maximum FPS

* **Default Value**: 0 (use screen refresh rate)
* **Range**: 0 - 120
* **Function**: Limits the rendering frame rate of the canvas. 0 means using the screen refresh rate. Higher FPS will make the canvas rendering smoother, but will also consume more performance. Too low values will cause more obvious stuttering.

### Always snap to grid

* **Default Value**: Disabled
* **Function**: When this option is not enabled, you can hold the `Shift` key to align node edges with the grid. When enabled, node edges will automatically align with the grid without holding the `Shift` key.

<video controls>
  <source src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/lite-graph/snap-to-grid.mp4?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=2afb1c5c09f9fca9f4b8333e975d0f44" type="video/mp4" data-path="images/interface/setting/lite-graph/snap-to-grid.mp4" />
</video>

### Snap to grid size

* **Range**: 1 - 500
* **Function**: When auto-snap is enabled or when moving nodes while holding the `Shift` key, this parameter determines the grid size for snapping. The default value is 10, and you can adjust it according to your needs.

### Enable fast-zoom shortcut (Ctrl + Shift + Drag)

* **Default Value**: Enabled
* **Function**: Enables the `Ctrl + Shift + Left Mouse Button Drag` fast zoom function, providing a faster zoom operation method

<video controls>
  <source src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/fast-zoom.mp4?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=2d6e53041600bb97d28cb81786af40fd" type="video/mp4" data-path="images/interface/setting/lite-graph/fast-zoom.mp4" />
</video>

### Show graph canvas menu

* **Default Value**: Enabled
* **Function**: Controls whether to display the canvas menu in the bottom right corner

The canvas menu is located in the bottom right corner of the entire ComfyUI interface, containing operations such as canvas zooming, temporarily hiding all connections, quickly scaling the workflow to fit the canvas, etc., as shown in the image below

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/canvas_menu.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=23c8c2f94f2befbfc9cd3a484140af5b" alt="Show graph canvas menu" data-og-width="360" width="360" data-og-height="764" height="764" data-path="images/interface/setting/lite-graph/canvas_menu.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/canvas_menu.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=114872d393c2d9e608b74229fc222b56 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/canvas_menu.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=4993f24b3b9953267d53e1936538deab 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/canvas_menu.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=d2a8481ae2e09ad7f3e72afccb3cefc6 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/canvas_menu.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=f60dc6b8de22500f4caa0167871cf4c8 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/canvas_menu.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=c703da828402b98155cd2f344f90c13e 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/canvas_menu.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=675a0c9e832d4b93662fdada952eaef7 2500w" />

### Canvas zoom speed

* **Default Value**: 1.1
* **Range**: 1.01 - 2.5
* **Function**: Controls the speed of canvas zooming, adjusts the sensitivity of mouse wheel zooming

### Show canvas info on bottom left corner (fps, etc.)

* **Default Value**: Enabled
* **Function**: Controls whether to display canvas information in the bottom left corner, showing performance metrics like FPS

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/canvas-info.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=5ed9833f409780098accd32464b864fe" alt="Canvas info" data-og-width="732" width="732" data-og-height="498" height="498" data-path="images/interface/setting/lite-graph/canvas-info.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/canvas-info.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=f853330ca1bbe5ee49583263d7f3ec70 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/canvas-info.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=9e5a8716fab45917d1d49f34160483f5 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/canvas-info.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=87abec047a50012befe4464c210f8b29 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/canvas-info.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=c9c1cbcffd7e80c87a02daa46972877d 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/canvas-info.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=0301e23c24b1506c37c300f89aa0b5e8 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/canvas-info.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=bcf5bb37e1dfc1cac0c54bf5598532d7 2500w" />

## Context Menu

### Scale node combo widget menus (lists) when zoomed in

* **Default Value**: Enabled
* **Function**: Controls whether to scale node combo widget menus (lists) when zoomed in, allowing users to select node combo widgets

## Graph

### Link Render Mode

* **Default Value**: 2 (Spline)
* **Options**: Straight, Linear, Spline, Hidden
* **Function**: Sets the rendering style of connections, controlling the visual style of links between nodes

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/link-render-mode.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=b5ab460adf08e167a83fb640d5c5a4bb" alt="Link Render Mode" data-og-width="1324" width="1324" data-og-height="478" height="478" data-path="images/interface/setting/lite-graph/link-render-mode.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/link-render-mode.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=f5a03cfe4e686e6548a5c8bcd5b1619c 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/link-render-mode.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=0a55cf7ab6448ac3dc17ce34b7a924d9 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/link-render-mode.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=86f3068b94667a57be6517bc998326a3 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/link-render-mode.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=87a42a420f29fc92d948360430ad96d2 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/link-render-mode.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=88ec0edbab4d1a5140ed41e3203e0ccd 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/link-render-mode.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=dbc7b9d2957443ab3971b4e4f818d598 2500w" />

## Group

This section of settings is mainly related to node group functionality

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/node-group.png?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=02765d28e1720e5cc0bc2f6a1f70ab02" alt="Node Group" data-og-width="1487" width="1487" data-og-height="915" height="915" data-path="images/interface/setting/lite-graph/node-group.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/node-group.png?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=344ee0c96c34e0186efb34451ce7733c 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/node-group.png?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=dce5fe6f6e7d320667e375307c7ba1de 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/node-group.png?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=ccaaa08b04e1ae8d79f3b53b088019c7 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/node-group.png?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=b32a9107f488d73ed314a8de830092d3 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/node-group.png?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=6c06dff62e5aaac38fee0cb76371fb14 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/node-group.png?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=ad20abab050fa338c1af7ea59f7215c2 2500w" />

### Double click group title to edit

* **Default Value**: Enabled
* **Function**: Controls whether you can double-click the node title to edit it, allowing users to rename nodes, marked as part `1` in the image

### Group selected nodes padding

* **Default Value**: 10
* **Range**: 0 - 100
* **Function**: Sets the inner padding when grouping selected nodes, controlling the spacing between the group frame and nodes, marked as the arrow annotation part `2` in the image

## Link

### Link midpoint markers

* **Default Value**: Circle
* **Options**: None, Circle, Arrow
* **Function**: Sets the marker style at link midpoints, displaying direction indicators at link midpoints

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/link-midpoint.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=119b8c37616b50cda464333cbc87d2d6" alt="Link midpoint markers" data-og-width="1026" width="1026" data-og-height="568" height="568" data-path="images/interface/setting/lite-graph/link-midpoint.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/link-midpoint.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=4b97534b43210749730acbb2062aefa6 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/link-midpoint.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=4b7fbf041cf69479991fd9e027ec1e20 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/link-midpoint.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=80b9f765622c9c3b1b7942406f61849b 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/link-midpoint.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=e007a98ff4c61bde6521e18f79bffa40 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/link-midpoint.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=c3b50f60ca0929e895f4dc984c6eab87 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/link-midpoint.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=67ecd934c9b7b7c650e7714be2610134 2500w" />

## Link Release

This menu section currently mainly controls related operations when link connections are released. The current two related operations are:

**A node recommendation list related to the current input/output will appear after release**

<video controls>
  <source src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/link-release-contenxt-menu.mp4?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=1b8a078d5a122fa327f9ffefebc9e489" type="video/mp4" data-path="images/interface/setting/lite-graph/link-release-contenxt-menu.mp4" />
</video>

**A search box will be launched after release**

<video controls>
  <source src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/link-release-search-box.mp4?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=52747dcfcf1d0c31b2163961c62e9efb" type="video/mp4" data-path="images/interface/setting/lite-graph/link-release-search-box.mp4" />
</video>

### Action on link release (Shift)

* **Default Value**: search box
* **Options**: context menu, search box, no action
* **Function**: Sets the action when releasing links while holding the Shift key, special behavior when releasing links while holding Shift

### Action on link release (No modifier)

* **Default Value**: context menu
* **Options**: context menu, search box, no action
* **Function**: Sets the default action when releasing links, controls the behavior after dragging and releasing links

## Node

### Always shrink new nodes

* **Default Value**: Enabled
* **Function**: Controls whether to automatically shrink when creating new nodes, so nodes can always display the minimum size, but may cause some text display to be truncated when adding, requiring manual adjustment of node size

### Enable DOM element clipping (enabling may reduce performance)

* **Default Value**: Enabled
* **Function**: Enables DOM element clipping (may affect performance), optimizes rendering but may reduce performance

### Middle-click creates a new Reroute node

* **Default Value**: Enabled
* **Function**: Creates a new reroute node when middle-clicking, quickly creates reroute nodes for organizing connections

### Keep all links when deleting nodes

* **Default Value**: Enabled
* **Function**: Automatically bypasses connections when deleting intermediate nodes, attempts to reconnect input and output links when deleting nodes

### Snap highlights node

* **Default Value**: Enabled
* **Function**: Highlights nodes when dragging links to them, provides visual feedback, shows connectable nodes. When enabled, the effect is as shown in the image below, the corresponding side of the link will display highlighted style

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/highlights-node.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=242f3b88d95229d99744f720cf86c935" alt="Snap highlights node" data-og-width="1600" width="1600" data-og-height="1117" height="1117" data-path="images/interface/setting/lite-graph/highlights-node.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/highlights-node.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=7f09e4ab23066f929261f914435ca863 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/highlights-node.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=ee512a2dc0f93c929bc63bc4044c68a9 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/highlights-node.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=97489e53e0bb76fdfca88ae9ea06c25a 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/highlights-node.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=b0f1a2bb9487a2c4f04c22d89c31290b 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/highlights-node.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=24cd0f898c96f82d8c83a8a7b9dcbc3d 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/highlights-node.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=4645004d83ccd6319f10d340b4e359b2 2500w" />

### Auto snap link to node slot

* **Default Value**: Enabled
* **Function**: Automatically snaps to available slots when dragging links to nodes, simplifies connection operations, automatically finds suitable input slots

<video controls>
  <source src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/snap-link-to-node-slot.mp4?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=1f517cf34f3f994c4c03a267d8fbbd91" type="video/mp4" data-path="images/interface/setting/lite-graph/snap-link-to-node-slot.mp4" />
</video>

### Enable Tooltips

* **Default Value**: Enabled
* **Function**: Some node information will contain tooltips, including parameter descriptions, etc. When enabled, these tooltips will be displayed when hovering the mouse, as shown in the image below

<img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/lite-graph/tooltips.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=df329458465d6f20efea955e638c9656" alt="Enable Tooltips" data-og-width="970" width="970" data-og-height="812" height="812" data-path="images/interface/setting/lite-graph/tooltips.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/lite-graph/tooltips.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=0b1dcdb41ced6b2a6e8e5149979b94e2 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/lite-graph/tooltips.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=9ae884ced3a5ba1e8f2e8ae57f029c48 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/lite-graph/tooltips.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=a02a16c504d702daaae15e6a1ae8db66 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/lite-graph/tooltips.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=bdc7306c00ae81da9436b75f16eff554 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/lite-graph/tooltips.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=d4f0ff45b6e5d07c05ded7e175bb5f6d 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/lite-graph/tooltips.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=033d80a751cdb677cfeee70d20696a40 2500w" />

### Tooltip Delay

* **Default Value**: 500
* **Function**: Controls the delay time for tooltips, in milliseconds. Setting to 0 means displaying tooltips immediately

### Node life cycle badge mode

* **Default Value**: Show all
* **Function**: Controls the display of node lifecycle markers, showing node status information

### Node ID badge mode

* **Default Value**: Show all
* **Function**: Controls the display of node ID markers, showing node unique identifiers

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/node-id-badge.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=ca8e1edfff6c7673f1432ba0ac2edde5" alt="Node ID badge mode" data-og-width="1934" width="1934" data-og-height="882" height="882" data-path="images/interface/setting/lite-graph/node-id-badge.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/node-id-badge.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=b4a7cb9bb4f3f93c8d809fe95edd1016 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/node-id-badge.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=2b0e969d8ba95913da96a89dbf053594 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/node-id-badge.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=b2bd300153771746ace189c8ab222f4d 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/node-id-badge.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=6b4ea2ddf5aaaf71c9cc67f080c941f0 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/node-id-badge.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=70260117330068ae383785ad5c2eb487 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/node-id-badge.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=3c979e6c55b7d0e905d803340b6c7d8c 2500w" />

### Node source badge mode

* **Options**:
  * None
  * Hide built-in
  * Show all
* **Function**: Controls the display mode of node source markers, showing node source information. The corresponding display effect is shown in the image below. If show all is selected, it will display labels for both custom nodes and built-in nodes, making it convenient for you to determine the corresponding node source. The corresponding fox logo represents ComfyUI built-in nodes

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/node-source-badge.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=383afc337b63f52be2f8d3cb9e7c4ed7" alt="Node source badge mode" data-og-width="1934" width="1934" data-og-height="1444" height="1444" data-path="images/interface/setting/lite-graph/node-source-badge.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/node-source-badge.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=2adbbf69732c7b73dcab2a8e01d0b402 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/node-source-badge.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=4baf2f78dfcb8b23db6804197dd009ec 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/node-source-badge.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=885a023913bd9b48ce59dbfbeaf403e1 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/node-source-badge.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=74d8da68c50aee918213f41748cb774e 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/node-source-badge.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=a5df61943a4fdb6a0f87980733371f5e 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/node-source-badge.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=033de7b26a0cb9d615c626b996350f0d 2500w" />

### Double click node title to edit

* **Default Value**: Enabled
* **Function**: Controls whether you can double-click the node title to edit it, allowing users to rename nodes

## Node Widget

### Float widget rounding decimal places \[0 = auto]

* **Default Value**: 0 (auto)
* **Range**: 0 - 6
* **Function**: Sets the decimal places for float widget rounding, 0 means auto, requires page reload

### Disable default float widget rounding

* **Default Value**: Disabled
* **Function**: Controls whether to disable default float widget rounding, requires page reload, cannot be disabled when the node backend has set rounding

### Disable node widget sliders

* **Default Value**: Disabled
* **Function**: Controls whether to disable slider controls in node widgets, forcing text input instead of sliders

### Preview image format

* **Default Value**: Empty string (use original format)
* **Function**: Sets the format for preview images in image widgets, converts to lightweight formats like webp, jpeg, etc.

### Show width  height below the image preview

* **Default Value**: Enabled
* **Function**: Displays width  height information below image previews, showing image dimension information

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/show-size.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=e7fdfd5227158a413b5b0051a6e171bf" alt="Show width  height below the image preview" data-og-width="1344" width="1344" data-og-height="904" height="904" data-path="images/interface/setting/lite-graph/show-size.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/show-size.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=11ef68d7975fd3e639e78bc2a184607c 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/show-size.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=039c262051b8d4340e7cb0282ad933ab 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/show-size.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=2c534553d00ee7944410ff839c3de645 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/show-size.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=74e330c841673a679ea01107154bdaf4 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/show-size.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=d42ab780f2ba87dcde3005ac34e765cb 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/show-size.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=e6641012043335c3254d14e223abc264 2500w" />

## Pointer

### Enable trackpad gestures

* **Default Value**: Enabled
* **Function**: This setting enables trackpad mode for the canvas, allowing two-finger pinch zoom and drag.

### Double click interval (maximum)

* **Default Value**: 300
* **Function**: Maximum time (milliseconds) between two clicks of a double-click. Increasing this value helps solve issues where double-clicks are sometimes not recognized.

### Pointer click drift delay

* **Default Value**: 150
* **Function**: Maximum time (milliseconds) to ignore pointer movement after pressing the pointer button. Helps prevent accidental mouse movement when clicking.

### Pointer click drift (maximum distance)

* **Default Value**: 6
* **Function**: If the pointer moves more than this distance while holding the button, it is considered a drag (rather than a click). Helps prevent accidental mouse movement when clicking.

## Reroute

### Reroute spline offset

* **Default Value**: 20
* **Function**: Used to determine the smoothness of curves on both sides of reroute nodes. Larger values make curves smoother, smaller values make curves sharper.

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/reroute-spline-offset.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=a76bc935bc5e37fe1b3c99129c7b5fe3" alt="Reroute spline offset" data-og-width="1568" width="1568" data-og-height="752" height="752" data-path="images/interface/setting/lite-graph/reroute-spline-offset.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/reroute-spline-offset.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=2554871390417e2446d7a54f068457ab 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/reroute-spline-offset.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=8fc90b4d5a202590e89d415db9476d64 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/reroute-spline-offset.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=b62484d0c4a42903c87d0c2017c48c6c 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/reroute-spline-offset.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=85b04e2c73fbe5b5c142a4194bdd4f09 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/reroute-spline-offset.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=5a9b9ae976c0df2ab1aa52b57cc377db 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/lite-graph/reroute-spline-offset.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=b10e3d24d3da03186411aa0d543c5e14 2500w" />


# ComfyUI Mask Editor Settings
Source: https://docs.comfy.org/interface/settings/mask-editor

Detailed description of ComfyUI mask editor setting options

## Brush Adjustment

### Brush adjustment speed multiplier

* **Function**: Controls the speed of brush size and hardness changes during adjustment
* **Description**: Higher values mean faster changes

### Lock brush adjustment to dominant axis

* **Function**: When enabled, brush adjustment will only affect size or hardness based on the direction you move
* **Description**: This feature allows users to more precisely control brush property adjustments

## New Editor

### Use new mask editor

* **Function**: Switch to the new brush editor interface
* **Description**: Allows users to switch between new and old editor interfaces

<Tabs>
  <Tab title="New Editor">
    The new version has a better UI interface and interaction, with more complete functionality
    <img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/maskeditor/new-mask-editor.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=025fc8216d7525b65bc4efd2390aa2f0" alt="new" data-og-width="2378" width="2378" data-og-height="1868" height="1868" data-path="images/interface/setting/maskeditor/new-mask-editor.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/maskeditor/new-mask-editor.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=05109997d353aab6d4798921bac2e97f 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/maskeditor/new-mask-editor.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=9d4e6da29acc0d79b7abe5ea2343cebb 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/maskeditor/new-mask-editor.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=47e1b7a618fe6bfb68af900d8465faf1 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/maskeditor/new-mask-editor.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=e352b0ee07f8a571534e8d3113ae4aee 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/maskeditor/new-mask-editor.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=e77dc8abefde893534b4fbf0e16f560f 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/maskeditor/new-mask-editor.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=85e19086a4aa33b113176a2da677246d 2500w" />
  </Tab>

  <Tab title="Old Editor">
    The old version is an early version with relatively simple functionality, but can meet basic needs. It will not be updated in the future.
    <img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/maskeditor/old-mask-editor.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=aaea01cf1eea27160add3e65affc15dd" alt="old" data-og-width="2120" width="2120" data-og-height="1692" height="1692" data-path="images/interface/setting/maskeditor/old-mask-editor.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/maskeditor/old-mask-editor.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=9f75403871c21bd8540d47106832cb57 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/maskeditor/old-mask-editor.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=ca7db31cbc0279d959c89c4c8f402ca3 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/maskeditor/old-mask-editor.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=0339e3e7a8ae13c79b4ef50cfd02351b 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/maskeditor/old-mask-editor.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=bdd4fc3e5a5e2b885e6abc98826dc578 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/maskeditor/old-mask-editor.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=cee841a30ca315d69e8fb1634779c619 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/maskeditor/old-mask-editor.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=8f2bd2d659cf735fcb84e579fc9050ec 2500w" />
  </Tab>
</Tabs>


# ComfyUI Settings Overview
Source: https://docs.comfy.org/interface/settings/overview

Detailed description of ComfyUI settings overview

This section covers detailed setting descriptions in the ComfyUI frontend settings menu. All user settings are automatically saved to the `ComfyUI/user/default/comfy.settings.json` file.

You can use the `Ctrl + ,` keyboard shortcut to open the settings panel, then click on the corresponding setting options to configure them.

Since custom nodes can also register corresponding setting categories in the menu, our official documentation currently only includes native setting content. Additionally, some setting options are **only effective for ComfyUI Desktop**, which we have noted on the corresponding pages.

## ComfyUI Settings Menu

<Columns cols={3}>
  <Card title="User" icon="user" href="/interface/user">
    User settings related to ComfyUI account, mainly used for logging into ComfyUI account to use API nodes
  </Card>

  <Card title="Credits" icon="credit-card" href="/interface/credits">
    Entry for purchasing credits and credit balance history, only visible after logging into ComfyUI account
  </Card>

  <Card title="Comfy" icon="sliders" href="/interface/settings/comfy">
    Detailed description of ComfyUI core setting options
  </Card>

  <Card title="Lite Graph" icon="diagram-project" href="/interface/settings/lite-graph">
    Detailed description of Canvas (Lite Graph) setting options in ComfyUI
  </Card>

  <Card title="Appearance" icon="palette" href="/interface/appearance">
    Modify ComfyUI appearance options such as themes, background colors, sidebar position, etc.
  </Card>

  <Card title="Extension" icon="puzzle-piece" href="/interface/settings/extension">
    Manage the enable/disable status of frontend extension plugins in ComfyUI
  </Card>

  <Card title="3D" icon="cube" href="/interface/settings/3d">
    Some setting options for 3D node initialization
  </Card>

  <Card title="Comfy Desktop" icon="desktop" href="/interface/settings/comfy-desktop">
    Desktop update settings, mirror settings, etc. (only effective for ComfyUI Desktop)
  </Card>

  <Card title="Mask Editor" icon="brush" href="/interface/settings/mask-editor">
    Adjust mask editor usage preferences
  </Card>

  <Card title="Keybinding" icon="keyboard" href="/interface/shortcuts">
    Modify ComfyUI keyboard shortcut settings
  </Card>

  <Card title="About" icon="info" href="/interface/settings/about">
    Learn about current ComfyUI version information, device runtime information, etc., which is very useful for daily feedback
  </Card>

  <Card title="Server Config" icon="server" href="/interface/settings/server-config">
    Modify ComfyUI configuration file, this setting is only effective for ComfyUI Desktop
  </Card>
</Columns>


# Server Config
Source: https://docs.comfy.org/interface/settings/server-config

Detailed description of ComfyUI server configuration options

<Note>
  Currently the `Server Config` settings menu only exists in the Desktop version, and this settings menu item does not exist in other versions
</Note>

## Network

### Host: The IP address to listen on

* **Function**: Sets the IP address the server binds to. Default `127.0.0.1` means only local access is allowed. If you need LAN access, you can set it to `0.0.0.0`

<Tip>
  Although we provide LAN listening settings for the Desktop version, as a desktop application, it is not suitable for use as a server. We recommend that if you need to use ComfyUI as a public service within the LAN, please refer to the manual deployment tutorial to deploy the corresponding ComfyUI service.
</Tip>

### Port: The port to listen on

**Function**: The port number the server listens on. Desktop version defaults to port 8000, Web version typically uses port 8188

### TLS Key File: Path to TLS key file for HTTPS

**Function**: The private key file path required for HTTPS encryption, used to establish secure connections

### TLS Certificate File: Path to TLS certificate file for HTTPS

**Function**: The certificate file path required for HTTPS encryption, used in conjunction with the private key

### Enable CORS header: Use "\*" for all origins or specify domain

**Function**: Cross-Origin Resource Sharing settings, allowing web browsers to access the server from different domains

### Maximum upload size (MB)

**Function**: Limits the maximum size of single file uploads, in MB, default 100MB. Affects upload limits for images, models and other files

## CUDA

### CUDA device index to use

**Function**: Specifies which NVIDIA graphics card to use. 0 represents the first graphics card, 1 represents the second, and so on. Important for multi-GPU systems

### Use CUDA malloc for memory allocation

**Function**: Controls whether to use CUDA's memory allocator. Can improve memory management efficiency in certain situations

## Inference

### Global floating point precision

**Function**: Sets the numerical precision for model calculations. FP16 saves VRAM but may affect quality, FP32 is more precise but uses more VRAM

### UNET precision

**Options**:

* `auto`: Automatically selects the most suitable precision
* `fp64`: 64-bit floating point precision, highest precision but largest VRAM usage
* `fp32`: 32-bit floating point precision, standard precision
* `fp16`: 16-bit floating point precision, can save VRAM
* `bf16`: 16-bit brain floating point precision, between fp16 and fp32
* `fp8_e4m3fn`: 8-bit floating point precision (e4m3), minimal VRAM usage
* `fp8_e5m2`: 8-bit floating point precision (e5m2), minimal VRAM usage

**Function**: Specifically controls the computational precision of the UNET core component of diffusion models. Higher precision can provide better image generation quality but uses more VRAM. Lower precision can significantly save VRAM but may affect the quality of generated results.

### VAE precision

**Options and Recommendations**:

* `auto`: Automatically selects the most suitable precision, recommended for users with 8-12GB VRAM
* `fp16`: 16-bit floating point precision, recommended for users with 6GB or less VRAM, can save VRAM but may affect quality
* `fp32`: 32-bit floating point precision, recommended for users with 16GB or more VRAM who pursue the best quality
* `bf16`: 16-bit brain floating point precision, recommended for newer graphics cards that support this format, can achieve better performance balance

**Function**: Controls the computational precision of the Variational Autoencoder (VAE), affecting the quality and speed of image encoding/decoding. Higher precision can provide better image reconstruction quality but uses more VRAM. Lower precision can save VRAM but may affect image detail restoration.

### Run VAE on CPU

**Function**: Forces VAE to run on CPU, can save VRAM but will reduce processing speed

### Text Encoder precision

**Options**:

* `auto`: Automatically selects the most suitable precision
* `fp8_e4m3fn`: 8-bit floating point precision (e4m3), minimal VRAM usage
* `fp8_e5m2`: 8-bit floating point precision (e5m2), minimal VRAM usage
* `fp16`: 16-bit floating point precision, can save VRAM
* `fp32`: 32-bit floating point precision, standard precision

**Function**: Controls the computational precision of the text prompt encoder, affecting the accuracy of text understanding and VRAM usage. Higher precision can provide more accurate text understanding but uses more VRAM. Lower precision can save VRAM but may affect prompt parsing effectiveness.

## Memory

### Force channels-last memory format

**Function**: Changes the data arrangement in memory, may improve performance on certain hardware

### DirectML device index

**Function**: Specifies the device when using DirectML acceleration on Windows, mainly for AMD graphics cards

### Disable IPEX optimization

**Function**: Disables Intel CPU optimization, mainly affects Intel processor performance

### VRAM management mode

**Options**:

* `auto`: Automatically manages VRAM, allocating VRAM based on model size and requirements
* `lowvram`: Low VRAM mode, uses minimal VRAM, may affect generation quality
* `normalvram`: Standard VRAM mode, balances VRAM usage and performance
* `highvram`: High VRAM mode, uses more VRAM for better performance
* `novram`: No VRAM usage, runs entirely on system memory
* `cpu`: CPU-only mode, doesn't use graphics card

**Function**: Controls VRAM usage strategy, such as automatic management, low VRAM mode, etc.

### Reserved VRAM (GB)

**Function**: Amount of VRAM reserved for the operating system and other programs, prevents system freezing

### Disable smart memory management

**Function**: Disables automatic memory optimization, forces models to move to system memory to free VRAM

## Preview

### Method used for latent previews

**Options**:

* `none`: No preview images displayed, only shows progress bar during generation
* `auto`: Automatically selects the most suitable preview method, dynamically adjusts based on system performance and VRAM
* `latent2rgb`: Directly converts latent space data to RGB images for preview, faster but average quality
* `taesd`: Uses lightweight TAESD model for preview, balances speed and quality

**Function**: Controls how to preview intermediate results during generation. Different preview methods affect preview quality and performance consumption. Choosing the right preview method can find a balance between preview effects and system resource usage.

### Size of preview images

**Function**: Sets the resolution of preview images, affects preview clarity and performance. Larger sizes provide higher preview quality but also consume more VRAM

## Cache

### Use classic cache system

**Function**: Uses traditional caching strategy, more conservative but stable

### Use LRU caching with a maximum of N node results cached

**Function**: Uses Least Recently Used (LRU) algorithm caching system, can cache a specified number of node computation results

**Description**:

* Set a specific number to control maximum cache count, such as 10, 50, 100, etc.
* Caching can avoid repeated computation of the same node operations, improving workflow execution speed
* When cache reaches the limit, automatically clears the least recently used results
* Cached results occupy system memory (RAM/VRAM), larger values use more memory

**Usage Recommendations**:

* Default value is null, meaning LRU caching is not enabled
* Set appropriate cache count based on system memory capacity and usage requirements
* Recommended for workflows that frequently reuse the same node configurations
* If system memory is sufficient, larger values can be set for better performance improvement

## Attention

### Cross attention method

**Options**:

* `auto`: Automatically selects the most suitable attention computation method
* `split`: Block-wise attention computation, can save VRAM but slower speed
* `quad`: Uses quad attention algorithm, balances speed and VRAM usage
* `pytorch`: Uses PyTorch native attention computation, faster but higher VRAM usage

**Function**: Controls the specific algorithm used when the model computes attention. Different algorithms make different trade-offs between generation quality, speed, and VRAM usage. Usually recommended to use auto for automatic selection.

### Force attention upcast

**Function**: Forces high-precision attention computation, improves quality but increases VRAM usage

### Prevent attention upcast

**Function**: Disables high-precision attention computation, saves VRAM

## General

### Disable xFormers optimization

**Function**: Disables the optimization features of the xFormers library. xFormers is a library specifically designed to optimize the attention mechanisms of Transformer models, typically improving computational efficiency, reducing memory usage, and accelerating inference speed. Disabling this optimization will:

* Fall back to standard attention computation methods
* May increase memory usage and computation time
* Provide a more stable runtime environment in certain situations

**Use Cases**:

* When encountering compatibility issues related to xFormers
* When more precise computation results are needed (some optimizations may affect numerical precision)
* When debugging or troubleshooting requires using standard implementations

### Default hashing function for model files

**Options**:

* `sha256`: Uses SHA-256 algorithm for hash verification, high security but slower computation
* `sha1`: Uses SHA-1 algorithm, faster but slightly lower security
* `sha512`: Uses SHA-512 algorithm, provides highest security but slowest computation
* `md5`: Uses MD5 algorithm, fastest but lowest security

**Function**: Sets the hash algorithm for model file verification, used to verify file integrity. Different hash algorithms have different trade-offs between computation speed and security. Usually recommended to use sha256 as the default option, which achieves a good balance between security and performance.

### Make pytorch use slower deterministic algorithms when it can

**Function**: Forces PyTorch to use deterministic algorithms when possible to improve result reproducibility.

**Description**:

* When enabled, PyTorch will prioritize deterministic algorithms over faster non-deterministic algorithms
* Same inputs will produce same outputs, helpful for debugging and result verification
* Deterministic algorithms typically run slower than non-deterministic algorithms
* Even with this setting enabled, completely identical image results cannot be guaranteed in all situations

**Use Cases**:

* Scientific research requiring strict result reproducibility
* Debugging processes requiring stable output results
* Production environments requiring result consistency

### Enable some untested and potentially quality deteriorating optimizations

**Function**: Enables experimental optimizations that may improve speed but could potentially affect generation quality

### Don't print server output to console

**Function**: Prevents displaying server runtime information in the console, keeping the interface clean.

**Description**:

* When enabled, ComfyUI server logs and runtime information will not be displayed
* Can reduce console information interference, making the interface cleaner
* May slightly improve system performance when there's heavy log output
* Default is disabled (false), meaning server output is displayed by default

**Use Cases**:

* Production environments where debugging information is not needed
* When wanting to keep the console interface clean
* When the system runs stably and log monitoring is not required

**Note**: It's recommended to keep this option disabled during development and debugging to promptly view server runtime status and error information.

### Disable saving prompt metadata in files

**Function**: Does not save workflow information in generated images, reducing file size, but also means the loss of corresponding workflow information, preventing you from using workflow output files to reproduce the corresponding generation results

### Disable loading all custom nodes

**Function**: Prevents loading all third-party extension nodes, typically used when troubleshooting issues to locate whether errors are caused by third-party extension nodes

### Logging verbosity level

**Function**: Controls the verbosity level of log output, used for debugging and monitoring system runtime status.

**Options**:

* `CRITICAL`: Only outputs critical error information that may cause the program to stop running
* `ERROR`: Outputs error information indicating some functions cannot work properly
* `WARNING`: Outputs warning information indicating possible issues that don't affect main functionality
* `INFO`: Outputs general information including system runtime status and important operation records
* `DEBUG`: Outputs the most detailed debugging information including system internal runtime details

**Description**:

* Log levels increase in verbosity from top to bottom
* Each level includes all log information from higher levels
* Recommended to set to INFO level for normal use
* Can be set to DEBUG level when troubleshooting for more information
* Can be set to WARNING or ERROR level in production environments to reduce log volume

## Directories

### Input directory

**Function**: Sets the default storage path for input files (such as images, models)

### Output directory

**Function**: Sets the save path for generation results


# ComfyUI Keyboard Shortcuts and Custom Settings
Source: https://docs.comfy.org/interface/shortcuts

Keyboard and mouse shortcuts for ComfyUI and related settings

{/* TODO(yoland): Add this back to comfyUI readme page */}

Currently, ComfyUI supports custom keyboard shortcuts. You can set the shortcuts by clicking on `Settings (gear icon)` --> `Keybinding`.

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/keybinding.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=ec261f919615b16c4b111e6370926543" alt="ComfyUI Shortcut Settings" data-og-width="1692" width="1692" data-og-height="1368" height="1368" data-path="images/interface/setting/keybinding.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/keybinding.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=d48c51c47e759dae0d9c5ea611590165 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/keybinding.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=622388e6fa819f8570231cb1a90f2055 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/keybinding.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=dcd5b6fc07a781a29421136ea1dca316 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/keybinding.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=7b07e58fd0e676b1fe95dee3d2337138 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/keybinding.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=50e401f316a5d73cbcf83c69ef04d452 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/setting/keybinding.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=14a051c626877fa3d7b5059c3b94f711 2500w" />

In the corresponding menu, you can see all the current shortcut settings for ComfyUI. Click the `edit icon` before the corresponding command to customize the shortcut.

Below is the current list of shortcuts for ComfyUI, which you can customize as needed.

<Tabs>
  <Tab title="Windows/Linux">
    | Shortcut                        | Command                                                                                                            |
    | ------------------------------- | ------------------------------------------------------------------------------------------------------------------ |
    | Ctrl + Enter                    | Queue prompt                                                                                                       |
    | Ctrl + Shift + Enter            | Queue prompt (Front)                                                                                               |
    | Ctrl + Alt + Enter              | Interrupt                                                                                                          |
    | Ctrl + Z / Ctrl + Y             | Undo/Redo                                                                                                          |
    | Ctrl + S                        | Save workflow                                                                                                      |
    | Ctrl + O                        | Load workflow                                                                                                      |
    | Ctrl + A                        | Select all nodes                                                                                                   |
    | Alt + C                         | Collapse/uncollapse selected nodes                                                                                 |
    | Ctrl + M                        | Mute/unmute selected nodes                                                                                         |
    | Ctrl + B                        | Bypass/unbypass selected nodes                                                                                     |
    | Delete<br />Backspace           | Delete selected nodes                                                                                              |
    | Backspace                       | Clear workflow                                                                                                     |
    | Space                           | Move canvas when holding and moving cursor                                                                         |
    | Ctrl + Click<br />Shift + Click | Add clicked node to selection                                                                                      |
    | Ctrl + C/Ctrl + V               | Copy and paste selected nodes (without maintaining connections to outputs of unselected nodes)                     |
    | Ctrl + C/Ctrl + Shift + V       | Copy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes) |
    | Shift + Drag                    | Move multiple selected nodes at the same time                                                                      |
    | Ctrl + G                        | Add frame to selected nodes                                                                                        |
    | Ctrl + ,                        | Show settings dialog                                                                                               |
    | Alt + =                         | Zoom in (canvas)                                                                                                   |
    | Alt + -                         | Zoom out (canvas)                                                                                                  |
    | .                               | Fit view to selected nodes                                                                                         |
    | P                               | Pin/unpin selected items                                                                                           |
    | Q                               | Toggle queue sidebar                                                                                               |
    | W                               | Toggle workflow sidebar                                                                                            |
    | N                               | Toggle node library sidebar                                                                                        |
    | M                               | Toggle model library sidebar                                                                                       |
    | Ctrl + \`                       | Toggle log bottom panel                                                                                            |
    | F                               | Toggle focus mode (full screen)                                                                                    |
    | R                               | Refresh node definitions                                                                                           |
    | Double-Click LMB                | Quick search for nodes to add                                                                                      |
  </Tab>

  <Tab title="MacOS">
    | Shortcut                         | Command                                                                                                            |
    | -------------------------------- | ------------------------------------------------------------------------------------------------------------------ |
    | Cmd  + Enter                    | Queue prompt                                                                                                       |
    | Cmd  + Shift + Enter            | Queue prompt (Front)                                                                                               |
    | Cmd  + Alt + Enter              | Interrupt                                                                                                          |
    | Cmd  + Z/Cmd  + Y              | Undo/Redo                                                                                                          |
    | Cmd  + S                        | Save workflow                                                                                                      |
    | Cmd  + O                        | Load workflow                                                                                                      |
    | Cmd  + A                        | Select all nodes                                                                                                   |
    | Opt  + C                        | Collapse/uncollapse selected nodes                                                                                 |
    | Cmd  + M                        | Mute/unmute selected nodes                                                                                         |
    | Cmd  + B                        | Bypass/unbypass selected nodes                                                                                     |
    | Delete<br />Backspace            | Delete selected nodes                                                                                              |
    | Backspace                        | Clear workflow                                                                                                     |
    | Space                            | Move canvas when holding and moving cursor                                                                         |
    | Cmd  + Click<br />Shift + Click | Add clicked node to selection                                                                                      |
    | Cmd  + C / Cmd  + V            | Copy and paste selected nodes (without maintaining connections to outputs of unselected nodes)                     |
    | Cmd  + C / Cmd  + Shift + V    | Copy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes) |
    | Shift + Drag                     | Move multiple selected nodes at the same time                                                                      |
    | Cmd  + G                        | Add frame to selected nodes                                                                                        |
    | Cmd  + ,                        | Show settings dialog                                                                                               |
    | Opt  + =                        | Zoom in (canvas)                                                                                                   |
    | Opt  + -                        | Zoom out (canvas)                                                                                                  |
    | .                                | Fit view to selected nodes                                                                                         |
    | P                                | Pin/unpin selected items                                                                                           |
    | Q                                | Toggle queue sidebar                                                                                               |
    | W                                | Toggle workflow sidebar                                                                                            |
    | N                                | Toggle node library sidebar                                                                                        |
    | M                                | Toggle model library sidebar                                                                                       |
    | Cmd  + \`                       | Toggle log bottom panel                                                                                            |
    | F                                | Toggle focus mode (full screen)                                                                                    |
    | R                                | Refresh node definitions                                                                                           |
    | Double-Click LMB                 | Quick search for nodes to add                                                                                      |
  </Tab>
</Tabs>


# Account Management
Source: https://docs.comfy.org/interface/user

In this document, we will introduce the account management features of ComfyUI, including account login, registration, and logout operations.

The account system was added to support `API Nodes`, which enable calls to closed-source model APIs, greatly expanding the possibilities of ComfyUI. Since these API calls consume tokens, we have added a corresponding user system.

Currently, we support the following login methods:

* Email login
* Google login
* Github login
* API Key login (for non-whitelisted site authorization)

We will provide relevant login requirements and explanations in this document.

## ComfyUI Version Requirements

You may need to use at least [ComfyUI v0.3.0](https://github.com/comfyanonymous/ComfyUI/releases/tag/v0.3.30) to use the account system. Ensure that the corresponding frontend version is at least `1.17.11`. Sometimes the frontend may fail to install and revert to an older version, so please check if the frontend version is greater than `1.17.11` in `Settings` -> `About`.

In some regions, network restrictions may prevent normal access to the login API, causing timeouts or failures. Before logging in, please **ensure that your network environment does not restrict access to the corresponding API**, and make sure you can access sites like Google or Github.

<Tip>
  Since we are still in rapid iterative updates, related functions may change. If there are no special circumstances, please try to update to the latest version to get support for relevant functions.
</Tip>

## Network Requirements

To login to ComfyUI account, you must be in a secure network environment:

* Only allow access from `127.0.0.1` or `localhost`.
* Do not support using the `--listen` parameter to access the API node through a local network.
* If you are using a non-SSL certificate or a site that does not start with `https`, you may not be able to successfully log in.
* You may not be able to log in on a site that is not in our whitelist (but you can log in using an API Key now).
* Ensure you can connect to our service normally (some regions may require a proxy).

## How to Log In

Log in via `Settings` -> `User`:

<img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=066170b38e0b9ead026029685e00fa65" alt="ComfyUI User Interface" data-og-width="3358" width="3358" data-og-height="1828" height="1828" data-path="images/interface/setting/user.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=c7f1a017e9b00c6224a440f83d121a59 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=92b830f85f4393d802f7f33bdce81634 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=e294573cc054158fb3a108d10bc67087 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=5160ea18d19dfdde201bbf41dbb1af0b 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=463f4645799b84ea5dcd4879ed3b87ca 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=e52e827f35eddf444e91e5ed4f11b331 2500w" />

## Login Methods

<img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user-login.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=70d1bb2128baed57bcd3d87d0744eb46" alt="user-login" data-og-width="3450" width="3450" data-og-height="1914" height="1914" data-path="images/interface/setting/user-login.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user-login.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=c2d53b9870525d895c8fa81f0e538e3d 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user-login.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=cd42ecbdbdf6125483de8371898b3fbd 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user-login.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=43374539ccc89e2c0280ebfc993ac99b 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user-login.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=35f4abe40eebf63264c33bb3683f11c2 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user-login.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=8eff64dceb83f2aab2b6ba22102e2136 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user-login.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=78a146c092c4a8b51f9839a395151aa3 2500w" />

If this is your first login, please create an account first.

<Card title="Logging in with an API Key" icon="key" href="/account/login#logging-in-with-an-api-key">
  For non-whitelisted sites or LAN environments, you can use API Key login. See the detailed guide in the account login documentation.
</Card>

## Post-Login Status

After logging in, a login button is displayed in the top menu bar of the ComfyUI interface. You can open the corresponding login interface through this button and log out of the corresponding account in the settings menu.

<img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user-logged.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=b23d8df619e6a9bf132ebb2184a8f96c" alt="user-logged" data-og-width="884" width="884" data-og-height="238" height="238" data-path="images/interface/setting/user-logged.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user-logged.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=6513231322c210692fd491202e111e7f 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user-logged.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=f376347fb12608a390e12e5a2c13e356 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user-logged.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=5bd461139043f7801d4c167e2b4b74f8 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user-logged.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=e97426ca6cfea8d2f9723f7762198c2b 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user-logged.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=1d2af01cd57d73a66db70fbf17623f1a 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user-logged.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=05267f070d7950ebc8d928b5df12bcef 2500w" />

<img src="https://mintcdn.com/dripart/i-QzV1V5IIRkC9tt/images/interface/setting/menu-user-logged.jpg?fit=max&auto=format&n=i-QzV1V5IIRkC9tt&q=85&s=114285c984763119766644a35dab4002" alt="menu-user-logged" data-og-width="4266" width="4266" data-og-height="3230" height="3230" data-path="images/interface/setting/menu-user-logged.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/i-QzV1V5IIRkC9tt/images/interface/setting/menu-user-logged.jpg?w=280&fit=max&auto=format&n=i-QzV1V5IIRkC9tt&q=85&s=5b42fd498d1796c9962de0aa1635e957 280w, https://mintcdn.com/dripart/i-QzV1V5IIRkC9tt/images/interface/setting/menu-user-logged.jpg?w=560&fit=max&auto=format&n=i-QzV1V5IIRkC9tt&q=85&s=f8306bcbf6161cc9e1983be741f689a6 560w, https://mintcdn.com/dripart/i-QzV1V5IIRkC9tt/images/interface/setting/menu-user-logged.jpg?w=840&fit=max&auto=format&n=i-QzV1V5IIRkC9tt&q=85&s=54745182bde2201633475e6a3839aa4a 840w, https://mintcdn.com/dripart/i-QzV1V5IIRkC9tt/images/interface/setting/menu-user-logged.jpg?w=1100&fit=max&auto=format&n=i-QzV1V5IIRkC9tt&q=85&s=00fb3349320582961915354a6e167394 1100w, https://mintcdn.com/dripart/i-QzV1V5IIRkC9tt/images/interface/setting/menu-user-logged.jpg?w=1650&fit=max&auto=format&n=i-QzV1V5IIRkC9tt&q=85&s=55e09b7785d7ae4d00c9a1aedff6e8d7 1650w, https://mintcdn.com/dripart/i-QzV1V5IIRkC9tt/images/interface/setting/menu-user-logged.jpg?w=2500&fit=max&auto=format&n=i-QzV1V5IIRkC9tt&q=85&s=18259c2d6696a984912e03d4a4889102 2500w" />

## Frequently Asked Questions

<AccordionGroup>
  <Accordion title="Are there any login device restrictions?">
    We do not restrict login devices. You can log in to your account on any device, but please note that your account information may be accessed by other devices, so do not log in to your account on public devices.
  </Accordion>

  <Accordion title="How to log in in a LAN environment?">
    Currently, only API Key login is supported in a LAN environment. If you are accessing ComfyUI services through a LAN, please use API Key to log in.
  </Accordion>

  <Accordion title="Why can't I log in on some websites?">
    Our login service has a whitelist, so you may not be able to log in to ComfyUI deployed on some servers, for this case, you can use API Key login to solve it.
  </Accordion>
</AccordionGroup>


# ComfyUI Hunyuan3D-2 Examples
Source: https://docs.comfy.org/tutorials/3d/hunyuan3D-2

This guide will demonstrate how to use Hunyuan3D-2 in ComfyUI to generate 3D assets.

# Hunyuan3D 2.0 Introduction

![Hunyuan 3D 2](https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/assets/images/e2e-1.gif)
![Hunyuan 3D 2](https://raw.githubusercontent.com/Tencent/Hunyuan3D-2/main/assets/images/e2e-2.gif)
[Hunyuan3D 2.0](https://github.com/Tencent/Hunyuan3D-2) is an open-source 3D asset generation model released by Tencent, capable of generating high-fidelity 3D models with high-resolution texture maps through text or images.

Hunyuan3D 2.0 adopts a two-stage generation approach, first generating a geometry model without textures, then synthesizing high-resolution texture maps. This effectively separates the complexity of shape and texture generation. Below are the two core components of Hunyuan3D 2.0:

1. **Geometry Generation Model (Hunyuan3D-DiT)**: Based on a flow diffusion Transformer architecture, it generates untextured geometric models that precisely match input conditions.
2. **Texture Generation Model (Hunyuan3D-Paint)**: Combines geometric conditions and multi-view diffusion techniques to add high-resolution textures to models, supporting PBR materials.

**Key Advantages**

* **High-Precision Generation**: Sharp geometric structures, rich texture colors, support for PBR material generation, achieving near-realistic lighting effects.
* **Diverse Usage Methods**: Provides code calls, Blender plugins, Gradio applications, and online experience through the official website, suitable for different user needs.
* **Lightweight and Compatibility**: The Hunyuan3D-2mini model requires only 5GB VRAM, the standard version needs 6GB VRAM for shape generation, and the complete process (shape + texture) requires only 12GB VRAM.

Recently (March 18, 2025), Hunyuan3D 2.0 also introduced a multi-view shape generation model (Hunyuan3D-2mv), which supports generating more detailed geometric structures from inputs at different angles.

This example includes three workflows:

* Using Hunyuan3D-2mv with multiple view inputs to generate 3D models
* Using Hunyuan3D-2mv-turbo with multiple view inputs to generate 3D models
* Using Hunyuan3D-2 with a single view input to generate 3D models

<Tip>
  ComfyUI now natively supports Hunyuan3D-2mv, but does not yet support texture and material generation. Please make sure you have updated to the latest version of [ComfyUI](https://github.com/comfyanonymous/ComfyUI) before starting.

  The workflow example PNG images in this tutorial contain workflow JSON in their metadata:

  * You can drag them directly into ComfyUI
  * Or use the menu `Workflows` -> `Open (ctrl+o)`

  This will load the corresponding workflow and prompt you to download the required models. The generated `.glb` format models will be output to the `ComfyUI/output/mesh` folder.
</Tip>

## ComfyUI Hunyuan3D-2mv Workflow Example

In the Hunyuan3D-2mv workflow, we'll use multi-view images to generate a 3D model. Note that multiple view images are not mandatory in this workflow - you can use only the `front` view image to generate a 3D model.

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

### 1. Workflow

Please download the images below and drag  into ComfyUI to load the workflow.
![Hunyuan3D-2mv workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/hunyuan-3d-multiview-elf.webp)

Download the images below we will use them as input images.

![input image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/front.png)
![input image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/left.png)
![input image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_elf/back.png)

<Tip>
  In this example, the input images have already been preprocessed to remove excess background. In actual use, you can use custom nodes like [ComfyUI\_essentials](https://github.com/cubiq/ComfyUI_essentials) to automatically remove excess background.
</Tip>

### 2. Manual Model Installation

Download the model below and save it to the corresponding ComfyUI folder

* hunyuan3d-dit-v2-mv: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2mv/resolve/main/hunyuan3d-dit-v2-mv/model.fp16.safetensors?download=true) - after downloading, you can rename it to `hunyuan3d-dit-v2-mv.safetensors`

```
ComfyUI/
 models/
    checkpoints/
       hunyuan3d-dit-v2-mv.safetensors  // renamed file
```

### 3. Steps to Run the Workflow

<img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=17e6ac738f0e0133536bceb6e3ea1b56" alt="ComfyUI hunyuan3d_2mv" data-og-width="3000" width="3000" data-og-height="1618" height="1618" data-path="images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=9e42b6d0f41ffc9f6ceaa572386dcf3e 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=28c3841fabf7f4e776285bbc05ce7d52 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=6bddcac5fe31ed37eec2b0ca80892364 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=123cc678058a9d792a1788ed4e246a2c 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=dbe9c37ce5d6241481b71a1c0965a4bd 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=9dcf8c15d43ca930dbb41af7072acc27 2500w" />

1. Ensure that the Image Only Checkpoint Loader(img2vid model) has loaded our downloaded and renamed `hunyuan3d-dit-v2-mv.safetensors` model
2. Load the corresponding view images in each of the `Load Image` nodes
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

If you need to add more views, make sure to load other view images in the `Hunyuan3Dv2ConditioningMultiView` node, and ensure that you load the corresponding view images in the `Load Image` nodes.

## Hunyuan3D-2mv-turbo Workflow

In the Hunyuan3D-2mv-turbo workflow, we'll use the Hunyuan3D-2mv-turbo model to generate 3D models. This model is a step distillation version of Hunyuan3D-2mv, allowing for faster 3D model generation. In this version of the workflow, we set `cfg` to 1.0 and add a `flux guidance` node to control the `distilled cfg` generation.

### 1. Workflow

Please download the images below and drag into ComfyUI to load the workflow.

![Hunyuan3D-2mv-turbo workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_turbo/hunyuan-3d-turbo.webp)

Download the images below we will use them as input images.

![input image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_turbo/front.png)
![input image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d_2mv_turbo/right.png)

### 2. Manual Model Installation

Download the model below and save it to the corresponding ComfyUI folder

* hunyuan3d-dit-v2-mv-turbo: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2mv/resolve/main/hunyuan3d-dit-v2-mv-turbo/model.fp16.safetensors?download=true) - after downloading, you can rename it to `hunyuan3d-dit-v2-mv-turbo.safetensors`

```
ComfyUI/
 models/
    checkpoints/
       hunyuan3d-dit-v2-mv-turbo.safetensors  // renamed file
```

### 3. Steps to Run the Workflow

<img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv_turbo.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=33bb861fd695fd4b10a53345f004d2cc" alt="ComfyUI hunyuan3d_2mv_turbo" data-og-width="3000" width="3000" data-og-height="1621" height="1621" data-path="images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv_turbo.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv_turbo.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=4f8fc51054f97a10e0c1d3b55cc3f6a5 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv_turbo.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=18b71c565a8ac2fed93fe1357d0f3675 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv_turbo.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=b9bcdff38ec468193b8e226fb29f75ae 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv_turbo.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=acb764f7ce1b01fb295bc2e4d2c62736 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv_turbo.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=dec0c68da4a0cadff5d596b2da3ec85e 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2mv_turbo.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=6380467b73bddf75b59a77e76e160273 2500w" />

1. Ensure that the `Image Only Checkpoint Loader(img2vid model)` node has loaded our renamed `hunyuan3d-dit-v2-mv-turbo.safetensors` model
2. Load the corresponding view images in each of the `Load Image` nodes
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## Hunyuan3D-2 Single View Workflow

In the Hunyuan3D-2 workflow, we'll use the Hunyuan3D-2 model to generate 3D models. This model is not a multi-view model. In this workflow, we use the `Hunyuan3Dv2Conditioning` node instead of the `Hunyuan3Dv2ConditioningMultiView` node.

### 1. Workflow

Please download the image below and drag it into ComfyUI to load the workflow.

![Hunyuan3D-2 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan3d-non-multiview-train.webp)

Download the image below we will use it as input image.
![ComfyUI Hunyuan 3D 2 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/3d/hunyuan3d-2/hunyuan_3d_v2_non_multiview_train.png)

### 2. Manual Model Installation

Download the model below and save it to the corresponding ComfyUI folder

* hunyuan3d-dit-v2-0: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2/resolve/main/hunyuan3d-dit-v2-0/model.fp16.safetensors?download=true) - after downloading, you can rename it to `hunyuan3d-dit-v2.safetensors`

```
ComfyUI/
 models/
    checkpoints/
       hunyuan3d-dit-v2.safetensors  // renamed file
```

### 3. Steps to Run the Workflow

<img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2_non_multiview.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=33c158fcfb133560674aa56bfdb5087d" alt="ComfyUI hunyuan3d_2" data-og-width="3000" width="3000" data-og-height="1624" height="1624" data-path="images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2_non_multiview.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2_non_multiview.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=c577c3cf599981076e6fbd785564e55c 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2_non_multiview.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=ab1a5bdb4aa0691bb7c41d643cf354a1 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2_non_multiview.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=67127b905771176d81f36e7a8e59b38b 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2_non_multiview.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=4648a8e392939372cd68cd10de587e8b 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2_non_multiview.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=84e03ff178ff77a94eb8fb806f6d4234 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/3d/hunyuan3d-2mv/hunyuan3d_2_non_multiview.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=d53501f12fcd700e312fddc1dd12154b 2500w" />

1. Ensure that the `Image Only Checkpoint Loader(img2vid model)` node has loaded our renamed `hunyuan3d-dit-v2.safetensors` model
2. Load the image in the `Load Image` node
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## Community Resources

Below are ComfyUI community resources related to Hunyuan3D-2

* [ComfyUI-Hunyuan3DWrapper](https://github.com/kijai/ComfyUI-Hunyuan3DWrapper)
* [Kijai/Hunyuan3D-2\_safetensors](https://huggingface.co/Kijai/Hunyuan3D-2_safetensors/tree/main)
* [ComfyUI-3D-Pack](https://github.com/MrForExample/ComfyUI-3D-Pack)

## Hunyuan3D 2.0 Open-Source Model Series

Currently, Hunyuan3D 2.0 has open-sourced multiple models covering the complete 3D generation process. You can visit [Hunyuan3D-2](https://github.com/Tencent/Hunyuan3D-2) for more information.

**Hunyuan3D-2mini Series**

| Model                 | Description               | Date       | Parameters | Huggingface                                                                             |
| --------------------- | ------------------------- | ---------- | ---------- | --------------------------------------------------------------------------------------- |
| Hunyuan3D-DiT-v2-mini | Mini Image to Shape Model | 2025-03-18 | 0.6B       | [Visit](https://huggingface.co/tencent/Hunyuan3D-2mini/tree/main/hunyuan3d-dit-v2-mini) |

**Hunyuan3D-2mv Series**

| Model                    | Description                                                                                                 | Date       | Parameters | Huggingface                                                                              |
| ------------------------ | ----------------------------------------------------------------------------------------------------------- | ---------- | ---------- | ---------------------------------------------------------------------------------------- |
| Hunyuan3D-DiT-v2-mv-Fast | Guidance Distillation Version, can halve DIT inference time                                                 | 2025-03-18 | 1.1B       | [Visit](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv-fast) |
| Hunyuan3D-DiT-v2-mv      | Multi-view Image to Shape Model, suitable for 3D creation requiring multiple angles to understand the scene | 2025-03-18 | 1.1B       | [Visit](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv)      |

**Hunyuan3D-2 Series**

| Model                   | Description                 | Date       | Parameters | Huggingface                                                                           |
| ----------------------- | --------------------------- | ---------- | ---------- | ------------------------------------------------------------------------------------- |
| Hunyuan3D-DiT-v2-0-Fast | Guidance Distillation Model | 2025-02-03 | 1.1B       | [Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0-fast) |
| Hunyuan3D-DiT-v2-0      | Image to Shape Model        | 2025-01-21 | 1.1B       | [Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0)      |
| Hunyuan3D-Paint-v2-0    | Texture Generation Model    | 2025-01-21 | 1.3B       | [Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-paint-v2-0)    |
| Hunyuan3D-Delight-v2-0  | Image Delight Model         | 2025-01-21 | 1.3B       | [Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-delight-v2-0)  |


# ComfyUI ACE-Step Native Example
Source: https://docs.comfy.org/tutorials/audio/ace-step/ace-step-v1

This guide will help you create dynamic music using the ACE-Step model in ComfyUI

ACE-Step is an open-source foundational music generation model jointly developed by Chinese team StepFun and ACE Studio, aimed at providing music creators with efficient, flexible and high-quality music generation and editing tools.

The model is released under the [Apache-2.0](https://github.com/ace-step/ACE-Step?tab=readme-ov-file#-license) license and is free for commercial use.

As a powerful music generation foundation, ACE-Step provides rich extensibility. Through fine-tuning techniques like LoRA and ControlNet, developers can customize the model according to their actual needs.
Whether it's audio editing, vocal synthesis, accompaniment production, voice cloning or style transfer applications, ACE-Step provides stable and reliable technical support.
This flexible architecture greatly simplifies the development process of music AI applications, allowing more creators to quickly apply AI technology to music creation.

Currently, ACE-Step has released related training code, including LoRA model training, and the corresponding ControlNet training code will be released in the future.
You can visit their [Github](https://github.com/ace-step/ACE-Step?tab=readme-ov-file#-roadmap) to learn more details.

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## ACE-Step ComfyUI Text-to-Audio Generation Workflow Example

### 1. Download Workflow and Related Models

Click the button below to download the corresponding workflow file. Drag it into ComfyUI to load the workflow information. The workflow includes model download information.

Click the button below to download the corresponding workflow file. Drag it into ComfyUI to load the workflow information. The workflow includes model download information.

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/audio/ace-step/ace_step_1_t2m.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download Json Format Workflow File</p>
</a>

You can also manually download [ace\_step\_v1\_3.5b.safetensors](https://huggingface.co/Comfy-Org/ACE-Step_ComfyUI_repackaged/blob/main/all_in_one/ace_step_v1_3.5b.safetensors) and save it to the `ComfyUI/models/checkpoints` folder

### 2. Complete the Workflow Step by Step

<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/audio/ace_step/ace_step_1_t2a_step_guide.jpg?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=236a3db013af8dda09a7730916f24da0" alt="Step Guide" data-og-width="2583" width="2583" data-og-height="1328" height="1328" data-path="images/tutorial/audio/ace_step/ace_step_1_t2a_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/audio/ace_step/ace_step_1_t2a_step_guide.jpg?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=309a437f463a28efba4d8ea61aa7500b 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/audio/ace_step/ace_step_1_t2a_step_guide.jpg?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=7468bdcde82ab2597c0d16e05d0e9b22 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/audio/ace_step/ace_step_1_t2a_step_guide.jpg?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=90a7d7daa9ac44f9e0c88c3cb3c696a4 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/audio/ace_step/ace_step_1_t2a_step_guide.jpg?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=695d947ba36f923d9532ade671ad36c2 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/audio/ace_step/ace_step_1_t2a_step_guide.jpg?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=67c9b001e188d03dad95100b7d0d6b6c 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/audio/ace_step/ace_step_1_t2a_step_guide.jpg?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=f6fd4bbb8a6a9b41b5943059ac984efb 2500w" />

1. Ensure the `Load Checkpoints` node has loaded the `ace_step_v1_3.5b.safetensors` model
2. (Optional) In the `EmptyAceStepLatentAudio` node, you can set the duration of the music to be generated
3. (Optional) In the `LatentOperationTonemapReinhard` node, you can adjust the `multiplier` to control the volume of the vocals (higher numbers result in more prominent vocals)
4. (Optional) Input corresponding music styles etc. in the `tags` field of `TextEncodeAceStepAudio`
5. (Optional) Input corresponding lyrics in the `lyrics` field of `TextEncodeAceStepAudio`
6. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the audio generation
7. After the workflow completes,, you can preview the generated audio in the `Save Audio` node. You can click to play and listen to it, and the audio will also be saved to `ComfyUI/output/audio` (subdirectory determined by the `Save Audio` node).

## ACE-Step ComfyUI Audio-to-Audio Workflow

Similar to image-to-image workflows, you can input a piece of music and use the workflow below to resample and generate music. You can also adjust the difference from the original audio by controlling the `denoise` parameter in the `Ksampler`.

### 1. Download Workflow File

Click the button below to download the corresponding workflow file. Drag it into ComfyUI to load the workflow information.

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/audio/ace-step/ace_step_1_m2m_editing.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download Json Format Workflow File</p>
</a>

Download the following audio file as the input audio:

<a className="prose" target="_blank" href="https://github.com/Comfy-Org/example_workflows/raw/refs/heads/main/audio/ace-step/input.mp3" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download Example Audio File for Input</p>
</a>

### 2. Complete the Workflow Step by Step

<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/audio/ace_step/ace_step_1_m2m_step_guide.jpg?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=74b041e1a727b4d4bfa22f6ae446d611" alt="ACE-Step Step Guide" data-og-width="2393" width="2393" data-og-height="1241" height="1241" data-path="images/tutorial/audio/ace_step/ace_step_1_m2m_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/audio/ace_step/ace_step_1_m2m_step_guide.jpg?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=183515127facdc31a45ecbffba2ee44f 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/audio/ace_step/ace_step_1_m2m_step_guide.jpg?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=0049a0613a2d5d52b2e17aa0680c184d 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/audio/ace_step/ace_step_1_m2m_step_guide.jpg?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=5b51994f52cc17a92bffac03fd3ddc36 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/audio/ace_step/ace_step_1_m2m_step_guide.jpg?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=a1e40f3dabde3aba980466e77d3a9b43 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/audio/ace_step/ace_step_1_m2m_step_guide.jpg?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=adacf1b3ec4887c5eee671ec5c26d40e 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/audio/ace_step/ace_step_1_m2m_step_guide.jpg?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=8696e5f924054ca98d687f3f8a08f9b8 2500w" />

1. Ensure the `Load Checkpoints` node has loaded the `ace_step_v1_3.5b.safetensors` model
2. Upload the provided audio file in the `LoadAudio` node
3. (Optional) Input corresponding music styles and lyrics in the `tags` and `lyrics` fields of `TextEncodeAceStepAudio`. Providing lyrics is very important for audio editing
4. (Optional) Modify the `denoise` parameter in the `Ksampler` node to adjust the noise added during sampling to control similarity with the original audio (smaller values result in more similarity to the original audio; setting it to `1.00` is approximately equivalent to having no audio input)
5. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the audio generation
6. After the workflow completes, you can preview the generated audio in the `Save Audio` node. You can click to play and listen to it, and the audio will also be saved to `ComfyUI/output/audio` (subdirectory determined by the `Save Audio` node).

You can also implement the lyrics modification and editing functionality from the ACE-Step project page, modifying the original lyrics to change the audio effect.

### 3. Additional Workflow Notes

1. In the example workflow, you can change the `tags` in `TextEncodeAceStepAudio` from `male voice` to `female voice` to generate female vocals.
2. You can also modify the `lyrics` in `TextEncodeAceStepAudio` to change the lyrics and thus the generated audio. Refer to the examples on the ACE-Step project page for more details.

## ACE-Step Prompt Guide

ACE currently uses two types of prompts: `tags` and `lyrics`.

* `tags`: Mainly used to describe music styles, scenes, etc. Similar to prompts we use for other generations, they primarily describe the overall style and requirements of the audio, separated by English commas
* `lyrics`: Mainly used to describe lyrics, supporting lyric structure tags such as \[verse], \[chorus], and \[bridge] to distinguish different parts of the lyrics. You can also input instrument names for purely instrumental music

You can find rich examples of `tags` and `lyrics` on the [ACE-Step model homepage](https://ace-step.github.io/). You can refer to these examples to try corresponding prompts. This document's prompt guide is organized based on the project to help you quickly try combinations to achieve your desired effect.

### Tags (prompt)

#### Mainstream Music Styles

Use short tag combinations to generate specific music styles

* electronic
* rock
* pop
* funk
* soul
* cyberpunk
* Acid jazz
* electro
* em (electronic music)
* soft electric drums
* melodic

#### Scene Types

Combine specific usage scenarios and atmospheres to generate music that matches the corresponding mood

* background music for parties
* radio broadcasts
* workout playlists

#### Instrumental Elements

* saxophone
* jazz
* piano, violin

#### Vocal Types

* female voice
* male voice
* clean vocals

#### Professional Terms

Use some professional terms commonly used in music to precisely control music effects

* 110 bpm (beats per minute is 110)

* fast tempo

* slow tempo

* loops

* fills

* acoustic guitar

* electric bass

{/* - Lyrics editing:
  - edit lyrics: 'When I was young' -> 'When you were kid' (lyrics editing example) */}

### Lyrics

#### Lyric Structure Tags

* \[outro]
* \[verse]
* \[chorus]
* \[bridge]

#### Multilingual Support

* ACE-Step V1 supports multiple languages. When used, ACE-Step converts different languages into English letters and then generates music.
* In ComfyUI, we haven't fully implemented the conversion of all languages to English letters. Currently, only [Japanese hiragana and katakana characters](https://github.com/comfyanonymous/ComfyUI/commit/5d3cc85e13833aeb6ef9242cdae243083e30c6fc) are implemented.
  So if you need to use multiple languages for music generation, you need to first convert the corresponding language to English letters, and then input the language code abbreviation at the beginning of the `lyrics`, such as Chinese `[zh]`, Korean `[ko]`, etc.

For example:

```
[verse]

[zh]wo3zou3guo4shen1ye4de5jie1dao4
[zh]leng3feng1chui1luan4si1nian4de5piao4liang4wai4tao4
[zh]ni3de5wei1xiao4xiang4xing1guang1hen3xuan4yao4
[zh]zhao4liang4le5wo3gu1du2de5mei3fen1mei3miao3

[chorus]

[verse]
[ko]hamkke si-kkeuleo-un sesang-ui sodong-eul pihae
[ko]honja ogsang-eseo dalbich-ui eolyeompus-ileul balaboda
[ko]niga salang-eun lideum-i ganghan eum-ag gatdago malhaess-eo
[ko]han ta han tamada ma-eum-ui ondoga eolmana heojeonhanji ijge hae

[bridge]
[es]cantar mi anhelo por ti sin ocultar
[es]como poesa y pintura, lleno de anhelo indescifrable
[es]tu sombra es tan terca como el viento, inborrable
[es]persiguindote en vuelo, brilla como cruzar una mar de nubes

[chorus]
[fr]que tu sois le vent qui souffle sur ma main
[fr]un contact chaud comme la douce pluie printanire
[fr]que tu sois le vent qui s'entoure de mon corps
[fr]un amour profond qui ne s'loignera jamais
```

Currently, ACE-Step supports 19 languages, but the following ten languages have better support:

* English
* Chinese: \[zh]
* Russian: \[ru]
* Spanish: \[es]
* Japanese: \[ja]
* German: \[de]
* French: \[fr]
* Portuguese: \[pt]
* Italian: \[it]
* Korean: \[ko]

<Note>
  The language tags above have not been fully tested at the time of writing this documentation. If any language tag is incorrect, please [submit an issue to our documentation repository](https://github.com/Comfy-Org/docs/issues) and we will make timely corrections.
</Note>

## ACE-Step Related Resources

* [Project Page](https://ace-step.github.io/)
* [Hugging Face](https://huggingface.co/ACE-Step/ACE-Step-v1-3.5B)
* [GitHub](https://github.com/ace-step/ACE-Step)
* [Training Scripts](https://github.com/ace-step/ACE-Step?tab=readme-ov-file#-train)


# ComfyUI Image to Image Workflow
Source: https://docs.comfy.org/tutorials/basic/image-to-image

This guide will help you understand and complete an image to image workflow

## What is Image to Image

Image to Image is a workflow in ComfyUI that allows users to input an image and generate a new image based on it.

Image to Image can be used in scenarios such as:

* Converting original image styles, like transforming realistic photos into artistic styles
* Converting line art into realistic images
* Image restoration
* Colorizing old photos
* ... and other scenarios

To explain it with an analogy:
It's like asking an artist to create a specific piece based on your reference image.

If you carefully compare this tutorial with the [Text to Image](/tutorials/basic/text-to-image) tutorial,
you'll notice that the Image to Image process is very similar to Text to Image,
just with an additional input reference image as a condition. In Text to Image, we let the artist (image model) create freely based on our prompts,
while in Image to Image, we let the artist create based on both our reference image and prompts.

## ComfyUI Image to Image Workflow Example Guide

### Model Installation

Download the [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) file and put it in your `ComfyUI/models/checkpoints` folder.

### Image to Image Workflow and Input Image

Download the image below and **drag it into ComfyUI** to load the workflow:
![Image to Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/image_to_image/workflow.png)

<Tip>
  Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
</Tip>

Download the image below and we will use it as the input image:
<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/img2img/input.jpeg?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=f8d1c4f2afa4a80f07a6ad862506c832" alt="Example Image" data-og-width="1024" width="1024" data-og-height="1024" height="1024" data-path="images/tutorial/basic/img2img/input.jpeg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/img2img/input.jpeg?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=2cf62d99d9595053f961e18bbae104fa 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/img2img/input.jpeg?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=25ee69405b3bb78b8ad9fffa930324ec 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/img2img/input.jpeg?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=96d0a0acd6f76c4b3d547f5911dd2888 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/img2img/input.jpeg?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=54694dfd7f86ad5d13363b8123ed95cb 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/img2img/input.jpeg?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=7dcb3f779c1a36505bde2f7f50828ea3 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/img2img/input.jpeg?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=fa0782cbc1a84bc318679384cbbb0873 2500w" />

### Complete the Workflow Step by Step

Follow the steps in the diagram below to ensure the workflow runs correctly.

<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/img2img/image-to-image-02-guide.jpg?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=dd85418c3e8070eb5c3f491a89fd29b3" alt="ComfyUI Image to Image Workflow - Steps" data-og-width="1197" width="1197" data-og-height="779" height="779" data-path="images/tutorial/basic/img2img/image-to-image-02-guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/img2img/image-to-image-02-guide.jpg?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=1b0cc094853876eb09edcd81575f3171 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/img2img/image-to-image-02-guide.jpg?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=4630801bea3f11270b97b29cfcc77bea 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/img2img/image-to-image-02-guide.jpg?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=4935b3f629d63f2bf0c8a79fa864eb16 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/img2img/image-to-image-02-guide.jpg?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=ba9c51655e26715be1b316c307c3a761 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/img2img/image-to-image-02-guide.jpg?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=90406adf554c73a31ffe9b86cfea06ed 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/img2img/image-to-image-02-guide.jpg?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=d813a71613511b97258f4b01317f565a 2500w" />

1. Ensure `Load Checkpoint` loads  **v1-5-pruned-emaonly-fp16.safetensors**
2. Upload the input image to the `Load Image` node
3. Click `Queue` or press `Ctrl/Cmd + Enter` to generate

## Key Points of Image to Image Workflow

The key to the Image to Image workflow lies in the `denoise` parameter in the `KSampler` node, which should be **less than 1**

If you've adjusted the `denoise` parameter and generated images, you'll notice:

* The smaller the `denoise` value, the smaller the difference between the generated image and the reference image
* The larger the `denoise` value, the larger the difference between the generated image and the reference image

This is because `denoise` determines the strength of noise added to the latent space image after converting the reference image. If `denoise` is 1, the latent space image will become completely random noise, making it the same as the latent space generated by the `empty latent image` node, losing all characteristics of the reference image.

For the corresponding principles, please refer to the principle explanation in the [Text to Image](/tutorials/basic/text-to-image) tutorial.

## Try It Yourself

1. Try modifying the `denoise` parameter in the **KSampler** node, gradually changing it from 1 to 0, and observe the changes in the generated images
2. Replace with your own prompts and reference images to generate your own image effects


# ComfyUI Inpainting Workflow
Source: https://docs.comfy.org/tutorials/basic/inpaint

This guide will introduce you to the inpainting workflow in ComfyUI, walk you through an inpainting example, and cover topics like using the mask editor

This article will introduce the concept of inpainting in AI image generation and guide you through creating an inpainting workflow in ComfyUI. We'll cover:

* Using inpainting workflows to modify images
* Using the ComfyUI mask editor to draw masks
* `VAE Encoder (for Inpainting)` node

## About Inpainting

In AI image generation, we often encounter situations where we're satisfied with the overall image but there are elements we don't want or that contain errors. Simply regenerating might produce a completely different image, so using inpainting to fix specific parts becomes very useful.

It's like having an **artist (AI model)** paint a picture, but we're still not satisfied with the specific details. We need to tell the artist **which areas to adjust (mask)**, and then let them **repaint (inpaint)** according to our requirements.

Common inpainting scenarios include:

* **Defect Repair:** Removing unwanted objects, fixing incorrect AI-generated body parts, etc.
* **Detail Optimization:** Precisely adjusting local elements (like modifying clothing textures, adjusting facial expressions)
* And other scenarios

## ComfyUI Inpainting Workflow Example

### Model and Resource Preparation

#### 1. Model Installation

Download the [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors)  file and put it in your `ComfyUI/models/checkpoints` folder:

#### 2. Inpainting Asset

Please download the following image which we'll use as input:

<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/input.png?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=a6557da482429ba4feff636d36f1bb54" alt="ComfyUI Inpainting Input Image" data-og-width="1024" width="1024" data-og-height="1024" height="1024" data-path="images/tutorial/basic/inpaint/input.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/input.png?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=ab6da5c62e779df51cab0462652ae03b 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/input.png?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=89b7bba32e0e50aa5e0b14a1780f9f81 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/input.png?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=c8a4c7369adda8b0e594c36c1253ad69 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/input.png?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=a5e3e567978c89496e6b301e17889a40 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/input.png?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=3f207a73f6d035e655d204467aa9a97f 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/input.png?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=ff19842492f61b33b7c7b6bb7fe8a791 2500w" />

<Note>This image already contains an alpha channel (transparency mask), so you don't need to manually draw a mask. This tutorial will also cover how to use the mask editor to draw masks.</Note>

#### 3. Inpainting Workflow

Download the image below and **drag it into ComfyUI** to load the workflow:

![ComfyUI Inpainting Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/image/basic/sd1.5_inpaint.png)

<Tip>
  Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
</Tip>

### ComfyUI Inpainting Workflow Example Explanation

Follow the steps in the diagram below to ensure the workflow runs correctly.

<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_workflow.png?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=eec79976ea5771763a00668de7c15bc3" alt="ComfyUI Inpainting Workflow" data-og-width="2000" width="2000" data-og-height="1108" height="1108" data-path="images/tutorial/basic/inpaint/inpaint_workflow.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_workflow.png?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=a6fdc071447c0798d9beb2ad963249c1 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_workflow.png?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=483c6cc059ba949df3d3ae302e904dfc 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_workflow.png?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=dae548fecc00b515bbf698f15457f760 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_workflow.png?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=e4e471b6cd770836162c82a14c31590f 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_workflow.png?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=937ae7a877e2ad00b161f1489d599207 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_workflow.png?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=551b69ce8a2eb8e4f52ca448abb87366 2500w" />

1. Ensure `Load Checkpoint` loads `512-inpainting-ema.safetensors`
2. Upload the input image to the `Load Image` node
3. Click `Queue` or use `Ctrl + Enter` to generate

For comparison, here's the result using the [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) model:

<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_sd1.5_pruned_emaonly.png?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=36916904f9884b5bc3c62f6b60ec17e8" alt="SD1.5 Inpainting Result" data-og-width="1024" width="1024" data-og-height="1024" height="1024" data-path="images/tutorial/basic/inpaint/inpaint_sd1.5_pruned_emaonly.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_sd1.5_pruned_emaonly.png?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=793ae8991e483962785f4a1ebdc66931 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_sd1.5_pruned_emaonly.png?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=5f36ef651544636822ef6b910cf93a45 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_sd1.5_pruned_emaonly.png?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=b8f76ae73e41564f61668de75dd3784f 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_sd1.5_pruned_emaonly.png?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=e04c184d8e04180645be207d60754f2b 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_sd1.5_pruned_emaonly.png?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=b725d2042a4228df0f14498cfbb742a7 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_sd1.5_pruned_emaonly.png?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=07e4eaecd7725da7adb146d9a06e37d6 2500w" />

You will find that the results generated by the [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors) model have better inpainting effects and more natural transitions.
This is because this model is specifically designed for inpainting, which helps us better control the generation area, resulting in improved inpainting effects.

Do you remember the analogy we've been using? Different models are like artists with varying abilities, but each artist has their own limits. Choosing the right model can help you achieve better generation results.

You can try these approaches to achieve better results:

1. Modify positive and negative prompts with more specific descriptions
2. Try multiple runs using different seeds in the `KSampler` for different generation results
3. After learning about the mask editor in this tutorial, you can re-inpaint the generated results to achieve satisfactory outcomes.

Next, we'll learn about using the **Mask Editor**. While our input image already includes an `alpha` transparency channel (the area we want to edit),
so manual mask drawing isn't necessary, you'll often use the Mask Editor to create masks in practical applications.

### Using the Mask Editor

First right-click the `Save Image` node and select `Copy(Clipspace)`:

<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_copy_clipspace.png?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=7d1bfea21723437c4ca09bba34edd1ed" alt="Copy Image to Clipboard" data-og-width="751" width="751" data-og-height="1112" height="1112" data-path="images/tutorial/basic/inpaint/inpaint_copy_clipspace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_copy_clipspace.png?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=471161782a68a4a1c9b6b93f90c359bf 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_copy_clipspace.png?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=19e42850cde5a3165aa297c9839a9262 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_copy_clipspace.png?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=ecc873dc3f9a633fee9cb2f030aec515 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_copy_clipspace.png?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=3e31c622dd7630f5eb12bed77beffb1a 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_copy_clipspace.png?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=232b22932471ab2713a99b4c9dbd9ad0 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_copy_clipspace.png?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=587e879f30a86eb915838ef2aba36ddd 2500w" />

Then right-click the **Load Image** node and select `Paste(Clipspace)`:

<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_paste_clipspace.png?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=42c942d88b97c696f6f2ed59f7ff9bc8" alt="Paste Image" data-og-width="750" width="750" data-og-height="947" height="947" data-path="images/tutorial/basic/inpaint/inpaint_paste_clipspace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_paste_clipspace.png?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=947f8a2ba53d5e91da74d83cbe7de177 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_paste_clipspace.png?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=aa0df4dd3f7700f2c817fdfeb6af7db8 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_paste_clipspace.png?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=7ec2cdd378a38fa9baab14823b4089b3 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_paste_clipspace.png?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=3ca8dc639d673d4c6094d48bb1112491 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_paste_clipspace.png?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=3ee5a66e68a9f6b2f3c72621986f5f6e 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_paste_clipspace.png?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=547532ec78c09b89905cf4a1efa28175 2500w" />

Right-click the **Load Image** node again and select `Open in MaskEditor`:

<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_open_in_maskeditor.jpg?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=682a66f54e0f61366c42e553d50f4e76" alt="Open Mask Editor" data-og-width="894" width="894" data-og-height="1000" height="1000" data-path="images/tutorial/basic/inpaint/inpaint_open_in_maskeditor.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_open_in_maskeditor.jpg?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=8aec5f48665275dbe971f1e6b06a8689 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_open_in_maskeditor.jpg?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=0d5c93b36e0bdb80bdd09329e4ae1f3a 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_open_in_maskeditor.jpg?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=e1eda1fc545857dbd5f5d7544c0562f8 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_open_in_maskeditor.jpg?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=fcf26f11d3563e21c4f6c1a821b98ce5 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_open_in_maskeditor.jpg?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=3fe4854c50fe874a310e8b08efa118e8 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint_open_in_maskeditor.jpg?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=4c0739d96a8d0b998a8be453bdeca656 2500w" />

<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/inpaint/inpaint-maskeditor.gif?s=217f481d8cd4e22183c0dd1551f3e831" alt="Mask Editor Demo" data-og-width="960" width="960" data-og-height="720" height="720" data-path="images/tutorial/basic/inpaint/inpaint-maskeditor.gif" data-optimize="true" data-opv="3" />

1. Adjust brush parameters on the right panel
2. Use eraser to correct mistakes
3. Click `Save` when finished

The drawn content will be used as a Mask input to the VAE Encoder (for Inpainting) node for encoding

Then try adjusting your prompts and generating again until you achieve satisfactory results.

## VAE Encoder (for Inpainting) Node

Comparing this workflow with [Text-to-Image](/tutorials/basic/text-to-image) and [Image-to-Image](/tutorials/basic/image-to-image), you'll notice the main differences are in the VAE section's conditional inputs.
In this workflow, we use the **VAE Encoder (for Inpainting)** node, specifically designed for inpainting to help us better control the generation area and achieve better results.

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/latent/inpaint/vae_encode_for_inpainting.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=2eeecee7cf23a0e1ab7b1a996256278f" alt="VAE Encoder (for Inpainting) Node" data-og-width="854" width="854" data-og-height="440" height="440" data-path="images/comfy_core/latent/inpaint/vae_encode_for_inpainting.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/latent/inpaint/vae_encode_for_inpainting.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=a4dab6ea6ede77a88491abcca4486b12 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/latent/inpaint/vae_encode_for_inpainting.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=504884ff3853ee9e623e077c85092b5c 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/latent/inpaint/vae_encode_for_inpainting.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=93307ae96b3c05fc271dd06af325a6c9 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/latent/inpaint/vae_encode_for_inpainting.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=120a4315d7eaeaacf754647e55f620d4 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/latent/inpaint/vae_encode_for_inpainting.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=267f53b6551eeac2952cea05913f6ff3 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/latent/inpaint/vae_encode_for_inpainting.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=a27dd641b206b80d3e6e79154b5b16f5 2500w" />

**Input Types**

| Parameter Name | Function                                                                                                                                              |
| -------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------- |
| `pixels`       | Input image to be encoded into latent space.                                                                                                          |
| `vae`          | VAE model used to encode the image from pixel space to latent space.                                                                                  |
| `mask`         | Image mask specifying which areas need modification.                                                                                                  |
| `grow_mask_by` | Pixel value to expand the original mask outward, ensuring a transition area around the mask to avoid hard edges between inpainted and original areas. |

**Output Types**

| Parameter Name | Function                                    |
| -------------- | ------------------------------------------- |
| `latent`       | Image encoded into latent space by the VAE. |


# ComfyUI LoRA Example
Source: https://docs.comfy.org/tutorials/basic/lora

This guide will help you understand and use a single LoRA model

**LoRA (Low-Rank Adaptation)** is an efficient technique for fine-tuning large generative models like Stable Diffusion.
It introduces trainable low-rank matrices to the pre-trained model, adjusting only a portion of parameters rather than retraining the entire model,
thus achieving optimization for specific tasks at a lower computational cost.
Compared to base models like SD1.5, LoRA models are smaller and easier to train.

<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/lora/compare.png?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=742213e2c13c5be487ea3e9827630e5a" alt="LoRA Model vs Base Model Comparison" data-og-width="1356" width="1356" data-og-height="678" height="678" data-path="images/tutorial/basic/lora/compare.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/lora/compare.png?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=8dd68996e3201f0cbc9c416cafe9d0e2 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/lora/compare.png?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=77fa89d09f71e667183c193fedb36cb1 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/lora/compare.png?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=f1cb5b3500526ccdd89e2b79617c78a8 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/lora/compare.png?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=36e4eeeb06a19e39a097b04ee47830a8 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/lora/compare.png?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=1169f2af3ce625f4ab28d40c3a3c5d76 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/lora/compare.png?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=cbcc1878cf286e1554e2909484c4cfb7 2500w" />

The image above compares generation with the same parameters using [dreamshaper\_8](https://civitai.com/models/4384?modelVersionId=128713) directly versus using the [blindbox\_V1Mix](https://civitai.com/models/25995/blindbox) LoRA model.
As you can see, by using a LoRA model, we can generate images in different styles without adjusting the base model.

We will demonstrate how to use a LoRA model. All LoRA variants: Lycoris, loha, lokr, locon, etc... are used in the same way.

In this example, we will learn how to load and use a LoRA model in [ComfyUI](https://github.com/comfyanonymous/ComfyUI), covering the following topics:

1. Installing a LoRA model
2. Generating images using a LoRA model
3. A simple introduction to the `Load LoRA` node

## Required Model Installation

Download the [dreamshaper\_8.safetensors](https://civitai.com/api/download/models/128713?type=Model\&format=SafeTensor\&size=pruned\&fp=fp16) file and put it in your `ComfyUI/models/checkpoints` folder.

Download the [blindbox\_V1Mix.safetensors](https://civitai.com/api/download/models/32988?type=Model\&format=SafeTensor\&size=full\&fp=fp16) file and put it in your `ComfyUI/models/loras` folder.

## LoRA Workflow File

Download the image below and **drag it into ComfyUI** to load the workflow.
<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/lora/lora.png?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=5f67dbeed58fd00da65100432c06edc7" alt="ComfyUI Workflow - LoRA" data-og-width="768" width="768" data-og-height="768" height="768" data-path="images/tutorial/basic/lora/lora.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/lora/lora.png?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=8bafbca505fc7df69aa336e757af2882 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/lora/lora.png?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=d5a60578805b6a5e1c95e6eaf7820f2a 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/lora/lora.png?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=5b6afce06fbc666994656a7ca34dd096 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/lora/lora.png?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=861c7ccacfcd1824c3e69eaa45c4b64c 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/lora/lora.png?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=7d6a66db46caf528a187be5b098abf48 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/lora/lora.png?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=7134404206b433d3dd4ea0c83c1de8c8 2500w" />

<Tip>
  Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
</Tip>

## Complete the Workflow Step by Step

Follow the steps in the diagram below to ensure the workflow runs correctly.

![ComfyUI Workflow - LoRA Flow Diagram](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/lora.png)

1. Ensure `Load Checkpoint` loads `dreamshaper_8.safetensors`
2. Ensure `Load LoRA` loads `blindbox_V1Mix.safetensors`
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to generate the image

## Load LoRA Node Introduction

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/loaders/load_lora.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=86dd2ba28327b5c6c182a9fb4f4404bb" alt="Load LoRA Node" data-og-width="807" width="807" data-og-height="443" height="443" data-path="images/comfy_core/loaders/load_lora.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/loaders/load_lora.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=eb9fa4352e0ba183d5f066d31a5cbc7b 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/loaders/load_lora.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=d79c7730598eae59e4e426ba148cc305 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/loaders/load_lora.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=3d668b9ec7f28887c32a5258c69e6bce 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/loaders/load_lora.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=1663b82a24c88b4dbfc910e98d3e1705 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/loaders/load_lora.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=cdbe392bf89945708e98a6bbc1dbc7e7 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/loaders/load_lora.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=f43a1ea6e81cb1b59d7ee7694ecf6e34 2500w" />

Models in the `ComfyUI\models\loras` folder will be detected by ComfyUI and can be loaded using this node.

### Input Types

| Parameter Name   | Function                                                                                               |
| ---------------- | ------------------------------------------------------------------------------------------------------ |
| `model`          | Connect to the base model                                                                              |
| `clip`           | Connect to the CLIP model                                                                              |
| `lora_name`      | Select the LoRA model to load and use                                                                  |
| `strength_model` | Affects how strongly the LoRA influences the model weights; higher values make the LoRA style stronger |
| `strength_clip`  | Affects how strongly the LoRA influences the CLIP text embeddings                                      |

### Output Types

| Parameter Name | Function                                             |
| -------------- | ---------------------------------------------------- |
| `model`        | Outputs the model with LoRA adjustments applied      |
| `clip`         | Outputs the CLIP model with LoRA adjustments applied |

This node supports chain connections, allowing multiple `Load LoRA` nodes to be linked in series to apply multiple LoRA models. For more details, please refer to [ComfyUI Multiple LoRAs Example](/tutorials/basic/multiple-loras)

<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/lora/chain_link.png?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=905ee3b2883bf67bd986989f57f62caf" alt="LoRA Node Chain Connection" data-og-width="1200" width="1200" data-og-height="380" height="380" data-path="images/tutorial/basic/lora/chain_link.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/lora/chain_link.png?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=d58794719ae40d147a4de000c8c1c7b3 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/lora/chain_link.png?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=03d1653a50204f3708deaddb5415b072 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/lora/chain_link.png?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=aa5a208389e3a0df9c0530685c6dbcb3 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/lora/chain_link.png?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=18b1343ad60feae04be40cef047226c3 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/lora/chain_link.png?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=c4d1622631b43b2605038b768c6f079e 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/lora/chain_link.png?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=0f9cbc8e9ac1682794e5f3ad07909a77 2500w" />

## Try It Yourself

1. Try modifying the prompt or adjusting different parameters of the `Load LoRA` node, such as `strength_model`, to observe changes in the generated images and become familiar with the `Load LoRA` node.
2. Visit [CivitAI](https://civitai.com/models) to download other kinds of LoRA models and try using them.


# ComfyUI Multiple LoRAs Example
Source: https://docs.comfy.org/tutorials/basic/multiple-loras

This guide demonstrates how to apply multiple LoRA models simultaneously in ComfyUI

In our [ComfyUI LoRA Example](/tutorials/basic/lora), we introduced how to load and use a single LoRA model, mentioning the node's chain connection capability.

<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/lora/chain_link.png?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=905ee3b2883bf67bd986989f57f62caf" alt="LoRA Node Chaining" data-og-width="1200" width="1200" data-og-height="380" height="380" data-path="images/tutorial/basic/lora/chain_link.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/lora/chain_link.png?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=d58794719ae40d147a4de000c8c1c7b3 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/lora/chain_link.png?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=03d1653a50204f3708deaddb5415b072 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/lora/chain_link.png?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=aa5a208389e3a0df9c0530685c6dbcb3 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/lora/chain_link.png?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=18b1343ad60feae04be40cef047226c3 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/lora/chain_link.png?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=c4d1622631b43b2605038b768c6f079e 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/lora/chain_link.png?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=0f9cbc8e9ac1682794e5f3ad07909a77 2500w" />

This tutorial demonstrates chaining multiple `Load LoRA` nodes to apply two LoRA models simultaneously: [blindbox\_V1Mix](https://civitai.com/models/25995?modelVersionId=32988) and [MoXinV1](https://civitai.com/models/12597?modelVersionId=14856).

The comparison below shows individual effects of these LoRAs using identical parameters:

<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/multiple_loras/compare.png?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=3f26ac1b06c82aa86bf13062a1409131" alt="Single LoRA Effects Comparison" data-og-width="1356" width="1356" data-og-height="678" height="678" data-path="images/tutorial/basic/multiple_loras/compare.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/multiple_loras/compare.png?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=2c29812bb695bf0ac7edf15d76a72b79 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/multiple_loras/compare.png?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=a2486af00654f43751ef97379ed4f78b 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/multiple_loras/compare.png?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=eddf667237902ddb6b3d74d063c5c987 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/multiple_loras/compare.png?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=6835d257153bca9cc73aa9ec996cbf35 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/multiple_loras/compare.png?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=f5f9e8f718ae270ee8dbfb998156a78e 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/multiple_loras/compare.png?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=3cf3180ecee417189333079029586368 2500w" />

By chaining multiple LoRA models, we achieve a blended style in the final output:

<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/multiple_loras/multiple_loras.png?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=f96d5ad3564a7ce69c8acc643eab0776" alt="Multi-LoRA Application Result" data-og-width="1024" width="1024" data-og-height="1024" height="1024" data-path="images/tutorial/basic/multiple_loras/multiple_loras.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/multiple_loras/multiple_loras.png?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=536b5dd5ce9cba96ff07b37a1f33c75a 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/multiple_loras/multiple_loras.png?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=123cc3e5e91e6bbbae18631b7388e9c8 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/multiple_loras/multiple_loras.png?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=6248fc473795103b532c72c06d6328a8 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/multiple_loras/multiple_loras.png?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=b3192a7e184f81c53bdaf11dc31c15d1 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/multiple_loras/multiple_loras.png?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=e7122d3180185bafa0f5ce0ab4941f47 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/multiple_loras/multiple_loras.png?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=8a023ec9896266a15c4c5ed2f3f34826 2500w" />

## Model Installation

Download the [dreamshaper\_8.safetensors](https://civitai.com/api/download/models/128713?type=Model\&format=SafeTensor\&size=pruned\&fp=fp16) file and put it in your `ComfyUI/models/checkpoints` folder.

Download the [blindbox\_V1Mix.safetensors](https://civitai.com/api/download/models/32988?type=Model\&format=SafeTensor\&size=full\&fp=fp16) file and put it in your `ComfyUI/models/loras` folder.

Download the [MoXinV1.safetensors](https://civitai.com/api/download/models/14856?type=Model\&format=SafeTensor\&size=full\&fp=fp16) file and put it in your `ComfyUI/models/loras` folder.

## Multi-LoRA Workflow

Download the image below and **drag it into ComfyUI** to load the workflow:
![ComfyUI Workflow - Multiple LoRAs](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/multiple_loras.png)

<Tip>
  Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
</Tip>

## Complete the Workflow Step by Step

Follow the steps in the diagram below to ensure the workflow runs correctly.

<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/multiple_loras/flow_diagram.png?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=daa5ef546221542dfe0b2ec8b408f689" alt="Multi-LoRA Workflow Diagram" data-og-width="2000" width="2000" data-og-height="632" height="632" data-path="images/tutorial/basic/multiple_loras/flow_diagram.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/multiple_loras/flow_diagram.png?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=966d31193e61260c54c8f14e85aa4717 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/multiple_loras/flow_diagram.png?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=e750bd33f8db493a6bd4858655b607df 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/multiple_loras/flow_diagram.png?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=24074167ebd91ac8c5b596fc77a984a5 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/multiple_loras/flow_diagram.png?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=b17f140123654a59594acda7fcbd7572 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/multiple_loras/flow_diagram.png?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=388e5159a3bb58a446572df3c1ecff29 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/multiple_loras/flow_diagram.png?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=2f8ef1fd667897efd420461df9e68c4e 2500w" />

1. Ensure `Load Checkpoint` loads  **dreamshaper\_8.safetensors**
2. Ensure first `Load LoRA` loads **blindbox\_V1Mix.safetensors**
3. Ensure second `Load LoRA` loads **MoXinV1.safetensors**
4. Click `Queue` or press `Ctrl/Cmd + Enter` to generate

## Try It Yourself

1. Adjust `strength_model` values in both `Load LoRA` nodes to control each LoRA's influence
2. Explore [CivitAI](https://civitai.com/models) for additional LoRAs and create custom combinations


# ComfyUI Outpainting Workflow Example
Source: https://docs.comfy.org/tutorials/basic/outpaint

This guide will introduce you to the outpainting workflow in ComfyUI and walk you through an outpainting example

This guide will introduce you to the concept of outpainting in AI image generation and how to create an outpainting workflow in ComfyUI. We will cover:

* Using outpainting workflow to extend an image
* Understanding and using outpainting-related nodes in ComfyUI
* Mastering the basic outpainting process

## About Outpainting

In AI image generation, we often encounter situations where an existing image has good composition but the canvas area is too small, requiring us to extend the canvas to get a larger scene. This is where outpainting comes in.

Basically, it requires similar content to [Inpainting](/tutorials/basic/inpaint), but we use different nodes to **build the mask**.

Outpainting applications include:

* **Scene Extension:** Expand the scene range of the original image to show a more complete environment
* **Composition Adjustment:** Optimize the overall composition by extending the canvas
* **Content Addition:** Add more related scene elements to the original image

## ComfyUI Outpainting Workflow Example Explanation

### Preparation

#### 1. Model Installation

Download the following model file and save it to `ComfyUI/models/checkpoints` directory:

* [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors)

#### 2. Input Image

Prepare an image you want to extend. In this example, we will use the following image:

<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/outpaint/input.png?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=70f072d92be155e740f8763e0265240f" alt="ComfyUI Outpainting Input Image" data-og-width="512" width="512" data-og-height="512" height="512" data-path="images/tutorial/basic/outpaint/input.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/outpaint/input.png?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=92d887b2159264b61ff97551fa3a0103 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/outpaint/input.png?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=4f4d454456ab5017e926209f958140c6 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/outpaint/input.png?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=3dbb31383ffba50c728506ca5fa52fe4 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/outpaint/input.png?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=f0b35b7011f4ab4bf79adfba40bfc172 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/outpaint/input.png?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=9b22ac3f4d4bcc2ce494612fdbd16990 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/outpaint/input.png?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=0660b6dcbdcf5d09d4e4f59390b9e639 2500w" />

#### 3. Outpainting Workflow

Download the image below and **drag it into ComfyUI** to load the workflow:

![ComfyUI Outpainting Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/image/basic/outpaint.png)

<Tip>
  Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
</Tip>

### Outpainting Workflow Usage Explanation

<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/outpaint/outpainting_workflow.jpg?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=b56842f634fedc85c66bccd0ead52bb6" alt="ComfyUI Outpainting Workflow Diagram" data-og-width="1818" width="1818" data-og-height="1160" height="1160" data-path="images/tutorial/basic/outpaint/outpainting_workflow.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/outpaint/outpainting_workflow.jpg?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=2d2fedc029bd619540fb6adcabc415c0 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/outpaint/outpainting_workflow.jpg?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=454f1af3352dfe61d228fea298b902e3 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/outpaint/outpainting_workflow.jpg?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=bd27f876157ca9c98c2c92065b4e15b2 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/outpaint/outpainting_workflow.jpg?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=1521fb04beb2fb50363dbb73ad117d22 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/outpaint/outpainting_workflow.jpg?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=0be5decddd0c7f26d3a0198090ec5978 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/outpaint/outpainting_workflow.jpg?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=b99fa7a0d0db884d881f3c2c9187936c 2500w" />

The key steps of the outpainting workflow are as follows:

1. Load the locally installed model file in the `Load Checkpoint` node
2. Click the `Upload` button in the `Load Image` node to upload your image
3. Click the `Queue` button or use the shortcut `Ctrl + Enter` to execute the image generation

In this workflow, we mainly use the `Pad Image for outpainting` node to control the direction and range of image extension. This is actually an [Inpaint](/tutorials/basic/inpaint) workflow, but we use different nodes to build the mask.

### Pad Image for outpainting Node

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/image/pad_image_for_outpainting.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=9a9eaf8b01a18692b50a684cbff5d8e1" alt="Pad Image for outpainting Node" data-og-width="852" width="852" data-og-height="570" height="570" data-path="images/comfy_core/image/pad_image_for_outpainting.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/image/pad_image_for_outpainting.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=e8eac3ee3ddf56488f7db2514850e050 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/image/pad_image_for_outpainting.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=c230cbc648eb8ee05ec65eff86c0e913 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/image/pad_image_for_outpainting.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=1998b2a7e14e5f5331b7c8156619782f 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/image/pad_image_for_outpainting.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=def26927601a05dc6e10c769e1b5827e 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/image/pad_image_for_outpainting.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=d091d198e5b0262df6919f666e62ac9b 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/image/pad_image_for_outpainting.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=cc14e0965caf245349ee55ec4579cd31 2500w" />

This node accepts an input image and outputs an extended image with a corresponding mask, where the mask is built based on the node parameters.

#### Input Parameters

| Parameter Name | Function                                                                                                                              |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------- |
| `image`        | Input image                                                                                                                           |
| `left`         | Left padding amount                                                                                                                   |
| `top`          | Top padding amount                                                                                                                    |
| `right`        | Right padding amount                                                                                                                  |
| `bottom`       | Bottom padding amount                                                                                                                 |
| `feathering`   | Controls the smoothness of the transition between the original image and the added padding, higher values create smoother transitions |

#### Output Parameters

| Parameter Name | Function                                                                   |
| -------------- | -------------------------------------------------------------------------- |
| `image`        | Output `image` represents the padded image                                 |
| `mask`         | Output `mask` indicates the original image area and the added padding area |

#### Node Output Content

After processing by the `Pad Image for outpainting` node, the output image and mask preview are as follows:

<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/outpaint/pad_Image_for_outpainting_result.jpg?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=cc8fd3e8159dca0e8abf9def1bfe6d39" alt="Pad Image for outpainting Node Results" data-og-width="1600" width="1600" data-og-height="798" height="798" data-path="images/tutorial/basic/outpaint/pad_Image_for_outpainting_result.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/outpaint/pad_Image_for_outpainting_result.jpg?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=c818dadb5ede0cc54de33eb23f095474 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/outpaint/pad_Image_for_outpainting_result.jpg?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=35588369ee8d526a7d9583e559cddd2d 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/outpaint/pad_Image_for_outpainting_result.jpg?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=3a772936215ac2c9975c902a01d3b354 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/outpaint/pad_Image_for_outpainting_result.jpg?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=c81e37842480fa81ab070d0dcfc24ea4 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/outpaint/pad_Image_for_outpainting_result.jpg?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=6bf66f3539d9127bf68c64a1dc5997cf 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/outpaint/pad_Image_for_outpainting_result.jpg?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=e6aaaf090bcdda387b8fc9f289f5d7cf 2500w" />

You can see the corresponding output results:

* The `Image` output is the extended image
* The `Mask` output is the mask marking the extension areas


# ComfyUI Text to Image Workflow
Source: https://docs.comfy.org/tutorials/basic/text-to-image

This guide will help you understand the concept of text-to-image in AI art generation and complete a text-to-image workflow in ComfyUI

This guide aims to introduce you to ComfyUI's text-to-image workflow and help you understand the functionality and usage of various ComfyUI nodes.

In this document, we will:

* Complete a text-to-image workflow
* Gain a basic understanding of diffusion model principles
* Learn about the functions and roles of workflow nodes
* Get an initial understanding of the SD1.5 model

We'll start by running a text-to-image workflow, followed by explanations of related concepts. Please choose the relevant sections based on your needs.

## About Text to Image

**Text to Image** is a fundamental process in AI art generation that creates images from text descriptions, with **diffusion models** at its core.

The text-to-image process requires the following elements:

* **Artist:** The image generation model
* **Canvas:** The latent space
* **Image Requirements (Prompts):** Including positive prompts (elements you want in the image) and negative prompts (elements you don't want)

This text-to-image generation process can be simply understood as telling your requirements (positive and negative prompts) to an **artist (the image model)**, who then creates what you want based on these requirements.

## ComfyUI Text to Image Workflow Example Guide

### 1. Preparation

Ensure you have at least one SD1.5 model file in your `ComfyUI/models/checkpoints` folder, such as [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors)

If you haven't installed it yet, please refer to the model installation section in [Getting Started with ComfyUI AI Art Generation](/get_started/first_generation).

### 2. Loading the Text to Image Workflow

Download the image below and **drag it into ComfyUI** to load the workflow:

<img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/text-to-image-workflow.png" alt="ComfyUI-Text to Image Workflow" />

<Tip>
  Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
</Tip>

### 3. Loading the Model and Generating Your First Image

After installing the image model, follow the steps in the image below to load the model and generate your first image

![Image Generation](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/image/basic/text-to-image-workflow.png)

Follow these steps according to the image numbers:

1. In the **Load Checkpoint** node, use the arrows or click the text area to ensure **v1-5-pruned-emaonly-fp16.safetensors** is selected, and the left/right arrows don't show **null** text
2. Click the `Queue` button or use the shortcut `Ctrl + Enter` to execute image generation

After the process completes, you should see the resulting image in the **Save Image** node interface, which you can right-click to save locally

<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-8-result.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=92b100190dd8f91abb37b01cc2e09a59" alt="ComfyUI First Image Generation Result" data-og-width="2642" width="2642" data-og-height="1390" height="1390" data-path="images/tutorial/gettingstarted/first-image-generation-8-result.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-8-result.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=f0c2219d45e70c2f21ebd64032fddb50 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-8-result.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=9391713ee886380f8302e396975a94aa 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-8-result.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=ace21872a174725c45788b7ce5ce8a34 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-8-result.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=a579037cd0b65cd2115280f8eee6e80b 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-8-result.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=8dba16d2de187f3f6356af67ee1040ef 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/gettingstarted/first-image-generation-8-result.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=adfc35ce5d546ca79f140b45dee3fe2d 2500w" />

<Tip>If you're not satisfied with the result, try running the generation multiple times. Each time you run it, **KSampler** will use a different random seed based on the `seed` parameter, so each generation will produce different results</Tip>

### 4. Start Experimenting

Try modifying the text in the **CLIP Text Encoder**

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/conditioning/clip_text_encode.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=94d83f96b628279b5b2fc865d522886e" alt="CLIP Text Encoder" data-og-width="778" width="778" data-og-height="477" height="477" data-path="images/comfy_core/conditioning/clip_text_encode.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/conditioning/clip_text_encode.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=1d5e4f945f83db77e339f0301ffb365c 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/conditioning/clip_text_encode.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=67671fa5fc37408cf9e2f315c8ff16f3 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/conditioning/clip_text_encode.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=f3cb23d3f4aba2f500b2d350f0a2a7c3 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/conditioning/clip_text_encode.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=a6fbf2d76a7722260d317152e5977c61 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/conditioning/clip_text_encode.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=ef744de37e931396fa5e6417730d7e9e 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/conditioning/clip_text_encode.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=1cd01f8ad444d9a4c2a18dae3cb6b4b1 2500w" />

The `Positive` connection to the KSampler node represents positive prompts, while the `Negative` connection represents negative prompts

Here are some basic prompting principles for the SD1.5 model:

* Use English whenever possible
* Separate prompts with English commas `,`
* Use phrases rather than long sentences
* Use specific descriptions
* Use expressions like `(golden hour:1.2)` to increase the weight of specific keywords, making them more likely to appear in the image. `1.2` is the weight, `golden hour` is the keyword
* Use keywords like `masterpiece, best quality, 4k` to improve generation quality

Here are several prompt examples you can try, or use your own prompts for generation:

**1. Anime Style**

Positive prompts:

```
anime style, 1girl with long pink hair, cherry blossom background, studio ghibli aesthetic, soft lighting, intricate details

masterpiece, best quality, 4k
```

Negative prompts:

```
low quality, blurry, deformed hands, extra fingers
```

**2. Realistic Style**

Positive prompts:

```
(ultra realistic portrait:1.3), (elegant woman in crimson silk dress:1.2), 
full body, soft cinematic lighting, (golden hour:1.2), 
(fujifilm XT4:1.1), shallow depth of field, 
(skin texture details:1.3), (film grain:1.1), 
gentle wind flow, warm color grading, (perfect facial symmetry:1.3)
```

Negative prompts:

```
(deformed, cartoon, anime, doll, plastic skin, overexposed, blurry, extra fingers)
```

**3. Specific Artist Style**

Positive prompts:

```
fantasy elf, detailed character, glowing magic, vibrant colors, long flowing hair, elegant armor, ethereal beauty, mystical forest, magical aura, high detail, soft lighting, fantasy portrait, Artgerm style
```

Negative prompts:

```
blurry, low detail, cartoonish, unrealistic anatomy, out of focus, cluttered, flat lighting
```

## Text to Image Working Principles

The entire text-to-image process can be understood as a **reverse diffusion process**. The [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) we downloaded is a pre-trained model that can **generate target images from pure Gaussian noise**. We only need to input our prompts, and it can generate target images through denoising random noise.

```mermaid  theme={null}
graph LR
A[Pure Gaussian Noise] --> B[Iterative Denoising]
B --> C[Intermediate Latents]
C --> D[Final Generated Image]
E[Text Prompts] --> F[CLIP Encoder]
F --> G[Semantic Vectors]
G --> B
```

We need to understand two concepts:

1. **Latent Space:** Latent Space is an abstract data representation method in diffusion models. Converting images from pixel space to latent space reduces storage space and makes it easier to train diffusion models and reduce denoising complexity. It's like architects using blueprints (latent space) for design rather than designing directly on the building (pixel space), maintaining structural features while significantly reducing modification costs
2. **Pixel Space:** Pixel Space is the storage space for images, which is the final image we see, used to store pixel values.

If you want to learn more about diffusion models, you can read these papers:

* [Denoising Diffusion Probabilistic Models (DDPM)](https://arxiv.org/pdf/2006.11239)
* [Denoising Diffusion Implicit Models (DDIM)](https://arxiv.org/pdf/2010.02502)
* [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/pdf/2112.10752)

## ComfyUI Text to Image Workflow Node Explanation

<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/text-image-workflow.jpg?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=3e22321faba12644c781cecc5746a71e" alt="ComfyUI Text to Image Workflow Explanation" data-og-width="2640" width="2640" data-og-height="1384" height="1384" data-path="images/tutorial/basic/text-image-workflow.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/text-image-workflow.jpg?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=00f7db5a22d5a7d186adee653c14761c 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/text-image-workflow.jpg?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=7fb01ec47ce2e701c513881a9e3f1e5b 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/text-image-workflow.jpg?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=25e071ca07a29643537ea8bdb3893cb3 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/text-image-workflow.jpg?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=4fed435b700cfe00931c32027881b801 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/text-image-workflow.jpg?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=9c6ccc91e4d28cad1700033497cd6dfd 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/text-image-workflow.jpg?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=5d3f62df6ef74b998814b604721c465f 2500w" />

### A. Load Checkpoint Node

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/loaders/load_checkpoint.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=1a046abe89763dd49e42f2447974a1f8" alt="Load Checkpoint" data-og-width="807" width="807" data-og-height="384" height="384" data-path="images/comfy_core/loaders/load_checkpoint.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/loaders/load_checkpoint.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=24e18a1cdc77c62189fc29c677e7165e 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/loaders/load_checkpoint.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=dd577f103a62c2124ca61cde280cbe4e 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/loaders/load_checkpoint.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=3bdc09d0944ef1c60230e73a742c8621 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/loaders/load_checkpoint.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=7861410e82e36ff7dd08b5c3cbb842a6 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/loaders/load_checkpoint.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=8824a755aba210b2a804502b0d49f907 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/loaders/load_checkpoint.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=86d741df610277979074c4353306c907 2500w" />

This node is typically used to load the image generation model. A `checkpoint` usually contains three components: `MODEL (UNet)`, `CLIP`, and `VAE`

* `MODEL (UNet)`: The UNet model responsible for noise prediction and image generation during the diffusion process
* `CLIP`: The text encoder that converts our text prompts into vectors that the model can understand, as the model cannot directly understand text prompts
* `VAE`: The Variational AutoEncoder that converts images between pixel space and latent space, as diffusion models work in latent space while our images are in pixel space

### B. Empty Latent Image Node

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/latent/empty_latent_image.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=e40da4b9e1953fa2488277c47a310187" alt="Empty Latent Image" data-og-width="855" width="855" data-og-height="420" height="420" data-path="images/comfy_core/latent/empty_latent_image.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/latent/empty_latent_image.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=f598f452e7fcd96ced27d409aa37f86a 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/latent/empty_latent_image.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=b348ec3897d361910c46d1451782cc77 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/latent/empty_latent_image.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=367ddd1429f0a9d5cade86c337ca7f27 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/latent/empty_latent_image.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=ae1d7f0228de9d381974bdd4264aceb2 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/latent/empty_latent_image.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=4bd95609794ead59eb95569f6442bb47 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/latent/empty_latent_image.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=53285a3dfa681f7680a2a907707b176f 2500w" />

Defines a latent space that outputs to the KSampler node. The Empty Latent Image node constructs a **pure noise latent space**

You can think of its function as defining the canvas size, which determines the dimensions of our final generated image

### C. CLIP Text Encoder Node

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/conditioning/clip_text_encode.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=94d83f96b628279b5b2fc865d522886e" alt="CLIP Text Encoder" data-og-width="778" width="778" data-og-height="477" height="477" data-path="images/comfy_core/conditioning/clip_text_encode.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/conditioning/clip_text_encode.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=1d5e4f945f83db77e339f0301ffb365c 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/conditioning/clip_text_encode.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=67671fa5fc37408cf9e2f315c8ff16f3 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/conditioning/clip_text_encode.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=f3cb23d3f4aba2f500b2d350f0a2a7c3 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/conditioning/clip_text_encode.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=a6fbf2d76a7722260d317152e5977c61 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/conditioning/clip_text_encode.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=ef744de37e931396fa5e6417730d7e9e 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/conditioning/clip_text_encode.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=1cd01f8ad444d9a4c2a18dae3cb6b4b1 2500w" />

Used to encode prompts, which are your requirements for the image

* The `Positive` condition input connected to the KSampler node represents positive prompts (elements you want in the image)
* The `Negative` condition input connected to the KSampler node represents negative prompts (elements you don't want in the image)

The prompts are encoded into semantic vectors by the `CLIP` component from the `Load Checkpoint` node and output as conditions to the KSampler node

### D. KSampler Node

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/sampling/k_sampler.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=aed5e1f863c37948107cfcb3458955b7" alt="KSampler" data-og-width="854" width="854" data-og-height="767" height="767" data-path="images/comfy_core/sampling/k_sampler.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/sampling/k_sampler.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=f891fb24b7fcbb6616bad811d797cd10 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/sampling/k_sampler.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=d3597cfd9ef1b324ceb8b8e529246540 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/sampling/k_sampler.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=e714d81c3a3214ef9fe1da89cdc2efd3 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/sampling/k_sampler.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=5001918af1c739866639c03df51a3c19 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/sampling/k_sampler.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=ff62cab54c0c1fa1432d98c69877706d 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/sampling/k_sampler.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=350b8494ce4c1e004ddd94f7977563d7 2500w" />

The **KSampler** is the core of the entire workflow, where the entire noise denoising process occurs, ultimately outputting a latent space image

```mermaid  theme={null}
graph LR
A[Diffusion Model] --> B{KSampler}
C[Random Noise<br>Latent Space] --> B
D[CLIP Semantic Vectors] --> B
B --> E[Denoised Latent]
```

Here's an explanation of the KSampler node parameters:

| Parameter Name               | Description                        | Function                                                                                                    |
| ---------------------------- | ---------------------------------- | ----------------------------------------------------------------------------------------------------------- |
| **model**                    | Diffusion model used for denoising | Determines the style and quality of generated images                                                        |
| **positive**                 | Positive prompt condition encoding | Guides generation to include specified elements                                                             |
| **negative**                 | Negative prompt condition encoding | Suppresses unwanted content                                                                                 |
| **latent\_image**            | Latent space image to be denoised  | Serves as the input carrier for noise initialization                                                        |
| **seed**                     | Random seed for noise generation   | Controls generation randomness                                                                              |
| **control\_after\_generate** | Seed control mode after generation | Determines seed variation pattern in batch generation                                                       |
| **steps**                    | Number of denoising iterations     | More steps mean finer details but longer processing time                                                    |
| **cfg**                      | Classifier-free guidance scale     | Controls prompt constraint strength (too high leads to overfitting)                                         |
| **sampler\_name**            | Sampling algorithm name            | Determines the mathematical method for denoising path                                                       |
| **scheduler**                | Scheduler type                     | Controls noise decay rate and step size allocation                                                          |
| **denoise**                  | Denoising strength coefficient     | Controls noise strength added to latent space, 0.0 preserves original input features, 1.0 is complete noise |

In the KSampler node, the latent space uses `seed` as an initialization parameter to construct random noise, and semantic vectors `Positive` and `Negative` are input as conditions to the diffusion model

Then, based on the number of denoising steps specified by the `steps` parameter, denoising is performed. Each denoising step uses the denoising strength coefficient specified by the `denoise` parameter to denoise the latent space and generate a new latent space image

### E. VAE Decode Node

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/latent/vae_decode.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=a9c0a75f0e442062a837289c5639a205" alt="VAE Decode" data-og-width="854" width="854" data-og-height="370" height="370" data-path="images/comfy_core/latent/vae_decode.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/latent/vae_decode.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=2f0afe5cd50c2c9ca8b99a19b72e7add 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/latent/vae_decode.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=08f5dc2c949c7b17285c0bf5c1727d58 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/latent/vae_decode.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=49bf1587c6eefcb9834168b240a50beb 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/latent/vae_decode.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=cbf29d240a132456bbdf3a1a397b4a15 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/latent/vae_decode.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=bd71062051c89305967dbbef0ec36d08 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/latent/vae_decode.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=d07f2b9bfb33486727f2ad936277eabd 2500w" />

Converts the latent space image output from the **KSampler** into a pixel space image

### F. Save Image Node

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/image/save_image.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=986a4163337dce0354c90b4bfb4471b2" alt="Save Image" data-og-width="854" width="854" data-og-height="410" height="410" data-path="images/comfy_core/image/save_image.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/image/save_image.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=ca1af2cc43f687bf101878f2f20c67b8 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/image/save_image.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=5928edfb970bf83050ba0c2f37756bac 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/image/save_image.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=4f06fdc7b9d3452aa2ee19621c014c09 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/image/save_image.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=b2c95b4fff95a5c73c77db8ad148b947 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/image/save_image.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=fcab88dcda0bd79518bbafc0a4e791b8 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/image/save_image.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=3a1d97fbb025ba4129567ed912cc2f84 2500w" />

Previews and saves the decoded image from latent space to the local `ComfyUI/output` folder

## Introduction to SD1.5 Model

**SD1.5 (Stable Diffusion 1.5)** is an AI image generation model developed by [Stability AI](https://stability.ai/). It's the foundational version of the Stable Diffusion series, trained on **512512** resolution images, making it particularly good at generating images at this resolution. With a size of about 4GB, it runs smoothly on **consumer-grade GPUs (e.g., 6GB VRAM)**. Currently, SD1.5 has a rich ecosystem, supporting various plugins (like ControlNet, LoRA) and optimization tools.
As a milestone model in AI art generation, SD1.5 remains the best entry-level choice thanks to its open-source nature, lightweight architecture, and rich ecosystem. Although newer versions like SDXL/SD3 have been released, its value for consumer-grade hardware remains unmatched.

### Basic Information

* **Release Date**: October 2022
* **Core Architecture**: Based on Latent Diffusion Model (LDM)
* **Training Data**: LAION-Aesthetics v2.5 dataset (approximately 590M training steps)
* **Open Source Features**: Fully open-source model/code/training data

### Advantages and Limitations

Model Advantages:

* Lightweight: Small size, only about 4GB, runs smoothly on consumer GPUs
* Low Entry Barrier: Supports a wide range of plugins and optimization tools
* Mature Ecosystem: Extensive plugin and tool support
* Fast Generation: Smooth operation on consumer GPUs

Model Limitations:

* Detail Handling: Hands/complex lighting prone to distortion
* Resolution Limits: Quality degrades for direct 1024x1024 generation
* Prompt Dependency: Requires precise English descriptions for control


# ComfyUI Image Upscale Workflow
Source: https://docs.comfy.org/tutorials/basic/upscale

This guide explains the concept of image upscaling in AI drawing and demonstrates how to implement an image upscaling workflow in ComfyUI

## What is Image Upscaling?

Image Upscaling is the process of converting low-resolution images to high-resolution using algorithms.
Unlike traditional interpolation methods, AI upscaling models (like ESRGAN) can intelligently reconstruct details while maintaining image quality.

For instance, the default SD1.5 model often struggles with large-size image generation.
To achieve high-resolution results,we typically generate smaller images first and then use upscaling techniques.

This article covers one of many upscaling methods in ComfyUI. In this tutorial, we'll guide you through:

1. Downloading and installing upscaling models
2. Performing basic image upscaling
3. Combining text-to-image workflows with upscaling

## Upscaling Workflow

### Model Installation

Required ESRGAN models download:

<Steps>
  <Step title="Visit OpenModelDB">
    Visit [OpenModelDB](https://openmodeldb.info/) to search and download upscaling models (e.g., RealESRGAN)

        <img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/upscale_OpenModelDB.jpg?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=e5a63a5efe9b72af78d7331179cef6ba" alt="openmodeldb" data-og-width="1200" width="1200" data-og-height="1209" height="1209" data-path="images/tutorial/basic/upscale/upscale_OpenModelDB.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/upscale_OpenModelDB.jpg?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=ed2e223f6fdf3841a3268e3925c214ca 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/upscale_OpenModelDB.jpg?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=e5a012d3e1827c0033d192b40e2657a7 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/upscale_OpenModelDB.jpg?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=f0a8e3b96e3812e0bc4382fe006e04ff 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/upscale_OpenModelDB.jpg?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=f04936c9b6c0215a784cb29d11e68b3d 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/upscale_OpenModelDB.jpg?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=b9220bb51a7b9f2fc281b3ec66a6777a 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/upscale_OpenModelDB.jpg?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=afc0f1083e090bf02b462353ff7f3f64 2500w" />

    As shown:

    1. Filter models by image type using the category selector
    2. The model's magnification factor is indicated in the top-right corner (e.g., 2x in the screenshot)

    We'll use the [4x-ESRGAN](https://openmodeldb.info/models/4x-ESRGAN) model for this tutorial. Click the `Download` button on the model detail page.

        <img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/upscale_OpenModelDB_download.jpg?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=678b9869dfcf612056bdc38fcec2f8f4" alt="OpenModelDB_download" data-og-width="1200" width="1200" data-og-height="848" height="848" data-path="images/tutorial/basic/upscale/upscale_OpenModelDB_download.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/upscale_OpenModelDB_download.jpg?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=273b61c09d981babb85968599141f7d6 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/upscale_OpenModelDB_download.jpg?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=9866ff347c00df5b0db446a5edcd8ae3 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/upscale_OpenModelDB_download.jpg?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=60b259f6fb7173ba85796c8b863ce144 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/upscale_OpenModelDB_download.jpg?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=5cef836154d2c860ea2dc76490cc22d7 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/upscale_OpenModelDB_download.jpg?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=065f7e6200a9087c87f66257c677357e 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/upscale_OpenModelDB_download.jpg?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=729c1efb8a117ebbd815b0a91e479e2a 2500w" />
  </Step>

  <Step title="Save Model Files in Directory">
    Save the model file (.pth) in `ComfyUI/models/upscale_models` directory
  </Step>
</Steps>

### Workflow and Assets

Download and drag the following image into ComfyUI to load the basic upscaling workflow:
![Upscale workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/image/basic/upscale_workflow.png)

<Tip>
  Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
</Tip>

Use this image in smaller size as input:
<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/upscale-input.jpg?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=ce8c5f3cd94c0cbde5ded50f09204d5b" alt="Upscale-input" data-og-width="512" width="512" data-og-height="512" height="512" data-path="images/tutorial/basic/upscale/upscale-input.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/upscale-input.jpg?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=1babcd7fcdcb1f3b2cc953c79f237753 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/upscale-input.jpg?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=6d8772a8c139cb4aa8f8554855552b1e 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/upscale-input.jpg?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=a331ba6e2e35bef944d672990a9f39ce 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/upscale-input.jpg?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=af8cd385a69e8c69d4a9166cfb0056c9 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/upscale-input.jpg?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=2c79e1219e9af3bed8c72cc5a79caca6 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/upscale-input.jpg?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=6ec8647f560daf3c2fe61670edc8fbca 2500w" />

### Complete the Workflow Step by Step

Follow the steps in the diagram below to ensure the workflow runs correctly.

<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/upscale_simple_workflow.jpg?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=632d1202982e8e7c59974a4152304967" alt="Upscale workflow" data-og-width="2136" width="2136" data-og-height="1192" height="1192" data-path="images/tutorial/basic/upscale/upscale_simple_workflow.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/upscale_simple_workflow.jpg?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=90de763eac7f4f322269592af72d6328 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/upscale_simple_workflow.jpg?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=edd7e70f7b9f80178b08b3984b7c3eec 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/upscale_simple_workflow.jpg?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=3c3000421314917ba8de3b74b754b15d 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/upscale_simple_workflow.jpg?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=6f53afad6d2ed56c155aa57d5ea9083e 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/upscale_simple_workflow.jpg?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=252e00d8d5f0b8f48554c4f9bf3a8b5b 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/upscale_simple_workflow.jpg?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=f48238be1a23ac57b5f766cdadf8dcd2 2500w" />

1. Ensure `Load Upscale Model` loads `4x-ESRGAN.pth`
2. Upload the input image to the `Load Image` node
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to generate the image

The core components are the `Load Upscale Model` and `Upscale Image (Using Model)` nodes, which receive an image input and upscale it using the selected model.

## Text-to-Image Combined Workflow

After mastering basic upscaling, we can combine it with the [text-to-image](/tutorials/basic/text-to-image) workflow. For text-to-image basics, refer to the [text-to-image tutorial](/tutorials/basic/text-to-image).

Download and drag this image into ComfyUI to load the combined workflow:
<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/esrgan_example.png?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=98c8d536f0a76025092f8fec157002f8" alt="Text-to-image upscale workflow" data-og-width="2533" width="2533" data-og-height="941" height="941" data-path="images/tutorial/basic/upscale/esrgan_example.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/esrgan_example.png?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=13fc9ef8db862b4c5b60289c5fe256ef 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/esrgan_example.png?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=4b8d819e10d503a630e98b092944c1f5 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/esrgan_example.png?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=b2dff912bcc2413b84f9d43fe88c1093 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/esrgan_example.png?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=99dae3b712b3ad7b1b9cfa7d71db947d 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/esrgan_example.png?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=f128f8649d0fc1d8f9bbd6e36fb49a3e 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/basic/upscale/esrgan_example.png?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=6f130e45a1bcd061fe4e4bc21301e1dc 2500w" />

This workflow connects the text-to-image output image directly to the upscaling nodes for final processing.

## Additional Tips

<Tip>
  Model characteristics:

  * **RealESRGAN**: General-purpose upscaling
  * **BSRGAN**: Excels with text and sharp edges
  * **SwinIR**: Preserves natural textures, ideal for landscapes
</Tip>

1. **Chained Upscaling**: Combine multiple upscale nodes (e.g., 2x  4x) for ultra-high magnification
2. **Hybrid Workflow**: Connect upscale nodes after generation for "generate+enhance" pipelines
3. **Comparative Testing**: Different models perform better on specific image types - test multiple options


# ComfyUI ControlNet Usage Example
Source: https://docs.comfy.org/tutorials/controlnet/controlnet

This guide will introduce you to the basic concepts of ControlNet and demonstrate how to generate corresponding images in ComfyUI

Achieving precise control over image creation in AI image generation cannot be done with just one click.
It typically requires numerous generation attempts to produce a satisfactory image. However, the emergence of **ControlNet** has effectively addressed this challenge.

ControlNet is a conditional control generation model based on diffusion models (such as Stable Diffusion),
first proposed by [Lvmin Zhang](https://lllyasviel.github.io/) and Maneesh Agrawala et al. in 2023 in the paper [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543).

ControlNet models significantly enhance the controllability of image generation and the ability to reproduce details by introducing multimodal input conditions,
such as edge detection maps, depth maps, and pose keypoints.

These conditioning constraints make image generation more controllable, allowing multiple ControlNet models to be used simultaneously during the drawing process for better results.

Before ControlNet, we could only rely on the model to generate images repeatedly until we were satisfied with the results, which involved a lot of randomness.

<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/generated_with_random_seed.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=81acd50958fd1df511237b3ba90b1b06" alt="Images generated with random seeds in ComfyUI" data-og-width="1024" width="1024" data-og-height="1024" height="1024" data-path="images/tutorial/controlnet/generated_with_random_seed.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/generated_with_random_seed.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=ecea01007c3a2d375a0ea70846fcefa0 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/generated_with_random_seed.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=7225cef825a3c4ff365c741a360ddc0a 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/generated_with_random_seed.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=1bbe42f0369bfa8d9891671b41979534 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/generated_with_random_seed.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=eee86e1fe36b8fe048e43a510e531bcf 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/generated_with_random_seed.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=7d1a369b55b8fcfdd9b1b465e3346c53 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/generated_with_random_seed.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=5a64bd353fbadff7b85acc589e19f8ed 2500w" />

With the advent of ControlNet, we can control image generation by introducing additional conditions.
For example, we can use a simple sketch to guide the image generation process, producing images that closely align with our sketch.

<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/scribble_example.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=36c37d9cadfaabb3690404f8828143a7" alt="Sketch-controlled image generation in ComfyUI" data-og-width="1024" width="1024" data-og-height="512" height="512" data-path="images/tutorial/controlnet/scribble_example.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/scribble_example.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=c442f3baa2efb48d21d7b037ce97acf1 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/scribble_example.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=a024658d6d9270ce18185ccaf6fe820b 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/scribble_example.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=ab4ce66b96385a6f6c07346f39a67f4c 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/scribble_example.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=9a2e79c32dfcdb73342e36682e147520 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/scribble_example.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=c03cc09cfcef43cb12dd0d90d8d57026 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/scribble_example.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=c8ae34966713cae3564c467f7ab5e0fe 2500w" />

In this example, we will guide you through installing and using ControlNet models in [ComfyUI](https://github.com/comfyanonymous/ComfyUI), and complete a sketch-controlled image generation example.

![ComfyUI ControlNet Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/scribble_controlnet.png)

<Tip>
  The workflows for other types of ControlNet V1.1 models are similar to this example. You only need to select the appropriate model and upload the corresponding reference image based on your needs.
</Tip>

## ControlNet Image Preprocessing Information

Different types of ControlNet models typically require different types of reference images:

![Reference Images](https://github.com/Fannovel16/comfyui_controlnet_aux/blob/main/examples/CNAuxBanner.jpg?raw=true)

> Image source: [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

Since the current **Comfy Core** nodes do not include all types of **preprocessors**, in the actual examples in this documentation, we will provide pre-processed images.
However, in practical use, you may need to use custom nodes to preprocess images to meet the requirements of different ControlNet models. Below are some relevant custom nodes:

* [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)
* [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

## ComfyUI ControlNet Workflow Example Explanation

### 1. ControlNet Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:

![ComfyUI Workflow - ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/scribble_controlnet.png)

<Tip>
  Images with workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
  This image already includes download links for the corresponding models, and dragging it into ComfyUI will automatically prompt for downloads.
</Tip>

Please download the image below, which we will use as input:

![ComfyUI Sketch Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/scribble_input.png)

### 2. Manual Model Installation

<Note>
  If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:
</Note>

* [dreamCreationVirtual3DECommerce\_v10.safetensors](https://civitai.com/api/download/models/731340?type=Model\&format=SafeTensor\&size=full\&fp=fp16)
* [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)
* [control\_v11p\_sd15\_scribble\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors?download=true)

```
ComfyUI/
 models/
    checkpoints/
       dreamCreationVirtual3DECommerce_v10.safetensors
    vae/
       vae-ft-mse-840000-ema-pruned.safetensors
    controlnet/
        control_v11p_sd15_scribble_fp16.safetensors
```

<Note>
  In this example, you could also use the VAE model embedded in dreamCreationVirtual3DECommerce\_v10.safetensors, but we're following the model author's recommendation to use a separate VAE model.
</Note>

### 3. Step-by-Step Workflow Execution

<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_scribble.png?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=d26abbfd1993f28b88efcf6231246945" alt="ComfyUI Workflow - ControlNet Flow Diagram" data-og-width="2000" width="2000" data-og-height="1086" height="1086" data-path="images/tutorial/controlnet/flow_diagram_scribble.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_scribble.png?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=08969e36a1eda27b8d60fb77cb41afee 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_scribble.png?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=94b8fabbcd1f910e52bf9834d3ac5678 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_scribble.png?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=0e809d2f8d501e7e308408d46a6a8588 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_scribble.png?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=6fea2381caed335cac0b7073ef9366e0 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_scribble.png?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=2d8133f58fc8c11cacf2ed11e8c181bd 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_scribble.png?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=a1c8aec6bfe0316295dcf600b09cebe2 2500w" />

1. Ensure that `Load Checkpoint` can load **dreamCreationVirtual3DECommerce\_v10.safetensors**
2. Ensure that `Load VAE` can load **vae-ft-mse-840000-ema-pruned.safetensors**
3. Click `Upload` in the `Load Image` node to upload the input image provided earlier
4. Ensure that `Load ControlNet` can load **control\_v11p\_sd15\_scribble\_fp16.safetensors**
5. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## Related Node Explanations

### Load ControlNet Node Explanation

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/loaders/load_controlnet_model.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=c3a99d9bd08baaedb7a4d426cc930880" alt="load controlnet" data-og-width="807" width="807" data-og-height="294" height="294" data-path="images/comfy_core/loaders/load_controlnet_model.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/loaders/load_controlnet_model.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=1d65dc81220e679eb2f17860ffe92562 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/loaders/load_controlnet_model.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=3d499a7fab5e9656b3043440bb259906 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/loaders/load_controlnet_model.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=54654343fb221fc3ae377467f2c799a6 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/loaders/load_controlnet_model.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=e363dc10dfa46d6f2e26e94018ef47ba 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/loaders/load_controlnet_model.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=4c90104f34b30cc2e7943bc047639345 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/loaders/load_controlnet_model.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=eaa65efbc295ed4f2bd3a8885acc02d0 2500w" />

Models located in `ComfyUI\models\controlnet` will be detected by ComfyUI and can be loaded through this node.

### Apply ControlNet Node Explanation

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/conditioning/controlnet/apply_controlnet.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=de9e90df2dcfe5cc7020a8c9cdb40a65" alt="apply controlnet" data-og-width="778" width="778" data-og-height="547" height="547" data-path="images/comfy_core/conditioning/controlnet/apply_controlnet.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/conditioning/controlnet/apply_controlnet.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=d2589a80afb77a1dd0d7fa12ae34142c 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/conditioning/controlnet/apply_controlnet.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=25ed3a4735b06b267519ac4313bcbaca 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/conditioning/controlnet/apply_controlnet.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=0b18dfc1c8fdd26f07a00c4d5e422f16 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/conditioning/controlnet/apply_controlnet.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=984444ad3f286a57a48cad3e9d30cb59 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/conditioning/controlnet/apply_controlnet.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=adbb7c39e09866c7c85fc9bbb59c9ebc 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/conditioning/controlnet/apply_controlnet.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=c1257bb2026d9ef99421d5474e1dea23 2500w" />

This node accepts the ControlNet model loaded by `load controlnet` and generates corresponding control conditions based on the input image.

**Input Types**

| Parameter Name  | Function                                                                                                                                   |
| --------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |
| `positive`      | Positive conditioning                                                                                                                      |
| `negative`      | Negative conditioning                                                                                                                      |
| `control_net`   | The ControlNet model to be applied                                                                                                         |
| `image`         | Preprocessed image used as reference for ControlNet application                                                                            |
| `vae`           | VAE model input                                                                                                                            |
| `strength`      | Strength of ControlNet application; higher values increase ControlNet's influence on the generated image                                   |
| `start_percent` | Determines when to start applying ControlNet as a percentage; e.g., 0.2 means ControlNet guidance begins when 20% of diffusion is complete |
| `end_percent`   | Determines when to stop applying ControlNet as a percentage; e.g., 0.8 means ControlNet guidance stops when 80% of diffusion is complete   |

**Output Types**

| Parameter Name | Function                                           |
| -------------- | -------------------------------------------------- |
| `positive`     | Positive conditioning data processed by ControlNet |
| `negative`     | Negative conditioning data processed by ControlNet |

You can use chain connections to apply multiple ControlNet models, as shown in the image below. You can also refer to the [Mixing ControlNet Models](/tutorials/controlnet/mixing-controlnets) guide to learn more about combining multiple ControlNet models.
<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/controlnet/apply_controlnet_chain_link.jpg?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=6fc3e75414f05e6c62062b4b746dc855" alt="apply controlnet chain link" data-og-width="1500" width="1500" data-og-height="1050" height="1050" data-path="images/tutorial/controlnet/apply_controlnet_chain_link.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/controlnet/apply_controlnet_chain_link.jpg?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=da6928840e7317cf4c0cc56cddd3bef9 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/controlnet/apply_controlnet_chain_link.jpg?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=be2b48a1a8c3f852ffbc1e0e7fa06f54 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/controlnet/apply_controlnet_chain_link.jpg?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=8488372f56c5d9e7a4f95574094bcd25 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/controlnet/apply_controlnet_chain_link.jpg?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=3893bbb66fd156f76713a5d51623cd12 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/controlnet/apply_controlnet_chain_link.jpg?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=aab42fb6856dc99be46f00bb333e8a6f 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/controlnet/apply_controlnet_chain_link.jpg?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=c8dee2a84c69fc57166788cae6387f17 2500w" />

<Note>
  You might see the `Apply ControlNet(Old)` node in some early workflows, which is an early version of the ControlNet node. It is currently deprecated and not visible by default in search and node lists.
  <img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/conditioning/controlnet/apply_controlnet_old.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=eaf2f8df7456df845f7408495d19b9e8" alt="apply controlnet old" data-og-width="778" width="778" data-og-height="370" height="370" data-path="images/comfy_core/conditioning/controlnet/apply_controlnet_old.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/conditioning/controlnet/apply_controlnet_old.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=d1b70261b17b6e3b46c4ed91e1be1fd4 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/conditioning/controlnet/apply_controlnet_old.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=9f81de62d9574bcb8b0519488cdc4195 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/conditioning/controlnet/apply_controlnet_old.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=b0878d93dea28563986e3a1e2c77450b 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/conditioning/controlnet/apply_controlnet_old.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=a5d3fdd9887f00d55fc68161fbed2ca6 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/conditioning/controlnet/apply_controlnet_old.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=e34942778977bc4a0898c09123123b26 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/conditioning/controlnet/apply_controlnet_old.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=11a8f3eb731d988689ce245a0a327d8d 2500w" />
  To enable it, go to **Settings** --> **comfy** --> **Node** and enable the `Show deprecated nodes in search` option. However, it's recommended to use the new node.
</Note>

## Start Your Exploration

1. Try creating similar sketches, or even draw your own, and use ControlNet models to generate images to experience the benefits of ControlNet.
2. Adjust the `Control Strength` parameter in the Apply ControlNet node to control the influence of the ControlNet model on the generated image.
3. Visit the [ControlNet-v1-1\_fp16\_safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/tree/main) repository to download other types of ControlNet models and try using them to generate images.


# ComfyUI Depth ControlNet Usage Example
Source: https://docs.comfy.org/tutorials/controlnet/depth-controlnet

This guide will introduce you to the basic concepts of Depth ControlNet and demonstrate how to generate corresponding images in ComfyUI

## Introduction to Depth Maps and Depth ControlNet

A depth map is a special type of image that uses grayscale values to represent the distance between objects in a scene and the observer or camera. In a depth map, the grayscale value is inversely proportional to distance: brighter areas (closer to white) indicate objects that are closer, while darker areas (closer to black) indicate objects that are farther away.

![Depth Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth_input.png)

Depth ControlNet is a ControlNet model specifically trained to understand and utilize depth map information. It helps AI correctly interpret spatial relationships, ensuring that generated images conform to the spatial structure specified by the depth map, thereby enabling precise control over three-dimensional spatial layouts.

### Application Scenarios for Depth Maps with ControlNet

Depth maps have numerous applications in various scenarios:

1. **Portrait Scenes**: Control the spatial relationship between subjects and backgrounds, avoiding distortion in critical areas such as faces
2. **Landscape Scenes**: Control the hierarchical relationships between foreground, middle ground, and background
3. **Architectural Scenes**: Control the spatial structure and perspective relationships of buildings
4. **Product Showcase**: Control the separation and spatial positioning of products against their backgrounds

In this example, we will use a depth map to generate an architectural visualization scene.

## ComfyUI ControlNet Workflow Example Explanation

### 1. ControlNet Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:

![Depth Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth_controlnet.png)

<Tip>
  Images with workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
  This image already includes download links for the corresponding models, and dragging it into ComfyUI will automatically prompt for downloads.
</Tip>

Please download the image below, which we will use as input:

![Depth Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth_input.png)

### 2. Model Installation

<Note>
  If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:
</Note>

* [architecturerealmix\_v11.safetensors](https://civitai.com/api/download/models/431755?type=Model\&format=SafeTensor\&size=full\&fp=fp16)
* [control\_v11f1p\_sd15\_depth\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11f1p_sd15_depth_fp16.safetensors?download=true)

```
ComfyUI/
 models/
    checkpoints/
       architecturerealmix_v11.safetensors
    controlnet/
        control_v11f1p_sd15_depth_fp16.safetensors
```

### 3. Step-by-Step Workflow Execution

<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_depth.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=7bcb7a518e96e88c6ccb73c1a7eb7087" alt="ComfyUI Workflow - Depth ControlNet Flow Diagram" data-og-width="2000" width="2000" data-og-height="1080" height="1080" data-path="images/tutorial/controlnet/flow_diagram_depth.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_depth.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=06ebada589ba06f3099c330e8bf59d73 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_depth.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=d954e861f557cd6e16512265bfb9ffe5 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_depth.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=2d91e0e82d1aae7bd1a9983ca349bf0e 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_depth.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=c44b0150592455946dcfa90643a519b6 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_depth.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=f7856a9bd50851e5881421ecab512941 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_depth.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=83a078d5ea4ff21e0d17a77c6921a8c6 2500w" />

1. Ensure that `Load Checkpoint` can load **architecturerealmix\_v11.safetensors**
2. Ensure that `Load ControlNet` can load **control\_v11f1p\_sd15\_depth\_fp16.safetensors**
3. Click `Upload` in the `Load Image` node to upload the depth image provided earlier
4. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## Combining Depth Control with Other Techniques

Based on different creative needs, you can combine Depth ControlNet with other types of ControlNet to achieve better results:

1. **Depth + Lineart**: Maintain spatial relationships while reinforcing outlines, suitable for architecture, products, and character design
2. **Depth + Pose**: Control character posture while maintaining correct spatial relationships, suitable for character scenes

For more information on using multiple ControlNet models together, please refer to the [Mixing ControlNet](/tutorials/controlnet/mixing-controlnets) example.


# ComfyUI Depth T2I Adapter Usage Example
Source: https://docs.comfy.org/tutorials/controlnet/depth-t2i-adapter

This guide will introduce you to the basic concepts of Depth T2I Adapter and demonstrate how to generate corresponding images in ComfyUI

## Introduction to T2I Adapter

[T2I-Adapter](https://huggingface.co/TencentARC/T2I-Adapter) is a lightweight adapter developed by [Tencent ARC Lab](https://github.com/TencentARC) designed to enhance the structural, color, and style control capabilities of text-to-image generation models (such as Stable Diffusion).
It works by aligning external conditions (such as edge detection maps, depth maps, sketches, or color reference images) with the model's internal features, achieving high-precision control without modifying the original model structure. With only about 77M parameters (approximately 300MB in size), its inference speed is about 3 times faster than [ControlNet](https://github.com/lllyasviel/ControlNet-v1-1-nightly), and it supports multiple condition combinations (such as sketch + color grid). Application scenarios include line art to image conversion, color style transfer, multi-element scene generation, and more.

### Comparison Between T2I Adapter and ControlNet

Although their functions are similar, there are notable differences in implementation and application:

1. **Lightweight Design**: T2I Adapter has fewer parameters and a smaller memory footprint
2. **Inference Speed**: T2I Adapter is typically about 3 times faster than ControlNet
3. **Control Precision**: ControlNet offers more precise control in certain scenarios, while T2I Adapter is more suitable for lightweight control
4. **Multi-condition Combination**: T2I Adapter shows more significant resource advantages when combining multiple conditions

### Main Types of T2I Adapter

T2I Adapter provides various types to control different aspects:

* **Depth**: Controls the spatial structure and depth relationships in images
* **Line Art (Canny/Sketch)**: Controls image edges and lines
* **Keypose**: Controls character poses and actions
* **Segmentation (Seg)**: Controls scene layout through semantic segmentation
* **Color**: Controls the overall color scheme of images

In ComfyUI, using T2I Adapter is similar to [ControlNet](/tutorials/controlnet/controlnet) in terms of interface and workflow. In this example, we will demonstrate how to use a depth T2I Adapter to control an interior scene.

![ComfyUI Depth T2I Adapter Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter.png)

## Value of Depth T2I Adapter Applications

Depth maps have several important applications in image generation:

1. **Spatial Layout Control**: Accurately describes three-dimensional spatial structures, suitable for interior design and architectural visualization
2. **Object Positioning**: Controls the relative position and size of objects in a scene, suitable for product showcases and scene construction
3. **Perspective Relationships**: Maintains reasonable perspective and proportions, suitable for landscape and urban scene generation
4. **Light and Shadow Layout**: Natural light and shadow distribution based on depth information, enhancing realism

We will use interior design as an example to demonstrate how to use the depth T2I Adapter, but these techniques are applicable to other scenarios as well.

## ComfyUI Depth T2I Adapter Workflow Example Explanation

### 1. Depth T2I Adapter Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:

![ComfyUI Workflow - Depth T2I Adapter](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter.png)

<Tip>
  Images with workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
  This image already includes download links for the corresponding models, and dragging it into ComfyUI will automatically prompt for downloads.
</Tip>

Please download the image below, which we will use as input:

![ComfyUI Interior Depth Map](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter_input.png)

### 2. Model Installation

<Note>
  If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:
</Note>

* [interiordesignsuperm\_v2.safetensors](https://civitai.com/api/download/models/93152?type=Model\&format=SafeTensor\&size=full\&fp=fp16)
* [t2iadapter\_depth\_sd15v2.pth](https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_depth_sd15v2.pth?download=true)

```
ComfyUI/
 models/
    checkpoints/
       interiordesignsuperm_v2.safetensors
    controlnet/
        t2iadapter_depth_sd15v2.pth
```

### 3. Step-by-Step Workflow Execution

<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_depth_ti2_adapter.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=3aeec924326e1bda6ff5d1eb390f6d4c" alt="ComfyUI Workflow - Depth T2I Adapter Flow Diagram" data-og-width="2030" width="2030" data-og-height="1112" height="1112" data-path="images/tutorial/controlnet/flow_diagram_depth_ti2_adapter.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_depth_ti2_adapter.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=5774206cc37451515e77bb78a1191812 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_depth_ti2_adapter.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=ff1e798cddb003bec84d72d91a9e32d6 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_depth_ti2_adapter.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=f6c2b5e86f6c9106b73a49370de2b493 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_depth_ti2_adapter.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=138344740eb983b122d8110e6cf74afd 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_depth_ti2_adapter.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=cdcd08b5f17bfbc1153f2161866a049c 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_depth_ti2_adapter.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=80f00904958816d030dec63d5047ef61 2500w" />

1. Ensure that `Load Checkpoint` can load **interiordesignsuperm\_v2.safetensors**
2. Ensure that `Load ControlNet` can load **t2iadapter\_depth\_sd15v2.pth**
3. Click `Upload` in the `Load Image` node to upload the input image provided earlier
4. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## General Tips for Using T2I Adapter

### Input Image Quality Optimization

Regardless of the application scenario, high-quality input images are key to successfully using T2I Adapter:

1. **Moderate Contrast**: Control images (such as depth maps, line art) should have clear contrast, but not excessively extreme
2. **Clear Boundaries**: Ensure that major structures and element boundaries are clearly distinguishable in the control image
3. **Noise Control**: Try to avoid excessive noise in control images, especially for depth maps and line art
4. **Reasonable Layout**: Control images should have a reasonable spatial layout and element distribution

## Characteristics of T2I Adapter Usage

One major advantage of T2I Adapter is its ability to easily combine multiple conditions for complex control effects:

1. **Depth + Edge**: Control spatial layout while maintaining clear structural edges, suitable for architecture and interior design
2. **Line Art + Color**: Control shapes while specifying color schemes, suitable for character design and illustrations
3. **Pose + Segmentation**: Control character actions while defining scene areas, suitable for complex narrative scenes

Mixing different T2I Adapters, or combining them with other control methods (such as ControlNet, regional prompts, etc.), can further expand creative possibilities. To achieve mixing, simply chain multiple `Apply ControlNet` nodes together in the same way as described in [Mixing ControlNet](/tutorials/controlnet/mixing-controlnets).


# ComfyUI Mixing ControlNet Examples
Source: https://docs.comfy.org/tutorials/controlnet/mixing-controlnets

In this example, we will demonstrate how to mix multiple ControlNets and learn to use multiple ControlNet models to control image generation

In AI image generation, a single control condition often fails to meet the requirements of complex scenes. Mixing multiple ControlNets allows you to control different regions or aspects of an image simultaneously, achieving more precise control over image generation.

In certain scenarios, mixing ControlNets can leverage the characteristics of different control conditions to achieve more refined conditional control:

1. **Scene Complexity**: Complex scenes require multiple control conditions working together
2. **Fine-grained Control**: By adjusting the strength parameter of each ControlNet, you can precisely control the degree of influence for each part
3. **Complementary Effects**: Different types of ControlNets can complement each other, compensating for the limitations of single controls
4. **Creative Expression**: Combining different controls can produce unique creative effects

### How to Mix ControlNets

When mixing multiple ControlNets, each ControlNet influences the image generation process according to its applied area. ComfyUI enables multiple ControlNet conditions to be applied sequentially in a layered manner through chain connections in the `Apply ControlNet` node:

<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/controlnet/apply_controlnet_chain_link.jpg?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=6fc3e75414f05e6c62062b4b746dc855" alt="apply controlnet chain link" data-og-width="1500" width="1500" data-og-height="1050" height="1050" data-path="images/tutorial/controlnet/apply_controlnet_chain_link.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/controlnet/apply_controlnet_chain_link.jpg?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=da6928840e7317cf4c0cc56cddd3bef9 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/controlnet/apply_controlnet_chain_link.jpg?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=be2b48a1a8c3f852ffbc1e0e7fa06f54 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/controlnet/apply_controlnet_chain_link.jpg?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=8488372f56c5d9e7a4f95574094bcd25 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/controlnet/apply_controlnet_chain_link.jpg?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=3893bbb66fd156f76713a5d51623cd12 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/controlnet/apply_controlnet_chain_link.jpg?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=aab42fb6856dc99be46f00bb333e8a6f 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/controlnet/apply_controlnet_chain_link.jpg?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=c8dee2a84c69fc57166788cae6387f17 2500w" />

## ComfyUI ControlNet Regional Division Mixing Example

In this example, we will use a combination of **Pose ControlNet** and **Scribble ControlNet** to generate a scene containing multiple elements: a character on the left controlled by Pose ControlNet and a cat on a scooter on the right controlled by Scribble ControlNet.

### 1. ControlNet Mixing Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:
![ComfyUI Workflow - Mixing ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/mixing_controlnets.png)

<Tip>
  This workflow image contains Metadata, and can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`. The system will automatically detect and prompt to download the required models.
</Tip>

Input pose image (controls the character pose on the left):

![ComfyUI Workflow - Mixing ControlNet Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/mixing_controlnets_input.png)

Input scribble image (controls the cat and scooter on the right):

![ComfyUI Workflow - Mixing ControlNet Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/mixing_controlnets_input_scribble.png)

### 2. Manual Model Installation

<Note>
  If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:
</Note>

* [awpainting\_v14.safetensors](https://civitai.com/api/download/models/624939?type=Model\&format=SafeTensor\&size=full\&fp=fp16)
* [control\_v11p\_sd15\_scribble\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors?download=true)
* [control\_v11p\_sd15\_openpose\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors?download=true)
* [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)

```
ComfyUI/
 models/
    checkpoints/
       awpainting_v14.safetensors
    controlnet/
       control_v11p_sd15_scribble_fp16.safetensors
       control_v11p_sd15_openpose_fp16.safetensors
    vae/
       vae-ft-mse-840000-ema-pruned.safetensors
```

### 3. Step-by-Step Workflow Execution

<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_mixing_controlnet.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=b839fc78bb316a334a908ff38389a3d0" alt="ComfyUI Workflow - Mixing ControlNet Flow Diagram" data-og-width="1681" width="1681" data-og-height="1081" height="1081" data-path="images/tutorial/controlnet/flow_diagram_mixing_controlnet.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_mixing_controlnet.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=27da5acfce3ec4d1907c1ab4afe95905 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_mixing_controlnet.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=3723acd76c10d49c6901565e28b57bf9 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_mixing_controlnet.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=b5b0ec7de5bc6ae4e24f378c5fc0a4ee 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_mixing_controlnet.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=63546a6d6c61d981bc110e2c1397ab5e 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_mixing_controlnet.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=7674c4309f93a53d6f18734790313eaa 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_mixing_controlnet.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=0e0979fa692936407c1b45930df2645d 2500w" />

Follow these steps according to the numbered markers in the image:

1. Ensure that `Load Checkpoint` can load **awpainting\_v14.safetensors**
2. Ensure that `Load VAE` can load **vae-ft-mse-840000-ema-pruned.safetensors**

First ControlNet group using the Openpose model:
3\. Ensure that `Load ControlNet Model` loads **control\_v11p\_sd15\_openpose\_fp16.safetensors**
4\. Click `Upload` in the `Load Image` node to upload the pose image provided earlier

Second ControlNet group using the Scribble model:
5\. Ensure that `Load ControlNet Model` loads **control\_v11p\_sd15\_scribble\_fp16.safetensors**
6\. Click `Upload` in the `Load Image` node to upload the scribble image provided earlier
7\. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## Workflow Explanation

#### Strength Balance

When controlling different regions of an image, balancing the strength parameters is particularly important:

* If the ControlNet strength for one region is significantly higher than another, it may cause that region's control effect to overpower and suppress the other region
* It's recommended to set similar strength values for ControlNets controlling different regions, for example, both set to 1.0

#### Prompt Techniques

For regional division mixing, the prompt needs to include descriptions of both regions:

```
"A woman in red dress, a cat riding a scooter, detailed background, high quality"
```

Such a prompt covers both the character and the cat on the scooter, ensuring the model pays attention to both control regions.

## Multi-dimensional Control Applications for a Single Subject

In addition to the regional division mixing shown in this example, another common mixing approach is to apply multi-dimensional control to the same subject. For example:

* **Pose + Depth**: Control character posture and spatial sense
* **Pose + Canny**: Control character posture and edge details
* **Pose + Reference**: Control character posture while referencing a specific style

In this type of application, reference images for multiple ControlNets should be aligned to the same subject, and their strengths should be adjusted to ensure proper balance.

By combining different types of ControlNets and specifying their control regions, you can achieve precise control over elements in your image.


# ComfyUI Pose ControlNet Usage Example
Source: https://docs.comfy.org/tutorials/controlnet/pose-controlnet-2-pass

This guide will introduce you to the basic concepts of Pose ControlNet, and demonstrate how to generate large-sized images in ComfyUI using a two-pass generation approach

## Introduction to OpenPose

[OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) is an open-source real-time multi-person pose estimation system developed by Carnegie Mellon University (CMU), representing a significant breakthrough in the field of computer vision. The system can simultaneously detect multiple people in an image, capturing:

* **Body skeleton**: 18 keypoints, including head, shoulders, elbows, wrists, hips, knees, and ankles
* **Facial expressions**: 70 facial keypoints for capturing micro-expressions and facial contours
* **Hand details**: 21 hand keypoints for precisely expressing finger positions and gestures
* **Foot posture**: 6 foot keypoints, recording standing postures and movement details

<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/openpose_example.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=bf85624e63f724a95635801aea0af8d5" alt="OpenPose Example" data-og-width="2048" width="2048" data-og-height="512" height="512" data-path="images/tutorial/controlnet/openpose_example.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/openpose_example.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=b41f289b721359a33ffc37287ac8384a 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/openpose_example.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=d51ebc3cd95276ccefa02559efe695f8 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/openpose_example.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=f2df9f53281cd7a27c4f1ed6a610d924 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/openpose_example.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=6e4256760e75758f7e53e80e7b1a82ad 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/openpose_example.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=974b7c1b9c31f679d4f9b8e13b5f3ef9 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/openpose_example.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=558727680b244ba2a38285e7fd029f25 2500w" />

In AI image generation, skeleton structure maps generated by OpenPose serve as conditional inputs for ControlNet, enabling precise control over the posture, actions, and expressions of generated characters. This allows us to generate realistic human figures with expected poses and actions, greatly improving the controllability and practical value of AI-generated content.
Particularly for early Stable Diffusion 1.5 series models, skeletal maps generated by OpenPose can effectively prevent issues with distorted character actions, limbs, and expressions.

## ComfyUI 2-Pass Pose ControlNet Usage Example

### 1. Pose ControlNet Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:

![ComfyUI Workflow - Pose ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/pose_controlnet_2_pass.png)

<Tip>
  Images with workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
  This image already includes download links for the corresponding models, and dragging it into ComfyUI will automatically prompt for downloads.
</Tip>

Please download the image below, which we will use as input:

![ComfyUI Pose Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/pose_controlnet_2_pass_input.png)

### 2. Manual Model Installation

<Note>
  If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:
</Note>

* [control\_v11p\_sd15\_openpose\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors?download=true)
* [majicmixRealistic\_v7.safetensors](https://civitai.com/api/download/models/176425?type=Model\&format=SafeTensor\&size=pruned\&fp=fp16)
* [japaneseStyleRealistic\_v20.safetensors](https://civitai.com/api/download/models/85426?type=Model\&format=SafeTensor\&size=pruned\&fp=fp16)
* [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)

```
ComfyUI/
 models/
    checkpoints/
       majicmixRealistic_v7.safetensors
       japaneseStyleRealistic_v20.safetensors
    vae/
       vae-ft-mse-840000-ema-pruned.safetensors
    controlnet/
        control_v11p_sd15_openpose_fp16.safetensors
```

### 3. Step-by-Step Workflow Execution

<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_pose_controlnet_2_pass.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=f953cc0188bb242b6f03389d295bf551" alt="ComfyUI Workflow - Pose ControlNet Flow Diagram" data-og-width="1803" width="1803" data-og-height="1081" height="1081" data-path="images/tutorial/controlnet/flow_diagram_pose_controlnet_2_pass.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_pose_controlnet_2_pass.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=e2ca788a11b7d9415b05dfc430258b01 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_pose_controlnet_2_pass.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=e7132662b1d03152042ca49cafc93d8f 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_pose_controlnet_2_pass.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=dd612fbf76ab6fe38306afd63a128c17 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_pose_controlnet_2_pass.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=9fd99919c6ea83863a61a530ce696148 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_pose_controlnet_2_pass.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=839e819834805cd482efc81a3f2ca6a6 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_pose_controlnet_2_pass.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=f039ec9330579f55a5a958fe8dc291e2 2500w" />

Follow these steps according to the numbered markers in the image:

1. Ensure that `Load Checkpoint` can load **majicmixRealistic\_v7.safetensors**
2. Ensure that `Load VAE` can load **vae-ft-mse-840000-ema-pruned.safetensors**
3. Ensure that `Load ControlNet Model` can load **control\_v11p\_sd15\_openpose\_fp16.safetensors**
4. Click the select button in the `Load Image` node to upload the pose input image provided earlier, or use your own OpenPose skeleton map
5. Ensure that `Load Checkpoint` can load **japaneseStyleRealistic\_v20.safetensors**
6. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## Explanation of the Pose ControlNet 2-Pass Workflow

This workflow uses a two-pass image generation approach, dividing the image creation process into two phases:

### First Phase: Basic Pose Image Generation

In the first phase, the **majicmixRealistic\_v7** model is combined with Pose ControlNet to generate an initial character pose image:

1. First, load the majicmixRealistic\_v7 model via the `Load Checkpoint` node
2. Load the pose control model through the `Load ControlNet Model` node
3. The input pose image is fed into the `Apply ControlNet` node and combined with positive and negative prompt conditions
4. The first `KSampler` node (typically using 20-30 steps) generates a basic character pose image
5. The pixel-space image for the first phase is obtained through `VAE Decode`

This phase primarily focuses on correct character posture, pose, and basic structure, ensuring that the generated character conforms to the input skeletal pose.

### Second Phase: Style Optimization and Detail Enhancement

In the second phase, the output image from the first phase is used as a reference, with the **japaneseStyleRealistic\_v20** model performing stylization and detail enhancement:

1. The image generated in the first phase creates a larger resolution latent space through the `Upscale latent` node
2. The second `Load Checkpoint` loads the japaneseStyleRealistic\_v20 model, which focuses on details and style
3. The second `KSampler` node uses a lower `denoise` strength (typically 0.4-0.6) for refinement, preserving the basic structure from the first phase
4. Finally, a higher quality, larger resolution image is output through the second `VAE Decode` and `Save Image` nodes

This phase primarily focuses on style consistency, detail richness, and enhancing overall image quality.

## Advantages of 2-Pass Image Generation

Compared to single-pass generation, the two-pass image generation method offers the following advantages:

1. **Higher Resolution**: Two-pass processing can generate high-resolution images beyond the capabilities of single-pass generation
2. **Style Blending**: Can combine advantages of different models, such as using a realistic model in the first phase and a stylized model in the second phase
3. **Better Details**: The second phase can focus on optimizing details without having to worry about overall structure
4. **Precise Control**: Once pose control is completed in the first phase, the second phase can focus on refining style and details
5. **Reduced GPU Load**: Generating in two passes allows for high-quality large images with limited GPU resources

<Tip>
  To learn more about techniques for mixing multiple ControlNets, please refer to the [Mixing ControlNet Models](/tutorials/controlnet/mixing-controlnets) tutorial.
</Tip>


# ComfyUI Flux.1 ControlNet Examples
Source: https://docs.comfy.org/tutorials/flux/flux-1-controlnet

This guide will demonstrate workflow examples using Flux.1 ControlNet.

<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux-1-canny-controlnet.png?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=9796a0a85863c068c836ff7351b21729" alt="Flux.1 Canny Controlnet" data-og-width="1600" width="1600" data-og-height="434" height="434" data-path="images/tutorial/flux/flux-1-canny-controlnet.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux-1-canny-controlnet.png?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=c8a38bf940731ef4f191acfbc4e7331d 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux-1-canny-controlnet.png?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=57197282f571d9d7fbe9da9afac18d91 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux-1-canny-controlnet.png?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=f0e887ea60fdcab432c04e9b9b8cd8be 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux-1-canny-controlnet.png?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=9a2e438661c73dba661598d8c85993fa 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux-1-canny-controlnet.png?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=7718559f1774a5378b2cce821a00e5db 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux-1-canny-controlnet.png?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=232c092e87b8022fe2e34d113796c18e 2500w" />
<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux-1-depth-controlnet.png?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=34c23b4e92011bb86fe97bc966441d80" alt="Flux.1 Depth Controlnet" data-og-width="1600" width="1600" data-og-height="285" height="285" data-path="images/tutorial/flux/flux-1-depth-controlnet.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux-1-depth-controlnet.png?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=65ff60ff81d6db718aaa5d0990150a0f 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux-1-depth-controlnet.png?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=5df60b44abf1779409b2531f58c372d7 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux-1-depth-controlnet.png?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=af9afb5910abd39fcdd9fb3a96c728a6 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux-1-depth-controlnet.png?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=471f82a699282e9f0d1ea5e1a5ed8bb1 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux-1-depth-controlnet.png?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=8b0b4a5361b74724b97d8984fc34abb6 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux-1-depth-controlnet.png?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=70accae2c7aaa0600b3d99827bd2b634 2500w" />

## FLUX.1 ControlNet Model Introduction

FLUX.1 Canny and Depth are two powerful models from the [FLUX.1 Tools](https://blackforestlabs.ai/flux-1-tools/) launched by [Black Forest Labs](https://blackforestlabs.ai/). This toolkit is designed to add control and guidance capabilities to FLUX.1, enabling users to modify and recreate real or generated images.

**FLUX.1-Depth-dev** and **FLUX.1-Canny-dev** are both 12B parameter Rectified Flow Transformer models that can generate images based on text descriptions while maintaining the structural features of the input image.
The Depth version maintains the spatial structure of the source image through depth map extraction techniques, while the Canny version uses edge detection techniques to preserve the structural features of the source image, allowing users to choose the appropriate control method based on different needs.

Both models have the following features:

* Top-tier output quality and detail representation
* Excellent prompt following ability while maintaining consistency with the original image
* Trained using guided distillation techniques for improved efficiency
* Open weights for the research community
* API interfaces (pro version) and open-source weights (dev version)

Additionally, Black Forest Labs also provides **FLUX.1-Depth-dev-lora** and **FLUX.1-Canny-dev-lora** adapter versions extracted from the complete models.
These can be applied to the FLUX.1 \[dev] base model to provide similar functionality with smaller file size, especially suitable for resource-constrained environments.

We will use the full version of **FLUX.1-Canny-dev** and **FLUX.1-Depth-dev-lora** to complete the workflow examples.

<Tip>
  All workflow images's Metadata contains the corresponding model download information. You can load the workflows by:

  * Dragging them directly into ComfyUI
  * Or using the menu `Workflows` -> `Openctrl+o`

  If you're not using the Desktop Version or some models can't be downloaded automatically, please refer to the manual installation sections to save the model files to the corresponding folder.

  For image preprocessors, you can use the following custom nodes to complete image preprocessing. In this example, we will provide processed images as input.

  * [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)
  * [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)
</Tip>

## FLUX.1-Canny-dev Complete Version Workflow

### 1. Workflow and Asset

Please download the workflow image below and drag it into ComfyUI to load the workflow

![ComfyUI Workflow - ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-canny-dev.png)

Please download the image below, which we will use as the input image

![ComfyUI Flux.1 Canny Controlnet input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-canny-dev-input.png)

### 2. Manual Models Installation

<Note>
  If you have previously used the [complete version of Flux related workflows](/tutorials/flux/flux-1-text-to-image), then you only need to download the **flux1-canny-dev.safetensors** model file.
  Since you need to first agree to the terms of [black-forest-labs/FLUX.1-Canny-dev](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev), please visit the [black-forest-labs/FLUX.1-Canny-dev](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev) page and make sure you have agreed to the corresponding terms as shown in the image below.
  <img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux1_canny_dev_agreement.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=64b9b0d0d2b8dd332dc1184dde9689be" alt="Flux Agreement" data-og-width="2000" width="2000" data-og-height="1091" height="1091" data-path="images/tutorial/flux/flux1_canny_dev_agreement.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux1_canny_dev_agreement.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=c3aacab6b791cf60520a0b4be1ee988b 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux1_canny_dev_agreement.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=72cde7f0cde89233c51a28598dcf996c 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux1_canny_dev_agreement.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=8d31cbd9704522774e3363a4174f1c50 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux1_canny_dev_agreement.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=37052cd503baba8bda5a10f153ad0710 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux1_canny_dev_agreement.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=656f8dcc4afab76118ed6ed3e7e253cf 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux1_canny_dev_agreement.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=96568aa34dc2455b23148d6796b480c5 2500w" />
</Note>

Complete model list:

* [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
* [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)
* [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
* [flux1-canny-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev/resolve/main/flux1-canny-dev.safetensors?download=true) (Please ensure you have agreed to the corresponding repo's terms)

File storage location:

```
ComfyUI/
 models/
    text_encoders/
       clip_l.safetensors
       t5xxl_fp16.safetensors
    vae/
       ae.safetensors
    diffusion_models/
        flux1-canny-dev.safetensors
```

### 3. Step-by-Step Workflow Execution

<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_flux_1_canny_dev.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=9dfe7fc30106a96d837a93cb27958e42" alt="ComfyUI Flux.1 Canny Controlnet Step Process" data-og-width="4000" width="4000" data-og-height="1736" height="1736" data-path="images/tutorial/flux/flow_diagram_flux_1_canny_dev.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_flux_1_canny_dev.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=751235f8031159be58c382a114dc43f3 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_flux_1_canny_dev.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=1946bcd3dec56bbf062cedb8bcbd8b97 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_flux_1_canny_dev.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=bca1ef585a0acbc78fc4d51140ec10b1 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_flux_1_canny_dev.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=0f4cbe67dcf79a8af597b3d5f8becacd 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_flux_1_canny_dev.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=a9f35eb1702f847f6b6300918be03393 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_flux_1_canny_dev.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=52c23b72214d6382f24f54c603962406 2500w" />

1. Make sure `ae.safetensors` is loaded in the `Load VAE` node
2. Make sure `flux1-canny-dev.safetensors` is loaded in the `Load Diffusion Model` node
3. Make sure the following models are loaded in the `DualCLIPLoader` node:
   * clip\_name1: t5xxl\_fp16.safetensors
   * clip\_name2: clip\_l.safetensors
4. Upload the provided input image in the `Load Image` node
5. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

### 4. Start Your Experimentation

Try using the [FLUX.1-Depth-dev](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev) model to complete the Depth version of the workflow

You can use the image below as input
![ComfyUI Indoor Depth Map](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/depth-t2i-adapter_input.png)

Or use the following custom nodes to complete image preprocessing:

* [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)
* [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

## FLUX.1-Depth-dev-lora Workflow

The LoRA version workflow builds on the complete version by adding the LoRA model. Compared to the [complete version of the Flux workflow](/tutorials/flux/flux-1-text-to-image), it adds nodes for loading and using the corresponding LoRA model.

### 1. Workflow and Asset

Please download the workflow image below and drag it into ComfyUI to load the workflow

![ComfyUI Workflow - ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-depth-dev-lora.png)

Please download the image below, which we will use as the input image

![ComfyUI Flux.1 Depth Controlnet input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/controlnet/flux-1-depth-dev-lora-input.png)

### 2. Manual Model Download

<Tip>
  If you have previously used the [complete version of Flux related workflows](/tutorials/flux/flux-1-text-to-image), then you only need to download the **flux1-depth-dev-lora.safetensors** model file.
</Tip>

Complete model list:

* [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
* [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)
* [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
* [flux1-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/flux1-dev.safetensors?download=true)
* [flux1-depth-dev-lora.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev-lora/resolve/main/flux1-depth-dev-lora.safetensors?download=true)

File storage location:

```
ComfyUI/
 models/
    text_encoders/
       clip_l.safetensors
       t5xxl_fp16.safetensors
    vae/
       ae.safetensors
    diffusion_models/
       flux1-dev.safetensors
    loras/
        flux1-depth-dev-lora.safetensors
```

### 3. Step-by-Step Workflow Execution

<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_flux_1_depth_dev_lora.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=ea682a93040eac4c930e05a66e08dd7f" alt="ComfyUI Flux.1 Depth Controlnet Step Process" data-og-width="4000" width="4000" data-og-height="1703" height="1703" data-path="images/tutorial/flux/flow_diagram_flux_1_depth_dev_lora.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_flux_1_depth_dev_lora.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=19eb4f1db7a87249bac15f18632c01c7 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_flux_1_depth_dev_lora.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=9f8739c81b7752be484a37383bad0027 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_flux_1_depth_dev_lora.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=fbe919a88bd28aa323efdad92ece7161 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_flux_1_depth_dev_lora.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=b0b2d96d4c2bbea7738e5d4a7c377324 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_flux_1_depth_dev_lora.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=e888396047285644984cc46db06f5922 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_flux_1_depth_dev_lora.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=16ae6afd59005c12beb9612a542a4f01 2500w" />

1. Make sure `flux1-dev.safetensors` is loaded in the `Load Diffusion Model` node
2. Make sure `flux1-depth-dev-lora.safetensors` is loaded in the `LoraLoaderModelOnly` node
3. Make sure the following models are loaded in the `DualCLIPLoader` node:
   * clip\_name1: t5xxl\_fp16.safetensors
   * clip\_name2: clip\_l.safetensors
4. Upload the provided input image in the `Load Image` node
5. Make sure `ae.safetensors` is loaded in the `Load VAE` node
6. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

### 4. Start Your Experimentation

Try using the [FLUX.1-Canny-dev-lora](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev-lora) model to complete the Canny version of the workflow

Use [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet) or [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux) to complete image preprocessing

## Community Versions of Flux Controlnets

XLab and InstantX + Shakker Labs have released Controlnets for Flux.

**InstantX:**

* [FLUX.1-dev-Controlnet-Canny](https://huggingface.co/InstantX/FLUX.1-dev-Controlnet-Canny/blob/main/diffusion_pytorch_model.safetensors)
* [FLUX.1-dev-ControlNet-Depth](https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Depth/blob/main/diffusion_pytorch_model.safetensors)
* [FLUX.1-dev-ControlNet-Union-Pro](https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro/blob/main/diffusion_pytorch_model.safetensors)

**XLab**: [flux-controlnet-collections](https://huggingface.co/XLabs-AI/flux-controlnet-collections)

Place these files in the `ComfyUI/models/controlnet` directory.

You can visit [Flux Controlnet Example](https://raw.githubusercontent.com/comfyanonymous/ComfyUI_examples/refs/heads/master/flux/flux_controlnet_example.png) to get the corresponding workflow image, and use the image from [here](https://raw.githubusercontent.com/comfyanonymous/ComfyUI_examples/refs/heads/master/flux/girl_in_field.png) as the input image.


# ComfyUI Flux.1 fill dev Example
Source: https://docs.comfy.org/tutorials/flux/flux-1-fill-dev

This guide demonstrates how to use Flux.1 fill dev to create Inpainting and Outpainting workflows.

<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux-fill-dev-demo.jpeg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=e9b567aff5fbd43dce197d819f5f028e" alt="Flux.1 fill dev" data-og-width="2425" width="2425" data-og-height="1439" height="1439" data-path="images/tutorial/flux/flux-fill-dev-demo.jpeg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux-fill-dev-demo.jpeg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=b9ff2e9905b59dfd976d6fa3ad5b5383 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux-fill-dev-demo.jpeg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=23a6543c141339e5de834a2226eac292 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux-fill-dev-demo.jpeg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=91df5bbb329f8f721c9a5249974b9dfd 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux-fill-dev-demo.jpeg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=7cc6c43d71043ce741e6383c5248bad7 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux-fill-dev-demo.jpeg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=48578042c0e47ddecbed8ad434fd2636 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux-fill-dev-demo.jpeg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=c4a294a48eaebafc00d865acec53be32 2500w" />

## Introduction to Flux.1 fill dev Model

Flux.1 fill dev is one of the core tools in the [FLUX.1 Tools suite](https://blackforestlabs.ai/flux-1-tools/) launched by [Black Forest Labs](https://blackforestlabs.ai/), specifically designed for image inpainting and outpainting.

Key features of Flux.1 fill dev:

* Powerful image inpainting and outpainting capabilities, with results second only to the commercial version FLUX.1 Fill \[pro].
* Excellent prompt understanding and following ability, precisely capturing user intent while maintaining high consistency with the original image.
* Advanced guided distillation training technology, making the model more efficient while maintaining high-quality output.
* Friendly licensing terms, with generated outputs usable for personal, scientific, and commercial purposes, please refer to the [FLUX.1 \[dev\] non-commercial license](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md) for details.

Open Source Repository: [FLUX.1 \[dev\]](https://huggingface.co/black-forest-labs/FLUX.1-dev)

This guide will demonstrate inpainting and outpainting workflows based on the Flux.1 fill dev model.
If you're not familiar with inpainting and outpainting workflows, you can refer to [ComfyUI Layout Inpainting Example](/tutorials/basic/inpaint) and [ComfyUI Image Extension Example](/tutorials/basic/outpaint) for some related explanations.

## Flux.1 Fill dev and related models installation

Before we begin, let's complete the installation of the Flux.1 Fill dev model files. The inpainting and outpainting workflows will use exactly the same model files.
If you've previously used the full version of the [Flux.1 Text-to-Image workflow](/tutorials/flux/flux-1-text-to-image),
then you only need to download the **flux1-fill-dev.safetensors** model file in this section.

However, since downloading the corresponding model requires agreeing to the corresponding usage agreement, please visit the [black-forest-labs/FLUX.1-Fill-dev](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev) page and make sure you have agreed to the corresponding agreement as shown in the image below.
<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux1_fill_dev_agreement.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=388a26b32b5841dbf6ef32f31537e43d" alt="Flux Agreement" data-og-width="2000" width="2000" data-og-height="1091" height="1091" data-path="images/tutorial/flux/flux1_fill_dev_agreement.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux1_fill_dev_agreement.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=e931f6ec07f32ef3e50afb225580be12 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux1_fill_dev_agreement.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=2aaed9bcdc52c7f57b48692fdce76071 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux1_fill_dev_agreement.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=cf0a3480405e849938cd569eba919065 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux1_fill_dev_agreement.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=b30a92913ebf2f5a8232df354bb8558c 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux1_fill_dev_agreement.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=f9147bf0268a4d70f6c6bff96381449a 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux1_fill_dev_agreement.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=251f4c63f5860161b93afcff28180161 2500w" />

Complete model list:

* [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
* [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)
* [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
* [flux1-fill-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev/resolve/main/flux1-fill-dev.safetensors?download=true)

File storage location:

```
ComfyUI/
 models/
    text_encoders/
        clip_l.safetensors
        t5xxl_fp16.safetensors
    vae/
        ae.safetensors
    diffusion_models/
         flux1-fill-dev.safetensors
```

## Flux.1 Fill dev inpainting workflow

### 1. Inpainting workflow and asset

Please download the image below and drag it into ComfyUI to load the corresponding workflow
![ComfyUI Flux.1 inpaint](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/inpaint/flux_fill_inpaint.png)

Please download the image below, we will use it as the input image
![ComfyUI Flux.1 inpaint input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/inpaint/flux_fill_inpaint_input.png)

<Note>
  The corresponding image already contains an alpha channel, so you don't need to draw a mask separately.
  If you want to draw your own mask, please [click here](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/inpaint/flux_fill_inpaint_input_original.png) to get the image without a mask, and refer to the MaskEditor usage section in the [ComfyUI Layout Inpainting Example](/tutorials/basic/inpaint#using-the-mask-editor) to learn how to draw a mask in the `Load Image` node.
</Note>

### 2. Steps to run the workflow

<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_inpaint.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=8d4238c0487b7ddf527ec044aa02d578" alt="ComfyUI Flux.1 Fill dev Inpainting Workflow" data-og-width="4000" width="4000" data-og-height="2163" height="2163" data-path="images/tutorial/flux/flow_diagram_inpaint.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_inpaint.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=1d8a3d3fcce18ee64563486bd76097fc 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_inpaint.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=c86c70674d8643e5f6f59191d7d620b2 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_inpaint.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=bba1c4942e902354ce3042f16a26ff43 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_inpaint.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=6b802ce2d64055da8708a830a6608968 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_inpaint.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=630ceb021fdf1fc335975f475236c73e 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_inpaint.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=77f2eaa8ba13517df4d9492f17ea55aa 2500w" />

1. Ensure the `Load Diffusion Model` node has `flux1-fill-dev.safetensors` loaded.
2. Ensure the `DualCLIPLoader` node has the following models loaded:
   * clip\_name1: `t5xxl_fp16.safetensors`
   * clip\_name2: `clip_l.safetensors`
3. Ensure the `Load VAE` node has `ae.safetensors` loaded.
4. Upload the input image provided in the document to the `Load Image` node; if you're using the version without a mask, remember to complete the mask drawing using the mask editor
5. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## Flux.1 Fill dev Outpainting Workflow

### 1. Outpainting workflow and asset

Please download the image below and drag it into ComfyUI to load the corresponding workflow
![ComfyUI Flux.1 outpaint](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/outpaint/flux_fill_dev_outpaint.png)

Please download the image below, we will use it as the input image
![ComfyUI Flux.1 outpaint input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/outpaint/flux_fill_dev_outpaint_input.png)

### 2. Steps to run the workflow

<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_outpaint.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=5f152f459be5f120b3895636685cb3d3" alt="ComfyUI Flux.1 Fill dev Outpainting Workflow" data-og-width="4000" width="4000" data-og-height="2159" height="2159" data-path="images/tutorial/flux/flow_diagram_outpaint.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_outpaint.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=d68cb7ec13cce28e41d5b129e5556bfa 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_outpaint.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=cd1347b12b768b5d154d93f934f82545 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_outpaint.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=d903a9a469414f553c4db60cedc0f334 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_outpaint.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=a3e792313923fcb209d769d742020460 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_outpaint.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=116cb8593bab3731e6f5e3a06f95d51c 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_outpaint.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=7af097f3ebfc0ec39ff8b85309d88c02 2500w" />

1. Ensure the `Load Diffusion Model` node has `flux1-fill-dev.safetensors` loaded.
2. Ensure the `DualCLIPLoader` node has the following models loaded:
   * clip\_name1: `t5xxl_fp16.safetensors`
   * clip\_name2: `clip_l.safetensors`
3. Ensure the `Load VAE` node has `ae.safetensors` loaded.
4. Upload the input image provided in the document to the `Load Image` node
5. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow


# ComfyUI Flux Kontext Dev Native Workflow Example
Source: https://docs.comfy.org/tutorials/flux/flux-1-kontext-dev

ComfyUI Flux Kontext Dev Native Workflow Example.

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/Y7L_cbNJHj0?si=zuaRiU3qJYMNW2uv" title="ComfyUI Selection Toolbox New Features" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

## About FLUX.1 Kontext Dev

FLUX.1 Kontext is a breakthrough multimodal image editing model from Black Forest Labs that supports simultaneous text and image input, intelligently understanding image context and performing precise editing. Its development version is an open-source diffusion transformer model with 12 billion parameters, featuring excellent context understanding and character consistency maintenance, ensuring that key elements such as character features and composition layout remain stable even after multiple iterative edits.

It shares the same core capabilities as the FLUX.1 Kontext suite:

* Character Consistency: Preserves unique elements in images across multiple scenes and environments, such as reference characters or objects in the image.
* Editing: Makes targeted modifications to specific elements in the image without affecting other parts.
* Style Reference: Generates novel scenes while preserving the unique style of the reference image according to text prompts.
* Interactive Speed: Minimal latency in image generation and editing.

While the previously released API version offers the highest fidelity and speed, FLUX.1 Kontext \[Dev] runs entirely on local machines, providing unparalleled flexibility for developers, researchers, and advanced users who wish to experiment.

### Version Information

* **\[FLUX.1 Kontext \[pro]** - Commercial version, focused on rapid iterative editing
* **FLUX.1 Kontext \[max]** - Experimental version with stronger prompt adherence
* **FLUX.1 Kontext \[dev]** - Open source version (used in this tutorial), 12B parameters, mainly for research

Currently in ComfyUI, you can use all these versions, where [Pro and Max versions](/tutorials/partner-nodes/black-forest-labs/flux-1-kontext) can be called through API nodes, while the Dev open source version please refer to the instructions in this guide.

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Model Download

To run the workflows in this guide successfully, you first need to download the following model files. You can also directly get the model download links from the corresponding workflows, which already contain the model file download information.

**Diffusion Model**

* [flux1-dev-kontext\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/flux1-kontext-dev_ComfyUI/resolve/main/split_files/diffusion_models/flux1-dev-kontext_fp8_scaled.safetensors)

<Tip>
  If you want to use the original weights, you can visit Black Forest Labs' related repository to obtain and use the original model weights.
</Tip>

**VAE**

* [ae.safetensors](https://huggingface.co/Comfy-Org/Lumina_Image_2.0_Repackaged/blob/main/split_files/vae/ae.safetensors)

**Text Encoder**

* [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/blob/main/clip_l.safetensors)
* [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors) or [t5xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn_scaled.safetensors)

Model save location

```
 ComfyUI/
  models/
     diffusion_models/
       flux1-dev-kontext_fp8_scaled.safetensors
     vae/
       ae.safetensor
     text_encoders/
        clip_l.safetensors
        t5xxl_fp16.safetensors or t5xxl_fp8_e4m3fn_scaled.safetensors
```

## Flux.1 Kontext Dev Workflow

This workflow uses the `Load Image(from output)` node to load the image to be edited, making it more convenient for you to access the edited image for multiple rounds of editing.

### 1. Workflow and Input Image Download

Download the following files and drag them into ComfyUI to load the corresponding workflow

![ComfyUI Flux.1 Kontext Pro Image API Node Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/kontext/dev/flux_1_kontext_dev_basic.png)

**Input Image**

![ComfyUI Flux Kontext Native Workflow Input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/kontext/dev/rabbit.jpg)

### 2. Complete the workflow step by step

<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_1_kontext_dev_basic_step_guide.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=90995e36fa39a53693aeff3b560c60ef" alt="Workflow Step Guide" data-og-width="3214" width="3214" data-og-height="2066" height="2066" data-path="images/tutorial/flux/flux_1_kontext_dev_basic_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_1_kontext_dev_basic_step_guide.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=00aff823a946c2b826884d073a9cf534 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_1_kontext_dev_basic_step_guide.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=87af4329e8112a8a49bd9f9960e472d3 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_1_kontext_dev_basic_step_guide.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=2dd7fca1d798423ca45abd085bc295ed 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_1_kontext_dev_basic_step_guide.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=0d810c51e34794160b6def779a6bb09d 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_1_kontext_dev_basic_step_guide.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=ebc044d661f75d01cec6d4c2aa299001 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_1_kontext_dev_basic_step_guide.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=85a5abb63a36a2f172e64239f41a4b78 2500w" />
You can refer to the numbers in the image to complete the workflow run:

1. In the `Load Diffusion Model` node, load the `flux1-dev-kontext_fp8_scaled.safetensors` model
2. In the `DualCLIP Load` node, ensure that `clip_l.safetensors` and `t5xxl_fp16.safetensors` or `t5xxl_fp8_e4m3fn_scaled.safetensors` are loaded
3. In the `Load VAE` node, ensure that `ae.safetensors` model is loaded
4. In the `Load Image(from output)` node, load the provided input image
5. In the `CLIP Text Encode` node, modify the prompts, only English is supported
6. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## Flux Kontext Prompt Techniques

### 1. Basic Modifications

* Simple and direct: `"Change the car color to red"`
* Maintain style: `"Change to daytime while maintaining the same style of the painting"`

### 2. Style Transfer

**Principles:**

* Clearly name style: `"Transform to Bauhaus art style"`
* Describe characteristics: `"Transform to oil painting with visible brushstrokes, thick paint texture"`
* Preserve composition: `"Change to Bauhaus style while maintaining the original composition"`

### 3. Character Consistency

**Framework:**

* Specific description: `"The woman with short black hair"` instead of "she"
* Preserve features: `"while maintaining the same facial features, hairstyle, and expression"`
* Step-by-step modifications: Change background first, then actions

### 4. Text Editing

* Use quotes: `"Replace 'joy' with 'BFL'"`
* Maintain format: `"Replace text while maintaining the same font style"`

## Common Problem Solutions

### Character Changes Too Much

 Wrong: `"Transform the person into a Viking"`
 Correct: `"Change the clothes to be a viking warrior while preserving facial features"`

### Composition Position Changes

 Wrong: `"Put him on a beach"`
 Correct: `"Change the background to a beach while keeping the person in the exact same position, scale, and pose"`

### Style Application Inaccuracy

 Wrong: `"Make it a sketch"`
 Correct: `"Convert to pencil sketch with natural graphite lines, cross-hatching, and visible paper texture"`

## Core Principles

1. **Be Specific and Clear** - Use precise descriptions, avoid vague terms
2. **Step-by-step Editing** - Break complex modifications into multiple simple steps
3. **Explicit Preservation** - State what should remain unchanged
4. **Verb Selection** - Use "change", "replace" rather than "transform"

## Best Practice Templates

**Object Modification:**
`"Change [object] to [new state], keep [content to preserve] unchanged"`

**Style Transfer:**
`"Transform to [specific style], while maintaining [composition/character/other] unchanged"`

**Background Replacement:**
`"Change the background to [new background], keep the subject in the exact same position and pose"`

**Text Editing:**
`"Replace '[original text]' with '[new text]', maintain the same font style"`

> **Remember:** The more specific, the better. Kontext excels at understanding detailed instructions and maintaining consistency.


# ComfyUI Flux.1 Text-to-Image Workflow Example
Source: https://docs.comfy.org/tutorials/flux/flux-1-text-to-image

This guide provides a brief introduction to the Flux.1 model and guides you through using the Flux.1 model for text-to-image generation with examples including the full version and the FP8 Checkpoint version.

<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_example.png?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=199a3f0f9e2381586b482029d246f4bb" alt="Flux" data-og-width="2048" width="2048" data-og-height="1171" height="1171" data-path="images/tutorial/flux/flux_example.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_example.png?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=809f1427a04668bc20fde161f79f8e8c 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_example.png?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=51679c65a29520e2eae4d5e529eb9d22 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_example.png?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=2b562246e400efca7612f7fd228beb60 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_example.png?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=142b51a1bc19a56ef5264c3ccd6beb61 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_example.png?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=f5dab39f12ef469b89729d99a3c2eb63 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_example.png?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=d9ced5a2d86b6ba2613c57ec43339ed9 2500w" />
Flux is one of the largest open-source text-to-image generation models, with 12B parameters and an original file size of approximately 23GB. It was developed by [Black Forest Labs](https://blackforestlabs.ai/), a team founded by former Stable Diffusion team members.
Flux is known for its excellent image quality and flexibility, capable of generating high-quality, diverse images.

Currently, the Flux.1 model has several main versions:

* **Flux.1 Pro:** The best performing model, closed-source, only available through API calls.
* **[Flux.1 \[dev\]](https://huggingface.co/black-forest-labs/FLUX.1-dev)** Open-source but limited to non-commercial use, distilled from the Pro version, with performance close to the Pro version.
* **[Flux.1 \[schnell\]](https://huggingface.co/black-forest-labs/FLUX.1-schnell)** Uses the Apache2.0 license, requires only 4 steps to generate images, suitable for low-spec hardware.

**Flux.1 Model Features**

* **Hybrid Architecture:** Combines the advantages of Transformer networks and diffusion models, effectively integrating text and image information, improving the alignment accuracy between generated images and prompts, with excellent fidelity to complex prompts.
* **Parameter Scale:** Flux has 12B parameters, capturing more complex pattern relationships and generating more realistic, diverse images.
* **Supports Multiple Styles:** Supports diverse styles, with excellent performance for various types of images.

In this example, we'll introduce text-to-image examples using both Flux.1 Dev and Flux.1 Schnell versions, including the full version model and the simplified FP8 Checkpoint version.

* **Flux Full Version:** Best performance, but requires larger VRAM resources and installation of multiple model files.
* **Flux FP8 Checkpoint:** Requires only one fp8 version of the model, but quality is slightly reduced compared to the full version.

<Tip>
  All workflow images's Metadata contains the corresponding model download information. You can load the workflows by:

  * Dragging them directly into ComfyUI
  * Or using the menu `Workflows` -> `Openctrl+o`

  If you're not using the Desktop Version or some models can't be downloaded automatically, please refer to the manual installation sections to save the model files to the corresponding folder.
  Make sure your ComfyUI is updated to the latest version before starting.
</Tip>

## Flux.1 Full Version Text-to-Image Example

<Note>
  If you can't download models from [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev), make sure you've logged into Huggingface and agreed to the corresponding repository's license agreement.
  <img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_agreement.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=7171d72127bc5b3d6943996a9ef06970" alt="Flux Agreement" data-og-width="3330" width="3330" data-og-height="1854" height="1854" data-path="images/tutorial/flux/flux_agreement.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_agreement.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=5fed306c4f611f1edaef827dc6e0d8b6 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_agreement.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=8d2968fdc71af13b4993583d9bd77c6d 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_agreement.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=6d731ba033d2044414a352f927521f4c 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_agreement.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=205f214d986b9eee863b1ac7b644adf5 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_agreement.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=f03cb20f999349c5c6d8bfde63b817e2 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_agreement.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=0e6f751cb15e267efa77287ebee79f35 2500w" />
</Note>

### Flux.1 Dev

#### 1. Workflow File

Please download the image below and drag it into ComfyUI to load the workflow.
![Flux Dev Original Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_dev_t5fp16.png)

#### 2. Manual Model Installation

<Note>
  * The `flux1-dev.safetensors` file requires agreeing to the [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev) agreement before downloading via browser.
  * If your VRAM is low, you can try using [t5xxl\_fp8\_e4m3fn.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn.safetensors?download=true) to replace the `t5xxl_fp16.safetensors` file.
</Note>

Please download the following model files:

* [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
* [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true) Recommended when your VRAM is greater than 32GB.
* [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
* [flux1-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/flux1-dev.safetensors)

Storage location:

```
ComfyUI/
 models/
    text_encoders/
       clip_l.safetensors
       t5xxl_fp16.safetensors
    vae/
       ae.safetensors
    diffusion_models/
        flux1-dev.safetensors
```

#### 3. Steps to Run the Workflow

Please refer to the image below to ensure all model files are loaded correctly

<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_flux_dev_t5fp16.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=fbdc3a3b3b795328425b4ab854b77f13" alt="ComfyUI Flux Dev Workflow" data-og-width="3000" width="3000" data-og-height="1564" height="1564" data-path="images/tutorial/flux/flow_diagram_flux_dev_t5fp16.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_flux_dev_t5fp16.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=62b855e6dc0a2c7009437c4c405e2e08 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_flux_dev_t5fp16.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=980bc0272a2045ad438bed17c14d169b 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_flux_dev_t5fp16.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=8716a037e0507c3299e61752ec61cd1e 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_flux_dev_t5fp16.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=1d329ee0b53c6148550d31521dbd0d5b 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_flux_dev_t5fp16.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=a4d8ce8f0c9a1266bef3e52f91883bd7 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_flux_dev_t5fp16.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=8b49df11330394e528d2c476d30a7cfc 2500w" />

1. Ensure the `DualCLIPLoader` node has the following models loaded:
   * clip\_name1: t5xxl\_fp16.safetensors
   * clip\_name2: clip\_l.safetensors
2. Ensure the `Load Diffusion Model` node has `flux1-dev.safetensors` loaded
3. Make sure the `Load VAE` node has `ae.safetensors` loaded
4. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

<Tip>
  Thanks to Flux's excellent prompt following capability, we don't need any negative prompts
</Tip>

### Flux.1 Schnell

#### 1. Workflow File

Please download the image below and drag it into ComfyUI to load the workflow.

![Flux Schnell Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_schnell_t5fp8.png)

#### 2. Manual Models Installation

<Note>
  In this workflow, only two model files are different from the Flux1 Dev version workflow. For t5xxl, you can still use the fp16 version for better results.

  * **t5xxl\_fp16.safetensors** -> **t5xxl\_fp8.safetensors**
  * **flux1-dev.safetensors** -> **flux1-schnell.safetensors**
</Note>

Complete model file list:

* [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
* [t5xxl\_fp8\_e4m3fn.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn.safetensors?download=true)
* [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
* [flux1-schnell.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/flux1-schnell.safetensors)

File storage location:

```
ComfyUI/
 models/
    text_encoders/
       clip_l.safetensors
       t5xxl_fp8_e4m3fn.safetensors
    vae/
       ae.safetensors
    diffusion_models/
        flux1-schnell.safetensors
```

#### 3. Steps to Run the Workflow

<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_flux_schnell_t5fp8.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=6ce852c65510f40dde8069dc6370869d" alt="Flux Schnell Version Workflow" data-og-width="4000" width="4000" data-og-height="1599" height="1599" data-path="images/tutorial/flux/flow_diagram_flux_schnell_t5fp8.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_flux_schnell_t5fp8.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=a693b1cabe10e1403554d980e3235789 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_flux_schnell_t5fp8.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=a5df5a560d6209426506c18a01592596 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_flux_schnell_t5fp8.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=f6f054bd91ee1e4c652c8131893f7346 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_flux_schnell_t5fp8.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=8c7918768349130fe0b64d0d631758a1 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_flux_schnell_t5fp8.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=0d8a64ea9b06142e8bb38c00487a2153 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flow_diagram_flux_schnell_t5fp8.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=d64840834a702aae0a8d3b0697ed8d27 2500w" />

1. Ensure the `DualCLIPLoader` node has the following models loaded:
   * clip\_name1: t5xxl\_fp8\_e4m3fn.safetensors
   * clip\_name2: clip\_l.safetensors
2. Ensure the `Load Diffusion Model` node has `flux1-schnell.safetensors` loaded
3. Ensure the `Load VAE` node has `ae.safetensors` loaded
4. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## Flux.1 FP8 Checkpoint Version Text-to-Image Example

The fp8 version is a quantized version of the original Flux.1 fp16 version.
To some extent, the quality of this version will be lower than that of the fp16 version,
but it also requires less VRAM, and you only need to install one model file to try running it.

### Flux.1 Dev

Please download the image below and drag it into ComfyUI to load the workflow.

![Flux Dev fp8 Checkpoint Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_dev_fp8.png)

Please download [flux1-dev-fp8.safetensors](https://huggingface.co/Comfy-Org/flux1-dev/resolve/main/flux1-dev-fp8.safetensors?download=true) and save it to the `ComfyUI/models/checkpoints/` directory.

Ensure that the corresponding `Load Checkpoint` node loads `flux1-dev-fp8.safetensors`, and you can try to run the workflow.

### Flux.1 Schnell

Please download the image below and drag it into ComfyUI to load the workflow.

![Flux Schnell fp8 Checkpoint Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/text-to-image/flux_schnell_fp8.png)

Please download [flux1-schnell-fp8.safetensors](https://huggingface.co/Comfy-Org/flux1-schnell/resolve/main/flux1-schnell-fp8.safetensors?download=true) and save it to the `ComfyUI/models/checkpoints/` directory.

Ensure that the corresponding `Load Checkpoint` node loads `flux1-schnell-fp8.safetensors`, and you can try to run the workflow.


# ByteDance USO ComfyUI Native Workflow example
Source: https://docs.comfy.org/tutorials/flux/flux-1-uso

Unified Style and Subject-Driven Generation with ByteDance's USO model

**USO (Unified Style-Subject Optimized)** is a model developed by ByteDance's UXO Team that unifies style-driven and subject-driven generation tasks.
Built on FLUX.1-dev architecture, the model achieves both style similarity and subject consistency through disentangled learning and style reward learning (SRL).

USO supports three main approaches:

* **Subject-Driven**: Place subjects into new scenes while maintaining identity consistency
* **Style-Driven**: Apply artistic styles to new content based on reference images
* **Combined**: Use both subject and style references simultaneously

**Related Links**

* [Project Page](https://bytedance.github.io/USO/)
* [GitHub](https://github.com/bytedance/USO)
* [Model Weights](https://huggingface.co/bytedance-research/USO)

## ByteDance USO ComfyUI Native Workflow

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

### 1. Workflow and input

Download the image below and drag it into ComfyUI to load the corresponding workflow.

![Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/flux/bytedance-uso/bytedance-uso.png)

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/flux1_dev_uso_reference_image_gen.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download JSON Workflow</p>
</a>

Use the image below as an input image.

![input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/flux/bytedance-uso/input.png)

### 2. Model links

**checkpoints**

* [flux1-dev-fp8.safetensors](https://huggingface.co/Comfy-Org/flux1-dev/resolve/main/flux1-dev-fp8.safetensors)

**loras**

* [uso-flux1-dit-lora-v1.safetensors](https://huggingface.co/Comfy-Org/USO_1.0_Repackaged/resolve/main/split_files/loras/uso-flux1-dit-lora-v1.safetensors)

**model\_patches**

* [uso-flux1-projector-v1.safetensors](https://huggingface.co/Comfy-Org/USO_1.0_Repackaged/resolve/main/split_files/model_patches/uso-flux1-projector-v1.safetensors)

**clip\_visions**

* [sigclip\_vision\_patch14\_384.safetensors](https://huggingface.co/Comfy-Org/sigclip_vision_384/resolve/main/sigclip_vision_patch14_384.safetensors)

Please download all models and place them in the following directories:

```
 ComfyUI/
  models/
     checkpoints/
       flux1-dev-fp8.safetensors
     loras/
       uso-flux1-dit-lora-v1.safetensors
     model_patches/
       uso-flux1-projector-v1.safetensors
     clip_visions/
       sigclip_vision_patch14_384.safetensors
```

### 3. Workflow instructions

<img src="https://mintcdn.com/dripart/3zBG7o3F8mQK7Rdk/images/tutorial/flux/flux1_uso_reference_image_gen.jpg?fit=max&auto=format&n=3zBG7o3F8mQK7Rdk&q=85&s=60e1dc7cccd4b732ff2a6e24767d7630" alt="Workflow instructions" data-og-width="2000" width="2000" data-og-height="1188" height="1188" data-path="images/tutorial/flux/flux1_uso_reference_image_gen.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/3zBG7o3F8mQK7Rdk/images/tutorial/flux/flux1_uso_reference_image_gen.jpg?w=280&fit=max&auto=format&n=3zBG7o3F8mQK7Rdk&q=85&s=610ae03284c242865999544b959c66bb 280w, https://mintcdn.com/dripart/3zBG7o3F8mQK7Rdk/images/tutorial/flux/flux1_uso_reference_image_gen.jpg?w=560&fit=max&auto=format&n=3zBG7o3F8mQK7Rdk&q=85&s=f175cef9066b7736a66927c13417631d 560w, https://mintcdn.com/dripart/3zBG7o3F8mQK7Rdk/images/tutorial/flux/flux1_uso_reference_image_gen.jpg?w=840&fit=max&auto=format&n=3zBG7o3F8mQK7Rdk&q=85&s=9a0b273f5e59dac1948b29bb5dfb8f6b 840w, https://mintcdn.com/dripart/3zBG7o3F8mQK7Rdk/images/tutorial/flux/flux1_uso_reference_image_gen.jpg?w=1100&fit=max&auto=format&n=3zBG7o3F8mQK7Rdk&q=85&s=d185827366c42e43dd057be377ff4010 1100w, https://mintcdn.com/dripart/3zBG7o3F8mQK7Rdk/images/tutorial/flux/flux1_uso_reference_image_gen.jpg?w=1650&fit=max&auto=format&n=3zBG7o3F8mQK7Rdk&q=85&s=356a8a3c5689c6a623bf97ecde0881d7 1650w, https://mintcdn.com/dripart/3zBG7o3F8mQK7Rdk/images/tutorial/flux/flux1_uso_reference_image_gen.jpg?w=2500&fit=max&auto=format&n=3zBG7o3F8mQK7Rdk&q=85&s=52b6b8eb1387a8fad683ad23b4033e3a 2500w" />

1. Load models:
   * 1.1 Ensure the `Load Checkpoint` node has `flux1-dev-fp8.safetensors` loaded
   * 1.2 Ensure the `LoraLoaderModelOnly` node has `dit_lora.safetensors` loaded
   * 1.3 Ensure the `ModelPatchLoader` node has `projector.safetensors` loaded
   * 1.4 Ensure the `Load CLIP Vision` node has `sigclip_vision_patch14_384.safetensors` loaded
2. Content Reference:
   * 2.1 Click `Upload` to upload the input image we provided
   * 2.2 The `ImageScaleToMaxDimension` node will scale your input image for content reference, 512px will keep more character features, but if you only use the character's head as input, the final output image often has issues like the character taking up too much space. Setting it to 1024px gives much better results.
3. In the example, we only use the `content reference` image input. If you want to use the `style reference` image input, you can use `Ctrl-B` to bypass the marked node group.
4. Write your prompt or keep default
5. Set the image size if you need
6. The EasyCache node is for inference acceleration, but it will also sacrifice some quality and details. You can bypass it (Ctrl+B) if you don't need to use it.
7. Click the `Run` button, or use the shortcut `Ctrl(Cmd) + Enter` to run the workflow

### 4. Additional Notes

1. Style reference only:

We also provide a workflow that only uses style reference in the same workflow we provided

<img src="https://mintcdn.com/dripart/6BfRVG5RoFiMLPEQ/images/tutorial/flux/flux1_uso_reference_image_gen_style_reference_only.jpg?fit=max&auto=format&n=6BfRVG5RoFiMLPEQ&q=85&s=d5f16dbe97ac74b1bf01587aaabe12b8" alt="Workflow" data-og-width="4366" width="4366" data-og-height="2498" height="2498" data-path="images/tutorial/flux/flux1_uso_reference_image_gen_style_reference_only.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/6BfRVG5RoFiMLPEQ/images/tutorial/flux/flux1_uso_reference_image_gen_style_reference_only.jpg?w=280&fit=max&auto=format&n=6BfRVG5RoFiMLPEQ&q=85&s=fad6ae6e6aa55a9d4d459db8676e2488 280w, https://mintcdn.com/dripart/6BfRVG5RoFiMLPEQ/images/tutorial/flux/flux1_uso_reference_image_gen_style_reference_only.jpg?w=560&fit=max&auto=format&n=6BfRVG5RoFiMLPEQ&q=85&s=57afc7c22638d7b3dcebc4f8ec71dac2 560w, https://mintcdn.com/dripart/6BfRVG5RoFiMLPEQ/images/tutorial/flux/flux1_uso_reference_image_gen_style_reference_only.jpg?w=840&fit=max&auto=format&n=6BfRVG5RoFiMLPEQ&q=85&s=d09612e1f21d5b99febd75ce686f788e 840w, https://mintcdn.com/dripart/6BfRVG5RoFiMLPEQ/images/tutorial/flux/flux1_uso_reference_image_gen_style_reference_only.jpg?w=1100&fit=max&auto=format&n=6BfRVG5RoFiMLPEQ&q=85&s=76e13af6286a5566eda2b66a669d9f87 1100w, https://mintcdn.com/dripart/6BfRVG5RoFiMLPEQ/images/tutorial/flux/flux1_uso_reference_image_gen_style_reference_only.jpg?w=1650&fit=max&auto=format&n=6BfRVG5RoFiMLPEQ&q=85&s=675299a02175c33325ea77435434ba96 1650w, https://mintcdn.com/dripart/6BfRVG5RoFiMLPEQ/images/tutorial/flux/flux1_uso_reference_image_gen_style_reference_only.jpg?w=2500&fit=max&auto=format&n=6BfRVG5RoFiMLPEQ&q=85&s=5868e6410eed5e4b64453a0f0e0ea092 2500w" />
The only different is we replaced the `content reference` node and only use an `Empty Latent Image` node.

2. You can also bypass whole `Style Reference` group and use the workflow as a text to image workflow, which means this workflow has 4 variations

* Only use content (subject) reference
* Only use style reference
* Mixed content and style reference
* As a text to image workflow


# Flux.1 Krea Dev ComfyUI Workflow Tutorial
Source: https://docs.comfy.org/tutorials/flux/flux1-krea-dev

Best open-source FLUX model developed by Black Forest Labs in collaboration with Krea, focusing on unique aesthetic style and natural details, avoiding AI look, providing exceptional realism and image quality.

<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_1_krea_dev_poster.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=79640b1911971da78cf2bedb7b3b75ca" alt="Flux.1 Krea Dev Poster" data-og-width="1080" width="1080" data-og-height="1080" height="1080" data-path="images/tutorial/flux/flux_1_krea_dev_poster.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_1_krea_dev_poster.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=390f151f239fedcd7a2e146ca35f51da 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_1_krea_dev_poster.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=f1a852ee1703607ed39688d813bf3e9b 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_1_krea_dev_poster.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=308ab9d5ef0e1f83061eb5d74e531671 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_1_krea_dev_poster.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=b3cbd4470da7891978a4b8f5439c14ef 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_1_krea_dev_poster.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=ff33d86c5935c09641c4b708e80c5310 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_1_krea_dev_poster.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=4554765c9af26d7e9eb85ed813ab3f57 2500w" />

[Flux.1 Krea Dev](https://huggingface.co/black-forest-labs/FLUX.1-Krea-dev) is an advanced text-to-image generation model developed in collaboration between Black Forest Labs (BFL) and Krea. This is currently the best open-source FLUX model, specifically designed for text-to-image generation.

**Model Features**

* **Unique Aesthetic Style**: Focuses on generating images with unique aesthetics, avoiding common "AI look" appearance
* **Natural Details**: Does not produce blown-out highlights, maintaining natural detail representatio
* **Exceptional Realism**: Provides outstanding realism and image quality
* **Fully Compatible Architecture**: Fully compatible architecture design with FLUX.1 \[dev]

**Model License**
This model is released under the [flux-1-dev-non-commercial-license](https://huggingface.co/black-forest-labs/FLUX.1-Krea-dev/blob/main/LICENSE.md)

## Flux.1 Krea Dev ComfyUI Workflow

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

#### 1. Workflow Files

Download the image or JSON below and drag it into ComfyUI to load the corresponding workflow
![Flux Krea Dev Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/krea/flux1_krea_dev.png)

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/flux1_krea_dev.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download JSON Workflow</p>
</a>

#### 2. Manual Model Installation

Please download the following model files:
**Diffusion model**

* [flux1-krea-dev\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/FLUX.1-Krea-dev_ComfyUI/blob/main/split_files/diffusion_models/flux1-krea-dev_fp8_scaled.safetensors)

If you want to pursue higher quality and have enough VRAM, you can try the original model weights

* [flux1-krea-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Krea-dev/resolve/main/flux1-krea-dev.safetensors)

<Note>
  The `flux1-dev.safetensors` file requires agreeing to the [black-forest-labs/FLUX.1-Krea-dev](https://huggingface.co/black-forest-labs/FLUX.1-Krea-dev/) agreement before downloading via browser.
</Note>

If you have used Flux related workflows before, the following models are the same and don't need to be downloaded again

**Text encoders**

* [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
* [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true) Recommended when your VRAM is greater than 32GB.
* [t5xxl\_fp8\_e4m3fn.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn.safetensors)  For Low VRAM

**VAE**

* [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)

File save location:

```
ComfyUI/
 models/
    diffusion_models/
       flux1-krea-dev_fp8_scaled.safetensors or flux1-krea-dev.safetensors
    text_encoders/
       clip_l.safetensors
       t5xxl_fp16.safetensors or t5xxl_fp8_e4m3fn.safetensors
    vae/
       ae.safetensors

```

#### 3. Step-by-step Verification to Ensure Workflow Runs Properly

<Tip>
  For low VRAM users, this model may not run smoothly on your device, you can wait for the community to provide FP8 or GGUF version.
</Tip>

Please refer to the image below to ensure all model files have been loaded correctly

<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_1_krea_dev_guide.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=8475817d518de939bb6c5aabf8c1770e" alt="ComfyUI Flux Krea Dev Workflow" data-og-width="2814" width="2814" data-og-height="1612" height="1612" data-path="images/tutorial/flux/flux_1_krea_dev_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_1_krea_dev_guide.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=f1cd1077597c59efb7e11a44f9219456 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_1_krea_dev_guide.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=ea811a58031f7fcd99426c1c0b947a30 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_1_krea_dev_guide.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=69fffd2a3c686f40d6566e2a70cb4595 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_1_krea_dev_guide.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=f7b6f4b5fc50d7b92ea8155e2b8949a8 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_1_krea_dev_guide.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=7c17e7b576636ce9a86483cc6bcf70e7 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/flux/flux_1_krea_dev_guide.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=f198e11cc581093f11e1ff1ce37e2d11 2500w" />

1. Ensure that `flux1-krea-dev_fp8_scaled.safetensors` or `flux1-krea-dev.safetensors` is loaded in the `Load Diffusion Model` node
   * `flux1-krea-dev_fp8_scaled.safetensors` is recommended for low VRAM users
   * `flux1-krea-dev.safetensors` is the original weights, if you have enough VRAM like 24GB you can use it for better quality
2. Ensure the following models are loaded in the `DualCLIPLoader` node:
   * clip\_name1: t5xxl\_fp16.safetensors or t5xxl\_fp8\_e4m3fn.safetensors
   * clip\_name2: clip\_l.safetensors
3. Ensure that `ae.safetensors` is loaded in the `Load VAE` node
4. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow


# Cosmos Predict2 Text-to-Image ComfyUI Official Example
Source: https://docs.comfy.org/tutorials/image/cosmos/cosmos-predict2-t2i

This guide demonstrates how to complete Cosmos-Predict2 text-to-image workflow in ComfyUI

Cosmos-Predict2 is NVIDIA's next-generation physical world foundation model, specifically designed for high-quality visual generation and prediction tasks in physical AI scenarios.
The model features exceptional physical accuracy, environmental interactivity, and detail reproduction capabilities, enabling realistic simulation of complex physical phenomena and dynamic scenes.

Cosmos-Predict2 supports various generation methods including Text-to-Image (Text2Image) and Video-to-World (Video2World), and is widely used in industrial simulation, autonomous driving, urban planning, scientific research, and other fields.

GitHub:[Cosmos-predict2](https://github.com/nvidia-cosmos/cosmos-predict2)
huggingface: [Cosmos-Predict2](https://huggingface.co/collections/nvidia/cosmos-predict2-68028efc052239369a0f2959)

This guide will walk you through completing **text-to-image** workflow in ComfyUI.

For the video generation section, please refer to the following part:

<Card title="Cosmos Predict2 Video Generation" icon="book" href="/tutorials/video/cosmos/cosmos-predict2-video2world">
  Using Cosmos-Predict2  for video generation
</Card>

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>


# ComfyUI Native HiDream-E1, E1.1 Workflow Example
Source: https://docs.comfy.org/tutorials/image/hidream/hidream-e1

This guide will help you understand and complete the ComfyUI native HiDream-I1 text-to-image workflow example

![HiDream-E1 Demo](https://raw.githubusercontent.com/HiDream-ai/HiDream-E1/refs/heads/main/assets/demo.jpg)

HiDream-E1 is an interactive image editing large model officially open-sourced by HiDream-ai, built based on HiDream-I1.

It allows you to edit images using natural language. The model is released under the [MIT License](https://github.com/HiDream-ai/HiDream-E1?tab=MIT-1-ov-file), supporting use in personal projects, scientific research, and commercial applications.
In combination with the previously released [hidream-i1](/tutorials/image/hidream/hidream-i1), it enables **creative capabilities from image generation to editing**.

| Name            | Update Date | Inference Steps | Resolution            | HuggingFace Repository                                                  |
| --------------- | ----------- | --------------- | --------------------- | ----------------------------------------------------------------------- |
| HiDream-E1-Full | 2025-4-28   | 28              | 768x768               |  [HiDream-E1-Full](https://huggingface.co/HiDream-ai/HiDream-E1-Full) |
| HiDream-E1.1    | 2025-7-16   | 28              | Dynamic (1 Megapixel) |  [HiDream-E1.1](https://huggingface.co/HiDream-ai/HiDream-E1-1)       |

[HiDream E1 - Github](https://github.com/HiDream-ai/HiDream-E1)

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## HiDream E1 and E1.1 Workflow Related Models

All the models involved in this guide can be found [here](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/tree/main/split_files). Except for the Diffusion model, E1 and E1.1 use the same models.
The corresponding workflow files also include the relevant model information. You can choose to manually download and save the models, or follow the workflow prompts to download them after loading the workflow. It is recommended to use E1.1.

This model requires a large amount of VRAM to run. Please refer to the relevant sections for specific VRAM requirements.

**Diffusion Model**

You do not need to download both models. Since E1.1 is an iterative version based on E1, our tests show that its quality and performance are significantly improved compared to E1.

* [hidream\_e1\_1\_bf16.safetensors (Recommended)](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_e1_1_bf16.safetensors) 34.2GB
* [hidream\_e1\_full\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_e1_full_bf16.safetensors) 34.2GB

**Text Encoder**:

* [clip\_l\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_l_hidream.safetensors) 236.12MB
* [clip\_g\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_g_hidream.safetensors) 1.29GB
* [t5xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/t5xxl_fp8_e4m3fn_scaled.safetensors) 4.8GB
* [llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/llama_3.1_8b_instruct_fp8_scaled.safetensors) 8.46GB

**VAE**

* [ae.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/vae/ae.safetensors) 319.77MB

> This is the VAE model for Flux. If you have used the Flux workflow before, you may have already downloaded this file.

Model Save Location

```
 ComfyUI/
  models/
     text_encoders/
       clip_l_hidream.safetensors
       clip_g_hidream.safetensors
       t5xxl_fp8_e4m3fn_scaled.safetensors
       llama_3.1_8b_instruct_fp8_scaled.safetensors
     vae/
       ae.safetensors
     diffusion_models/
        hidream_e1_1_bf16.safetensors
        hidream_e1_full_bf16.safetensors
```

## HiDream E1.1 ComfyUI Native Workflow Example

E1.1 is an updated version released on July 16, 2025. This version supports dynamic 1-megapixel resolution, and the workflow uses the `Scale Image to Total Pixels` node to dynamically adjust the input image to 1 million pixels.

<Tip>
  Here are the VRAM usage references during testing:

  1. A100 40GB (VRAM usage 95%): First generation: 211s, second generation: 73s

  2. 4090D 24GB (VRAM usage 98%)

  * Full version: Out of memory
  * FP8\_e4m3fn\_fast (VRAM 98%) First generation: 120s, second generation: 91s
</Tip>

### 1. HiDream E1.1 Workflow and Related Materials

Download the image below and drag it into ComfyUI with the corresponding workflow and models loaded:
![HiDream E1.1 Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/image/hidream/e1.1/hidream_e1_1.png)

Download the image below as input:
![HiDream E1.1 Workflow Input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/image/hidream/e1.1/input.webp)

### 2. Step-by-step Guide to Running the HiDream-e1 Workflow

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/hidream/hidream-e1-1-guide.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=7cb0e9f7bd4629121f484f14e68e2841" alt="hidream_e1_1_guide" data-og-width="4148" width="4148" data-og-height="2916" height="2916" data-path="images/tutorial/image/hidream/hidream-e1-1-guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/hidream/hidream-e1-1-guide.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=345082b53e21270a783026072c4ef950 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/hidream/hidream-e1-1-guide.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=6a52a7c80d70d13ed104ac61b5a9af7d 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/hidream/hidream-e1-1-guide.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=3cda12787ff791f14a2eb90746687af8 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/hidream/hidream-e1-1-guide.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=84326c20c1ba5dd8b2733ffa4ac39703 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/hidream/hidream-e1-1-guide.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=a4782832f10b467aa09cc2fd554908c4 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/hidream/hidream-e1-1-guide.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=0878bb2ace3e4cd9ecc116ad86ea3ea5 2500w" />

Follow these steps to run the workflow:

1. Make sure the `Load Diffusion Model` node loads the `hidream_e1_1_bf16.safetensors` model.
2. Make sure the four corresponding text encoders in `QuadrupleCLIPLoader` are loaded correctly:
   * clip\_l\_hidream.safetensors
   * clip\_g\_hidream.safetensors
   * t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   * llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. Make sure the `Load VAE` node uses the `ae.safetensors` file.
4. In the `Load Image` node, load the provided input or your desired image.
5. In the `Empty Text Encoder(Positive)` node, enter **the modifications you want to make to the image**.
6. In the `Empty Text Encoder(Negative)` node, enter **the content you do not want to appear in the image**.
7. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute image generation.

### 3. Additional Notes on the Workflow

* Since HiDream E1.1 supports dynamic input with a total of 1 million pixels, the workflow uses `Scale Image to Total Pixels` to process and convert all input images, which may cause the aspect ratio to differ from the original input image.
* When using the fp16 version of the model, in actual tests, the full version ran out of memory on both A100 40GB and 4090D 24GB, so the workflow is set by default to use `fp8_e4m3fn_fast` for inference.

## HiDream E1 ComfyUI Native Workflow Example

E1 is a model released on April 28, 2025. This model only supports 768\*768 resolution.

<Tip>
  For reference, this workflow takes about 500s for the first run and 370s for the second run with 28 sampling steps on Google Colab L4 with 22.5GB VRAM.
</Tip>

### 1. HiDream-e1 workflow

Please download the image below and drag it into ComfyUI. The workflow already contains model download information, and after loading, it will prompt you to download the corresponding models.

![ComfyUI Native HiDream-e1 Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_e1/hidream_e1_full.png)

Download this image below as input:
![ComfyUI Native HiDream-e1 Workflow Input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_e1/input.webp)

### 2. Complete the HiDream-e1 Workflow Step by Step

<img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hidream/hidream_e1_full_step_guide.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=c218858bea510e5fd5ab71f7885cd7db" alt="hidream_e1_full_step_guide" data-og-width="2277" width="2277" data-og-height="1725" height="1725" data-path="images/tutorial/advanced/hidream/hidream_e1_full_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hidream/hidream_e1_full_step_guide.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=c5330924f662371659a846e0465cce44 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hidream/hidream_e1_full_step_guide.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=25ab45c23a4736ea6215efc616e69a90 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hidream/hidream_e1_full_step_guide.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=4a8921e2fe6e4939a27b66aec62aa33c 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hidream/hidream_e1_full_step_guide.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=5ca055a649e143526bbae0a8f2960699 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hidream/hidream_e1_full_step_guide.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=7727631573100f50a4baf1a39ac2fa07 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hidream/hidream_e1_full_step_guide.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=f939a5dd407c8cb786d8215281f14e0c 2500w" />

Follow these steps to complete the workflow:

1. Make sure the `Load Diffusion Model` node has loaded the `hidream_e1_full_bf16.safetensors` model
2. Ensure that the four corresponding text encoders are correctly loaded in the `QuadrupleCLIPLoader`
   * clip\_l\_hidream.safetensors
   * clip\_g\_hidream.safetensors
   * t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   * llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. Make sure the `Load VAE` node is using the `ae.safetensors` file
4. Load the input image we downloaded earlier in the `Load Image` node
5. (Important) Enter **the prompt for how you want to modify the image** in the `Empty Text Encoder(Positive)` node
6. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to generate the image

### Additional Notes on ComfyUI HiDream-e1 Workflow

* You may need to modify the prompt multiple times or generate multiple times to get better results
* This model has difficulty maintaining consistency when changing image styles, so try to make your prompts as complete as possible
* As the model supports a resolution of 768\*768, in actual testing with other dimensions, the image performance is poor or even significantly different at other dimensions


# ComfyUI Native HiDream-I1 Text-to-Image Workflow Example
Source: https://docs.comfy.org/tutorials/image/hidream/hidream-i1

This guide will walk you through completing a ComfyUI native HiDream-I1 text-to-image workflow example

![HiDream-I1 Demo](https://raw.githubusercontent.com/HiDream-ai/HiDream-I1/main/assets/demo.jpg)

HiDream-I1 is a text-to-image model officially open-sourced by HiDream-ai on April 7, 2025. The model has 17B parameters and is released under the [MIT license](https://github.com/HiDream-ai/HiDream-I1/blob/main/LICENSE), supporting personal projects, scientific research, and commercial use.
It currently performs excellently in multiple benchmark tests.

## Model Features

**Hybrid Architecture Design**
A combination of Diffusion Transformer (DiT) and Mixture of Experts (MoE) architecture:

* Based on Diffusion Transformer (DiT), with dual-stream MMDiT modules processing multimodal information and single-stream DiT modules optimizing global consistency.
* Dynamic routing mechanism flexibly allocates computing resources, enhancing complex scene processing capabilities and delivering excellent performance in color restoration, edge processing, and other details.

**Multimodal Text Encoder Integration**
Integrates four text encoders:

* OpenCLIP ViT-bigG, OpenAI CLIP ViT-L (visual semantic alignment)
* T5-XXL (long text parsing)
* Llama-3.1-8B-Instruct (instruction understanding)
  This combination achieves SOTA performance in complex semantic parsing of colors, quantities, spatial relationships, etc., with Chinese prompt support significantly outperforming similar open-source models.

**Original Model Versions**

HiDream-ai provides three versions of the HiDream-I1 model to meet different needs. Below are the links to the original model repositories:

| Model Name      | Description    | Inference Steps | Repository Link                                                         |
| --------------- | -------------- | --------------- | ----------------------------------------------------------------------- |
| HiDream-I1-Full | Full version   | 50              | [ HiDream-I1-Full](https://huggingface.co/HiDream-ai/HiDream-I1-Full) |
| HiDream-I1-Dev  | Distilled dev  | 28              | [ HiDream-I1-Dev](https://huggingface.co/HiDream-ai/HiDream-I1-Dev)   |
| HiDream-I1-Fast | Distilled fast | 16              | [ HiDream-I1-Fast](https://huggingface.co/HiDream-ai/HiDream-I1-Fast) |

## About This Workflow Example

In this example, we will use the repackaged version from ComfyOrg. You can find all the model files we'll use in this example in the [HiDream-I1\_ComfyUI](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/) repository.

<Tip>
  Before starting, please update your ComfyUI version to ensure it's at least after this [commit](https://github.com/comfyanonymous/ComfyUI/commit/9ad792f92706e2179c58b2e5348164acafa69288) to make sure your ComfyUI has native support for HiDream
</Tip>

## HiDream-I1 Workflow

The model requirements for different ComfyUI native HiDream-I1 workflows are basically the same, with only the [diffusion models](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/tree/main/split_files/diffusion_models) files being different.

If you don't know which version to choose, please refer to the following suggestions:

* **HiDream-I1-Full** can generate the highest quality images
* **HiDream-I1-Dev** balances high-quality image generation with speed
* **HiDream-I1-Fast** can generate images in just 16 steps, suitable for scenarios requiring real-time iteration

For the **dev** and **fast** versions, negative prompts are not needed, so please set the `cfg` parameter to `1.0` during sampling. We have noted the corresponding parameter settings in the relevant workflows.

<Tip>
  The full versions of all three versions require a lot of VRAM - you may need more than 27GB of VRAM to run them smoothly. In the corresponding workflow tutorials,
  we will use the **fp8** version as a demonstration example to ensure that most users can run it smoothly.
  However, we will still provide download links for different versions of the model in the corresponding examples, and you can choose the appropriate file based on your VRAM situation.
</Tip>

### Model Installation

The following model files are common files that we will use.
Please click on the corresponding links to download and save them according to the model file save location.
We will guide you to download the corresponding **diffusion models** in the corresponding workflows.

**text\_encoders**

* [clip\_l\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_l_hidream.safetensors)
* [clip\_g\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_g_hidream.safetensors)
* [t5xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/t5xxl_fp8_e4m3fn_scaled.safetensors) This model has been used in many workflows, you may have already downloaded this file.
* [llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/llama_3.1_8b_instruct_fp8_scaled.safetensors)

**VAE**

* [ae.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/vae/ae.safetensors) This is Flux's VAE model, if you have used Flux's workflow before, you may have already downloaded this file.

**diffusion models**
We will guide you to download the corresponding model files in the corresponding workflows.

Model file save location

```
 ComfyUI/
  models/
     text_encoders/
       clip_l_hidream.safetensors
       clip_g_hidream.safetensors
       t5xxl_fp8_e4m3fn_scaled.safetensors
       llama_3.1_8b_instruct_fp8_scaled.safetensors
     vae/
       ae.safetensors
     diffusion_models/
        ...               # We will guide you to install in the corresponding version workflow       
```

### HiDream-I1 Full Version Workflow

#### 1. Model File Download

Please select the appropriate version based on your hardware. Click the link and download the corresponding model file to save it to the `ComfyUI/models/diffusion_models/` folder.

* FP8 version: [hidream\_i1\_full\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_full_fp8.safetensors?download=true) requires more than 16GB of VRAM
* Full version: [hidream\_i1\_full\_f16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_full_fp16.safetensors?download=true) requires more than 27GB of VRAM

#### 2. Workflow File Download

Please download the image below and drag it into ComfyUI to load the corresponding workflow
![HiDream-I1 Full Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_i1/hidream_i1_full.png)

#### 3. Complete the Workflow Step by Step

<img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hidream/hidream_i1_full_flow_diagram.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=d7cd0444bdb290b4724bf01b80e5b1dd" alt="HiDream-I1 Full Version Flow Diagram" data-og-width="3000" width="3000" data-og-height="1620" height="1620" data-path="images/tutorial/advanced/hidream/hidream_i1_full_flow_diagram.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hidream/hidream_i1_full_flow_diagram.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=57444e4459fbac669797acf7075bd544 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hidream/hidream_i1_full_flow_diagram.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=e83e9e59c0847b984af3ad6b178e9494 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hidream/hidream_i1_full_flow_diagram.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=0af3aba1e4b43d3aeadf08304dc004c6 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hidream/hidream_i1_full_flow_diagram.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=b57f0178ad0cf8c1fa5f0490ebfdfcfc 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hidream/hidream_i1_full_flow_diagram.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=fce43e17a4779f1757f9a95948e24858 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hidream/hidream_i1_full_flow_diagram.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=6cae50583dbd7b6f091d7e28df1a64e8 2500w" />

Complete the workflow execution step by step

1. Make sure the `Load Diffusion Model` node is using the `hidream_i1_full_fp8.safetensors` file
2. Make sure the four corresponding text encoders in `QuadrupleCLIPLoader` are loaded correctly
   * clip\_l\_hidream.safetensors
   * clip\_g\_hidream.safetensors
   * t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   * llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. Make sure the `Load VAE` node is using the `ae.safetensors` file
4. For the **full** version, you need to set the `shift` parameter in `ModelSamplingSD3` to `3.0`
5. For the `Ksampler` node, you need to make the following settings
   * Set `steps` to `50`
   * Set `cfg` to `5.0`
   * (Optional) Set `sampler` to `lcm`
   * (Optional) Set `scheduler` to `normal`
6. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

### HiDream-I1 Dev Version Workflow

#### 1. Model File Download

Please select the appropriate version based on your hardware, click the link and download the corresponding model file to save to the `ComfyUI/models/diffusion_models/` folder.

* FP8 version: [hidream\_i1\_dev\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_dev_fp8.safetensors?download=true) requires more than 16GB of VRAM
* Full version: [hidream\_i1\_dev\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_dev_bf16.safetensors?download=true) requires more than 27GB of VRAM

#### 2. Workflow File Download

Please download the image below and drag it into ComfyUI to load the corresponding workflow

![HiDream-I1 Dev Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_i1/hidream_i1_dev.png)

#### 3. Complete the Workflow Step by Step

<img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hidream/hidream_i1_dev_flow_diagram.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=ec40487c298368e902ddac3549a23f09" alt="HiDream-I1 Dev Version Flow Diagram" data-og-width="3000" width="3000" data-og-height="1620" height="1620" data-path="images/tutorial/advanced/hidream/hidream_i1_dev_flow_diagram.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hidream/hidream_i1_dev_flow_diagram.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=60089a16b296f7931777fe5f516c75c5 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hidream/hidream_i1_dev_flow_diagram.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=c9a5c339380b2cda2c81ffc22c107ce2 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hidream/hidream_i1_dev_flow_diagram.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=68a3b8434c000845e1a3d21d60c2c295 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hidream/hidream_i1_dev_flow_diagram.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=572ccba5e3ae07b49d4afd76bebfd633 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hidream/hidream_i1_dev_flow_diagram.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=83780daf5a04294791d7209439e2eb04 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hidream/hidream_i1_dev_flow_diagram.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=19149609d7810003a767599a3845c702 2500w" />
Complete the workflow execution step by step

1. Make sure the `Load Diffusion Model` node is using the `hidream_i1_dev_fp8.safetensors` file
2. Make sure the four corresponding text encoders in `QuadrupleCLIPLoader` are loaded correctly
   * clip\_l\_hidream.safetensors
   * clip\_g\_hidream.safetensors
   * t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   * llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. Make sure the `Load VAE` node is using the `ae.safetensors` file
4. For the **dev** version, you need to set the `shift` parameter in `ModelSamplingSD3` to `6.0`
5. For the `Ksampler` node, you need to make the following settings
   * Set `steps` to `28`
   * (Important) Set `cfg` to `1.0`
   * (Optional) Set `sampler` to `lcm`
   * (Optional) Set `scheduler` to `normal`
6. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

### HiDream-I1 Fast Version Workflow

#### 1. Model File Download

Please select the appropriate version based on your hardware, click the link and download the corresponding model file to save to the `ComfyUI/models/diffusion_models/` folder.

* FP8 version: [hidream\_i1\_fast\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_fast_fp8.safetensors?download=true) requires more than 16GB of VRAM
* Full version: [hidream\_i1\_fast\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_fast_bf16.safetensors?download=true) requires more than 27GB of VRAM

#### 2. Workflow File Download

Please download the image below and drag it into ComfyUI to load the corresponding workflow

![HiDream-I1 Fast Version Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hidream_i1/hidream_i1_fast.png)

#### 3. Complete the Workflow Step by Step

<img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hidream/hidream_i1_fast_flow_diagram.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=8969d355742543b20158bbefa4705bfe" alt="HiDream-I1 Fast Version Flow Diagram" data-og-width="3000" width="3000" data-og-height="1620" height="1620" data-path="images/tutorial/advanced/hidream/hidream_i1_fast_flow_diagram.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hidream/hidream_i1_fast_flow_diagram.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=225090529328f08cae2a900e783a52f7 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hidream/hidream_i1_fast_flow_diagram.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=68c6898820435105c79bfbe3cd581f98 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hidream/hidream_i1_fast_flow_diagram.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=5029a867eafc2f0dc3ed3ea1947078cd 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hidream/hidream_i1_fast_flow_diagram.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=16ac4c99632a3eff8e3c270f5e5ae54c 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hidream/hidream_i1_fast_flow_diagram.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=44f8f8b29411aaefc3ce3b903f380abc 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hidream/hidream_i1_fast_flow_diagram.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=d49f08328c107693e3d1c6e85e6f9a0d 2500w" />

Complete the workflow execution step by step

1. Make sure the `Load Diffusion Model` node is using the `hidream_i1_fast_fp8.safetensors` file
2. Make sure the four corresponding text encoders in `QuadrupleCLIPLoader` are loaded correctly
   * clip\_l\_hidream.safetensors
   * clip\_g\_hidream.safetensors
   * t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   * llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. Make sure the `Load VAE` node is using the `ae.safetensors` file
4. For the **fast** version, you need to set the `shift` parameter in `ModelSamplingSD3` to `3.0`
5. For the `Ksampler` node, you need to make the following settings
   * Set `steps` to `16`
   * (Important) Set `cfg` to `1.0`
   * (Optional) Set `sampler` to `lcm`
   * (Optional) Set `scheduler` to `normal`
6. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## Other Related Resources

### GGUF Version Models

* [HiDream-I1-Full-gguf](https://huggingface.co/city96/HiDream-I1-Full-gguf)
* [HiDream-I1-Dev-gguf](https://huggingface.co/city96/HiDream-I1-Dev-gguf)

You need to use the Unet Loader (GGUF) node in City96's [ComfyUI-GGUF](https://github.com/city96/ComfyUI-GGUF) to replace the Load Diffusion Model node.

### NF4 Version Models

* [HiDream-I1-nf4](https://github.com/hykilpikonna/HiDream-I1-nf4)
* Use the [ComfyUI-HiDream-Sampler](https://github.com/SanDiegoDude/ComfyUI-HiDream-Sampler) node to use the NF4 version model.


# ComfyUI OmniGen2 Native Workflow Examples
Source: https://docs.comfy.org/tutorials/image/omnigen/omnigen2

ComfyUI OmniGen2 Native Workflow Examples - Unified text-to-image, image editing, and multi-image composition model.

## About OmniGen2

OmniGen2 is a powerful and efficient unified multimodal generation model with approximately **7B** total parameters (3B text model + 4B image generation model). Unlike OmniGen v1, OmniGen2 adopts an innovative dual-path Transformer architecture with completely independent text autoregressive model and image diffusion model, achieving parameter decoupling and specialized optimization.

### Model Highlights

* **Visual Understanding**: Inherits the powerful image content interpretation and analysis capabilities of the Qwen-VL-2.5 foundation model
* **Text-to-Image Generation**: Creates high-fidelity and aesthetically pleasing images from text prompts
* **Instruction-guided Image Editing**: Performs complex, instruction-based image modifications, achieving state-of-the-art performance among open-source models
* **Contextual Generation**: Versatile capabilities to process and flexibly combine diverse inputs (including people, reference objects, and scenes), producing novel and coherent visual outputs

### Technical Features

* **Dual-path Architecture**: Based on Qwen 2.5 VL (3B) text encoder + independent diffusion Transformer (4B)
* **Omni-RoPE Position Encoding**: Supports multi-image spatial positioning and identity distinction
* **Parameter Decoupling Design**: Avoids negative impact of text generation on image quality
* Support for complex text understanding and image understanding
* Controllable image generation and editing
* Excellent detail preservation capabilities
* Unified architecture supporting multiple image generation tasks
* Text generation capability: Can generate clear text content within images

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## OmniGen2 Model Download

Since this article involves different workflows, the corresponding model files and installation locations are as follows. The download information for model files is also included in the corresponding workflows:

**Diffusion Models**

* [omnigen2\_fp16.safetensors](https://huggingface.co/Comfy-Org/Omnigen2_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/omnigen2_fp16.safetensors)

**VAE**

* [ae.safetensors](https://huggingface.co/Comfy-Org/Lumina_Image_2.0_Repackaged/resolve/main/split_files/vae/ae.safetensors)

**Text Encoders**

* [qwen\_2.5\_vl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Omnigen2_ComfyUI_repackaged/resolve/main/split_files/text_encoders/qwen_2.5_vl_fp16.safetensors)

File save location:

```
 ComfyUI/
  models/
     diffusion_models/
       omnigen2_fp16.safetensors
     vae/
       ae.safetensors
     text_encoders/
        qwen_2.5_vl_fp16.safetensors
```

## ComfyUI OmniGen2 Text-to-Image Workflow

### 1. Download Workflow File

![Text-to-Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/image/omnigen2/image_omnigen2_t2i.png)

### 2. Complete Workflow Step by Step

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/omnigen/omnigen2_t2i_step_guide.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=0811f0f5d6dbe974749752b85fa7e3b9" alt="Workflow Step Guide" data-og-width="4148" width="4148" data-og-height="1576" height="1576" data-path="images/tutorial/image/omnigen/omnigen2_t2i_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/omnigen/omnigen2_t2i_step_guide.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=45ad3f6862e902ee370dae6e6c73a6f8 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/omnigen/omnigen2_t2i_step_guide.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=740f03d026f930d75bee360164fc5dac 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/omnigen/omnigen2_t2i_step_guide.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=102c411e373d18a82c28e4862bd8c1cd 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/omnigen/omnigen2_t2i_step_guide.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=ae539c85f4bc00c0a4b637f6e6fddf83 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/omnigen/omnigen2_t2i_step_guide.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=2ab1b75cafc02f71855e7719baaafd9f 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/omnigen/omnigen2_t2i_step_guide.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=0b2037bfacdd554dc19b67fa276dd876 2500w" />

Please follow the numbered steps in the image for step-by-step confirmation to ensure smooth operation of the corresponding workflow:

1. **Load Main Model**: Ensure the `Load Diffusion Model` node loads `omnigen2_fp16.safetensors`
2. **Load Text Encoder**: Ensure the `Load CLIP` node loads `qwen_2.5_vl_fp16.safetensors`
3. **Load VAE**: Ensure the `Load VAE` node loads `ae.safetensors`
4. **Set Image Dimensions**: Set the generated image dimensions in the `EmptySD3LatentImage` node (recommended 1024x1024)
5. **Input Prompts**:
   * Input positive prompts in the first `CLipTextEncode` node (content you want to appear in the image)
   * Input negative prompts in the second `CLipTextEncode` node (content you don't want to appear in the image)
6. **Start Generation**: Click the `Queue Prompt` button, or use the shortcut `Ctrl(cmd) + Enter` to execute text-to-image generation
7. **View Results**: After generation is complete, the corresponding images will be automatically saved to the `ComfyUI/output/` directory, and you can also preview them in the `SaveImage` node

## ComfyUI OmniGen2 Image Editing Workflow

OmniGen2 has rich image editing capabilities and supports adding text to images

### 1. Download Workflow File

![Text-to-Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/image/omnigen2/image_omnigen2_image_edit.png)

Download the image below, which we will use as the input image.
![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/image/omnigen2/input_fairy.png)

### 2. Complete Workflow Step by Step

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/omnigen/omnigen2_image_edit_step_guide.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=09899794b2d041a8c0e0775da557684a" alt="Workflow Step Guide" data-og-width="4056" width="4056" data-og-height="2808" height="2808" data-path="images/tutorial/image/omnigen/omnigen2_image_edit_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/omnigen/omnigen2_image_edit_step_guide.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=6f080c727c50b8152d8b00e92447cb29 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/omnigen/omnigen2_image_edit_step_guide.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=9daf1f956e69bc958427639852c9d1f2 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/omnigen/omnigen2_image_edit_step_guide.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=abfae5034a336d261b872d0cd18d9b8e 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/omnigen/omnigen2_image_edit_step_guide.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=5a1e3dc977d640a6c15beedcdad9991b 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/omnigen/omnigen2_image_edit_step_guide.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=9a9e59aa4a1b0ade064ad613d8428abf 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/omnigen/omnigen2_image_edit_step_guide.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=2cd7842cc0b4bb00b2416a257fffa06d 2500w" />

1. **Load Main Model**: Ensure the `Load Diffusion Model` node loads `omnigen2_fp16.safetensors`
2. **Load Text Encoder**: Ensure the `Load CLIP` node loads `qwen_2.5_vl_fp16.safetensors`
3. **Load VAE**: Ensure the `Load VAE` node loads `ae.safetensors`
4. **Upload Image**: Upload the provided image in the `Load Image` node
5. **Input Prompts**:
   * Input positive prompts in the first `CLipTextEncode` node (content you want to appear in the image)
   * Input negative prompts in the second `CLipTextEncode` node (content you don't want to appear in the image)
6. **Start Generation**: Click the `Queue Prompt` button, or use the shortcut `Ctrl(cmd) + Enter` to execute text-to-image generation
7. **View Results**: After generation is complete, the corresponding images will be automatically saved to the `ComfyUI/output/` directory, and you can also preview them in the `SaveImage` node

### 3. Additional Workflow Instructions

* If you want to enable the second image input, you can use the shortcut **Ctrl + B** to enable the corresponding node inputs for nodes that are in pink/purple state in the workflow
* If you want to customize dimensions, you can delete the `Get image size` node linked to the `EmptySD3LatentImage` node and input custom dimensions


# Qwen-Image ComfyUI Native Workflow Example
Source: https://docs.comfy.org/tutorials/image/qwen/qwen-image

Qwen-Image is a 20B parameter MMDiT (Multimodal Diffusion Transformer) model open-sourced under the Apache 2.0 license.

**Qwen-Image** is the first image generation foundation model released by Alibaba's Qwen team. It's a 20B parameter MMDiT (Multimodal Diffusion Transformer) model open-sourced under the Apache 2.0 license. The model has made significant advances in **complex text rendering** and **precise image editing**, achieving high-fidelity output for multiple languages including English and Chinese.

**Model Highlights**:

* **Excellent Multilingual Text Rendering**: Supports high-precision text generation in multiple languages including English, Chinese, Korean, Japanese, maintaining font details and layout consistency
* **Diverse Artistic Styles**: From photorealistic scenes to impressionist paintings, from anime aesthetics to minimalist design, fluidly adapting to various creative prompts

**Related Links**:

* [GitHub](https://github.com/QwenLM/Qwen-Image)
* [Hugging Face](https://huggingface.co/Qwen/Qwen-Image)
* [ModelScope](https://modelscope.cn/models/qwen/Qwen-Image)

Currently Qwen-Image has multiple ControlNet support options available:

* [Qwen-Image-DiffSynth-ControlNets/model\_patches](https://huggingface.co/Comfy-Org/Qwen-Image-DiffSynth-ControlNets/tree/main/split_files/model_patches): Includes canny, depth, and inpaint models
* [qwen\_image\_union\_diffsynth\_lora.safetensors](https://huggingface.co/Comfy-Org/Qwen-Image-DiffSynth-ControlNets/blob/main/split_files/loras/qwen_image_union_diffsynth_lora.safetensors): Image structure control LoRA supporting canny, depth, pose, lineart, softedge, normal, openpose
* InstantX ControlNet: To be updated

## ComfyOrg Qwen-Image live stream

**Qwen-Image in ComfyUI - Lightning & LoRAs**

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/WBFHwrpYRtY?si=uREGRhBDryTJBIry" title="Qwen-Image in ComfyUI - Lightning & LoRAs / August 15th, 2025" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

**Qwen-Image ControlNet in ComfyUI - DiffSynth**

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/bXMClHfEFn4?si=dcaNdqOMSwvu3t8x" title="Qwen-Image ControlNet in ComfyUI - DiffSynth / August 26th, 2025" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

## Qwen-Image Native Workflow Example

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

There are three different models used in the workflow attached to this document:

1. Qwen-Image original model fp8\_e4m3fn
2. 8-step accelerated version: Qwen-Image original model fp8\_e4m3fn with lightx2v 8-step LoRA
3. Distilled version: Qwen-Image distilled model fp8\_e4m3fn

**VRAM Usage Reference**
GPU: RTX4090D 24GB

| Model Used                            | VRAM Usage | First Generation | Second Generation |
| ------------------------------------- | ---------- | ---------------- | ----------------- |
| fp8\_e4m3fn                           | 86%        |  94s            |  71s             |
| fp8\_e4m3fn with lightx2v 8-step LoRA | 86%        |  55s            |  34s             |
| Distilled fp8\_e4m3fn                 | 86%        |  69s            |  36s             |

### 1. Workflow File

After updating ComfyUI, you can find the workflow file in the templates, or drag the workflow below into ComfyUI to load it.
![Qwen-image Text-to-Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/image/qwen/qwen-image.png)

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/image_qwen_image.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download Workflow for Qwen-Image Official Model</p>
</a>

Distilled version

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/image/qwen/image_qwen_image_distill.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download Workflow for Distilled Model</p>
</a>

### 2. Model Download

**Available Models in ComfyUI**

* Qwen-Image\_bf16 (40.9 GB)
* Qwen-Image\_fp8 (20.4 GB)
* Distilled versions (non-official, requires only 15 steps)

All models are available at [Huggingface](https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/tree/main) and [Modelscope](https://modelscope.cn/models/Comfy-Org/Qwen-Image_ComfyUI/files)

**Diffusion model**

* [qwen\_image\_fp8\_e4m3fn.safetensors](https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/resolve/main/split_files/diffusion_models/qwen_image_fp8_e4m3fn.safetensors)

Qwen\_image\_distill

* [qwen\_image\_distill\_full\_fp8\_e4m3fn.safetensors](https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/resolve/main/non_official/diffusion_models/qwen_image_distill_full_fp8_e4m3fn.safetensors)
* [qwen\_image\_distill\_full\_bf16.safetensors](https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/resolve/main/non_official/diffusion_models/qwen_image_distill_full_bf16.safetensors)

<Note>
  - The original author of the distilled version recommends using 15 steps with cfg 1.0.
  - According to tests, this distilled version also performs well at 10 steps with cfg 1.0. You can choose either euler or res\_multistep based on the type of image you want.
</Note>

**LoRA**

* [Qwen-Image-Lightning-8steps-V1.0.safetensors](https://huggingface.co/lightx2v/Qwen-Image-Lightning/resolve/main/Qwen-Image-Lightning-8steps-V1.0.safetensors)

**Text encoder**

* [qwen\_2.5\_vl\_7b\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/resolve/main/split_files/text_encoders/qwen_2.5_vl_7b_fp8_scaled.safetensors)

**VAE**

[qwen\_image\_vae.safetensors](https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/resolve/main/split_files/vae/qwen_image_vae.safetensors)

**Model Storage Location**

```
 ComfyUI/
  models/
     diffusion_models/
       qwen_image_fp8_e4m3fn.safetensors
       qwen_image_distill_full_fp8_e4m3fn.safetensors ## 
     loras/
       Qwen-Image-Lightning-8steps-V1.0.safetensors   ## 8 LoRA 
     vae/
       qwen_image_vae.safetensors
     text_encoders/
        qwen_2.5_vl_7b_fp8_scaled.safetensors
```

### 3. Workflow Instructions

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image-guide.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=123e11f9be1c31a9e3a3c0ff1df299da" alt="Step Guide" data-og-width="3111" width="3111" data-og-height="1829" height="1829" data-path="images/tutorial/image/qwen/image_qwen_image-guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image-guide.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=fd031c2e037ec5ada7bfc72d5dab4f32 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image-guide.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=890775205ed1e33ad3de3e01136e1c73 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image-guide.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=ffb8a3d81da16b3c663cc7d6ca3a1e86 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image-guide.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=807ca8d050cb51f8f4b5057a00d6ad41 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image-guide.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=2938f2fc67819563457a78ebea810aae 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image-guide.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=2b234cc0550c4157b194d76f8e1066ee 2500w" />

1. Make sure the `Load Diffusion Model` node has loaded `qwen_image_fp8_e4m3fn.safetensors`
2. Make sure the `Load CLIP` node has loaded `qwen_2.5_vl_7b_fp8_scaled.safetensors`
3. Make sure the `Load VAE` node has loaded `qwen_image_vae.safetensors`
4. Make sure the `EmptySD3LatentImage` node is set with the correct image dimensions
5. Set your prompt in the `CLIP Text Encoder` node; currently, it supports at least English, Chinese, Korean, Japanese, Italian, etc.
6. If you want to enable the 8-step acceleration LoRA by lightx2v, select the node and use `Ctrl + B` to enable it, and modify the Ksampler settings as described in step 8
7. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow
8. For different model versions and workflows, adjust the KSampler parameters accordingly

<Note>
  The distilled model and the 8-step acceleration LoRA by lightx2v do not seem to be compatible for simultaneous use. You can experiment with different combinations to verify if they can be used together.
</Note>

## Qwen Image InstantX ControlNet Workflow

This is a ControlNet model, so you can use it as normal ControlNet.

### 1. Workflow and Input Images

Download the image below and drag it into ComfyUI to load the workflow
![workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/image/qwen/qwen-image-instantx-controlnet/image_qwen_image_instantx_controlnet.png)

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/image_qwen_image_instantx_controlnet.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download JSON Format Workflow</p>
</a>

Download the image below as input
![input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/image/qwen/qwen-image-instantx-controlnet/input.jpg)

### 2. Model Links

1. InstantX Controlnet

Download [Qwen-Image-InstantX-ControlNet-Union.safetensors](https://huggingface.co/Comfy-Org/Qwen-Image-InstantX-ControlNets/resolve/main/split_files/controlnet/Qwen-Image-InstantX-ControlNet-Union.safetensors) and save it to the `ComfyUI/models/controlnet/` folder

2. **Lotus Depth model**

We will use this model to generate the depth map of the image. The following two models need to be downloaded:

**Diffusion Model**

* [lotus-depth-d-v1-1.safetensors](https://huggingface.co/Comfy-Org/lotus/resolve/main/lotus-depth-d-v1-1.safetensors)

**VAE Model**

* [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors)  or any SD1.5 VAE

```
ComfyUI/
 models/
    diffusion_models/
       lotus-depth-d-v1-1.safetensors
    vae/
         lvae-ft-mse-840000-ema-pruned.safetensors
```

> You can also use custom nodes like [comfyui\_controlnet\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux) to generate depth map.

### 3. Workflow Instructions

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image_instantx_controlnet.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=a73a3f375691a80eb87a6cec12c48ad9" alt="Process Instructions" data-og-width="3800" width="3800" data-og-height="1730" height="1730" data-path="images/tutorial/image/qwen/image_qwen_image_instantx_controlnet.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image_instantx_controlnet.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=d1c3841692bb9868512816925f74ba85 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image_instantx_controlnet.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=4633d161d5b014a83c5cf2deec05ea2f 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image_instantx_controlnet.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=fbd0674ae9fd596c5975e6c02d235136 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image_instantx_controlnet.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=dae55f9d59a7d33e3bf20ca9a032aabf 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image_instantx_controlnet.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=56b395e407793ea121f6f1497c45316a 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image_instantx_controlnet.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=b6eceab00f771c33de88f74f8e498d19 2500w" />

1. Ensure that the `Load ControlNet Model` node correctly loads the `Qwen-Image-InstantX-ControlNet-Union.safetensors` model
2. Upload input image
3. This subgraph uses the Lotus Depth model. You can find it in the templates or edit the subgraph to learn more, make sure all the models are loaded correctly
4. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## Qwen Image ControlNet DiffSynth-ControlNets Model Patches Workflow

This model is actually not a ControlNet, but a Model patch that supports three different control modes: canny, depth, and inpaint.

Original model address: [DiffSynth-Studio/Qwen-Image ControlNet](https://www.modelscope.cn/collections/Qwen-Image-ControlNet-6157b44e89d444)
Comfy Org rehost address: [Qwen-Image-DiffSynth-ControlNets/model\_patches](https://huggingface.co/Comfy-Org/Qwen-Image-DiffSynth-ControlNets/tree/main/split_files/model_patches)

### 1. Workflow and Input Images

Download the image below and drag it into ComfyUI to load the corresponding workflow
![workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/image/qwen/qwen-image-controlnet-model-patch/image_qwen_image_controlnet_patch.png)

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/image_qwen_image_controlnet_patch.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download JSON Format Workflow</p>
</a>

Download the image below as input:

![input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/image/qwen/qwen-image-controlnet-model-patch/input.png)

### 2. Model Links

Other models are the same as the Qwen-Image basic workflow. You only need to download the models below and save them to the `ComfyUI/models/model_patches` folder

* [qwen\_image\_canny\_diffsynth\_controlnet.safetensors](https://huggingface.co/Comfy-Org/Qwen-Image-DiffSynth-ControlNets/resolve/main/split_files/model_patches/qwen_image_canny_diffsynth_controlnet.safetensors)
* [qwen\_image\_depth\_diffsynth\_controlnet.safetensors](https://huggingface.co/Comfy-Org/Qwen-Image-DiffSynth-ControlNets/resolve/main/split_files/model_patches/qwen_image_depth_diffsynth_controlnet.safetensors)
* [qwen\_image\_inpaint\_diffsynth\_controlnet.safetensors](https://huggingface.co/Comfy-Org/Qwen-Image-DiffSynth-ControlNets/resolve/main/split_files/model_patches/qwen_image_inpaint_diffsynth_controlnet.safetensors)

### 3. Workflow Usage Instructions

Currently, diffsynth has three patch models: Canny, Depth, and Inpaint.

If you're using ControlNet-related workflows for the first time, you need to understand that control images need to be preprocessed into supported image formats before they can be used and recognized by the model.

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/controlnet_input_types.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=3459806aaf59d27e2e1554c4bd2d0de7" alt="Input Type Diagram" data-og-width="5920" width="5920" data-og-height="2438" height="2438" data-path="images/tutorial/image/qwen/controlnet_input_types.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/controlnet_input_types.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=98a48facb7eecc0ecf0cb783d620a17c 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/controlnet_input_types.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=dc05aface8427790488a1db20060354f 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/controlnet_input_types.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=fd4d165a1edd3ce9459f5210155c37b5 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/controlnet_input_types.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=3dc34a088c42b62b73cf344071944115 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/controlnet_input_types.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=ec72ea0b9fa642056de6f6700df2bbf9 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/controlnet_input_types.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=f3a9a233cc4c513b7818b9463fae0ac4 2500w" />

* Canny: Processed canny edge, line art contours
* Depth: Preprocessed depth map showing spatial relationships
* Inpaint: Requires using Mask to mark areas that need to be repainted

Since this patch model is divided into three different models, you need to select the correct preprocessing type when inputting to ensure proper image preprocessing.

**Canny Model ControlNet Usage Instructions**

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image_controlnet_patch-canny.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=ca49f6ba72d3897cb14614314bc0d3c8" alt="Canny Workflow" data-og-width="3800" width="3800" data-og-height="2046" height="2046" data-path="images/tutorial/image/qwen/image_qwen_image_controlnet_patch-canny.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image_controlnet_patch-canny.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=1dd6c853b2c2b9cc69877f3f3088a583 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image_controlnet_patch-canny.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=b25f3d2ecff52c8a63ee0b671200b38e 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image_controlnet_patch-canny.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=3a09344131390ad832d0948cac2f5d32 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image_controlnet_patch-canny.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=48aedbe0e779c62f482183934754ba33 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image_controlnet_patch-canny.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=bc723145ccca9fa5f4b766d49da10ae8 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image_controlnet_patch-canny.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=6f5f3196d120979c783e841fb2deb82f 2500w" />

1. Ensure that `qwen_image_canny_diffsynth_controlnet.safetensors` is loaded
2. Upload input image for subsequent processing
3. The Canny node is a native preprocessing node that will preprocess the input image according to your set parameters to control generation
4. If needed, you can modify the `strength` in the `QwenImageDiffsynthControlnet` node to control the intensity of line art control
5. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

> For using qwen\_image\_depth\_diffsynth\_controlnet.safetensors, you need to preprocess the image into a depth map and replace the `image processing` part. For this usage, please refer to the InstantX processing method in this document. Other parts are similar to using the Canny model.

**Inpaint Model ControlNet Usage Instructions**
<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image_controlnet_patch-inpaint.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=c3a3493ab00a11eba1e634624b40a2c1" alt="Inpaint Workflow" data-og-width="3808" width="3808" data-og-height="2046" height="2046" data-path="images/tutorial/image/qwen/image_qwen_image_controlnet_patch-inpaint.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image_controlnet_patch-inpaint.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=2cd61275d4f008dd88c8f80d79e8abc4 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image_controlnet_patch-inpaint.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=85f2dcdc8fce631cc8977a168af9f580 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image_controlnet_patch-inpaint.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=7d330a90db88bf316006c5f17dba19f0 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image_controlnet_patch-inpaint.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=74930608b678ed937bf707bb02341fd6 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image_controlnet_patch-inpaint.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=3c3e6b0170c96a481cb1a969bc891f4b 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image_controlnet_patch-inpaint.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=7ec9aeca94135ef2945033492dfbad82 2500w" />

For the Inpaint model, it requires using the [Mask Editor](/interface/maskeditor) to draw a mask and use it as input control condition.

1. Ensure that `ModelPatchLoader` loads the `qwen_image_inpaint_diffsynth_controlnet.safetensors` model
2. Upload image and use the [Mask Editor](/interface/maskeditor) to draw a mask. You need to connect the `mask` output of the corresponding `Load Image` node to the `mask` input of `QwenImageDiffsynthControlnet` to ensure the corresponding mask is loaded
3. Use the `Ctrl-B` shortcut to set the original Canny in the workflow to bypass mode, making the corresponding Canny node processing ineffective
4. In `CLIP Text Encoder`, input what you want to change the masked area to
5. If needed, you can modify the `strength` in the `QwenImageDiffsynthControlnet` node to control the corresponding control intensity
6. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## Qwen Image Union ControlNet LoRA Workflow

Original model address: [DiffSynth-Studio/Qwen-Image-In-Context-Control-Union](https://www.modelscope.cn/models/DiffSynth-Studio/Qwen-Image-In-Context-Control-Union/)
Comfy Org rehost address: [qwen\_image\_union\_diffsynth\_lora.safetensors](https://huggingface.co/Comfy-Org/Qwen-Image-DiffSynth-ControlNets/blob/main/split_files/loras/qwen_image_union_diffsynth_lora.safetensors): Image structure control LoRA supporting canny, depth, pose, lineart, softedge, normal, openpose

### 1. Workflow and Input Images

Download the image below and drag it into ComfyUI to load the workflow
![workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/image/qwen/qwen-image-union-control-lora/image_qwen_image_union_control_lora.png)

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/image_qwen_image_union_control_lora.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download JSON Format Workflow</p>
</a>

Download the image below as input

![workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/image/qwen/qwen-image-union-control-lora/input.png)

### 2. Model Links

Download the model below. Since this is a LoRA model, it needs to be saved to the `ComfyUI/models/loras/` folder

* [qwen\_image\_union\_diffsynth\_lora.safetensors](https://huggingface.co/Comfy-Org/Qwen-Image-DiffSynth-ControlNets/blob/main/split_files/loras/qwen_image_union_diffsynth_lora.safetensors): Image structure control LoRA supporting canny, depth, pose, lineart, softedge, normal, openpose

### 3. Workflow Instructions

This model is a unified control LoRA that supports canny, depth, pose, lineart, softedge, normal, openpose controls. Since many image preprocessing native nodes are not fully supported, you should use something like [comfyui\_controlnet\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux) to complete other image preprocessing.

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image_union_control_lora.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=428cafa03e7759a771c9f1e63d759dd0" alt="Union Control LoRA" data-og-width="3800" width="3800" data-og-height="2238" height="2238" data-path="images/tutorial/image/qwen/image_qwen_image_union_control_lora.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image_union_control_lora.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=e964f8cad5f3b943f677dc4c7d5ef992 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image_union_control_lora.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=8ed100d1fc4a43f99736ced29d2d51e3 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image_union_control_lora.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=5316d6291981443405e65adb048b2f6f 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image_union_control_lora.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=58bd2cda362ebf0909786bb5cdb6f7ac 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image_union_control_lora.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=7e1152f424d5746e13ffeb149b3336ee 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/image_qwen_image_union_control_lora.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=c212ec01f142df185da8ce853571b99f 2500w" />

1. Ensure that `LoraLoaderModelOnly` correctly loads the `qwen_image_union_diffsynth_lora.safetensors` model
2. Upload input image
3. If needed, you can adjust the `Canny` node parameters. Since different input images require different parameter settings to get better image preprocessing results, you can try adjusting the corresponding parameter values to get more/fewer details
4. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

> For other types of control, you also need to replace the image processing part.


# Qwen-Image-Edit ComfyUI Native Workflow Example
Source: https://docs.comfy.org/tutorials/image/qwen/qwen-image-edit

Qwen-Image-Edit is the image editing version of Qwen-Image, further trained based on the 20B model, supporting precise text editing and dual semantic/appearance editing capabilities.

**Qwen-Image-Edit** is the image editing version of Qwen-Image. It is further trained based on the 20B Qwen-Image model, successfully extending Qwen-Image's unique text rendering capabilities to editing tasks, enabling precise text editing. In addition, Qwen-Image-Edit feeds the input image into both Qwen2.5-VL (for visual semantic control) and the VAE Encoder (for visual appearance control), thus achieving dual semantic and appearance editing capabilities.

**Model Features**

Features include:

* Precise Text Editing: Qwen-Image-Edit supports bilingual (Chinese and English) text editing, allowing direct addition, deletion, and modification of text in images while preserving the original text size, font, and style.
* Dual Semantic/Appearance Editing: Qwen-Image-Edit supports not only low-level visual appearance editing (such as style transfer, addition, deletion, modification, etc.) but also high-level visual semantic editing (such as IP creation, object rotation, etc.).
* Strong Cross-Benchmark Performance: Evaluations on multiple public benchmarks show that Qwen-Image-Edit achieves SOTA in editing tasks, making it a powerful foundational model for image generation.

**Official Links**:

* [GitHub Repository](https://github.com/QwenLM/Qwen-Image)
* [Hugging Face](https://huggingface.co/Qwen/Qwen-Image-Edit)
* [ModelScope](https://modelscope.cn/models/qwen/Qwen-Image-Edit)

## ComfyOrg Qwen-Image-Edit Live Stream

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/TZIijn-tvoc?si=Vb-ZNwTvJC67_UEE" title="Qwen-Image Edit in ComfyUI - Image Editing Model / August 19th, 2025" frameborder="0" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen />

## Qwen-Image-Edit ComfyUI Native Workflow Example

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

### 1. Workflow File

After updating ComfyUI, you can find the workflow file from the templates, or drag the workflow below into ComfyUI to load it.
![Qwen-image Text-to-Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/image/qwen/qwen-image-edit/qwen_image_edit.png)

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/image_qwen_image_edit.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download JSON Workflow</p>
</a>

Download the image below as input
![Qwen-image Text-to-Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/image/qwen/qwen-image-edit/input.png)

### 2. Model Download

All models can be found at [Comfy-Org/Qwen-Image\_ComfyUI](https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/tree/main) or [Comfy-Org/Qwen-Image-Edit\_ComfyUI](https://huggingface.co/Comfy-Org/Qwen-Image-Edit_ComfyUI)

**Diffusion model**

* [qwen\_image\_edit\_fp8\_e4m3fn.safetensors](https://huggingface.co/Comfy-Org/Qwen-Image-Edit_ComfyUI/resolve/main/split_files/diffusion_models/qwen_image_edit_fp8_e4m3fn.safetensors)

**LoRA**

* [Qwen-Image-Lightning-4steps-V1.0.safetensors](https://huggingface.co/lightx2v/Qwen-Image-Lightning/resolve/main/Qwen-Image-Lightning-4steps-V1.0.safetensors)

**Text encoder**

* [qwen\_2.5\_vl\_7b\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/resolve/main/split_files/text_encoders/qwen_2.5_vl_7b_fp8_scaled.safetensors)

**VAE**

* [qwen\_image\_vae.safetensors](https://huggingface.co/Comfy-Org/Qwen-Image_ComfyUI/resolve/main/split_files/vae/qwen_image_vae.safetensors)

Model Storage Location

```
 ComfyUI/
  models/
     diffusion_models/
       qwen_image_edit_fp8_e4m3fn.safetensors
     loras/
       Qwen-Image-Lightning-4steps-V1.0.safetensors
     vae/
       qwen_image_vae.safetensors
     text_encoders/
        qwen_2.5_vl_7b_fp8_scaled.safetensors
```

### 3. Follow the Steps to Complete the Workflow

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/qwen_image_edit.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=98a706bfa8f1578a4dfd7f2a0a415926" alt="Steps Diagram" data-og-width="3782" width="3782" data-og-height="2196" height="2196" data-path="images/tutorial/image/qwen/qwen_image_edit.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/qwen_image_edit.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=19405afcc977851e15bcfc3b94820416 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/qwen_image_edit.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=a0720956a5358e98678346edee183065 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/qwen_image_edit.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=45c6c3c2eba0b3978be8b870bd120f3e 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/qwen_image_edit.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=38a3d11d44454d7cba7aa82bad0e5882 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/qwen_image_edit.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=94d0ab046cf6c37737b9b1d3ccd4639c 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/image/qwen/qwen_image_edit.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=ae5925edff9f4889665f7436821e9d77 2500w" />

1. Model Loading
   * Ensure the `Load Diffusion Model` node loads `qwen_image_edit_fp8_e4m3fn.safetensors`
   * Ensure the `Load CLIP` node loads `qwen_2.5_vl_7b_fp8_scaled.safetensors`
   * Ensure the `Load VAE` node loads `qwen_image_vae.safetensors`
2. Image Loading
   * Ensure the `Load Image` node uploads the image to be edited
3. Prompt Setting
   * Set the prompt in the `CLIP Text Encoder` node
4. The Scale Image to Total Pixels node will scale your input image to a total of one million pixels,
   * Mainly to avoid quality loss in output images caused by oversized input images such as 2048x2048
   * If you are familiar with your input image size, you can bypass this node using `Ctrl+B`
5. If you want to use the 4-step Lighting LoRA to speed up image generation, you can select the `LoraLoaderModelOnly` node and press `Ctrl+B` to enable it
6. For the `steps` and `cfg` settings of the Ksampler node, we've added a note below the node where you can test the optimal parameter settings
7. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow


# Flux 1.1 Pro Ultra Image API Node ComfyUI Official Workflow Examples
Source: https://docs.comfy.org/tutorials/partner-nodes/black-forest-labs/flux-1-1-pro-ultra-image

This guide covers how to use the Flux 1.1 Pro Ultra Image Partner node in ComfyUI

FLUX 1.1 Pro Ultra is a high-performance AI image generation tool by BlackForestLabs, featuring ultra-high resolution and efficient generation capabilities. It supports up to 4MP resolution (4x the standard version) while keeping single image generation time under 10 seconds - 2.5x faster than similar high-resolution models.

The tool offers two core modes:

* **Ultra Mode**: Designed for high-resolution needs, perfect for advertising and e-commerce where detail magnification is important. It accurately reflects prompts while maintaining generation speed.
* **Raw Mode**: Focuses on natural realism, optimizing skin tones, lighting, and landscape details. Reduces the "AI look" and is ideal for photography and realistic style creation.

We now support the Flux 1.1 Pro Ultra Image node in ComfyUI. This guide will cover:

* Flux 1.1 Pro Text-to-Image
* Flux 1.1 Pro Image-to-Image (Remix)

<Tip>
  To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](/tutorials/partner-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.
</Tip>

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Flux 1.1 Pro Ultra Image Node Documentation

Check the following documentation for detailed node parameter settings:

* [Flux 1.1 Pro Ultra Image](/images/built-in-nodes/api_nodes/bfl/flux-1-1-pro-ultra-image.jpg)

## Flux 1.1 \[pro] Text-to-Image Tutorial

### 1. Download Workflow File

Download and drag the following file into ComfyUI to load the workflow:

![Flux 1.1 pro Text-to-Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_1_pro_t2i.png)

### 2. Complete the Workflow Steps

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_1_pro_t2i_step_guide.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=b9403dd3baf434686961672addef2a39" alt="Workflow Steps" data-og-width="3030" width="3030" data-og-height="1370" height="1370" data-path="images/tutorial/api_nodes/bfl/flux_1_1_pro_t2i_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_1_pro_t2i_step_guide.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=1dcf01f603e0a9405e06db0f203f6b2b 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_1_pro_t2i_step_guide.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=475c7975f874a7fb2bc09755400ac4e0 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_1_pro_t2i_step_guide.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=2d73cf69387fdf0e83acebf0361a8ceb 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_1_pro_t2i_step_guide.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=a59ca6310db555c0f8a3be86b7f76f42 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_1_pro_t2i_step_guide.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=c1790db75c5aaf75635767c055599d12 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_1_pro_t2i_step_guide.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=fae50fb73c4fa303b17c6a4ee900bcfd 2500w" />

Follow the numbered steps to complete the basic workflow:

1. (Optional) Modify the prompt in the `Flux 1.1 [pro] Ultra Image` node
2. (Optional) Set `raw` parameter to `false` for more realistic output
3. Click `Run` or use shortcut `Ctrl(cmd) + Enter` to generate the image
4. After the API returns results, view the generated image in the `Save Image` node. Images are saved to the `ComfyUI/output/` directory

## Flux 1.1\[pro] Image-to-Image Tutorial

When adding an `image_prompt` to the node input, the output will blend features from the input image (Remix). The `image_prompt_strength` value affects the blend ratio: higher values make the output more similar to the input image.

### 1. Download Workflow File

Download and drag the following file into ComfyUI, or right-click the purple node in the Text-to-Image workflow and set `mode` to `always` to enable `image_prompt` input:

![Flux 1.1 pro Image-to-Image Remix](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_1_pro_i2i.png)

We'll use this image as input:
![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/openai-dall-e-3/text2image.png)

### 2. Complete the Workflow Steps

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_1_pro_i2i_step_guide.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=0217606be6c3f599e3d6ebd2fb14829a" alt="Workflow Steps" data-og-width="2825" width="2825" data-og-height="1076" height="1076" data-path="images/tutorial/api_nodes/bfl/flux_1_1_pro_i2i_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_1_pro_i2i_step_guide.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=ea1043fe60afb4874f2a0cfa7bfaaf88 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_1_pro_i2i_step_guide.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=a326c96e4efbd900ad8889d6664465e6 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_1_pro_i2i_step_guide.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=1583add49f0c0dd731630ddab7564d9c 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_1_pro_i2i_step_guide.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=61e7728a9cf17f93396d1af290ae15af 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_1_pro_i2i_step_guide.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=2dbfbff8d7cb2552ed5daad1e6d1e8c6 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_1_pro_i2i_step_guide.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=ea4e5fc6ce0dd3dfa287970ed4bc7b29 2500w" />

Follow these numbered steps:

1. Click **Upload** on the `Load Image` node to upload your input image
2. (Optional) Adjust `image_prompt_strength` in `Flux 1.1 [pro] Ultra Image` to change the blend ratio
3. Click `Run` or use shortcut `Ctrl(cmd) + Enter` to generate the image
4. After the API returns results, view the generated image in the `Save Image` node. Images are saved to the `ComfyUI/output/` directory

Here's a comparison of outputs with different `image_prompt_strength` values:
<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_1_pro_image_prompt_strength.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=eea7014b92e63f0b8d1bb50d5dae11c3" alt="Comparison" data-og-width="3150" width="3150" data-og-height="1173" height="1173" data-path="images/tutorial/api_nodes/bfl/flux_1_1_pro_image_prompt_strength.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_1_pro_image_prompt_strength.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=c3fd516ee3fbd72d0d7d5d408a2e3841 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_1_pro_image_prompt_strength.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=4988d52a69ca1108662a380aabfc3bec 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_1_pro_image_prompt_strength.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=b85cf288f13cad89155e50242583eb1a 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_1_pro_image_prompt_strength.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=24dd19e8afa467fb104b8dd75352afd6 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_1_pro_image_prompt_strength.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=66c3a578ec5b785123d53d9bfa5b3b29 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_1_pro_image_prompt_strength.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=4e199e171ba2aaf90d64bc1e911cd2b0 2500w" />


# ComfyUI Flux.1 Kontext Pro Image API Node Official Example
Source: https://docs.comfy.org/tutorials/partner-nodes/black-forest-labs/flux-1-kontext

This guide will show you how to use the Flux.1 Kontext Pro Image Partner node in ComfyUI to perform image editing

FLUX.1 Kontext is a professional image-to-image editing model developed by Black Forest Labs, focusing on intelligent understanding of image context and precise editing.
It can perform various editing tasks without complex descriptions, including object modification, style transfer, background replacement, character consistency editing, and text editing.
The core advantage of Kontext lies in its excellent context understanding ability and character consistency maintenance, ensuring that key elements such as character features and composition layout remain stable even after multiple iterations of editing.

Currently, ComfyUI has supported two models of Flux.1 Kontext:

* **Kontext Pro** is ideal for editing, composing, and remixing.
* **Kontext Max** pushes the limits on typography, prompt precision, and speed.

In this guide, we will briefly introduce how to use the Flux.1 Kontext Partner nodes to perform image editing through corresponding workflows.

<Tip>
  To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](/tutorials/partner-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.
</Tip>

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Flux.1 Kontext Multiple Image Input Workflow

We have recently updated to support multiple image input workflows. Using the new `Image Stitch` node, you can stitch multiple images into a single image and edit it using Flux.1 Kontext.

### 1. Workflow File Download

The `metadata` of the images below contains the workflow information. Please download and drag them into ComfyUI to load the corresponding workflow.

![ComfyUI Flux.1 Kontext Pro Image API Node Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/multiple_image_input/multiple_image_input.png)

Download the following images for input or use your own images:

![ComfyUI Flux.1 Kontext Pro Image API Node Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/multiple_image_input/girl.jpg)
![ComfyUI Flux.1 Kontext Pro Image API Node Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/multiple_image_input/dog.jpg)
![ComfyUI Flux.1 Kontext Pro Image API Node Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/multiple_image_input/sofa.jpg)

### 2. Complete the Workflow Step by Step

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_kontext_multiple_image_input_guide.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=bf6e2ac1877f37556b354699f1960129" alt="ComfyUI Flux.1 Kontext Pro Image API Node Workflow Steps" data-og-width="3256" width="3256" data-og-height="1326" height="1326" data-path="images/tutorial/api_nodes/bfl/flux_1_kontext_multiple_image_input_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_kontext_multiple_image_input_guide.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=7f8a0ed32e4444d8bf7108e335cd6f05 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_kontext_multiple_image_input_guide.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=7cf0b90ebc65bb64ba464ea3c30bb300 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_kontext_multiple_image_input_guide.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=c545b6d0e39738cd9959cf6de208e751 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_kontext_multiple_image_input_guide.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=fefb762d6f91c69481f72013e8c39f06 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_kontext_multiple_image_input_guide.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=3517ea12027f6c06c69696c0acdadc5d 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_kontext_multiple_image_input_guide.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=cdb684fa104961afe449e47ff6f72eaf 2500w" />

You can follow the numbered steps in the image to complete the workflow:

1. Upload the provided images in the `Load image` node
2. Modify the necessary parameters in `Flux.1 Kontext Pro Image`:
   * `prompt` Enter the prompt for the image you want to edit
   * `aspect_ratio` Set the aspect ratio of the original image, which must be between 1:4 and 4:1
   * `prompt_upsampling` Set whether to use prompt upsampling. If enabled, it will automatically modify the prompt to get richer results, but the results are not reproducible
3. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image editing
4. After waiting for the API to return results, you can view the edited image in the `Save Image` node, and the corresponding image will also be saved to the `ComfyUI/output/` directory

<Tip>
  The subsequent two workflows only differ in the Partner nodes used. In fact, you only need to modify based on the multiple image input workflow, with no significant differences
</Tip>

## Flux.1 Kontext Pro Image API Node Workflow

### 1. Workflow File Download

The `metadata` of the image below contains the workflow information. Please download and drag it into ComfyUI to load the corresponding workflow.

![ComfyUI Flux.1 Kontext Pro Image API Node Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_kontext_pro_image.png)

Download the image below for input or use your own image:

![ComfyUI Flux.1 Kontext Pro Image API Node Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_kontext_pro_image_input.png)

### 2. Complete the Workflow Step by Step

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_kontext_pro_image_step_guide.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=ff2c42a34894ae54e32a24058ad12755" alt="ComfyUI Flux.1 Kontext Pro Image API Node Workflow Steps" data-og-width="2058" width="2058" data-og-height="1155" height="1155" data-path="images/tutorial/api_nodes/bfl/flux_1_kontext_pro_image_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_kontext_pro_image_step_guide.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=ea8dbd24df6ed72d7819de96823e92b6 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_kontext_pro_image_step_guide.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=08a7e0dbfa39801a533ca2df037271f0 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_kontext_pro_image_step_guide.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=95f6734754ec5350ec8e54145e331037 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_kontext_pro_image_step_guide.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=6ab34242b1b7fd2370f75d6ca31e8b02 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_kontext_pro_image_step_guide.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=e056cecb55441fcc233c0257040ed298 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_kontext_pro_image_step_guide.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=286093e893c5171fdac45dc649fa5d95 2500w" />

You can follow the numbered steps in the image to complete the workflow:

1. Load the image you want to edit in the `Load Image` node
2. (Optional) Modify the necessary parameters in `Flux.1 Kontext Pro Image`
3. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image editing
4. After waiting for the API to return results, you can view the edited image in the `Save Image` node, and the corresponding image will also be saved to the `ComfyUI/output/` directory

## Flux.1 Kontext Max Image API Node Workflow

### 1. Workflow File Download

The `metadata` of the image below contains the workflow information. Please download and drag it into ComfyUI to load the corresponding workflow.

![ComfyUI Flux.1 Kontext Max Image API Node Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_kontext_max_image.png)

Download the image below for input or use your own image for demonstration:

![ComfyUI Flux.1 Kontext Max Image API Node Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/bfl/flux_1_kontext_max_image_input.png)

### 2. Complete the Workflow Step by Step

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_kontext_max_image_step_guide.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=3f91d4f54822bef88bca4e7024170950" alt="ComfyUI Flux.1 Kontext Max Image API Node Workflow Steps" data-og-width="2132" width="2132" data-og-height="1209" height="1209" data-path="images/tutorial/api_nodes/bfl/flux_1_kontext_max_image_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_kontext_max_image_step_guide.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=a0974111de4d56ee290f83b38764bc44 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_kontext_max_image_step_guide.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=25172a52bda142d3a0a6f332c93ff287 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_kontext_max_image_step_guide.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=389e34b12c1f14bbfdb045ec3de673db 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_kontext_max_image_step_guide.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=b72386f9f1950894a91f81f4ad0eedd2 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_kontext_max_image_step_guide.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=4c616381e3a7b5092c0fb6d1ae643364 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/bfl/flux_1_kontext_max_image_step_guide.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=9076344039b8541827bdb0b8bc092d5d 2500w" />

You can follow the numbered steps in the image to complete the workflow:

1. Load the image you want to edit in the `Load Image` node
2. (Optional) Modify the necessary parameters in `Flux.1 Kontext Max Image`
3. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image editing
4. After waiting for the API to return results, you can view the edited image in the `Save Image` node, and the corresponding image will also be saved to the `ComfyUI/output/` directory

## Flux Kontext Prompt Techniques

### 1. Basic Modifications

* Simple and direct: `"Change the car color to red"`
* Maintain style: `"Change to daytime while maintaining the same style of the painting"`

### 2. Style Transfer

**Principles:**

* Clearly name style: `"Transform to Bauhaus art style"`
* Describe characteristics: `"Transform to oil painting with visible brushstrokes, thick paint texture"`
* Preserve composition: `"Change to Bauhaus style while maintaining the original composition"`

### 3. Character Consistency

**Framework:**

* Specific description: `"The woman with short black hair"` instead of "she"
* Preserve features: `"while maintaining the same facial features, hairstyle, and expression"`
* Step-by-step modifications: Change background first, then actions

### 4. Text Editing

* Use quotes: `"Replace 'joy' with 'BFL'"`
* Maintain format: `"Replace text while maintaining the same font style"`

## Common Problem Solutions

### Character Changes Too Much

 Wrong: `"Transform the person into a Viking"`
 Correct: `"Change the clothes to be a viking warrior while preserving facial features"`

### Composition Position Changes

 Wrong: `"Put him on a beach"`
 Correct: `"Change the background to a beach while keeping the person in the exact same position, scale, and pose"`

### Style Application Inaccuracy

 Wrong: `"Make it a sketch"`
 Correct: `"Convert to pencil sketch with natural graphite lines, cross-hatching, and visible paper texture"`

## Core Principles

1. **Be Specific and Clear** - Use precise descriptions, avoid vague terms
2. **Step-by-step Editing** - Break complex modifications into multiple simple steps
3. **Explicit Preservation** - State what should remain unchanged
4. **Verb Selection** - Use "change", "replace" rather than "transform"

## Best Practice Templates

**Object Modification:**
`"Change [object] to [new state], keep [content to preserve] unchanged"`

**Style Transfer:**
`"Transform to [specific style], while maintaining [composition/character/other] unchanged"`

**Background Replacement:**
`"Change the background to [new background], keep the subject in the exact same position and pose"`

**Text Editing:**
`"Replace '[original text]' with '[new text]', maintain the same font style"`

> **Remember:** The more specific, the better. Kontext excels at understanding detailed instructions and maintaining consistency.


# FAQs about Partner Nodes
Source: https://docs.comfy.org/tutorials/partner-nodes/faq

Some FAQs you may encounter when using Partner Nodes.

This article addresses common questions regarding the use of Partner nodes.

<AccordionGroup>
  <Accordion title="Why can't I find the API nodes?">
    Please update your ComfyUI to the latest version (the latest commit or the latest [desktop version](https://www.comfy.org/download)).
    We may add more API support in the future, and the corresponding nodes will be updated, so please keep your ComfyUI up to date.

    <Tip>
      Please note that you need to distinguish between the nightly version and the release version.
      In some cases, the latest `release` version may not be updated in time compared to the `nightly` version.
      Since we are still iterating quickly, please ensure you are using the latest version when you cannot find the corresponding node.
    </Tip>
  </Accordion>

  <Accordion title="Why can't I use / log in to the API Nodes?">
    API access requires that your current request is based on a secure network environment. The current requirements for API access are as follows:

    * The local network only allows access from `127.0.0.1` or `localhost`, which may mean that you cannot use the API Nodes in a ComfyUI service started with the `--listen` parameter in a LAN environment.
    * Able to access our API service normally (a proxy service may be required in some regions).
    * Your account does not have enough [credits](/interface/credits).
  </Accordion>

  <Accordion title="Why can't I use API node even after logging in, or why does it keep asking me to log in while using?">
    * Currently, only `127.0.0.1` or `localhost` access is supported.
    * Ensure your account has enough credits.
  </Accordion>

  <Accordion title="Can API Nodes be used for free?">
    API Nodes require credits for API calls to closed-source models, so they do not support free usage.
  </Accordion>

  <Accordion title="How to purchase credits?">
    Please refer to the following documentation:

    1. [Comfy Account](/interface/user): Find the `User` section in the settings menu to log in.
    2. [Credits](/interface/credits): After logging in, the settings interface will show the credits menu. You can purchase credits in `Settings`  `Credits`. We use a prepaid system, so there will be no unexpected charges.
    3. Complete the payment through Stripe.
    4. Check if the credits have been updated. If not, try restarting or refreshing the page.
  </Accordion>

  <Accordion title="Are unused credits refundable?">
    Currently, we do not support refunds for credits.
    If you believe there is an error resulting in unused balance due to technical issues, please [contact support](mailto:support@comfy.org).
  </Accordion>

  <Accordion title="Can credits go negative?">
    Credits cannot go negative, so please ensure you have enough credits before making the corresponding API calls.
  </Accordion>

  <Accordion title="Where can I check usage and expenses?">
    Please visit the [Credits](/interface/credits) menu after logging in to check the corresponding credits.
  </Accordion>

  <Accordion title="Is it possible to use my own API Key?">
    Currently, the API Nodes are still in the testing phase and do not support this feature yet, but we have considered adding it.
  </Accordion>

  <Accordion title="Do credits expire?">
    No, your credits do not expire.
  </Accordion>

  <Accordion title="Can credits be transferred or shared?">
    No, your credits cannot be transferred to other users and are limited to the currently logged-in account, but we do not restrict the number of devices that can log in.
  </Accordion>

  <Accordion title="Can I use the same account on different devices?">
    We do not limit the number of devices that can log in; you can use your account anywhere you want.
  </Accordion>

  <Accordion title="How can I request for my account or information to be deleted??">
    Email a request to [support@comfy.org](mailto:support@comfy.org) and we will delete your information
  </Accordion>
</AccordionGroup>


# Google Gemini API Node ComfyUI Official Example
Source: https://docs.comfy.org/tutorials/partner-nodes/google/gemini

This article will introduce how to use Google Gemini Partner nodes in ComfyUI to complete conversational functions

Google Gemini is a powerful AI model developed by Google, supporting conversational and text generation functions. Currently, ComfyUI has integrated the Google Gemini API, allowing you to directly use the related nodes in ComfyUI to complete conversational functions.

In this guide, we will walk you through completing the corresponding conversational functionality.

<Tip>
  To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](/tutorials/partner-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.
</Tip>

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Google Gemini Chat Workflow

### 1. Workflow File Download

Please download the Json file below and drag it into ComfyUI to load the corresponding workflow.

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/google/api_google_gemini.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download Json Format Workflow File</p>
</a>

### 2. Complete the Workflow Execution Step by Step

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/google/tripo_image_to_model_step_guide.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=f5069d4edbc353783cf79fc1898df568" alt="OpenAI Chat Step Guide" data-og-width="3940" width="3940" data-og-height="2092" height="2092" data-path="images/tutorial/api_nodes/google/tripo_image_to_model_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/google/tripo_image_to_model_step_guide.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=297f9266b6fe23c3e06053f4a605d9f5 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/google/tripo_image_to_model_step_guide.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=65af09f4006c2aefb634da623430bdde 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/google/tripo_image_to_model_step_guide.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=0581bfd8f5e1fd73a08ee9d6cfe45381 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/google/tripo_image_to_model_step_guide.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=5e22076889c086e5c5568d7b87fc94da 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/google/tripo_image_to_model_step_guide.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=df4f5e9bf6f0d1e312f0d0a77319be93 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/google/tripo_image_to_model_step_guide.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=8048fad10d4849720aac37264af222d2 2500w" />

<Note>
  In the corresponding template, we have built a prompt for analyzing and generating role prompts, used to interpret your images into corresponding drawing prompts
</Note>

You can refer to the numbers in the image to complete the basic text-to-image workflow execution:

1. In the `Load Image` node, load the image you need AI to interpret
2. (Optional) If needed, you can modify the prompt in `Google Gemini` to have AI execute specific tasks
3. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the conversation.
4. After waiting for the API to return results, you can view the corresponding AI returned content in the `Preview Any` node.

### 3. Additional Notes

* Currently, the file input node `Gemini Input Files` requires files to be uploaded to the `ComfyUI/input/` directory first. This node is being improved, and we will modify the template after updates
* The workflow provides an example using `Batch Images` for input. If you have multiple images that need AI interpretation, you can refer to the step diagram and use right-click to set the corresponding node mode to `Always` to enable it


# Nano Banana Pro and ComfyUI Official Example
Source: https://docs.comfy.org/tutorials/partner-nodes/google/nano-banana-pro

This article will introduce how to use Google's Nano Banana Pro (Gemini 3 Pro Image) in ComfyUI for high-fidelity image generation and editing

Nano Banana Pro is Google DeepMind's new flagship model for high-fidelity image generation and editing. It pushes beyond casual creation into production-ready visuals with advanced capabilities including 4K generation, text rendering, and character consistency.

In this guide, we will walk you through using Nano Banana Pro in ComfyUI.

## Highlights of Nano Banana Pro

* **World knowledge:** Generates accurate, real-world images by tapping into Search's knowledge base.
* **Text & translation:** Renders clean text and supports detection/translation across **10 languages**.
* **High resolutions and studio controls:** Blend up to **14 images**, adjust angles and focus, apply color grading, and create in native **4K**.
* **Consistency:** Turns sketches into dresses or blueprints into 3D visuals with high-fidelity consistency.

<Tip>
  To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](/tutorials/partner-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.
</Tip>

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Get started

### For new users

The easiest way to use Nano Banana Pro is to go to Comfy Cloud through the link below. No extra setup needed.

<a className="prose" target="_blank" href="https://cloud.comfy.org/?template=api_nano_banana_pro&utm_source=blog" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Try Nano Banana Pro on Comfy Cloud</p>
</a>

### For local users

Please update ComfyUI and go to Template  Nano Banana Pro to access the workflow.


# ComfyUI Ideogram 3.0 API Node Official Examples
Source: https://docs.comfy.org/tutorials/partner-nodes/ideogram/ideogram-v3

This guide covers how to use the Ideogram 3.0 Partner node in ComfyUI

Ideogram 3.0 is a powerful text-to-image model by Ideogram, known for its photorealistic quality, accurate text rendering, and consistent style control.

The [Ideogram V3](/built-in-nodes/partner-node/image/ideogram/ideogram-v3) node currently supports two modes:

* Text-to-Image mode
* Image Editing mode (when both image and mask inputs are provided)

<Tip>
  To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](/tutorials/partner-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.
</Tip>

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Ideogram 3.0 Node Documentation

Check the following documentation for detailed node parameter settings:

* [Ideogram V3](/built-in-nodes/partner-node/image/ideogram/ideogram-v3)

## Ideogram 3.0 API Node Text-to-Image Mode

When using [Ideogram V3](/built-in-nodes/partner-node/image/ideogram/ideogram-v3) without image and mask inputs, the node operates in Text-to-Image mode.

### 1. Download Workflow File

Download and drag the following file into ComfyUI to load the workflow:

![Ideogram 3.0 ComfyUI Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/ideogram/v3/ideogram_v3_t2i.png)

### 2. Complete the Workflow Steps

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/ideogram/ideogram_v3_t2i.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=3f51da844d34d588570a037c3ef7a264" alt="Ideogram 3.0 Workflow Steps" data-og-width="2037" width="2037" data-og-height="1250" height="1250" data-path="images/tutorial/api_nodes/ideogram/ideogram_v3_t2i.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/ideogram/ideogram_v3_t2i.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=fb13382fe73f03cb4c7ce1e863ce4097 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/ideogram/ideogram_v3_t2i.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=a849b6be7e0a3149ef1ed52c58109eac 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/ideogram/ideogram_v3_t2i.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=cce20da548057f45ea7ed32691d6e10e 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/ideogram/ideogram_v3_t2i.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=48413ebebe11db98eaf9f0858b0ee76c 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/ideogram/ideogram_v3_t2i.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=b87b10ed4fc43827aa9b213cb48a816d 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/ideogram/ideogram_v3_t2i.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=07083fad209c20ec07201c3e7ac45721 2500w" />

Follow the numbered steps to complete the basic workflow:

1. Enter your image description in the `prompt` field of the `Ideogram V3` node
2. Click `Run` or use shortcut `Ctrl(cmd) + Enter` to generate the image
3. After the API returns results, view the generated image in the `Save Image` node. Images are saved to the `ComfyUI/output/` directory

## Ideogram 3.0 API Node Image Editing Mode

\[To be updated]


# Luma Image to Image API Node ComfyUI Official Example
Source: https://docs.comfy.org/tutorials/partner-nodes/luma/luma-image-to-image

This guide covers how to use the Luma Image to Image Partner node in ComfyUI

The [Luma Image to Image](/built-in-nodes/partner-node/image/luma/luma-image-to-image) node allows you to modify existing images based on text prompts using Luma AI technology, while preserving certain features and structures from the original image.

In this guide, we'll show you how to set up an image-to-image workflow using this node.

<Tip>
  To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](/tutorials/partner-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.
</Tip>

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Luma Image to Image Node Documentation

Check the following documentation for detailed node parameter settings:

<Card title="Luma Image to Image Node Documentation" icon="book" href="/built-in-nodes/partner-node/image/luma/luma-image-to-image">
  Luma Image to Image API Node Documentation
</Card>

## Luma Image to Image API Node Workflow

<Tip>
  This feature works well for changing objects and shapes. However, it may not be ideal for color changes. We recommend using lower weight values, around 0.0 to 0.1.
</Tip>

### 1. Download Workflow File

Download and drag the following image into ComfyUI to load the workflow (workflow information is included in the image metadata):

![Luma Image to Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/i2i/luma_i2i.png)

Download this image to use as input:

![Luma Image to Image Workflow Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/i2i/input.png)

### 2. Complete the Workflow Steps

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/luma_i2i_step_guide.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=0169a042868519a3cd76ef832fdc714a" alt="Luma Image to Image Workflow Steps" data-og-width="2577" width="2577" data-og-height="1139" height="1139" data-path="images/tutorial/api_nodes/luma/luma_i2i_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/luma_i2i_step_guide.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=d8384233bba5478b0c86cfc9decc214f 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/luma_i2i_step_guide.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=54c9dad2fe0366b78de2adc1993c1323 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/luma_i2i_step_guide.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=3fd59ee81bfac6a5ecda6aa722f8d102 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/luma_i2i_step_guide.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=d13fff5d7609ac2fa5805b6849635c88 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/luma_i2i_step_guide.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=7b3d318fc1eb089f8ff39b63bf4ff8ee 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/luma_i2i_step_guide.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=0ddc3da1e3aeb0b4c3f9bff7e0407975 2500w" />

Follow these numbered steps:

1. Click **Upload** on the `Load Image` node to upload your input image
2. (Optional) Modify the workflow prompts
3. (Optional) Adjust `image_weight` to change input image influence (lower values stay closer to original)
4. Click `Run` or use shortcut `Ctrl(cmd) + Enter` to generate the image
5. After API returns results, view the generated image in the `Save Image` node. Images are saved to the `ComfyUI/output/` directory

### 3. Results with Different `image_weight` Values

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/i2i_image_weight.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=d6fc32e5bdbf4f1368f73cfac583643d" alt="Weight Comparison" data-og-width="3150" width="3150" data-og-height="1173" height="1173" data-path="images/tutorial/api_nodes/luma/i2i_image_weight.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/i2i_image_weight.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=084939a3ef759130180a46e36b0b240c 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/i2i_image_weight.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=359a906ca12d76263eba36a4e55185c0 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/i2i_image_weight.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=826d2123c3c5f8497e92b3f977bfaac7 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/i2i_image_weight.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=dcf51db85f394a66dffadbf651eeba7b 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/i2i_image_weight.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=563736803d4fa3ec5e92d9c0f05ec5cb 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/i2i_image_weight.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=7403afda95b9ca243af876284db65271 2500w" />


# Luma Image to Video API Node ComfyUI Official Example
Source: https://docs.comfy.org/tutorials/partner-nodes/luma/luma-image-to-video

Learn how to use the Luma Image to Video Partner node in ComfyUI

The [Luma Image to Video](/built-in-nodes/partner-node/video/luma/luma-image-to-video) node allows you to convert static images into smooth, dynamic videos using Luma AI's advanced technology, bringing life and motion to your images.

In this guide, we'll show you how to set up a workflow for image-to-video conversion using this node.

<Tip>
  To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](/tutorials/partner-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.
</Tip>

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Luma Image to Video Node Documentation

Check out the following documentation to learn more about the node's parameters:

<Card title="Luma Image to Video Node Docs" icon="book" href="/built-in-nodes/partner-node/video/luma/luma-image-to-video">
  Luma Image to Video Partner node documentation
</Card>

<Card title="Luma Concepts Node Docs" icon="book" href="/built-in-nodes/partner-node/video/luma/luma-concepts">
  Luma Concepts Partner node documentation
</Card>

## Image to Video Workflow with Luma API Node

The Luma Image to Video node requires at least one image input (`first_image` or `last_image`) along with text prompts to determine the video's motion effects. In this guide, we've created an example using `first_image` and `luma_concepts` to showcase Luma AI's video generation capabilities.

### 1. Download the Workflow

The workflow information is included in the metadata of the video below. Download and drag it into ComfyUI to load the workflow.

![Luma Image to Video Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/i2v/luma_i2v.mp4)

Download the following image to use as input:

![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/i2v/input.png)

### 2. Follow the Workflow Steps

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/luma_i2v_step_guide.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=f55c824ff729382814a2e86e35b9f309" alt="Luma Image to Video Workflow Steps" data-og-width="2492" width="2492" data-og-height="1515" height="1515" data-path="images/tutorial/api_nodes/luma/luma_i2v_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/luma_i2v_step_guide.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=87d66e016ff643f5d0d9e260ffe5f2b3 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/luma_i2v_step_guide.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=6f939ce7a2ebb6f25c9c8a9f896282a4 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/luma_i2v_step_guide.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=4c67f8b817722d7d9cbf51c97b755e6f 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/luma_i2v_step_guide.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=3517730a261ab09b09da43b354a4f181 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/luma_i2v_step_guide.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=f4f6e700862bf6483f1c1fe8bfc3cabc 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/luma_i2v_step_guide.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=eb3983d8bd61e7e960ef16bc385706d5 2500w" />

Follow these basic steps to run the workflow:

1. Upload your input image in the `first_image` node
2. (Optional) Write prompts in the Luma Image to Video node to describe how you want the image animated
3. (Optional) Modify the `Luma Concepts` node to control camera movement for professional cinematography
4. Click `Run` or use `Ctrl(cmd) + Enter` to generate the video
5. Once the API returns results, view the generated video in the `Save Video` node. The video will also be saved to the `ComfyUI/output/` directory

### 3. Additional Notes

* **Image Input Requirements**: At least one of `first_image` or `last_image` is required, with a maximum of 1 image per input
* **Luma Concepts**: Controls camera movement for professional video effects
* **Seed Parameter**: Only determines if the node should rerun, doesn't affect generation results
* **Enable Input Nodes**: Right-click on purple "Bypass" mode nodes and set "mode" to "always" to enable inputs
* **Model Selection**: Different video generation models have unique characteristics, adjustable via the model parameter
* **Resolution and Duration**: Adjust output video resolution and length using resolution and duration parameters


# Luma Text to Image API Node ComfyUI Official Example
Source: https://docs.comfy.org/tutorials/partner-nodes/luma/luma-text-to-image

This guide explains how to use the Luma Text to Image Partner node in ComfyUI

The [Luma Text to Image](/built-in-nodes/partner-node/image/luma/luma-text-to-image) node allows you to generate high-quality images from text prompts using Luma AI's advanced technology, capable of creating photorealistic content and artistic style images.

In this guide, we'll show you how to set up workflows using this node for text-to-image generation.

<Tip>
  To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](/tutorials/partner-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.
</Tip>

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Luma Text to Image Node Documentation

You can refer to the following documentation for detailed parameter settings:

<Card title="Luma Text to Image Node Documentation" icon="book" href="/built-in-nodes/partner-node/image/luma/luma-text-to-image">
  Luma Text to Image API Node Documentation
</Card>

<Card title="Luma Reference Node Documentation" icon="book" href="/built-in-nodes/partner-node/image/luma/luma-reference">
  Luma Reference API Node Documentation
</Card>

## Luma Text to Image API Node Workflow

When the `Luma Text to Image` node is used without any image inputs, it functions as a text-to-image workflow. In this guide, we've created examples using `style_image` and `image_luma_ref` to showcase Luma AI's excellent image processing capabilities.

### 1. Download Workflow Files

The workflow information is included in the metadata of the image below. Download and drag it into ComfyUI to load the workflow.

![Luma Text to Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/t2i/luma_t2i.png)

Please download these images for input:

![Input Image - Reference](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/t2i/input_ref.png)
![Input Image - Style](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/t2i/input_style.png)

### 2. Follow Steps to Run the Workflow

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/luma_t2i_step_guide.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=e3122c711034e8270d086ae8c139cb64" alt="Luma Text to Image Workflow Steps" data-og-width="2796" width="2796" data-og-height="1694" height="1694" data-path="images/tutorial/api_nodes/luma/luma_t2i_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/luma_t2i_step_guide.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=34ce72b35a6ce916e2a7cf68bf4d7fab 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/luma_t2i_step_guide.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=b76ef973b5623f2575f08f49279a2c96 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/luma_t2i_step_guide.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=9dbd9de85a6331dbc3dcc0c231adc666 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/luma_t2i_step_guide.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=870888519eeb09fdabafdf4b867be154 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/luma_t2i_step_guide.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=69acf9a0fa4ce079286baa22ba66be8d 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/luma_t2i_step_guide.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=82a2b77c63649c49c8ceae305f539915 2500w" />

Follow the numbered steps in the image to complete the basic workflow:

1. Upload the reference image in the `Load image` node
2. Upload the style reference image in the `Load image (renamed to styleref)` node
3. (Optional) Modify the prompts in the `Luma Text to Image` node
4. (Optional) Adjust the `style_image_weight` to control the style reference image's influence
5. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to generate the image
6. After the API returns results, view the generated image in the `Save Image` node. Images are saved to the `ComfyUI/output/` directory

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/t2i_style_image_weight.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=7f47d2d3a6acdde87df046460afa393f" alt="Style Image Weight Comparison" data-og-width="2100" width="2100" data-og-height="1867" height="1867" data-path="images/tutorial/api_nodes/luma/t2i_style_image_weight.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/t2i_style_image_weight.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=155cadfd9a346447a26ece679de9b2b8 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/t2i_style_image_weight.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=e2b792354983726d5c33e3c69f8273a5 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/t2i_style_image_weight.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=0bbb226eb88b380096e0a52238f80898 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/t2i_style_image_weight.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=1889cb637e62fdcc7b2be4b7ee925355 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/t2i_style_image_weight.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=7d237e41b44d0249359126ae88c8d1c6 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/t2i_style_image_weight.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=5e6f5eb502e8347dc1267d7bff119215 2500w" />

### 3. Additional Notes

* The [node](/built-in-nodes/partner-node/image/luma/luma-text-to-image) allows up to 4 reference images and character references simultaneously.
* To enable multiple image inputs, right-click on the purple "Bypassed" nodes and set their `mode` to `always`


# Luma Text to Video API Node ComfyUI Official Guide
Source: https://docs.comfy.org/tutorials/partner-nodes/luma/luma-text-to-video

Learn how to use the Luma Text to Video Partner node in ComfyUI

The [Luma Text to Video](/built-in-nodes/partner-node/video/luma/luma-text-to-video) node allows you to create high-quality, smooth videos from text descriptions using Luma AI's innovative video generation technology.

In this guide, we'll show you how to set up a text-to-video workflow using this node.

<Tip>
  To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](/tutorials/partner-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.
</Tip>

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Luma Text to Video Node Documentation

Check out the following documentation to learn more about the node parameters:

<Card title="Luma Text to Video Node Docs" icon="book" href="/built-in-nodes/partner-node/video/luma/luma-text-to-video">
  Documentation for the Luma Text to Video Partner node
</Card>

<Card title="Luma Concepts Node Docs" icon="book" href="/built-in-nodes/partner-node/video/luma/luma-concepts">
  Documentation for the Luma Concepts Partner node
</Card>

## Text to Video Workflow with Luma API Node

The Luma Text to Video node requires text prompts to describe the video content. In this guide, we've created examples using `prompt` and `luma_concepts` to showcase Luma AI's excellent video generation capabilities.

### 1. Download the Workflow

The workflow information is included in the metadata of the video below. Download and drag it into ComfyUI to load the workflow.

![Luma Text to Video Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/luma/t2v/luma_t2v.mp4)

### 2. Follow the Steps

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/luma_t2v_step_guide.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=96c92693534372a8522603f9c13f088c" alt="Luma Text to Video Workflow Steps" data-og-width="2097" width="2097" data-og-height="1691" height="1691" data-path="images/tutorial/api_nodes/luma/luma_t2v_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/luma_t2v_step_guide.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=7650813bb283f9aabc784d63f2b51547 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/luma_t2v_step_guide.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=ec60e49f9dd61d17f616779b56f785bc 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/luma_t2v_step_guide.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=de082bf41aef1a9d5464b2971d4f2e2b 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/luma_t2v_step_guide.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=566048bcd6c3df8e54bb999af0e94715 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/luma_t2v_step_guide.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=7cfc31fc22b5ebd5c567e9f1fecb51bf 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/luma/luma_t2v_step_guide.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=c0c093965f3aa083066ed153937f8dab 2500w" />

Follow these basic steps to run the workflow:

1. Write your prompt in the `Luma Text to Video` node to describe the video content you want
2. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to generate the video
3. After the API returns results, you can view the generated video in the `Save Video` node. The video will also be saved to the `ComfyUI/output/` directory

> (Optional) Modify the `Luma Concepts` node to control camera movements and add professional cinematography

### 3. Additional Notes

* **Writing Prompts**: Describe scenes, subjects, actions, and mood in detail for best results
* **Luma Concepts**: Mainly used for camera control to create professional video shots
* **Seed Parameter**: Only determines if the node should rerun, doesn't affect generation results
* **Model Selection**: Different video models have different features, adjustable via the model parameter
* **Resolution and Duration**: Adjust output video resolution and length using these parameters
* **Ray 1.6 Model Note**: Duration and resolution parameters don't work when using the Ray 1.6 model


# Moonvalley API Node Official ComfyUI Example
Source: https://docs.comfy.org/tutorials/partner-nodes/moonvalley/moonvalley-video-generation

This article introduces how to use Moonvalley Partner nodes for text-to-video, image-to-video, and video-to-video capabilities in ComfyUI.

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/8Z8X9fFKapQ?si=lK_KFSzDcT0Mepk9" title="ComfyUI Selection Toolbox New Features" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

Moonvalley Marey Realism v1.5 is an AI video generation model designed for cinematic-level creation. The model is **trained entirely with commercially licensed content**, ensuring **copyright compliance and commercial safety**.

## Product Highlights

* Exceptional prompt comprehension: Accurately interprets complex prompt instructions.
* Native 1080p HD quality: The training dataset is based on **1080P** videos, resulting in fine and detailed output.
* Realistic physics and dynamic performance: Precisely simulates physical motion models and natural dynamics, delivering professional-grade realism.
* Complex scene layering and advanced lighting effects: Supports foreground, midground, and background layering in complex scenes, with intelligent spatial relationship understanding.
* Production-level control features such as motion and pose transfer: Automatically generates realistic lighting for composite scenes.

Currently, Moonvalley-related Partner nodes are natively supported in ComfyUI. You can use the corresponding text-to-video, image-to-video, and video-to-video capabilities directly in ComfyUI.

<Tip>
  To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](/tutorials/partner-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.
</Tip>

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Moonvalley Text-to-Video Workflow

### 1. Download the Workflow File

<video controls className="w-full aspect-video" src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/moonvalley/api_moonvalley_text_to_video.mp4" />

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/moonvalley/api_moonvalley_text_to_video.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download the workflow file in JSON format</p>
</a>

### 2. Follow the Steps to Run the Workflow

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/moonvalley/api_moonvalley_text_to_video.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=75dc005efbab705ab95cea22b36f7a24" alt="Text-to-Video Workflow" data-og-width="3160" width="3160" data-og-height="2434" height="2434" data-path="images/tutorial/api_nodes/moonvalley/api_moonvalley_text_to_video.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/moonvalley/api_moonvalley_text_to_video.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=28c9273328f87a89dbd8471160792362 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/moonvalley/api_moonvalley_text_to_video.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=e3b69032638a7e8c21f48648587263ba 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/moonvalley/api_moonvalley_text_to_video.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=44a6909ce38a300f0fb289592229a7c1 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/moonvalley/api_moonvalley_text_to_video.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=b1ace57a8eff4446f831b6348c37abd5 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/moonvalley/api_moonvalley_text_to_video.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=27e34daf102a1d0866aa937a2c6dd780 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/moonvalley/api_moonvalley_text_to_video.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=415c88b1d9a8b5bfbbefefa19e50bc2a 2500w" />

1. Enter the positive prompt (content you want to appear in the video)
2. Enter the negative prompt (content you do not want to appear in the video)
3. Modify the video output resolution
4. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to start video generation
5. After the API returns the result, you can view the generated video in the `Save Video` node. The video will also be saved in the `ComfyUI/output/` directory

## Moonvalley Image-to-Video Workflow

### 1. Download the Workflow File

<video controls className="w-full aspect-video" src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/moonvalley/api_moonvalley_image_to_video.mp4" />

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/moonvalley/api_moonvalley_image_to_video.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download the workflow file in JSON format</p>
</a>

Download the image below as the input image

![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/moonvalley/api_moonvalley_image_to_video_input.webp)

### 2. Follow the Steps to Run the Workflow

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/moonvalley/api_moonvalley_image_to_video.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=9467e0f19d709f9b77cc4778a97f04a8" alt="Image-to-Video Workflow" data-og-width="3966" width="3966" data-og-height="1350" height="1350" data-path="images/tutorial/api_nodes/moonvalley/api_moonvalley_image_to_video.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/moonvalley/api_moonvalley_image_to_video.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=15f722730ad8914c66953fd80343c68b 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/moonvalley/api_moonvalley_image_to_video.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=09c1e830f9a49d385496d720fa9cc2f8 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/moonvalley/api_moonvalley_image_to_video.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=75d4f6b4f7f6774e696c8c66b5b5fdaf 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/moonvalley/api_moonvalley_image_to_video.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=f2835d3b2719b0738ef0fb005be0d919 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/moonvalley/api_moonvalley_image_to_video.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=467702de371d0a5f4f2069313bef68b2 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/moonvalley/api_moonvalley_image_to_video.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=3e79e56ce7f9516a5ab105dfb47aa4c9 2500w" />

1. Load the input image in the `Load Image` node
2. Enter the positive prompt (content you want to appear in the video)
3. Enter the negative prompt (content you do not want to appear in the video)
4. Modify the video output resolution
5. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to start video generation
6. After the API returns the result, you can view the generated video in the `Save Video` node. The video will also be saved in the `ComfyUI/output/` directory

## Moonvalley Video-to-Video Workflow

The `Moonvalley Marey Video to Video` node allows you to input a reference video for video re-drawing. You can use the reference video's motion or character poses for video generation.

### 1. Download the Workflow File

<video controls className="w-full aspect-video" src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/moonvalley/api_moonvalley_video_to_video.mp4" />

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/moonvalley/api_moonvalley_video_to_video.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download the workflow file in JSON format</p>
</a>

Download the video below as the input video:

<video controls className="w-full aspect-video" src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/moonvalley/api_moonvalley_video_to_video_input.mp4" />

### 2. Follow the Steps to Run the Workflow

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/moonvalley/api_moonvalley_video_to_video.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=ee9e202bdc9a3b39c8dc6ba5b10577b5" alt="Video-to-Video Workflow" data-og-width="3966" width="3966" data-og-height="1350" height="1350" data-path="images/tutorial/api_nodes/moonvalley/api_moonvalley_video_to_video.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/moonvalley/api_moonvalley_video_to_video.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=cf6664f2196ff60462c2d1d28003eea5 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/moonvalley/api_moonvalley_video_to_video.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=77a0cd776ba0564f497915231d13f1cb 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/moonvalley/api_moonvalley_video_to_video.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=0d6f58d692e5dcfc16703a678c3621dd 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/moonvalley/api_moonvalley_video_to_video.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=91f3ea51b33634ed0c54c1b1501bf7f0 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/moonvalley/api_moonvalley_video_to_video.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=99f1bfd447b88531d16c38c55b9aadeb 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/moonvalley/api_moonvalley_video_to_video.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=82f6b3a3c04808d4f5d5997b3dc53bc6 2500w" />

1. Load the reference video (or your own material) in the `Load Video` node
   * If the final video duration is 5s, the input video must be longer than 5s
   * If the final video duration is 10s, the input video must be longer than 10s
2. Enter the positive prompt (content you want to appear in the video)
3. Enter the negative prompt (content you do not want to appear in the video)
4. Set the `control_type` parameter to choose the reference type for video re-drawing
   * `Motion Transfer`: Generate based on the motion in the reference video
   * `Pose Transfer`: Generate based on the character poses in the reference video
5. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to start video generation
6. After the API returns the result, you can view the generated video in the `Save Video` node. The video will also be saved in the `ComfyUI/output/` directory


# OpenAI DALLE 2 Node
Source: https://docs.comfy.org/tutorials/partner-nodes/openai/dall-e-2

Learn how to use the OpenAI DALLE 2 Partner node to generate images in ComfyUI

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/api_nodes/openai-dall-e-2.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=42a0b800a337494eebec116cfea8d71a" alt="OpenAI DALLE 2 node screenshot" data-og-width="1576" width="1576" data-og-height="954" height="954" data-path="images/comfy_core/api_nodes/openai-dall-e-2.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/api_nodes/openai-dall-e-2.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=6ecf05f49bca8201e70d91bf6c39e2c5 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/api_nodes/openai-dall-e-2.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=5d03fc5dc0289a0219f278b0747bf2b4 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/api_nodes/openai-dall-e-2.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=9d8742d23f44e4a6f2752bd0126f6fd0 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/api_nodes/openai-dall-e-2.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=41023d403a268fed8120a91446837843 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/api_nodes/openai-dall-e-2.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=41fd9ca39c2d652fcaed9889c2ed3b59 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/api_nodes/openai-dall-e-2.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=8434b4a8a25e5b2d86286d44d31a778a 2500w" />

OpenAI DALLE 2 is part of the ComfyUI Partner Nodes series, allowing users to generate images through OpenAI's **DALLE 2** model.

This node supports:

* Text-to-image generation
* Image editing functionality (inpainting through masks)

## Node Overview

The **OpenAI DALLE 2** node generates images synchronously through OpenAI's image generation API. It receives text prompts and returns images that match the description.

<Tip>
  To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](/tutorials/partner-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.
</Tip>

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Parameter Description

### Required Parameters

| Parameter | Description                                                   |
| --------- | ------------------------------------------------------------- |
| `prompt`  | Text prompt describing the image content you want to generate |

### Widget Parameters

| Parameter | Description                                                                | Options/Range                     | Default Value |
| --------- | -------------------------------------------------------------------------- | --------------------------------- | ------------- |
| `seed`    | Seed value for image generation (currently not implemented in the backend) | 0 to 2^31-1                       | 0             |
| `size`    | Output image dimensions                                                    | "256x256", "512x512", "1024x1024" | "1024x1024"   |
| `n`       | Number of images to generate                                               | 1 to 8                            | 1             |

### Optional Parameters

| Parameter | Description                                | Options/Range   | Default Value |
| --------- | ------------------------------------------ | --------------- | ------------- |
| `image`   | Optional reference image for image editing | Any image input | None          |
| `mask`    | Optional mask for local inpainting         | Mask input      | None          |

## Usage Method

## Workflow Examples

This Partner node currently supports two workflows:

* Text to Image
* Inpainting

<Note>
  Image to Image workflow is not supported
</Note>

### Text to Image Example

The image below contains a simple text-to-image workflow. Please download the corresponding image and drag it into ComfyUI to load the workflow.
![ComfyUI openai-dall-e-2 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/openai-dall-e-2/text2image.png)

The corresponding example is very simple
<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/openai-dall-e-2/text2image.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=cd0262919a3be35855c9891b6de8c9c0" alt="ComfyUI openai-dall-e-2 workflow example" data-og-width="3086" width="3086" data-og-height="1423" height="1423" data-path="images/tutorial/api_nodes/openai/openai-dall-e-2/text2image.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/openai-dall-e-2/text2image.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=766a95093f1ef90769a2e02718dd27a8 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/openai-dall-e-2/text2image.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=315f7cb9d4b49293c18476e725869615 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/openai-dall-e-2/text2image.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=1f52369d732c2040e395861d57fca2ba 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/openai-dall-e-2/text2image.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=60d125ab1c8f9759cda7a14a81fbedd1 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/openai-dall-e-2/text2image.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=dd64dbeec33bba7c9329dcb56746cf41 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/openai-dall-e-2/text2image.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=135961dddf3f7b28a431a5a92899db51 2500w" />

You only need to load the `OpenAI DALLE 2` node, input the description of the image you want to generate in the `prompt` node, connect a `Save Image` node, and then run the workflow.

### Inpainting Workflow

DALLE 2 supports image editing functionality, allowing you to use a mask to specify the area to be replaced. Below is a simple inpainting workflow example:

#### 1. Workflow File Download

Download the image below and drag it into ComfyUI to load the corresponding workflow.

![ComfyUI openai-dall-e-2 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/openai-dall-e-2/inpainting.png)

We will use the image below as input:
![ComfyUI openai-dall-e-2 workflow input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/openai-dall-e-2/input.jpg)

#### 2. Workflow File Usage Instructions

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/openai-dall-e-2/inpainting.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=29719686459aadec1d533f8fe26e0370" alt="ComfyUI openai-dall-e-2 workflow example" data-og-width="3609" width="3609" data-og-height="1425" height="1425" data-path="images/tutorial/api_nodes/openai/openai-dall-e-2/inpainting.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/openai-dall-e-2/inpainting.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=bb303fab72131dd5d985117514275fea 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/openai-dall-e-2/inpainting.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=cb22d377322c4e1db0d9866ee9c46843 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/openai-dall-e-2/inpainting.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=1eb5f9818cfc708f620a4e8861b64b83 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/openai-dall-e-2/inpainting.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=719e8e5bd24b645cbae7c36aee1ee03e 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/openai-dall-e-2/inpainting.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=f4a7e78a2c9be90d26800628267ab236 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/openai-dall-e-2/inpainting.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=0aaad83a58eb9bd3beed8ee0afa5b959 2500w" />

Since this workflow is relatively simple, if you want to manually implement the corresponding workflow yourself, you can follow the steps below:

1. Use the `Load Image` node to load the image
2. Right-click on the load image node and select `MaskEditor`
3. In the mask editor, use the brush to draw the area you want to redraw
4. Connect the loaded image to the `image` input of the **OpenAI DALLE 2** node
5. Connect the mask to the `mask` input of the **OpenAI DALLE 2** node
6. Edit the prompt in the `prompt` node
7. Run the workflow

**Notes**

* If you want to use the image editing functionality, you must provide both an image and a mask (both are required)
* The mask and image must be the same size
* When inputting large images, the node will automatically resize the image to an appropriate size
* The URLs returned by the API are valid for a short period, please save the results promptly
* Each generation consumes credits, charged according to image size and quantity

## FAQs

<AccordionGroup>
  <Accordion title="Why can't I find the API nodes?">
    Please update your ComfyUI to the latest version (the latest commit or the latest [desktop version](https://www.comfy.org/download)).
    We may add more API support in the future, and the corresponding nodes will be updated, so please keep your ComfyUI up to date.

    <Tip>
      Please note that you need to distinguish between the nightly version and the release version.
      In some cases, the latest `release` version may not be updated in time compared to the `nightly` version.
      Since we are still iterating quickly, please ensure you are using the latest version when you cannot find the corresponding node.
    </Tip>
  </Accordion>

  <Accordion title="Why can't I use / log in to the API Nodes?">
    API access requires that your current request is based on a secure network environment. The current requirements for API access are as follows:

    * The local network only allows access from `127.0.0.1` or `localhost`, which may mean that you cannot use the API Nodes in a ComfyUI service started with the `--listen` parameter in a LAN environment.
    * Able to access our API service normally (a proxy service may be required in some regions).
    * Your account does not have enough [credits](/interface/credits).
  </Accordion>

  <Accordion title="Why can't I use API node even after logging in, or why does it keep asking me to log in while using?">
    * Currently, only `127.0.0.1` or `localhost` access is supported.
    * Ensure your account has enough credits.
  </Accordion>

  <Accordion title="Can API Nodes be used for free?">
    API Nodes require credits for API calls to closed-source models, so they do not support free usage.
  </Accordion>

  <Accordion title="How to purchase credits?">
    Please refer to the following documentation:

    1. [Comfy Account](/interface/user): Find the `User` section in the settings menu to log in.
    2. [Credits](/interface/credits): After logging in, the settings interface will show the credits menu. You can purchase credits in `Settings`  `Credits`. We use a prepaid system, so there will be no unexpected charges.
    3. Complete the payment through Stripe.
    4. Check if the credits have been updated. If not, try restarting or refreshing the page.
  </Accordion>

  <Accordion title="Are unused credits refundable?">
    Currently, we do not support refunds for credits.
    If you believe there is an error resulting in unused balance due to technical issues, please [contact support](mailto:support@comfy.org).
  </Accordion>

  <Accordion title="Can credits go negative?">
    Credits cannot go negative, so please ensure you have enough credits before making the corresponding API calls.
  </Accordion>

  <Accordion title="Where can I check usage and expenses?">
    Please visit the [Credits](/interface/credits) menu after logging in to check the corresponding credits.
  </Accordion>

  <Accordion title="Is it possible to use my own API Key?">
    Currently, the API Nodes are still in the testing phase and do not support this feature yet, but we have considered adding it.
  </Accordion>

  <Accordion title="Do credits expire?">
    No, your credits do not expire.
  </Accordion>

  <Accordion title="Can credits be transferred or shared?">
    No, your credits cannot be transferred to other users and are limited to the currently logged-in account, but we do not restrict the number of devices that can log in.
  </Accordion>

  <Accordion title="Can I use the same account on different devices?">
    We do not limit the number of devices that can log in; you can use your account anywhere you want.
  </Accordion>

  <Accordion title="How can I request for my account or information to be deleted??">
    Email a request to [support@comfy.org](mailto:support@comfy.org) and we will delete your information
  </Accordion>
</AccordionGroup>


# OpenAI DALLE 3 Node
Source: https://docs.comfy.org/tutorials/partner-nodes/openai/dall-e-3

Learn how to use the OpenAI DALLE 3 Partner node to generate images in ComfyUI

OpenAI DALLE 3 is part of the ComfyUI Partner Nodes series, allowing users to generate images through OpenAI's **DALLE 3** model. This node supports text-to-image generation functionality.

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/openai/openai-dall-e-3.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=1d4da35db8baddcd1321ef98365e26d0" alt="OpenAI DALLE 3 node screenshot" data-og-width="1576" width="1576" data-og-height="966" height="966" data-path="images/built-in-nodes/api_nodes/openai/openai-dall-e-3.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/openai/openai-dall-e-3.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=c4991a2a05a37c4f63b9251cc4f5cb66 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/openai/openai-dall-e-3.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=bdb4fb6d52ed938a4a6f3dfb830e79f2 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/openai/openai-dall-e-3.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=f4de073cc71e3209f66be6eb0f45a575 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/openai/openai-dall-e-3.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=e525c1ca97e0ee9bd5e0b812e812530d 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/openai/openai-dall-e-3.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=d50bd7dabc5e9e5adca0683bc6777de4 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/openai/openai-dall-e-3.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=b1867a36a2203ad0a1b2fba9117a42ec 2500w" />

## Node Overview

DALLE 3 is OpenAI's latest image generation model, capable of creating detailed and high-quality images based on text prompts. Through this node in ComfyUI, you can directly access DALLE 3's generation capabilities without leaving the ComfyUI interface.

The **OpenAI DALLE 3** node generates images synchronously through OpenAI's image generation API. It receives text prompts and returns images that match the description.

<Tip>
  To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](/tutorials/partner-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.
</Tip>

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Parameter Details

### Required Parameters

| Parameter | Type | Description                                                                                                                  |
| --------- | ---- | ---------------------------------------------------------------------------------------------------------------------------- |
| prompt    | Text | Text prompt for generating images. Supports multi-line input, can describe in detail the image content you want to generate. |

### Widget Parameters

| Parameter | Type    | Options                         | Default Value | Description                                                                                                                               |
| --------- | ------- | ------------------------------- | ------------- | ----------------------------------------------------------------------------------------------------------------------------------------- |
| seed      | Integer | 0-2147483647                    | 0             | Random seed used to control the generation result                                                                                         |
| quality   | Option  | standard, hd                    | standard      | Image quality setting. The "hd" option generates higher quality images but may require more computational resources                       |
| style     | Option  | natural, vivid                  | natural       | Image style. "Vivid" tends to generate hyperrealistic and dramatic images, while "natural" produces more natural, less exaggerated images |
| size      | Option  | 1024x1024, 1024x1792, 1792x1024 | 1024x1024     | Size of the generated image. You can choose square or rectangular images in different orientations                                        |

## Usage Examples

You can download the image below and drag it into ComfyUI to load the corresponding workflow
![ComfyUI openai-dall-e-3 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/openai-dall-e-3/text2image.png)

Since the corresponding workflow is very simple, you can also directly add the **OpenAI DALLE 3** node in ComfyUI, input the description of the image you want to generate, and then run the workflow

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/openai-dall-e-3/text2image.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=29cd1bbda083d98eb24196d554586c43" alt="ComfyUI openai-dall-e-3 workflow" data-og-width="2308" width="2308" data-og-height="1059" height="1059" data-path="images/tutorial/api_nodes/openai/openai-dall-e-3/text2image.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/openai-dall-e-3/text2image.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=80240ed5377b1c5215e92795dd950b86 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/openai-dall-e-3/text2image.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=2b522272027cc91a93db758416496694 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/openai-dall-e-3/text2image.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=66c4318cfa7a4b18b34a18122cd34116 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/openai-dall-e-3/text2image.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=398e16e036cc013d9774512302038da7 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/openai-dall-e-3/text2image.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=8df52f2b31d8d1c39306f259007c5659 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/openai-dall-e-3/text2image.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=401dc6e74fc88045c42dbf90557fd82f 2500w" />

1. Add the **OpenAI DALLE 3** node in ComfyUI
2. Enter the description of the image you want to generate in the prompt text box
3. Adjust optional parameters as needed (quality, style, size, etc.)
4. Run the workflow to generate the image

## FAQs

<AccordionGroup>
  <Accordion title="Why can't I find the API nodes?">
    Please update your ComfyUI to the latest version (the latest commit or the latest [desktop version](https://www.comfy.org/download)).
    We may add more API support in the future, and the corresponding nodes will be updated, so please keep your ComfyUI up to date.

    <Tip>
      Please note that you need to distinguish between the nightly version and the release version.
      In some cases, the latest `release` version may not be updated in time compared to the `nightly` version.
      Since we are still iterating quickly, please ensure you are using the latest version when you cannot find the corresponding node.
    </Tip>
  </Accordion>

  <Accordion title="Why can't I use / log in to the API Nodes?">
    API access requires that your current request is based on a secure network environment. The current requirements for API access are as follows:

    * The local network only allows access from `127.0.0.1` or `localhost`, which may mean that you cannot use the API Nodes in a ComfyUI service started with the `--listen` parameter in a LAN environment.
    * Able to access our API service normally (a proxy service may be required in some regions).
    * Your account does not have enough [credits](/interface/credits).
  </Accordion>

  <Accordion title="Why can't I use API node even after logging in, or why does it keep asking me to log in while using?">
    * Currently, only `127.0.0.1` or `localhost` access is supported.
    * Ensure your account has enough credits.
  </Accordion>

  <Accordion title="Can API Nodes be used for free?">
    API Nodes require credits for API calls to closed-source models, so they do not support free usage.
  </Accordion>

  <Accordion title="How to purchase credits?">
    Please refer to the following documentation:

    1. [Comfy Account](/interface/user): Find the `User` section in the settings menu to log in.
    2. [Credits](/interface/credits): After logging in, the settings interface will show the credits menu. You can purchase credits in `Settings`  `Credits`. We use a prepaid system, so there will be no unexpected charges.
    3. Complete the payment through Stripe.
    4. Check if the credits have been updated. If not, try restarting or refreshing the page.
  </Accordion>

  <Accordion title="Are unused credits refundable?">
    Currently, we do not support refunds for credits.
    If you believe there is an error resulting in unused balance due to technical issues, please [contact support](mailto:support@comfy.org).
  </Accordion>

  <Accordion title="Can credits go negative?">
    Credits cannot go negative, so please ensure you have enough credits before making the corresponding API calls.
  </Accordion>

  <Accordion title="Where can I check usage and expenses?">
    Please visit the [Credits](/interface/credits) menu after logging in to check the corresponding credits.
  </Accordion>

  <Accordion title="Is it possible to use my own API Key?">
    Currently, the API Nodes are still in the testing phase and do not support this feature yet, but we have considered adding it.
  </Accordion>

  <Accordion title="Do credits expire?">
    No, your credits do not expire.
  </Accordion>

  <Accordion title="Can credits be transferred or shared?">
    No, your credits cannot be transferred to other users and are limited to the currently logged-in account, but we do not restrict the number of devices that can log in.
  </Accordion>

  <Accordion title="Can I use the same account on different devices?">
    We do not limit the number of devices that can log in; you can use your account anywhere you want.
  </Accordion>

  <Accordion title="How can I request for my account or information to be deleted??">
    Email a request to [support@comfy.org](mailto:support@comfy.org) and we will delete your information
  </Accordion>
</AccordionGroup>


# OpenAI GPT-Image-1 Node
Source: https://docs.comfy.org/tutorials/partner-nodes/openai/gpt-image-1

Learn how to use the OpenAI GPT-Image-1 Partner node to generate images in ComfyUI

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/api_nodes/openai-gpt-image-1.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=243b4200a707b55f1c4fd88d4edf51a9" alt="OpenAI GPT-Image-1 Node Screenshot" data-og-width="1576" width="1576" data-og-height="1123" height="1123" data-path="images/comfy_core/api_nodes/openai-gpt-image-1.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/api_nodes/openai-gpt-image-1.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=2d4ad0d8fdf6ac49815d5de1f21c0df7 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/api_nodes/openai-gpt-image-1.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=868f4b2403c8ce07803227285cd6f2fc 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/api_nodes/openai-gpt-image-1.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=ab08d31d2151a589f569d13cc10692ce 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/api_nodes/openai-gpt-image-1.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=5993067ce1d818c9bcfd5147537ae256 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/api_nodes/openai-gpt-image-1.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=f2e101e964fad443efd7374c2f9ce37b 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/api_nodes/openai-gpt-image-1.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=44cf580b967693bfdc2c49a6cc25aeb7 2500w" />

OpenAI GPT-Image-1 is part of the ComfyUI Partner nodes series that allows users to generate images through OpenAI's **GPT-Image-1** model. This is the same model used for image generation in ChatGPT 4o.

This node supports:

* Text-to-image generation
* Image editing functionality (inpainting through masks)

## Node Overview

The **OpenAI GPT-Image-1** node synchronously generates images through OpenAI's image generation API. It receives text prompts and returns images matching the description. GPT-Image-1 is OpenAI's most advanced image generation model currently available, capable of creating highly detailed and realistic images.

<Tip>
  To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](/tutorials/partner-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.
</Tip>

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Parameter Description

### Required Parameters

| Parameter | Type | Description                                                   |
| --------- | ---- | ------------------------------------------------------------- |
| `prompt`  | Text | Text prompt describing the image content you want to generate |

### Widget Parameters

| Parameter    | Type    | Options                               | Default | Description                                             |
| ------------ | ------- | ------------------------------------- | ------- | ------------------------------------------------------- |
| `seed`       | Integer | 0-2147483647                          | 0       | Random seed used to control generation results          |
| `quality`    | Option  | low, medium, high                     | low     | Image quality setting, affects cost and generation time |
| `background` | Option  | opaque, transparent                   | opaque  | Whether the returned image has a background             |
| `size`       | Option  | auto, 1024x1024, 1024x1536, 1536x1024 | auto    | Size of the generated image                             |
| `n`          | Integer | 1-8                                   | 1       | Number of images to generate                            |

### Optional Parameters

| Parameter | Type  | Options         | Default | Description                                                 |
| --------- | ----- | --------------- | ------- | ----------------------------------------------------------- |
| `image`   | Image | Any image input | None    | Optional reference image for image editing                  |
| `mask`    | Mask  | Mask input      | None    | Optional mask for inpainting (white areas will be replaced) |

## Usage Examples

### Text-to-Image Example

The image below contains a simple text-to-image workflow. Please download the image and drag it into ComfyUI to load the corresponding workflow.
![ComfyUI openai-gpt-image-1 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/text2image.png)

The corresponding workflow is very simple:
<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/gpt-image-1/text2image.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=e7308997e027153d914a61418a9c0058" alt="ComfyUI openai-gpt-image-1 workflow example" data-og-width="3580" width="3580" data-og-height="1956" height="1956" data-path="images/tutorial/api_nodes/openai/gpt-image-1/text2image.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/gpt-image-1/text2image.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=e21f0cd8c845d34ed3fd4016bce71fd5 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/gpt-image-1/text2image.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=e06c844a22bd4bf1e2fca9ef2052f720 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/gpt-image-1/text2image.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=34f42055304ee347d962051da6a00102 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/gpt-image-1/text2image.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=3ad0d9e30f20fe8c535d4238e331a5bc 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/gpt-image-1/text2image.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=24830df172d325f9593696d59c0aedcf 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/gpt-image-1/text2image.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=a3b050cf1382fb378f45aac540259d0c 2500w" />

You only need to load the `OpenAI GPT-Image-1` node, input the description of the image you want to generate in the `prompt` node, connect a `Save Image` node, and then run the workflow.

### Image-to-Image Example

The image below contains a simple image-to-image workflow. Please download the image and drag it into ComfyUI to load the corresponding workflow.
![ComfyUI openai-gpt-image-1 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/image2image.png)

We will use the image below as input:
![ComfyUI openai-gpt-image-1 workflow input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/input.webp)

In this workflow, we use the `OpenAI GPT-Image-1` node to generate images and the `Load Image` node to load the input image, then connect it to the `image` input of the `OpenAI GPT-Image-1` node.

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/gpt-image-1/image2image.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=05cab30de9ab5bc2e0865c0a367f82fc" alt="ComfyUI openai-gpt-image-1 workflow example" data-og-width="3180" width="3180" data-og-height="1114" height="1114" data-path="images/tutorial/api_nodes/openai/gpt-image-1/image2image.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/gpt-image-1/image2image.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=419c2f8470d671c9f9876ba8d2b64379 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/gpt-image-1/image2image.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=402654cfaf0701617b2e0d6c2b47414e 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/gpt-image-1/image2image.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=1916ef7321e796d7e31b3d05462d64ce 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/gpt-image-1/image2image.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=b5b6d1a301172a877855fb42c896e978 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/gpt-image-1/image2image.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=08ea4d43044625afecb002b0cedd6aaf 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/gpt-image-1/image2image.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=c7a65d6109ebd439564cb1830c93db00 2500w" />

### Multiple Image Input Example

Please download the image below and drag it into ComfyUI to load the corresponding workflow.

![Multiple Image Input Example](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/multiple_image_input.png)

Use the hat image below as an additional input image.
![Hat](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/hat.webp)

The corresponding workflow is shown in the image below:
<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/gpt-image-1/multi_images_input.png?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=125236ed762b17ae1e711fa702663f74" alt="Multiple Image Input Example" data-og-width="1432" width="1432" data-og-height="748" height="748" data-path="images/tutorial/api_nodes/openai/gpt-image-1/multi_images_input.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/gpt-image-1/multi_images_input.png?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=a51e55f0d4de107f42cdc2f576d798f9 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/gpt-image-1/multi_images_input.png?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=7ffbaa360a6a18c11e1c37c4cb93de6a 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/gpt-image-1/multi_images_input.png?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=c95bf1edf0e7c09b269693788d856149 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/gpt-image-1/multi_images_input.png?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=c0fad9f36754684b3fa540c2f4e5575a 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/gpt-image-1/multi_images_input.png?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=cd3e53553401ddbdf57db68659c30be7 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/gpt-image-1/multi_images_input.png?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=07213db91090f513a00548170a21c346 2500w" />

The `Batch Images` node is used to load multiple images into the `OpenAI GPT-Image-1` node.

### Inpainting Workflow

GPT-Image-1 also supports image editing functionality, allowing you to specify areas to replace using a mask. Below is a simple inpainting workflow example:

Download the image below and drag it into ComfyUI to load the corresponding workflow. We will continue to use the input image from the image-to-image workflow section.

![ComfyUI openai-gpt-image-1 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/GPT-Image-1/inpaint.png)

The corresponding workflow is shown in the image
<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/gpt-image-1/inpaint.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=12d74944bf8905025606740b820918a8" alt="ComfyUI openai-gpt-image-1 workflow example" data-og-width="3154" width="3154" data-og-height="1156" height="1156" data-path="images/tutorial/api_nodes/openai/gpt-image-1/inpaint.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/gpt-image-1/inpaint.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=6fef6c79f60a91ae564e84f8361dd5a7 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/gpt-image-1/inpaint.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=1cf77f379b0f0326bdc4165f445c87b4 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/gpt-image-1/inpaint.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=d23e66dde56fcab9ea4cf500151848ec 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/gpt-image-1/inpaint.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=fa900de7686e8c71a858c4e0e24c7687 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/gpt-image-1/inpaint.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=60618c269a1433faf14fa72fb7da0a97 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/gpt-image-1/inpaint.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=937496ea2ab4f3b6bda82b20bd43db80 2500w" />

Compared to the image-to-image workflow, we use the MaskEditor in the `Load Image` node through the right-click menu to draw a mask, then connect it to the `mask` input of the `OpenAI GPT-Image-1` node to complete the workflow.

**Notes**

* The mask and image must be the same size
* When inputting large images, the node will automatically resize the image to an appropriate size

## FAQs

<AccordionGroup>
  <Accordion title="Why can't I find the API nodes?">
    Please update your ComfyUI to the latest version (the latest commit or the latest [desktop version](https://www.comfy.org/download)).
    We may add more API support in the future, and the corresponding nodes will be updated, so please keep your ComfyUI up to date.

    <Tip>
      Please note that you need to distinguish between the nightly version and the release version.
      In some cases, the latest `release` version may not be updated in time compared to the `nightly` version.
      Since we are still iterating quickly, please ensure you are using the latest version when you cannot find the corresponding node.
    </Tip>
  </Accordion>

  <Accordion title="Why can't I use / log in to the API Nodes?">
    API access requires that your current request is based on a secure network environment. The current requirements for API access are as follows:

    * The local network only allows access from `127.0.0.1` or `localhost`, which may mean that you cannot use the API Nodes in a ComfyUI service started with the `--listen` parameter in a LAN environment.
    * Able to access our API service normally (a proxy service may be required in some regions).
    * Your account does not have enough [credits](/interface/credits).
  </Accordion>

  <Accordion title="Why can't I use API node even after logging in, or why does it keep asking me to log in while using?">
    * Currently, only `127.0.0.1` or `localhost` access is supported.
    * Ensure your account has enough credits.
  </Accordion>

  <Accordion title="Can API Nodes be used for free?">
    API Nodes require credits for API calls to closed-source models, so they do not support free usage.
  </Accordion>

  <Accordion title="How to purchase credits?">
    Please refer to the following documentation:

    1. [Comfy Account](/interface/user): Find the `User` section in the settings menu to log in.
    2. [Credits](/interface/credits): After logging in, the settings interface will show the credits menu. You can purchase credits in `Settings`  `Credits`. We use a prepaid system, so there will be no unexpected charges.
    3. Complete the payment through Stripe.
    4. Check if the credits have been updated. If not, try restarting or refreshing the page.
  </Accordion>

  <Accordion title="Are unused credits refundable?">
    Currently, we do not support refunds for credits.
    If you believe there is an error resulting in unused balance due to technical issues, please [contact support](mailto:support@comfy.org).
  </Accordion>

  <Accordion title="Can credits go negative?">
    Credits cannot go negative, so please ensure you have enough credits before making the corresponding API calls.
  </Accordion>

  <Accordion title="Where can I check usage and expenses?">
    Please visit the [Credits](/interface/credits) menu after logging in to check the corresponding credits.
  </Accordion>

  <Accordion title="Is it possible to use my own API Key?">
    Currently, the API Nodes are still in the testing phase and do not support this feature yet, but we have considered adding it.
  </Accordion>

  <Accordion title="Do credits expire?">
    No, your credits do not expire.
  </Accordion>

  <Accordion title="Can credits be transferred or shared?">
    No, your credits cannot be transferred to other users and are limited to the currently logged-in account, but we do not restrict the number of devices that can log in.
  </Accordion>

  <Accordion title="Can I use the same account on different devices?">
    We do not limit the number of devices that can log in; you can use your account anywhere you want.
  </Accordion>

  <Accordion title="How can I request for my account or information to be deleted??">
    Email a request to [support@comfy.org](mailto:support@comfy.org) and we will delete your information
  </Accordion>
</AccordionGroup>


# Partner Nodes
Source: https://docs.comfy.org/tutorials/partner-nodes/overview

In this article, we will introduce ComfyUI's Partner Nodes and related information.

Partner Nodes are ComfyUI's new way of calling closed-source models through API requests, providing ComfyUI users with access to external state-of-the-art AI models without complex API key setup.

## What are Partner Nodes?

Partner Nodes are a set of special nodes that connect to external API services, allowing you to use closed-source or third-party hosted AI models directly in your ComfyUI workflows. These nodes are designed to seamlessly integrate the capabilities of external models while maintaining the open-source nature of ComfyUI's core.

Currently supported models include:

* **Black Forest Labs**: Flux 1.1\[pro] Ultra, Flux .1\[pro], Flux .1 Kontext Pro, Flux .1 Kontext Max, Flux.1 Canny Control, Flux.1 Depth Control, Flux.1 Expand, Flux.1 Fill
* **Google**: Veo2, Veo 3.0, Gemini 2.5 Pro, Gemini 2.5 Flash, Gemini Image
* **Ideogram**: V3, V2, V1
* **Kling**: 2.0, 1.6, 1.5, v1, v2-1, v2-1-master, v2-maser & Various Effects
* **Luma**: Photon, Ray2, Ray1.6, Ray-flash-2, Photo-flash-1
* **MiniMax**: Text-to-Video, Image-to-Video, Hailuo-02, T2V-01, I2V-01
* **Moonvalley**: Image-to-Video, Text-to-Video, Video-to-Video
* **OpenAI**: o1, o1-pro, o3, o4-mini, gpt-4o, gpt-4.1, gpt-4.1-mini, gpt-4.1-nano, gpt-5, gpt-5-mini, gpt-5-nano, DALLE 2, DALLE 3, GPT-Image-1
* **PixVerse**: V4 & Effects
* **Pika**: 2.2
* **Recraft**: V3, V2 & Various Tools
* **Rodin**: 3D Generation
* **Runway**: Text-to-Image, First-Last-Frame to Video, Image to Video (Gen3a Turbo, Gen4 Turbo)
* **Stability AI**: Stable Image Ultra, Stable Diffusion 3.5 Large, Stable Diffusion 3.5 Medium, Image Upscale
* **Tripo**: v1-4, v2.0, v2.5
* **Vidu**: Image-to-Video, Reference Video, Start-End to Video, Text-to-Video

## Prerequisites for Using API Nodes

To use API Nodes, the following requirements must be met:

### 1. ComfyUI Version Requirements

Please update your ComfyUI to the latest version, as we may add more API support in the future, and corresponding nodes will be updated, so please keep your ComfyUI up to date.

<Tip>
  Please note the distinction between nightly and release versions. We recommend using the `nightly` version (which is the latest code commit), as the release version may not be updated in a timely manner.
  This refers to the development version and the stable version, and since we are still rapidly iterating, this document may not be updated promptly, so please pay attention to the version differences.
</Tip>

### 2. Account and Credits Requirements

You need to be logged into your ComfyUI with a [Comfy account](/interface/user) and have a credit balance of [credits](/interface/credits) greater than 0.

Log in via `Settings` -> `User`:

<img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=066170b38e0b9ead026029685e00fa65" alt="ComfyUI User Interface" data-og-width="3358" width="3358" data-og-height="1828" height="1828" data-path="images/interface/setting/user.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=c7f1a017e9b00c6224a440f83d121a59 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=92b830f85f4393d802f7f33bdce81634 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=e294573cc054158fb3a108d10bc67087 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=5160ea18d19dfdde201bbf41dbb1af0b 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=463f4645799b84ea5dcd4879ed3b87ca 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=e52e827f35eddf444e91e5ed4f11b331 2500w" />

Go to `Settings` -> `Credits` to purchase credits
<img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/purchase-1.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=a324b33ce2d6dce5bf2d0e895ac0d1eb" alt="Credits Interface" data-og-width="3358" width="3358" data-og-height="1828" height="1828" data-path="images/interface/setting/purchase-1.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/purchase-1.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=3be3740eb5ebef1f96177e261d356394 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/purchase-1.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=639eca3dce7544af869871fbc50187a8 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/purchase-1.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=e36ad4124f9eaeeb5ac3c83481c55eef 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/purchase-1.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=05042870ef1f4b2270c6053b3903018e 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/purchase-1.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=06ae7e8245f405be7e80c80808a50dbe 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/purchase-1.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=3544af87748d5ef4b2a4966d08a2bcdd 2500w" />

Please refer to the corresponding documentation for account and credits to ensure this requirement:

* [Comfy account](/interface/user): Find the `User` section in the settings menu to log in.
* [Credits](/interface/credits): After logging in, the settings interface will show a credits menu where you can purchase credits. We use a prepaid system, so there will be no unexpected charges.

### 3. Network Environment Requirements

API access requires that your current requests are based on a secure network environment. The current requirements for API access are as follows:

* The local network only allows access from `127.0.0.1` or `localhost`, and you can directly use the login function.
* If you are accessing from a local area network or a website that is not on the whitelist, please log in with an API Key. Please refer to [Log in with an API Key](/interface/user#logging-in-with-an-api-key).
* You should be able to access our API service normally (in some regions, you may need to use a proxy service).
* Access should be carried out in an `https` environment to ensure the security of the requests.

<Note>
  Accessing in an insecure context poses significant risks, which may result in the following consequences:

  1. Authentication may be stolen, leading to the leakage of your account information.
  2. Your account may be maliciously used, resulting in financial losses.

  Even if we open this restriction in the future, we strongly advise against accessing API services through insecure network requests due to the high risks involved.
</Note>

### 4. Using the Corresponding Nodes

**Add to Workflow**: Add the API node to your workflow just like you would with other nodes.
**Run**: Set the parameters and then run the workflow.

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/sidebar.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=91c9219b9648e55c19a95359060eeb56" alt="API Nodes" data-og-width="784" width="784" data-og-height="773" height="773" data-path="images/tutorial/api_nodes/sidebar.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/sidebar.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=7179545b60c6a046bd0d309914a0000f 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/sidebar.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=f92ab198e88fa7e15fd03ec421214ac9 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/sidebar.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=b3384e5b0bd6a825865c30d169abcea0 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/sidebar.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=c41b83cf96aea58cef7d0191e35ec278 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/sidebar.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=f3f4d2d7183c91491b8ba7672906950f 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/sidebar.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=4485b5b141edfc6f27a421e9d91f6eeb 2500w" />

## Log in with ComfyUI Account API Key on non-whitelisted websites

Currently, we have set up a whitelist to restrict the websites where you can log in to your ComfyUI account.
If you need to log in to your ComfyUI account on some non-whitelisted websites, please refer to the account management section to learn how to log in using an API Key.
In this case, the corresponding website does not need to be on our whitelist.

<Card title="Account Management" icon="book" href="/interface/user#logging-in-with-an-api-key">
  Learn how to log in with ComfyUI Account API Key
</Card>

<img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-1.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=bdb8d99a4bd52a6cc1de1b3453e8cfda" alt="Select Comfy API Key Login" data-og-width="3450" width="3450" data-og-height="1914" height="1914" data-path="images/interface/setting/user/user-login-api-1.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-1.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=84683bcf8e9b1f53885f54175cd83b87 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-1.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=ae30f89ae6d9a7b97e41f69d3ae0e9f6 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-1.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=4dce9815e1729742abd819ce400429ad 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-1.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=148e2ee529690c7985999f64841f8fcd 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-1.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=dca8a17b4e613ba65ee0d1dca67f4b28 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-1.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=0d1ab01184bd504d62531abfb88abb57 2500w" />

## Use ComfyUI Account API Key Integration to call paid model Partner nodes

Currently, we support accessing our services through ComfyUI API Key Integration to call paid model Partner nodes. Please refer to the API Key Integration section to learn how to use API Key Integration to call paid model Partner nodes.

<Note>
  **Important:** The API key discussed here is your **ComfyUI Account API Key** (used for accessing paid Partner nodes in workflows). This is **NOT** the same as the **Registry Publishing API Key** used by developers to publish custom nodes to the registry. If you're looking to publish custom nodes, see [Publishing Nodes](/registry/publishing).
</Note>

<Card title="API Key Integration" icon="link" href="/development/comfyui-server/api-key-integration">
  Please refer to the API Key Integration section to learn how to use API Key Integration to call paid model Partner nodes
</Card>

## Advantages of Partner Nodes

Partner Nodes provide several important advantages for ComfyUI users:

* **Access to closed-source models**: Use state-of-the-art AI models without having to deploy them yourself
* **Seamless integration**: Partner nodes are fully compatible with other ComfyUI nodes and can be combined to create complex workflows
* **Simplified experience**: No need to manage API keys or handle complex API requests
* **Controlled costs**: The prepaid system ensures you have complete control over your spending with no unexpected charges

## Pricing

<Card title="Partner Node Pricing" icon="coin" href="/tutorials/partner-nodes/pricing">
  Please refer to the pricing page for the corresponding Partner pricing
</Card>

## About Open Source and Opt-in

It's important to note that **Partner Nodes are completely optional**. ComfyUI will always remain fully open-source and free for local users. Partner nodes are designed as an "opt-in" feature, providing convenience for those who want access to external SOTA (state-of-the-art) models.

## Use Cases

A powerful application of Partner Nodes is combining the output of external models with local nodes. For example:

* Using GPT-Image-1 to generate a base image, then transforming it into video with a local `wan` node
* Combining externally generated images with local upscaling or style transfer nodes
* Creating hybrid workflows that leverage the advantages of both closed-source and open-source models

This flexibility makes ComfyUI a truly universal generative AI interface, integrating various AI capabilities into a unified workflow, opening up more possibilities

## FAQs

<AccordionGroup>
  <Accordion title="Why can't I find the API nodes?">
    Please update your ComfyUI to the latest version (the latest commit or the latest [desktop version](https://www.comfy.org/download)).
    We may add more API support in the future, and the corresponding nodes will be updated, so please keep your ComfyUI up to date.

    <Tip>
      Please note that you need to distinguish between the nightly version and the release version.
      In some cases, the latest `release` version may not be updated in time compared to the `nightly` version.
      Since we are still iterating quickly, please ensure you are using the latest version when you cannot find the corresponding node.
    </Tip>
  </Accordion>

  <Accordion title="Why can't I use / log in to the API Nodes?">
    API access requires that your current request is based on a secure network environment. The current requirements for API access are as follows:

    * The local network only allows access from `127.0.0.1` or `localhost`, which may mean that you cannot use the API Nodes in a ComfyUI service started with the `--listen` parameter in a LAN environment.
    * Able to access our API service normally (a proxy service may be required in some regions).
    * Your account does not have enough [credits](/interface/credits).
  </Accordion>

  <Accordion title="Why can't I use API node even after logging in, or why does it keep asking me to log in while using?">
    * Currently, only `127.0.0.1` or `localhost` access is supported.
    * Ensure your account has enough credits.
  </Accordion>

  <Accordion title="Can API Nodes be used for free?">
    API Nodes require credits for API calls to closed-source models, so they do not support free usage.
  </Accordion>

  <Accordion title="How to purchase credits?">
    Please refer to the following documentation:

    1. [Comfy Account](/interface/user): Find the `User` section in the settings menu to log in.
    2. [Credits](/interface/credits): After logging in, the settings interface will show the credits menu. You can purchase credits in `Settings`  `Credits`. We use a prepaid system, so there will be no unexpected charges.
    3. Complete the payment through Stripe.
    4. Check if the credits have been updated. If not, try restarting or refreshing the page.
  </Accordion>

  <Accordion title="Are unused credits refundable?">
    Currently, we do not support refunds for credits.
    If you believe there is an error resulting in unused balance due to technical issues, please [contact support](mailto:support@comfy.org).
  </Accordion>

  <Accordion title="Can credits go negative?">
    Credits cannot go negative, so please ensure you have enough credits before making the corresponding API calls.
  </Accordion>

  <Accordion title="Where can I check usage and expenses?">
    Please visit the [Credits](/interface/credits) menu after logging in to check the corresponding credits.
  </Accordion>

  <Accordion title="Is it possible to use my own API Key?">
    Currently, the API Nodes are still in the testing phase and do not support this feature yet, but we have considered adding it.
  </Accordion>

  <Accordion title="Do credits expire?">
    No, your credits do not expire.
  </Accordion>

  <Accordion title="Can credits be transferred or shared?">
    No, your credits cannot be transferred to other users and are limited to the currently logged-in account, but we do not restrict the number of devices that can log in.
  </Accordion>

  <Accordion title="Can I use the same account on different devices?">
    We do not limit the number of devices that can log in; you can use your account anywhere you want.
  </Accordion>

  <Accordion title="How can I request for my account or information to be deleted??">
    Email a request to [support@comfy.org](mailto:support@comfy.org) and we will delete your information
  </Accordion>
</AccordionGroup>


# Pricing
Source: https://docs.comfy.org/tutorials/partner-nodes/pricing

This article lists the pricing of the current Partner Nodes.

The following table lists the pricing of the current Partner Nodes. All prices are in USD.

## BFL

| Model(Node name)            | Category | Parameters that affect price | Parameter combo | Price \$ |
| --------------------------- | -------- | ---------------------------- | --------------- | -------- |
| Flux 1.1 \[pro] Ultra Image | Image    | NA                           | NA              | 0.06     |
| Flux.1 Canny Control Image  | Image    | NA                           | NA              | 0.05     |
| Flux.1 Depth Control Image  | Image    | NA                           | NA              | 0.05     |
| Flux.1 Expand Image         | Image    | NA                           | NA              | 0.05     |
| Flux.1 Fill Image           | Image    | NA                           | NA              | 0.05     |
| Flux.1 Kontext \[max] Image | Image    | NA                           | NA              | 0.08     |
| Flux.1 Kontext \[pro] Image | Image    | NA                           | NA              | 0.04     |
| Flux.1 Kontext \[pro] Image | Image    | NA                           | NA              | 0.05     |

## ByteDance

| Model(Node name)                        | Category | Parameters that affect price | Parameter combo                                | Price \$              |
| --------------------------------------- | -------- | ---------------------------- | ---------------------------------------------- | --------------------- |
| Image Edit                              | Image    | model                        | bytedance-seedream-4.0                         | 0.03                  |
| Image Edit                              | Image    | model                        | bytedance-seededit-3.0-i2i                     | 0.03                  |
| Image Generation                        | Image    | model                        | bytedance-seedream-3.0-t2i                     | 0.03                  |
| Video Generation - Lite - Image 2 Video | Video    | duration, model, type        | seedance-1-0-lite-i2v-250428, image\_to\_video | \$1.8/1M total tokens |
| Video Generation - Lite - Text 2 Video  | Video    | duration, model, type        | seedance-1-0-lite-t2v-250428, text\_to\_video  | \$1.8/1M total tokens |
| Video Generation - Pro - Image 2 Video  | Video    | duration, model, type        | seedance-1-0-pro-250528, image\_to\_video      | \$2.5/1M total tokens |
| Video Generation - Pro - Text 2 Video   | Video    | duration, model, type        | seedance-1-0-pro-250528, text\_to\_video       | \$2.5/1M total tokens |

## Kling

| Model(Node name)                     | Category | Parameters that affect price | Parameter combo                                | Price \$ |
| ------------------------------------ | -------- | ---------------------------- | ---------------------------------------------- | -------- |
| Kling Image Generation               | Image    | image input, model\_name, n  | kling-v1-5, 1, image to image                  | 0.028    |
| Kling Image Generation               | Image    | image input, model\_name, n  | kling-v1-5, 1, text to image                   | 0.014    |
| Kling Image Generation               | Image    | image input, model\_name, n  | kling-v1, 1, image to image                    | 0.0035   |
| Kling Image Generation               | Image    | image input, model\_name, n  | kling-v1, 1, text to image                     | 0.0035   |
| Kling Image Generation               | Image    | image input, model\_name, n  | kling-v2, 1, text to image                     | 0.014    |
| Kling Virtual Try On                 | Image    | NA                           | NA                                             | 0.07     |
| Kling Text to Video (Camera Control) | Video    | NA                           | NA                                             | 0.49     |
| Kling Dual Character Video Effects   | Video    | duration, mode, model\_name  | kling-v1-5, pro, 5                             | 0.49     |
| Kling Dual Character Video Effects   | Video    | duration, mode, model\_name  | kling-v1-5, pro, 10                            | 0.98     |
| Kling Dual Character Video Effects   | Video    | duration, mode, model\_name  | kling-v1-5, std, 5                             | 0.28     |
| Kling Dual Character Video Effects   | Video    | duration, mode, model\_name  | kling-v1-5, std, 10                            | 0.56     |
| Kling Dual Character Video Effects   | Video    | duration, mode, model\_name  | kling-v1-6, pro, 5                             | 0.49     |
| Kling Dual Character Video Effects   | Video    | duration, mode, model\_name  | kling-v1-6, pro, 10                            | 0.98     |
| Kling Dual Character Video Effects   | Video    | duration, mode, model\_name  | kling-v1-6, std, 5                             | 0.28     |
| Kling Dual Character Video Effects   | Video    | duration, mode, model\_name  | kling-v1-6, std, 10                            | 0.56     |
| Kling Dual Character Video Effects   | Video    | duration, mode, model\_name  | kling-v1, pro, 5                               | 0.49     |
| Kling Dual Character Video Effects   | Video    | duration, mode, model\_name  | kling-v1, pro, 10                              | 0.98     |
| Kling Dual Character Video Effects   | Video    | duration, mode, model\_name  | kling-v1, std, 5                               | 0.14     |
| Kling Dual Character Video Effects   | Video    | duration, mode, model\_name  | kling-v1, std, 10                              | 0.28     |
| Kling Image to Video                 | Video    | duration, mode, model\_name  | kling-v1-5, pro, 5                             | 0.49     |
| Kling Image to Video                 | Video    | duration, mode, model\_name  | kling-v1-5, pro, 10                            | 0.98     |
| Kling Image to Video                 | Video    | duration, mode, model\_name  | kling-v1-5, std, 5                             | 0.28     |
| Kling Image to Video                 | Video    | duration, mode, model\_name  | kling-v1-5, std, 10                            | 0.56     |
| Kling Image to Video                 | Video    | duration, mode, model\_name  | kling-v1-6, pro, 5                             | 0.49     |
| Kling Image to Video                 | Video    | duration, mode, model\_name  | kling-v1-6, pro, 10                            | 0.98     |
| Kling Image to Video                 | Video    | duration, mode, model\_name  | kling-v1-6, std, 5                             | 0.28     |
| Kling Image to Video                 | Video    | duration, mode, model\_name  | kling-v1-6, std, 10                            | 0.56     |
| Kling Image to Video                 | Video    | duration, mode, model\_name  | kling-v1, pro, 5                               | 0.49     |
| Kling Image to Video                 | Video    | duration, mode, model\_name  | kling-v1, pro, 10                              | 0.98     |
| Kling Image to Video                 | Video    | duration, mode, model\_name  | kling-v1, std, 5                               | 0.14     |
| Kling Image to Video                 | Video    | duration, mode, model\_name  | kling-v1, std, 10                              | 0.28     |
| Kling Image to Video                 | Video    | duration, model, model\_name | kling-v2-1-master, 5s                          | 1.4      |
| Kling Image to Video                 | Video    | duration, model, model\_name | kling-v2-1-master, 10s                         | 2.8      |
| Kling Image to Video                 | Video    | duration, model, model\_name | kling-v2-1, pro, 5s                            | 0.49     |
| Kling Image to Video                 | Video    | duration, model, model\_name | kling-v2-1, pro, 10s                           | 0.98     |
| Kling Image to Video                 | Video    | duration, model, model\_name | kling-v2-1, std, 5s                            | 0.28     |
| Kling Image to Video                 | Video    | duration, model, model\_name | kling-v2-1, std, 10s                           | 0.56     |
| Kling Image to Video                 | Video    | duration, mode, model\_name  | kling-v2-maser, pro, 5s                        | 1.4      |
| Kling Image to Video                 | Video    | duration, mode, model\_name  | kling-v2-maser, pro, 10s                       | 2.8      |
| Kling Image to Video                 | Video    | duration, mode, model\_name  | kling-v2-maser, std, 5s                        | 1.4      |
| Kling Image to Video                 | Video    | duration, mode, model\_name  | kling-v2-maser, std, 10s                       | 2.8      |
| Kling Lip Sync Video with Audio      | Video    | output\_length               | 5s                                             | 0.07     |
| Kling Lip Sync Video with Audio      | Video    | output\_length               | 10s                                            | 0.14     |
| Kling Lip Sync Video with Text       | Video    | output\_length               | 5s                                             | 0.07     |
| Kling Lip Sync Video with Text       | Video    | output\_length               | 10s                                            | 0.14     |
| Kling Start-End Frame to Video       | Video    | mode                         | pro mode / 5s duration / kling-v1              | 0.49     |
| Kling Start-End Frame to Video       | Video    | mode                         | pro mode / 5s duration / kling-v1-5            | 0.49     |
| Kling Start-End Frame to Video       | Video    | mode                         | pro mode / 5s duration / kling-v1-6            | 0.49     |
| Kling Start-End Frame to Video       | Video    | mode                         | pro mode / 10s duration / kling-v1-5           | 0.98     |
| Kling Start-End Frame to Video       | Video    | mode                         | pro mode / 10s duration / kling-v1-6           | 0.98     |
| Kling Start-End Frame to Video       | Video    | mode                         | standard mode / 5s duration / kling-v1         | 0.14     |
| Kling Text to Video                  | Video    | mode                         | 5s duration / kling-v2-1-master                | 1.4      |
| Kling Text to Video                  | Video    | mode                         | 10s duration / kling-v2-1-master               | 2.8      |
| Kling Text to Video                  | Video    | mode                         | pro mode / 5s duration / kling-v1              | 0.49     |
| Kling Text to Video                  | Video    | mode                         | pro mode / 5s duration / kling-v2-master       | 1.4      |
| Kling Text to Video                  | Video    | mode                         | pro mode / 10s duration / kling-v1             | 0.98     |
| Kling Text to Video                  | Video    | mode                         | pro mode / 10s duration / kling-v2-master      | 2.8      |
| Kling Text to Video                  | Video    | mode                         | standard mode / 5s duration / kling-v1         | 0.14     |
| Kling Text to Video                  | Video    | mode                         | standard mode / 5s duration / kling-v1-6       | 0.28     |
| Kling Text to Video                  | Video    | mode                         | standard mode / 5s duration / kling-v2-master  | 1.4      |
| Kling Text to Video                  | Video    | mode                         | standard mode / 10s duration / kling-v1        | 0.28     |
| Kling Text to Video                  | Video    | mode                         | standard mode / 10s duration / kling-v1-6      | 0.56     |
| Kling Text to Video                  | Video    | mode                         | standard mode / 10s duration / kling-v2-master | 2.8      |
| Kling Video Effects                  | Video    | duration, effect\_scene      | dizzydizzy or bloombloom, 5                    | 0.49     |
| Kling Video Effects                  | Video    | duration, effect\_scene      | fuzzyfuzzy or squish or expansion, 5           | 0.28     |
| Kling Video Extend                   | Video    | NA                           | NA                                             | 0.28     |
| Kling Text to Video                  | Video    | duration, model\_name        | kling-v2-5-turbo, 5s                           | 0.35     |
| Kling Text to Video                  | Video    | duration, model\_name        | kling-v2-5-turbo, 10s                          | 0.7      |

## Lightricks

| Model(Node name)   | Category | Parameters that affect price | Parameter combo       | Price \$    |
| ------------------ | -------- | ---------------------------- | --------------------- | ----------- |
| LTX Text to Video  | Video    | model, resolution, duration  | ltx-2-fast, 1920x1080 | 0.04/second |
| LTX Text to Video  | Video    | model, resolution, duration  | ltx-2-fast, 2560x1440 | 0.08/second |
| LTX Text to Video  | Video    | model, resolution, duration  | ltx-2-fast, 3840x2160 | 0.16/second |
| LTX Text to Video  | Video    | model, resolution, duration  | ltx-2-pro, 1920x1080  | 0.06/second |
| LTX Text to Video  | Video    | model, resolution, duration  | ltx-2-pro, 2560x1440  | 0.12/second |
| LTX Text to Video  | Video    | model, resolution, duration  | ltx-2-pro, 3840x2160  | 0.24/second |
| LTX Image to Video | Video    | model, resolution, duration  | ltx-2-fast, 1920x1080 | 0.04/second |
| LTX Image to Video | Video    | model, resolution, duration  | ltx-2-fast, 2560x1440 | 0.08/second |
| LTX Image to Video | Video    | model, resolution, duration  | ltx-2-fast, 3840x2160 | 0.16/second |
| LTX Image to Video | Video    | model, resolution, duration  | ltx-2-pro, 1920x1080  | 0.06/second |
| LTX Image to Video | Video    | model, resolution, duration  | ltx-2-pro, 2560x1440  | 0.12/second |
| LTX Image to Video | Video    | model, resolution, duration  | ltx-2-pro, 3840x2160  | 0.24/second |

## Luma

| Model(Node name)    | Category | Parameters that affect price | Parameter combo        | Price \$ |
| ------------------- | -------- | ---------------------------- | ---------------------- | -------- |
| Luma Text to Image  | Image    | model                        | photo-flash-1          | 0.0019   |
| Luma Text to Image  | Image    | model                        | photo-flash-1          | 0.0019   |
| Luma Image to Image | Image    | model                        | photon-1               | 0.0073   |
| Luma Image to Image | Image    | model                        | photon-1               | 0.0073   |
| Luma Image to Video | Video    | duration, model, resolution  | ray-1-6, 720p, 5s      | 0.35     |
| Luma Image to Video | Video    | duration, model, resolution  | ray-2, 4k, 5s          | 6.37     |
| Luma Image to Video | Video    | duration, model, resolution  | ray-2, 4k, 9s          | 11.47    |
| Luma Image to Video | Video    | duration, model, resolution  | ray-2, 540p, 5s        | 0.4      |
| Luma Image to Video | Video    | duration, model, resolution  | ray-2, 540p, 9s        | 0.72     |
| Luma Image to Video | Video    | duration, model, resolution  | ray-2, 720p, 5s        | 0.71     |
| Luma Image to Video | Video    | duration, model, resolution  | ray-2, 720p, 9s        | 1.27     |
| Luma Image to Video | Video    | duration, model, resolution  | ray-2, 1080p, 5s       | 1.59     |
| Luma Image to Video | Video    | duration, model, resolution  | ray-2, 1080p, 9s       | 2.87     |
| Luma Image to Video | Video    | duration, model, resolution  | ray-flash-2, 4k, 5s    | 2.19     |
| Luma Image to Video | Video    | duration, model, resolution  | ray-flash-2, 4k, 9s    | 3.94     |
| Luma Image to Video | Video    | duration, model, resolution  | ray-flash-2, 540p, 5s  | 0.14     |
| Luma Image to Video | Video    | duration, model, resolution  | ray-flash-2, 540p, 9s  | 0.25     |
| Luma Image to Video | Video    | duration, model, resolution  | ray-flash-2, 720p, 5s  | 0.24     |
| Luma Image to Video | Video    | duration, model, resolution  | ray-flash-2, 720p, 9s  | 0.44     |
| Luma Image to Video | Video    | duration, model, resolution  | ray-flash-2, 1080p, 5s | 0.55     |
| Luma Image to Video | Video    | duration, model, resolution  | ray-flash-2, 1080p, 9s | 0.99     |
| Luma Text-to-video  | Video    | duration, model, resolution  | ray-1-6, 720p, 5s      | 0.35     |
| Luma Text-to-video  | Video    | duration, model, resolution  | ray-2, 4k, 5s          | 6.37     |
| Luma Text-to-video  | Video    | duration, model, resolution  | ray-2, 4k, 9s          | 11.47    |
| Luma Text-to-video  | Video    | duration, model, resolution  | ray-2, 540p, 5s        | 0.4      |
| Luma Text-to-video  | Video    | duration, model, resolution  | ray-2, 540p, 9s        | 0.72     |
| Luma Text-to-video  | Video    | duration, model, resolution  | ray-2, 720p, 5s        | 0.71     |
| Luma Text-to-video  | Video    | duration, model, resolution  | ray-2, 720p, 9s        | 1.27     |
| Luma Text-to-video  | Video    | duration, model, resolution  | ray-2, 1080p, 5s       | 1.59     |
| Luma Text-to-video  | Video    | duration, model, resolution  | ray-2, 1080p, 9s       | 2.87     |
| Luma Text-to-video  | Video    | duration, model, resolution  | ray-flash-2, 4k, 5s    | 2.19     |
| Luma Text-to-video  | Video    | duration, model, resolution  | ray-flash-2, 4k, 9s    | 3.94     |
| Luma Text-to-video  | Video    | duration, model, resolution  | ray-flash-2, 540p, 5s  | 0.14     |
| Luma Text-to-video  | Video    | duration, model, resolution  | ray-flash-2, 540p, 9s  | 0.25     |
| Luma Text-to-video  | Video    | duration, model, resolution  | ray-flash-2, 720p, 5s  | 0.24     |
| Luma Text-to-video  | Video    | duration, model, resolution  | ray-flash-2, 720p, 9s  | 0.44     |
| Luma Text-to-video  | Video    | duration, model, resolution  | ray-flash-2, 1080p, 5s | 0.55     |
| Luma Text-to-video  | Video    | duration, model, resolution  | ray-flash-2, 1080p, 9s | 0.99     |

## Google

| Model(Node name)                     | Category | Parameters that affect price | Parameter combo                     | Price \$                                                                                                                             |
| ------------------------------------ | -------- | ---------------------------- | ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |
| Google Veo 3.1 Video Generation      | Video    | model, generate\_audio       | veo-3.1-fast-generate-001, true     | \$0.15/second                                                                                                                        |
| Google Veo 3.1 Video Generation      | Video    | model, generate\_audio       | veo-3.1-standard-generate-001, true | \$0.40/second                                                                                                                        |
| Google Veo 3 Video Generatoin        | Video    | model, generate\_audio       | veo-3.0-fast-generate-001, false    | \$0.10/second                                                                                                                        |
| Google Veo 3 Video Generatoin        | Video    | model, generate\_audio       | veo-3.0-fast-generate-001, true     | \$0.15/second                                                                                                                        |
| Google Veo 3 Video Generatoin        | Video    | model, generate\_audio       | veo-3.0-generate-001, false         | \$0.20/second                                                                                                                        |
| Google Veo 3 Video Generatoin        | Video    | model, generate\_audio       | veo-3.0-generate-001, true          | \$0.40/second                                                                                                                        |
| Google Veo2 Video Generation         | Video    | duration                     | 5                                   | 2.5                                                                                                                                  |
| Google Veo2 Video Generation         | Video    | duration                     | 8                                   | 4                                                                                                                                    |
| Google Gemini                        | Text     | model                        | gemini-2.5-flash-preview-04-17      | $1.25/1M input tokens + $10/1M output tokens (\< 200K tokens)                                                                        |
| Google Gemini                        | Text     | model                        | gemini-2.5-pro-preview-05-06        | $0.16/1M input tokens + $0.6/1M output tokens + \$1/1M input audio tokens (\< 200K tokens)                                           |
| Google Gemini                        | Text     | model                        | gemini-2.5-flash                    | $0.3/1M input tokens(txt, img, vid) + $2.5/1M output tex tokens + \$1/1M input audio tokens (\< 200K tokens)                         |
| Google Gemini                        | Text     | model                        | gemini-2.5-pro                      | $1.25/1M input tokens (text, img, vid) + $10/1M output tokens (\< 200K total tokens)                                                 |
| Google Gemini                        | Text     | model                        | gemini-2.5-pro-preview-05-06        | $1.25/1M input tokens(text, img, vid) + $10/1M output tokens (\< 200K total tokens)                                                  |
| Google Gemini Image                  | Image    | model                        | gemini-2.5-flash-image-preview      | $0.3/1M input tokens (text, video, image) + $1/1M input tokens (audio) + $2.5/1M output tokens (text) + $30/1M output tokens (image) |
| Nano Banana Pro (Gemini 3 Pro Image) | Image    | model                        | gemini-pro-image-preview            | $0.134 per 1K & 2K image output + $0.24 per 4K image output                                                                          |

## Minimax

| Model(Node name)       | Category | Parameters that affect price | Parameter combo                    | Price \$ |
| ---------------------- | -------- | ---------------------------- | ---------------------------------- | -------- |
| Minimax Text to Video  | Video    | model                        | Hailuo-02 / 768 P / 6s             | 0.28     |
| Minimax Text to Video  | Video    | model                        | Hailuo-02 / 768 P / 10s            | 0.56     |
| Minimax Text to Video  | Video    | model                        | Hailuo-02 / 1080 P / 6s            | 0.49     |
| Minimax Text to Video  | Video    | model                        | T2V-01                             | 0.43     |
| Minimax Text to Video  | Video    | model                        | T2V-01-Director                    | 0.43     |
| Minimax Image to Video | Video    | model                        | Hailuo-02 / 768 P / 6s             | 0.28     |
| Minimax Image to Video | Video    | model                        | Hailuo-02 / 768 P / 10s            | 0.56     |
| Minimax Image to Video | Video    | model                        | Hailuo-02 / 1080 P / 6s            | 0.49     |
| Minimax Image to Video | Video    | model                        | I2V-01                             | 0.43     |
| Minimax Image to Video | Video    | model                        | I2V-01-live                        | 0.43     |
| Minimax Video          | Video    | model                        | 768P, 6  (Text or Image to video)  | 0.28     |
| Minimax Video          | Video    | model                        | 768P, 10  (Text or Image to video) | 0.56     |
| Minimax Video          | Video    | model                        | 1080P, 6  (Text or Image to video) | 0.49     |

## Recraft

| Model(Node name)               | Category | Parameters that affect price | Parameter combo | Price \$ |
| ------------------------------ | -------- | ---------------------------- | --------------- | -------- |
| Recraft Creative Upscale Image | Image    | NA                           | NA              | 0.25     |
| Recraft Crisp Upscale Image    | Image    | NA                           | NA              | 0.004    |
| Recraft Image to Image         | Image    | n                            | 1               | 0.04     |
| Recraft Remove Background      | Image    | NA                           | NA              | 0.01     |
| Recraft Replace Background     | Image    | n                            | 1               | 0.04     |
| Recraft Text to Image          | Image    | n                            | 1               | 0.04     |
| Recraft Vectorize Image        | Image    | NA                           | NA              | 0.01     |
| Recraft Text to Vector         | Image    | n                            | 1               | 0.08     |
| Recraft Image Inpainting       | Image    | n                            | 1               | 0.04     |

## Ideogram

| Model(Node name)                | Category | Parameters that affect price   | Parameter combo     | Price \$ |
| ------------------------------- | -------- | ------------------------------ | ------------------- | -------- |
| Ideogram V1                     | Image    | num\_images, turbo(true/false) | 1, false            | 0.06     |
| Ideogram V1                     | Image    | num\_images, turbo(true/false) | 1, true             | 0.02     |
| Ideogram V2                     | Image    | num\_images, turbo(true/false) | 1, false            | 0.08     |
| Ideogram V2                     | Image    | num\_images, turbo(true/false) | 1, true             | 0.05     |
| Ideogram V3                     | Image    | num\_images, rendering\_speed  | 1, Balanced         | 0.06     |
| Ideogram V3                     | Image    | num\_images, rendering\_speed  | 1, Quality          | 0.09     |
| Ideogram V3                     | Image    | num\_images, rendering\_speed  | 1, Turbo            | 0.03     |
| Ideogram V3 Character Reference | Image    | num\_images, rendering\_speed  | 1, Balanced/Default | 0.15     |
| Ideogram V3 Character Reference | Image    | num\_images, rendering\_speed  | 1, Quality          | 0.20     |
| Ideogram V3 Character Reference | Image    | num\_images, rendering\_speed  | 1, Turbo            | 0.10     |

## Runway

| Model(Node name)                    | Category | Parameters that affect price | Parameter combo | Price \$ |
| ----------------------------------- | -------- | ---------------------------- | --------------- | -------- |
| Ruway Text to Image                 | Image    | NA                           | NA              | 0.08     |
| Runway First-Last-Frame to Video    | Video    | duration                     | 5s              | 0.25     |
| Runway First-Last-Frame to Video    | Video    | duration                     | 10s             | 0.5      |
| Runway Image to Video (Gen3a Turbo) | Video    | duration                     | 5s              | 0.25     |
| Runway Image to Video (Gen3a Turbo) | Video    | duration                     | 10s             | 0.5      |
| Runway Image to Video (Gen4 Turbo)  | Video    | duration                     | 5s              | 0.25     |
| Runway Image to Video (Gen4 Turbo)  | Video    | duration                     | 10s             | 0.5      |

## OpenAI

| Model(Node name)                | Category | Parameters that affect price | Parameter combo          | Price \$                                                                                           |
| ------------------------------- | -------- | ---------------------------- | ------------------------ | -------------------------------------------------------------------------------------------------- |
| Sora-2                          | Video    | duration, resolution, size   | 720x1280 or 1280x720     | \$0.1/second                                                                                       |
| Sora-2 Pro                      | Video    | duration, resolution, size   | 720x1280 or 1280x720     | \$0.3/second                                                                                       |
| Sora-2 Pro                      | Video    | duration, resolution, size   | 1024x1792 or 1792x1024   | \$0.5/second                                                                                       |
| GPT-Image-1 - Actual            | Image    | n, quality, size             |                          | input image tokens$10 / 1M tokens + input text tokens$5 / 1M tokens +output tokens\$40 / 1M tokens |
| GPT-Image-1 (Approximate price) | Image    | n, quality, size             | high, 1024x1024          | 0.167                                                                                              |
| GPT-Image-1 (Approximate price) | Image    | n, quality, size             | high, 1024x1536          | 0.25                                                                                               |
| GPT-Image-1 (Approximate price) | Image    | n, quality, size             | high, 1536x1024          | 0.25                                                                                               |
| GPT-Image-1 (Approximate price) | Image    | n, quality, size             | low, 1024x1024           | 0.011                                                                                              |
| GPT-Image-1 (Approximate price) | Image    | n, quality, size             | low, 1024x1536           | 0.016                                                                                              |
| GPT-Image-1 (Approximate price) | Image    | n, quality, size             | low, 1536x1024           | 0.016                                                                                              |
| GPT-Image-1 (Approximate price) | Image    | n, quality, size             | medium, 1024x1024        | 0.042                                                                                              |
| GPT-Image-1 (Approximate price) | Image    | n, quality, size             | medium, 1024x1536        | 0.063                                                                                              |
| GPT-Image-1 (Approximate price) | Image    | n, quality, size             | medium, 1536x1024        | 0.063                                                                                              |
| Image Generation (DALLE 2)     | Image    | size                         | size = 512 \* 512        | 0.018                                                                                              |
| Image Generation (DALLE 2)     | Image    | size                         | size = 1024 \* 1024      | 0.02                                                                                               |
| Image Generation (DALLE 2)     | Image    | size                         | size 256 \* 256          | 0.016                                                                                              |
| Image Generation (DALLE 3 HD)  | Image    | quality, size                | size = 1024 \* 1024, hd  | 0.08                                                                                               |
| Image Generation (DALLE 3 HD)  | Image    | quality, size                | size = 1024 \* 1792, hd  | 0.12                                                                                               |
| Image Generation (DALLE 3 HD)  | Image    | quality, size                | size = 1792 \* 1024, hd  | 0.12                                                                                               |
| Image Generation (DALLE 3 Std) | Image    | quality, size                | size = 1024 \* 1024,std  | 0.04                                                                                               |
| Image Generation (DALLE 3 Std) | Image    | quality, size                | size = 1024 \* 1792, std | 0.08                                                                                               |
| Image Generation (DALLE 3 Std) | Image    | quality, size                | size = 1792 \* 1024, std | 0.08                                                                                               |
| OpenAI Chat                     | Text     | gpt-4.1                      |                          | Per 1M tokens: Input Text: $0.05, Cached Input text: $0.005, Output text: \$0.4                    |
| OpenAI Chat                     | Text     | gpt-4.1-mini                 |                          | Per 1M tokens: Input Text: $0.4, Cached Input text: $0.1, Output text: \$1.6                       |
| OpenAI Chat                     | Text     | gpt-4.1-nano                 |                          | Per 1M tokens: Input Text: $0.1, Cached Input text: $0.025, Output text: \$0.4                     |
| OpenAI Chat                     | Text     | gpt-5                        |                          | Per 1M tokens: Input Text: $1.25, Cached Input text: $0.125, Output text: \$10                     |
| OpenAI Chat                     | Text     | gpt-5-mini                   |                          | Per 1M tokens: Input Text: $0.25 ,Cached Input text: $0.025, Output text: \$2                      |
| OpenAI Chat                     | Text     | gpt-5-nano                   |                          | Per 1M tokens: Input Text: $0.05, Cached Input text: $0.005, Output text: \$0.4                    |
| OpenAI Chat                     | Text     | gpt-4o                       |                          | Per 1M tokens: Input Text: $2.5, Cached Input text: $1.25, Output text: \$10                       |
| OpenAI Chat                     | Text     | o1                           |                          | Per 1M tokens: Input Text: $15, Cached Input text: $7.5, Output text: \$60                         |
| OpenAI Chat                     | Text     | o1-pro                       |                          | Per 1M tokens: Input Text: $150, Cached Input text: NA, Output text: $600                          |
| OpenAI Chat                     | Text     | o3                           |                          | Per 1M tokens: Input Text: $2, Cached Input text: $0.5, Output text: \$8                           |
| OpenAI Chat                     | Text     | o4-mini                      |                          | Per 1M tokens: Input Text: $1.1, Cached Input text: $0.275, Output text: \$4.4                     |

## Pixverse

| Model(Node name)          | Category | Parameters that affect price  | Parameter combo | Price \$ |
| ------------------------- | -------- | ----------------------------- | --------------- | -------- |
| PixVerse Text to Video    | Video    | duration, quality, resolution | 360p fast 5s    | 0.9      |
| PixVerse Text to Video    | Video    | duration, quality, resolution | 360p normal 5s  | 0.45     |
| PixVerse Text to Video    | Video    | duration, quality, resolution | 360p normal 8s  | 0.9      |
| PixVerse Text to Video    | Video    | duration, quality, resolution | 540p fast 5s    | 0.9      |
| PixVerse Text to Video    | Video    | duration, quality, resolution | 540p normal 5s  | 0.45     |
| PixVerse Text to Video    | Video    | duration, quality, resolution | 540p normal 8s  | 0.9      |
| PixVerse Text to Video    | Video    | duration, quality, resolution | 720p fast 5s    | 1.2      |
| PixVerse Text to Video    | Video    | duration, quality, resolution | 720p normal 5s  | 0.6      |
| PixVerse Text to Video    | Video    | duration, quality, resolution | 720p normal 8s  | 1.2      |
| PixVerse Text to Video    | Video    | duration, quality, resolution | 1080p normal 5s | 1.2      |
| PixVerse Transition Video | Video    | duration, quality, resolution | 360p fast 5s    | 0.9      |
| PixVerse Transition Video | Video    | duration, quality, resolution | 360p normal 5s  | 0.45     |
| PixVerse Transition Video | Video    | duration, quality, resolution | 360p normal 8s  | 0.9      |
| PixVerse Transition Video | Video    | duration, quality, resolution | 540p fast 5s    | 0.9      |
| PixVerse Transition Video | Video    | duration, quality, resolution | 540p normal 5s  | 0.45     |
| PixVerse Transition Video | Video    | duration, quality, resolution | 540p normal 8s  | 0.9      |
| PixVerse Transition Video | Video    | duration, quality, resolution | 720p fast 5s    | 1.2      |
| PixVerse Transition Video | Video    | duration, quality, resolution | 720p normal 5s  | 0.6      |
| PixVerse Transition Video | Video    | duration, quality, resolution | 720p normal 8s  | 1.2      |
| PixVerse Transition Video | Video    | duration, quality, resolution | 1080p normal 5s | 1.2      |
| PixVerseImage to Video    | Video    | duration, quality, resolution | 360p fast 5s    | 0.9      |
| PixVerseImage to Video    | Video    | duration, quality, resolution | 360p normal 5s  | 0.45     |
| PixVerseImage to Video    | Video    | duration, quality, resolution | 360p normal 8s  | 0.9      |
| PixVerseImage to Video    | Video    | duration, quality, resolution | 540p fast 5s    | 0.9      |
| PixVerseImage to Video    | Video    | duration, quality, resolution | 540p normal 5s  | 0.45     |
| PixVerseImage to Video    | Video    | duration, quality, resolution | 540p normal 8s  | 0.9      |
| PixVerseImage to Video    | Video    | duration, quality, resolution | 720p fast 5s    | 1.2      |
| PixVerseImage to Video    | Video    | duration, quality, resolution | 720p normal 5s  | 0.6      |
| PixVerseImage to Video    | Video    | duration, quality, resolution | 720p normal 8s  | 1.2      |
| PixVerseImage to Video    | Video    | duration, quality, resolution | 1080p normal 5s | 1.2      |

## Pika

| Model(Node name)                      | Category | Parameters that affect price | Parameter combo | Price \$ |
| ------------------------------------- | -------- | ---------------------------- | --------------- | -------- |
| Pika Scenes (Video Image Composition) | Video    | duration, resolution         | 720p, 5s        | 0.3      |
| Pika Scenes (Video Image Composition) | Video    | duration, resolution         | 720p, 10s       | 0.4      |
| Pika Scenes (Video Image Composition) | Video    | duration, resolution         | 1080p, 5s       | 0.5      |
| Pika Scenes (Video Image Composition) | Video    | duration, resolution         | 1080p, 10s      | 1.5      |
| Pika Start and End Frame to Video     | Video    | duration, resolution         | 720p, 5s        | 0.2      |
| Pika Start and End Frame to Video     | Video    | duration, resolution         | 720p, 10s       | 0.25     |
| Pika Start and End Frame to Video     | Video    | duration, resolution         | 1080p, 5s       | 0.3      |
| Pika Start and End Frame to Video     | Video    | duration, resolution         | 1080p, 10s      | 1        |
| Pika Text to Video                    | Video    | duration, resolution         | 720p, 5s        | 0.2      |
| Pika Text to Video                    | Video    | duration, resolution         | 720p, 10s       | 0.6      |
| Pika Text to Video                    | Video    | duration, resolution         | 1080p, 5s       | 0.45     |
| Pika Text to Video                    | Video    | duration, resolution         | 1080p, 10s      | 1        |
| PikaImage to Video                    | Video    | duration, resolution         | 720p, 5s        | 0.2      |
| PikaImage to Video                    | Video    | duration, resolution         | 720p, 10s       | 0.6      |
| PikaImage to Video                    | Video    | duration, resolution         | 1080p, 5s       | 0.45     |
| PikaImage to Video                    | Video    | duration, resolution         | 1080p, 10s      | 1        |
| Pika Swaps (Video Object Replacement) | Video    | NA                           | NA              | 0.3      |
| Pikadditios (Video Object Insertion)  | Video    | NA                           | NA              | 0.3      |
| Pikaffects (Video Effects)            | Video    | NA                           | NA              | 0.45     |

## Moonvalley

| Model(Node name)    | Category | Parameters that affect price | Parameter combo | Price \$ |
| ------------------- | -------- | ---------------------------- | --------------- | -------- |
| Image to video - 5s | Video    | NA                           | NA              | 1.5      |
| Text to video - 5s  | Video    | NA                           | NA              | 1.5      |
| Video to video - 5s | Video    | NA                           | NA              | 2.25     |

## Rodin

| Model(Node name)                     | Category | Parameters that affect price | Parameter combo | Price \$ |
| ------------------------------------ | -------- | ---------------------------- | --------------- | -------- |
| Rodin 3D Generate - Regular Generate | 3D       | NA                           | NA              | 0.4      |
| Rodin 3D Generate - Detail Generate  | 3D       | NA                           | NA              | 0.4      |
| Rodin 3D Generate - Sketch Generate  | 3D       | NA                           | NA              | 0.4      |
| Rodin 3D Generate - Smooth Generate  | 3D       | NA                           | NA              | 0.4      |

## Tripo

| Model(Node name)                          | Category | Parameters that affect price           | Parameter combo                      | Price \$ |
| ----------------------------------------- | -------- | -------------------------------------- | ------------------------------------ | -------- |
| Tripo: Text to Model                      | 3D       | quad, style, texture, texture\_quality | any style, false, any quality, false | 0.15     |
| Tripo: Text to Model                      | 3D       | quad, style, texture, texture\_quality | any style, false, any quality, true  | 0.2      |
| Tripo: Text to Model                      | 3D       | quad, style, texture, texture\_quality | any style, true, detailed, false     | 0.35     |
| Tripo: Text to Model                      | 3D       | quad, style, texture, texture\_quality | any style, true, detailed, true      | 0.4      |
| Tripo: Text to Model                      | 3D       | quad, style, texture, texture\_quality | any style, true, standard, false     | 0.25     |
| Tripo: Text to Model                      | 3D       | quad, style, texture, texture\_quality | any style, true, standard, true      | 0.3      |
| Tripo: Text to Model                      | 3D       | quad, style, texture, texture\_quality | none, false, any quality, false      | 0.1      |
| Tripo: Text to Model                      | 3D       | quad, style, texture, texture\_quality | none, false, any quality, true       | 0.15     |
| Tripo: Text to Model                      | 3D       | quad, style, texture, texture\_quality | none, true, detailed, false          | 0.3      |
| Tripo: Text to Model                      | 3D       | quad, style, texture, texture\_quality | none, true, detailed, true           | 0.35     |
| Tripo: Text to Model                      | 3D       | quad, style, texture, texture\_quality | none, true, standard, false          | 0.2      |
| Tripo: Text to Model                      | 3D       | quad, style, texture, texture\_quality | none, true, standard, true           | 0.25     |
| Tripo:Image to Model / Multiview to Model | 3D       | quad, style, texture, texture\_quality | any style, false, any quality, false | 0.25     |
| Tripo:Image to Model / Multiview to Model | 3D       | quad, style, texture, texture\_quality | any style, false, any quality, true  | 0.3      |
| Tripo:Image to Model / Multiview to Model | 3D       | quad, style, texture, texture\_quality | any style, true, detailed, false     | 0.45     |
| Tripo:Image to Model / Multiview to Model | 3D       | quad, style, texture, texture\_quality | any style, true, detailed, true      | 0.5      |
| Tripo:Image to Model / Multiview to Model | 3D       | quad, style, texture, texture\_quality | any style, true, standard, false     | 0.35     |
| Tripo:Image to Model / Multiview to Model | 3D       | quad, style, texture, texture\_quality | any style, true, standard, true      | 0.4      |
| Tripo:Image to Model / Multiview to Model | 3D       | quad, style, texture, texture\_quality | none, false, any quality, false      | 0.2      |
| Tripo:Image to Model / Multiview to Model | 3D       | quad, style, texture, texture\_quality | none, false, any quality, true       | 0.25     |
| Tripo:Image to Model / Multiview to Model | 3D       | quad, style, texture, texture\_quality | none, true, detailed, false          | 0.4      |
| Tripo:Image to Model / Multiview to Model | 3D       | quad, style, texture, texture\_quality | none, true, detailed, true           | 0.45     |
| Tripo:Image to Model / Multiview to Model | 3D       | quad, style, texture, texture\_quality | none, true, standard, false          | 0.3      |
| Tripo:Image to Model / Multiview to Model | 3D       | quad, style, texture, texture\_quality | none, true, standard, true           | 0.35     |
| Tripo: Convert model                      | 3D       | NA                                     | NA                                   | 0.1      |
| Tripo: Refine Draft model                 | 3D       | NA                                     | NA                                   | 0.3      |
| Tripo: Retarget rigged model              | 3D       | NA                                     | NA                                   | 0.1      |
| Tripo: Rig model                          | 3D       | NA                                     | NA                                   | 0.25     |
| Tripo: Texture model                      | 3D       | texture\_quality                       | detailed                             | 0.2      |
| Tripo: Texture model                      | 3D       | texture\_quality                       | standard                             | 0.1      |

## Stability AI

| Model(Node name)                        | Category | Parameters that affect price | Parameter combo     | Price \$ |
| --------------------------------------- | -------- | ---------------------------- | ------------------- | -------- |
| Stability AI Stable Image Ultra         | Image    | NA                           | NA                  | 0.08     |
| Stability AI Stable Diffusion 3.5 Image | Image    | model                        | sd3.5-large         | 0.065    |
| Stability AI Stable Diffusion 3.5 Image | Image    | model                        | sd3.5-medium        | 0.035    |
| Stability AI Upscale Conservative       | Image    | NA                           | NA                  | 0.40     |
| Stability AI Upscale Creative           | Image    | NA                           | NA                  | 0.60     |
| Stability AI Upscale Fast               | Image    | NA                           | NA                  | 0.02     |
| Stability AI Audio 2.5 - Audio To Audio | Audio    | model                        | stability-audio-2.5 | 0.2      |
| Stability AI Audio 2.5 - Inpaint        | Audio    | model                        | stability-audio-2.5 | 0.2      |
| Stability AI Audio 2.5 - Text To Audio  | Audio    | model                        | stability-audio-2.5 | 0.2      |

## Vidu

| Model(Node name)        | Category | Parameters that affect price | Parameter combo | Price \$ |
| ----------------------- | -------- | ---------------------------- | --------------- | -------- |
| ViduImageToVideoNode    | Video    | NA                           | NA              | 0.4      |
| ViduReferenceVideoNode  | Video    | NA                           | NA              | 0.4      |
| ViduStartEndToVideoNode | Video    | NA                           | NA              | 0.4      |
| ViduTextToVideoNode     | Video    | NA                           | NA              | 0.4      |

## WAN

| Model(Node name)   | Category | Parameters that affect price | Parameter combo           | Price \$      |
| ------------------ | -------- | ---------------------------- | ------------------------- | ------------- |
| WAN Text to Video  | Video    | model, resolution, duration  | wan2.5-t2v-preview, 1080p | \$0.15/second |
| WAN Text to Video  | Video    | model, resolution, duration  | wan2.5-t2v-preview, 720p  | \$0.10/second |
| WAN Text to Video  | Video    | model, resolution, duration  | wan2.5-t2v-preview, 480p  | \$0.05/second |
| WAN Image to Video | Video    | model, resolution, duration  | wan2.5-i2v-preview, 1080p | \$0.15/second |
| WAN Image to Video | Video    | model, resolution, duration  | wan2.5-i2v-preview, 720p  | \$0.10/second |
| WAN Image to Video | Video    | model, resolution, duration  | wan2.5-i2v-preview, 480p  | \$0.05/second |
| WAN Text to Image  | Image    | model                        | wan2.5-t2i-preview        | \$0.03/run    |

## Topaz

| Model(Node name)    | Category | Parameters that affect price     | Parameter combo                                  | Price \$    |
| ------------------- | -------- | -------------------------------- | ------------------------------------------------ | ----------- |
| Topaz Image Enhance | Image    | model, output\_resolution        | Reimagine, 12 MP                                 | 0.08        |
| Topaz Image Enhance | Image    | model, output\_resolution        | Reimagine, 24 MP                                 | 0.08        |
| Topaz Image Enhance | Image    | model, output\_resolution        | Reimagine, 36 MP                                 | 0.16        |
| Topaz Image Enhance | Image    | model, output\_resolution        | Reimagine, 48 MP                                 | 0.16        |
| Topaz Image Enhance | Image    | model, output\_resolution        | Reimagine, 60 MP                                 | 0.24        |
| Topaz Image Enhance | Image    | model, output\_resolution        | Reimagine, 96 MP                                 | 0.32        |
| Topaz Image Enhance | Image    | model, output\_resolution        | Reimagine, 132 MP                                | 0.40        |
| Topaz Image Enhance | Image    | model, output\_resolution        | Reimagine, 168 MP                                | 0.48        |
| Topaz Image Enhance | Image    | model, output\_resolution        | Reimagine, 336 MP                                | 0.88        |
| Topaz Image Enhance | Image    | model, output\_resolution        | Reimagine, 512 MP                                | 1.36        |
| Topaz Video Enhance | Video    | upscaler\_model, resolution, fps | Starlight (Astra) Fast, 720p/30fps  720p/60fps  | 0.01/second |
| Topaz Video Enhance | Video    | upscaler\_model, resolution, fps | Starlight (Astra) Fast, 720p/30fps  1080p/30fps | 0.02/second |
| Topaz Video Enhance | Video    | upscaler\_model, resolution, fps | Starlight (Astra) Fast, 720p/30fps  1080p/60fps | 0.04/second |
| Topaz Video Enhance | Video    | upscaler\_model, resolution, fps | Starlight (Astra) Fast, 720p/30fps  4K/30fps    | 0.08/second |
| Topaz Video Enhance | Video    | upscaler\_model, resolution, fps | Starlight (Astra) Fast, 720p/30fps  4K/60fps    | 0.15/second |


# Stability AI Stable Audio 2.5 API Node ComfyUI Official workflow example
Source: https://docs.comfy.org/tutorials/partner-nodes/stability-ai/stable-audio

This article will introduce how to use Stability AI Stable Audio 2.5 Partner node's text-to-audio, audio-to-audio and audio-inpainting capabilities in ComfyUI

The Stability AI Stable Audio 2.5 Partner node allows you to use Stability AI's latest audio generation model to create high-quality music through text prompts, audio transformations, and audio inpainting capabilities.

Stable Audio 2.5 is designed for enterprise use, featuring improved musical structure, better prompt adherence, and the ability to generate minutes-long compositions in seconds. The model offers three main workflows: **Text-to-Audio** for generating music from descriptions, **Audio-to-Audio** for transforming existing audio into new compositions, and **Audio Inpainting** for completing or extending existing tracks.

Trained exclusively on licensed audio, Stable Audio 2.5 is commercially safe and perfect for advertisers, game studios, and content creators who need professional-quality audio generation with enterprise-grade reliability.

<Tip>
  To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](/tutorials/partner-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.
</Tip>

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Text-to-Audio Workflow

For text-to-audio, you can generate audio through text prompts. You need to describe the music you want to generate.

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/api_stability_ai_text_to_audio.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download JSON Workflow</p>
</a>

<img src="https://mintcdn.com/dripart/odc_rZSmR0RrwUYs/images/tutorial/api_nodes/stability_ai/api_stability_ai_text_to_audio.jpg?fit=max&auto=format&n=odc_rZSmR0RrwUYs&q=85&s=7e369b21bd7a974f40ed38bcbc4c8afe" alt="workflow" data-og-width="2170" width="2170" data-og-height="1130" height="1130" data-path="images/tutorial/api_nodes/stability_ai/api_stability_ai_text_to_audio.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/odc_rZSmR0RrwUYs/images/tutorial/api_nodes/stability_ai/api_stability_ai_text_to_audio.jpg?w=280&fit=max&auto=format&n=odc_rZSmR0RrwUYs&q=85&s=f933d31a488d96c95c717ae3630f1cbe 280w, https://mintcdn.com/dripart/odc_rZSmR0RrwUYs/images/tutorial/api_nodes/stability_ai/api_stability_ai_text_to_audio.jpg?w=560&fit=max&auto=format&n=odc_rZSmR0RrwUYs&q=85&s=f7fed196827182261976aea32e522c2a 560w, https://mintcdn.com/dripart/odc_rZSmR0RrwUYs/images/tutorial/api_nodes/stability_ai/api_stability_ai_text_to_audio.jpg?w=840&fit=max&auto=format&n=odc_rZSmR0RrwUYs&q=85&s=078a21a78fabbe870444a21d12a003ae 840w, https://mintcdn.com/dripart/odc_rZSmR0RrwUYs/images/tutorial/api_nodes/stability_ai/api_stability_ai_text_to_audio.jpg?w=1100&fit=max&auto=format&n=odc_rZSmR0RrwUYs&q=85&s=6eccc85d2feab74797b7e9d3fb16f42d 1100w, https://mintcdn.com/dripart/odc_rZSmR0RrwUYs/images/tutorial/api_nodes/stability_ai/api_stability_ai_text_to_audio.jpg?w=1650&fit=max&auto=format&n=odc_rZSmR0RrwUYs&q=85&s=4dd0da78dd4a6efe6f7a0ff3117dce60 1650w, https://mintcdn.com/dripart/odc_rZSmR0RrwUYs/images/tutorial/api_nodes/stability_ai/api_stability_ai_text_to_audio.jpg?w=2500&fit=max&auto=format&n=odc_rZSmR0RrwUYs&q=85&s=a9b327a847b32dbe99750e9bf217baf2 2500w" />

1. Modify the text prompt. You should use keywords to describe the music you want to generate.
2. (Optional) Modify the `duration` parameter. It's `190` by default.
3. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the audio generation. The audio will be saved to the `ComfyUI/output/audio` directory.

## Audio-to-Audio Workflow

Audio-to-audio is basically music re-sampling. You can use it to generate new music from a given piece of music, or you can just hum a melody, and then the model will generate new music based on the input audio.

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/api_stability_ai_audio_to_audio.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download JSON Workflow</p>
</a>

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/stability_ai/stable_audio/beatbox.mp3" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download Input  Audio</p>
</a>

<img src="https://mintcdn.com/dripart/odc_rZSmR0RrwUYs/images/tutorial/api_nodes/stability_ai/api_stability_ai_audio_to_audio.jpg?fit=max&auto=format&n=odc_rZSmR0RrwUYs&q=85&s=e3fb5f33fd60d42d65544874a574a483" alt="workflow" data-og-width="2366" width="2366" data-og-height="890" height="890" data-path="images/tutorial/api_nodes/stability_ai/api_stability_ai_audio_to_audio.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/odc_rZSmR0RrwUYs/images/tutorial/api_nodes/stability_ai/api_stability_ai_audio_to_audio.jpg?w=280&fit=max&auto=format&n=odc_rZSmR0RrwUYs&q=85&s=0986fd5d755ae5a1dd16d1c918d83cf4 280w, https://mintcdn.com/dripart/odc_rZSmR0RrwUYs/images/tutorial/api_nodes/stability_ai/api_stability_ai_audio_to_audio.jpg?w=560&fit=max&auto=format&n=odc_rZSmR0RrwUYs&q=85&s=5edbd9254f46138721015beea5669b2d 560w, https://mintcdn.com/dripart/odc_rZSmR0RrwUYs/images/tutorial/api_nodes/stability_ai/api_stability_ai_audio_to_audio.jpg?w=840&fit=max&auto=format&n=odc_rZSmR0RrwUYs&q=85&s=957beac2b40d48517c68396b8dcdfa72 840w, https://mintcdn.com/dripart/odc_rZSmR0RrwUYs/images/tutorial/api_nodes/stability_ai/api_stability_ai_audio_to_audio.jpg?w=1100&fit=max&auto=format&n=odc_rZSmR0RrwUYs&q=85&s=00b8997207fae2c4aa18c7e86de428a2 1100w, https://mintcdn.com/dripart/odc_rZSmR0RrwUYs/images/tutorial/api_nodes/stability_ai/api_stability_ai_audio_to_audio.jpg?w=1650&fit=max&auto=format&n=odc_rZSmR0RrwUYs&q=85&s=de12e64a619cebb83ea44e55928bfe27 1650w, https://mintcdn.com/dripart/odc_rZSmR0RrwUYs/images/tutorial/api_nodes/stability_ai/api_stability_ai_audio_to_audio.jpg?w=2500&fit=max&auto=format&n=odc_rZSmR0RrwUYs&q=85&s=147ead09b14ab884e5fbafc851e0b129 2500w" />

1. In this workflow, we have provided two nodes for you to input the audioat least 6 seconds you want to edit:
   * 1.1 `Record Audio` node: You can use it to record any of your music ideas, such as a hummed melody. It should be at least 6 seconds.
   * 1.2 `LoadAudio` node: You can use it to upload audio that you want to be used in this workflow.
2. Modify the text prompt. You should use keywords to describe the music you want to generate.
3. The `strength` parameter is used to control the difference from the original audio. The lower the value, the more similar the generated audio will be to the original audio.
4. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the audio generation. The audio will be saved to the `ComfyUI/output/audio` directory.

## Audio Inpainting Workflow

Audio inpainting is used to complete or extend existing tracks. You can use it to complete the missing part of music or extend the music to a longer duration.

You need to set where you want to start and end the inpainting.

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/api_stability_ai_audio_inpaint.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download JSON Workflow</p>
</a>

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/stability_ai/stable_audio/audio_input.wav" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download Input  Audio</p>
</a>

<img src="https://mintcdn.com/dripart/odc_rZSmR0RrwUYs/images/tutorial/api_nodes/stability_ai/api_stability_ai_audio_inpaint.jpg?fit=max&auto=format&n=odc_rZSmR0RrwUYs&q=85&s=733ebe929fe884c94701a36e38549c58" alt="workflow" data-og-width="2358" width="2358" data-og-height="848" height="848" data-path="images/tutorial/api_nodes/stability_ai/api_stability_ai_audio_inpaint.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/odc_rZSmR0RrwUYs/images/tutorial/api_nodes/stability_ai/api_stability_ai_audio_inpaint.jpg?w=280&fit=max&auto=format&n=odc_rZSmR0RrwUYs&q=85&s=b1e3d149cbf62cab22b311663b54da1e 280w, https://mintcdn.com/dripart/odc_rZSmR0RrwUYs/images/tutorial/api_nodes/stability_ai/api_stability_ai_audio_inpaint.jpg?w=560&fit=max&auto=format&n=odc_rZSmR0RrwUYs&q=85&s=01606a6efb0c05c8ec39917c2f03afd6 560w, https://mintcdn.com/dripart/odc_rZSmR0RrwUYs/images/tutorial/api_nodes/stability_ai/api_stability_ai_audio_inpaint.jpg?w=840&fit=max&auto=format&n=odc_rZSmR0RrwUYs&q=85&s=2dc4160b3619080eaa29e0a576e2aeb5 840w, https://mintcdn.com/dripart/odc_rZSmR0RrwUYs/images/tutorial/api_nodes/stability_ai/api_stability_ai_audio_inpaint.jpg?w=1100&fit=max&auto=format&n=odc_rZSmR0RrwUYs&q=85&s=dd812ab64f8939469a5226e14faf3fa9 1100w, https://mintcdn.com/dripart/odc_rZSmR0RrwUYs/images/tutorial/api_nodes/stability_ai/api_stability_ai_audio_inpaint.jpg?w=1650&fit=max&auto=format&n=odc_rZSmR0RrwUYs&q=85&s=ad241c5858411f087968e400833197a7 1650w, https://mintcdn.com/dripart/odc_rZSmR0RrwUYs/images/tutorial/api_nodes/stability_ai/api_stability_ai_audio_inpaint.jpg?w=2500&fit=max&auto=format&n=odc_rZSmR0RrwUYs&q=85&s=6f851ac856dced5b14f7236112e9eb08 2500w" />

1. Upload audio to the `LoadAudio` node.
2. Modify the text prompt. You should use keywords to describe the music you want to generate.
3. (Optional) Modify the `duration` parameter. It's `190` by default.
4. (Important) Modify the `mask_start` and `mask_end` parameters. You need to set where you want to start and end the inpainting.
5. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the audio generation. The audio will be saved to the `ComfyUI/output/audio` directory.


# Stability AI Stable Diffusion 3.5 API Node ComfyUI Official Example
Source: https://docs.comfy.org/tutorials/partner-nodes/stability-ai/stable-diffusion-3-5-image

This article will introduce how to use Stability AI Stable Diffusion 3.5 Partner node's text-to-image and image-to-image capabilities in ComfyUI

The [Stability AI Stable Diffusion 3.5 Image](/built-in-nodes/partner-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image) node allows you to use Stability AI's Stable Diffusion 3.5 model to create high-quality, detail-rich image content through text prompts or reference images.

In this guide, we will show you how to set up workflows for both text-to-image and image-to-image generation using this node.

<Tip>
  To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](/tutorials/partner-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.
</Tip>

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Stability AI Stable Diffusion 3.5 Text-to-Image Workflow

### 1. Workflow File Download

The image below contains workflow information in its `metadata`. Please download and drag it into ComfyUI to load the corresponding workflow.

![Stability AI Stable Diffusion 3.5 Text-to-Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/stable_diffusion_3-5-t2i.png)

### 2. Complete the Workflow Step by Step

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_t2i_step_guide.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=d56ae409c1f885bb54a8ca91144e3924" alt="Stability AI Stable Diffusion 3.5 Text-to-Image Step Guide" data-og-width="2906" width="2906" data-og-height="1541" height="1541" data-path="images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_t2i_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_t2i_step_guide.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=e3fc95a0f3863b12372071e30f7db967 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_t2i_step_guide.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=6385f4d8d5f053a9b608fe4d49917f28 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_t2i_step_guide.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=71ea0bc7452504f29f729cfc08b9f607 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_t2i_step_guide.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=48221dfbf9e9eb8728573fe7cb1a147f 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_t2i_step_guide.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=5192b0e55ad55767337260df769cc989 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_t2i_step_guide.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=f3dd577e99c20a7a81e1ddf5bbaa62e6 2500w" />

You can follow the numbered steps in the image to complete the basic text-to-image workflow:

1. (Optional) Modify the `prompt` parameter in the `Stability AI Stable Diffusion 3.5 Image` node to input your desired image description. More detailed prompts often result in better image quality.
2. (Optional) Select the `model` parameter to choose which SD 3.5 model version to use.
3. (Optional) Select the `style_preset` parameter to control the visual style of the image. Different presets produce images with different stylistic characteristics, such as "cinematic" or "anime". Select "None" to not apply any specific style.
4. (Optional) Edit the `String(Multiline)` to modify negative prompts, specifying elements you don't want to appear in the generated image.
5. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation.
6. After the API returns results, you can view the generated image in the `Save Image` node. The image will also be saved to the `ComfyUI/output/` directory.

### 3. Additional Notes

* **Prompt**: The prompt is one of the most important parameters in the generation process. Detailed, clear descriptions lead to better results. Can include elements like scene, subject, colors, lighting, and style.
* **CFG Scale**: Controls how closely the generator follows the prompt. Higher values make the image more closely match the prompt description, but too high may result in oversaturated or unnatural results.
* **Style Preset**: Offers various preset styles for quickly defining the overall style of the image.
* **Negative Prompt**: Used to specify elements you don't want to appear in the generated image.
* **Seed Parameter**: Can be used to reproduce or fine-tune generation results, helpful for iteration during creation.
* Currently the `Load Image` node is in "Bypass" mode. To enable it, refer to the step guide and right-click the node to set "Mode" to "Always" to enable input, switching to image-to-image mode.
* `image_denoise` has no effect when there is no input image.

## Stability AI Stable Diffusion 3.5 Image-to-Image Workflow

### 1. Workflow File Download

The image below contains workflow information in its `metadata`. Please download and drag it into ComfyUI to load the corresponding workflow.

![Stability AI Stable Diffusion 3.5 Image-to-Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/sd3-5-i2i/stable_diffusion_3_5-i2i.png)

Download the image below to use as input
!\[Stability AI Stable Diffusion 3.5 Image-to-Image Workflow Input Image]\(![Stability AI Stable Diffusion 3.5 ](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/sd3-5-i2i/input.jpg)

### 2. Complete the Workflow Step by Step

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_i2i_step_guide.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=3f31575a609f91bd083cc22a99cf3991" alt="Stability AI Stable Diffusion 3.5 Image-to-Image Step Guide" data-og-width="2436" width="2436" data-og-height="1446" height="1446" data-path="images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_i2i_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_i2i_step_guide.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=9f2daefb02ba44d89b0719d823a2d95f 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_i2i_step_guide.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=5e6ace02b6a2b117837959e9f2fe94fa 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_i2i_step_guide.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=e928b22ef9e3ef164f237c2031e74375 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_i2i_step_guide.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=80a9cf25056d6eeb67f923d785a2467a 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_i2i_step_guide.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=e7fa6441ad2093f0130824c03077d4ff 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_i2i_step_guide.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=2c00041d1f7f670570580d8134180e18 2500w" />

You can follow the numbered steps in the image to complete the image-to-image workflow:

1. Load a reference image through the `Load Image` node, which will serve as the basis for generation.
2. (Optional) Modify the `prompt` parameter in the `Stability AI Stable Diffusion 3.5 Image` node to describe elements you want to change or enhance in the reference image.
3. (Optional) Select the `style_preset` parameter to control the visual style of the image. Different presets produce images with different stylistic characteristics.
4. (Optional|Important) Adjust the `image_denoise` parameter (range 0.0-1.0) to control how much the original image is modified:
   * Values closer to 0.0 make the generated image more similar to the input reference image (at 0.0, it's basically identical to the original)
   * Values closer to 1.0 make the generated image more like pure text-to-image generation (at 1.0, it's as if no reference image was provided)
5. (Optional) Edit the `String(Multiline)` to modify negative prompts, specifying elements you don't want to appear in the generated image.
6. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation.
7. After the API returns results, you can view the generated image in the `Save Image` node. The image will also be saved to the `ComfyUI/output/` directory.

### 3. Additional Notes

The image below shows a comparison of results with and without input image using the same parameter settings:

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_compare.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=13d41c3d9718abe4d40321b5d9a85b70" alt="Stability AI Stable Diffusion 3.5 With/Without Image Input Comparison" data-og-width="1400" width="1400" data-og-height="700" height="700" data-path="images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_compare.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_compare.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=b87317f78e02060552894bfe586bcb2c 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_compare.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=67e2f9597c292ec17a3a9cb80b83c3cc 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_compare.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=d2a40aa65ca5f640fcb5229d6a3c1035 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_compare.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=425f0b61b27ea3885160969ed294a1e9 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_compare.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=657ee7666879076cbcd47d8aba120d41 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_compare.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=07bcc8bf00ade469352cbb56067c819d 2500w" />

**Image Denoise**: This parameter determines how much of the original image's features are preserved during generation. It's the most crucial adjustment parameter in image-to-image mode. The image below shows the effects of different denoising strengths:

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_i2i_image_denoise.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=db7a75bbb9c6151f093fb9881f88a522" alt="Stability AI Stable Diffusion 3.5 Image-to-Image Denoise Strength Explanation" data-og-width="2100" width="2100" data-og-height="1400" height="1400" data-path="images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_i2i_image_denoise.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_i2i_image_denoise.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=a3a2d76f4fed07b3016b5a53f43e26b9 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_i2i_image_denoise.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=530d2d1c0bcb789ec9b05f7e20286adf 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_i2i_image_denoise.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=52bd9d150069a3f81a0885a81d2b3590 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_i2i_image_denoise.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=da006a1ef9d06fc15e72d1151cfa7430 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_i2i_image_denoise.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=08a6c0ea76307d1733345b5238e588dd 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_diffusion_3_5_image_i2i_image_denoise.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=98434f7a1c0823f4217bc3fb5d1d4f04 2500w" />

* **Reference Image Selection**: Choosing images with clear subjects and good composition usually yields better results.
* **Prompt Tips**: In image-to-image mode, prompts should focus more on elements you want to change or enhance, rather than describing everything already present in the image.
* **Mode Switching**: When an input image is provided, the node automatically switches from text-to-image mode to image-to-image mode, and aspect ratio parameters are ignored.

## Related Node Details

You can refer to the documentation below to understand detailed parameter settings for the corresponding node

<Card title="Stability Stable Diffusion 3.5 Image Node Documentation" icon="book" href="/built-in-nodes/partner-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image">
  Stability Stable Diffusion 3.5 Image API Node Documentation
</Card>


# Stability AI Stable Image Ultra API Node ComfyUI Official Example
Source: https://docs.comfy.org/tutorials/partner-nodes/stability-ai/stable-image-ultra

This article will introduce how to use the Stability AI Stable Image Ultra Partner node's text-to-image and image-to-image capabilities in ComfyUI

The [Stability Stable Image Ultra](/images/built-in-nodes/api_nodes/stability-ai/stability-ai-stable-image-ultra.jpg) node allows you to use Stability AI's Stable Image Ultra model to create high-quality, detailed image content through text prompts or reference images.

In this guide, we will show you how to set up workflows for both text-to-image and image-to-image generation using this node.

<Tip>
  To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](/tutorials/partner-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.
</Tip>

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Stability AI Stable Image Ultra Text-to-Image Workflow

### 1. Workflow File Download

The workflow information is included in the metadata of the image below. Please download and drag it into ComfyUI to load the corresponding workflow.

![Stability AI Stable Image Ultra Text-to-Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/stable_image_ultra_t2i.png)

### 2. Complete the Workflow Execution Step by Step

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_image_ultra_t2i_step_guide.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=bbbb22660e1757bfba34771d438e968d" alt="Stability AI Stable Image Ultra Text-to-Image Step Guide" data-og-width="2366" width="2366" data-og-height="959" height="959" data-path="images/tutorial/api_nodes/stability_ai/stable_image_ultra_t2i_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_image_ultra_t2i_step_guide.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=cf51af0c2f320a2f05286ebd71ef883c 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_image_ultra_t2i_step_guide.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=c087debb849a1e5f4af365113850d9b5 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_image_ultra_t2i_step_guide.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=a441c8deeef138d41a2a212a70e38084 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_image_ultra_t2i_step_guide.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=912ac3ba2ee9ce14a54e5ac52141bcce 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_image_ultra_t2i_step_guide.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=167f4386090ad7a7d80b0e8051557dbf 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_image_ultra_t2i_step_guide.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=b47f2835f4939457adb23d1429389271 2500w" />

You can follow the numbered steps in the image to complete the basic text-to-image workflow:

1. (Optional) Modify the `prompt` parameter in the `Stability AI Stable Image Ultra` node to input your desired image description. More detailed prompts often lead to better image quality. You can use the `(word:weight)` format to control specific word weights, for example: `The sky was crisp (blue:0.3) and (green:0.8)` indicates the sky is blue and green, but green is more prominent.
2. (Optional) Select the `style_preset` parameter to control the visual style of the image. Different preset styles will produce images with different stylistic characteristics, such as "cinematic", "anime", etc. Selecting "None" will not apply any specific style.
3. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation.
4. After the API returns the result, you can view the generated image in the `Save Image` node, and the image will also be saved to the `ComfyUI/output/` directory.

### 3. Additional Notes

* **Prompt**: The prompt is one of the most important parameters in the generation process. Detailed, clear descriptions will lead to better results. It can include elements like scene, subject, colors, lighting, and style.
* **Style Preset**: Provides multiple preset styles such as cinematic, anime, digital art, etc., which can quickly define the overall style of the image.
* **Negative Prompt**: Used to specify elements you don't want to appear in the generated image, helping avoid common issues like extra limbs or distorted faces.
* **Seed Parameter**: Can be used to reproduce or fine-tune generation results, helpful for iteration during the creative process.
* Currently, the `Load Image` node is in "Bypass" mode. To enable it, refer to the step guide and right-click on the corresponding node to set "Mode" to "Always" to enable input, which will switch to image-to-image mode.

## Stability AI Stable Image Ultra Image-to-Image Workflow

### 1. Workflow File Download

The workflow information is included in the metadata of the image below. Please download and drag it into ComfyUI to load the corresponding workflow.

![Stability Stable Image Ultra Image-to-Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/i2i/stable_image_ultra_i2i.png)

Download the image below which we will use as input
![Stability Stable Image Ultra Image-to-Image Workflow Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/stability_ai/i2i/input.png)

### 2. Complete the Workflow Execution Step by Step

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_image_ultra_i2i_step_guide.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=90799d9abc85f845d786eac7f204f778" alt="Stability Stable Image Ultra Image-to-Image Step Guide" data-og-width="2366" width="2366" data-og-height="959" height="959" data-path="images/tutorial/api_nodes/stability_ai/stable_image_ultra_i2i_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_image_ultra_i2i_step_guide.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=2c343c6ac266e26c96fc27cb4a48e1d6 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_image_ultra_i2i_step_guide.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=bf35f2db6f76912b4b443b81e5ffdde1 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_image_ultra_i2i_step_guide.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=dbc6c0ae0b2f44e2ed9c0eeab2643693 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_image_ultra_i2i_step_guide.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=0d0b599d4dee680889ccab25c18a6e90 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_image_ultra_i2i_step_guide.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=56f6fd121ece0e1e7aedb8758beedd98 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/stable_image_ultra_i2i_step_guide.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=a1f259d1251093d0ae76fdad82d6a3c7 2500w" />

You can follow the numbered steps in the image to complete the image-to-image workflow:

1. Load a reference image through the `Load Image` node, which will serve as the basis for generation.
2. (Optional) Modify the `prompt` parameter in the `Stability Stable Image Ultra` node to describe elements you want to change or enhance in the reference image.
3. (Optional) Adjust the `image_denoise` parameter (range 0.0-1.0) to control the degree of modification to the original image:
   * Values closer to 0.0 will make the generated image more similar to the input reference image
   * Values closer to 1.0 will make the generated image more like pure text-to-image generation
4. (Optional) You can also set `style_preset` and other parameters to further control the generation effect.
5. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation.
6. After the API returns the result, you can view the generated image in the `Save Image` node, and the image will also be saved to the `ComfyUI/output/` directory.

### 3. Additional Notes

**Image Denoise**: This parameter determines how much of the original image's features are preserved during generation, and is the most crucial adjustment parameter in image-to-image mode. The image below shows the effects of different denoising strengths

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/i2i_image_denoise.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=8cb266b02864aff7664f56bb1d0e8890" alt="Stability Stable Image Ultra Image-to-Image Denoise Explanation" data-og-width="2100" width="2100" data-og-height="1400" height="1400" data-path="images/tutorial/api_nodes/stability_ai/i2i_image_denoise.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/i2i_image_denoise.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=38d6772021197d309aeea0b412187d16 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/i2i_image_denoise.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=5dcd1d04635b534f9b9574904102857b 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/i2i_image_denoise.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=a6bae9f044fe9ea86e11e92b6378e5fa 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/i2i_image_denoise.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=8c75d228c89aa01a470e45e108bdce71 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/i2i_image_denoise.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=d0864c9de07f0aedcb22b88f71392f95 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/stability_ai/i2i_image_denoise.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=5c759a07cf3ed8d4212f151afbe676be 2500w" />

* **Reference Image Selection**: Choosing images with clear subjects and good composition usually leads to better results.
* **Prompt Tips**: In image-to-image mode, prompts should focus more on what you want to change or enhance, rather than describing all elements already present in the image.

## Related Node Documentation

You can refer to the documentation below for detailed parameter settings and more information about the corresponding nodes

<Card title="Stability Stable Image Ultra Node Documentation" icon="book" href="/built-in-nodes/partner-node/image/stability-ai/stability-ai-stable-image-ultra">
  Stability Stable Image Ultra API Node Documentation
</Card>


# Cosmos Predict2 Video2World  ComfyUI Official Example
Source: https://docs.comfy.org/tutorials/video/cosmos/cosmos-predict2-video2world

This guide demonstrates how to complete Cosmos-Predict2 Video2World workflows in ComfyUI

Cosmos-Predict2 is NVIDIA's next-generation physical world foundation model, specifically designed for high-quality visual generation and prediction tasks in physical AI scenarios.
The model features exceptional physical accuracy, environmental interactivity, and detail reproduction capabilities, enabling realistic simulation of complex physical phenomena and dynamic scenes.

Cosmos-Predict2 supports various generation methods including Text-to-Image (Text2Image) and Video-to-World (Video2World),
and is widely used in industrial simulation, autonomous driving, urban planning, scientific research, and other fields.
It serves as a crucial foundational tool for promoting deep integration of intelligent vision and the physical world.

GitHub:[Cosmos-predict2](https://github.com/nvidia-cosmos/cosmos-predict2)
huggingface: [Cosmos-Predict2](https://huggingface.co/collections/nvidia/cosmos-predict2-68028efc052239369a0f2959)

This guide will walk you through completing **Video2World** generation in ComfyUI.

For the text-to-image section, please refer to the following part:

<Card title="Cosmos Predict2 Text to Image" icon="book" href="/tutorials/image/cosmos/cosmos-predict2-t2i">
  Using Cosmos-Predict2 for text-to-image generation
</Card>


# ComfyUI Hunyuan Video Examples
Source: https://docs.comfy.org/tutorials/video/hunyuan/hunyuan-video

This guide shows how to use Hunyuan Text-to-Video and Image-to-Video workflows in ComfyUI

<video controls className="w-full aspect-video" src="https://github.com/user-attachments/assets/442afb73-3092-454f-bc46-02361c285930" />

Hunyuan Video series is developed and open-sourced by [Tencent](https://huggingface.co/tencent), featuring a hybrid architecture that supports both [Text-to-Video](https://github.com/Tencent/HunyuanVideo) and [Image-to-Video](https://github.com/Tencent/HunyuanVideo-I2V) generation with a parameter scale of 13B.

Technical features:

* **Core Architecture:** Uses a DiT (Diffusion Transformer) architecture similar to Sora, effectively fusing text, image, and motion information to improve consistency, quality, and alignment between generated video frames. A unified full-attention mechanism enables multi-view camera transitions while ensuring subject consistency.
* **3D VAE:** The custom 3D VAE compresses videos into a compact latent space, making image-to-video generation more efficient.
* **Superior Image-Video-Text Alignment:** Utilizing MLLM text encoders that excel in both image and video generation, better following text instructions, capturing details, and performing complex reasoning.

You can learn more through the official repositories: [Hunyuan Video](https://github.com/Tencent/HunyuanVideo) and [Hunyuan Video-I2V](https://github.com/Tencent/HunyuanVideo-I2V).

This guide will walk you through setting up both **Text-to-Video** and **Image-to-Video** workflows in ComfyUI.

<Tip>
  The workflow images in this tutorial contain metadata with model download information.

  Simply drag them into ComfyUI or use the menu `Workflows` -> `Open (ctrl+o)` to load the corresponding workflow, which will prompt you to download the required models.

  Alternatively, this guide provides direct model links if automatic downloads fail or you are not using the Desktop version. All models are available [here](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/tree/main/split_files) for download.
</Tip>

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Common Models for All Workflows

The following models are used in both Text-to-Video and Image-to-Video workflows. Please download and save them to the specified directories:

* [clip\_l.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/clip_l.safetensors?download=true)
* [llava\_llama3\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/llava_llama3_fp8_scaled.safetensors?download=true)
* [hunyuan\_video\_vae\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/vae/hunyuan_video_vae_bf16.safetensors?download=true)

Storage location:

```
ComfyUI/
 models/
    text_encoders/
       clip_l.safetensors
       llava_llama3_fp8_scaled.safetensors
    vae/
       hunyuan_video_vae_bf16.safetensors
```

## Hunyuan Text-to-Video Workflow

Hunyuan Text-to-Video was open-sourced in December 2024, supporting 5-second short video generation through natural language descriptions in both Chinese and English.

### 1. Workflow

Download the image below and drag it into ComfyUI to load the workflow:
![ComfyUI Workflow - Hunyuan Text-to-Video](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/t2v/kitchen.webp)

### 2. Manual Models Installation

Download [hunyuan\_video\_t2v\_720p\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_t2v_720p_bf16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models` folder.

Ensure you have all these model files in the correct locations:

```
ComfyUI/
 models/
    text_encoders/
       clip_l.safetensors                       // Shared model
       llava_llama3_fp8_scaled.safetensors      // Shared model
    vae/
       hunyuan_video_vae_bf16.safetensors       // Shared model
    diffusion_models/
        hunyuan_video_t2v_720p_bf16.safetensors  // T2V model
```

### 3. Steps to Run the Workflow

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_t2v.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=8cf9db23e82c1c31702966878d7d6326" alt="ComfyUI Hunyuan Video T2V Workflow" data-og-width="4004" width="4004" data-og-height="1810" height="1810" data-path="images/tutorial/advanced/hunyuanvideo/flow_diagram_t2v.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_t2v.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=46ecaef5c813cfe73e61b8f9f4cdbe6b 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_t2v.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=5d87f06e2721307cf44973080a3cc4c1 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_t2v.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=351426c249545789f7fddcd6bfdbd545 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_t2v.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=5f7c8c5315be22ad33d24525c7ea75af 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_t2v.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=121b5d422c627eb576f607141c91084c 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_t2v.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=7c1054fb6ba08ec4b3f388ecbd1292c2 2500w" />

1. Ensure the `DualCLIPLoader` node has loaded these models:
   * clip\_name1: clip\_l.safetensors
   * clip\_name2: llava\_llama3\_fp8\_scaled.safetensors
2. Ensure the `Load Diffusion Model` node has loaded `hunyuan_video_t2v_720p_bf16.safetensors`
3. Ensure the `Load VAE` node has loaded `hunyuan_video_vae_bf16.safetensors`
4. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

<Tip>
  When the `length` parameter in the `EmptyHunyuanLatentVideo` node is set to 1, the model can generate a static image.
</Tip>

## Hunyuan Image-to-Video Workflow

Hunyuan Image-to-Video model was open-sourced on March 6, 2025, based on the HunyuanVideo framework. It transforms static images into smooth, high-quality videos and also provides LoRA training code to customize special video effects like hair growth, object transformation, etc.

Currently, the Hunyuan Image-to-Video model has two versions:

* v1 "concat": Better motion fluidity but less adherence to the image guidance
* v2 "replace": Updated the day after v1, with better image guidance but seemingly less dynamic compared to v1

<div class="flex justify-between">
  <div class="text-center">
    <p>v1 "concat"</p>

    <img src="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/hunyuan_video_image_to_video.webp" alt="HunyuanVideo v1" />
  </div>

  <div class="text-center">
    <p>v2 "replace"</p>

    <img src="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/hunyuan_video_image_to_video_v2.webp" alt="HunyuanVideo v2" />
  </div>
</div>

### Shared Model for v1 and v2 Versions

Download the following file and save it to the `ComfyUI/models/clip_vision` directory:

* [llava\_llama3\_vision.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/clip_vision/llava_llama3_vision.safetensors?download=true)

### V1 "concat" Image-to-Video Workflow

#### 1. Workflow and Asset

Download the workflow image below and drag it into ComfyUI to load the workflow:
![ComfyUI Workflow - Hunyuan Image-to-Video v1](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/i2v/v1_robot.webp)

Download the image below, which we'll use as the starting frame for the image-to-video generation:
![Starting Frame](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/hunyuan-video/i2v/robot-ballet.png)

#### 2. Related models manual installation

* [hunyuan\_video\_image\_to\_video\_720p\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_image_to_video_720p_bf16.safetensors?download=true)

Ensure you have all these model files in the correct locations:

```
ComfyUI/
 models/
    clip_vision/
       llava_llama3_vision.safetensors                     // I2V shared model
    text_encoders/
       clip_l.safetensors                                  // Shared model
       llava_llama3_fp8_scaled.safetensors                 // Shared model
    vae/
       hunyuan_video_vae_bf16.safetensors                  // Shared model
    diffusion_models/
        hunyuan_video_image_to_video_720p_bf16.safetensors  // I2V v1 "concat" version model
```

#### 3. Steps to Run the Workflow

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v1.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=66d1fd271cc2a45a4fbcd92850074912" alt="ComfyUI Hunyuan Video I2V v1 Workflow" data-og-width="4604" width="4604" data-og-height="1780" height="1780" data-path="images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v1.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v1.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=c69870b0963cf8328ef655ada46aff2e 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v1.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=b8445274ec2b9c64952eca5d43d80afd 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v1.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=7d2f125a2e28660e4fe97dca2c223951 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v1.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=91eab50c724944dc6cf09f2213360ea4 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v1.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=6aacdffc0aa5c73dc82ca3857f89761b 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v1.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=080cb021fdef78a1b6dbbf5538f4a2fc 2500w" />

1. Ensure that `DualCLIPLoader` has loaded these models:
   * clip\_name1: clip\_l.safetensors
   * clip\_name2: llava\_llama3\_fp8\_scaled.safetensors
2. Ensure that `Load CLIP Vision` has loaded `llava_llama3_vision.safetensors`
3. Ensure that `Load Image Model` has loaded `hunyuan_video_image_to_video_720p_bf16.safetensors`
4. Ensure that `Load VAE` has loaded `vae_name: hunyuan_video_vae_bf16.safetensors`
5. Ensure that `Load Diffusion Model` has loaded `hunyuan_video_image_to_video_720p_bf16.safetensors`
6. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

### v2 "replace" Image-to-Video Workflow

The v2 workflow is essentially the same as the v1 workflow. You just need to download the **replace** model and use it in the `Load Diffusion Model` node.

#### 1. Workflow and Asset

Download the workflow image below and drag it into ComfyUI to load the workflow:
![ComfyUI Workflow - Hunyuan Image-to-Video v2](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/i2v/v2_fennec_gril.webp)

Download the image below, which we'll use as the starting frame for the image-to-video generation:
![Starting Frame](https://comfyanonymous.github.io/ComfyUI_examples/flux/flux_dev_example.png)

#### 2. Related models manual installation

* [hunyuan\_video\_v2\_replace\_image\_to\_video\_720p\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors?download=true)

Ensure you have all these model files in the correct locations:

```
ComfyUI/
 models/
    clip_vision/
       llava_llama3_vision.safetensors                                // I2V shared model
    text_encoders/
       clip_l.safetensors                                             // Shared model
       llava_llama3_fp8_scaled.safetensors                            // Shared model
    vae/
       hunyuan_video_vae_bf16.safetensors                             // Shared model
    diffusion_models/
        hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors  // V2 "replace" version model
```

#### 3. Steps to Run the Workflow

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v2.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=f26ddd8edc837fbf9d2bb4f6459a82ee" alt="ComfyUI Hunyuan Video I2V v2 Workflow" data-og-width="4604" width="4604" data-og-height="1780" height="1780" data-path="images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v2.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v2.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=d8baea25dfcaf7566c43eee90ddf30e2 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v2.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=095646b3588b08ac62943b23d4a4a381 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v2.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=c4fb1b7af59a8236189b723f182384c0 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v2.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=41e6f324ab148f773a1fbf7a68d1eecc 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v2.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=a428533a8b26e8a34f7cdc240d60a0c3 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v2.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=c4af9cc4415d6b5c355bae31a57488ae 2500w" />

1. Ensure the `DualCLIPLoader` node has loaded these models:
   * clip\_name1: clip\_l.safetensors
   * clip\_name2: llava\_llama3\_fp8\_scaled.safetensors
2. Ensure the `Load CLIP Vision` node has loaded `llava_llama3_vision.safetensors`
3. Ensure the `Load Image Model` node has loaded `hunyuan_video_image_to_video_720p_bf16.safetensors`
4. Ensure the `Load VAE` node has loaded `hunyuan_video_vae_bf16.safetensors`
5. Ensure the `Load Diffusion Model` node has loaded `hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors`
6. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## Try it yourself

Here are some images and prompts we provide. Based on that content or make an adjustment to create your own video.

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/humanoid_android_dressed_in_a_flowing.png?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=16d6b0ea15e14c74e8d2e5bfacfe4bf8" alt="example" data-og-width="1024" width="1024" data-og-height="1024" height="1024" data-path="images/tutorial/advanced/hunyuanvideo/humanoid_android_dressed_in_a_flowing.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/humanoid_android_dressed_in_a_flowing.png?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=fb99a02e45e6c7f270d9ce083790314c 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/humanoid_android_dressed_in_a_flowing.png?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=d9a4f1f52d7215e48a374391428822af 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/humanoid_android_dressed_in_a_flowing.png?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=9e906c0b832af401f05c4cc052dc8fcf 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/humanoid_android_dressed_in_a_flowing.png?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=c8da46339d87ae93596c5e61bd5ec0c3 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/humanoid_android_dressed_in_a_flowing.png?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=19badecb5287b9aa7230d6e5610f5d09 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/humanoid_android_dressed_in_a_flowing.png?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=d4a1c639e320e34572b495821a1a9408 2500w" />

```
Futuristic robot dancing ballet, dynamic motion, fast motion, fast shot, moving scene
```

***

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/samurai.png?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=f8a76a691a7a9ebda088941350538375" alt="example" data-og-width="1024" width="1024" data-og-height="1024" height="1024" data-path="images/tutorial/advanced/hunyuanvideo/samurai.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/samurai.png?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=8a06ce9e5abc61680c56b8f50d351b51 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/samurai.png?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=69324963fb61e59c1228c1bc8e74e230 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/samurai.png?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=23bd95f28cf382d0653eabb11f7db077 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/samurai.png?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=87ce3e4a1a8df22d48cdec2b10c27fdb 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/samurai.png?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=ba0bedcdaa6d746d03cea491e9ecd1db 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/samurai.png?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=766950cfbd96fde9037be7605cf3674e 2500w" />

```
Samurai waving sword and hitting the camera. camera angle movement, zoom in, fast scene, super fast, dynamic
```

***

<img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hunyuanvideo/a_flying_car.png?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=aa2ce4f30c4d367170a8a696e62a7c7e" alt="example" data-og-width="1024" width="1024" data-og-height="1024" height="1024" data-path="images/tutorial/advanced/hunyuanvideo/a_flying_car.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hunyuanvideo/a_flying_car.png?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=6a8b34642f416da105998b377bde9dc8 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hunyuanvideo/a_flying_car.png?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=25f4eb4f74901534255e2b0bf552c218 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hunyuanvideo/a_flying_car.png?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=f68ad26ec9cef9228518da359e75acb8 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hunyuanvideo/a_flying_car.png?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=321b298270f365989f9ff1b39d8f9b97 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hunyuanvideo/a_flying_car.png?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=58b7c90be2e12ad056058c47720e5fbe 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hunyuanvideo/a_flying_car.png?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=e3d9e0536bdf426014c8f8a27de20065 2500w" />

```
flying car fastly moving and flying through the city
```

***

<img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hunyuanvideo/cyber_car_race.png?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=875b086bd33ac131bda4fe6c718b9bc7" alt="example" data-og-width="1024" width="1024" data-og-height="1024" height="1024" data-path="images/tutorial/advanced/hunyuanvideo/cyber_car_race.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hunyuanvideo/cyber_car_race.png?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=ba4b75e9f735ba0cc5f136bec09360f9 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hunyuanvideo/cyber_car_race.png?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=c4a722a42e56ba171385fc7a78893227 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hunyuanvideo/cyber_car_race.png?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=4a04c0664b3e5bb3d6441243e5a7802c 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hunyuanvideo/cyber_car_race.png?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=eab9135cbe99a2b42a3bf29cc901736a 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hunyuanvideo/cyber_car_race.png?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=2a2173570ea181e0711e6984dd13e3c8 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hunyuanvideo/cyber_car_race.png?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=ea36cb8fda2435a47eec887bb130d6d8 2500w" />

```
cyberpunk car race in night city, dynamic, super fast, fast shot
```


# HunyuanVideo 1.5
Source: https://docs.comfy.org/tutorials/video/hunyuan/hunyuan-video-1-5

Learn how to use HunyuanVideo 1.5, a lightweight 8.3B parameter model for high-quality video generation on consumer GPUs

[HunyuanVideo 1.5](https://github.com/Tencent/HunyuanVideo) is a lightweight 8.3B parameter model developed by Tencent's Hunyuan team. It delivers flagship-quality video generation on consumer GPUs (24GB VRAM), dramatically lowering the barrier to entry without compromising on quality.

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Model highlights

* **Compact powerhouse**: Delivers SOTA performance comparable to larger models while running on consumer hardware.
* **Versatile generation**: Supports high-quality Text-to-Video and Image-to-Video (5-10s) with exceptional consistency.
* **Precise control**: Strong instruction following for camera movements, physics, and emotional expressions.
* **Cinematic quality**: Native 720p output (upscalable to 1080p) with professional aesthetics.
* **Rich features**: Supports diverse styles (realistic, anime, 3D) and in-video text rendering (Chinese/English).

## Workflow templates

[video\_hunyuan\_video\_1.5\_720p\_i2v.json](https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/video_hunyuan_video_1.5_720p_i2v.json)

[video\_hunyuan\_video\_1.5\_720p\_t2v.json](https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/video_hunyuan_video_1.5_720p_t2v.json)

## Model links

**text\_encoders**

* [qwen\_2.5\_vl\_7b\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/text_encoders/qwen_2.5_vl_7b_fp8_scaled.safetensors)
* [byt5\_small\_glyphxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/text_encoders/byt5_small_glyphxl_fp16.safetensors)

**diffusion\_models**

* [hunyuanvideo1.5\_1080p\_sr\_distilled\_fp16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/diffusion_models/hunyuanvideo1.5_1080p_sr_distilled_fp16.safetensors)
* [hunyuanvideo1.5\_720p\_t2v\_fp16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/diffusion_models/hunyuanvideo1.5_720p_t2v_fp16.safetensors)

**vae**

* [hunyuanvideo15\_vae\_fp16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/vae/hunyuanvideo15_vae_fp16.safetensors)

Model Storage Location

```
:open_file_folder: ComfyUI/
 :open_file_folder: models/
    :open_file_folder: text_encoders/
          qwen_2.5_vl_7b_fp8_scaled.safetensors
          byt5_small_glyphxl_fp16.safetensors
    :open_file_folder: diffusion_models/
          hunyuanvideo1.5_1080p_sr_distilled_fp16.safetensors
          hunyuanvideo1.5_720p_t2v_fp16.safetensors
    :open_file_folder: vae/
           hunyuanvideo15_vae_fp16.safetensors
```


# LTX-Video
Source: https://docs.comfy.org/tutorials/video/ltxv



[LTX-Video](https://huggingface.co/Lightricks/LTX-Video) is a very efficient video model by lightricks. The important thing with this model is to give it long descriptive prompts.

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Multi Frame Control

Allows you to control the video with a series of images. You can download the input images: [starting frame](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/house1.png) and [ending frame](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/house2.png).

<img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/workflow.webp" alt="LTX-Video Multi Frame Control" />

<Tip>
  Drag the video directly into ComfyUI to run the workflow.
</Tip>

## Image to Video

Allows you to control the video with a first [frame image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/i2v/girl1.png).

<img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/i2v/workflow.webp" alt="LTX-Video Image to Video" />

<Tip>
  Drag the video directly into ComfyUI to run the workflow.
</Tip>

## Text to Video

<img src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/t2v.webp" alt="LTX-Video Text to Video" />

<Tip>
  Drag the video directly into ComfyUI to run the workflow.
</Tip>

## Requirements

Download the following models and place them in the locations specified below:

* [ltx-video-2b-v0.9.5.safetensors](https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltx-video-2b-v0.9.5.safetensors?download=true)
* [t5xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/mochi_preview_repackaged/resolve/main/split_files/text_encoders/t5xxl_fp16.safetensors?download=true)

```
 checkpoints/
    ltx-video-2b-v0.9.5.safetensors
 text_encoders/
     t5xxl_fp16.safetensors
```


# ComfyUI Wan2.1 Fun Camera Official Examples
Source: https://docs.comfy.org/tutorials/video/wan/fun-camera

This guide demonstrates how to use Wan2.1 Fun Camera in ComfyUI for video generation

## About Wan2.1 Fun Camera

**Wan2.1 Fun Camera** is a video generation project launched by the Alibaba team, focusing on controlling video generation effects through camera motion.

**Model Weights Download**:

* [14B Version](https://huggingface.co/alibaba-pai/Wan2.1-Fun-V1.1-14B-Control-Camera)
* [1.3B Version](https://huggingface.co/alibaba-pai/Wan2.1-Fun-V1.1-1.3B-Control-Camera)

**Code Repository**: [VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)

**ComfyUI now natively supports the Wan2.1 Fun Camera model**.

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Model Installation

These models only need to be installed once. Additionally, model download information is included in the corresponding workflow images, so you can choose your preferred way to download the models.

All of the following models can be found at [Wan\_2.1\_ComfyUI\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged)

**Diffusion Models** choose either 1.3B or 14B:

* [wan2.1\_fun\_camera\_v1.1\_1.3B\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_camera_v1.1_1.3B_bf16.safetensors)
* [wan2.1\_fun\_camera\_v1.1\_14B\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_camera_v1.1_14B_bf16.safetensors)

If you've used Wan2.1 related models before, you should already have the following models. If not, please download them:

**Text Encoders** choose one:

* [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors)
* [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors)

**VAE**

* [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors)

**CLIP Vision**

* [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors)

File Storage Location:

```
 ComfyUI/
  models/
   diffusion_models/
     wan2.1_fun_camera_v1.1_1.3B_bf16.safetensors # 1.3B version
     wan2.1_fun_camera_v1.1_14B_bf16.safetensors # 14B version
   text_encoders/
     umt5_xxl_fp8_e4m3fn_scaled.safetensors
   vae/
     wan_2.1_vae.safetensors
   clip_vision/
      clip_vision_h.safetensors
```

## ComfyUI Wan2.1 Fun Camera 1.3B Native Workflow Example

### 1. Workflow Related Files Download

#### 1.1 Workflow File

Download the video below and drag it into ComfyUI to load the corresponding workflow:

<video controls className="w-full aspect-video" src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/fun-camera/v1.1/wan2.1_fun_camera_1.3B.mp4" />

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/fun-camera/v1.1/wan2.1_fun_camera_1.3B.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download Json Workflow File</p>
</a>

<Note>
  If you want to use the 14B version, simply replace the model file with the 14B version, but please be aware of the VRAM requirements.
</Note>

#### 1.2 Input Image Download

Please download the image below, which we will use as the starting frame:

![Input Reference Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/fun-camera/v1.1/wan2.1_fun_camera_1.3B_input.jpg)

### 2. Complete the Workflow Step by Step

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2-1-fun-camera-1-3b-step-guide.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=f6fb10f8a36cb9036a66e1e99986c641" alt="Wan2.1 Fun Camera Workflow Steps" data-og-width="3746" width="3746" data-og-height="2130" height="2130" data-path="images/tutorial/video/wan/wan2-1-fun-camera-1-3b-step-guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2-1-fun-camera-1-3b-step-guide.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=22ee500d14a988a10103816501d23c79 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2-1-fun-camera-1-3b-step-guide.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=f93891d54566878c22b523cb9f72b9e2 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2-1-fun-camera-1-3b-step-guide.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=8dcb5b3cbf393f606d14b25dfc042b90 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2-1-fun-camera-1-3b-step-guide.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=c6fea8a46345ac06fa26538615751e30 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2-1-fun-camera-1-3b-step-guide.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=1b3de6566e1c172cae30fd29bc01a7cf 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2-1-fun-camera-1-3b-step-guide.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=f710b32856e9f27a40efa605fd884871 2500w" />

1. Ensure the correct version of model file is loaded:
   * 1.3B version: `wan2.1_fun_camera_v1.1_1.3B_bf16.safetensors`
   * 14B version: `wan2.1_fun_camera_v1.1_14B_bf16.safetensors`
2. Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
3. Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`
4. Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`
5. Upload the starting frame to the `Load Image` node
6. Modify the Prompt if you're using your own input image
7. Set camera motion in the `WanCameraEmbedding` node
8. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute generation

## ComfyUI Wan2.1 Fun Camera 14B Workflow and Input Image

<video controls className="w-full aspect-video" src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/fun-camera/v1.1/wan2.1_fun_camera_14B.mp4" />

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/fun-camera/v1.1/wan2.1_fun_camera_14B.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download Json Workflow File</p>
</a>

**Input Image**
![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/fun-camera/v1.1/wan2.1_fun_camera_14B_input.jpg)

## Performance Reference

**1.3B Version**:

* 512512 resolution on RTX 4090 takes about 72 seconds to generate 81 frames

**14B Version**:

* RTX4090 24GB VRAM may experience insufficient memory when generating 512512 resolution, and memory issues have also occurred on A100 when using larger sizes


# ComfyUI Wan2.1 Fun Control Video Examples
Source: https://docs.comfy.org/tutorials/video/wan/fun-control

This guide demonstrates how to use Wan2.1 Fun Control in ComfyUI to generate videos with control videos

## About Wan2.1-Fun-Control

**Wan2.1-Fun-Control** is an open-source video generation and control project developed by Alibaba team.
It introduces innovative Control Codes mechanisms combined with deep learning and multimodal conditional inputs to generate high-quality videos that conform to preset control conditions. The project focuses on precisely guiding generated video content through multimodal control conditions.

Currently, the Fun Control model supports various control conditions, including **Canny (line art), Depth, OpenPose (human posture), MLSD (geometric edges), and trajectory control.**
The model also supports multi-resolution video prediction with options for 512, 768, and 1024 resolutions at 16 frames per second, generating videos up to 81 frames (approximately 5 seconds) in length.

Model versions:

* **1.3B** Lightweight: Suitable for local deployment and quick inference with **lower VRAM requirements**
* **14B** High-performance: Model size reaches 32GB+, offering better results but **requiring higher VRAM**

Here are the relevant code repositories:

* [Wan2.1-Fun-1.3B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-Control)
* [Wan2.1-Fun-14B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control)
* Code repository: [VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)

ComfyUI now **natively supports** the Wan2.1 Fun Control model. Before starting this tutorial, please update your ComfyUI to ensure you're using a version after [this commit](https://github.com/comfyanonymous/ComfyUI/commit/3661c833bcc41b788a7c9f0e7bc48524f8ee5f82).

In this guide, we'll provide two workflows:

1. A workflow using only native Comfy Core nodes
2. A workflow using custom nodes

<Tip>
  Due to current limitations in native nodes for video support, the native-only workflow ensures users can complete the process without installing custom nodes.
  However, we've found that providing a good user experience for video generation is challenging without custom nodes, so we're providing both workflow versions in this guide.
</Tip>

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Model Installation

You only need to install these models once. The workflow images also contain model download information, so you can choose your preferred download method.

The following models can be found at [Wan\_2.1\_ComfyUI\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged) and [Wan2.1-Fun](https://huggingface.co/collections/alibaba-pai/wan21-fun-67e4fb3b76ca01241eb7e334)

Click the corresponding links to download. If you've used Wan-related workflows before, you only need to download the **Diffusion models**.

**Diffusion models** - choose 1.3B or 14B. The 14B version has a larger file size (32GB) and higher VRAM requirements:

* [wan2.1\_fun\_control\_1.3B\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_control_1.3B_bf16.safetensors?download=true)
* [Wan2.1-Fun-14B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control/blob/main/diffusion_pytorch_model.safetensors?download=true): Rename to `Wan2.1-Fun-14B-Control.safetensors` after downloading

**Text encoders** - choose one of the following models (fp16 precision has a larger size and higher performance requirements):

* [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
* [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**VAE**

* [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**CLIP Vision**

* [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)

File storage location:

```
 ComfyUI/
  models/
     diffusion_models/
       wan2.1_fun_control_1.3B_bf16.safetensors
     text_encoders/
       umt5_xxl_fp8_e4m3fn_scaled.safetensors
     vae/
       wan_2.1_vae.safetensors
     clip_vision/
         clip_vision_h.safetensors                 
```

## ComfyUI Native Workflow

In this workflow, we use videos converted to **WebP format** since the `Load Image` node doesn't currently support mp4 format. We also use **Canny Edge** to preprocess the original video.
Because many users encounter installation failures and environment issues when installing custom nodes, this version of the workflow uses only native nodes to ensure a smoother experience.

Thanks to our powerful ComfyUI authors who provide feature-rich nodes. If you want to directly check the related version, see [Workflow Using Custom Nodes](#workflow-using-custom-nodes).

### 1. Workflow File Download

#### 1.1 Workflow File

Download the image below and drag it into ComfyUI to load the workflow:

![Wan2.1 Fun Control Native Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/wan2.1_fun_control_native.webp)

#### 1.2 Input Images and Videos Download

Please download the following image and video for input:

![Input Reference Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/01-portrait_remix.png)

![Input Reference Video](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/01-portrait_video.webp)

### 2. Complete the Workflow Step by Step

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/fun_control_native_flow_diagram.png?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=97774a50d0e95007c34d94a6eb2d9580" alt="Wan2.1 Fun Control Workflow Steps" data-og-width="2201" width="2201" data-og-height="1907" height="1907" data-path="images/tutorial/video/wan/fun_control_native_flow_diagram.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/fun_control_native_flow_diagram.png?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=7223492ed08271f6722dbbbc03205e47 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/fun_control_native_flow_diagram.png?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=94960baeeaa96f9a0bac5a2e627135e6 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/fun_control_native_flow_diagram.png?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=c323157e3482f4dbc1882e395bee4fac 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/fun_control_native_flow_diagram.png?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=ca92e9b2239e9d1c69e9409b1b8eadd4 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/fun_control_native_flow_diagram.png?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=4aea0fb344f2738824e3a551c32535a6 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/fun_control_native_flow_diagram.png?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=cddab1cd5381dd9ba96f510f18ad5809 2500w" />

1. Ensure the `Load Diffusion Model` node has loaded `wan2.1_fun_control_1.3B_bf16.safetensors`
2. Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
3. Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`
4. Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`
5. Upload the starting frame to the `Load Image` node (renamed to `Start_image`)
6. Upload the control video to the second `Load Image` node. Note: This node currently doesn't support mp4, only WebP videos
7. (Optional) Modify the prompt (both English and Chinese are supported)
8. (Optional) Adjust the video size in `WanFunControlToVideo`, avoiding overly large dimensions
9. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation

### 3. Usage Notes

* Since we need to input the same number of frames as the control video into the `WanFunControlToVideo` node, if the specified frame count exceeds the actual control video frames, the excess frames may display scenes not conforming to control conditions. We'll address this issue in the [Workflow Using Custom Nodes](#workflow-using-custom-nodes)
* Avoid setting overly large dimensions, as this can make the sampling process very time-consuming. Try generating smaller images first, then upscale
* Use your imagination to build upon this workflow by adding text-to-image or other types of workflows to achieve direct text-to-video generation or style transfer
* Use tools like [ComfyUI-comfyui\_controlnet\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux) for richer control options

## Workflow Using Custom Nodes

We'll need to install the following two custom nodes:

* [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)
* [ComfyUI-comfyui\_controlnet\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

You can use [ComfyUI Manager](https://github.com/Comfy-Org/ComfyUI-Manager) to install missing nodes or follow the installation instructions for each custom node package.

### 1. Workflow File Download

#### 1.1 Workflow File

Download the image below and drag it into ComfyUI to load the workflow:

![Workflow File](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/wan2.1_fun_control_use_custom_nodes.webp)

<Note>
  Due to the large size of video files, you can also click [here](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/wan2.1_fun_control_use_custom_nodes.json) to download the workflow file in JSON format.
</Note>

#### 1.2 Input Images and Videos Download

Please download the following image and video for input:
![Input Reference Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/02-robot's_eye.png)

<video controls className="w-full aspect-video" src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/input/02-man's_eye.mp4" />

### 2. Complete the Workflow Step by Step

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/fun_control_using_custom_nodes_flow_diagram.png?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=b22315f2f052e9196bd49b3d95a983f5" alt="Wan2.1 Fun Control Workflow Using Custom Nodes Steps" data-og-width="2201" width="2201" data-og-height="1907" height="1907" data-path="images/tutorial/video/wan/fun_control_using_custom_nodes_flow_diagram.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/fun_control_using_custom_nodes_flow_diagram.png?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=675cdc7eefa6f2533ec005c00aed0019 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/fun_control_using_custom_nodes_flow_diagram.png?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=d8a519cf9a760ea5f1d4948b646566ba 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/fun_control_using_custom_nodes_flow_diagram.png?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=259d2448c192245c2a0797c045025a5f 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/fun_control_using_custom_nodes_flow_diagram.png?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=0968a67a850f1668fc03a7fffa5a3f80 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/fun_control_using_custom_nodes_flow_diagram.png?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=d7f950bcd4b060a60ffb6d63b8df3184 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/fun_control_using_custom_nodes_flow_diagram.png?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=c056f1c3a7bc09933386c9f8182c7254 2500w" />

> The model part is essentially the same. If you've already experienced the native-only workflow, you can directly upload the corresponding images and run it.

1. Ensure the `Load Diffusion Model` node has loaded `wan2.1_fun_control_1.3B_bf16.safetensors`
2. Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
3. Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`
4. Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`
5. Upload the starting frame to the `Load Image` node
6. Upload an mp4 format video to the `Load Video(Upload)` custom node. Note that the workflow has adjusted the default `frame_load_cap`
7. For the current image, the `DWPose Estimator` only uses the `detect_face` option
8. (Optional) Modify the prompt (both English and Chinese are supported)
9. (Optional) Adjust the video size in `WanFunControlToVideo`, avoiding overly large dimensions
10. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation

### 3. Workflow Notes

Thanks to the ComfyUI community authors for their custom node packages:

* This example uses `Load Video(Upload)` to support mp4 videos
* The `video_info` obtained from `Load Video(Upload)` allows us to maintain the same `fps` for the output video
* You can replace `DWPose Estimator` with other preprocessors from the `ComfyUI-comfyui_controlnet_aux` node package
* Prompts support multiple languages

## Usage Tips

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/apply_multi_control_videos.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=7bb51781350a318a61068b1b369b816f" alt="Apply Multi Control Videos" data-og-width="1726" width="1726" data-og-height="1156" height="1156" data-path="images/tutorial/video/wan/apply_multi_control_videos.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/apply_multi_control_videos.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=46a23cf7862f32a64df3d1d01b73e577 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/apply_multi_control_videos.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=6fbce18d1761b87f42e37a0cbf323124 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/apply_multi_control_videos.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=b7d1094d3948dae2188e0e859d6d0b6e 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/apply_multi_control_videos.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=2af733309ec65443188bbaecae9023fe 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/apply_multi_control_videos.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=5e17b08c24145eb84e7abc14360042b8 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/apply_multi_control_videos.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=ca67be00b8e19483845fb3c12bed1cd4 2500w" />

* A useful tip is that you can combine multiple image preprocessing techniques and then use the `Image Blend` node to achieve the goal of applying multiple control methods simultaneously.

* You can use the `Video Combine` node from `ComfyUI-VideoHelperSuite` to save videos in mp4 format

* We use `SaveAnimatedWEBP` because we currently don't support embedding workflow into **mp4** and some other custom nodes may not support embedding workflow too. To preserve the workflow in the video, we choose  `SaveAnimatedWEBP` node.

* In the `WanFunControlToVideo` node, `control_video` is not mandatory, so sometimes you can skip using a control video, first generate a very small video size like 320x320, and then use them as control video input to achieve consistent results.

* [ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper)

* [ComfyUI-KJNodes](https://github.com/kijai/ComfyUI-KJNodes)


# ComfyUI Wan2.1 Fun InP Video Examples
Source: https://docs.comfy.org/tutorials/video/wan/fun-inp

This guide demonstrates how to use Wan2.1 Fun InP in ComfyUI to generate videos with first and last frame control

## About Wan2.1-Fun-InP

**Wan-Fun InP** is an open-source video generation model released by Alibaba, part of the Wan2.1-Fun series, focusing on generating videos from images with first and last frame control.

**Key features**:

* **First and last frame control**: Supports inputting both first and last frame images to generate transitional video between them, enhancing video coherence and creative freedom. Compared to earlier community versions, Alibaba's official model produces more stable and significantly higher quality results.
* **Multi-resolution support**: Supports generating videos at 512512, 768768, 10241024 and other resolutions to accommodate different scenario requirements.

**Model versions**:

* **1.3B** Lightweight: Suitable for local deployment and quick inference with **lower VRAM requirements**
* **14B** High-performance: Model size reaches 32GB+, offering better results but requiring **higher VRAM**

Below are the relevant model weights and code repositories:

* [Wan2.1-Fun-1.3B-Input](https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-Input)
* [Wan2.1-Fun-14B-Input](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Input)
* Code repository: [VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)

<Tip>
  Currently, ComfyUI natively supports the Wan2.1 Fun InP model. Before starting this tutorial, please update your ComfyUI to ensure your version is after [this commit](https://github.com/comfyanonymous/ComfyUI/commit/0a1f8869c9998bbfcfeb2e97aa96a6d3e0a2b5df).
</Tip>

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Wan2.1 Fun InP Workflow

Download the image below and drag it into ComfyUI to load the workflow:

![Workflow File](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_inp/wan2.1_fun_inp.webp)

### 1. Workflow File Download

### 2. Manual Model Installation

If automatic model downloading is ineffective, please download the models manually and save them to the corresponding folders.

The following models can be found at [Wan\_2.1\_ComfyUI\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged) and [Wan2.1-Fun](https://huggingface.co/collections/alibaba-pai/wan21-fun-67e4fb3b76ca01241eb7e334)

**Diffusion models** - choose 1.3B or 14B. The 14B version has a larger file size (32GB) and higher VRAM requirements:

* [wan2.1\_fun\_inp\_1.3B\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_inp_1.3B_bf16.safetensors?download=true)
* [Wan2.1-Fun-14B-InP](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-InP/resolve/main/diffusion_pytorch_model.safetensors?download=true): Rename to `Wan2.1-Fun-14B-InP.safetensors` after downloading

**Text encoders** - choose one of the following models (fp16 precision has a larger size and higher performance requirements):

* [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
* [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**VAE**

* [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**CLIP Vision**

* [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)

File storage location:

```
 ComfyUI/
  models/
     diffusion_models/
       wan2.1_fun_inp_1.3B_bf16.safetensors
     text_encoders/
       umt5_xxl_fp8_e4m3fn_scaled.safetensors
     vae/
       wan_2.1_vae.safetensors
     clip_vision/
         clip_vision_h.safetensors                 
```

### 3. Complete the Workflow Step by Step

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/fun_inp_flow_diagram.png?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=421176e8ecfb5b00bfc56979f396e249" alt="ComfyUI Wan2.1 Fun InP Video Generation Workflow Diagram" data-og-width="3000" width="3000" data-og-height="1548" height="1548" data-path="images/tutorial/video/wan/fun_inp_flow_diagram.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/fun_inp_flow_diagram.png?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=d6421229cae3de162152354dbd5513d9 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/fun_inp_flow_diagram.png?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=c2a82f0c27e2fd99b172b2277f6d2ec9 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/fun_inp_flow_diagram.png?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=7e5705e8b68b9edfc65e5ee8cad4b294 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/fun_inp_flow_diagram.png?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=bc9c000021eb2725cb132b7578fac683 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/fun_inp_flow_diagram.png?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=3e67178ae084ea15c0cbb72a39dabc5b 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/fun_inp_flow_diagram.png?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=ed0a9b16eeb932ecd0a42463dce7a8ec 2500w" />

1. Ensure the `Load Diffusion Model` node has loaded `wan2.1_fun_inp_1.3B_bf16.safetensors`
2. Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
3. Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`
4. Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`
5. Upload the starting frame to the `Load Image` node (renamed to `Start_image`)
6. Upload the ending frame to the second `Load Image` node
7. (Optional) Modify the prompt (both English and Chinese are supported)
8. (Optional) Adjust the video size in `WanFunInpaintToVideo`, avoiding overly large dimensions
9. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation

### 4. Workflow Notes

<Tip>
  Please make sure to use the correct model, as `wan2.1_fun_inp_1.3B_bf16.safetensors` and `wan2.1_fun_control_1.3B_bf16.safetensors` are stored in the same folder and have very similar names. Ensure you're using the right model.
</Tip>

* When using Wan Fun InP, you may need to frequently modify prompts to ensure the accuracy of the corresponding scene transitions.

## Other Wan2.1 Fun InP or video-related custom node packages

* [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)
* [ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper)
* [ComfyUI-KJNodes](https://github.com/kijai/ComfyUI-KJNodes)


# ComfyUI Wan2.1 VACE Video Examples
Source: https://docs.comfy.org/tutorials/video/wan/vace

This article introduces how to complete Wan VACE video generation examples in ComfyUI

<Warning>
  As we have made adjustments to the template and added related usage and instructions for CausVid LoRA, this document needs to be updated and requires some preparation time. Until then, please refer to the notes in the template for usage
</Warning>

## About VACE

VACE 14B is an open-source unified video editing model launched by the Alibaba Tongyi Wanxiang team. Through integrating multi-task capabilities, supporting high-resolution processing and flexible multi-modal input mechanisms, this model significantly improves the efficiency and quality of video creation.

The model is open-sourced under the [Apache-2.0](https://github.com/ali-vilab/VACE?tab=Apache-2.0-1-ov-file) license and can be used for personal or commercial purposes.

Here is a comprehensive analysis of its core features and technical highlights:

* Multi-modal input: Supports multiple input forms including text, images, video, masks, and control signals
* Unified architecture: Single model supports multiple tasks with freely combinable functions
* Motion transfer: Generates coherent actions based on reference videos
* Local replacement: Replaces specific areas in videos through masks
* Video extension: Completes actions or extends backgrounds
* Background replacement: Preserves subjects while changing environmental backgrounds

Currently VACE has released two versions - 1.3B and 14B. Compared to the 1.3B version, the 14B version supports 720P resolution output with better image details and stability.

| Model                                                       | 480P | 720P |
| ----------------------------------------------------------- | ---- | ---- |
| [VACE-1.3B](https://huggingface.co/Wan-AI/Wan2.1-VACE-1.3B) |     |     |
| [VACE-14B](https://huggingface.co/Wan-AI/Wan2.1-VACE-14B)   |     |     |

Related model weights and code repositories:

* [VACE-1.3B](https://huggingface.co/Wan-AI/Wan2.1-VACE-1.3B)
* [VACE-14B](https://huggingface.co/Wan-AI/Wan2.1-VACE-14B)
* [Github](https://github.com/ali-vilab/VACE)
* [VACE Project Homepage](https://ali-vilab.github.io/VACE-Page/)

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Model Download and Loading in Workflows

Since the workflows covered in this document all use the same workflow template, we can first complete the model download and loading information introduction, then enable/disable different inputs through Bypassing different nodes to achieve different workflows.
The model download information is already embedded in the workflow information in specific examples, so you can also complete the model download when downloading specific example workflows.

### Model Download

**diffusion\_models**
[wan2.1\_vace\_14B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_vace_14B_fp16.safetensors)
[wan2.1\_vace\_1.3B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_vace_1.3B_fp16.safetensors)

<Tip>
  If you have used Wan Video related workflows before, you have already downloaded the following model files.
</Tip>

**VAE**

* [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

Choose one version from **Text encoders** to download

* [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
* [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

File save location

```
 ComfyUI/
  models/
     diffusion_models/
       wan2.1_vace_14B_fp16.safetensors
     text_encoders/
       umt5_xxl_fp8_e4m3fn_scaled.safetensors # or umt5_xxl_fp16.safetensors
     vae/
         wan_2.1_vae.safetensors
```

### Model Loading

Since the models used in the workflows covered in this document are consistent, the workflows are also the same, and only the nodes are bypassed to enable/disable different inputs, please refer to the following image to ensure that the corresponding models are correctly loaded in different workflows.

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan-vace-model-loading.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=d46629d2ee7cfa1b942d4d92217992b1" alt="Wan2.1 VACE Model Loading" data-og-width="2910" width="2910" data-og-height="1394" height="1394" data-path="images/tutorial/video/wan/wan-vace-model-loading.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan-vace-model-loading.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=91477fcc563509d8b100585ddebac58e 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan-vace-model-loading.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=fc8e48cf61a14dfcefa53557fd607bba 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan-vace-model-loading.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=4c95a9bfd255626b27f53e53ef190f74 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan-vace-model-loading.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=e1541bcd0a68bf62b80881009dc45ce6 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan-vace-model-loading.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=9c721327faf7e078b44f4e22c3c17f80 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan-vace-model-loading.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=a59ba6ef698d23b87b13b21c56dba925 2500w" />

1. Make sure the `Load Diffusion Model` node has loaded `wan2.1_vace_14B_fp16.safetensors`
2. Make sure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors` or `umt5_xxl_fp16.safetensors`
3. Make sure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`

### How to toggle Node Bypass Status

When a node is set to Bypass status, data passing through the node will not be affected by the node and will be output directly. We often set nodes to Bypass status when we don't need them.
Here are three ways to toggle a node's Bypass status:

<img src="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/nodes/cancel-bypass.jpg?fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=3a57352f6bc6f1c16ce0792d384d16fd" alt="Toggle Bypass" data-og-width="1830" width="1830" data-og-height="1128" height="1128" data-path="images/interface/nodes/cancel-bypass.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/nodes/cancel-bypass.jpg?w=280&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=7d117f2ce9b248c5dfef9a2a60f4e19f 280w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/nodes/cancel-bypass.jpg?w=560&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=4aaf480c01ed83648e715bc1db351865 560w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/nodes/cancel-bypass.jpg?w=840&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=672c588fab6405b4abe849298ed662ff 840w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/nodes/cancel-bypass.jpg?w=1100&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=c1bf919830e35f35c02b847721aeebc7 1100w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/nodes/cancel-bypass.jpg?w=1650&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=62d75544a82f04cfa7c5861cdeeed1c0 1650w, https://mintcdn.com/dripart/qYv6P0RgI3co7-eH/images/interface/nodes/cancel-bypass.jpg?w=2500&fit=max&auto=format&n=qYv6P0RgI3co7-eH&q=85&s=27e6b1f470ec9a7e5879b57330d50bee 2500w" />

1. After selecting the node, click the arrow in the indicator section of the selection toolbox to quickly toggle the node's Bypass status
2. After selecting the node, right-click the node and select `Mode` -> `Always` to switch to Always mode
3. After selecting the node, right-click the node and select the `Bypass` option to toggle the Bypass status

## VACE Text-to-Video Workflow

<Tip>
  If you cannot load the workflow from mp4 file, please ensure that your ComfyUI front-end version is up to date version in [requirements.txt](https://github.com/comfyanonymous/ComfyUI/blob/master/requirements.txt) , make sure you can load the workflow from mp4 file.

  Currently 1.19.9 is the latest ComfyUI front-end version in the requirements.txt file.
</Tip>

### 1. Workflow Download

Download the video below and drag it into ComfyUI to load the corresponding workflow

<video controls className="w-full aspect-video" src="https://github.com/Comfy-Org/example_workflows/raw/refs/heads/main/video/wan/vace/vace-t2v.mp4" />

### 2. Complete the Workflow Step by Step

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan-vace-t2v-step-guide.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=6f24e0be5f1ee91ea5becd79501053bd" alt="image" data-og-width="3018" width="3018" data-og-height="1394" height="1394" data-path="images/tutorial/video/wan/wan-vace-t2v-step-guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan-vace-t2v-step-guide.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=0cd5b40ceedcf75c9fbde402ce4a14da 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan-vace-t2v-step-guide.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=b1f90e2fdac271316123b90a857234f5 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan-vace-t2v-step-guide.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=ff5306447eb8ac27b41493f81403e6a7 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan-vace-t2v-step-guide.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=82b65902537bc52e8bec0cd1ab902533 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan-vace-t2v-step-guide.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=7ed23e4062bcd3eef1f7fb626e5e963d 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan-vace-t2v-step-guide.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=6c2b3687c3865ee1cd442b9adf3c8812 2500w" />

Please follow the numbered steps in the image to ensure smooth workflow execution

1. Enter positive prompts in the `CLIP Text Encode (Positive Prompt)` node
2. Enter negative prompts in the `CLIP Text Encode (Negative Prompt)` node
3. Set the image dimensions (640x640 resolution recommended for first run) and frame count (video duration) in `WanVaceToVideo`
4. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation
5. Once generated, the video will automatically save to `ComfyUI/output/video` directory (subfolder location depends on `save video` node settings)

<Tip>
  During testing with a 4090 GPU:

  * 720x1280 resolution, generating 81 frames takes about 40 minutes
  * 640x640 resolution, generating 49 frames takes about 7 minutes

  However, 720P video quality is better.
</Tip>

## VACE Image-to-Video Workflow

You can continue using the workflow above, just unbypass the `Load image` node in **Load reference image** and input your image. You can also use the image below - in this file we've already set up the corresponding parameters.

### 1. Workflow Download

Download the video below and drag it into ComfyUI to load the corresponding workflow

<video controls className="w-full aspect-video" src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/vace/i2v/vace_i2v.mp4" />

Please download the image below as input

![vace-i2v-input](https://github.com/Comfy-Org/example_workflows/raw/refs/heads/main/video/wan/vace/i2v/input.jpg)

### 2. Complete the Workflow Step by Step

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan-vace-i2v-step-guide.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=e22bf953a5900de76c55ed9af3881ea3" alt="Workflow Steps" data-og-width="2912" width="2912" data-og-height="1317" height="1317" data-path="images/tutorial/video/wan/wan-vace-i2v-step-guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan-vace-i2v-step-guide.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=7f1807f6dad3f93106ff5ad6329c8f7c 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan-vace-i2v-step-guide.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=c09e0a08107d4a6ae30a4a5784c6e652 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan-vace-i2v-step-guide.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=b7863d0b4601fa037084d2d008131601 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan-vace-i2v-step-guide.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=531e554960d84d24b2ba6d4facadba50 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan-vace-i2v-step-guide.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=ae5cd06c50fc44a1fc4e690c4c3d4821 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan-vace-i2v-step-guide.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=7a4bb7922bc8e81b1bc4b5ddc1f3c8e8 2500w" />

Please follow the numbered steps in the image to ensure smooth workflow execution

1. Input the corresponding image in the `Load image` node
2. You can modify and edit prompts like in the text-to-video workflow
3. Set the image dimensions (640x640 resolution recommended for first run) and frame count (video duration) in `WanVaceToVideo`
4. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation
5. Once generated, the video will automatically save to `ComfyUI/output/video` directory (subfolder location depends on `save video` node settings)

<Tip>
  You may want to use nodes like getting image dimensions to set the resolution, but due to width and height step requirements of the corresponding nodes, you may get error messages if your image dimensions are not divisible by 16.
</Tip>

### 3. Additional Workflow Notes

VACE also supports inputting multiple reference images in a single image to generate corresponding videos. You can see related examples on the VACE project [page](https://ali-vilab.github.io/VACE-Page/)

## VACE Video-to-Video Workflow

### 1. Workflow Download

Download the video below and drag it into ComfyUI to load the corresponding workflow

<video controls className="w-full aspect-video" src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/vace/v2v/vace_v2v.mp4" />

We will use the following materials as input:

1. Input image for reference
   ![v2v-input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/vace/v2v/input.jpg)

2. The video below has been preprocessed and will be used to control video generation

<video controls className="w-full aspect-video" src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/vace/v2v/post+depth.mp4" />

3. The video below is the original video. You can download these materials and use preprocessing nodes like [comfyui\_controlnet\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux) to preprocess the images

<video controls className="w-full aspect-video" src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/vace/v2v/original.mp4" />

### 2. Complete the Workflow Step by Step

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan-vace-v2v-step-guide.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=a59e79204a1faf9b3f63c482b65df764" alt="Workflow Steps" data-og-width="2912" width="2912" data-og-height="1317" height="1317" data-path="images/tutorial/video/wan/wan-vace-v2v-step-guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan-vace-v2v-step-guide.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=2e445a8b2597f81581dffcb4e122bdaa 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan-vace-v2v-step-guide.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=ee5d4773ce441c2ca25e9e738a6affde 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan-vace-v2v-step-guide.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=dbc1798f5324e9a62b5415b61c321364 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan-vace-v2v-step-guide.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=65372a3779b7c267e7c36b1b937a9499 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan-vace-v2v-step-guide.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=c68336ae95a110ec9eb9214c8c45b79f 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan-vace-v2v-step-guide.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=fb99bc848cb951a0c43388166fec7d77 2500w" />

Please follow the numbered steps in the image to ensure smooth workflow execution

1. Input the reference image in the `Load Image` node under `Load reference image`
2. Input the control video in the `Load Video` node under `Load control video`. Since the provided video is preprocessed, no additional processing is needed
3. If you need to preprocess the original video yourself, you can modify the `Image preprocessing` group or use `comfyui_controlnet_aux` nodes to complete the preprocessing
4. Modify prompts
5. Set the image dimensions (640x640 resolution recommended for first run) and frame count (video duration) in `WanVaceToVideo`
6. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation
7. Once generated, the video will automatically save to `ComfyUI/output/video` directory (subfolder location depends on `save video` node settings)

## VACE Video Outpainting Workflow

\[To be updated]

## VACE First-Last Frame Video Generation

\[To be updated]

To ensure that the first and last frames are effective, the video `length` setting must satisfy that `length-1` is divisible by 4.

The corresponding `Batch_size` setting must satisfy `Batch_size = length - 2`

## Related Node Documentation

Please refer to the documentation below to learn about related nodes

<Card title="WanVaceToVideo Node Documentation" icon="book" href="/built-in-nodes/conditioning/video-models/wan-vace-to-video">
  WanVaceToVideo Node Documentation
</Card>

<Card title="TrimVideoLatent Node Documentation" icon="book" href="/built-in-nodes/latent/video/trim-video-latent">
  ComfyUI TrimVideoLatent Node Documentation
</Card>


# Wan-Alpha Tutorial
Source: https://docs.comfy.org/tutorials/video/wan/wan-alpha

Learn how to generate videos with alpha channel transparency using Wan-Alpha in ComfyUI

Wan-Alpha is a specialized text-to-video model that generates high-quality videos with alpha channel transparency. Built on the Wan2.1-14B-T2V base model, it creates videos with transparent backgrounds and semi-transparent objects, perfect for compositing workflows.

The model excels at generating transparent backgrounds, semi-transparent objects (bubbles, glass, water), glowing effects, and fine details with proper alpha channels (hair, smoke, particles).

# Video tutorial

<iframe width="560" height="315" src="https://www.youtube.com/embed/OyDjj7OPUzA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen />

[Download workflow](https://github.com/Comfy-Org/workflows/blob/main/tutorial_workflows/Get_Comfy_With_Comfy_Wan_Alpha.json)

## Resources

* [Wan-Alpha GitHub](https://github.com/WeChatCV/Wan-Alpha)
* [Hugging Face Model](https://huggingface.co/htdong/Wan-Alpha)
* [ComfyUI Version](https://huggingface.co/htdong/Wan-Alpha_ComfyUI)
* [Research Paper](https://arxiv.org/pdf/2509.24979)


# Wan ATI ComfyUI Native Workflow Tutorial
Source: https://docs.comfy.org/tutorials/video/wan/wan-ati

Using trajectory control for video generation.

**ATI (Any Trajectory Instruction)** is a controllable video generation framework proposed by the ByteDance team. ATI is implemented based on Wan2.1 and supports unified control of objects, local regions, and camera motion in videos through arbitrary trajectory instructions.

Project URL: [https://github.com/bytedance/ATI](https://github.com/bytedance/ATI)

## Key Features

* **Unified Motion Control**: Supports trajectory control for multiple motion types including objects, local regions, and camera movements.
* **Interactive Trajectory Editor**: Visual tool that allows users to freely draw and edit motion trajectories on images.
* **Wan2.1 Compatible**: Based on the official Wan2.1 implementation, compatible with environments and model structures.
* **Rich Visualization Tools**: Supports visualization of input trajectories, output videos, and trajectory overlays.

## WAN ATI Trajectory Control Workflow Example

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

### 1. Workflow Download

Download the video below and drag it into ComfyUI to load the corresponding workflow

<video controls className="w-full aspect-video" src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/ati/wan_ati.mp4" />

We will use the following image as input:
![v2v-input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/ati/input.jpg)

### 2. Model Download

If you haven't successfully downloaded the model files from the workflow, you can try downloading them manually using the links below

**Diffusion Model**

* [Wan2\_1-I2V-ATI-14B\_fp8\_e4m3fn.safetensors](https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/Wan2_1-I2V-ATI-14B_fp8_e4m3fn.safetensors)

**VAE**

* [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**Text encoders**   Chose one of following model

* [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
* [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**clip\_vision**

* [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors)

File save location

```
ComfyUI/
 models/
    diffusion_models/
      Wan2_1-I2V-ATI-14B_fp8_e4m3fn.safetensors
    text_encoders/
       umt5_xxl_fp8_e4m3fn_scaled.safetensors # or other version
    clip_vision/
       clip_vision_h.safetensors
    vae/
         wan_2.1_vae.safetensors
```

### 3. Complete the workflow execution step by step

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan_ati_guide.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=300a2b5bd7002d944e1224e22ed4cd92" alt="Workflow step diagram" data-og-width="3746" width="3746" data-og-height="2924" height="2924" data-path="images/tutorial/video/wan/wan_ati_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan_ati_guide.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=c8046adac94511cc1fb4c3f5d0e6b033 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan_ati_guide.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=abcae62b721b60748ae2af6b4f1b1f52 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan_ati_guide.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=1b107bdd946e957568fed198a02c35c0 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan_ati_guide.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=f08203be22e16cbe402365df10d8963e 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan_ati_guide.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=19745c84e95bc68703f514f5a0303259 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan_ati_guide.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=aa23cb94ca4f9edea4bba0aafdddeb31 2500w" />

Please follow the numbered steps in the image to ensure smooth execution of the corresponding workflow

1. Ensure the `Load Diffusion Model` node has loaded the `Wan2_1-I2V-ATI-14B_fp8_e4m3fn.safetensors` model
2. Ensure the `Load CLIP` node has loaded the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model
3. Ensure the `Load VAE` node has loaded the `wan_2.1_vae.safetensors` model
4. Ensure the `Load CLIP Vision` node has loaded the `clip_vision_h.safetensors` model
5. Upload the provided input image in the `Load Image` node
6. Trajectory editing: Currently there is no corresponding trajectory editor in ComfyUI yet, you can use the following link to complete trajectory editing
   * [Online Trajectory Editing Tool](https://comfyui-wiki.github.io/Trajectory-Annotation-Tool/)
7. If you need to modify the prompts (positive and negative), please make changes in the `CLIP Text Encoder` node numbered `5`
8. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute video generation


# ComfyUI Wan2.1 FLF2V Native Example
Source: https://docs.comfy.org/tutorials/video/wan/wan-flf

This guide explains how to complete Wan2.1 FLF2V video generation examples in ComfyUI

Wan FLF2V (First-Last Frame Video Generation) is an open-source video generation model developed by the Alibaba Tongyi Wanxiang team. Its open-source license is [Apache 2.0](https://github.com/Wan-Video/Wan2.1?tab=Apache-2.0-1-ov-file).
Users only need to provide two images as the starting and ending frames, and the model automatically generates intermediate transition frames, outputting a logically coherent and naturally flowing 720p high-definition video.

**Core Technical Highlights**

1. **Precise First-Last Frame Control**: The matching rate of first and last frames reaches 98%, defining video boundaries through starting and ending scenes, intelligently filling intermediate dynamic changes to achieve scene transitions and object morphing effects.
2. **Stable and Smooth Video Generation**: Using CLIP semantic features and cross-attention mechanisms, the video jitter rate is reduced by 37% compared to similar models, ensuring natural and smooth transitions.
3. **Multi-functional Creative Capabilities**: Supports dynamic embedding of Chinese and English subtitles, generation of anime/realistic/fantasy and other styles, adapting to different creative needs.
4. **720p HD Output**: Directly generates 1280720 resolution videos without post-processing, suitable for social media and commercial applications.
5. **Open-source Ecosystem Support**: Model weights, code, and training framework are fully open-sourced, supporting deployment on mainstream AI platforms.

**Technical Principles and Architecture**

1. **DiT Architecture**: Based on diffusion models and Diffusion Transformer architecture, combined with Full Attention mechanism to optimize spatiotemporal dependency modeling, ensuring video coherence.
2. **3D Causal Variational Encoder**: Wan-VAE technology compresses HD frames to 1/128 size while retaining subtle dynamic details, significantly reducing memory requirements.
3. **Three-stage Training Strategy**: Starting from 480P resolution pre-training, gradually upgrading to 720P, balancing generation quality and computational efficiency through phased optimization.

**Related Links**

* **GitHub Repository**: [GitHub](https://github.com/Wan-Video/Wan2.1)
* **Hugging Face Model Page**: [Hugging Face](https://huggingface.co/Wan-AI/Wan2.1-FLF2V-14B-720P)
* **ModelScope Community**: [ModelScope](https://www.modelscope.cn/models/Wan-AI/Wan2.1-FLF2V-14B-720P)

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Wan2.1 FLF2V 720P ComfyUI Native Workflow Example

### 1. Download Workflow Files and Related Input Files

<Tip>
  Since this model is trained on high-resolution images, using smaller sizes may not yield good results. In the example, we use a size of 720 \* 1280, which may cause users with lower VRAM hard to run smoothly and will take a long time to generate.
  If needed, please adjust the video generation size for testing. A small generation size may not produce good output with this model, please notice that.
</Tip>

Please download the WebP file below, and drag it into ComfyUI to load the corresponding workflow. The workflow has embedded the corresponding model download file information.

![Wan2.1 FLF2V 720P f16 workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1_flf2v/wan2.1_flf2v_720_f16.webp)

Please download the two images below, which we will use as the starting and ending frames of the video

![start\_image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1_flf2v/input/start_image.png)
![end\_image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1_flf2v/input/end_image.png)

### 2. Manual Model Installation

If corresponding

All models involved in this guide can be found [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files).

**diffusion\_models** Choose one version based on your hardware conditions

* FP16:[wan2.1\_flf2v\_720p\_14B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_flf2v_720p_14B_fp16.safetensors?download=true)
* FP8:[wan2.1\_flf2v\_720p\_14B\_fp8\_e4m3fn.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/blob/main/split_files/diffusion_models/wan2.1_flf2v_720p_14B_fp8_e4m3fn.safetensors)

<Tip>
  If you have previously tried Wan Video related workflows, you may already have the following files.
</Tip>

Choose one version from **Text encoders** for download,

* [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
* [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**VAE**

* [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**CLIP Vision**

* [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)

File Storage Location

```
ComfyUI/
 models/
    diffusion_models/
       wan2.1_flf2v_720p_14B_fp16.safetensors           # or FP8 version
    text_encoders/
       umt5_xxl_fp8_e4m3fn_scaled.safetensors           # or your chosen version
    vae/
        wan_2.1_vae.safetensors
    clip_vision/
         clip_vision_h.safetensors   
```

### 3. Complete Workflow Execution Step by Step

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2.1_flf2v_14B_720P_step_guide.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=f52356ef98601526215d4c21802f261f" alt="Wan2.1 FLF2V 720P Native Workflow Steps" data-og-width="2361" width="2361" data-og-height="1623" height="1623" data-path="images/tutorial/video/wan/wan2.1_flf2v_14B_720P_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2.1_flf2v_14B_720P_step_guide.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=bad90a020284dacfc3a2ca4c15efcf4e 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2.1_flf2v_14B_720P_step_guide.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=f4dfb39e04e712cef989a2d2f0e5ee0c 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2.1_flf2v_14B_720P_step_guide.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=0ecd3d88d56303af22e744f38f3feefc 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2.1_flf2v_14B_720P_step_guide.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=6b7a7d6e2f1f0efa36421362c685e0fb 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2.1_flf2v_14B_720P_step_guide.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=cdc5e13ece470f7d4b3fb9fae5176ae0 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2.1_flf2v_14B_720P_step_guide.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=00569638fd71ae80e7d36880b04fdf07 2500w" />

1. Ensure the `Load Diffusion Model` node has loaded `wan2.1_flf2v_720p_14B_fp16.safetensors` or `wan2.1_flf2v_720p_14B_fp8_e4m3fn.safetensors`
2. Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
3. Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`
4. Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`
5. Upload the starting frame to the `Start_image` node
6. Upload the ending frame to the `End_image` node
7. (Optional) Modify the positive and negative prompts, both Chinese and English are supported
8. (**Important**) In `WanFirstLastFrameToVideo` we use 720*1280 as default size.because it's a 720P model, so using a small size will not yield good output. Please use size around 720*1280 for good generation.
9. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute video generation


# ComfyUI Wan2.1 Video Examples
Source: https://docs.comfy.org/tutorials/video/wan/wan-video

This guide demonstrates how to generate videos with first and last frames using Wan2.1 Video in ComfyUI

Wan2.1 Video series is a video generation model open-sourced by Alibaba in February 2025 under the [Apache 2.0 license](https://github.com/Wan-Video/Wan2.1?tab=Apache-2.0-1-ov-file).
It offers two versions:

* 14B (14 billion parameters)
* 1.3B (1.3 billion parameters)
  Covering multiple tasks including text-to-video (T2V) and image-to-video (I2V).
  The model not only outperforms existing open-source models in performance but more importantly, its lightweight version requires only 8GB of VRAM to run, significantly lowering the barrier to entry.

<video controls>
  <source src="https://github.com/user-attachments/assets/4aca6063-60bf-4953-bfb7-e265053f49ef" type="video/mp4" />
</video>

* [Wan2.1 Code Repository](https://github.com/Wan-Video/Wan2.1)
* [Wan2.1 Model Repository](https://huggingface.co/Wan-AI)

<UpdateReminder />

## Wan2.1 ComfyUI Native Workflow Examples

<Tip>
  Please update ComfyUI to the latest version before starting the examples to make sure you have native Wan Video support.
</Tip>

## Model Installation

All models mentioned in this guide can be found [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files). Below are the common models you'll need for the examples in this guide, which you can download in advance:

Choose one version from **Text encoders** to download:

* [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
* [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**VAE**

* [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**CLIP Vision**

* [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)

File storage locations:

```
ComfyUI/
 models/
    diffusion_models/
    ...                  # Let's download the models in the corresponding workflow
    text_encoders/
       umt5_xxl_fp8_e4m3fn_scaled.safetensors
    vae/
        wan_2.1_vae.safetensors
    clip_vision/
         clip_vision_h.safetensors   
```

<Note>
  For diffusion models, we'll use the fp16 precision models in this guide because we've found that they perform better than the bf16 versions. If you need other precision versions, please visit [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/diffusion_models) to download them.
</Note>

## Wan2.1 Text-to-Video Workflow

Before starting the workflow, please download [wan2.1\_t2v\_1.3B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_t2v_1.3B_fp16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models/` directory.

> If you need other t2v precision versions, please visit [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/diffusion_models) to download them.

### 1. Workflow File Download

Download the file below and drag it into ComfyUI to load the corresponding workflow:

![Wan2.1 Text-to-Video Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/wan2.1_t2v_1.3b.webp)

### 2. Complete the Workflow Step by Step

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2.1_t2v_1.3b_flow_diagram.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=db6d32906ab2db132e6b92fdb0823419" alt="ComfyUI Wan2.1 Workflow Steps" data-og-width="1901" width="1901" data-og-height="1616" height="1616" data-path="images/tutorial/video/wan/wan2.1_t2v_1.3b_flow_diagram.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2.1_t2v_1.3b_flow_diagram.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=3d33a53eefadf27c9e21c3eebb0f57a9 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2.1_t2v_1.3b_flow_diagram.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=10062c75bbe505525ccd60ec14e4e2cc 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2.1_t2v_1.3b_flow_diagram.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=94d8226fa2a59cd9f34ada362cb4505c 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2.1_t2v_1.3b_flow_diagram.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=7e82be47f4a146000679fd6d09a6cdf0 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2.1_t2v_1.3b_flow_diagram.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=4bbd7285318b9e280ae5c655f81b7579 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2.1_t2v_1.3b_flow_diagram.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=8e08f01a0de9f723a0d074e93768a65d 2500w" />

1. Make sure the `Load Diffusion Model` node has loaded the `wan2.1_t2v_1.3B_fp16.safetensors` model
2. Make sure the `Load CLIP` node has loaded the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model
3. Make sure the `Load VAE` node has loaded the `wan_2.1_vae.safetensors` model
4. (Optional) You can modify the video dimensions in the `EmptyHunyuanLatentVideo` node if needed
5. (Optional) If you need to modify the prompts (positive and negative), make changes in the `CLIP Text Encoder` node at number `5`
6. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the video generation

## Wan2.1 Image-to-Video Workflow

**Since Wan Video separates the 480P and 720P models**, we'll need to provide examples for both resolutions in this guide. In addition to using different models, they also have slight parameter differences.

### 480P Version

#### 1. Workflow and Input Image

Download the image below and drag it into ComfyUI to load the corresponding workflow:
![Wan2.1 Image-to-Video Workflow 14B 480P Workflow Example Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/wan2.1_i2v_14b_480P.webp)

We'll use the following image as input:

![Wan2.1 Image-to-Video Workflow 14B 480P Workflow Example Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/input/flux_dev_example.png)

#### 2. Model Download

Please download [wan2.1\_i2v\_480p\_14B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_i2v_480p_14B_fp16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models/` directory.

#### 3. Complete the Workflow Step by Step

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2.1_i2v_14b_480p_flow_diagram.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=91de28e61a2633cf39b3a52afc2a6459" alt="ComfyUI Wan2.1 Workflow Steps" data-og-width="2318" width="2318" data-og-height="1616" height="1616" data-path="images/tutorial/video/wan/wan2.1_i2v_14b_480p_flow_diagram.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2.1_i2v_14b_480p_flow_diagram.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=e42c99bf5719d85818e6099fac6193b7 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2.1_i2v_14b_480p_flow_diagram.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=7fd87c3dcdfee4208d08491a1697f2e5 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2.1_i2v_14b_480p_flow_diagram.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=4f97468699aa3f27a2ab7be260929c58 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2.1_i2v_14b_480p_flow_diagram.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=8beddee89d2a19e281b4dc234af0503d 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2.1_i2v_14b_480p_flow_diagram.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=97ed81e85ee272d45b89ae54ee6b0e18 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2.1_i2v_14b_480p_flow_diagram.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=6801f558dfa4f00f5c8d4e2f2e424fd4 2500w" />

1. Make sure the `Load Diffusion Model` node has loaded the `wan2.1_i2v_480p_14B_fp16.safetensors` model
2. Make sure the `Load CLIP` node has loaded the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model
3. Make sure the `Load VAE` node has loaded the `wan_2.1_vae.safetensors` model
4. Make sure the `Load CLIP Vision` node has loaded the `clip_vision_h.safetensors` model
5. Upload the provided input image in the `Load Image` node
6. (Optional) Enter the video description content you want to generate in the `CLIP Text Encoder` node
7. (Optional) You can modify the video dimensions in the `WanImageToVideo` node if needed
8. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the video generation

### 720P Version

#### 1. Workflow and Input Image

Download the image below and drag it into ComfyUI to load the corresponding workflow:
![Wan2.1 Image-to-Video Workflow 14B 720P Workflow Example Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/wan2.1_i2v_14b_720P.webp)

We'll use the following image as input:

![Wan2.1 Image-to-Video Workflow 14B 720P Workflow Example Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/wan2.1/input/magician.png)

#### 2. Model Download

Please download [wan2.1\_i2v\_720p\_14B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_i2v_720p_14B_fp16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models/` directory.

#### 3. Complete the Workflow Step by Step

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2.1_i2v_14b_720p_flow_diagram.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=c99af487f3164515ac1ffaf94f0add71" alt="ComfyUI Wan2.1 Workflow Steps" data-og-width="2318" width="2318" data-og-height="1548" height="1548" data-path="images/tutorial/video/wan/wan2.1_i2v_14b_720p_flow_diagram.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2.1_i2v_14b_720p_flow_diagram.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=412fe69cc3d3e3b86ad4bb5344a3df26 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2.1_i2v_14b_720p_flow_diagram.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=f5fef8f2d5234b2bd81dd9e84c36b36c 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2.1_i2v_14b_720p_flow_diagram.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=025653767f20665e0a01e2c74d325836 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2.1_i2v_14b_720p_flow_diagram.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=c1ad2a584a20d6a68057edbc30337b6f 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2.1_i2v_14b_720p_flow_diagram.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=32d21160c3af6618e6c67ca2fe0f6308 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2.1_i2v_14b_720p_flow_diagram.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=0fe836f7c7f9de6d13562379d4178171 2500w" />

1. Make sure the `Load Diffusion Model` node has loaded the `wan2.1_i2v_720p_14B_fp16.safetensors` model
2. Make sure the `Load CLIP` node has loaded the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model
3. Make sure the `Load VAE` node has loaded the `wan_2.1_vae.safetensors` model
4. Make sure the `Load CLIP Vision` node has loaded the `clip_vision_h.safetensors` model
5. Upload the provided input image in the `Load Image` node
6. (Optional) Enter the video description content you want to generate in the `CLIP Text Encoder` node
7. (Optional) You can modify the video dimensions in the `WanImageToVideo` node if needed
8. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the video generation


# Wan2.2 Animate ComfyUI native workflow
Source: https://docs.comfy.org/tutorials/video/wan/wan2-2-animate

Unified character animation and replacement framework with precise motion and expression replication.

Wan-Animate is a unified framework for character animation and replacement developed by the WAN Team.

The model can animate any character based on a performers video, precisely replicating the performers facial expressions and movements to generate highly realistic character videos.

It can also replace characters in a video with animated characters, preserving their expressions and movements while replicating the original lighting and color tone for seamless environmental integration.

## Model Highlights

* Dual Mode Functionality: A single architecture supports both animation and replacement functions, enabling easy operation switching.
* Advanced Body Motion Control: Uses spatially-aligned skeleton signals for accurate body movement replication
* Precise Motion and Expression: Accurately reproduces the movements and facial expressions from the reference video.
* Natural Environment Integration: Seamlessly blends the replaced character with the original video environment.
* Smooth Long Video Generation: Iterative generation ensures consistent motion and visual flow in extended videos

## ComfyOrg Wan2.2 Animate stream replay

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/5kb-rP0m5BA?si=lbIYkCP5akkG2N6D" title="Wan 2.2 Animate in ComfyUI with Flipping Sigmas / September 19th, 2025" frameborder="0" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen />

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## About Wan2.2 Animate workflow

In this docs, we will provide two workflow:

1. Workflow that only uses core nodes (It is incomplete; you need to preprocess the image by yourself first)
2. Workflow that includes some custom nodes (It is complete; you can use it directly, but some new user might not know how to install the custom nodes)

## Wan2.2 Anmate ComfyUI native workflow(without custom nodes)

### 1. Download Workflow File

Download the following workflow file and drag it into ComfyUI to load the workflow.

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/video_wan2_2_14B_animate.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download JSON Workflow</p>
</a>

Download materials below as input:

**Reference Image:**
![Reference\_Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/wan2.2_animate/ref_image.png)

**Input Video**

<video controls className="w-full aspect-video" src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/wan2.2_animate/original_video.mp4" />

### 2. Model links

**diffusion\_models**

* [Wan2\_2-Animate-14B\_fp8\_e4m3fn\_scaled\_KJ.safetensors](https://huggingface.co/Kijai/WanVideo_comfy_fp8_scaled/resolve/main/Wan22Animate/Wan2_2-Animate-14B_fp8_e4m3fn_scaled_KJ.safetensors) This is the model that from Kijai's repo
* [wan2.2\_animate\_14B\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_animate_14B_bf16.safetensors) original model weight

**clip\_visions**

* [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors)

**loras**

* [lightx2v\_I2V\_14B\_480p\_cfg\_step\_distill\_rank64\_bf16.safetensors](https://huggingface.co/Kijai/WanVideo_comfy/resolve/main/Lightx2v/lightx2v_I2V_14B_480p_cfg_step_distill_rank64_bf16.safetensors)  4  lora

**vae**

* [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors)

**text\_encoders**

* [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors)

```
ComfyUI/
 models/
    diffusion_models/
       Wan2_2-Animate-14B_fp8_e4m3fn_scaled_KJ.safetensors
       wan2.2_animate_14B_bf16.safetensors
    loras/
       lightx2v_I2V_14B_480p_cfg_step_distill_rank64_bf16.safetensors
    text_encoders/
       umt5_xxl_fp8_e4m3fn_scaled.safetensors 
    clip_visions/ 
       clip_vision_h.safetensors
    vae/
        wan_2.1_vae.safetensors
```

### 3. Install custom nodes

Download the following workflow file and drag it into ComfyUI to load the workflow, if you have [ComfyUI-Manager](https://github.com/Comfy-Org/ComfyUI-Manager) installed, you can just click the `Install missing nodes` button to install the missing nodes.

We need to install the following custom nodes:

* [ComfyUI-KJNodes](https://github.com/kijai/ComfyUI-KJNodes)
* [ComfyUI-comfyui\_controlnet\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

If you don't know how to install custom nodes please refer to [How to install custom nodes](/installation/install_custom_node)

### 4. Workflow Instructions

The Wan2.2 animate has two modes: Mix and move

* Mix: use the reference image to replace the character in the video
* Move: Use the character movement from the input video to animate the character in the reference image (like Wan2.2 Fun Control)

#### 4.1 Mix mode

<img src="https://mintcdn.com/dripart/UGcNWAMSJO6Toi_8/images/tutorial/video/wan/wan2_2/wan_2.2_14b_animate.jpg?fit=max&auto=format&n=UGcNWAMSJO6Toi_8&q=85&s=d451c51be227ab808ff8aa2723a054ad" alt="Workflow Instructions" data-og-width="4088" width="4088" data-og-height="4096" height="4096" data-path="images/tutorial/video/wan/wan2_2/wan_2.2_14b_animate.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/UGcNWAMSJO6Toi_8/images/tutorial/video/wan/wan2_2/wan_2.2_14b_animate.jpg?w=280&fit=max&auto=format&n=UGcNWAMSJO6Toi_8&q=85&s=83c177be0fee53c909f8af41a80fc6d4 280w, https://mintcdn.com/dripart/UGcNWAMSJO6Toi_8/images/tutorial/video/wan/wan2_2/wan_2.2_14b_animate.jpg?w=560&fit=max&auto=format&n=UGcNWAMSJO6Toi_8&q=85&s=1700a85f1f06283a5af25b5f85a83b2d 560w, https://mintcdn.com/dripart/UGcNWAMSJO6Toi_8/images/tutorial/video/wan/wan2_2/wan_2.2_14b_animate.jpg?w=840&fit=max&auto=format&n=UGcNWAMSJO6Toi_8&q=85&s=07870a19b2e634c84b68949ab4353af6 840w, https://mintcdn.com/dripart/UGcNWAMSJO6Toi_8/images/tutorial/video/wan/wan2_2/wan_2.2_14b_animate.jpg?w=1100&fit=max&auto=format&n=UGcNWAMSJO6Toi_8&q=85&s=44a7243272099fd5935aa7b65202ad1e 1100w, https://mintcdn.com/dripart/UGcNWAMSJO6Toi_8/images/tutorial/video/wan/wan2_2/wan_2.2_14b_animate.jpg?w=1650&fit=max&auto=format&n=UGcNWAMSJO6Toi_8&q=85&s=1b7831761ee0e4274f488857484d8056 1650w, https://mintcdn.com/dripart/UGcNWAMSJO6Toi_8/images/tutorial/video/wan/wan2_2/wan_2.2_14b_animate.jpg?w=2500&fit=max&auto=format&n=UGcNWAMSJO6Toi_8&q=85&s=cb092054225dc49519fb8b350fe45e34 2500w" />

0. If you are running this workflow for the first time, please use a small size for video generation, in case you don't have enough VRAM to run the workflow, and due to the `WanAnimateToVideo` limited, the video width or height should be multiples of 16.
1. Make sure all the models are loaded correctly
2. Update the prompt if you want
3. Upload the reference image, the character is this image will be the target character
4. You can use the videos we provided as input videos for the first time the **DWPose Estimator** node  in [comfyui\_controlnet\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux) will preprocess the input video to pose and face control videos
5. The `Points Editor` is from [KJNodes](https://github.com/kijai/ComfyUI-KJNodes/), by default this node will not load the first frame from the input video, you need to run the workflow once or manually upload the first frame
   * Bleow the `Points Editor` node, we have added note about how this node work, and how to edit it please refer to it
6. For the "Video Extend" group, it's in order to extend to the output video length
   * Each `Video Extend` will extend another 77 frames(Around 4.8125 seconds)
   * If your input video is less then 5s, you might not need it
   * If you want to extend longer, you need to copy and paste multiple times, you need to link the `batch_images` from last Video Extend to next one, and also the `video_frame_offset` from last Video Extend to next one
7. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation

#### 4.2 Move mode

We used [subgraph](/interface/features/subgraph) in the Wan2.2 animate workflow, here is how to switching to move mode:

<img src="https://mintcdn.com/dripart/UGcNWAMSJO6Toi_8/images/tutorial/video/wan/wan2_2/wan2.2_animate_subgraph.jpg?fit=max&auto=format&n=UGcNWAMSJO6Toi_8&q=85&s=193caf05c2494550d6b92318523c54a4" alt="Subgraph" data-og-width="1228" width="1228" data-og-height="1206" height="1206" data-path="images/tutorial/video/wan/wan2_2/wan2.2_animate_subgraph.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/UGcNWAMSJO6Toi_8/images/tutorial/video/wan/wan2_2/wan2.2_animate_subgraph.jpg?w=280&fit=max&auto=format&n=UGcNWAMSJO6Toi_8&q=85&s=d1d50af10d00c280d4ebf10f1bfd0eee 280w, https://mintcdn.com/dripart/UGcNWAMSJO6Toi_8/images/tutorial/video/wan/wan2_2/wan2.2_animate_subgraph.jpg?w=560&fit=max&auto=format&n=UGcNWAMSJO6Toi_8&q=85&s=23a3b3c0db8a431aebad4da9f6c34d7a 560w, https://mintcdn.com/dripart/UGcNWAMSJO6Toi_8/images/tutorial/video/wan/wan2_2/wan2.2_animate_subgraph.jpg?w=840&fit=max&auto=format&n=UGcNWAMSJO6Toi_8&q=85&s=776ecdf12ca9d2158f3a962f7862e613 840w, https://mintcdn.com/dripart/UGcNWAMSJO6Toi_8/images/tutorial/video/wan/wan2_2/wan2.2_animate_subgraph.jpg?w=1100&fit=max&auto=format&n=UGcNWAMSJO6Toi_8&q=85&s=041c2d30fd31f983cac31f1ddeae29b3 1100w, https://mintcdn.com/dripart/UGcNWAMSJO6Toi_8/images/tutorial/video/wan/wan2_2/wan2.2_animate_subgraph.jpg?w=1650&fit=max&auto=format&n=UGcNWAMSJO6Toi_8&q=85&s=e19c71b745f74c7b295607b89c4a01a4 1650w, https://mintcdn.com/dripart/UGcNWAMSJO6Toi_8/images/tutorial/video/wan/wan2_2/wan2.2_animate_subgraph.jpg?w=2500&fit=max&auto=format&n=UGcNWAMSJO6Toi_8&q=85&s=835d846ddc484de3bd0aff40e394e023 2500w" />

If you want to switch to Move mode, you can disconnect `background_video` and `character_mask` inputs from the `Video Sampling and output(Subgraph)` node.


# ComfyUI Wan2.2 Fun Camera Control: Video Generation Workflow Example
Source: https://docs.comfy.org/tutorials/video/wan/wan2-2-fun-camera

This article demonstrates how to use camera control for video generation with Wan2.2 Fun Camera Control in ComfyUI.

**Wan2.2-Fun-Camera-Control** is a next-generation video generation and camera control model developed by Alibaba PAI. By introducing innovative Camera Control Codes and combining deep learning with multimodal conditional inputs, it generates high-quality videos that adhere to predefined camera motion conditions. The model is released under the **Apache 2.0 license**, allowing for commercial use.

**Key Features**:

* **Camera Motion Control**: Supports various camera motion modes, including **Pan Up**, **Pan Down**, **Pan Left**, **Pan Right**, **Zoom In**, **Zoom Out**, and combinations thereof.
* **High-Quality Video Generation**: Based on the Wan2.2 architecture, it outputs cinematic-quality videos.

Here are the relevant model weights and code repository:

* [Wan2.2-Fun-A14B-Control-Camera](https://huggingface.co/alibaba-pai/Wan2.2-Fun-A14B-Control-Camera)
* Code Repository: [VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)

## Wan2.2 Fun Camera Control: Video Generation Workflow Example

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

The workflow provided includes two versions:

1. Using the [Wan2.2-Lightning](https://huggingface.co/lightx2v/Wan2.2-Lightning) 4-step LoRA via lightx2v: This may result in reduced video dynamics but offers faster generation.
2. The fp8\_scaled version without the acceleration LoRA.

Below are the timing results tested on an RTX4090D 24GB GPU for 640\*640 resolution and 81-frame length:

| Model Type                | Resolution | VRAM Usage | First Generation Time | Second Generation Time |
| ------------------------- | ---------- | ---------- | --------------------- | ---------------------- |
| fp8\_scaled               | 640640    | 84%        |  536 seconds         |  513 seconds          |
| fp8\_scaled + 4-step LoRA | 640640    | 89%        |  108 seconds         |  71 seconds           |

While the 4-step LoRA improves initial user experience, it may reduce video dynamism. By default, the accelerated LoRA version is enabled. To switch workflows, select the nodes and press **Ctrl+B**.

### 1. Workflow and Asset Download

Download the video or JSON file below and drag it into ComfyUI to load the corresponding workflow. The workflow will prompt you to download the models.

<video controls className="w-full aspect-video" src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/wan2.2_fun_camera/wan2.2_14B_fun_camera.mp4" />

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/video_wan2_2_14B_fun_camera.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download JSON Workflow</p>
</a>

Please download the image below, which we will use as input.

![Input Starting Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/wan2.2_fun_camera/input.jpg)

### 2. Model Links

The following models can be found in [Wan\_2.2\_ComfyUI\_Repackaged](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged):

**Diffusion Model**

* [wan2.2\_fun\_camera\_high\_noise\_14B\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_fun_camera_high_noise_14B_fp8_scaled.safetensors)
* [wan2.2\_fun\_camera\_low\_noise\_14B\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_fun_camera_low_noise_14B_fp8_scaled.safetensors)

**Wan2.2-Lightning LoRA (Optional, for acceleration)**

* [wan2.2\_i2v\_lightx2v\_4steps\_lora\_v1\_high\_noise.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/loras/wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors)
* [wan2.2\_i2v\_lightx2v\_4steps\_lora\_v1\_low\_noise.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/loras/wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors)

**VAE**

* [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors)

**Text Encoder**

* [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors)

File save location

```
ComfyUI/
 models/
    diffusion_models/
       wan2.2_fun_camera_low_noise_14B_fp8_scaled.safetensors
       wan2.2_fun_camera_high_noise_14B_fp8_scaled.safetensors
    loras/
       wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors
       wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors
    text_encoders/
       umt5_xxl_fp8_e4m3fn_scaled.safetensors 
    vae/
        wan_2.1_vae.safetensors
```

### 3. Complete the Workflow Step-by-Step

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan_2.2_14b_fun_camera.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=8fd9faf8734e1f53f4838d25bb7eb822" alt="Wan2.2 Fun Camera Control Workflow Steps" data-og-width="4088" width="4088" data-og-height="2540" height="2540" data-path="images/tutorial/video/wan/wan_2.2_14b_fun_camera.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan_2.2_14b_fun_camera.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=d262d31cd7e339260756e26c0ebf8c9e 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan_2.2_14b_fun_camera.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=eedf4a26f322d0e7bd58e8f66fc487f8 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan_2.2_14b_fun_camera.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=8aa611a4639dfa11a9a360f54f1d71ee 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan_2.2_14b_fun_camera.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=1c5297db4eacc231d2ba64b95ff772c6 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan_2.2_14b_fun_camera.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=d2b3901874e51a8522dcf161f110394a 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan_2.2_14b_fun_camera.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=693281cc8b4ab59e0b61f9b0d1cff1e9 2500w" />

<Note>
  This workflow uses LoRA. Ensure the Diffusion model and LoRA are consistent; high noise and low noise models and LoRA must be paired accordingly.
</Note>

1. **High noise** model and **LoRA** loading

* Ensure the `Load Diffusion Model` node loads `wan2.2_fun_camera_high_noise_14B_fp8_scaled.safetensors`
* Ensure the `LoraLoaderModelOnly` node loads `wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors`

2. **Low noise** model and **LoRA** loading

* Ensure the `Load Diffusion Model` node loads `wan2.2_fun_camera_low_noise_14B_fp8_scaled.safetensors`
* Ensure the `LoraLoaderModelOnly` node loads `wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors`

3. Ensure the `Load CLIP` node loads `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
4. Ensure the `Load VAE` node loads `wan_2.1_vae.safetensors`
5. Upload the starting frame in the `Load Image` node
6. Modify the Prompt (both Chinese and English are acceptable)
7. Set camera control parameters in the `WanCameraEmbedding` node:
   * **Camera Motion**: Select the camera motion type (Zoom In, Zoom Out, Pan Up, Pan Down, Pan Left, Pan Right, Static, etc.)
   * **Width/Height**: Set video resolution
   * **Length**: Set the number of video frames (default is 81 frames)
   * **Speed**: Set video speed (default is 1.0)
8. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation


# ComfyUI Wan2.2 Fun Control Video Generation Example
Source: https://docs.comfy.org/tutorials/video/wan/wan2-2-fun-control

This article introduces how to use ComfyUI to complete the Wan2.2 Fun Control video generation using control videos

**Wan2.2-Fun-Control** is a next-generation video generation and control model launched by Alibaba PAI team. Through innovative Control Codes mechanism combined with deep learning and multi-modal conditional inputs, it can generate high-quality videos that comply with preset control conditions. The model is released under the **Apache 2.0 license** and supports commercial use.

**Key Features**:

* **Multi-modal Control**: Supports multiple control conditions including **Canny (line art)**, **Depth**, **OpenPose (human pose)**, **MLSD (geometric edges)**, and **trajectory control**
* **High-Quality Video Generation**: Based on the Wan2.2 architecture, outputs film-level quality videos
* **Multi-language Support**: Supports multi-language prompts including Chinese and English

Below are the relevant model weights and code repositories:

* [Wan2.2-Fun-A14B-Control](https://huggingface.co/alibaba-pai/Wan2.2-Fun-A14B-Control)
* Code repository: [VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)

## ComfyOrg Wan2.2 Fun InP & Control Youtube Live Stream Replay

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/YcAerNYIvho?si=Zh8tzRwI_OTAFx3m" title="ComfyUI Selection Toolbox New Features" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

## Wan2.2 Fun Control Video Generation Workflow Example

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

This workflow provides two versions:

1. A version using [Wan2.2-Lightning](https://huggingface.co/lightx2v/Wan2.2-Lightning) 4-step LoRA from lightx2v: may cause some loss in video dynamics but offers faster speed
2. A fp8\_scaled version without acceleration LoRA

Below are the test results using an RTX4090D 24GB VRAM GPU at 640640 resolution with 81 frames

| Model Type                | VRAM Usage | First Generation Time | Second Generation Time |
| ------------------------- | ---------- | --------------------- | ---------------------- |
| fp8\_scaled               | 83%        |  524s                |  520s                 |
| fp8\_scaled + 4-step LoRA | 89%        |  138s                |  79s                  |

Since using the 4-step LoRA provides a better experience for first-time workflow users, but may cause some loss in video dynamics, we have enabled the accelerated LoRA version by default. If you want to enable the other workflow, select it and use **Ctrl+B** to activate.

### 1. Download Workflow and Materials

Download the video below or JSON file and drag it into ComfyUI to load the workflow

<video controls className="w-full aspect-video" src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/wan2.2_fun_control/wan2.2_14B_fun_inp.mp4" />

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/video_wan2_2_14B_fun_control.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download JSON Workflow</p>
</a>

Please download the following images and videos as input materials.

![Input start image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/wan2.2_fun_control/input.jpg)

<video controls className="w-full aspect-video" src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/wan2.2_fun_control/control_video.mp4" />

> We use a preprocessed video here.

### 2. Models

You can find the models below at [Wan\_2.2\_ComfyUI\_Repackaged](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged)

**Diffusion Model**

* [wan2.2\_fun\_control\_high\_noise\_14B\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_fun_control_high_noise_14B_fp8_scaled.safetensors)
* [wan2.2\_fun\_control\_low\_noise\_14B\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_fun_control_low_noise_14B_fp8_scaled.safetensors)

**Wan2.2-Lightning LoRA (Optional, for acceleration)**

* [wan2.2\_i2v\_lightx2v\_4steps\_lora\_v1\_high\_noise.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/loras/wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors)
* [wan2.2\_i2v\_lightx2v\_4steps\_lora\_v1\_low\_noise.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/loras/wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors)

**VAE**

* [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors)

**Text Encoder**

* [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors)

```
ComfyUI/
 models/
    diffusion_models/
       wan2.2_fun_control_low_noise_14B_fp8_scaled.safetensors
       wan2.2_fun_control_high_noise_14B_fp8_scaled.safetensors
    loras/
       wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors
       wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors
    text_encoders/
       umt5_xxl_fp8_e4m3fn_scaled.safetensors 
    vae/
        wan_2.1_vae.safetensors
```

### 3. Workflow Guide

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_fun_control.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=1c5d26bea52f7f790ac6059e7a195c8d" alt="Wan2.2 Fun Control Workflow Steps" data-og-width="4088" width="4088" data-og-height="2138" height="2138" data-path="images/tutorial/video/wan/wan2_2/wan_2.2_14b_fun_control.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_fun_control.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=d8ed6961696acf9bc7873cc8e2205ef9 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_fun_control.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=b2fbdef2bebb0062d41edce5d6956d94 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_fun_control.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=1cff6e967bef4a5c089cd08078b7bea3 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_fun_control.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=7d37bf6785db4ef7d541381860663513 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_fun_control.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=472acf76ca37f17786fd3950c3ca0857 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_fun_control.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=bc7b5ea03c54650e9773d6c0e766d281 2500w" />

<Note>
  This workflow uses LoRA. Please ensure the corresponding Diffusion model and LoRA are matched - high noise and low noise models and LoRAs need to be used correspondingly.
</Note>

1. **High noise** model and **LoRA** loading
   * Ensure the `Load Diffusion Model` node loads the `wan2.2_fun_control_high_noise_14B_fp8_scaled.safetensors` model
   * Ensure the `LoraLoaderModelOnly` node loads the `wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors`
2. **Low noise** model and **LoRA** loading
   * Ensure the `Load Diffusion Model` node loads the `wan2.2_fun_control_low_noise_14B_fp8_scaled.safetensors` model
   * Ensure the `LoraLoaderModelOnly` node loads the `wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors`
3. Ensure the `Load CLIP` node loads the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model
4. Ensure the `Load VAE` node loads the `wan_2.1_vae.safetensors` model
5. Upload the start frame in the `Load Image` node
6. In the second `Load video` node, load the pose control video. The provided video has been preprocessed and can be used directly
7. Since we provide a preprocessed pose video, the corresponding video image preprocessing node needs to be disabled. You can select it and use `Ctrl + B` to disable it
8. Modify the Prompt - you can use both Chinese and English
9. In `Wan22FunControlToVideo`, modify the video dimensions. The default is set to 640640 resolution to avoid excessive processing time for users with low VRAM
10. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute video generation

### Additional Notes

Since ComfyUI's built-in nodes only include Canny preprocessor, you can use tools like [ComfyUI-comfyui\_controlnet\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux) to implement other types of image preprocessing


# ComfyUI Wan2.2 Fun Inp Start-End Frame Video Generation Example
Source: https://docs.comfy.org/tutorials/video/wan/wan2-2-fun-inp

This article introduces how to use ComfyUI to complete the Wan2.2 Fun Inp start-end frame video generation example

**Wan2.2-Fun-Inp** is a start-end frame controlled video generation model launched by Alibaba PAI team. It supports inputting **start and end frame images** to generate intermediate transition videos, providing creators with greater creative control. The model is released under the **Apache 2.0 license** and supports commercial use.

**Key Features**:

* **Start-End Frame Control**: Supports inputting start and end frame images to generate intermediate transition videos, enhancing video coherence and creative freedom
* **High-Quality Video Generation**: Based on the Wan2.2 architecture, outputs film-level quality videos
* **Multi-Resolution Support**: Supports generating videos at 512512, 768768, 10241024 and other resolutions to suit different scenarios

**Model Version**:

* **14B High-Performance Version**: Model size exceeds 32GB, with better results but requires high VRAM

Below are the relevant model weights and code repositories:

* [Wan2.2-Fun-Inp-14B](https://huggingface.co/alibaba-pai/Wan2.2-Fun-A14B-InP)
* Code repository: [VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)

## ComfyOrg Wan2.2 Fun InP & Control Youtube Live Stream Replay

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/YcAerNYIvho?si=Zh8tzRwI_OTAFx3m" title="ComfyUI Selection Toolbox New Features" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

## Wan2.2 Fun Inp Start-End Frame Video Generation Workflow Example

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

This workflow provides two versions:

1. A version using [Wan2.2-Lightning](https://huggingface.co/lightx2v/Wan2.2-Lightning) 4-step LoRA from lightx2v for accelerated video generation
2. A fp8\_scaled version without acceleration LoRA

Below are the test results using an RTX4090D 24GB VRAM GPU at 640640 resolution with 81 frames

| Model Type                | VRAM Usage | First Generation Time | Second Generation Time |
| ------------------------- | ---------- | --------------------- | ---------------------- |
| fp8\_scaled               | 83%        |  524s                |  520s                 |
| fp8\_scaled + 4-step LoRA | 89%        |  138s                |  79s                  |

Since the acceleration with LoRA is significant but the video dynamic is lost,  the provided workflows enable the accelerated LoRA version by default. If you want to enable the other workflow, select it and use **Ctrl+B** to activate.

### 1. Download Workflow File

Please update your ComfyUI to the latest version, and find "**Wan2.2 Fun Inp**" under the menu `Workflow` -> `Browse Templates` -> `Video` to load the workflow.

Or, after updating ComfyUI to the latest version, download the workflow below and drag it into ComfyUI to load.

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/video_wan2_2_14B_animate.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download JSON Workflow</p>
</a>

Use the following materials as the start and end frames

![Wan2.2 Fun Control ComfyUI Workflow Start Frame Material](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/wan2.2_fun_inp/start_image.png)
![Wan2.2 Fun Control ComfyUI Workflow End Frame Material](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/wan2.2_fun_inp/end_image.png)

### 2. Models

**Diffusion Model**

* [wan2.2\_fun\_inpaint\_high\_noise\_14B\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_fun_inpaint_high_noise_14B_fp8_scaled.safetensors)
* [wan2.2\_fun\_inpaint\_low\_noise\_14B\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_fun_inpaint_low_noise_14B_fp8_scaled.safetensors)

**Lightning LoRA (Optional, for acceleration)**

* [wan2.2\_i2v\_lightx2v\_4steps\_lora\_v1\_high\_noise.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/loras/wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors)
* [wan2.2\_i2v\_lightx2v\_4steps\_lora\_v1\_low\_noise.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/loras/wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors)

**VAE**

* [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors)

**Text Encoder**

* [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors)

```
ComfyUI/
 models/
    diffusion_models/
       wan2.2_fun_inpaint_high_noise_14B_fp8_scaled.safetensors
       wan2.2_fun_inpaint_low_noise_14B_fp8_scaled.safetensors
    loras/
       wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors
       wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors
    text_encoders/
       umt5_xxl_fp8_e4m3fn_scaled.safetensors 
    vae/
        wan_2.1_vae.safetensors
```

### 3. Workflow Guide

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_fun_inp.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=3d87de35d3eaa2f9599c35e9963c6c18" alt="Workflow Step Image" data-og-width="4182" width="4182" data-og-height="2048" height="2048" data-path="images/tutorial/video/wan/wan2_2/wan_2.2_14b_fun_inp.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_fun_inp.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=595d1eedd8f8e7dabb57e67b8e08c818 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_fun_inp.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=3b61565f90b622b60617fa5ae68d5ab6 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_fun_inp.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=67aeb504b9f7d07edead943a7e3ef955 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_fun_inp.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=75c98443817fdd4732c5bffee6d7a030 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_fun_inp.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=737315a4d31d70f14414243e982f6a67 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_fun_inp.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=7c038f0c1ba94de7333fd79ebaa03bb2 2500w" />

<Note>
  This workflow uses LoRA. Please make sure the corresponding Diffusion model and LoRA are matched.
</Note>

1. **High noise** model and **LoRA** loading
   * Ensure the `Load Diffusion Model` node loads the `wan2.2_fun_inpaint_high_noise_14B_fp8_scaled.safetensors` model
   * Ensure the `LoraLoaderModelOnly` node loads the `wan2.2_i2v_lightx2v_4steps_lora_v1_high_noise.safetensors`
2. **Low noise** model and **LoRA** loading
   * Ensure the `Load Diffusion Model` node loads the `wan2.2_fun_inpaint_low_noise_14B_fp8_scaled.safetensors` model
   * Ensure the `LoraLoaderModelOnly` node loads the `wan2.2_i2v_lightx2v_4steps_lora_v1_low_noise.safetensors`
3. Ensure the `Load CLIP` node loads the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model
4. Ensure the `Load VAE` node loads the `wan_2.1_vae.safetensors` model
5. Upload the start and end frame images as materials
6. Enter your prompt in the Prompt group
7. Adjust the size and video length in the `WanFunInpaintToVideo` node
   * Adjust the `width` and `height` parameters. The default is `640`. We set a smaller size, but you can modify it as needed.
   * Adjust the `length`, which is the total number of frames. The current workflow fps is 16. For example, if you want to generate a 5-second video, you should set it to 5\*16 = 80.
8. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute video generation


# Wan2.2-S2V Audio-Driven Video Generation ComfyUI Native Workflow Example
Source: https://docs.comfy.org/tutorials/video/wan/wan2-2-s2v

This is a native workflow example for Wan2.2-S2V audio-driven video generation in ComfyUI.

We're excited to announce that Wan2.2-S2V, the advanced audio-driven video generation model, is now natively supported in ComfyUI! This powerful AI model can transform static images and audio inputs into dynamic video content, supporting dialogue, singing, performance, and various creative content needs.

**Model Highlights**

* **Audio-Driven Video Generation**: Transforms static images and audio into synchronized videos
* **Cinematic-Grade Quality**: Generates film-quality videos with natural expressions and movements
* **Minute-Level Generation**: Supports long-form video creation
* **Multi-Format Support**: Works with full-body and half-body characters
* **Enhanced Motion Control**: Generates actions and environments from text instructions

Wan2.2 S2V Code: [GitHub](https://github.com/aigc-apps/VideoX-Fun)
Wan2.2 S2V Model: [Hugging Face](https://huggingface.co/Wan-AI/Wan2.2-S2V-14B)

## Wan2.2 S2V ComfyUI Native Workflow

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

### 1. Download Workflow File

Download the following workflow file and drag it into ComfyUI to load the workflow.

<video controls className="w-full aspect-video" src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/wan2.2_s2v/wan2.2-s2v.mp4" />

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/video_wan2_2_14B_s2v.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download JSON Workflow</p>
</a>

Download the following image and audio as input:
![input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/wan2.2_s2v/input.jpg)

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/wan2.2_s2v/input_audio.MP3" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download Input  Audio</p>
</a>

### 2. Model Links

You can find the models in [our repo](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged)

**diffusion\_models**

* [wan2.2\_s2v\_14B\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_s2v_14B_fp8_scaled.safetensors)
* [wan2.2\_s2v\_14B\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_s2v_14B_bf16.safetensors)

**audio\_encoders**

* [wav2vec2\_large\_english\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/audio_encoders/wav2vec2_large_english_fp16.safetensors)

**vae**

* [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors)

**text\_encoders**

* [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors)

```
ComfyUI/
 models/
    diffusion_models/
       wan2.2_s2v_14B_fp8_scaled.safetensors
       wan2.2_s2v_14B_bf16.safetensors
    text_encoders/
       umt5_xxl_fp8_e4m3fn_scaled.safetensors 
    audio_encoders/ # Create one if you can't find this folder
       wav2vec2_large_english_fp16.safetensors 
    vae/
        wan_2.1_vae.safetensors
```

### 3. Workflow Instructions

<img src="https://mintcdn.com/dripart/ht3vzHrjy1qaRsl9/images/tutorial/video/wan/wan_2.2_14b_s2v.jpg?fit=max&auto=format&n=ht3vzHrjy1qaRsl9&q=85&s=295f87179e12d937cbfbcc3e21d474c0" alt="Workflow Instructions" data-og-width="4000" width="4000" data-og-height="2131" height="2131" data-path="images/tutorial/video/wan/wan_2.2_14b_s2v.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/ht3vzHrjy1qaRsl9/images/tutorial/video/wan/wan_2.2_14b_s2v.jpg?w=280&fit=max&auto=format&n=ht3vzHrjy1qaRsl9&q=85&s=5205bb7cb437730ee400253eabd457d2 280w, https://mintcdn.com/dripart/ht3vzHrjy1qaRsl9/images/tutorial/video/wan/wan_2.2_14b_s2v.jpg?w=560&fit=max&auto=format&n=ht3vzHrjy1qaRsl9&q=85&s=93c52b494e66a77e048c0a4a02839259 560w, https://mintcdn.com/dripart/ht3vzHrjy1qaRsl9/images/tutorial/video/wan/wan_2.2_14b_s2v.jpg?w=840&fit=max&auto=format&n=ht3vzHrjy1qaRsl9&q=85&s=e15f5e98797ff26315e7a8cbeff62231 840w, https://mintcdn.com/dripart/ht3vzHrjy1qaRsl9/images/tutorial/video/wan/wan_2.2_14b_s2v.jpg?w=1100&fit=max&auto=format&n=ht3vzHrjy1qaRsl9&q=85&s=bc70484c0ad625edbf5657a5fa294cb8 1100w, https://mintcdn.com/dripart/ht3vzHrjy1qaRsl9/images/tutorial/video/wan/wan_2.2_14b_s2v.jpg?w=1650&fit=max&auto=format&n=ht3vzHrjy1qaRsl9&q=85&s=79fd15e0520faac165db2b7763df58c2 1650w, https://mintcdn.com/dripart/ht3vzHrjy1qaRsl9/images/tutorial/video/wan/wan_2.2_14b_s2v.jpg?w=2500&fit=max&auto=format&n=ht3vzHrjy1qaRsl9&q=85&s=6ccabebbab0bc20c0c0fc415ba3ab9aa 2500w" />

#### 3.1 About Lightning LoRA

#### 3.2 About fp8\_scaled and bf16 Models

You can find both models [here](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/tree/main/split_files/diffusion_models):

* [wan2.2\_s2v\_14B\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_s2v_14B_fp8_scaled.safetensors)
* [wan2.2\_s2v\_14B\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_s2v_14B_bf16.safetensors)

This template uses `wan2.2_s2v_14B_fp8_scaled.safetensors`, which requires less VRAM. But you can try `wan2.2_s2v_14B_bf16.safetensors` to reduce quality degradation.

#### 3.3 Step-by-Step Operation Instructions

**Step 1: Load Models**

1. **Load Diffusion Model**: Load `wan2.2_s2v_14B_fp8_scaled.safetensors` or `wan2.2_s2v_14B_bf16.safetensors`
   * The provided workflow uses `wan2.2_s2v_14B_fp8_scaled.safetensors`, which requires less VRAM
   * But you can try `wan2.2_s2v_14B_bf16.safetensors` to reduce quality degradation

2. **Load CLIP**: Load `umt5_xxl_fp8_e4m3fn_scaled.safetensors`

3. **Load VAE**: Load `wan_2.1_vae.safetensors`

4. **AudioEncoderLoader**: Load `wav2vec2_large_english_fp16.safetensors`

5. **LoraLoaderModelOnly**: Load `wan2.2_t2v_lightx2v_4steps_lora_v1.1_high_noise.safetensors` (Lightning LoRA)
   * We tested all wan2.2 lightning LoRAs. Since this is not a LoRA specifically trained for Wan2.2 S2V, many key values don't match, but we added it because it significantly reduces generation time. We will continue to optimize this template
   * Using it will cause significant dynamic and quality loss
   * If you find the output quality too poor, you can try the original 20-step workflow

6. **LoadAudio**: Upload our provided audio file or your own audio

7. **Load Image**: Upload reference image

8. **Batch sizes**: Set according to the number of Video S2V Extend subgraph nodes you add
   * Each Video S2V Extend subgraph adds 77 frames to the final output
   * For example: If you added 2 Video S2V Extend subgraphs, the batch size should be 3, which means the total number of sampling iterations
   * **Chunk Length**: Keep the default value of 77

9. **Sampler Settings**: Choose different settings based on whether you use Lightning LoRA
   * With 4-step Lightning LoRA: steps: 4, cfg: 1.0
   * Without 4-step Lightning LoRA: steps: 20, cfg: 6.0

10. **Size Settings**: Set the output video dimensions

11. **Video S2V Extend**: Video extension subgraph nodes. Since our default frames per sampling is 77, and this is a 16fps model, each extension will generate 77 / 16 = 4.8125 seconds of video
    * You need some calculation to match the number of video extension subgraph nodes with the input audio length. For example: If input audio is 14s, the total frames needed are 14x16=224, each video extension is 77 frames, so you need 224/77 = 2.9, rounded up to 3 video extension subgraph nodes

12. Use Ctrl-Enter or click the Run button to execute the workflow


# Wan2.2 Video Generation ComfyUI Official Native Workflow Example
Source: https://docs.comfy.org/tutorials/video/wan/wan2_2

Official usage guide for Alibaba Cloud Tongyi Wanxiang 2.2 video generation model in ComfyUI

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/S45XQXFOutM?si=Qvfco7Cyr_3akZ3Hv" title="ComfyUI Selection Toolbox New Features" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

Wan 2.2 is a new generation multimodal generative model launched by WAN AI. This model adopts an innovative MoE (Mixture of Experts) architecture, consisting of high-noise and low-noise expert models. It can divide expert models according to denoising timesteps, thus generating higher quality video content.

Wan 2.2 has three core features: cinematic-level aesthetic control, deeply integrating professional film industry aesthetic standards, supporting multi-dimensional visual control such as lighting, color, and composition; large-scale complex motion, easily restoring various complex motions and enhancing the smoothness and controllability of motion; precise semantic compliance, excelling in complex scenes and multi-object generation, better restoring users' creative intentions.
The model supports multiple generation modes such as text-to-video and image-to-video, suitable for content creation, artistic creation, education and training, and other application scenarios.

[Wan2.2 Prompt Guide](https://alidocs.dingtalk.com/i/nodes/EpGBa2Lm8aZxe5myC99MelA2WgN7R35y)

## Model Highlights

* **Cinematic-level Aesthetic Control**: Professional camera language, supports multi-dimensional visual control such as lighting, color, and composition
* **Large-scale Complex Motion**: Smoothly restores various complex motions, enhances motion controllability and naturalness
* **Precise Semantic Compliance**: Complex scene understanding, multi-object generation, better restoring creative intentions
* **Efficient Compression Technology**: 5B version with high compression ratio VAE, memory optimization, supports mixed training

## Wan2.2 Open Source Model Versions

The Wan2.2 series models are based on the Apache 2.0 open source license and support commercial use. The Apache 2.0 license allows you to freely use, modify, and distribute these models, including for commercial purposes, as long as you retain the original copyright notice and license text.

| Model Type     | Model Name      | Parameters | Main Function                                                                                                                | Model Repository                                                    |
| -------------- | --------------- | ---------- | ---------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------- |
| Hybrid Model   | Wan2.2-TI2V-5B  | 5B         | Hybrid version supporting both text-to-video and image-to-video, a single model meets two core task requirements             |  [Wan2.2-TI2V-5B](https://huggingface.co/Wan-AI/Wan2.2-TI2V-5B)   |
| Image-to-Video | Wan2.2-I2V-A14B | 14B        | Converts static images into dynamic videos, maintaining content consistency and smooth dynamic process                       |  [Wan2.2-I2V-A14B](https://huggingface.co/Wan-AI/Wan2.2-I2V-A14B) |
| Text-to-Video  | Wan2.2-T2V-A14B | 14B        | Generates high-quality videos from text descriptions, with cinematic-level aesthetic control and precise semantic compliance |  [Wan2.2-T2V-A14B](https://huggingface.co/Wan-AI/Wan2.2-T2V-A14B) |

## ComfyOrg Wan2.2 Live Streams

For ComfyUI Wan2.2 usage, we have conducted live streams, which you can view to learn how to use them.

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/Z0yo16LzReA?si=I-BlUfktxqt9URQk" title="ComfyUI Wan2.2 Live Streams" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/z62QLQ3XqSA?si=yUenvPa9Q4-VX28M" title="ComfyUI Wan2.2 Deep Dive" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

<iframe className="w-full aspect-video rounded-xl" src="https://www.youtube.com/embed/0fyZhXga8P8?si=PMv9xQLP32wP8Ni9" title="ComfyUI Wan2.2 Deep Dive #2" allow="accelerometer; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowFullScreen />

This tutorial will use the [ Comfy-Org/Wan\_2.2\_ComfyUI\_Repackaged](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged) version.

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/template.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=ccf5d56775dac715ac40f767978d4371" alt="Wan2.2 template" data-og-width="3450" width="3450" data-og-height="1944" height="1944" data-path="images/tutorial/video/wan/wan2_2/template.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/template.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=3a370d08de0764cc851c0d08de9183a3 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/template.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=a8eb6733fb6a6bfabafe1722bf428f9c 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/template.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=974dda9a8a36e0e56e489d60c8252c1e 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/template.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=d1ecdc72c324c21a14e3c75260dc6636 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/template.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=e9298fea453b141f231b98eacda03c8a 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/template.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=dae5c8f09553dc9f5e2895c24b4200cf 2500w" />

## Wan2.2 TI2V 5B Hybrid Version Workflow Example

<Tip>
  The Wan2.2 5B version should fit well on 8GB vram with the ComfyUI native offloading.
</Tip>

### 1. Download Workflow File

Please update your ComfyUI to the latest version, and through the menu `Workflow` -> `Browse Templates` -> `Video`, find "Wan2.2 5B video generation" to load the workflow.

<video controls className="w-full aspect-video" src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/2.2/wan_2_2_5B_t2v.mp4" />

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/video_wan2_2_5B_ti2v.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download JSON Workflow File</p>
</a>

### 2. Manually Download Models

**Diffusion Model**

* [wan2.2\_ti2v\_5B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_ti2v_5B_fp16.safetensors)

**VAE**

* [wan2.2\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/vae/wan2.2_vae.safetensors)

**Text Encoder**

* [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors)

```
ComfyUI/
 models/
    diffusion_models/
      wan2.2_ti2v_5B_fp16.safetensors
    text_encoders/
       umt5_xxl_fp8_e4m3fn_scaled.safetensors 
    vae/
        wan2.2_vae.safetensors
```

### 3. Follow the Steps

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_5b_t2v.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=964289dd7f3c51119c95f98372dbd209" alt="Step Diagram" data-og-width="4182" width="4182" data-og-height="2027" height="2027" data-path="images/tutorial/video/wan/wan2_2/wan_2.2_5b_t2v.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_5b_t2v.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=610a2a4f3a050187e58ce0fd61b619be 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_5b_t2v.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=81da2a077caa2ab0fac0a5b7947711dc 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_5b_t2v.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=294f3d3cb6eb69d1e145b5678c0294ad 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_5b_t2v.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=3dbca42c849ead6457dad35ce4b640de 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_5b_t2v.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=9e8b8ebbe5cb9458449ea202ef0bb518 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_5b_t2v.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=bff729d4a7943a3877a619933770601e 2500w" />

1. Ensure the `Load Diffusion Model` node loads the `wan2.2_ti2v_5B_fp16.safetensors` model.
2. Ensure the `Load CLIP` node loads the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model.
3. Ensure the `Load VAE` node loads the `wan2.2_vae.safetensors` model.
4. (Optional) If you need to perform image-to-video generation, you can use the shortcut Ctrl+B to enable the `Load image` node to upload an image.
5. (Optional) In the `Wan22ImageToVideoLatent` node, you can adjust the size settings and the total number of video frames (`length`).
6. (Optional) If you need to modify the prompts (positive and negative), please do so in the `CLIP Text Encoder` node at step 5.
7. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute video generation.

## Wan2.2 14B T2V Text-to-Video Workflow Example

### 1. Workflow File

Please update your ComfyUI to the latest version, and through the menu `Workflow` -> `Browse Templates` -> `Video`, find "Wan2.2 14B T2V" to load the workflow.

Or update your ComfyUI to the latest version, then download the following video and drag it into ComfyUI to load the workflow.

<video controls className="w-full aspect-video" src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/2.2/wan_2_2_14B_t2v.mp4" />

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/video_wan2_2_14B_t2v.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download JSON Workflow File</p>
</a>

### 2. Manually Download Models

**Diffusion Model**

* [wan2.2\_t2v\_high\_noise\_14B\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_t2v_high_noise_14B_fp8_scaled.safetensors)
* [wan2.2\_t2v\_low\_noise\_14B\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_t2v_low_noise_14B_fp8_scaled.safetensors)

**VAE**

* [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors)

**Text Encoder**

* [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors)

```
ComfyUI/
 models/
    diffusion_models/
       wan2.2_t2v_low_noise_14B_fp8_scaled.safetensors
       wan2.2_t2v_high_noise_14B_fp8_scaled.safetensors
    text_encoders/
       umt5_xxl_fp8_e4m3fn_scaled.safetensors 
    vae/
        wan_2.1_vae.safetensors
```

### 3. Follow the Steps

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_t2v.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=0ce44ad0e8f5e8dff6c9324404ee5e46" alt="Step Diagram" data-og-width="4182" width="4182" data-og-height="2255" height="2255" data-path="images/tutorial/video/wan/wan2_2/wan_2.2_14b_t2v.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_t2v.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=36116adfbdba4e7bcb67ad6c9e613387 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_t2v.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=3070fc1eb3736451054f1f1336c7b1d6 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_t2v.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=9acc8bdb71542b8a4c940eeede74dc68 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_t2v.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=2255e183d00f8dbfa3878add257c1436 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_t2v.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=fc533adad23782d442693837f3b88b65 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_t2v.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=7c240ef2b8046c79384a72e4fb1dd6ef 2500w" />

1. Ensure the first `Load Diffusion Model` node loads the `wan2.2_t2v_high_noise_14B_fp8_scaled.safetensors` model.
2. Ensure the second `Load Diffusion Model` node loads the `wan2.2_t2v_low_noise_14B_fp8_scaled.safetensors` model.
3. Ensure the `Load CLIP` node loads the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model.
4. Ensure the `Load VAE` node loads the `wan_2.1_vae.safetensors` model.
5. (Optional) In the `EmptyHunyuanLatentVideo` node, you can adjust the size settings and the total number of video frames (`length`).
6. (Optional) If you need to modify the prompts (positive and negative), please do so in the `CLIP Text Encoder` node at step 5.
7. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute video generation.

## Wan2.2 14B I2V Image-to-Video Workflow Example

### 1. Workflow File

Please update your ComfyUI to the latest version, and through the menu `Workflow` -> `Browse Templates` -> `Video`, find "Wan2.2 14B I2V" to load the workflow.

Or update your ComfyUI to the latest version, then download the following video and drag it into ComfyUI to load the workflow.

<video controls className="w-full aspect-video" src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/2.2/wan_2_2_14B_i2v.mp4" />

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/video_wan2_2_14B_i2v.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download JSON Workflow File</p>
</a>

You can use the following image as input:
![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/2.2/input.jpg)

### 2. Manually Download Models

**Diffusion Model**

* [wan2.2\_i2v\_high\_noise\_14B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_i2v_high_noise_14B_fp16.safetensors)
* [wan2.2\_i2v\_low\_noise\_14B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/diffusion_models/wan2.2_i2v_low_noise_14B_fp16.safetensors)

**VAE**

* [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.2_ComfyUI_Repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors)

**Text Encoder**

* [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors)

```
ComfyUI/
 models/
    diffusion_models/
       wan2.2_i2v_low_noise_14B_fp16.safetensors
       wan2.2_i2v_high_noise_14B_fp16.safetensors
    text_encoders/
       umt5_xxl_fp8_e4m3fn_scaled.safetensors 
    vae/
        wan_2.1_vae.safetensors
```

### 3. Follow the Steps

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_i2v.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=a9c4c6cfa365f7a0e4d7274b6c799316" alt="Step Diagram" data-og-width="4182" width="4182" data-og-height="2336" height="2336" data-path="images/tutorial/video/wan/wan2_2/wan_2.2_14b_i2v.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_i2v.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=73611770041093ac8431217ffd97ee54 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_i2v.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=96b2ee5717f5a981d9d1143880d7c5fa 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_i2v.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=65141a2c12393339961080a4d94dbb11 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_i2v.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=a326793c8feabd5de08c85f6d828269f 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_i2v.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=3cc839df08cf7f06bc745de92236981d 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_i2v.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=9b3c4bf4352fb753e9878098452bfb39 2500w" />

1. Make sure the first `Load Diffusion Model` node loads the `wan2.2_t2v_high_noise_14B_fp8_scaled.safetensors` model.
2. Make sure the second `Load Diffusion Model` node loads the `wan2.2_t2v_low_noise_14B_fp8_scaled.safetensors` model.
3. Make sure the `Load CLIP` node loads the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model.
4. Make sure the `Load VAE` node loads the `wan_2.1_vae.safetensors` model.
5. In the `Load Image` node, upload the image to be used as the initial frame.
6. If you need to modify the prompts (positive and negative), do so in the `CLIP Text Encoder` node at step 6.
7. (Optional) In `EmptyHunyuanLatentVideo`, you can adjust the size settings and the total number of video frames (`length`).
8. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute video generation.

## Wan2.2 14B FLF2V Workflow Example

The first and last frame workflow uses the same model locations as the I2V section.

### 1. Workflow and Input Material Preparation

Download the video or the JSON workflow below and open it in ComfyUI.

<video controls className="w-full aspect-video" src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/2.2/wan22_14B_flf2v.mp4" />

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/video_wan2_2_14B_flf2v.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download JSON Workflow</p>
</a>

Download the following images as input materials:

![Input Material](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/2.2/wan22_14B_flf2v_start_image.png)
![Input Material](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/video/wan/2.2/wan22_14B_flf2v_end_image.png)

### 2. Follow the Steps

<img src="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_flf2v.jpg?fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=6def22f3de024219e66dd4ea7cae7c03" alt="Step Diagram" data-og-width="2091" width="2091" data-og-height="1540" height="1540" data-path="images/tutorial/video/wan/wan2_2/wan_2.2_14b_flf2v.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_flf2v.jpg?w=280&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=c49a70cd83704dc313144a224d8b384e 280w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_flf2v.jpg?w=560&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=f387777923416ff75fda5318b3dea01f 560w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_flf2v.jpg?w=840&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=41dc482b545e11b4c666b38b03239b8d 840w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_flf2v.jpg?w=1100&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=789e93dc113653a909d3f01799b50430 1100w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_flf2v.jpg?w=1650&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=c1844f0c2b70aacf3262350ab824f4af 1650w, https://mintcdn.com/dripart/SIDaLac8vBogzwm7/images/tutorial/video/wan/wan2_2/wan_2.2_14b_flf2v.jpg?w=2500&fit=max&auto=format&n=SIDaLac8vBogzwm7&q=85&s=ae6a4485c77f8a75a12983129ebe5251 2500w" />

1. Upload the image to be used as the starting frame in the first `Load Image` node.
2. Upload the image to be used as the ending frame in the second `Load Image` node.
3. Adjust the size settings in the `WanFirstLastFrameToVideo` node.
   * By default, a relatively small size is set to prevent low VRAM users from consuming too many resources.
   * If you have enough VRAM, you can try a resolution around 720P.
4. Write appropriate prompts according to your first and last frames.
5. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute video generation.

## Community Resources

### GGUF Versions

* [bullerwins/Wan2.2-I2V-A14B-GGUF/](https://huggingface.co/bullerwins/Wan2.2-I2V-A14B-GGUF/)
* [bullerwins/Wan2.2-T2V-A14B-GGUF](https://huggingface.co/bullerwins/Wan2.2-T2V-A14B-GGUF)
* [QuantStack/Wan2.2 GGUFs](https://huggingface.co/collections/QuantStack/wan22-ggufs-6887ec891bdea453a35b95f3)

**Custom Node**
[City96/ComfyUI-GGUF](https://github.com/city96/ComfyUI-GGUF)

### WanVideoWrapper

[Kijai/ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper)

**Wan2.2 models**
[Kijai/WanVideo\_comfy\_fp8\_scaled](https://hf-mirror.com/Kijai/WanVideo_comfy_fp8_scaled)

**Wan2.1 models**
[Kijai/WanVideo\_comfy/Lightx2v](https://huggingface.co/Kijai/WanVideo_comfy/tree/main/Lightx2v)

**Lightx2v 4steps LoRA**

* [Wan2.2-T2V-A14B-4steps-lora-rank64-V1](https://huggingface.co/lightx2v/Wan2.2-Lightning/tree/main/Wan2.2-T2V-A14B-4steps-lora-rank64-V1)


# BasicScheduler - ComfyUI Built-in Node Documentation
Source: https://docs.comfy.org/built-in-nodes/BasicScheduler

The BasicScheduler node is used to compute a sequence of sigma values for diffusion models based on the provided scheduler, model, and denoising parameters.

The `BasicScheduler` node is designed to compute a sequence of sigma values for diffusion models based on the provided scheduler, model, and denoising parameters. It dynamically adjusts the total number of steps based on the denoise factor to fine-tune the diffusion process, providing precise "recipes" for different stages in advanced sampling processes that require fine control (such as multi-stage sampling).

## Inputs

| Parameter   | Data Type      | Input Type | Default | Range     | Metaphor Description                                                      | Technical Purpose                                          |
| ----------- | -------------- | ---------- | ------- | --------- | ------------------------------------------------------------------------- | ---------------------------------------------------------- |
| `model`     | MODEL          | Input      | -       | -         | **Canvas Type**: Different canvas materials need different paint formulas | Diffusion model object, determines sigma calculation basis |
| `scheduler` | COMBO\[STRING] | Widget     | -       | 9 options | **Mixing Technique**: Choose how paint concentration changes              | Scheduling algorithm, controls noise decay mode            |
| `steps`     | INT            | Widget     | 20      | 1-10000   | **Mixing Count**: 20 mixes vs 50 mixes precision difference               | Sampling steps, affects generation quality and speed       |
| `denoise`   | FLOAT          | Widget     | 1.0     | 0.0-1.0   | **Creation Intensity**: Control level from fine-tuning to repainting      | Denoising strength, supports partial repainting scenarios  |

### Scheduler Types

Based on source code `comfy.samplers.SCHEDULER_NAMES`, supports the following 9 schedulers:

| Scheduler Name        | Characteristics   | Use Cases                     | Noise Decay Pattern           |
| --------------------- | ----------------- | ----------------------------- | ----------------------------- |
| **normal**            | Standard linear   | General scenarios, balanced   | Uniform decay                 |
| **karras**            | Smooth transition | High quality, detail-rich     | Smooth non-linear decay       |
| **exponential**       | Exponential decay | Fast generation, efficiency   | Exponential rapid decay       |
| **sgm\_uniform**      | SGM uniform       | Specific model optimization   | SGM optimized decay           |
| **simple**            | Simple scheduling | Quick testing, basic use      | Simplified decay              |
| **ddim\_uniform**     | DDIM uniform      | DDIM sampling optimization    | DDIM specific decay           |
| **beta**              | Beta distribution | Special distribution needs    | Beta function decay           |
| **linear\_quadratic** | Linear quadratic  | Complex scenario optimization | Quadratic function decay      |
| **kl\_optimal**       | KL optimal        | Theoretical optimization      | KL divergence optimized decay |

## Outputs

| Parameter | Data Type | Output Type | Metaphor Description                                                           | Technical Meaning                                              |
| --------- | --------- | ----------- | ------------------------------------------------------------------------------ | -------------------------------------------------------------- |
| `sigmas`  | SIGMAS    | Output      | **Paint Recipe Chart**: Detailed paint concentration list for step-by-step use | Noise level sequence, guides diffusion model denoising process |

## Node Role: Artist's Color Mixing Assistant

Imagine you are an artist creating a clear image from a chaotic mixture of paint (noise). `BasicScheduler` acts like your **professional color mixing assistant**, whose job is to prepare a series of precise paint concentration recipes:

### Workflow

* **Step 1**: Use 90% concentration paint (high noise level)
* **Step 2**: Use 80% concentration paint
* **Step 3**: Use 70% concentration paint
* **...**
* **Final Step**: Use 0% concentration (clean canvas, no noise)

### Color Assistant's Special Skills

**Different mixing methods (scheduler)**:

* **"karras" mixing method**: Paint concentration changes very smoothly, like professional artist's gradient technique
* **"exponential" mixing method**: Paint concentration decreases rapidly, suitable for quick creation
* **"linear" mixing method**: Paint concentration decreases uniformly, stable and controllable

**Fine control (steps)**:

* **20 mixes**: Quick painting, efficiency priority
* **50 mixes**: Fine painting, quality priority

**Creation intensity (denoise)**:

* **1.0 = Complete new creation**: Start completely from blank canvas
* **0.5 = Half transformation**: Keep half of original painting, transform half
* **0.2 = Fine adjustment**: Only make subtle adjustments to original painting

### Collaboration with Other Nodes

`BasicScheduler` (Color Assistant)  Prepare Recipe  `SamplerCustom` (Artist)  Actual Painting  Completed Work


# Canny - ComfyUI Built-in Node Documentation
Source: https://docs.comfy.org/built-in-nodes/Canny

The Canny node used to extract edge lines from photos.

Extract all edge lines from photos, like using a pen to outline a photo, drawing out the contours and detail boundaries of objects.

## Working Principle

Imagine you are an artist who needs to use a pen to outline a photo. The Canny node acts like an intelligent assistant, helping you decide where to draw lines (edges) and where not to.

This process is like a screening job:

* **High threshold** is the "must draw line standard": only very obvious and clear contour lines will be drawn, such as facial contours of people and building frames
* **Low threshold** is the "definitely don't draw line standard": edges that are too weak will be ignored to avoid drawing noise and meaningless lines
* **Middle area**: edges between the two standards will be drawn together if they connect to "must draw lines", but won't be drawn if they are isolated

The final output is a black and white image, where white parts are detected edge lines and black parts are areas without edges.

## Inputs

| Parameter Name   | Data Type | Input Type | Default | Range     | Function Description                                                                                            |
| ---------------- | --------- | ---------- | ------- | --------- | --------------------------------------------------------------------------------------------------------------- |
| `image`          | IMAGE     | Input      | -       | -         | Original photo that needs edge extraction                                                                       |
| `low_threshold`  | FLOAT     | Widget     | 0.4     | 0.01-0.99 | Low threshold, determines how weak edges to ignore. Lower values preserve more details but may produce noise    |
| `high_threshold` | FLOAT     | Widget     | 0.8     | 0.01-0.99 | High threshold, determines how strong edges to preserve. Higher values only keep the most obvious contour lines |

## Outputs

| Output Name | Data Type | Description                                                                                     |
| ----------- | --------- | ----------------------------------------------------------------------------------------------- |
| `image`     | IMAGE     | Black and white edge image, white lines are detected edges, black areas are parts without edges |

## Parameter Comparison

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/canny/input.webp?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=e19fdaa3bd2abafb185fb663b1e3d2ed" alt="Original Image" data-og-width="716" width="716" data-og-height="716" height="716" data-path="images/built-in-nodes/canny/input.webp" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/canny/input.webp?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=824b6c04bf12f0178b9e6df714395391 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/canny/input.webp?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=6042657960eb1ec2bb202ed95eb67459 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/canny/input.webp?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=ca28e7b476c36587135b2ea4e1be2561 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/canny/input.webp?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=5be0ade4e451d2ba917c170d63d8f867 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/canny/input.webp?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=55ca7259b256dc41949fd5b35bb340eb 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/canny/input.webp?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=c4b1dcf33487963a29e728f91bb66082 2500w" />

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/canny/compare.webp?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=9992b55c628d860dbf1f676687e638ce" alt="Parameter Comparison" data-og-width="1039" width="1039" data-og-height="1331" height="1331" data-path="images/built-in-nodes/canny/compare.webp" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/canny/compare.webp?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=40ce24f56d9bb8f4a0a9487f3472cf96 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/canny/compare.webp?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=224588df1839ae86519deed88b555dc6 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/canny/compare.webp?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=dd4e3a6d33e18024452f1273acd2de1d 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/canny/compare.webp?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=46fe8ded5d1d7095fdb1b01a24843df8 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/canny/compare.webp?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=313a9b558bdbfebc2b20abfb4122450f 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/canny/compare.webp?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=4329cbfbdc01e671a82dfb6e7c69d6f6 2500w" />

**Common Issues:**

* Broken edges: Try lowering high threshold
* Too much noise: Raise low threshold
* Missing important details: Lower low threshold
* Edges too rough: Check input image quality and resolution


# CheckpointLoaderSimple - ComfyUI Built-in Node Documentation
Source: https://docs.comfy.org/built-in-nodes/CheckpointLoaderSimple

The CheckpointLoaderSimple node is used to load model files from specified locations and decompose them into three core components: the main model, text encoder, and image encoder/decoder.

This is a model loader node that loads model files from specified locations and decomposes them into three core components: the main model, text encoder, and image encoder/decoder.

This node automatically detects all model files in the `ComfyUI/models/checkpoints` folder, as well as additional paths configured in your `extra_model_paths.yaml` file.

1. **Model Compatibility**: Ensure the selected model is compatible with your workflow. Different model types (such as SD1.5, SDXL, Flux, etc.) need to be paired with corresponding samplers and other nodes
2. **File Management**: Place model files in the `ComfyUI/models/checkpoints` folder, or configure other paths through extra\_model\_paths.yaml
3. **Interface Refresh**: If new model files are added while ComfyUI is running, you need to refresh the browser (Ctrl+R) to see the new files in the dropdown list

## Inputs

| Parameter   | Data Type | Input Type | Default | Range                                 | Description                                                                                                       |
| ----------- | --------- | ---------- | ------- | ------------------------------------- | ----------------------------------------------------------------------------------------------------------------- |
| `ckpt_name` | STRING    | Widget     | null    | All model files in checkpoints folder | Select the checkpoint model file name to load, which determines the AI model used for subsequent image generation |

## Outputs

| Output Name | Data Type | Description                                                                                                     |
| ----------- | --------- | --------------------------------------------------------------------------------------------------------------- |
| `MODEL`     | MODEL     | The main diffusion model used for image denoising generation, the core component of AI image creation           |
| `CLIP`      | CLIP      | The model used for encoding text prompts, converting text descriptions into information that AI can understand  |
| `VAE`       | VAE       | The model used for image encoding and decoding, responsible for converting between pixel space and latent space |


# ClipLoader - ComfyUI Built-in Node Documentation
Source: https://docs.comfy.org/built-in-nodes/ClipLoader

The ClipLoader node is used to load CLIP text encoder models independently.

This node is primarily used for loading CLIP text encoder models independently.
The model files can be detected in the following paths:

* "ComfyUI/models/text\_encoders/"
* "ComfyUI/models/clip/"

> If you save a model after ComfyUI has started, you'll need to refresh the ComfyUI frontend to get the latest model file path list

Supported model formats:

* `.ckpt`
* `.pt`
* `.pt2`
* `.bin`
* `.pth`
* `.safetensors`
* `.pkl`
* `.sft`

For more details on the latest model file loading, please refer to [folder\_paths](https://github.com/comfyanonymous/ComfyUI/blob/master/folder_paths.py)

## Inputs

| Parameter   | Data Type      | Description                                                                                                                                                                                                                                        |
| ----------- | -------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `clip_name` | COMBO\[STRING] | Specifies the name of the CLIP model to be loaded. This name is used to locate the model file within a predefined directory structure.                                                                                                             |
| `type`      | COMBO\[STRING] | Determines the type of CLIP model to load. As ComfyUI supports more models, new types will be added here. Please check the `CLIPLoader` class definition in [node.py](https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py) for details. |
| `device`    | COMBO\[STRING] | Choose the device for loading the CLIP model. `default` will run the model on GPU, while selecting `CPU` will force loading on CPU.                                                                                                                |

### Device Options Explained

**When to choose "default":**

* Have sufficient GPU memory
* Want the best performance
* Let the system optimize memory usage automatically

**When to choose "cpu":**

* Insufficient GPU memory
* Need to reserve GPU memory for other models (like UNet)
* Running in a low VRAM environment
* Debugging or special purpose needs

**Performance Impact**

Running on CPU will be much slower than GPU, but it can save valuable GPU memory for other more important model components. In memory-constrained environments, putting the CLIP model on CPU is a common optimization strategy.

### Supported Combinations

| Model Type        | Corresponding Encoder   |
| ----------------- | ----------------------- |
| stable\_diffusion | clip-l                  |
| stable\_cascade   | clip-g                  |
| sd3               | t5 xxl/ clip-g / clip-l |
| stable\_audio     | t5 base                 |
| mochi             | t5 xxl                  |
| cosmos            | old t5 xxl              |
| lumina2           | gemma 2 2B              |
| wan               | umt5 xxl                |

As ComfyUI updates, these combinations may expand. For details, please refer to the `CLIPLoader` class definition in [node.py](https://github.com/comfyanonymous/ComfyUI/blob/master/nodes.py)

## Outputs

| Parameter | Data Type | Description                                                                     |
| --------- | --------- | ------------------------------------------------------------------------------- |
| `clip`    | CLIP      | The loaded CLIP model, ready for use in downstream tasks or further processing. |

## Additional Notes

CLIP models play a core role as text encoders in ComfyUI, responsible for converting text prompts into numerical representations that diffusion models can understand. You can think of them as translators, responsible for translating your text into a language that large models can understand. Of course, different models have their own "dialects," so different CLIP encoders are needed between different architectures to complete the text encoding process.


# ClipMergeSimple - ComfyUI Built-in Node Documentation
Source: https://docs.comfy.org/built-in-nodes/ClipMergeSimple

The ClipMergeSimple node is used to combine two CLIP text encoder models based on a specified ratio.

`CLIPMergeSimple` is an advanced model merging node used to combine two CLIP text encoder models based on a specified ratio.

This node specializes in merging two CLIP models based on a specified ratio, effectively blending their characteristics. It selectively applies patches from one model to another, excluding specific components like position IDs and logit scale, to create a hybrid model that combines features from both source models.

## Inputs

| Parameter | Data Type | Description                                                                                                                                                                                                                       |
| --------- | --------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `clip1`   | CLIP      | The first CLIP model to be merged. It serves as the base model for the merging process.                                                                                                                                           |
| `clip2`   | CLIP      | The second CLIP model to be merged. Its key patches, except for position IDs and logit scale, are applied to the first model based on the specified ratio.                                                                        |
| `ratio`   | FLOAT     | Range `0.0 - 1.0`, determines the proportion of features from the second model to blend into the first model. A ratio of 1.0 means fully adopting the second model's features, while 0.0 retains only the first model's features. |

## Outputs

| Parameter | Data Type | Description                                                                                                      |
| --------- | --------- | ---------------------------------------------------------------------------------------------------------------- |
| `clip`    | CLIP      | The resulting merged CLIP model, incorporating features from both input models according to the specified ratio. |

## Merging Mechanism Explained

### Merging Algorithm

The node uses weighted averaging to merge the two models:

1. **Clone Base Model**: First clones `clip1` as the base model
2. **Get Patches**: Obtains all key patches from `clip2`
3. **Filter Special Keys**: Skips keys ending with `.position_ids` and `.logit_scale`
4. **Apply Weighted Merge**: Uses the formula `(1.0 - ratio) * clip1 + ratio * clip2`

### Ratio Parameter Explained

* **ratio = 0.0**: Fully uses clip1, ignores clip2
* **ratio = 0.5**: 50% contribution from each model
* **ratio = 1.0**: Fully uses clip2, ignores clip1

## Use Cases

1. **Model Style Fusion**: Combine characteristics of CLIP models trained on different data
2. **Performance Optimization**: Balance strengths and weaknesses of different models
3. **Experimental Research**: Explore combinations of different CLIP encoders


# ClipSave - ComfyUI Built-in Node Documentation
Source: https://docs.comfy.org/built-in-nodes/ClipSave

The ClipSave node is used to save CLIP text encoder models in SafeTensors format.

The `CLIPSave` node is designed for saving CLIP text encoder models in SafeTensors format. This node is part of advanced model merging workflows and is typically used in conjunction with nodes like `CLIPMergeSimple` and `CLIPMergeAdd`. The saved files use the SafeTensors format to ensure security and compatibility.

## Inputs

| Parameter        | Data Type      | Required | Default Value  | Description                                |
| ---------------- | -------------- | -------- | -------------- | ------------------------------------------ |
| clip             | CLIP           | Yes      | -              | The CLIP model to be saved                 |
| filename\_prefix | STRING         | Yes      | "clip/ComfyUI" | The prefix path for the saved file         |
| prompt           | PROMPT         | Hidden   | -              | Workflow prompt information (for metadata) |
| extra\_pnginfo   | EXTRA\_PNGINFO | Hidden   | -              | Additional PNG information (for metadata)  |

## Outputs

This node has no defined output types. It saves the processed files to the `ComfyUI/output/` folder.

### Multi-file Saving Strategy

The node saves different components based on the CLIP model type:

| Prefix Type  | File Suffix | Description           |
| ------------ | ----------- | --------------------- |
| `clip_l.`    | `_clip_l`   | CLIP-L text encoder   |
| `clip_g.`    | `_clip_g`   | CLIP-G text encoder   |
| Empty prefix | No suffix   | Other CLIP components |

## Usage Notes

1. **File Location**: All files are saved in the `ComfyUI/output/` directory
2. **File Format**: Models are saved in SafeTensors format for security
3. **Metadata**: Includes workflow information and PNG metadata if available
4. **Naming Convention**: Uses the specified prefix plus appropriate suffixes based on model type


# ClipSetLastLayer - ComfyUI Built-in Node Documentation
Source: https://docs.comfy.org/built-in-nodes/ClipSetLastLayer

The ClipSetLastLayer node is used to control the processing depth of CLIP models.

`CLIP Set Last Layer` is a core node in ComfyUI for controlling the processing depth of CLIP models. It allows users to precisely control where the CLIP text encoder stops processing, affecting both the depth of text understanding and the style of generated images.

Imagine the CLIP model as a 24-layer intelligent brain:

* Shallow layers (1-8): Recognize basic letters and words
* Middle layers (9-16): Understand grammar and sentence structure
* Deep layers (17-24): Grasp abstract concepts and complex semantics

`CLIP Set Last Layer` works like a **"thinking depth controller"**:

-1: Use all 24 layers (complete understanding)
-2: Stop at layer 23 (slightly simplified)
-12: Stop at layer 13 (medium understanding)
-24: Use only layer 1 (basic understanding)

## Inputs

| Parameter            | Data Type | Default | Range     | Description                                                                         |
| -------------------- | --------- | ------- | --------- | ----------------------------------------------------------------------------------- |
| `clip`               | CLIP      | -       | -         | The CLIP model to be modified                                                       |
| `stop_at_clip_layer` | INT       | -1      | -24 to -1 | Specifies which layer to stop at, -1 uses all layers, -24 uses only the first layer |

## Outputs

| Output Name | Data Type | Description                                                          |
| ----------- | --------- | -------------------------------------------------------------------- |
| clip        | CLIP      | The modified CLIP model with the specified layer set as the last one |

## Why Set the Last Layer

* **Performance Optimization**: Like not needing a PhD to understand simple sentences, sometimes shallow understanding is enough and faster
* **Style Control**: Different levels of understanding produce different artistic styles
* **Compatibility**: Some models might perform better at specific layers


# ClipTextEncode - ComfyUI Built-in Node Documentation
Source: https://docs.comfy.org/built-in-nodes/ClipTextEncode

The ClipTextEncode node is used to convert text prompts into AI-understandable 'language' for image generation.

`CLIP Text Encode (CLIPTextEncode)` acts like a translator, converting your creative text prompts into a special "language" that AI can understand, helping the AI accurately interpret what kind of image you want to create.

Imagine communicating with a foreign artist - you need a translator to help accurately convey the artwork you want. This node acts as that translator, using the CLIP model (an AI model trained on vast amounts of image-text pairs) to understand your text descriptions and convert them into "instructions" that the AI art model can understand.

## Inputs

| Parameter | Data Type | Input Method    | Default | Range              | Description                                                                                                                                          |
| --------- | --------- | --------------- | ------- | ------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------- |
| text      | STRING    | Text Input      | Empty   | Any text           | Like detailed instructions to an artist, enter your image description here. Supports multi-line text for detailed descriptions.                      |
| clip      | CLIP      | Model Selection | None    | Loaded CLIP models | Like choosing a specific translator, different CLIP models are like different translators with slightly different understandings of artistic styles. |

## Outputs

| Output Name  | Data Type    | Description                                                                                                                                                                                                     |
| ------------ | ------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| CONDITIONING | CONDITIONING | These are the translated "painting instructions" containing detailed creative guidance that the AI model can understand. These instructions tell the AI model how to create an image matching your description. |

## Usage Tips

1. **Basic Text Prompt Usage**
   * Write detailed descriptions like you're writing a short essay
   * More specific descriptions lead to more accurate results
   * Use English commas to separate different descriptive elements

2. **Special Feature: Using Embedding Models**
   * Embedding models are like preset art style packages that can quickly apply specific artistic effects
   * Currently supports .safetensors, .pt, and .bin file formats, and you don't necessarily need to use the complete model name
   * How to use:

     1. Place the embedding model file (in .pt format) in the `ComfyUI/models/embeddings` folder
     2. Use `embedding:model_name` in your text
        Example: If you have a model called `EasyNegative.pt`, you can use it like this:

     ```
     a beautiful landscape, embedding:EasyNegative, high quality
     ```

3. **Prompt Weight Adjustment**
   * Use parentheses to adjust the importance of certain descriptions
   * For example: `(beautiful:1.2)` will make the "beautiful" feature more prominent
   * Regular parentheses `()` have a default weight of 1.1
   * Use keyboard shortcuts `ctrl + up/down arrow` to quickly adjust weights
   * The weight adjustment step size can be modified in settings

4. **Important Notes**
   * Ensure the CLIP model is properly loaded
   * Use positive and clear text descriptions
   * When using embedding models, make sure the file name is correct and compatible with your current main model's architecture


# ClipTextEncodeFlux - ComfyUI Built-in Node Documentation
Source: https://docs.comfy.org/built-in-nodes/ClipTextEncodeFlux

The ClipTextEncodeFlux node is used to encode text prompts into Flux-compatible conditioning embeddings.

`CLIPTextEncodeFlux` is an advanced text encoding node in ComfyUI, specifically designed for the Flux architecture. It uses a dual-encoder mechanism (CLIP-L and T5XXL) to process both structured keywords and detailed natural language descriptions, providing the Flux model with more accurate and comprehensive text understanding for improved text-to-image generation quality.

This node is based on a dual-encoder collaboration mechanism:

1. The `clip_l` input is processed by the CLIP-L encoder, extracting style, theme, and other keyword featuresideal for concise descriptions.
2. The `t5xxl` input is processed by the T5XXL encoder, which excels at understanding complex and detailed natural language scene descriptions.
3. The outputs from both encoders are fused, and combined with the `guidance` parameter to generate unified conditioning embeddings (`CONDITIONING`) for downstream Flux sampler nodes, controlling how closely the generated content matches the text description.

## Inputs

| Parameter  | Data Type | Input Method | Default | Range            | Description                                                                                                            |
| ---------- | --------- | ------------ | ------- | ---------------- | ---------------------------------------------------------------------------------------------------------------------- |
| `clip`     | CLIP      | Node input   | None    | -                | Must be a CLIP model supporting the Flux architecture, including both CLIP-L and T5XXL encoders                        |
| `clip_l`   | STRING    | Text box     | None    | Up to 77 tokens  | Suitable for concise keyword descriptions, such as style or theme                                                      |
| `t5xxl`    | STRING    | Text box     | None    | Nearly unlimited | Suitable for detailed natural language descriptions, expressing complex scenes and details                             |
| `guidance` | FLOAT     | Slider       | 3.5     | 0.0 - 100.0      | Controls the influence of text conditions on the generation process; higher values mean stricter adherence to the text |

## Outputs

| Output Name    | Data Type    | Description                                                                                                        |
| -------------- | ------------ | ------------------------------------------------------------------------------------------------------------------ |
| `CONDITIONING` | CONDITIONING | Contains the fused embeddings from both encoders and the guidance parameter, used for conditional image generation |

## Usage Examples

### Prompt Examples

* **clip\_l input** (keyword style):
  * Use structured, concise keyword combinations
  * Example: `masterpiece, best quality, portrait, oil painting, dramatic lighting`
  * Focus on style, quality, and main subject

* **t5xxl input** (natural language description):
  * Use complete, fluent scene descriptions
  * Example: `A highly detailed portrait in oil painting style, featuring dramatic chiaroscuro lighting that creates deep shadows and bright highlights, emphasizing the subject's features with renaissance-inspired composition.`
  * Focus on scene details, spatial relationships, and lighting effects

### Notes

1. Make sure to use a CLIP model compatible with the Flux architecture
2. It is recommended to fill in both `clip_l` and `t5xxl` to leverage the dual-encoder advantage
3. Note the 77-token limit for `clip_l`
4. Adjust the `guidance` parameter based on the generated results


# ClipTextEncodeHunyuanDit - ComfyUI Built-in Node Documentation
Source: https://docs.comfy.org/built-in-nodes/ClipTextEncodeHunyuanDit

The ClipTextEncodeHunyuanDit node is used to encode text prompts into HunyuanDiT-compatible conditioning embeddings.

The `CLIPTextEncodeHunyuanDiT` node's main function is to convert input text into a form that the model can understand. It is an advanced conditioning node specifically designed for the dual text encoder architecture of the HunyuanDiT model.
Its primary role is like a translator, converting our text descriptions into "machine language" that the AI model can understand. The `bert` and `mt5xl` inputs prefer different types of prompt inputs.

## Inputs

| Parameter | Data Type | Description                                                                                                                                  |
| --------- | --------- | -------------------------------------------------------------------------------------------------------------------------------------------- |
| `clip`    | CLIP      | A CLIP model instance used for text tokenization and encoding, which is core to generating conditions.                                       |
| `bert`    | STRING    | Text input for encoding, prefers phrases and keywords, supports multiline and dynamic prompts.                                               |
| `mt5xl`   | STRING    | Another text input for encoding, supports multiline and dynamic prompts (multilingual), can use complete sentences and complex descriptions. |

## Outputs

| Parameter      | Data Type    | Description                                                                     |
| -------------- | ------------ | ------------------------------------------------------------------------------- |
| `CONDITIONING` | CONDITIONING | The encoded conditional output used for further processing in generation tasks. |


# ClipTextEncodeSdxl - ComfyUI Built-in Node Documentation
Source: https://docs.comfy.org/built-in-nodes/ClipTextEncodeSdxl

The ClipTextEncodeSdxl node is used to encode text prompts into SDXL-compatible conditioning embeddings.

This node is designed to encode text input using a CLIP model specifically customized for the SDXL architecture. It uses a dual encoder system (CLIP-L and CLIP-G) to process text descriptions, resulting in more accurate image generation.

## Inputs

| Parameter       | Data Type | Description                                            |
| --------------- | --------- | ------------------------------------------------------ |
| `clip`          | CLIP      | CLIP model instance used for text encoding.            |
| `width`         | INT       | Specifies the image width in pixels, default 1024.     |
| `height`        | INT       | Specifies the image height in pixels, default 1024.    |
| `crop_w`        | INT       | Width of the crop area in pixels, default 0.           |
| `crop_h`        | INT       | Height of the crop area in pixels, default 0.          |
| `target_width`  | INT       | Target width for the output image, default 1024.       |
| `target_height` | INT       | Target height for the output image, default 1024.      |
| `text_g`        | STRING    | Global text description for overall scene description. |
| `text_l`        | STRING    | Local text description for detail description.         |

## Outputs

| Parameter      | Data Type    | Description                                                                    |
| -------------- | ------------ | ------------------------------------------------------------------------------ |
| `CONDITIONING` | CONDITIONING | Contains encoded text and conditional information needed for image generation. |


# ClipTextEncodeSdxlRefiner - ComfyUI Built-in Node Documentation
Source: https://docs.comfy.org/built-in-nodes/ClipTextEncodeSdxlRefiner

The ClipTextEncodeSdxlRefiner node is used to encode text prompts into SDXL Refiner-compatible conditioning embeddings.

This node is specifically designed for the SDXL Refiner model to convert text prompts into conditioning information by incorporating aesthetic scores and dimensional information to enhance the conditions for generation tasks, thereby improving the final refinement effect. It acts like a professional art director, not only conveying your creative intent but also injecting precise aesthetic standards and specification requirements into the work.

## About SDXL Refiner

SDXL Refiner is a specialized refinement model that focuses on enhancing image details and quality based on the SDXL base model. This process is like having an art retoucher:

1. First, it receives preliminary images or text descriptions generated by the base model
2. Then, it guides the refinement process through precise aesthetic scoring and dimensional parameters
3. Finally, it focuses on processing high-frequency image details to improve overall quality

Refiner can be used in two ways:

* As a standalone refinement step for post-processing images generated by the base model
* As part of an expert integration system, taking over processing during the low-noise phase of generation

## Inputs

| Parameter Name | Data Type | Input Type | Default Value | Value Range | Description                                                                                                                                                                                                                                                                                                |
| -------------- | --------- | ---------- | ------------- | ----------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `clip`         | CLIP      | Required   | -             | -           | CLIP model instance used for text tokenization and encoding, the core component for converting text into model-understandable format                                                                                                                                                                       |
| `ascore`       | FLOAT     | Optional   | 6.0           | 0.0-1000.0  | Controls the visual quality and aesthetics of generated images, similar to setting quality standards for artwork:<br />- High scores(7.5-8.5): Pursues more refined, detail-rich effects<br />- Medium scores(6.0-7.0): Balanced quality control<br />- Low scores(2.0-3.0): Suitable for negative prompts |
| `width`        | INT       | Required   | 1024          | 64-16384    | Specifies output image width (pixels), must be multiple of 8. SDXL performs best when total pixel count is close to 10241024 (about 1M pixels)                                                                                                                                                            |
| `height`       | INT       | Required   | 1024          | 64-16384    | Specifies output image height (pixels), must be multiple of 8. SDXL performs best when total pixel count is close to 10241024 (about 1M pixels)                                                                                                                                                           |
| `text`         | STRING    | Required   | -             | -           | Text prompt description, supports multi-line input and dynamic prompt syntax. In Refiner, text prompts should focus more on describing desired visual quality and detail characteristics                                                                                                                   |

## Outputs

| Output Name    | Data Type    | Description                                                                                                                                                                                            |
| -------------- | ------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `CONDITIONING` | CONDITIONING | Refined conditional output containing integrated encoding of text semantics, aesthetic standards, and dimensional information, specifically for guiding SDXL Refiner model in precise image refinement |

## Notes

1. This node is specifically optimized for the SDXL Refiner model and differs from regular CLIPTextEncode nodes
2. An aesthetic score of 7.5 is recommended as the baseline, which is the standard setting used in SDXL training
3. All dimensional parameters must be multiples of 8, and total pixel count close to 10241024 (about 1M pixels) is recommended
4. The Refiner model focuses on enhancing image details and quality, so text prompts should emphasize desired visual effects rather than scene content
5. In practical use, Refiner is typically used in the later stages of generation (approximately the last 20% of steps), focusing on detail optimization


# ClipVisionEncode - ComfyUI Built-in Node Documentation
Source: https://docs.comfy.org/built-in-nodes/ClipVisionEncode

The ClipVisionEncode node is used to encode input images into visual feature vectors through the CLIP Vision model.

The `CLIP Vision Encode` node is an image encoding node in ComfyUI, used to convert input images into visual feature vectors through the CLIP Vision model. This node is an important bridge connecting image and text understanding, and is widely used in various AI image generation and processing workflows.

**Node Functionality**

* **Image feature extraction**: Converts input images into high-dimensional feature vectors
* **Multimodal bridging**: Provides a foundation for joint processing of images and text
* **Conditional generation**: Provides visual conditions for image-based conditional generation

## Inputs

| Parameter Name | Data Type    | Description                                                          |
| -------------- | ------------ | -------------------------------------------------------------------- |
| `clip_vision`  | CLIP\_VISION | CLIP vision model, usually loaded via the CLIPVisionLoader node      |
| `image`        | IMAGE        | The input image to be encoded                                        |
| `crop`         | Dropdown     | Image cropping method, options: center (center crop), none (no crop) |

## Outputs

| Output Name          | Data Type            | Description             |
| -------------------- | -------------------- | ----------------------- |
| CLIP\_VISION\_OUTPUT | CLIP\_VISION\_OUTPUT | Encoded visual features |

This output object contains:

* `last_hidden_state`: The last hidden state
* `image_embeds`: Image embedding vector
* `penultimate_hidden_states`: The penultimate hidden state
* `mm_projected`: Multimodal projection result (if available)


# Load CLIP Vision - ComfyUI Built-in Node Documentation
Source: https://docs.comfy.org/built-in-nodes/ClipVisionLoader

The Load CLIP Vision node is used to load CLIP Vision models from the `ComfyUI/models/clip_vision` folder.

This node automatically detects models located in the `ComfyUI/models/clip_vision` folder, as well as any additional model paths configured in the `extra_model_paths.yaml` file. If you add models after starting ComfyUI, please **refresh the ComfyUI interface** to ensure the latest model files are listed.

## Inputs

| Field       | Data Type      | Description                                                                 |
| ----------- | -------------- | --------------------------------------------------------------------------- |
| `clip_name` | COMBO\[STRING] | Lists all supported model files in the `ComfyUI/models/clip_vision` folder. |

## Outputs

| Field         | Data Type    | Description                                                                        |
| ------------- | ------------ | ---------------------------------------------------------------------------------- |
| `clip_vision` | CLIP\_VISION | Loaded CLIP Vision model, ready for encoding images or other vision-related tasks. |


# Load3D - ComfyUI Built-in Node Documentation
Source: https://docs.comfy.org/built-in-nodes/Load3D

The Load3D node is a core node in ComfyUI for loading and previewing various 3D model files, supporting multi-format import and rich three-dimensional view operations.

The Load3D node is a core node for loading and processing 3D model files. When loading the node, it automatically retrieves available 3D resources from `ComfyUI/input/3d/`. You can also upload supported 3D files for preview using the upload function.

**Supported Formats**
Currently, this node supports multiple 3D file formats, including `.gltf`, `.glb`, `.obj`, `.fbx`, and `.stl`.

**3D Node Preferences**
Some related preferences for 3D nodes can be configured in ComfyUI's settings menu. Please refer to the following documentation for corresponding settings:

[Settings Menu - 3D](/interface/settings/3d)

Besides regular node outputs, Load3D has lots of 3D view-related settings in the canvas menu.

## Inputs

| Parameter Name | Type           | Description                                                                                   | Default | Range             |
| -------------- | -------------- | --------------------------------------------------------------------------------------------- | ------- | ----------------- |
| model\_file    | File Selection | 3D model file path, supports upload, defaults to reading model files from `ComfyUI/input/3d/` | -       | Supported formats |
| width          | INT            | Canvas rendering width                                                                        | 1024    | 1-4096            |
| height         | INT            | Canvas rendering height                                                                       | 1024    | 1-4096            |

## Outputs

| Parameter Name   | Data Type      | Description                                                                                    |
| ---------------- | -------------- | ---------------------------------------------------------------------------------------------- |
| image            | IMAGE          | Canvas rendered image                                                                          |
| mask             | MASK           | Mask containing current model position                                                         |
| mesh\_path       | STRING         | Model file path                                                                                |
| normal           | IMAGE          | Normal map                                                                                     |
| lineart          | IMAGE          | Line art image output, corresponding `edge_threshold` can be adjusted in the canvas model menu |
| camera\_info     | LOAD3D\_CAMERA | Camera information                                                                             |
| recording\_video | VIDEO          | Recorded video (only when recording exists)                                                    |

All corresponding outputs preview
<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/load3d_outputs.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=a8b0622a7e96c8085692046f539218fa" alt="View Operation Demo" data-og-width="2594" width="2594" data-og-height="1272" height="1272" data-path="images/comfy_core/load3d/load3d_outputs.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/load3d_outputs.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=4bb4a98f81496ee779d5b2610198fc93 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/load3d_outputs.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=7919d0d4724666101b4110eabab747e7 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/load3d_outputs.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=ced77b204c999954bb4b5d560eddd86c 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/load3d_outputs.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=cee88e3a213c56013ba08f5e1966fc9c 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/load3d_outputs.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=34658b0f24f5935e241bda5f43b3bc6c 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/load3d_outputs.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=59c9883857d9fda02988144a7585ac57 2500w" />

## Canvas Area Description

The Load3D node's Canvas area contains numerous view operations, including:

* Preview view settings (grid, background color, preview view)
* Camera control: Control FOV, camera type
* Global illumination intensity: Adjust lighting intensity
* Video recording: Record and export videos
* Model export: Supports `GLB`, `OBJ`, `STL` formats
* And more

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/load3d_ui.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=79fb8f464bc62086be17f0082625d0b8" alt="Load 3D Node UI" data-og-width="2025" width="2025" data-og-height="1696" height="1696" data-path="images/comfy_core/load3d/load3d_ui.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/load3d_ui.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=880997a87fa95cf38bdb5cf6be494bb5 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/load3d_ui.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=f558829882997fb902ee37cd7f3f7887 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/load3d_ui.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=e1d5bee3c3c31905fd034e4622da5513 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/load3d_ui.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=7cc603401811185c9af72d869a3fe4ea 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/load3d_ui.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=53532669e02277f4b94c1ceab595e8e0 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/load3d_ui.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=82c8743cc193239ab5de05254cb345a7 2500w" />

1. Contains multiple menus and hidden menus of the Load 3D node
2. Menu for `resizing preview window` and `canvas video recording`
3. 3D view operation axis
4. Preview thumbnail
5. Preview size settings, scale preview view display by setting dimensions and then resizing window

### 1. View Operations

<video controls muted loop playsInline className="w-full aspect-video rounded-xl" src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/view_operations.mp4?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=651f1cf60f5868704e03b4fb0c773761" data-path="images/comfy_core/load3d/view_operations.mp4" />

View control operations:

* Left-click + drag: Rotate the view
* Right-click + drag: Pan the view
* Middle wheel scroll or middle-click + drag: Zoom in/out
* Coordinate axis: Switch views

### 2. Left Menu Functions

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=f6e11c260b9c3f3aad82e82eded36030" alt="Menu" data-og-width="1184" width="1184" data-og-height="1582" height="1582" data-path="images/comfy_core/load3d/menu.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=8c0168da03202fbb8a053c03343d1941 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=b9b245f4cd95892cbb1ffeea5563e918 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=7d8c70ed6b986745ce20c3507d61e8c8 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=5a70ad2f39ea37f90a6eb1848e15b4e8 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=07c6fc74935556b0db9e8bfdc2caa932 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=71d2e2694e536e89e585e2c567841588 2500w" />

In the canvas, some settings are hidden in the menu. Click the menu button to expand different menus

* 1. Scene: Contains preview window grid, background color, preview settings
* 2. Model: Model rendering mode, texture materials, up direction settings
* 3. Camera: Switch between orthographic and perspective views, and set the perspective angle size
* 4. Light: Scene global illumination intensity
* 5. Export: Export model to other formats (GLB, OBJ, STL)

#### Scene

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_scene.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=065dcbc017397af3fc9393cf07d45735" alt="scene menu" data-og-width="1671" width="1671" data-og-height="1106" height="1106" data-path="images/comfy_core/load3d/menu_scene.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_scene.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=a870f6f996d5a2845ebd501b3c174a4f 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_scene.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=01c5802e199086069ecedeb52128f490 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_scene.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=f4c68255bc6da4f5ccab382848cb5c09 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_scene.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=6219802420efaa01fc709bea116d7d3c 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_scene.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=e434ccef5e3756ccb614a45e42c2c6ca 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_scene.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=838be02caf4d19fda9e1384ad51dab86 2500w" />

The Scene menu provides some basic scene setting functions

1. Show/Hide grid
2. Set background color
3. Click to upload a background image
4. Hide the preview

#### Model

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_model.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=5abbf34a279bffcb89bccaaeec4dc0d5" alt="Menu_Scene" data-og-width="3605" width="3605" data-og-height="1911" height="1911" data-path="images/comfy_core/load3d/menu_model.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_model.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=b6958b73e23adbdf99f588b5671f0c4f 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_model.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=79541430c16eec94141e3256beb4c3c9 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_model.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=c49e84eab6fe15c0f761442981ef4290 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_model.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=163d7d04d19ff8b7388d4fb60151cbc5 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_model.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=74443cf59c5cc8147b41ac9713e6a6e9 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_model.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=a2654354b3c42b4c99f0781becfefe34 2500w" />

The Model menu provides some model-related functions

1. **Up direction**: Determine which axis is the up direction for the model
2. **Material mode**: Switch model rendering modes - Original, Normal, Wireframe, Lineart

#### Camera

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_camera.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=41698c4ccc2a711dcd1d6b78788c2f57" alt="menu_modelmenu_camera" data-og-width="1729" width="1729" data-og-height="1434" height="1434" data-path="images/comfy_core/load3d/menu_camera.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_camera.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=5bb365052ce2cf4bc79092371649e34c 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_camera.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=46c0ec016c2e6525a57ba95036e0908a 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_camera.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=c4286312edf6752b6e99f84fbbb95350 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_camera.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=32427a62fd4f48503cb03a7b652277c4 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_camera.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=31ea7b4e7e32afd99768fee63d544108 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_camera.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=65e8d80228a737f616b1a75a6da49e48 2500w" />

This menu provides switching between orthographic and perspective views, and perspective angle size settings

1. **Camera**: Quickly switch between orthographic and orthographic views
2. **FOV**: Adjust FOV angle

#### Light

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_light.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=ffd6bd85f4dd12d35626e2d2799b7944" alt="menu_modelmenu_camera" data-og-width="1729" width="1729" data-og-height="740" height="740" data-path="images/comfy_core/load3d/menu_light.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_light.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=e6435fd6a500cf11f8e65e4aa59ef91b 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_light.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=198ef3ba79e99d7a7f377af1b62547ba 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_light.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=32984b4552fa350394b2cf03eda60b7c 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_light.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=e6211fa0ed56e3b79974892ff47403be 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_light.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=ee103a6ae3351eac002024aea2d7ecc3 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_light.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=3d3a86bd5e2f40ab36741d90afe421b0 2500w" />

Through this menu, you can quickly adjust the scene's global illumination intensity

#### Export

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_export.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=fc2488b6681e8269f7d9eac8ce8123b5" alt="menu_export" data-og-width="1729" width="1729" data-og-height="740" height="740" data-path="images/comfy_core/load3d/menu_export.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_export.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=e275701e81e42ed77d1aec794db7bb3b 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_export.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=7d36035e06c25b62ef319738c92ab670 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_export.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=1ef633b32843162d27464b90cecc7682 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_export.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=336cf22bbd01b35c6df1206f9a0ce230 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_export.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=ad9838e8a2b1ed8422980ffd6416ce29 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/menu_export.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=264cdfde8395392952100e468536a12d 2500w" />

This menu provides the ability to quickly convert and export model formats

### 3. Right Menu Functions

<video controls muted loop playsInline className="w-full aspect-video rounded-xl" src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfy_core/load3d/recording.mp4?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=4a5ed3f8ebba73beb99e7f86bd35c71e" data-path="images/comfy_core/load3d/recording.mp4" />

The right menu has two main functions:

1. **Reset view ratio**: After clicking the button, the view will adjust the canvas rendering area ratio according to the set width and height
2. **Video recording**: Allows you to record current 3D view operations as video, allows import, and can be output as `recording_video` to subsequent nodes


# Wan Vace To Video - ComfyUI Built-in Node Documentation
Source: https://docs.comfy.org/built-in-nodes/conditioning/video-models/wan-vace-to-video

Create videos using Alibaba Tongyi Wanxiang's high-resolution video generation API

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/conditioning/video-models/wan-vace-to-video.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=07b2c7fe941a73bd59d7b2a9c5a406d8" alt="Wan Vace To Video" data-og-width="1614" width="1614" data-og-height="1415" height="1415" data-path="images/built-in-nodes/conditioning/video-models/wan-vace-to-video.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/conditioning/video-models/wan-vace-to-video.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=7ca5f0885d533e782eabcb5cf5ec703c 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/conditioning/video-models/wan-vace-to-video.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=fb3512c9677300c30f31348e80c41512 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/conditioning/video-models/wan-vace-to-video.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=d3221fcf0bae8977bc745e421c224efa 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/conditioning/video-models/wan-vace-to-video.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=9adfe6cf986cd990b14877f2ca4f180b 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/conditioning/video-models/wan-vace-to-video.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=024192c015fbcb4193193e895f434aa2 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/conditioning/video-models/wan-vace-to-video.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=7f9205831f5f350fd65672cece10939d 2500w" />

The Wan Vace To Video node allows you to generate videos through text prompts and supports multiple input methods, including text, images, videos, masks, and control signals.

This node combines input conditions (prompts), control videos, and masks to generate high-quality videos. It first preprocesses and encodes the inputs, then applies the conditional information to generate the final video latent representation.
When a reference image is provided, it serves as the initial reference for the video. Control videos and masks can be used to guide the generation process, making the generated video more aligned with expectations.

## Parameter Description

### Required Parameters

| Parameter   | Type         | Default | Range              | Description                         |
| ----------- | ------------ | ------- | ------------------ | ----------------------------------- |
| positive    | CONDITIONING | -       | -                  | Positive prompt condition           |
| negative    | CONDITIONING | -       | -                  | Negative prompt condition           |
| vae         | VAE          | -       | -                  | VAE model for encoding/decoding     |
| width       | INT          | 832     | 16-MAX\_RESOLUTION | Video width, step size 16           |
| height      | INT          | 480     | 16-MAX\_RESOLUTION | Video height, step size 16          |
| length      | INT          | 81      | 1-MAX\_RESOLUTION  | Number of video frames, step size 4 |
| batch\_size | INT          | 1       | 1-4096             | Batch size                          |
| strength    | FLOAT        | 1.0     | 0.0-1000.0         | Condition strength, step size 0.01  |

### Optional Parameters

| Parameter        | Type  | Description                                                   |
| ---------------- | ----- | ------------------------------------------------------------- |
| control\_video   | IMAGE | Control video for guiding the generation process              |
| control\_masks   | MASK  | Control masks defining which areas should be controlled       |
| reference\_image | IMAGE | Reference image as starting point or reference (single image) |

### Output Parameters

| Parameter    | Type         | Description                                                                                                                                                                                                                                                                                                                                                                                           |
| ------------ | ------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| positive     | CONDITIONING | Processed positive prompt condition                                                                                                                                                                                                                                                                                                                                                                   |
| negative     | CONDITIONING | Processed negative prompt condition                                                                                                                                                                                                                                                                                                                                                                   |
| latent       | LATENT       | Generated video latent representation                                                                                                                                                                                                                                                                                                                                                                 |
| trim\_latent | INT          | Parameter for trimming latent representation, default value is 0. When a reference image is provided, this value is set to the shape size of the reference image in latent space. It indicates how much content from the reference image downstream nodes should trim from the generated latent representation to ensure proper control of the reference image's influence in the final video output. |

## Source Code

\[Source code update time: 2025-05-15]

```Python  theme={null}
class WanVaceToVideo:
    @classmethod
    def INPUT_TYPES(s):
        return {"required": {"positive": ("CONDITIONING", ),
                             "negative": ("CONDITIONING", ),
                             "vae": ("VAE", ),
                             "width": ("INT", {"default": 832, "min": 16, "max": nodes.MAX_RESOLUTION, "step": 16}),
                             "height": ("INT", {"default": 480, "min": 16, "max": nodes.MAX_RESOLUTION, "step": 16}),
                             "length": ("INT", {"default": 81, "min": 1, "max": nodes.MAX_RESOLUTION, "step": 4}),
                             "batch_size": ("INT", {"default": 1, "min": 1, "max": 4096}),
                             "strength": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 1000.0, "step": 0.01}),
                },
                "optional": {"control_video": ("IMAGE", ),
                             "control_masks": ("MASK", ),
                             "reference_image": ("IMAGE", ),
                }}

    RETURN_TYPES = ("CONDITIONING", "CONDITIONING", "LATENT", "INT")
    RETURN_NAMES = ("positive", "negative", "latent", "trim_latent")
    FUNCTION = "encode"

    CATEGORY = "conditioning/video_models"

    EXPERIMENTAL = True

    def encode(self, positive, negative, vae, width, height, length, batch_size, strength, control_video=None, control_masks=None, reference_image=None):
        latent_length = ((length - 1) // 4) + 1
        if control_video is not None:
            control_video = comfy.utils.common_upscale(control_video[:length].movedim(-1, 1), width, height, "bilinear", "center").movedim(1, -1)
            if control_video.shape[0] < length:
                control_video = torch.nn.functional.pad(control_video, (0, 0, 0, 0, 0, 0, 0, length - control_video.shape[0]), value=0.5)
        else:
            control_video = torch.ones((length, height, width, 3)) * 0.5

        if reference_image is not None:
            reference_image = comfy.utils.common_upscale(reference_image[:1].movedim(-1, 1), width, height, "bilinear", "center").movedim(1, -1)
            reference_image = vae.encode(reference_image[:, :, :, :3])
            reference_image = torch.cat([reference_image, comfy.latent_formats.Wan21().process_out(torch.zeros_like(reference_image))], dim=1)

        if control_masks is None:
            mask = torch.ones((length, height, width, 1))
        else:
            mask = control_masks
            if mask.ndim == 3:
                mask = mask.unsqueeze(1)
            mask = comfy.utils.common_upscale(mask[:length], width, height, "bilinear", "center").movedim(1, -1)
            if mask.shape[0] < length:
                mask = torch.nn.functional.pad(mask, (0, 0, 0, 0, 0, 0, 0, length - mask.shape[0]), value=1.0)

        control_video = control_video - 0.5
        inactive = (control_video * (1 - mask)) + 0.5
        reactive = (control_video * mask) + 0.5

        inactive = vae.encode(inactive[:, :, :, :3])
        reactive = vae.encode(reactive[:, :, :, :3])
        control_video_latent = torch.cat((inactive, reactive), dim=1)
        if reference_image is not None:
            control_video_latent = torch.cat((reference_image, control_video_latent), dim=2)

        vae_stride = 8
        height_mask = height // vae_stride
        width_mask = width // vae_stride
        mask = mask.view(length, height_mask, vae_stride, width_mask, vae_stride)
        mask = mask.permute(2, 4, 0, 1, 3)
        mask = mask.reshape(vae_stride * vae_stride, length, height_mask, width_mask)
        mask = torch.nn.functional.interpolate(mask.unsqueeze(0), size=(latent_length, height_mask, width_mask), mode='nearest-exact').squeeze(0)

        trim_latent = 0
        if reference_image is not None:
            mask_pad = torch.zeros_like(mask[:, :reference_image.shape[2], :, :])
            mask = torch.cat((mask_pad, mask), dim=1)
            latent_length += reference_image.shape[2]
            trim_latent = reference_image.shape[2]

        mask = mask.unsqueeze(0)
        positive = node_helpers.conditioning_set_values(positive, {"vace_frames": control_video_latent, "vace_mask": mask, "vace_strength": strength})
        negative = node_helpers.conditioning_set_values(negative, {"vace_frames": control_video_latent, "vace_mask": mask, "vace_strength": strength})

        latent = torch.zeros([batch_size, 16, latent_length, height // 8, width // 8], device=comfy.model_management.intermediate_device())
        out_latent = {}
        out_latent["samples"] = latent
        return (positive, negative, out_latent, trim_latent)

```


# TrimVideoLatent Node
Source: https://docs.comfy.org/built-in-nodes/latent/video/trim-video-latent

Trim video frames in latent space

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/latent/video/trim-video-latent.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=87bf02eabffeba90db7e9409ddf48dd1" alt="ComfyUI TrimVideoLatent Node" data-og-width="1608" width="1608" data-og-height="762" height="762" data-path="images/built-in-nodes/latent/video/trim-video-latent.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/latent/video/trim-video-latent.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=8cabf59929e83251d9c4faced9f4559f 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/latent/video/trim-video-latent.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=51e4b50bfc4f860f1118006743c72add 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/latent/video/trim-video-latent.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=50cbec822d7d2788a5130da31b75e5c2 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/latent/video/trim-video-latent.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=c654433a8b05804a967cf79eaf9d30b4 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/latent/video/trim-video-latent.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=e2aacbe2d5b4822bb66da6e5e0a47fd7 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/latent/video/trim-video-latent.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=90e007a0181c33d3d285eaefccd5fba3 2500w" />

The TrimVideoLatent node is used to trim video frames in latent space (LATENT). It is commonly used when processing video latent sequences to remove unwanted frames from the beginning, achieving "forward trimming" of the video.

Basic usage: Input the video latent data to be trimmed into samples, and set trim\_amount to the number of frames to trim. The node will trim the specified number of frames from the beginning of the video and output the remaining latent sequence.
Typical scenarios: Used in video generation, video editing and other scenarios to remove unwanted leading frames, or to work with other nodes to achieve video segment splicing and processing.

## Parameters

### Input Parameters

| Parameter    | Type   | Required | Default | Description                           |
| ------------ | ------ | -------- | ------- | ------------------------------------- |
| samples      | LATENT | Yes      | None    | Input latent video data               |
| trim\_amount | INT    | Yes      | 0       | Number of frames to trim (from start) |

### Output Parameters

| Parameter | Type   | Description               |
| --------- | ------ | ------------------------- |
| samples   | LATENT | Trimmed video latent data |

## Usage Example

<Card title="Wan2.1 VACE Video Generation Workflow Example" icon="book" href="/tutorials/video/wan/vace">
  Wan2.1 VACE Video Generation Workflow Example
</Card>

### Source Code

```python  theme={null}
class TrimVideoLatent:
    @classmethod
    def INPUT_TYPES(s):
        return {"required": { "samples": ("LATENT",),
                              "trim_amount": ("INT", {"default": 0, "min": 0, "max": 99999}),
                             }}

    RETURN_TYPES = ("LATENT",)
    FUNCTION = "op"

    CATEGORY = "latent/video"

    EXPERIMENTAL = True

    def op(self, samples, trim_amount):
        samples_out = samples.copy()

        s1 = samples["samples"]
        samples_out["samples"] = s1[:, :, trim_amount:]
        return (samples_out,)

```


# ComfyUI Built-in Nodes
Source: https://docs.comfy.org/built-in-nodes/overview

Introduction to ComfyUI Built-in Nodes

Built-in nodes are ComfyUI's default nodes. They are core functionalities of ComfyUI that you can use without installing any third-party custom node packages.

## About built-in node document

We have now added built-in node help documentation, so the content of this section is periodically synced from [this repo](https://github.com/Comfy-Org/embedded-docs). We will update the content manually once a week.

## Contribute

If you find any errors in the content, or want to contribute missing content, please submit an issue or PR in [this repo](https://github.com/Comfy-Org/embedded-docs) to help us improve.


# Flux 1.1 [pro] Ultra Image - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/image/bfl/flux-1-1-pro-ultra-image

Create images using Black Forest Labs' high-resolution image generation API

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/bfl/flux-1-1-pro-ultra-image.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=51f4ad7b93c0df61bcc855eac5324802" alt="ComfyUI Native Flux 1.1 [pro] Ultra Image node" data-og-width="1731" width="1731" data-og-height="1163" height="1163" data-path="images/built-in-nodes/api_nodes/bfl/flux-1-1-pro-ultra-image.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/bfl/flux-1-1-pro-ultra-image.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=2d5e0d3353e130ce452dc6d3748fe40b 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/bfl/flux-1-1-pro-ultra-image.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=b5594a0b49f78853bb09550ac0500d51 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/bfl/flux-1-1-pro-ultra-image.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=174b45e936d2349e977e7d0a68b995b8 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/bfl/flux-1-1-pro-ultra-image.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=fe883d69f481734a08e3ec34a9df487a 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/bfl/flux-1-1-pro-ultra-image.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=39dc4540e3af081e5eeddfc476ce0a9d 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/bfl/flux-1-1-pro-ultra-image.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=904755f3be6c655e228912f05b684ec0 2500w" />

The Flux 1.1 \[pro] Ultra Image node allows you to generate ultra-high-resolution images through text prompts, directly connecting to Black Forest Labs' latest image generation API.

This node supports two main usage modes:

1. **Text-to-Image**: Generate high-quality images from text prompts (when no image input is used)
2. **Image-to-Image**: Combine existing images with prompts to create new images that blend features from both (Remix mode)

This node supports Ultra mode through API calls, capable of generating images at 4 times the resolution of standard Flux 1.1 \[pro] (up to 4MP), without sacrificing prompt adherence, and maintaining super-fast generation times of just 10 seconds. Compared to other high-resolution models, it's more than 2.5 times faster.

## Parameter Description

### Basic Parameters

| Parameter          | Type    | Default | Description                                                                                                                                                                                                                      |
| ------------------ | ------- | ------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| prompt             | String  | ""      | Text description for generating the image                                                                                                                                                                                        |
| prompt\_upsampling | Boolean | False   | Whether to use prompt upsampling technique to enhance details. When enabled, automatically modifies prompts for more creative generation, but results become non-deterministic (same seed won't produce exactly the same result) |
| seed               | Integer | 0       | Random seed value, controls generation randomness                                                                                                                                                                                |
| aspect\_ratio      | String  | "16:9"  | Width-to-height ratio of the image, must be between 1:4 and 4:1                                                                                                                                                                  |
| raw                | Boolean | False   | When set to True, generates less processed, more natural-looking images                                                                                                                                                          |

### Optional Parameters

| Parameter               | Type  | Default | Description                                                                                                                                               |
| ----------------------- | ----- | ------- | --------------------------------------------------------------------------------------------------------------------------------------------------------- |
| image\_prompt           | Image | None    | Optional input, used for Image-to-Image (Remix) mode                                                                                                      |
| image\_prompt\_strength | Float | 0.1     | Active when `image_prompt` is input, adjusts the blend between prompt and image prompt. Higher values make output closer to input image, range is 0.0-1.0 |

### Output

| Output | Type  | Description                            |
| ------ | ----- | -------------------------------------- |
| IMAGE  | Image | Generated high-resolution image result |

## Usage Examples

Please visit the tutorial below to see corresponding usage examples

* [Flux 1.1 Pro Ultra Image API Node ComfyUI Official Example Workflow](/tutorials/partner-nodes/black-forest-labs/flux-1-1-pro-ultra-image)

## How It Works

Flux 1.1 \[pro] Ultra mode uses optimized deep learning architecture and efficient GPU acceleration technology to achieve high-resolution image generation without sacrificing speed. When a request is sent to the API, the system parses the prompt, applies appropriate parameters, then computes the image in parallel, finally generating and returning the high-resolution result.

Compared to regular models, Ultra mode particularly focuses on detail preservation and consistency at large scales, ensuring impressive quality even at 4MP high resolution.

## Source Code

\[Node Source Code (Updated on 2025-05-03)]

```python  theme={null}
class FluxProUltraImageNode(ComfyNodeABC):
    """
    Generates images synchronously based on prompt and resolution.
    """

    MINIMUM_RATIO = 1 / 4
    MAXIMUM_RATIO = 4 / 1
    MINIMUM_RATIO_STR = "1:4"
    MAXIMUM_RATIO_STR = "4:1"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation",
                    },
                ),
                "prompt_upsampling": (
                    IO.BOOLEAN,
                    {
                        "default": False,
                        "tooltip": "Whether to perform upsampling on the prompt. If active, automatically modifies the prompt for more creative generation, but results are nondeterministic (same seed will not produce exactly the same result).",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "The random seed used for creating the noise.",
                    },
                ),
                "aspect_ratio": (
                    IO.STRING,
                    {
                        "default": "16:9",
                        "tooltip": "Aspect ratio of image; must be between 1:4 and 4:1.",
                    },
                ),
                "raw": (
                    IO.BOOLEAN,
                    {
                        "default": False,
                        "tooltip": "When True, generate less processed, more natural-looking images.",
                    },
                ),
            },
            "optional": {
                "image_prompt": (IO.IMAGE,),
                "image_prompt_strength": (
                    IO.FLOAT,
                    {
                        "default": 0.1,
                        "min": 0.0,
                        "max": 1.0,
                        "step": 0.01,
                        "tooltip": "Blend between the prompt and the image prompt.",
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    @classmethod
    def VALIDATE_INPUTS(cls, aspect_ratio: str):
        try:
            validate_aspect_ratio(
                aspect_ratio,
                minimum_ratio=cls.MINIMUM_RATIO,
                maximum_ratio=cls.MAXIMUM_RATIO,
                minimum_ratio_str=cls.MINIMUM_RATIO_STR,
                maximum_ratio_str=cls.MAXIMUM_RATIO_STR,
            )
        except Exception as e:
            return str(e)
        return True

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/bfl"

    def api_call(
        self,
        prompt: str,
        aspect_ratio: str,
        prompt_upsampling=False,
        raw=False,
        seed=0,
        image_prompt=None,
        image_prompt_strength=0.1,
        auth_token=None,
        **kwargs,
    ):
        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/bfl/flux-pro-1.1-ultra/generate",
                method=HttpMethod.POST,
                request_model=BFLFluxProUltraGenerateRequest,
                response_model=BFLFluxProGenerateResponse,
            ),
            request=BFLFluxProUltraGenerateRequest(
                prompt=prompt,
                prompt_upsampling=prompt_upsampling,
                seed=seed,
                aspect_ratio=validate_aspect_ratio(
                    aspect_ratio,
                    minimum_ratio=self.MINIMUM_RATIO,
                    maximum_ratio=self.MAXIMUM_RATIO,
                    minimum_ratio_str=self.MINIMUM_RATIO_STR,
                    maximum_ratio_str=self.MAXIMUM_RATIO_STR,
                ),
                raw=raw,
                image_prompt=(
                    image_prompt
                    if image_prompt is None
                    else convert_image_to_base64(image_prompt)
                ),
                image_prompt_strength=(
                    None if image_prompt is None else round(image_prompt_strength, 2)
                ),
            ),
            auth_token=auth_token,
        )
        output_image = handle_bfl_synchronous_operation(operation)
        return (output_image,)
```


# Ideogram V1 - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/image/ideogram/ideogram-v1

Node for creating precise text rendering images using Ideogram API

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/ideogram/ideogram-v1.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=10b0ea0f9a0ddb9c1184540f977e2cef" alt="ComfyUI Native Ideogram V1 Node" data-og-width="1731" width="1731" data-og-height="1374" height="1374" data-path="images/built-in-nodes/api_nodes/ideogram/ideogram-v1.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/ideogram/ideogram-v1.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=95b9ab3556b117e6dcededef89df7f05 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/ideogram/ideogram-v1.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=c1d4b0ff22e2e42f5aa598680e5aaa93 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/ideogram/ideogram-v1.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=1c00cda4beee6d743b5f85d4ea399bda 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/ideogram/ideogram-v1.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=e06aba34eb0a3ddc3f84c59486c5b7f3 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/ideogram/ideogram-v1.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=29e030418b7b97ad941342ea49904c11 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/ideogram/ideogram-v1.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=9acea2d45fbcc54c7c52a522adc6777f 2500w" />

The Ideogram V1 node allows you to generate images with high-quality text rendering capabilities using Ideogram's text-to-image API.

## Parameter Description

### Required Parameters

| Parameter             | Type    | Default | Description                                                                 |
| --------------------- | ------- | ------- | --------------------------------------------------------------------------- |
| prompt                | string  | ""      | Text prompt describing the content to generate                              |
| turbo                 | boolean | False   | Whether to use turbo mode (faster but possibly lower quality)               |
| aspect\_ratio         | select  | "1:1"   | Image aspect ratio                                                          |
| magic\_prompt\_option | select  | "AUTO"  | Determines whether to use MagicPrompt in generation, options: AUTO, ON, OFF |
| seed                  | integer | 0       | Random seed value (0-2147483647)                                            |
| negative\_prompt      | string  | ""      | Specifies elements you don't want in the image                              |
| num\_images           | integer | 1       | Number of images to generate (1-8)                                          |

### Output

| Output | Type  | Description            |
| ------ | ----- | ---------------------- |
| IMAGE  | image | Generated image result |

## Source Code

\[Node Source Code (Updated on 2025-05-03)]

```python  theme={null}
class IdeogramV1(ComfyNodeABC):
    """
    Generates images synchronously using the Ideogram V1 model.

    Images links are available for a limited period of time; if you would like to keep the image, you must download it.
    """

    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls) -> InputTypeDict:
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation",
                    },
                ),
                "turbo": (
                    IO.BOOLEAN,
                    {
                        "default": False,
                        "tooltip": "Whether to use turbo mode (faster generation, potentially lower quality)",
                    }
                ),
            },
            "optional": {
                "aspect_ratio": (
                    IO.COMBO,
                    {
                        "options": list(V1_V2_RATIO_MAP.keys()),
                        "default": "1:1",
                        "tooltip": "The aspect ratio for image generation.",
                    },
                ),
                "magic_prompt_option": (
                    IO.COMBO,
                    {
                        "options": ["AUTO", "ON", "OFF"],
                        "default": "AUTO",
                        "tooltip": "Determine if MagicPrompt should be used in generation",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2147483647,
                        "step": 1,
                        "control_after_generate": True,
                        "display": "number",
                    },
                ),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Description of what to exclude from the image",
                    },
                ),
                "num_images": (
                    IO.INT,
                    {"default": 1, "min": 1, "max": 8, "step": 1, "display": "number"},
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = (IO.IMAGE,)
    FUNCTION = "api_call"
    CATEGORY = "api node/image/ideogram/v1"
    DESCRIPTION = cleandoc(__doc__ or "")
    API_NODE = True

    def api_call(
        self,
        prompt,
        turbo=False,
        aspect_ratio="1:1",
        magic_prompt_option="AUTO",
        seed=0,
        negative_prompt="",
        num_images=1,
        auth_token=None,
    ):
        # Determine the model based on turbo setting
        aspect_ratio = V1_V2_RATIO_MAP.get(aspect_ratio, None)
        model = "V_1_TURBO" if turbo else "V_1"

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/ideogram/generate",
                method=HttpMethod.POST,
                request_model=IdeogramGenerateRequest,
                response_model=IdeogramGenerateResponse,
            ),
            request=IdeogramGenerateRequest(
                image_request=ImageRequest(
                    prompt=prompt,
                    model=model,
                    num_images=num_images,
                    seed=seed,
                    aspect_ratio=aspect_ratio if aspect_ratio != "ASPECT_1_1" else None,
                    magic_prompt_option=(
                        magic_prompt_option if magic_prompt_option != "AUTO" else None
                    ),
                    negative_prompt=negative_prompt if negative_prompt else None,
                )
            ),
            auth_token=auth_token,
        )

        response = operation.execute()

        if not response.data or len(response.data) == 0:
            raise Exception("No images were generated in the response")

        image_urls = [image_data.url for image_data in response.data if image_data.url]

        if not image_urls:
            raise Exception("No image URLs were generated in the response")

        return (download_and_process_images(image_urls),)
```


# Ideogram V2 - ComfyUI Built-in Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/image/ideogram/ideogram-v2

Node for creating high-quality images and text rendering using Ideogram V2 API

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/ideogram/ideogram-v2.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=b9df9b148b7fdd0552ee35417c59a769" alt="ComfyUI Built-in Ideogram V2 Node" data-og-width="1731" width="1731" data-og-height="1549" height="1549" data-path="images/built-in-nodes/api_nodes/ideogram/ideogram-v2.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/ideogram/ideogram-v2.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=badf9dba828282b1c95733987eb72e16 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/ideogram/ideogram-v2.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=4dda2cc921e486f5365c3bdcd30698f0 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/ideogram/ideogram-v2.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=7dc5e5f0bd31dc7ecb91d23f8e32a310 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/ideogram/ideogram-v2.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=db0be8dfbe7887459475e00c328ead6d 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/ideogram/ideogram-v2.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=27768b2fd49e3b66c29911288e8b95c0 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/ideogram/ideogram-v2.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=4cc40653275b5e6b04fce642c4fa2993 2500w" />

The Ideogram V2 node allows you to generate more refined images using Ideogram's second-generation AI model, with significant improvements in text rendering, image quality, and overall aesthetics.

## Parameters

### Required Parameters

| Parameter | Type    | Default | Description                                                           |
| --------- | ------- | ------- | --------------------------------------------------------------------- |
| prompt    | string  | ""      | Text prompt describing the content to generate                        |
| turbo     | boolean | False   | Whether to use turbo mode (faster generation, possibly lower quality) |

### Optional Parameters

| Parameter             | Type     | Default | Description                                                                                                     |
| --------------------- | -------- | ------- | --------------------------------------------------------------------------------------------------------------- |
| aspect\_ratio         | dropdown | "1:1"   | Image aspect ratio, effective when resolution is set to "Auto"                                                  |
| resolution            | dropdown | "Auto"  | Output image resolution, if not set to "Auto", it will override the aspect\_ratio setting                       |
| magic\_prompt\_option | dropdown | "AUTO"  | Determines whether to use MagicPrompt feature during generation, options are \["AUTO", "ON", "OFF"]             |
| seed                  | integer  | 0       | Random seed value, range 0-2147483647                                                                           |
| style\_type           | dropdown | "NONE"  | Generation style type (V2 only), options are \["AUTO", "GENERAL", "REALISTIC", "DESIGN", "RENDER\_3D", "ANIME"] |
| negative\_prompt      | string   | ""      | Specifies elements you don't want to appear in the image                                                        |
| num\_images           | integer  | 1       | Number of images to generate, range 1-8                                                                         |

### Output

| Output | Type  | Description        |
| ------ | ----- | ------------------ |
| IMAGE  | image | Generated image(s) |

## Source Code

\[Node Source Code (Updated on 2025-05-03)]

```python  theme={null}

class IdeogramV2(ComfyNodeABC):
    """
    Generates images synchronously using the Ideogram V2 model.

    Images links are available for a limited period of time; if you would like to keep the image, you must download it.
    """

    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls) -> InputTypeDict:
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation",
                    },
                ),
                "turbo": (
                    IO.BOOLEAN,
                    {
                        "default": False,
                        "tooltip": "Whether to use turbo mode (faster generation, potentially lower quality)",
                    }
                ),
            },
            "optional": {
                "aspect_ratio": (
                    IO.COMBO,
                    {
                        "options": list(V1_V2_RATIO_MAP.keys()),
                        "default": "1:1",
                        "tooltip": "The aspect ratio for image generation. Ignored if resolution is not set to AUTO.",
                    },
                ),
                "resolution": (
                    IO.COMBO,
                    {
                        "options": list(V1_V1_RES_MAP.keys()),
                        "default": "Auto",
                        "tooltip": "The resolution for image generation. If not set to AUTO, this overrides the aspect_ratio setting.",
                    },
                ),
                "magic_prompt_option": (
                    IO.COMBO,
                    {
                        "options": ["AUTO", "ON", "OFF"],
                        "default": "AUTO",
                        "tooltip": "Determine if MagicPrompt should be used in generation",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2147483647,
                        "step": 1,
                        "control_after_generate": True,
                        "display": "number",
                    },
                ),
                "style_type": (
                    IO.COMBO,
                    {
                        "options": ["AUTO", "GENERAL", "REALISTIC", "DESIGN", "RENDER_3D", "ANIME"],
                        "default": "NONE",
                        "tooltip": "Style type for generation (V2 only)",
                    },
                ),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Description of what to exclude from the image",
                    },
                ),
                "num_images": (
                    IO.INT,
                    {"default": 1, "min": 1, "max": 8, "step": 1, "display": "number"},
                ),
                #"color_palette": (
                #    IO.STRING,
                #    {
                #        "multiline": False,
                #        "default": "",
                #        "tooltip": "Color palette preset name or hex colors with weights",
                #    },
                #),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = (IO.IMAGE,)
    FUNCTION = "api_call"
    CATEGORY = "api node/image/ideogram/v2"
    DESCRIPTION = cleandoc(__doc__ or "")
    API_NODE = True

    def api_call(
        self,
        prompt,
        turbo=False,
        aspect_ratio="1:1",
        resolution="Auto",
        magic_prompt_option="AUTO",
        seed=0,
        style_type="NONE",
        negative_prompt="",
        num_images=1,
        color_palette="",
        auth_token=None,
    ):
        aspect_ratio = V1_V2_RATIO_MAP.get(aspect_ratio, None)
        resolution = V1_V1_RES_MAP.get(resolution, None)
        # Determine the model based on turbo setting
        model = "V_2_TURBO" if turbo else "V_2"

        # Handle resolution vs aspect_ratio logic
        # If resolution is not AUTO, it overrides aspect_ratio
        final_resolution = None
        final_aspect_ratio = None

        if resolution != "AUTO":
            final_resolution = resolution
        else:
            final_aspect_ratio = aspect_ratio if aspect_ratio != "ASPECT_1_1" else None

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/ideogram/generate",
                method=HttpMethod.POST,
                request_model=IdeogramGenerateRequest,
                response_model=IdeogramGenerateResponse,
            ),
            request=IdeogramGenerateRequest(
                image_request=ImageRequest(
                    prompt=prompt,
                    model=model,
                    num_images=num_images,
                    seed=seed,
                    aspect_ratio=final_aspect_ratio,
                    resolution=final_resolution,
                    magic_prompt_option=(
                        magic_prompt_option if magic_prompt_option != "AUTO" else None
                    ),
                    style_type=style_type if style_type != "NONE" else None,
                    negative_prompt=negative_prompt if negative_prompt else None,
                    color_palette=color_palette if color_palette else None,
                )
            ),
            auth_token=auth_token,
        )

        response = operation.execute()

        if not response.data or len(response.data) == 0:
            raise Exception("No images were generated in the response")

        image_urls = [image_data.url for image_data in response.data if image_data.url]

        if not image_urls:
            raise Exception("No image URLs were generated in the response")

        return (download_and_process_images(image_urls),)

```


# Ideogram V3 - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/image/ideogram/ideogram-v3

Node for creating top-quality images and text rendering using Ideogram's latest V3 API

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/ideogram/ideogram-v3.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=6328a71e416e5f6be2751808136d6e00" alt="ComfyUI Native Ideogram V3 Node" data-og-width="1731" width="1731" data-og-height="1329" height="1329" data-path="images/built-in-nodes/api_nodes/ideogram/ideogram-v3.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/ideogram/ideogram-v3.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=bcfb5cc379ee9f0ef5424c5350dde168 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/ideogram/ideogram-v3.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=5f21bde1ae37a96b74ae7ce73dd1e705 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/ideogram/ideogram-v3.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=881abf56d20d60d6f9ff44f0528cba1a 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/ideogram/ideogram-v3.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=b9fdf8a7979615e45a8c35e157a92176 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/ideogram/ideogram-v3.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=bfa4d3fa6cc121c44a17350227ca4385 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/ideogram/ideogram-v3.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=427bc53f16ba8a2537e48517f128ece4 2500w" />

This node connects to the Ideogram V3 API to perform image generation tasks.

Currently, this node supports two image generation modes:

* **Text-to-Image Mode** - Generate new images from text prompts
* **Inpainting Mode** - Regenerate specific areas by providing an original image and mask

### Text-to-Image Mode

This is the default mode, activated when no image or mask inputs are provided. Simply provide a prompt and the desired parameters:

1. Describe the image you want in the prompt field
2. Select an appropriate aspect ratio or resolution
3. Adjust other parameters like magic prompt, seed, and rendering quality
4. Run the node to generate the image

### Inpainting Mode

**Important Note**: This mode requires both image and mask inputs. If only one is provided, the node will throw an error.

1. Connect the original image to the `image` input port
2. Create a mask with the same dimensions as the original image, where white areas represent parts to be regenerated
3. Connect the mask to the `mask` input port
4. Describe what you want to generate in the masked area in the prompt
5. Run the node to perform local editing

## Parameter Descriptions

### Basic Parameters

| Parameter             | Type   | Default    | Description                                       |
| --------------------- | ------ | ---------- | ------------------------------------------------- |
| prompt                | string | ""         | Text prompt describing the content to generate    |
| aspect\_ratio         | combo  | "1:1"      | Image aspect ratio (text-to-image mode only)      |
| resolution            | combo  | "Auto"     | Image resolution, overrides aspect ratio when set |
| magic\_prompt\_option | combo  | "AUTO"     | Magic prompt enhancement: AUTO, ON, or OFF        |
| seed                  | int    | 0          | Random seed value, 0 for random generation        |
| num\_images           | int    | 1          | Number of images to generate (1-8)                |
| rendering\_speed      | combo  | "BALANCED" | Rendering speed: BALANCED, TURBO, or QUALITY      |

### Optional Parameters

| Parameter | Type  | Description                                                                         |
| --------- | ----- | ----------------------------------------------------------------------------------- |
| image     | image | Input image for inpainting mode (**must be provided with mask**)                    |
| mask      | mask  | Mask for inpainting, white areas will be replaced (**must be provided with image**) |

### Output

| Output | Type  | Description     |
| ------ | ----- | --------------- |
| IMAGE  | image | Generated image |

## How It Works

The Ideogram V3 node uses state-of-the-art AI models to process user input, capable of understanding complex design intentions and text layout requirements. It supports two main modes:

1. **Generation Mode**: Creates new images from text prompts
2. **Edit Mode**: Uses original image + mask combination, replacing only the areas specified by the mask

## Source Code

\[Node Source Code (Updated 2025-05-03)]

```python  theme={null}

class IdeogramV3(ComfyNodeABC):
    """
    Generates images synchronously using the Ideogram V3 model.

    Supports both regular image generation from text prompts and image editing with mask.
    Images links are available for a limited period of time; if you would like to keep the image, you must download it.
    """

    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls) -> InputTypeDict:
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation or editing",
                    },
                ),
            },
            "optional": {
                "image": (
                    IO.IMAGE,
                    {
                        "default": None,
                        "tooltip": "Optional reference image for image editing.",
                    },
                ),
                "mask": (
                    IO.MASK,
                    {
                        "default": None,
                        "tooltip": "Optional mask for inpainting (white areas will be replaced)",
                    },
                ),
                "aspect_ratio": (
                    IO.COMBO,
                    {
                        "options": list(V3_RATIO_MAP.keys()),
                        "default": "1:1",
                        "tooltip": "The aspect ratio for image generation. Ignored if resolution is not set to Auto.",
                    },
                ),
                "resolution": (
                    IO.COMBO,
                    {
                        "options": V3_RESOLUTIONS,
                        "default": "Auto",
                        "tooltip": "The resolution for image generation. If not set to Auto, this overrides the aspect_ratio setting.",
                    },
                ),
                "magic_prompt_option": (
                    IO.COMBO,
                    {
                        "options": ["AUTO", "ON", "OFF"],
                        "default": "AUTO",
                        "tooltip": "Determine if MagicPrompt should be used in generation",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2147483647,
                        "step": 1,
                        "control_after_generate": True,
                        "display": "number",
                    },
                ),
                "num_images": (
                    IO.INT,
                    {"default": 1, "min": 1, "max": 8, "step": 1, "display": "number"},
                ),
                "rendering_speed": (
                    IO.COMBO,
                    {
                        "options": ["BALANCED", "TURBO", "QUALITY"],
                        "default": "BALANCED",
                        "tooltip": "Controls the trade-off between generation speed and quality",
                    },
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = (IO.IMAGE,)
    FUNCTION = "api_call"
    CATEGORY = "api node/image/ideogram/v3"
    DESCRIPTION = cleandoc(__doc__ or "")
    API_NODE = True

    def api_call(
        self,
        prompt,
        image=None,
        mask=None,
        resolution="Auto",
        aspect_ratio="1:1",
        magic_prompt_option="AUTO",
        seed=0,
        num_images=1,
        rendering_speed="BALANCED",
        auth_token=None,
    ):
        # Check if both image and mask are provided for editing mode
        if image is not None and mask is not None:
            # Edit mode
            path = "/proxy/ideogram/ideogram-v3/edit"

            # Process image and mask
            input_tensor = image.squeeze().cpu()

            # Validate mask dimensions match image
            if mask.shape[1:] != image.shape[1:-1]:
                raise Exception("Mask and Image must be the same size")

            # Process image
            img_np = (input_tensor.numpy() * 255).astype(np.uint8)
            img = Image.fromarray(img_np)
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format="PNG")
            img_byte_arr.seek(0)
            img_binary = img_byte_arr
            img_binary.name = "image.png"

            # Process mask - white areas will be replaced
            mask_np = (mask.squeeze().cpu().numpy() * 255).astype(np.uint8)
            mask_img = Image.fromarray(mask_np)
            mask_byte_arr = io.BytesIO()
            mask_img.save(mask_byte_arr, format="PNG")
            mask_byte_arr.seek(0)
            mask_binary = mask_byte_arr
            mask_binary.name = "mask.png"

            # Create edit request
            edit_request = IdeogramV3EditRequest(
                prompt=prompt,
                rendering_speed=rendering_speed,
            )

            # Add optional parameters
            if magic_prompt_option != "AUTO":
                edit_request.magic_prompt = magic_prompt_option
            if seed != 0:
                edit_request.seed = seed
            if num_images > 1:
                edit_request.num_images = num_images

            # Execute the operation for edit mode
            operation = SynchronousOperation(
                endpoint=ApiEndpoint(
                    path=path,
                    method=HttpMethod.POST,
                    request_model=IdeogramV3EditRequest,
                    response_model=IdeogramGenerateResponse,
                ),
                request=edit_request,
                files={
                    "image": img_binary,
                    "mask": mask_binary,
                },
                content_type="multipart/form-data",
                auth_token=auth_token,
            )

        elif image is not None or mask is not None:
            # If only one of image or mask is provided, raise an error
            raise Exception("Ideogram V3 image editing requires both an image AND a mask")
        else:
            # Generation mode
            path = "/proxy/ideogram/ideogram-v3/generate"

            # Create generation request
            gen_request = IdeogramV3Request(
                prompt=prompt,
                rendering_speed=rendering_speed,
            )

            # Handle resolution vs aspect ratio
            if resolution != "Auto":
                gen_request.resolution = resolution
            elif aspect_ratio != "1:1":
                v3_aspect = V3_RATIO_MAP.get(aspect_ratio)
                if v3_aspect:
                    gen_request.aspect_ratio = v3_aspect

            # Add optional parameters
            if magic_prompt_option != "AUTO":
                gen_request.magic_prompt = magic_prompt_option
            if seed != 0:
                gen_request.seed = seed
            if num_images > 1:
                gen_request.num_images = num_images

            # Execute the operation for generation mode
            operation = SynchronousOperation(
                endpoint=ApiEndpoint(
                    path=path,
                    method=HttpMethod.POST,
                    request_model=IdeogramV3Request,
                    response_model=IdeogramGenerateResponse,
                ),
                request=gen_request,
                auth_token=auth_token,
            )

        # Execute the operation and process response
        response = operation.execute()

        if not response.data or len(response.data) == 0:
            raise Exception("No images were generated in the response")

        image_urls = [image_data.url for image_data in response.data if image_data.url]

        if not image_urls:
            raise Exception("No image URLs were generated in the response")

        return (download_and_process_images(image_urls),)

```


# Luma Image to Image - ComfyUI Built-in Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/image/luma/luma-image-to-image

Node for modifying images using Luma AI

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-image-to-image.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=c59f5ee7cefc230c298f9e3d8a24e507" alt="ComfyUI Built-in Luma Image to Image Node" data-og-width="1731" width="1731" data-og-height="1036" height="1036" data-path="images/built-in-nodes/api_nodes/luma/luma-image-to-image.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-image-to-image.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=ecac3f917e86aabc2b1dc029063303f2 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-image-to-image.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=3f9ca15c8600b37f9d25e8f2de7cfae6 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-image-to-image.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=e5479916063e4ed86cd0283710d6512d 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-image-to-image.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=66aff37c622c3eca50e97e354636e206 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-image-to-image.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=9144938fd44937e50a07c27df37809e6 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-image-to-image.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=1628ad1fc847ab42536547a448ebc192 2500w" />

The Luma Image to Image node allows you to modify existing images using Luma AI technology based on text prompts, while preserving certain features and structure of the original image.

## Node Function

This node connects to Luma AI's text-to-image API, enabling users to generate images through detailed text prompts. Luma AI is known for its excellent realism and detail, particularly excelling at generating photorealistic content and artistic style images.

## Parameters

### Basic Parameters

| Parameter            | Type    | Default | Description                                                                             |
| -------------------- | ------- | ------- | --------------------------------------------------------------------------------------- |
| prompt               | string  | ""      | Text prompt describing the content to generate                                          |
| model                | select  | -       | Select which generation model to use                                                    |
| aspect\_ratio        | select  | 16:9    | Set the aspect ratio of the output image                                                |
| seed                 | integer | 0       | Seed value to determine if node should rerun, but actual results don't depend on seed   |
| style\_image\_weight | float   | 1.0     | Weight of the style image, range 0.02-1.0, only effective when style\_image is provided |

### Optional Parameters

Without the following parameter inputs, the node functions in text-to-image mode

| Parameter        | Type      | Description                                                                                                               |
| ---------------- | --------- | ------------------------------------------------------------------------------------------------------------------------- |
| image\_luma\_ref | LUMA\_REF | Luma reference node connection, influences generation results through input images, can consider up to 4 images           |
| style\_image     | image     | Style reference image, uses only 1 image, influences the style of generated images, adjusted through `style_image_weight` |
| character\_image | image     | Adds character features to the generated results, can be a batch of multiple images, up to 4 images                       |

### Output

| Output | Type  | Description         |
| ------ | ----- | ------------------- |
| IMAGE  | image | The generated image |

## Usage Examples

## How It Works

The Luma Image to Image node analyzes the input image and combines it with text prompts to guide the modification process. It uses Luma AI's generation models to make creative changes to images based on prompts.

Node process:

1. First uploads the input image to ComfyAPI
2. Then sends the image URL with the prompt to Luma API
3. Waits for Luma AI to complete processing
4. Downloads and returns the generated image

The image\_weight parameter controls the degree of influence from the original image - values closer to 0 will preserve more of the original image features, while values closer to 1 allow for more substantial modifications.

## Source Code

\[Node Source Code (Updated on 2025-05-05)]

```python  theme={null}

class LumaImageModifyNode(ComfyNodeABC):
    """
    Modifies images synchronously based on prompt and aspect ratio.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Luma"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (IO.IMAGE,),
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation",
                    },
                ),
                "image_weight": (
                    IO.FLOAT,
                    {
                        "default": 1.0,
                        "min": 0.02,
                        "max": 1.0,
                        "step": 0.01,
                        "tooltip": "Weight of the image; the closer to 0.0, the less the image will be modified.",
                    },
                ),
                "model": ([model.value for model in LumaImageModel],),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {},
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        prompt: str,
        model: str,
        image: torch.Tensor,
        image_weight: float,
        seed,
        auth_token=None,
        **kwargs,
    ):
        # first, upload image
        download_urls = upload_images_to_comfyapi(
            image, max_images=1, auth_token=auth_token
        )
        image_url = download_urls[0]
        # next, make Luma call with download url provided
        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/luma/generations/image",
                method=HttpMethod.POST,
                request_model=LumaImageGenerationRequest,
                response_model=LumaGeneration,
            ),
            request=LumaImageGenerationRequest(
                prompt=prompt,
                model=model,
                modify_image_ref=LumaModifyImageRef(
                    url=image_url, weight=round(image_weight, 2)
                ),
            ),
            auth_token=auth_token,
        )
        response_api: LumaGeneration = operation.execute()

        operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path=f"/proxy/luma/generations/{response_api.id}",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=LumaGeneration,
            ),
            completed_statuses=[LumaState.completed],
            failed_statuses=[LumaState.failed],
            status_extractor=lambda x: x.state,
            auth_token=auth_token,
        )
        response_poll = operation.execute()

        img_response = requests.get(response_poll.assets.image)
        img = process_image_response(img_response)
        return (img,)

```


# Luma Reference - ComfyUI Built-in Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/image/luma/luma-reference

Helper node providing reference images for Luma image generation

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-reference.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=95659c8c2dd4e09b26feef3901678c83" alt="ComfyUI Built-in Luma Reference Node" data-og-width="1590" width="1590" data-og-height="641" height="641" data-path="images/built-in-nodes/api_nodes/luma/luma-reference.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-reference.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=b62dffc7dd027319adea82eef7337cd6 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-reference.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=66ed27e41f09d77fab45b1f757a7134f 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-reference.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=43b0ecbf1f93f70cb59d3378a77f5b3d 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-reference.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=78eb0fcefc89fcfd92b0e0e81e444fc2 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-reference.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=5afd7d8618e27805e1293dcf418402dd 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-reference.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=04703b9e962a398a5c1cd5800a516c0d 2500w" />

The Luma Reference node allows you to set reference images and weights to guide the creation process of Luma image generation nodes, making the generated images closer to specific features of the reference images.

## Node Function

This node works as a helper tool for Luma generation nodes, allowing users to provide reference images to influence generation results. It enables users to set the weight of reference images to control how much they affect the final result.
Multiple Luma Reference nodes can be chained together, with a maximum of 4 working simultaneously according to API requirements.

## Parameters

### Basic Parameters

| Parameter | Type  | Default | Description                                                    |
| --------- | ----- | ------- | -------------------------------------------------------------- |
| image     | Image | -       | Input image used as reference                                  |
| weight    | Float | 1.0     | Controls the strength of the reference image's influence (0-1) |

### Output

| Output    | Type      | Description                                  |
| --------- | --------- | -------------------------------------------- |
| luma\_ref | LUMA\_REF | Reference object containing image and weight |

## Usage Example

<Card title="Luma Text to Image Workflow Example" icon="book" href="/tutorials/partner-nodes/luma/luma-text-to-image">
  Luma Text to Image Workflow Example
</Card>

## How It Works

The Luma Reference node receives image input and allows setting a weight value. The node doesn't directly generate or modify images but creates a reference object containing image data and weight information, which is then passed to Luma generation nodes.

During the generation process, Luma AI analyzes the features of the reference image and incorporates these features into the generation results based on the set weight. Higher weight values mean the generated image will be closer to the reference image's features, while lower weight values indicate the reference image will only slightly influence the final result.

## Source Code

\[Node Source Code (Updated on 2025-05-03)]

```python  theme={null}

class LumaReferenceNode(ComfyNodeABC):
    """
    Holds an image and weight for use with Luma Generate Image node.
    """

    RETURN_TYPES = (LumaIO.LUMA_REF,)
    RETURN_NAMES = ("luma_ref",)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "create_luma_reference"
    CATEGORY = "api node/image/Luma"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (
                    IO.IMAGE,
                    {
                        "tooltip": "Image to use as reference.",
                    },
                ),
                "weight": (
                    IO.FLOAT,
                    {
                        "default": 1.0,
                        "min": 0.0,
                        "max": 1.0,
                        "step": 0.01,
                        "tooltip": "Weight of image reference.",
                    },
                ),
            },
            "optional": {"luma_ref": (LumaIO.LUMA_REF,)},
        }

    def create_luma_reference(
        self, image: torch.Tensor, weight: float, luma_ref: LumaReferenceChain = None
    ):
        if luma_ref is not None:
            luma_ref = luma_ref.clone()
        else:
            luma_ref = LumaReferenceChain()
        luma_ref.add(LumaReference(image=image, weight=round(weight, 2)))
        return (luma_ref,)

```


# Luma Text to Image - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/image/luma/luma-text-to-image

A node that converts text descriptions into high-quality images using Luma AI

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-text-to-image.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=19025e6d9d71bde0458f444404afe2b6" alt="ComfyUI Native Luma Text to Image Node" data-og-width="1731" width="1731" data-og-height="1218" height="1218" data-path="images/built-in-nodes/api_nodes/luma/luma-text-to-image.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-text-to-image.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=04dac25d3b0df7c78a1c577f6817b188 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-text-to-image.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=e2296dbd807aee153a2cdba84c4103e1 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-text-to-image.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=4ddb5c76d0e154abef23ff8cb3024e21 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-text-to-image.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=1fc029fcc7147dd529a51420daf96f13 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-text-to-image.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=00b4e5db96d95cfc23c0ff093899a7c2 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-text-to-image.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=f19f237fc7b4d0f130d51296c2bc8f2b 2500w" />

The Luma Text to Image node allows you to create highly realistic and artistic images from text descriptions using Luma AI's advanced image generation capabilities.

## Node Function

This node connects to Luma AI's text-to-image API, enabling users to generate images through detailed text prompts. Luma AI is known for its excellent realism and detail representation, particularly excelling at producing photorealistic content and artistic style images.

## Parameters

### Basic Parameters

| Parameter            | Type    | Default | Description                                                                                                          |
| -------------------- | ------- | ------- | -------------------------------------------------------------------------------------------------------------------- |
| prompt               | String  | ""      | Text prompt describing the content to generate                                                                       |
| model                | Select  | -       | Choose which generation model to use                                                                                 |
| aspect\_ratio        | Select  | 16:9    | Set the output image's aspect ratio                                                                                  |
| seed                 | Integer | 0       | Seed value to determine if the node should re-run, but actual results are independent of the seed                    |
| style\_image\_weight | Float   | 1.0     | Style image weight, range 0.0-1.0, only applies when style\_image is provided, higher means stronger style reference |

### Optional Parameters

| Parameter        | Type      | Description                                                                              |
| ---------------- | --------- | ---------------------------------------------------------------------------------------- |
| image\_luma\_ref | LUMA\_REF | Luma reference node connection to influence generation with input images; up to 4 images |
| style\_image     | Image     | Style reference image; only 1 image will be used                                         |
| character\_image | Image     | Character reference images; can be a batch of multiple, up to 4 images                   |

### Output

| Output | Type  | Description            |
| ------ | ----- | ---------------------- |
| IMAGE  | Image | Generated image result |

## Usage Example

<Card title="Luma Text to Image Usage Example" icon="book" href="/tutorials/partner-nodes/luma/luma-text-to-image">
  Detailed guide for Luma Text to Image workflow
</Card>

## How It Works

The Luma Text to Image node analyzes the text prompt provided by the user and creates corresponding images through Luma AI's generation models. This process uses deep learning technology to understand text descriptions and convert them into visual representations. Users can fine-tune the generation process by adjusting various parameters, including resolution, guidance scale, and negative prompts.

Additionally, the node supports using reference images and concept guidance to further influence the generation results, allowing creators to more precisely achieve their creative vision.

## Source Code

\[Node Source Code (Updated on 2025-05-03)]

```python  theme={null}

class LumaImageGenerationNode(ComfyNodeABC):
    """
    Generates images synchronously based on prompt and aspect ratio.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Luma"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation",
                    },
                ),
                "model": ([model.value for model in LumaImageModel],),
                "aspect_ratio": (
                    [ratio.value for ratio in LumaAspectRatio],
                    {
                        "default": LumaAspectRatio.ratio_16_9,
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
                "style_image_weight": (
                    IO.FLOAT,
                    {
                        "default": 1.0,
                        "min": 0.0,
                        "max": 1.0,
                        "step": 0.01,
                        "tooltip": "Weight of style image. Ignored if no style_image provided.",
                    },
                ),
            },
            "optional": {
                "image_luma_ref": (
                    LumaIO.LUMA_REF,
                    {
                        "tooltip": "Luma Reference node connection to influence generation with input images; up to 4 images can be considered."
                    },
                ),
                "style_image": (
                    IO.IMAGE,
                    {"tooltip": "Style reference image; only 1 image will be used."},
                ),
                "character_image": (
                    IO.IMAGE,
                    {
                        "tooltip": "Character reference images; can be a batch of multiple, up to 4 images can be considered."
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        prompt: str,
        model: str,
        aspect_ratio: str,
        seed,
        style_image_weight: float,
        image_luma_ref: LumaReferenceChain = None,
        style_image: torch.Tensor = None,
        character_image: torch.Tensor = None,
        auth_token=None,
        **kwargs,
    ):
        # handle image_luma_ref
        api_image_ref = None
        if image_luma_ref is not None:
            api_image_ref = self._convert_luma_refs(
                image_luma_ref, max_refs=4, auth_token=auth_token
            )
        # handle style_luma_ref
        api_style_ref = None
        if style_image is not None:
            api_style_ref = self._convert_style_image(
                style_image, weight=style_image_weight, auth_token=auth_token
            )
        # handle character_ref images
        character_ref = None
        if character_image is not None:
            download_urls = upload_images_to_comfyapi(
                character_image, max_images=4, auth_token=auth_token
            )
            character_ref = LumaCharacterRef(
                identity0=LumaImageIdentity(images=download_urls)
            )

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/luma/generations/image",
                method=HttpMethod.POST,
                request_model=LumaImageGenerationRequest,
                response_model=LumaGeneration,
            ),
            request=LumaImageGenerationRequest(
                prompt=prompt,
                model=model,
                aspect_ratio=aspect_ratio,
                image_ref=api_image_ref,
                style_ref=api_style_ref,
                character_ref=character_ref,
            ),
            auth_token=auth_token,
        )
        response_api: LumaGeneration = operation.execute()

        operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path=f"/proxy/luma/generations/{response_api.id}",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=LumaGeneration,
            ),
            completed_statuses=[LumaState.completed],
            failed_statuses=[LumaState.failed],
            status_extractor=lambda x: x.state,
            auth_token=auth_token,
        )
        response_poll = operation.execute()

        img_response = requests.get(response_poll.assets.image)
        img = process_image_response(img_response)
        return (img,)

    def _convert_luma_refs(
        self, luma_ref: LumaReferenceChain, max_refs: int, auth_token=None
    ):
        luma_urls = []
        ref_count = 0
        for ref in luma_ref.refs:
            download_urls = upload_images_to_comfyapi(
                ref.image, max_images=1, auth_token=auth_token
            )
            luma_urls.append(download_urls[0])
            ref_count += 1
            if ref_count >= max_refs:
                break
        return luma_ref.create_api_model(download_urls=luma_urls, max_refs=max_refs)

    def _convert_style_image(
        self, style_image: torch.Tensor, weight: float, auth_token=None
    ):
        chain = LumaReferenceChain(
            first_ref=LumaReference(image=style_image, weight=weight)
        )
        return self._convert_luma_refs(chain, max_refs=1, auth_token=auth_token)

```


# OpenAI DALLE 2 - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/image/openai/openai-dalle2

Node for generating images using OpenAI's DALLE 2 model

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/openai/openai-dall-e-2.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=2a1ad9dab55bb9b02c4551dbf05078ff" alt="ComfyUI Native Stability AI Stable Image Ultra Node" data-og-width="1576" width="1576" data-og-height="954" height="954" data-path="images/built-in-nodes/api_nodes/openai/openai-dall-e-2.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/openai/openai-dall-e-2.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=53b28734e20336b812c35ffea924981b 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/openai/openai-dall-e-2.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=b30dbe9ccd774fe3571a3906a17bda65 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/openai/openai-dall-e-2.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=be84d3151baae3d9ac80a9bccdacad6f 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/openai/openai-dall-e-2.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=b941cec97ff07c2d88f89dee636cea5e 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/openai/openai-dall-e-2.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=a4665033990c3947974a9befb8293466 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/openai/openai-dall-e-2.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=c727c2192f64c414a57ad745fc6faccf 2500w" />

The OpenAI DALLE 2 node allows you to use OpenAI's DALLE 2 API to generate creative images from text descriptions.

## Parameters

### Basic Parameters

| Parameter | Type    | Default     | Description                                                                                          |
| --------- | ------- | ----------- | ---------------------------------------------------------------------------------------------------- |
| prompt    | string  | ""          | Text prompt for DALLE to generate images, supports multi-line input                                 |
| seed      | integer | 0           | The result is not actually related to the seed, this parameter only determines whether to re-execute |
| size      | select  | "1024x1024" | Output image size, options: 256x256, 512x512, 1024x1024                                              |
| n         | integer | 1           | Number of images to generate, range 1-8                                                              |

### Optional Parameters

| Parameter | Type  | Default | Description                                                 |
| --------- | ----- | ------- | ----------------------------------------------------------- |
| image     | image | None    | Optional reference image for image editing                  |
| mask      | mask  | None    | Optional mask for inpainting (white areas will be replaced) |

### Output

| Output | Type  | Description        |
| ------ | ----- | ------------------ |
| IMAGE  | image | Generated image(s) |

## Features

* Basic function: Generate images from text prompts
* Image editing: When both image and mask parameters are provided, performs image editing (white masked areas will be replaced)

## Source Code

\[Node source code (Updated on 2025-05-03)]

```python  theme={null}

class OpenAIDalle2(ComfyNodeABC):
    """
    Generates images synchronously via OpenAI's DALLE 2 endpoint.

    Uses the proxy at /proxy/openai/images/generations. Returned URLs are shortlived,
    so download or cache results if you need to keep them.
    """

    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls) -> InputTypeDict:
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Text prompt for DALLE",
                    },
                ),
            },
            "optional": {
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2**31 - 1,
                        "step": 1,
                        "display": "number",
                        "control_after_generate": True,
                        "tooltip": "not implemented yet in backend",
                    },
                ),
                "size": (
                    IO.COMBO,
                    {
                        "options": ["256x256", "512x512", "1024x1024"],
                        "default": "1024x1024",
                        "tooltip": "Image size",
                    },
                ),
                "n": (
                    IO.INT,
                    {
                        "default": 1,
                        "min": 1,
                        "max": 8,
                        "step": 1,
                        "display": "number",
                        "tooltip": "How many images to generate",
                    },
                ),
                "image": (
                    IO.IMAGE,
                    {
                        "default": None,
                        "tooltip": "Optional reference image for image editing.",
                    },
                ),
                "mask": (
                    IO.MASK,
                    {
                        "default": None,
                        "tooltip": "Optional mask for inpainting (white areas will be replaced)",
                    },
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = (IO.IMAGE,)
    FUNCTION = "api_call"
    CATEGORY = "api node/image/openai"
    DESCRIPTION = cleandoc(__doc__ or "")
    API_NODE = True

    def api_call(
        self,
        prompt,
        seed=0,
        image=None,
        mask=None,
        n=1,
        size="1024x1024",
        auth_token=None,
    ):
        model = "dall-e-2"
        path = "/proxy/openai/images/generations"
        content_type = "application/json"
        request_class = OpenAIImageGenerationRequest
        img_binary = None

        if image is not None and mask is not None:
            path = "/proxy/openai/images/edits"
            content_type = "multipart/form-data"
            request_class = OpenAIImageEditRequest

            input_tensor = image.squeeze().cpu()
            height, width, channels = input_tensor.shape
            rgba_tensor = torch.ones(height, width, 4, device="cpu")
            rgba_tensor[:, :, :channels] = input_tensor

            if mask.shape[1:] != image.shape[1:-1]:
                raise Exception("Mask and Image must be the same size")
            rgba_tensor[:, :, 3] = 1 - mask.squeeze().cpu()

            rgba_tensor = downscale_image_tensor(rgba_tensor.unsqueeze(0)).squeeze()

            image_np = (rgba_tensor.numpy() * 255).astype(np.uint8)
            img = Image.fromarray(image_np)
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format="PNG")
            img_byte_arr.seek(0)
            img_binary = img_byte_arr  # .getvalue()
            img_binary.name = "image.png"
        elif image is not None or mask is not None:
            raise Exception("Dall-E 2 image editing requires an image AND a mask")

        # Build the operation
        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path=path,
                method=HttpMethod.POST,
                request_model=request_class,
                response_model=OpenAIImageGenerationResponse,
            ),
            request=request_class(
                model=model,
                prompt=prompt,
                n=n,
                size=size,
                seed=seed,
            ),
            files=(
                {
                    "image": img_binary,
                }
                if img_binary
                else None
            ),
            content_type=content_type,
            auth_token=auth_token,
        )

        response = operation.execute()

        img_tensor = validate_and_cast_response(response)
        return (img_tensor,)
```


# OpenAI DALLE 3 - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/image/openai/openai-dalle3

Node for generating high-quality images using OpenAI's DALLE 3 model

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/openai/openai-dall-e-3.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=1d4da35db8baddcd1321ef98365e26d0" alt="ComfyUI Native OpenAI DALLE 3 Node" data-og-width="1576" width="1576" data-og-height="966" height="966" data-path="images/built-in-nodes/api_nodes/openai/openai-dall-e-3.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/openai/openai-dall-e-3.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=c4991a2a05a37c4f63b9251cc4f5cb66 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/openai/openai-dall-e-3.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=bdb4fb6d52ed938a4a6f3dfb830e79f2 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/openai/openai-dall-e-3.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=f4de073cc71e3209f66be6eb0f45a575 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/openai/openai-dall-e-3.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=e525c1ca97e0ee9bd5e0b812e812530d 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/openai/openai-dall-e-3.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=d50bd7dabc5e9e5adca0683bc6777de4 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/openai/openai-dall-e-3.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=b1867a36a2203ad0a1b2fba9117a42ec 2500w" />

This node connects to OpenAI's DALLE 3 API, allowing users to generate high-quality images through detailed text prompts. DALLE 3 is OpenAI's image generation model that offers significantly improved image quality, more accurate prompt understanding, and better detail rendering compared to previous versions.

## Parameters

### Input Parameters

| Parameter | Type      | Default     | Description                                                                                                                                                                         |
| --------- | --------- | ----------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| prompt    | string    | ""          | Detailed text prompt describing what to generate                                                                                                                                    |
| seed      | integer   | 0           | Final result is not related to seed, this parameter only determines whether to re-execute                                                                                           |
| quality   | selection | "standard"  | Image quality, options: "standard" or "hd"                                                                                                                                          |
| style     | selection | "natural"   | Visual style, options: "natural" or "vivid". "Vivid" makes the model tend to create more surreal and dramatic images, while "natural" generates more realistic, less surreal images |
| size      | selection | "1024x1024" | Output image size, options: "1024x1024", "1024x1792", or "1792x1024"                                                                                                                |

### Output Parameters

| Output | Type  | Description                |
| ------ | ----- | -------------------------- |
| IMAGE  | image | The generated image result |

## Source Code

\[Node source code (Updated on 2025-05-03)]

```python  theme={null}

class OpenAIDalle3(ComfyNodeABC):
    """
    Generates images synchronously via OpenAI's DALLE 3 endpoint.

    Uses the proxy at /proxy/openai/images/generations. Returned URLs are shortlived,
    so download or cache results if you need to keep them.
    """

    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls) -> InputTypeDict:
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Text prompt for DALLE",
                    },
                ),
            },
            "optional": {
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2**31 - 1,
                        "step": 1,
                        "display": "number",
                        "control_after_generate": True,
                        "tooltip": "not implemented yet in backend",
                    },
                ),
                "quality": (
                    IO.COMBO,
                    {
                        "options": ["standard", "hd"],
                        "default": "standard",
                        "tooltip": "Image quality",
                    },
                ),
                "style": (
                    IO.COMBO,
                    {
                        "options": ["natural", "vivid"],
                        "default": "natural",
                        "tooltip": "Vivid causes the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images.",
                    },
                ),
                "size": (
                    IO.COMBO,
                    {
                        "options": ["1024x1024", "1024x1792", "1792x1024"],
                        "default": "1024x1024",
                        "tooltip": "Image size",
                    },
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = (IO.IMAGE,)
    FUNCTION = "api_call"
    CATEGORY = "api node/image/openai"
    DESCRIPTION = cleandoc(__doc__ or "")
    API_NODE = True

    def api_call(
        self,
        prompt,
        seed=0,
        style="natural",
        quality="standard",
        size="1024x1024",
        auth_token=None,
    ):
        model = "dall-e-3"

        # build the operation
        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/openai/images/generations",
                method=HttpMethod.POST,
                request_model=OpenAIImageGenerationRequest,
                response_model=OpenAIImageGenerationResponse,
            ),
            request=OpenAIImageGenerationRequest(
                model=model,
                prompt=prompt,
                quality=quality,
                size=size,
                style=style,
                seed=seed,
            ),
            auth_token=auth_token,
        )

        response = operation.execute()

        img_tensor = validate_and_cast_response(response)
        return (img_tensor,)
```


# OpenAI GPT Image 1 - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/image/openai/openai-gpt-image1

Node for generating images using OpenAI's GPT-4 Vision model

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/openai/openai-gpt-image-1.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=b3bb270a55ced01ef601c882f66778c0" alt="ComfyUI Native OpenAI GPT Image 1 Node" data-og-width="1576" width="1576" data-og-height="1123" height="1123" data-path="images/built-in-nodes/api_nodes/openai/openai-gpt-image-1.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/openai/openai-gpt-image-1.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=cea244ac5a4e4ac1d49f666ec822706e 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/openai/openai-gpt-image-1.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=66924bc6f72a5974d6d911fd83379cf1 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/openai/openai-gpt-image-1.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=148c6f19852c82665273093b4f07eafe 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/openai/openai-gpt-image-1.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=8dc974e86ed09904dd11f290d83543b2 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/openai/openai-gpt-image-1.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=ec1de16faca9d56e22a5b4cb4ec75afe 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/openai/openai-gpt-image-1.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=5259d2ce9aa544cb8544bdf2c73ad395 2500w" />

This node connects to OpenAI's GPT Image 1 API, allowing users to generate images through detailed text prompts. Unlike traditional DALLE models, GPT Image 1 leverages GPT-4's language understanding capabilities to handle more complex prompts and generate images that better match user intent.

## Parameters

### Basic Parameters

| Parameter | Type      | Default | Description                                                               |
| --------- | --------- | ------- | ------------------------------------------------------------------------- |
| prompt    | string    | ""      | Text prompt describing what to generate                                   |
| quality   | selection | "low"   | Image quality level, options: "low", "medium", "high"                     |
| size      | selection | "auto"  | Output image size, options: "auto", "1024x1024", "1024x1536", "1536x1024" |

### Image Editing Parameters

| Parameter | Type  | Description                                                  |
| --------- | ----- | ------------------------------------------------------------ |
| image     | image | Input image for editing, supports multiple images            |
| mask      | mask  | Optional mask to specify areas to modify (single image only) |

### Optional Parameters

| Parameter  | Type      | Description                                   |
| ---------- | --------- | --------------------------------------------- |
| background | selection | Background options: "opaque" or "transparent" |
| seed       | integer   | Random seed (not yet implemented in backend)  |
| n          | integer   | Number of images to generate, range 1-8       |

### Output

| Output | Type  | Description            |
| ------ | ----- | ---------------------- |
| IMAGE  | image | Generated image result |

## How It Works

The OpenAI GPT Image 1 node combines GPT-4's language understanding with image generation. It analyzes the text prompt to understand its meaning and intent, then generates matching images.

In image editing mode, the node can modify existing images. Using a mask allows precise control over which areas to change. Note that mask input only works with single image input.

Users can control the output by adjusting parameters like quality level, size, background handling, and number of generations.

## Source Code

\[Node source code (Updated on 2025-05-03)]

```python  theme={null}

class OpenAIGPTImage1(ComfyNodeABC):
    """
    Generates images synchronously via OpenAI's GPT Image 1 endpoint.

    Uses the proxy at /proxy/openai/images/generations. Returned URLs are shortlived,
    so download or cache results if you need to keep them.
    """

    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls) -> InputTypeDict:
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Text prompt for GPT Image 1",
                    },
                ),
            },
            "optional": {
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2**31 - 1,
                        "step": 1,
                        "display": "number",
                        "control_after_generate": True,
                        "tooltip": "not implemented yet in backend",
                    },
                ),
                "quality": (
                    IO.COMBO,
                    {
                        "options": ["low", "medium", "high"],
                        "default": "low",
                        "tooltip": "Image quality, affects cost and generation time.",
                    },
                ),
                "background": (
                    IO.COMBO,
                    {
                        "options": ["opaque", "transparent"],
                        "default": "opaque",
                        "tooltip": "Return image with or without background",
                    },
                ),
                "size": (
                    IO.COMBO,
                    {
                        "options": ["auto", "1024x1024", "1024x1536", "1536x1024"],
                        "default": "auto",
                        "tooltip": "Image size",
                    },
                ),
                "n": (
                    IO.INT,
                    {
                        "default": 1,
                        "min": 1,
                        "max": 8,
                        "step": 1,
                        "display": "number",
                        "tooltip": "How many images to generate",
                    },
                ),
                "image": (
                    IO.IMAGE,
                    {
                        "default": None,
                        "tooltip": "Optional reference image for image editing.",
                    },
                ),
                "mask": (
                    IO.MASK,
                    {
                        "default": None,
                        "tooltip": "Optional mask for inpainting (white areas will be replaced)",
                    },
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = (IO.IMAGE,)
    FUNCTION = "api_call"
    CATEGORY = "api node/image/openai"
    DESCRIPTION = cleandoc(__doc__ or "")
    API_NODE = True

    def api_call(
        self,
        prompt,
        seed=0,
        quality="low",
        background="opaque",
        image=None,
        mask=None,
        n=1,
        size="1024x1024",
        auth_token=None,
    ):
        model = "gpt-image-1"
        path = "/proxy/openai/images/generations"
        content_type="application/json"
        request_class = OpenAIImageGenerationRequest
        img_binaries = []
        mask_binary = None
        files = []

        if image is not None:
            path = "/proxy/openai/images/edits"
            request_class = OpenAIImageEditRequest
            content_type ="multipart/form-data"

            batch_size = image.shape[0]

            for i in range(batch_size):
                single_image = image[i : i + 1]
                scaled_image = downscale_image_tensor(single_image).squeeze()

                image_np = (scaled_image.numpy() * 255).astype(np.uint8)
                img = Image.fromarray(image_np)
                img_byte_arr = io.BytesIO()
                img.save(img_byte_arr, format="PNG")
                img_byte_arr.seek(0)
                img_binary = img_byte_arr
                img_binary.name = f"image_{i}.png"

                img_binaries.append(img_binary)
                if batch_size == 1:
                    files.append(("image", img_binary))
                else:
                    files.append(("image[]", img_binary))

        if mask is not None:
            if image.shape[0] != 1:
                raise Exception("Cannot use a mask with multiple image")
            if image is None:
                raise Exception("Cannot use a mask without an input image")
            if mask.shape[1:] != image.shape[1:-1]:
                raise Exception("Mask and Image must be the same size")
            batch, height, width = mask.shape
            rgba_mask = torch.zeros(height, width, 4, device="cpu")
            rgba_mask[:, :, 3] = 1 - mask.squeeze().cpu()

            scaled_mask = downscale_image_tensor(rgba_mask.unsqueeze(0)).squeeze()

            mask_np = (scaled_mask.numpy() * 255).astype(np.uint8)
            mask_img = Image.fromarray(mask_np)
            mask_img_byte_arr = io.BytesIO()
            mask_img.save(mask_img_byte_arr, format="PNG")
            mask_img_byte_arr.seek(0)
            mask_binary = mask_img_byte_arr
            mask_binary.name = "mask.png"
            files.append(("mask", mask_binary))

        # Build the operation
        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path=path,
                method=HttpMethod.POST,
                request_model=request_class,
                response_model=OpenAIImageGenerationResponse,
            ),
            request=request_class(
                model=model,
                prompt=prompt,
                quality=quality,
                background=background,
                n=n,
                seed=seed,
                size=size,
            ),
            files=files if files else None,
            content_type=content_type,
            auth_token=auth_token,
        )

        response = operation.execute()

        img_tensor = validate_and_cast_response(response)
        return (img_tensor,)
```


# Recraft Color RGB - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/image/recraft/recraft-color-rgb

Helper node for defining color controls in Recraft image generation

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-color-rgb.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=406b336f43d8d8872ab620d8d5777f5f" alt="ComfyUI Native Recraft Color RGB Node" data-og-width="1506" width="1506" data-og-height="819" height="819" data-path="images/built-in-nodes/api_nodes/recraft/recraft-color-rgb.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-color-rgb.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=be322b9b302d150a36e81ed0c10ef67c 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-color-rgb.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=a8c1bf7219288c4996059121900edf4e 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-color-rgb.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=87a17664ad9c3e57123d97e503103f12 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-color-rgb.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=28e64e100a1eac1f4b0b2fc5dfec0095 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-color-rgb.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=b4d421d5f9ec44105ffa1430af7e89e8 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-color-rgb.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=a87dc6d0a03ef2446a85c62ca728adc7 2500w" />
The Recraft Color RGB node lets you define precise RGB color values to control colors in Recraft image generation.

## Node Function

This node creates a color configuration object that connects to the Recraft Controls node to specify colors used in generated images.

## Parameters

### Basic Parameters

| Parameter | Type    | Default | Description           |
| --------- | ------- | ------- | --------------------- |
| r         | integer | 0       | Red channel (0-255)   |
| g         | integer | 0       | Green channel (0-255) |
| b         | integer | 0       | Blue channel (0-255)  |

### Output

| Output         | Type          | Description                                        |
| -------------- | ------------- | -------------------------------------------------- |
| recraft\_color | Recraft Color | Color config object to connect to Recraft Controls |

## Usage Example

<Card title="Recraft Text to Image Workflow Example" icon="book" href="/tutorials/partner-nodes/recraft/recraft-text-to-image">
  Recraft Text to Image Workflow Example
</Card>

## Source Code

\[Node source code (Updated on 2025-05-03)]

```python  theme={null}
class RecraftColorRGBNode:
    """
    Create Recraft Color by choosing specific RGB values.
    """

    RETURN_TYPES = (RecraftIO.COLOR,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    RETURN_NAMES = ("recraft_color",)
    FUNCTION = "create_color"
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "r": (IO.INT, {
                    "default": 0,
                    "min": 0,
                    "max": 255,
                    "tooltip": "Red value of color."
                }),
                "g": (IO.INT, {
                    "default": 0,
                    "min": 0,
                    "max": 255,
                    "tooltip": "Green value of color."
                }),
                "b": (IO.INT, {
                    "default": 0,
                    "min": 0,
                    "max": 255,
                    "tooltip": "Blue value of color."
                }),
            },
            "optional": {
                "recraft_color": (RecraftIO.COLOR,),
            }
        }

    def create_color(self, r: int, g: int, b: int, recraft_color: RecraftColorChain=None):
        recraft_color = recraft_color.clone() if recraft_color else RecraftColorChain()
        recraft_color.add(RecraftColor(r, g, b))
        return (recraft_color, )

```


# Recraft Controls - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/image/recraft/recraft-controls

Node providing advanced control parameters for Recraft image generation

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-contorols.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=6c5b9b0c3f9e255975e9bca7d12a9d2b" alt="ComfyUI Native Recraft Controls Node" data-og-width="1506" width="1506" data-og-height="556" height="556" data-path="images/built-in-nodes/api_nodes/recraft/recraft-contorols.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-contorols.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=f9af5a68245f18802627cb8fac6236eb 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-contorols.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=0a248fae86e42157a33fa0f494faf417 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-contorols.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=eaa5a11dde8d914734484925b30a8e60 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-contorols.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=8f2eb8275c9267cd02ae62360c613460 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-contorols.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=8156e45f55c046a11b139e79f0376ad6 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-contorols.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=eff587249e8652cd80b6f4f1088d671a 2500w" />

The Recraft Controls node lets you define control parameters (like colors and background colors) to guide Recraft's image generation process. This node combines multiple control inputs into a unified control object.

## Parameters

### Optional Parameters

| Parameter         | Type          | Description                         |
| ----------------- | ------------- | ----------------------------------- |
| colors            | Recraft Color | Color controls for image generation |
| background\_color | Recraft Color | Background color control            |

### Output

| Output            | Type             | Description                                        |
| ----------------- | ---------------- | -------------------------------------------------- |
| recraft\_controls | Recraft Controls | Control config object for Recraft generation nodes |

## Usage Example

<Card title="Recraft Text to Image Workflow Example" icon="book" href="/tutorials/partner-nodes/recraft/recraft-text-to-image">
  Recraft Text to Image Workflow Example
</Card>

## How It Works

Node process:

1. Collects input control parameters (colors and background\_color)
2. Combines these parameters into a structured control object
3. Outputs this control object for connecting to Recraft generation nodes

When connected to Recraft generation nodes, these control parameters influence the AI generation process. The AI considers multiple factors beyond just the text prompt's semantic content. If color inputs are configured, the AI will try to use these colors appropriately in the generated image.

## Source Code

\[Node source code (Updated on 2025-05-03)]

```python  theme={null}
class RecraftControlsNode:
    """
    Create Recraft Controls for customizing Recraft generation.
    """

    RETURN_TYPES = (RecraftIO.CONTROLS,)
    RETURN_NAMES = ("recraft_controls",)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "create_controls"
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
            },
            "optional": {
                "colors": (RecraftIO.COLOR,),
                "background_color": (RecraftIO.COLOR,),
            }
        }

    def create_controls(self, colors: RecraftColorChain=None, background_color: RecraftColorChain=None):
        return (RecraftControls(colors=colors, background_color=background_color), )

```


# Recraft Creative Upscale - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/image/recraft/recraft-creative-upscale

A Recraft Partner node that uses AI to creatively enhance image details and resolution

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-creative-upscale-image.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=7478834eaacd7370aad308c51518c743" alt="ComfyUI Native Recraft Creative Upscale Node" data-og-width="1506" width="1506" data-og-height="547" height="547" data-path="images/built-in-nodes/api_nodes/recraft/recraft-creative-upscale-image.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-creative-upscale-image.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=d7a2bfb2973f947441a41fc72fdd9c4d 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-creative-upscale-image.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=527b4e2932b567ae3e1dad257bc54def 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-creative-upscale-image.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=d906b58bfbe8eb97f96e14b78affd1ea 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-creative-upscale-image.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=56f4cc3114f3c59ac52aa34215c8e5f4 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-creative-upscale-image.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=acc6507de6af817f90a37ccd58e72c98 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-creative-upscale-image.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=e4b03f62886a34f4fa862e4b74a86cd6 2500w" />

The Recraft Creative Upscale node uses Recraft's API to increase image resolution while creatively enhancing and enriching image details.

## Parameters

### Basic Parameters

| Parameter | Type  | Default | Description                           |
| --------- | ----- | ------- | ------------------------------------- |
| image     | image | -       | Input image to be creatively upscaled |

### Output

| Output | Type  | Description                                    |
| ------ | ----- | ---------------------------------------------- |
| IMAGE  | image | High-resolution image after creative upscaling |

## Source Code

\[Node source code (Updated on 2025-05-03)]

```python  theme={null}
class RecraftCreativeUpscaleNode(RecraftCrispUpscaleNode):
    """
    Upscale image synchronously.
    Enhances a given raster image using creative upscale tool, boosting resolution with a focus on refining small details and faces.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    RECRAFT_PATH = "/proxy/recraft/images/creativeUpscale"
```


# Recraft Crisp Upscale - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/image/recraft/recraft-crisp-upscale

A Recraft Partner node that enhances image clarity and resolution using AI

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-crisp-upscale-image.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=69f9f1c716832688e95c562c1f9be1c2" alt="ComfyUI Native Recraft Crisp Upscale Node" data-og-width="1506" width="1506" data-og-height="557" height="557" data-path="images/built-in-nodes/api_nodes/recraft/recraft-crisp-upscale-image.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-crisp-upscale-image.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=0c0393fa898e99926335b0025566d595 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-crisp-upscale-image.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=b4c7f9d1a267e2d5597dfa3c48091f24 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-crisp-upscale-image.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=0b3dc9225c597b0600eab950406aa14f 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-crisp-upscale-image.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=0deb877d4dbdb129dfae58e912984525 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-crisp-upscale-image.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=fd780c684c97061454bf0ab8349b824f 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-crisp-upscale-image.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=52407efe5e34d3a18134a16885bdd483 2500w" />

The Recraft Crisp Upscale node uses Recraft's API to improve image resolution and clarity.

## Parameters

### Basic Parameters

| Parameter | Type  | Default | Description                |
| --------- | ----- | ------- | -------------------------- |
| image     | image | -       | Input image to be upscaled |

### Output

| Output | Type  | Description                        |
| ------ | ----- | ---------------------------------- |
| IMAGE  | image | Upscaled and enhanced output image |

## Source Code

\[Node source code (Updated on 2025-05-03)]

```python  theme={null}
class RecraftCrispUpscaleNode:
    """
    Upscale image synchronously.
    Enhances a given raster image using crisp upscale tool, increasing image resolution, making the image sharper and cleaner.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    RECRAFT_PATH = "/proxy/recraft/images/crispUpscale"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (IO.IMAGE, ),
            },
            "optional": {
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        image: torch.Tensor,
        auth_token=None,
        **kwargs,
    ):
        images = []
        total = image.shape[0]
        pbar = ProgressBar(total)
        for i in range(total):
            sub_bytes = handle_recraft_file_request(
                image=image[i],
                path=self.RECRAFT_PATH,
                auth_token=auth_token,
            )
            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))
            pbar.update(1)

        images_tensor = torch.cat(images, dim=0)
        return (images_tensor,)
```


# Recraft Image Inpainting - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/image/recraft/recraft-image-inpainting

Selectively modify image regions using Recraft API

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-image-inpainting.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=ce37045a2a53e129f5bec3e425f0002b" alt="ComfyUI Native Recraft Image Inpainting Node" data-og-width="1731" width="1731" data-og-height="1109" height="1109" data-path="images/built-in-nodes/api_nodes/recraft/recraft-image-inpainting.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-image-inpainting.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=d07ac3bf1ac8adc554c7e5501ade97c0 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-image-inpainting.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=1bf04240ed1b57ef3d29e7237bd80200 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-image-inpainting.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=7bd6ae3d2278cae4bd26590dc97fce31 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-image-inpainting.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=bebca092cdfa569d08e73fcab712541a 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-image-inpainting.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=02ea2f2c6ffead1cb6631a256e00176b 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-image-inpainting.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=c913e6fc3844208d68cfca9745c1fa7b 2500w" />

The Recraft Image Inpainting node lets you modify specific areas of an image while keeping the rest unchanged. By providing an image, mask and text prompt, you can generate new content to fill the selected areas.

## Parameters

### Basic Parameters

| Parameter | Type    | Default | Description                                     |
| --------- | ------- | ------- | ----------------------------------------------- |
| image     | image   | -       | Input image to modify                           |
| mask      | mask    | -       | Black and white mask defining areas to change   |
| prompt    | string  | ""      | Text describing what to generate in masked area |
| n         | integer | 1       | Number of results to generate (1-6)             |
| seed      | integer | 0       | Random seed value                               |

### Optional Parameters

| Parameter         | Type             | Description                            |
| ----------------- | ---------------- | -------------------------------------- |
| recraft\_style    | Recraft Style    | Style settings for generated content   |
| negative\_prompt  | string           | Elements to avoid in generated content |
| recraft\_controls | Recraft Controls | Additional controls like colors        |

### Output

| Output | Type  | Description           |
| ------ | ----- | --------------------- |
| IMAGE  | image | Modified image result |

## Source Code

\[Node source code (Updated on 2025-05-03)]

```python  theme={null}
class RecraftImageInpaintingNode:
    """
    Modify image based on prompt and mask.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (IO.IMAGE, ),
                "mask": (IO.MASK, ),
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation.",
                    },
                ),
                "n": (
                    IO.INT,
                    {
                        "default": 1,
                        "min": 1,
                        "max": 6,
                        "tooltip": "The number of images to generate.",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {
                "recraft_style": (RecraftIO.STYLEV3,),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "An optional text description of undesired elements on an image.",
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        image: torch.Tensor,
        mask: torch.Tensor,
        prompt: str,
        n: int,
        seed,
        auth_token=None,
        recraft_style: RecraftStyle = None,
        negative_prompt: str = None,
        **kwargs,
    ):
        default_style = RecraftStyle(RecraftStyleV3.realistic_image)
        if recraft_style is None:
            recraft_style = default_style

        if not negative_prompt:
            negative_prompt = None

        request = RecraftImageGenerationRequest(
            prompt=prompt,
            negative_prompt=negative_prompt,
            model=RecraftModel.recraftv3,
            n=n,
            style=recraft_style.style,
            substyle=recraft_style.substyle,
            style_id=recraft_style.style_id,
            random_seed=seed,
        )

        # prepare mask tensor
        _, H, W, _ = image.shape
        mask = mask.unsqueeze(-1)
        mask = mask.movedim(-1,1)
        mask = common_upscale(mask, width=W, height=H, upscale_method="nearest-exact", crop="disabled")
        mask = mask.movedim(1,-1)
        mask = (mask > 0.5).float()

        images = []
        total = image.shape[0]
        pbar = ProgressBar(total)
        for i in range(total):
            sub_bytes = handle_recraft_file_request(
                image=image[i],
                mask=mask[i:i+1],
                path="/proxy/recraft/images/inpaint",
                request=request,
                auth_token=auth_token,
            )
            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))
            pbar.update(1)

        images_tensor = torch.cat(images, dim=0)
        return (images_tensor, )
```


# Recraft Image to Image - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/image/recraft/recraft-image-to-image

A Recraft Partner node that generates new images based on text prompts and reference images

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-image-to-image.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=80e1a35d96e8943dee2cd18151db2db4" alt="ComfyUI Native Recraft Image to Image Node" data-og-width="1731" width="1731" data-og-height="1208" height="1208" data-path="images/built-in-nodes/api_nodes/recraft/recraft-image-to-image.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-image-to-image.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=4ee37ccf9a9b15ccb7877da37ba6f7be 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-image-to-image.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=6551c1942eb8dc2ac83446307092cfa5 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-image-to-image.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=364f5d3e4f8dc1027cb8893861b6670f 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-image-to-image.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=3d872985bb0b4d5c4625c527de14bce5 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-image-to-image.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=f8e2b5a9a6ed620b1b169f13b0eecabe 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-image-to-image.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=f53e962ba4484ebbd775d60a3306756c 2500w" />

The Recraft Image to Image node uses Recraft's API to generate new images based on a reference image and text prompts.

## Parameters

### Basic Parameters

| Parameter | Type    | Default | Description                              |
| --------- | ------- | ------- | ---------------------------------------- |
| image     | image   | -       | Reference image input                    |
| prompt    | string  | ""      | Text description for the generated image |
| n         | integer | 1       | Number of images to generate (1-6)       |
| seed      | integer | 0       | Random seed value                        |

### Optional Parameters

| Parameter         | Type             | Description                           |
| ----------------- | ---------------- | ------------------------------------- |
| recraft\_style    | Recraft Style    | Style settings for generated images   |
| negative\_prompt  | string           | Elements to avoid in generated images |
| recraft\_controls | Recraft Controls | Additional controls like colors       |

### Output

| Output | Type  | Description            |
| ------ | ----- | ---------------------- |
| IMAGE  | image | Generated image result |

## Source Code

\[Node source code (Updated on 2025-05-03)]

```python  theme={null}

class RecraftImageToImageNode:
    """
    Modify image based on prompt and strength.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (IO.IMAGE, ),
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation.",
                    },
                ),
                "n": (
                    IO.INT,
                    {
                        "default": 1,
                        "min": 1,
                        "max": 6,
                        "tooltip": "The number of images to generate.",
                    },
                ),
                "strength": (
                    IO.FLOAT,
                    {
                        "default": 0.5,
                        "min": 0.0,
                        "max": 1.0,
                        "step": 0.01,
                        "tooltip": "Defines the difference with the original image, should lie in [0, 1], where 0 means almost identical, and 1 means miserable similarity."
                    }
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {
                "recraft_style": (RecraftIO.STYLEV3,),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "An optional text description of undesired elements on an image.",
                    },
                ),
                "recraft_controls": (
                    RecraftIO.CONTROLS,
                    {
                        "tooltip": "Optional additional controls over the generation via the Recraft Controls node."
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        image: torch.Tensor,
        prompt: str,
        n: int,
        strength: float,
        seed,
        auth_token=None,
        recraft_style: RecraftStyle = None,
        negative_prompt: str = None,
        recraft_controls: RecraftControls = None,
        **kwargs,
    ):
        default_style = RecraftStyle(RecraftStyleV3.realistic_image)
        if recraft_style is None:
            recraft_style = default_style

        controls_api = None
        if recraft_controls:
            controls_api = recraft_controls.create_api_model()

        if not negative_prompt:
            negative_prompt = None

        request = RecraftImageGenerationRequest(
            prompt=prompt,
            negative_prompt=negative_prompt,
            model=RecraftModel.recraftv3,
            n=n,
            strength=round(strength, 2),
            style=recraft_style.style,
            substyle=recraft_style.substyle,
            style_id=recraft_style.style_id,
            controls=controls_api,
            random_seed=seed,
        )

        images = []
        total = image.shape[0]
        pbar = ProgressBar(total)
        for i in range(total):
            sub_bytes = handle_recraft_file_request(
                image=image[i],
                path="/proxy/recraft/images/imageToImage",
                request=request,
                auth_token=auth_token,
            )
            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))
            pbar.update(1)

        images_tensor = torch.cat(images, dim=0)
        return (images_tensor, )
```


# Recraft Remove Background - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/image/recraft/recraft-remove-background

A Recraft Partner node that automatically removes image backgrounds and creates transparent alpha channels

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-remove-background.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=1248f93a0a34044b97c8d43a384fcf42" alt="ComfyUI Native Recraft Remove Background Node" data-og-width="1506" width="1506" data-og-height="576" height="576" data-path="images/built-in-nodes/api_nodes/recraft/recraft-remove-background.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-remove-background.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=022060f5697acf3ab310425a4df21961 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-remove-background.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=6ce54c49ab75a6de6cad82eab3b39452 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-remove-background.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=0591c2f04665f3c9081ec6567a8c9cc5 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-remove-background.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=cfa052abab198fb5ad0a627b28918c78 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-remove-background.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=86a0c5382a198f5d3d9ed6cea471093d 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-remove-background.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=3967b3ea3c785716274d6d2c1a0acd04 2500w" />

The Recraft Remove Background node uses Recraft's API to intelligently detect and remove image backgrounds, creating images with transparent backgrounds and corresponding alpha masks.

## Parameters

### Basic Parameters

| Parameter | Type  | Default | Description                           |
| --------- | ----- | ------- | ------------------------------------- |
| image     | image | -       | Input image to remove background from |

### Output

| Output | Type  | Description                                          |
| ------ | ----- | ---------------------------------------------------- |
| IMAGE  | image | Image with background removed (with alpha channel)   |
| MASK   | mask  | Mask of the main subject (white areas are preserved) |

## Source Code

\[Node source code (Updated on 2025-05-03)]

```python  theme={null}
class RecraftRemoveBackgroundNode:
    """
    Remove background from image, and return processed image and mask.
    """

    RETURN_TYPES = (IO.IMAGE, IO.MASK)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (IO.IMAGE, ),
            },
            "optional": {
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        image: torch.Tensor,
        auth_token=None,
        **kwargs,
    ):
        images = []
        total = image.shape[0]
        pbar = ProgressBar(total)
        for i in range(total):
            sub_bytes = handle_recraft_file_request(
                image=image[i],
                path="/proxy/recraft/images/removeBackground",
                auth_token=auth_token,
            )
            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))
            pbar.update(1)

        images_tensor = torch.cat(images, dim=0)
        # use alpha channel as masks, in B,H,W format
        masks_tensor = images_tensor[:,:,:,-1:].squeeze(-1)
        return (images_tensor, masks_tensor)

```


# Recraft Replace Background - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/image/recraft/recraft-replace-background

A Recraft Partner node that automatically detects foreground subjects and replaces backgrounds

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-replace-background.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=798d76cef5bcc224546b066e8a9b2db2" alt="ComfyUI Native Recraft Replace Background Node" data-og-width="1625" width="1625" data-og-height="1158" height="1158" data-path="images/built-in-nodes/api_nodes/recraft/recraft-replace-background.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-replace-background.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=517a4b80ea434c8d62f20b0d1ff48a5d 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-replace-background.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=085a235f7d2f230c840e6458685ce85d 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-replace-background.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=49a62e636876eba8670f60e71f9e129c 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-replace-background.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=66af47ca9c4dad461dbce9ff2ab93702 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-replace-background.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=564fe13233e518e89815e6393296f423 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-replace-background.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=c0ea9a57f6f4e7efea60820441a4291b 2500w" />

The Recraft Replace Background node uses Recraft's API to intelligently detect subjects in images and generate new backgrounds based on text prompts.

## Parameters

### Basic Parameters

| Parameter | Type    | Default | Description                           |
| --------- | ------- | ------- | ------------------------------------- |
| image     | image   | -       | Input image with subject to preserve  |
| prompt    | string  | ""      | Text prompt for background generation |
| n         | integer | 1       | Number of images to generate (1-6)    |
| seed      | integer | 0       | Random seed value for node re-runs    |

### Optional Parameters

| Parameter        | Type          | Description                              |
| ---------------- | ------------- | ---------------------------------------- |
| recraft\_style   | Recraft Style | Style settings for background generation |
| negative\_prompt | string        | Text describing elements to avoid        |

### Output

| Output | Type  | Description                          |
| ------ | ----- | ------------------------------------ |
| IMAGE  | image | Final image with replaced background |

## Source Code

\[Node source code (Updated on 2025-05-03)]

```python  theme={null}

class RecraftReplaceBackgroundNode:
    """
    Replace background on image, based on provided prompt.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (IO.IMAGE, ),
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation.",
                    },
                ),
                "n": (
                    IO.INT,
                    {
                        "default": 1,
                        "min": 1,
                        "max": 6,
                        "tooltip": "The number of images to generate.",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {
                "recraft_style": (RecraftIO.STYLEV3,),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "An optional text description of undesired elements on an image.",
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        image: torch.Tensor,
        prompt: str,
        n: int,
        seed,
        auth_token=None,
        recraft_style: RecraftStyle = None,
        negative_prompt: str = None,
        **kwargs,
    ):
        default_style = RecraftStyle(RecraftStyleV3.realistic_image)
        if recraft_style is None:
            recraft_style = default_style

        if not negative_prompt:
            negative_prompt = None

        request = RecraftImageGenerationRequest(
            prompt=prompt,
            negative_prompt=negative_prompt,
            model=RecraftModel.recraftv3,
            n=n,
            style=recraft_style.style,
            substyle=recraft_style.substyle,
            style_id=recraft_style.style_id,
        )

        images = []
        total = image.shape[0]
        pbar = ProgressBar(total)
        for i in range(total):
            sub_bytes = handle_recraft_file_request(
                image=image[i],
                path="/proxy/recraft/images/replaceBackground",
                request=request,
                auth_token=auth_token,
            )
            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))
            pbar.update(1)

        images_tensor = torch.cat(images, dim=0)
        return (images_tensor, )

```


# Recraft Style - Digital Illustration - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/image/recraft/recraft-style-digital-illustration

A helper node for setting digital illustration style in Recraft image generation

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-style-digital-illustraion.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=dce25f2de84056b2fe15b3264ffcf80e" alt="ComfyUI Native Recraft Style Digital Illustration Node" data-og-width="1506" width="1506" data-og-height="559" height="559" data-path="images/built-in-nodes/api_nodes/recraft/recraft-style-digital-illustraion.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-style-digital-illustraion.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=1b7aff4378f2ff6fbc49b9c6ac03b611 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-style-digital-illustraion.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=04ea86ec8c3929ef84a3a9ad377e858a 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-style-digital-illustraion.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=716dbb9bc5b09f88fee46700a1e03474 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-style-digital-illustraion.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=d1c579f74e4ff39e9cd00f0fdd46b46f 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-style-digital-illustraion.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=32878d0e6f167d30cb50e2d405cb73ec 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-style-digital-illustraion.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=e512147ed8a424cf24887bb4b0125db5 2500w" />

This node creates a style configuration object that guides Recraft's image generation process towards a digital illustration look.

## Parameters

### Basic Parameters

| Parameter | Type   | Default | Description                               |
| --------- | ------ | ------- | ----------------------------------------- |
| substyle  | select | None    | Specific substyle of digital illustration |

### Output

| Output         | Type          | Description                                                |
| -------------- | ------------- | ---------------------------------------------------------- |
| recraft\_style | Recraft Style | Style config object to connect to Recraft generation nodes |

## Usage Example

<Card title="Recraft Text to Image Workflow Example" icon="book" href="/tutorials/partner-nodes/recraft/recraft-text-to-image">
  Recraft Text to Image Workflow Example
</Card>

## Source Code

\[Node source code (Updated on 2025-05-03)]

```python  theme={null}
class RecraftStyleV3DigitalIllustrationNode(RecraftStyleV3RealisticImageNode):
    """
    Select digital_illustration style and optional substyle.
    """

    RECRAFT_STYLE = RecraftStyleV3.digital_illustration

```


# Recraft Style - Logo Raster - ComfyUI Built-in Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/image/recraft/recraft-style-logo-raster

Helper node for setting logo raster style in Recraft image generation

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-style-logo-raster.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=ec7753b1c7e871abc94ce5ea90472301" alt="ComfyUI Built-in Recraft Style - Logo Raster Node" data-og-width="1506" width="1506" data-og-height="559" height="559" data-path="images/built-in-nodes/api_nodes/recraft/recraft-style-logo-raster.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-style-logo-raster.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=0832354a50cd4765a6a4f992a3d26a36 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-style-logo-raster.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=59b35d3a5f4177c426f1d46207bbf081 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-style-logo-raster.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=326dfef3cde71e431ea62b58cc23828f 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-style-logo-raster.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=74597c93195f41a6960bee3d9a7ef7d2 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-style-logo-raster.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=7b457d9b4a563b22d5c810e400a122ac 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-style-logo-raster.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=0b41eba69cccc600e8e3bfe45e3c3c0d 2500w" />

This node creates a style configuration object that guides Recraft's image generation process toward professional logo design effects. By selecting different substyles, you can define the design style, complexity and use cases of the generated logo.

## Parameters

### Basic Parameters

| Parameter | Type      | Default | Description                                  |
| --------- | --------- | ------- | -------------------------------------------- |
| substyle  | Selection | -       | Specific substyle for logo raster (Required) |

### Output

| Output         | Type          | Description                                                     |
| -------------- | ------------- | --------------------------------------------------------------- |
| recraft\_style | Recraft Style | Style configuration object, connects to Recraft generation node |

## Usage Example

<Card title="Recraft Text to Image Workflow Example" icon="book" href="/tutorials/partner-nodes/recraft/recraft-text-to-image">
  Recraft Text to Image Workflow Example
</Card>

## Source Code

\[Node Source Code (Updated 2025-05-03)]

```python  theme={null}
class RecraftStyleV3LogoRasterNode(RecraftStyleV3RealisticImageNode):
    """
    Select vector_illustration style and optional substyle.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "substyle": (get_v3_substyles(s.RECRAFT_STYLE, include_none=False),),
            }
        }

    RECRAFT_STYLE = RecraftStyleV3.logo_raster
```


# Recraft Style - Realistic Image - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/image/recraft/recraft-style-realistic-image

A helper node for setting realistic photo style in Recraft image generation

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-style-realistic-image.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=d841ce2d09abd4dd25d467f459f3734f" alt="ComfyUI Native Recraft Style - Realistic Image Node" data-og-width="1506" width="1506" data-og-height="596" height="596" data-path="images/built-in-nodes/api_nodes/recraft/recraft-style-realistic-image.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-style-realistic-image.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=a64f8338a4dc2d541f82729051be06eb 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-style-realistic-image.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=ebb48df01dcd01b3510f54a48117e837 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-style-realistic-image.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=0d724b58b204c8f17055b4db1dbf9dbc 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-style-realistic-image.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=f95e20e4b19070f5eae88e2d13d62982 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-style-realistic-image.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=bb647eba9abbbb0d79cb234affde4696 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-style-realistic-image.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=4384ca703b9cc5f99c4e2aed5d437d4e 2500w" />

The Recraft Style - Realistic Image node helps set up realistic photo styles for Recraft image generation, with various substyle options to control the visual characteristics of generated images.

## Node Function

This node creates a style configuration object that guides Recraft's image generation process towards realistic photo effects.

## Parameters

### Basic Parameters

| Parameter | Type   | Default | Description                                     |
| --------- | ------ | ------- | ----------------------------------------------- |
| substyle  | select | None    | Specific substyle of realistic photo (Required) |

### Output

| Output         | Type          | Description                                                |
| -------------- | ------------- | ---------------------------------------------------------- |
| recraft\_style | Recraft Style | Style config object to connect to Recraft generation nodes |

## Usage Example

<Card title="Recraft Text to Image Workflow Example" icon="book" href="/tutorials/partner-nodes/recraft/recraft-text-to-image">
  Recraft Text to Image Workflow Example
</Card>

## Source Code

\[Node source code (Updated on 2025-05-03)]

```python  theme={null}

class RecraftStyleV3RealisticImageNode:
    """
    Select realistic_image style and optional substyle.
    """

    RETURN_TYPES = (RecraftIO.STYLEV3,)
    RETURN_NAMES = ("recraft_style",)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "create_style"
    CATEGORY = "api node/image/Recraft"

    RECRAFT_STYLE = RecraftStyleV3.realistic_image

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "substyle": (get_v3_substyles(s.RECRAFT_STYLE),),
            }
        }

    def create_style(self, substyle: str):
        if substyle == "None":
            substyle = None
        return (RecraftStyle(self.RECRAFT_STYLE, substyle),)

```


# Recraft Text to Image - ComfyUI Built-in Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/image/recraft/recraft-text-to-image

A Recraft Partner node that generates high-quality images from text descriptions

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-text-to-image.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=0cb81000a29e5fadef2c0e5e164ca60f" alt="ComfyUI Built-in Recraft Text to Image Node" data-og-width="1731" width="1731" data-og-height="1138" height="1138" data-path="images/built-in-nodes/api_nodes/recraft/recraft-text-to-image.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-text-to-image.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=9f82afc139628c1a109950e976d0dea6 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-text-to-image.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=d7661bde04e3af82c067e07706982a97 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-text-to-image.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=4368ef74c90010ff2a46058bd0d08a58 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-text-to-image.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=9564343bb32ef404cbc57fe069b3c06e 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-text-to-image.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=bf084b9a7ded85168483158941b471ed 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-text-to-image.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=ac7bffe9d2d364da06aa6af84a75edad 2500w" />

The Recraft Text to Image node lets you generate high-quality images from text prompts by directly connecting to Recraft AI's image generation API to create images in various styles.

## Parameters

### Basic Parameters

| Parameter | Type   | Default   | Description                    |
| --------- | ------ | --------- | ------------------------------ |
| prompt    | string | ""        | Text description for the image |
| size      | select | 1024x1024 | Output image size              |
| n         | int    | 1         | Number of images (1-6)         |
| seed      | int    | 0         | Random seed value              |

### Optional Parameters

| Parameter         | Type             | Description                                       |
| ----------------- | ---------------- | ------------------------------------------------- |
| recraft\_style    | Recraft Style    | Image style setting, default is "realistic photo" |
| negative\_prompt  | string           | Elements to exclude from generation               |
| recraft\_controls | Recraft Controls | Additional control parameters (colors, etc.)      |

### Output

| Output | Type  | Description        |
| ------ | ----- | ------------------ |
| IMAGE  | image | Generated image(s) |

## Source Code

\[Node source code (Updated on 2025-05-03)]

```python  theme={null}
class RecraftTextToImageNode:
    """
    Generates images synchronously based on prompt and resolution.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation.",
                    },
                ),
                "size": (
                    [res.value for res in RecraftImageSize],
                    {
                        "default": RecraftImageSize.res_1024x1024,
                        "tooltip": "The size of the generated image.",
                    },
                ),
                "n": (
                    IO.INT,
                    {
                        "default": 1,
                        "min": 1,
                        "max": 6,
                        "tooltip": "The number of images to generate.",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {
                "recraft_style": (RecraftIO.STYLEV3,),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "An optional text description of undesired elements on an image.",
                    },
                ),
                "recraft_controls": (
                    RecraftIO.CONTROLS,
                    {
                        "tooltip": "Optional additional controls over the generation via the Recraft Controls node."
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        prompt: str,
        size: str,
        n: int,
        seed,
        recraft_style: RecraftStyle = None,
        negative_prompt: str = None,
        recraft_controls: RecraftControls = None,
        auth_token=None,
        **kwargs,
    ):
        default_style = RecraftStyle(RecraftStyleV3.realistic_image)
        if recraft_style is None:
            recraft_style = default_style

        controls_api = None
        if recraft_controls:
            controls_api = recraft_controls.create_api_model()

        if not negative_prompt:
            negative_prompt = None

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/recraft/image_generation",
                method=HttpMethod.POST,
                request_model=RecraftImageGenerationRequest,
                response_model=RecraftImageGenerationResponse,
            ),
            request=RecraftImageGenerationRequest(
                prompt=prompt,
                negative_prompt=negative_prompt,
                model=RecraftModel.recraftv3,
                size=size,
                n=n,
                style=recraft_style.style,
                substyle=recraft_style.substyle,
                style_id=recraft_style.style_id,
                controls=controls_api,
            ),
            auth_token=auth_token,
        )
        response: RecraftImageGenerationResponse = operation.execute()
        images = []
        for data in response.data:
            image = bytesio_to_image_tensor(
                download_url_to_bytesio(data.url, timeout=1024)
            )
            if len(image.shape) < 4:
                image = image.unsqueeze(0)
            images.append(image)
        output_image = torch.cat(images, dim=0)

        return (output_image,)
```


# Recraft Text to Vector - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/image/recraft/recraft-text-to-vector

A Recraft Partner node that generates scalable vector graphics from text descriptions

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-text-to-vector.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=868aafbbe8b41edf218df85bd0f17fa0" alt="ComfyUI Native Recraft Text to Vector Node" data-og-width="1731" width="1731" data-og-height="1148" height="1148" data-path="images/built-in-nodes/api_nodes/recraft/recraft-text-to-vector.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-text-to-vector.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=e3d6ec2992615565923dbbb36f60a251 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-text-to-vector.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=df217a184ae2c7a423de51f13a2b2086 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-text-to-vector.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=83e13573fcb5bcbb12d95c720535f17e 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-text-to-vector.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=99c07d85409f33603a5c68e946adff98 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-text-to-vector.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=824ddf41c71681d5b59fd05b2e8e72b3 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-text-to-vector.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=e8903d5001d3eca5eb633173e514390c 2500w" />

The Recraft Text to Vector node lets you create high-quality vector graphics (SVG format) from text descriptions using Recraft's API. It's perfect for making logos, icons, and infinitely scalable illustrations.

## Parameters

### Basic Parameters

| Parameter | Type    | Default   | Description                                        |
| --------- | ------- | --------- | -------------------------------------------------- |
| prompt    | string  | ""        | Text description of the vector graphic to generate |
| substyle  | select  | -         | Vector style subtype                               |
| size      | select  | 1024x1024 | Canvas size for the vector output                  |
| n         | integer | 1         | Number of results to generate (1-6)                |
| seed      | integer | 0         | Random seed value                                  |

### Optional Parameters

| Parameter         | Type             | Description                                  |
| ----------------- | ---------------- | -------------------------------------------- |
| negative\_prompt  | string           | Elements to exclude from generation          |
| recraft\_controls | Recraft Controls | Additional control parameters (colors, etc.) |

### Output

| Output | Type   | Description                                                   |
| ------ | ------ | ------------------------------------------------------------- |
| SVG    | vector | Generated SVG vector graphic, connect to SaveSVG node to save |

## Source Code

\[Node source code (Updated on 2025-05-03)]

```python  theme={null}

class RecraftTextToVectorNode:
    """
    Generates SVG synchronously based on prompt and resolution.
    """

    RETURN_TYPES = (RecraftIO.SVG,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation.",
                    },
                ),
                "substyle": (get_v3_substyles(RecraftStyleV3.vector_illustration),),
                "size": (
                    [res.value for res in RecraftImageSize],
                    {
                        "default": RecraftImageSize.res_1024x1024,
                        "tooltip": "The size of the generated image.",
                    },
                ),
                "n": (
                    IO.INT,
                    {
                        "default": 1,
                        "min": 1,
                        "max": 6,
                        "tooltip": "The number of images to generate.",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "An optional text description of undesired elements on an image.",
                    },
                ),
                "recraft_controls": (
                    RecraftIO.CONTROLS,
                    {
                        "tooltip": "Optional additional controls over the generation via the Recraft Controls node."
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        prompt: str,
        substyle: str,
        size: str,
        n: int,
        seed,
        negative_prompt: str = None,
        recraft_controls: RecraftControls = None,
        auth_token=None,
        **kwargs,
    ):
        # create RecraftStyle so strings will be formatted properly (i.e. "None" will become None)
        recraft_style = RecraftStyle(RecraftStyleV3.vector_illustration, substyle=substyle)

        controls_api = None
        if recraft_controls:
            controls_api = recraft_controls.create_api_model()

        if not negative_prompt:
            negative_prompt = None

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/recraft/image_generation",
                method=HttpMethod.POST,
                request_model=RecraftImageGenerationRequest,
                response_model=RecraftImageGenerationResponse,
            ),
            request=RecraftImageGenerationRequest(
                prompt=prompt,
                negative_prompt=negative_prompt,
                model=RecraftModel.recraftv3,
                size=size,
                n=n,
                style=recraft_style.style,
                substyle=recraft_style.substyle,
                controls=controls_api,
            ),
            auth_token=auth_token,
        )
        response: RecraftImageGenerationResponse = operation.execute()
        svg_data = []
        for data in response.data:
            svg_data.append(download_url_to_bytesio(data.url, timeout=1024))

        return (SVG(svg_data),)
```


# Recraft Vectorize Image - ComfyUI Built-in Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/image/recraft/recraft-vectorize-image

A Recraft Partner node that converts raster images to vector SVG format

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-vectorize-image.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=5d51a1b8e3e0147a608b40a2b63fd03b" alt="ComfyUI Built-in Recraft Vectorize Image Node" data-og-width="1506" width="1506" data-og-height="532" height="532" data-path="images/built-in-nodes/api_nodes/recraft/recraft-vectorize-image.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-vectorize-image.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=23fbfb9abc4484081911d9429b7189c8 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-vectorize-image.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=be602747c0232c39af30f0f3eaf6e4dc 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-vectorize-image.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=c54d33accece97bb0eb1ceb2684c59a2 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-vectorize-image.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=63a99715430be89ccc99c8d180e70ae9 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-vectorize-image.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=ef44ee8a4f8a61c60adf74f27921888e 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/recraft-vectorize-image.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=d10dd46c68bd11dddcb3042ba5a8da38 2500w" />

The Recraft Vectorize Image node uses Recraft's API to convert raster images (like photos, PNGs or JPEGs) into vector SVG format.

## Parameters

### Basic Parameters

| Parameter | Type  | Default | Description                           |
| --------- | ----- | ------- | ------------------------------------- |
| image     | Image | -       | Input image to be converted to vector |

### Output

| Output | Type   | Description                                                                 |
| ------ | ------ | --------------------------------------------------------------------------- |
| SVG    | Vector | Converted SVG vector graphic, needs to be connected to SaveSVG node to save |

## Usage Example

<Card title="Recraft Text to Image Workflow Example" icon="book" href="/tutorials/partner-nodes/recraft/recraft-text-to-image">
  Recraft Text to Image Workflow Example
</Card>

## Source Code

\[Node Source Code (Updated 2025-05-03)]

```python  theme={null}

class RecraftVectorizeImageNode:
    """
    Generates SVG synchronously from an input image.
    """

    RETURN_TYPES = (RecraftIO.SVG,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (IO.IMAGE, ),
            },
            "optional": {
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        image: torch.Tensor,
        auth_token=None,
        **kwargs,
    ):
        svgs = []
        total = image.shape[0]
        pbar = ProgressBar(total)
        for i in range(total):
            sub_bytes = handle_recraft_file_request(
                image=image[i],
                path="/proxy/recraft/images/vectorize",
                auth_token=auth_token,
            )
            svgs.append(SVG(sub_bytes))
            pbar.update(1)

        return (SVG.combine_all(svgs), )

```


# Save SVG - ComfyUI Built-in Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/image/recraft/save-svg

A utility node for saving SVG vector graphics to files

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/save-svg.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=f9f08f55c06ee2d300d66cd2e82194f0" alt="ComfyUI Built-in Save SVG Node" data-og-width="1506" width="1506" data-og-height="556" height="556" data-path="images/built-in-nodes/api_nodes/recraft/save-svg.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/save-svg.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=a3878f79b90b8979284536dc5d50bc52 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/save-svg.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=2ce3ad09054602a333359f425dfe4277 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/save-svg.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=f1907a36c4a5aae2b816a9ee898a5240 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/save-svg.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=627818b10357ef4862b4944c9d65cfe1 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/save-svg.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=8a1b3c855d5e179dd58fd2ff389b88a9 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/recraft/save-svg.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=02bfe0b698fa84d7b0b711729f8fe870 2500w" />

The Save SVG node allows you to save SVG data from Recraft vector generation nodes as usable files in the filesystem. This is an essential component for handling and exporting vector graphics.

## Node Function

This node receives SVG vector data and saves it as standard SVG files in the filesystem. It supports automatic file naming and save path specification, allowing vector graphics to be opened and edited in other software.

## Parameters

### Basic Parameters

| Parameter        | Type    | Default   | Description                                                                  |
| ---------------- | ------- | --------- | ---------------------------------------------------------------------------- |
| svg              | SVG     | -         | SVG vector data to save                                                      |
| filename\_prefix | string  | "recraft" | Prefix for the filename                                                      |
| output\_dir      | string  | -         | Output directory, defaults to ComfyUI output folder at `ComfyUI/output/svg/` |
| index            | integer | -1        | Save index, -1 means save all generated SVGs                                 |

### Output

| Output | Type | Description                       |
| ------ | ---- | --------------------------------- |
| SVG    | SVG  | Passes through the input SVG data |

## Usage Example

<Card title="Recraft Text to Image Workflow Example" icon="book" href="/tutorials/partner-nodes/recraft/recraft-text-to-image">
  Recraft Text to Image Workflow Example
</Card>

## Source Code

\[Node Source Code (Updated 2025-05-03)]

```python  theme={null}
class SaveSVGNode:
    """
    Save SVG files on disk.
    """

    def __init__(self):
        self.output_dir = folder_paths.get_output_directory()
        self.type = "output"
        self.prefix_append = ""

    RETURN_TYPES = ()
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "save_svg"
    CATEGORY = "api node/image/Recraft"
    OUTPUT_NODE = True

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "svg": (RecraftIO.SVG,),
                "filename_prefix": ("STRING", {"default": "svg/ComfyUI", "tooltip": "The prefix for the file to save. This may include formatting information such as %date:yyyy-MM-dd% or %Empty Latent Image.width% to include values from nodes."})
            },
            "hidden": {
                "prompt": "PROMPT",
                "extra_pnginfo": "EXTRA_PNGINFO"
            }
        }

    def save_svg(self, svg: SVG, filename_prefix="svg/ComfyUI", prompt=None, extra_pnginfo=None):
        filename_prefix += self.prefix_append
        full_output_folder, filename, counter, subfolder, filename_prefix = folder_paths.get_save_image_path(filename_prefix, self.output_dir)
        results = list()

        # Prepare metadata JSON
        metadata_dict = {}
        if prompt is not None:
            metadata_dict["prompt"] = prompt
        if extra_pnginfo is not None:
            metadata_dict.update(extra_pnginfo)

        # Convert metadata to JSON string
        metadata_json = json.dumps(metadata_dict, indent=2) if metadata_dict else None

        for batch_number, svg_bytes in enumerate(svg.data):
            filename_with_batch_num = filename.replace("%batch_num%", str(batch_number))
            file = f"{filename_with_batch_num}_{counter:05}_.svg"

            # Read SVG content
            svg_bytes.seek(0)
            svg_content = svg_bytes.read().decode('utf-8')

            # Inject metadata if available
            if metadata_json:
                # Create metadata element with CDATA section
                metadata_element = f"""  <metadata>
    <![CDATA[
{metadata_json}
    ]]>
  </metadata>
"""
                # Insert metadata after opening svg tag using regex
                import re
                svg_content = re.sub(r'(<svg[^>]*>)', r'\1\n' + metadata_element, svg_content)

            # Write the modified SVG to file
            with open(os.path.join(full_output_folder, file), 'wb') as svg_file:
                svg_file.write(svg_content.encode('utf-8'))

            results.append({
                "filename": file,
                "subfolder": subfolder,
                "type": self.type
            })
            counter += 1
        return { "ui": { "images": results } }

```


# Stability AI Stable Diffusion 3.5 - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image

A node that generates high-quality images using Stability AI Stable Diffusion 3.5 model

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/stability-ai/stability-ai-stable-image-sd-3-5.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=535150f9957846dbd63d16c62e9de695" alt="ComfyUI Native Stability AI Stable Diffusion 3.5 Node" data-og-width="1731" width="1731" data-og-height="1332" height="1332" data-path="images/built-in-nodes/api_nodes/stability-ai/stability-ai-stable-image-sd-3-5.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/stability-ai/stability-ai-stable-image-sd-3-5.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=d1a49ecbfa9f49b4ac66408422974a37 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/stability-ai/stability-ai-stable-image-sd-3-5.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=b2d60eba5c6ce54f51c96826505cab45 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/stability-ai/stability-ai-stable-image-sd-3-5.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=6e2616bd4d7d938526b450be1abfbcb8 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/stability-ai/stability-ai-stable-image-sd-3-5.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=bf52f7b97c394cc9f29c3a3cc5be680f 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/stability-ai/stability-ai-stable-image-sd-3-5.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=41c37aeb008c30367878026fe880eda2 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/stability-ai/stability-ai-stable-image-sd-3-5.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=b699433ce2e188c5101ea18b53f56d44 2500w" />

The Stability AI Stable Diffusion 3.5 Image node uses Stability AI's Stable Diffusion 3.5 API to generate high-quality images. It supports both text-to-image and image-to-image generation, capable of creating detailed visual content from text prompts.

## Parameters

### Required Parameters

| Parameter     | Type    | Default | Description                                                                                                                                       |
| ------------- | ------- | ------- | ------------------------------------------------------------------------------------------------------------------------------------------------- |
| prompt        | string  | ""      | What you want to see in the output image. Strong, descriptive prompts that clearly define elements, colors and themes will yield better results   |
| model         | select  | -       | Choose which Stability SD 3.5 model to use                                                                                                        |
| aspect\_ratio | select  | "1:1"   | Width to height ratio of generated image                                                                                                          |
| style\_preset | select  | "None"  | Optional preset style for the desired image                                                                                                       |
| cfg\_scale    | float   | 4.0     | How strictly the diffusion process adheres to the prompt text (higher values keep your image closer to your prompt). Range: 1.0 - 10.0, Step: 0.1 |
| seed          | integer | 0       | Random seed for noise generation (0-4294967294)                                                                                                   |

### Optional Parameters

| Parameter        | Type   | Default | Description                                                                                                                                                                             |
| ---------------- | ------ | ------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| image            | image  | -       | Input image. When provided, the node switches to image-to-image mode                                                                                                                    |
| negative\_prompt | string | ""      | Keywords of what you don't want to see in the output image. This is an advanced feature                                                                                                 |
| image\_denoise   | float  | 0.5     | Denoising strength for input image. 0.0 yields image identical to input, 1.0 is as if no image was provided at all. Range: 0.0 - 1.0, Step: 0.01. Only effective when image is provided |

### Output

| Output | Type  | Description     |
| ------ | ----- | --------------- |
| IMAGE  | image | Generated image |

## Usage Example

<Card title="Stability AI Stable Diffusion 3.5 Image Workflow Example" icon="book" href="/tutorials/partner-nodes/stability-ai/stable-diffusion-3-5-image">
  Stability AI Stable Diffusion 3.5 Image Workflow Example
</Card>

## Notes

* When an input image is provided, the node switches from text-to-image mode to image-to-image mode
* In image-to-image mode, aspect ratio parameters are ignored
* Mode selection automatically switches based on whether an image is provided:
  * No image provided: text-to-image mode
  * Image provided: image-to-image mode
* If style\_preset is set to "None", no preset style will be applied

## Source Code

\[Node source code (Updated on 2025-05-07)]

```python  theme={null}
class StabilityStableImageSD_3_5Node:
    """
    Generates images synchronously based on prompt and resolution.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Stability AI"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "What you wish to see in the output image. A strong, descriptive prompt that clearly defines elements, colors, and subjects will lead to better results."
                    },
                ),
                "model": ([x.value for x in Stability_SD3_5_Model],),
                "aspect_ratio": ([x.value for x in StabilityAspectRatio],
                    {
                        "default": StabilityAspectRatio.ratio_1_1,
                        "tooltip": "Aspect ratio of generated image.",
                    },
                ),
                "style_preset": (get_stability_style_presets(),
                    {
                        "tooltip": "Optional desired style of generated image.",
                    },
                ),
                "cfg_scale": (
                    IO.FLOAT,
                    {
                        "default": 4.0,
                        "min": 1.0,
                        "max": 10.0,
                        "step": 0.1,
                        "tooltip": "How strictly the diffusion process adheres to the prompt text (higher values keep your image closer to your prompt)",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 4294967294,
                        "control_after_generate": True,
                        "tooltip": "The random seed used for creating the noise.",
                    },
                ),
            },
            "optional": {
                "image": (IO.IMAGE,),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "Keywords of what you do not wish to see in the output image. This is an advanced feature."
                    },
                ),
                "image_denoise": (
                    IO.FLOAT,
                    {
                        "default": 0.5,
                        "min": 0.0,
                        "max": 1.0,
                        "step": 0.01,
                        "tooltip": "Denoise of input image; 0.0 yields image identical to input, 1.0 is as if no image was provided at all.",
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(self, model: str, prompt: str, aspect_ratio: str, style_preset: str, seed: int, cfg_scale: float,
                 negative_prompt: str=None, image: torch.Tensor = None, image_denoise: float=None,
                 auth_token=None):
        validate_string(prompt, strip_whitespace=False)
        # prepare image binary if image present
        image_binary = None
        mode = Stability_SD3_5_GenerationMode.text_to_image
        if image is not None:
            image_binary = tensor_to_bytesio(image, total_pixels=1504*1504).read()
            mode = Stability_SD3_5_GenerationMode.image_to_image
            aspect_ratio = None
        else:
            image_denoise = None

        if not negative_prompt:
            negative_prompt = None
        if style_preset == "None":
            style_preset = None

        files = {
            "image": image_binary
        }

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/stability/v2beta/stable-image/generate/sd3",
                method=HttpMethod.POST,
                request_model=StabilityStable3_5Request,
                response_model=StabilityStableUltraResponse,
            ),
            request=StabilityStable3_5Request(
                prompt=prompt,
                negative_prompt=negative_prompt,
                aspect_ratio=aspect_ratio,
                seed=seed,
                strength=image_denoise,
                style_preset=style_preset,
                cfg_scale=cfg_scale,
                model=model,
                mode=mode,
            ),
            files=files,
            content_type="multipart/form-data",
            auth_token=auth_token,
        )
        response_api = operation.execute()

        if response_api.finish_reason != "SUCCESS":
            raise Exception(f"Stable Diffusion 3.5 Image generation failed: {response_api.finish_reason}.")

        image_data = base64.b64decode(response_api.image)
        returned_image = bytesio_to_image_tensor(BytesIO(image_data))

        return (returned_image,)
```


# Stability Stable Image Ultra - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/image/stability-ai/stability-ai-stable-image-ultra

A node that generates high-quality images using Stability AI's ultra stable diffusion model

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/api_nodes/stability-ai/stability-ai-stable-image-ultra.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=9e4876faeb241e65b347da9ec53420b6" alt="ComfyUI Native Stability Stable Image Ultra Node" data-og-width="1731" width="1731" data-og-height="1499" height="1499" data-path="images/built-in-nodes/api_nodes/stability-ai/stability-ai-stable-image-ultra.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/api_nodes/stability-ai/stability-ai-stable-image-ultra.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=bd79cfb4090cde78bd5ad5388c36c9b3 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/api_nodes/stability-ai/stability-ai-stable-image-ultra.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=077579812d5c8c643eedf8a23f04d968 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/api_nodes/stability-ai/stability-ai-stable-image-ultra.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=6fe6deae28ddf0195d504fa88b827e96 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/api_nodes/stability-ai/stability-ai-stable-image-ultra.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=a6005153737e4859d10dc8de41beed75 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/api_nodes/stability-ai/stability-ai-stable-image-ultra.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=c08475f82b9c7fce6ff0f84d38543cdb 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/api_nodes/stability-ai/stability-ai-stable-image-ultra.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=ea0703fd121790af89fd74fb1cb7ed64 2500w" />

The Stability Stable Image Ultra node uses Stability AI's Stable Diffusion Ultra API to generate high-quality images. It supports both text-to-image and image-to-image generation, creating detailed and artistic visuals from text prompts.

## Parameters

### Required Parameters

| Parameter     | Type    | Default | Description                                                                                                                                                                                                                                                                                                               |
| ------------- | ------- | ------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| prompt        | string  | ""      | Text description of what you want to generate. Better results come from clear, descriptive prompts that specify elements, colors and themes. You can control word importance using `(word:weight)` format, where weight is 0-1. For example: `The sky was (blue:0.3) and (green:0.8)` makes the sky more green than blue. |
| aspect\_ratio | select  | "1:1"   | Width to height ratio of output image                                                                                                                                                                                                                                                                                     |
| style\_preset | select  | "None"  | Optional preset style for generated image                                                                                                                                                                                                                                                                                 |
| seed          | integer | 0       | Random seed for noise generation (0-4294967294)                                                                                                                                                                                                                                                                           |

### Optional Parameters

| Parameter        | Type   | Default | Description                                                                                                      |
| ---------------- | ------ | ------- | ---------------------------------------------------------------------------------------------------------------- |
| image            | image  | -       | Input image for image-to-image generation                                                                        |
| negative\_prompt | string | ""      | Describes what you don't want in the output image. This is an advanced feature                                   |
| image\_denoise   | float  | 0.5     | Denoising strength for input image (0.0-1.0). 0.0 keeps input image unchanged, 1.0 is like having no input image |

### Output

| Output | Type  | Description     |
| ------ | ----- | --------------- |
| IMAGE  | image | Generated image |

## Notes

* image\_denoise has no effect when no input image is provided
* No preset style is applied when style\_preset is "None"
* For image-to-image, input images are converted to the proper format before API submission

## Source Code

\[Node source code (Updated on 2025-05-03)]

```python  theme={null}

class StabilityStableImageUltraNode:
    """
    Generates images synchronously based on prompt and resolution.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/stability"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "What you wish to see in the output image. A strong, descriptive prompt that clearly defines" +
                                    "What you wish to see in the output image. A strong, descriptive prompt that clearly defines" +
                                    "elements, colors, and subjects will lead to better results. " +
                                    "To control the weight of a given word use the format `(word:weight)`," +
                                    "where `word` is the word you'd like to control the weight of and `weight`" +
                                    "is a value between 0 and 1. For example: `The sky was a crisp (blue:0.3) and (green:0.8)`" +
                                    "would convey a sky that was blue and green, but more green than blue."
                    },
                ),
                "aspect_ratio": ([x.value for x in StabilityAspectRatio],
                    {
                        "default": StabilityAspectRatio.ratio_1_1,
                        "tooltip": "Aspect ratio of generated image.",
                    },
                ),
                "style_preset": (get_stability_style_presets(),
                    {
                        "tooltip": "Optional desired style of generated image.",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 4294967294,
                        "control_after_generate": True,
                        "tooltip": "The random seed used for creating the noise.",
                    },
                ),
            },
            "optional": {
                "image": (IO.IMAGE,),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "A blurb of text describing what you do not wish to see in the output image. This is an advanced feature."
                    },
                ),
                "image_denoise": (
                    IO.FLOAT,
                    {
                        "default": 0.5,
                        "min": 0.0,
                        "max": 1.0,
                        "step": 0.01,
                        "tooltip": "Denoise of input image; 0.0 yields image identical to input, 1.0 is as if no image was provided at all.",
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(self, prompt: str, aspect_ratio: str, style_preset: str, seed: int,
                 negative_prompt: str=None, image: torch.Tensor = None, image_denoise: float=None,
                 auth_token=None):
        # prepare image binary if image present
        image_binary = None
        if image is not None:
            image_binary = tensor_to_bytesio(image, 1504 * 1504).read()
        else:
            image_denoise = None

        if not negative_prompt:
            negative_prompt = None
        if style_preset == "None":
            style_preset = None

        files = {
            "image": image_binary
        }

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/stability/v2beta/stable-image/generate/ultra",
                method=HttpMethod.POST,
                request_model=StabilityStableUltraRequest,
                response_model=StabilityStableUltraResponse,
            ),
            request=StabilityStableUltraRequest(
                prompt=prompt,
                negative_prompt=negative_prompt,
                aspect_ratio=aspect_ratio,
                seed=seed,
                strength=image_denoise,
                style_preset=style_preset,
            ),
            files=files,
            content_type="multipart/form-data",
            auth_token=auth_token,
        )
        response_api = operation.execute()

        if response_api.finish_reason != "SUCCESS":
            raise Exception(f"Stable Image Ultra generation failed: {response_api.finish_reason}.")

        image_data = base64.b64decode(response_api.image)
        returned_image = bytesio_to_image_tensor(BytesIO(image_data))

        return (returned_image,)


```


# Google Veo2 Video - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/video/google/google-veo2-video

A node that generates videos from text descriptions using Google's Veo2 technology

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/google/veo2-video-generation.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=7188b0bfb821b1fc12f9e4670a84223f" alt="ComfyUI Native Google Veo2 Video Node" data-og-width="1731" width="1731" data-og-height="1645" height="1645" data-path="images/built-in-nodes/api_nodes/google/veo2-video-generation.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/google/veo2-video-generation.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=5bb8cc72e70485b6640936c97014e43a 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/google/veo2-video-generation.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=d66ec182bc9d402ed4b15893ca416b6f 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/google/veo2-video-generation.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=9d37ce58de83d05a0adb0730cc6291cb 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/google/veo2-video-generation.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=444db725e0ab7f50be4c5a663cb0cedb 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/google/veo2-video-generation.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=ea6a5765b5f28c4793472e979146b3b5 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/google/veo2-video-generation.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=fc61f4a8faa831690268083087d2d938 2500w" />

The Google Veo2 Video node generates high-quality videos from text descriptions using Google's Veo2 API technology, converting text prompts into dynamic video content.

## Parameters

### Basic Parameters

| Parameter          | Type    | Default | Description                                          |
| ------------------ | ------- | ------- | ---------------------------------------------------- |
| prompt             | string  | ""      | Text description of the video content to generate    |
| aspect\_ratio      | select  | "16:9"  | Output video aspect ratio, "16:9" or "9:16"          |
| negative\_prompt   | string  | ""      | Text describing what to avoid in the video           |
| duration\_seconds  | integer | 5       | Video duration, 5-8 seconds                          |
| enhance\_prompt    | boolean | True    | Whether to use AI to enhance the prompt              |
| person\_generation | select  | "ALLOW" | Allow or block person generation, "ALLOW" or "BLOCK" |
| seed               | integer | 0       | Random seed, 0 means randomly generated              |

### Optional Parameters

| Parameter | Type  | Default | Description                                      |
| --------- | ----- | ------- | ------------------------------------------------ |
| image     | image | None    | Optional reference image to guide video creation |

### Output

| Output | Type  | Description     |
| ------ | ----- | --------------- |
| VIDEO  | video | Generated video |

## Source Code

\[Node Source Code (Updated 2025-05-03)]

```python  theme={null}

class VeoVideoGenerationNode(ComfyNodeABC):
    """
    Generates videos from text prompts using Google's Veo API.

    This node can create videos from text descriptions and optional image inputs,
    with control over parameters like aspect ratio, duration, and more.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Text description of the video",
                    },
                ),
                "aspect_ratio": (
                    IO.COMBO,
                    {
                        "options": ["16:9", "9:16"],
                        "default": "16:9",
                        "tooltip": "Aspect ratio of the output video",
                    },
                ),
            },
            "optional": {
                "negative_prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Negative text prompt to guide what to avoid in the video",
                    },
                ),
                "duration_seconds": (
                    IO.INT,
                    {
                        "default": 5,
                        "min": 5,
                        "max": 8,
                        "step": 1,
                        "display": "number",
                        "tooltip": "Duration of the output video in seconds",
                    },
                ),
                "enhance_prompt": (
                    IO.BOOLEAN,
                    {
                        "default": True,
                        "tooltip": "Whether to enhance the prompt with AI assistance",
                    }
                ),
                "person_generation": (
                    IO.COMBO,
                    {
                        "options": ["ALLOW", "BLOCK"],
                        "default": "ALLOW",
                        "tooltip": "Whether to allow generating people in the video",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFF,
                        "step": 1,
                        "display": "number",
                        "control_after_generate": True,
                        "tooltip": "Seed for video generation (0 for random)",
                    },
                ),
                "image": (IO.IMAGE, {
                    "default": None,
                    "tooltip": "Optional reference image to guide video generation",
                }),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    RETURN_TYPES = (IO.VIDEO,)
    FUNCTION = "generate_video"
    CATEGORY = "api node/video/Veo"
    DESCRIPTION = "Generates videos from text prompts using Google's Veo API"
    API_NODE = True

    def generate_video(
        self,
        prompt,
        aspect_ratio="16:9",
        negative_prompt="",
        duration_seconds=5,
        enhance_prompt=True,
        person_generation="ALLOW",
        seed=0,
        image=None,
        auth_token=None,
    ):
        # Prepare the instances for the request
        instances = []

        instance = {
            "prompt": prompt
        }

        # Add image if provided
        if image is not None:
            image_base64 = convert_image_to_base64(image)
            if image_base64:
                instance["image"] = {
                    "bytesBase64Encoded": image_base64,
                    "mimeType": "image/png"
                }

        instances.append(instance)

        # Create parameters dictionary
        parameters = {
            "aspectRatio": aspect_ratio,
            "personGeneration": person_generation,
            "durationSeconds": duration_seconds,
            "enhancePrompt": enhance_prompt,
        }

        # Add optional parameters if provided
        if negative_prompt:
            parameters["negativePrompt"] = negative_prompt
        if seed > 0:
            parameters["seed"] = seed

        # Initial request to start video generation
        initial_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/veo/generate",
                method=HttpMethod.POST,
                request_model=Veo2GenVidRequest,
                response_model=Veo2GenVidResponse
            ),
            request=Veo2GenVidRequest(
                instances=instances,
                parameters=parameters
            ),
            auth_token=auth_token
        )

        initial_response = initial_operation.execute()
        operation_name = initial_response.name

        logging.info(f"Veo generation started with operation name: {operation_name}")

        # Define status extractor function
        def status_extractor(response):
            # Only return "completed" if the operation is done, regardless of success or failure
            # We'll check for errors after polling completes
            return "completed" if response.done else "pending"

        # Define progress extractor function
        def progress_extractor(response):
            # Could be enhanced if the API provides progress information
            return None

        # Define the polling operation
        poll_operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path="/proxy/veo/poll",
                method=HttpMethod.POST,
                request_model=Veo2GenVidPollRequest,
                response_model=Veo2GenVidPollResponse
            ),
            completed_statuses=["completed"],
            failed_statuses=[],  # No failed statuses, we'll handle errors after polling
            status_extractor=status_extractor,
            progress_extractor=progress_extractor,
            request=Veo2GenVidPollRequest(
                operationName=operation_name
            ),
            auth_token=auth_token,
            poll_interval=5.0
        )

        # Execute the polling operation
        poll_response = poll_operation.execute()

        # Now check for errors in the final response
        # Check for error in poll response
        if hasattr(poll_response, 'error') and poll_response.error:
            error_message = f"Veo API error: {poll_response.error.message} (code: {poll_response.error.code})"
            logging.error(error_message)
            raise Exception(error_message)

        # Check for RAI filtered content
        if (hasattr(poll_response.response, 'raiMediaFilteredCount') and
            poll_response.response.raiMediaFilteredCount > 0):

            # Extract reason message if available
            if (hasattr(poll_response.response, 'raiMediaFilteredReasons') and
                poll_response.response.raiMediaFilteredReasons):
                reason = poll_response.response.raiMediaFilteredReasons[0]
                error_message = f"Content filtered by Google's Responsible AI practices: {reason} ({poll_response.response.raiMediaFilteredCount} videos filtered.)"
            else:
                error_message = f"Content filtered by Google's Responsible AI practices ({poll_response.response.raiMediaFilteredCount} videos filtered.)"

            logging.error(error_message)
            raise Exception(error_message)

        # Extract video data
        video_data = None
        if poll_response.response and hasattr(poll_response.response, 'videos') and poll_response.response.videos and len(poll_response.response.videos) > 0:
            video = poll_response.response.videos[0]

            # Check if video is provided as base64 or URL
            if hasattr(video, 'bytesBase64Encoded') and video.bytesBase64Encoded:
                # Decode base64 string to bytes
                video_data = base64.b64decode(video.bytesBase64Encoded)
            elif hasattr(video, 'gcsUri') and video.gcsUri:
                # Download from URL
                video_url = video.gcsUri
                video_response = requests.get(video_url)
                video_data = video_response.content
            else:
                raise Exception("Video returned but no data or URL was provided")
        else:
            raise Exception("Video generation completed but no video was returned")

        if not video_data:
            raise Exception("No video data was returned")

        logging.info("Video generation completed successfully")

        # Convert video data to BytesIO object
        video_io = io.BytesIO(video_data)

        # Return VideoFromFile object
        return (VideoFromFile(video_io),)

```


# Kling Image to Video (Camera Control) - ComfyUI Built-in Node
Source: https://docs.comfy.org/built-in-nodes/partner-node/video/kwai_vgi/kling-camera-control-i2v

Image to video conversion node with camera control features

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-control-i2v.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=0a03149e96eb31344041b9fe455bba1d" alt="ComfyUI Built-in Kling Image to Video (Camera Control) Node" data-og-width="1731" width="1731" data-og-height="1339" height="1339" data-path="images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-control-i2v.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-control-i2v.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=65d73e44206f5161db5572dace860692 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-control-i2v.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=734b98820da37dd2f96de34e8abe43a1 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-control-i2v.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=f687675a86cfb9507165ba3716836df1 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-control-i2v.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=7f8eb6b8586146eae677cfd0959d6058 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-control-i2v.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=8191888415f17dab1c9fadf88989cf5a 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-control-i2v.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=3a66bef78061a4fdd6d2b876e22bac0c 2500w" />

The Kling Image to Video (Camera Control) node converts static images into videos with professional camera movements. It supports camera controls like zoom, rotation, pan, tilt and first-person view while maintaining focus on the original image content.

## Parameters

### Basic Parameters

| Parameter        | Type   | Default | Description                                     |
| ---------------- | ------ | ------- | ----------------------------------------------- |
| start\_frame     | Image  | -       | Input image to convert to video                 |
| prompt           | String | ""      | Text prompt describing video action and content |
| negative\_prompt | String | ""      | Elements to avoid in the video                  |
| cfg\_scale       | Float  | 7.0     | Controls how closely to follow the prompt       |
| aspect\_ratio    | Select | 16:9    | Output video aspect ratio                       |

### Camera Control Parameters

| Parameter       | Type          | Description                                           |
| --------------- | ------------- | ----------------------------------------------------- |
| camera\_control | CameraControl | Camera control config from Kling Camera Controls node |

### Output

| Output | Type  | Description     |
| ------ | ----- | --------------- |
| VIDEO  | Video | Generated video |

## Source Code

\[Node Source Code (Updated 2025-05-03)]

```python  theme={null}

class KlingCameraControlI2VNode(KlingImage2VideoNode):
    """
    Kling Image to Video Camera Control Node. This node is a image to video node, but it supports controlling the camera.
    Duration, mode, and model_name request fields are hard-coded because camera control is only supported in pro mode with the kling-v1-5 model at 5s duration as of 2025-05-02.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "start_frame": model_field_to_node_input(
                    IO.IMAGE, KlingImage2VideoRequest, "image"
                ),
                "prompt": model_field_to_node_input(
                    IO.STRING, KlingImage2VideoRequest, "prompt", multiline=True
                ),
                "negative_prompt": model_field_to_node_input(
                    IO.STRING,
                    KlingImage2VideoRequest,
                    "negative_prompt",
                    multiline=True,
                ),
                "cfg_scale": model_field_to_node_input(
                    IO.FLOAT, KlingImage2VideoRequest, "cfg_scale"
                ),
                "aspect_ratio": model_field_to_node_input(
                    IO.COMBO,
                    KlingImage2VideoRequest,
                    "aspect_ratio",
                    enum_type=AspectRatio,
                ),
                "camera_control": (
                    "CAMERA_CONTROL",
                    {
                        "tooltip": "Can be created using the Kling Camera Controls node. Controls the camera movement and motion during the video generation.",
                    },
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    DESCRIPTION = "Transform still images into cinematic videos with professional camera movements that simulate real-world cinematography. Control virtual camera actions including zoom, rotation, pan, tilt, and first-person view, while maintaining focus on your original image."

    def api_call(
        self,
        start_frame: torch.Tensor,
        prompt: str,
        negative_prompt: str,
        cfg_scale: float,
        aspect_ratio: str,
        camera_control: CameraControl,
        auth_token: Optional[str] = None,
    ):
        return super().api_call(
            model_name="kling-v1-5",
            start_frame=start_frame,
            cfg_scale=cfg_scale,
            mode="pro",
            aspect_ratio=aspect_ratio,
            duration="5",
            prompt=prompt,
            negative_prompt=negative_prompt,
            camera_control=camera_control,
            auth_token=auth_token,
        )



```


# Kling Text to Video (Camera Control) - ComfyUI Built-in Node
Source: https://docs.comfy.org/built-in-nodes/partner-node/video/kwai_vgi/kling-camera-control-t2v

A text to video generation node with camera control features

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-control-t2v.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=6ed35f4f4394e141a9405d2f1cefb22c" alt="ComfyUI Built-in Kling Text to Video (Camera Control) Node" data-og-width="1731" width="1731" data-og-height="1301" height="1301" data-path="images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-control-t2v.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-control-t2v.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=8f7f64f82e66f744775afe183479c034 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-control-t2v.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=0f78117d6f187bc8ba40050fb374589f 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-control-t2v.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=5642ec9a908be1e89ca088e43375b0f5 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-control-t2v.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=7cc6e6de384c093de245e20299cb542b 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-control-t2v.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=c2b0f57422764c251257be6bcd72f98b 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-control-t2v.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=1f9a2a9215323df503ec8ac582c5b346 2500w" />

The Kling Text to Video (Camera Control) node converts text into videos with professional camera movements. It extends the standard Kling Text to Video node by adding camera control capabilities.

## Parameters

### Basic Parameters

| Parameter        | Type            | Default | Description                                     |
| ---------------- | --------------- | ------- | ----------------------------------------------- |
| prompt           | String          | ""      | Text prompt describing video content            |
| negative\_prompt | String          | ""      | Elements to avoid in the video                  |
| cfg\_scale       | Float           | 7.0     | Controls how closely to follow the prompt       |
| aspect\_ratio    | Select          | "16:9"  | Output video aspect ratio                       |
| camera\_control  | CAMERA\_CONTROL | -       | Camera settings from Kling Camera Controls node |

### Fixed Parameters

Note: The following parameters are fixed and cannot be changed:

* Model: kling-v1-5
* Mode: pro
* Duration: 5 seconds

### Output

| Output | Type  | Description     |
| ------ | ----- | --------------- |
| VIDEO  | Video | Generated video |

## Source Code

\[Node Source Code (Updated 2025-05-03)]

```python  theme={null}

class KlingCameraControlT2VNode(KlingTextToVideoNode):
    """
    Kling Text to Video Camera Control Node. This node is a text to video node, but it supports controlling the camera.
    Duration, mode, and model_name request fields are hard-coded because camera control is only supported in pro mode with the kling-v1-5 model at 5s duration as of 2025-05-02.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": model_field_to_node_input(
                    IO.STRING, KlingText2VideoRequest, "prompt", multiline=True
                ),
                "negative_prompt": model_field_to_node_input(
                    IO.STRING,
                    KlingText2VideoRequest,
                    "negative_prompt",
                    multiline=True,
                ),
                "cfg_scale": model_field_to_node_input(
                    IO.FLOAT, KlingText2VideoRequest, "cfg_scale"
                ),
                "aspect_ratio": model_field_to_node_input(
                    IO.COMBO,
                    KlingText2VideoRequest,
                    "aspect_ratio",
                    enum_type=AspectRatio,
                ),
                "camera_control": (
                    "CAMERA_CONTROL",
                    {
                        "tooltip": "Can be created using the Kling Camera Controls node. Controls the camera movement and motion during the video generation.",
                    },
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    DESCRIPTION = "Transform text into cinematic videos with professional camera movements that simulate real-world cinematography. Control virtual camera actions including zoom, rotation, pan, tilt, and first-person view, while maintaining focus on your original text."

    def api_call(
        self,
        prompt: str,
        negative_prompt: str,
        cfg_scale: float,
        aspect_ratio: str,
        camera_control: Optional[CameraControl] = None,
        auth_token: Optional[str] = None,
    ):
        return super().api_call(
            model_name="kling-v1-5",
            cfg_scale=cfg_scale,
            mode="pro",
            aspect_ratio=aspect_ratio,
            duration="5",
            prompt=prompt,
            negative_prompt=negative_prompt,
            camera_control=camera_control,
            auth_token=auth_token,
        )



```


# Kling Camera Controls - ComfyUI Built-in Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/video/kwai_vgi/kling-camera-controls

A node that provides camera control parameters for Kling video generation

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-controls.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=916ab6bab186b773cfda29470f7a45b6" alt="ComfyUI Built-in Kling Camera Controls Node" data-og-width="1731" width="1731" data-og-height="1230" height="1230" data-path="images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-controls.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-controls.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=73ee6a25e834211e7b59b3ac934a944d 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-controls.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=482ea3be2c12fd99d54cef71f13df76f 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-controls.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=95918400a1034806f08a9cb42dd4c7b6 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-controls.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=c8092b9da0d15b1e84c02f5607130863 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-controls.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=36be5336daba8a6d44fa6d7305f744b0 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-camera-controls.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=c4f2beeb6cd023272b577d4194c7571c 2500w" />

The Kling Camera Controls node defines virtual camera behavior parameters to control camera movement and view changes during Kling video generation.

## Parameters

| Parameter             | Type   | Default  | Description                                                                                                                                                                                                                                          |
| --------------------- | ------ | -------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| camera\_control\_type | Select | "simple" | Preset camera motion types. simple: Custom camera movement; down\_back: Camera moves down and back; forward\_up: Camera moves forward and up; right\_turn\_forward: Rotate right and move forward; left\_turn\_forward: Rotate left and move forward |
| horizontal\_movement  | Float  | 0        | Controls camera movement on horizontal axis (x-axis). Negative values move left, positive values move right                                                                                                                                          |
| vertical\_movement    | Float  | 0        | Controls camera movement on vertical axis (y-axis). Negative values move down, positive values move up                                                                                                                                               |
| pan                   | Float  | 0.5      | Controls camera rotation in vertical plane (x-axis). Negative values rotate down, positive values rotate up                                                                                                                                          |
| tilt                  | Float  | 0        | Controls camera rotation in horizontal plane (y-axis). Negative values rotate left, positive values rotate right                                                                                                                                     |
| roll                  | Float  | 0        | Controls camera roll amount (z-axis). Negative values rotate counterclockwise, positive values rotate clockwise                                                                                                                                      |
| zoom                  | Float  | 0        | Controls camera focal length. Negative values narrow field of view, positive values widen it                                                                                                                                                         |

**Note**: At least one non-zero camera control parameter is required for the effect to work.

### Output

| Output          | Type            | Description                               |
| --------------- | --------------- | ----------------------------------------- |
| camera\_control | CAMERA\_CONTROL | Configuration object with camera settings |

**Note**: Not all model and mode combinations support camera control. Please check the Kling API documentation for details.

## Source Code

\[Node Source Code (Updated 2025-05-03)]

```python  theme={null}

class KlingCameraControls(KlingNodeBase):
    """Kling Camera Controls Node"""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "camera_control_type": (
                    IO.COMBO,
                    {
                        "options": [
                            camera_control_type.value
                            for camera_control_type in CameraType
                        ],
                        "default": "simple",
                        "tooltip": "Predefined camera movements type. simple: Customizable camera movement. down_back: Camera descends and moves backward. forward_up: Camera moves forward and tilts up. right_turn_forward: Rotate right and move forward. left_turn_forward: Rotate left and move forward.",
                    },
                ),
                "horizontal_movement": get_camera_control_input_config(
                    "Controls camera's movement along horizontal axis (x-axis). Negative indicates left, positive indicates right"
                ),
                "vertical_movement": get_camera_control_input_config(
                    "Controls camera's movement along vertical axis (y-axis). Negative indicates downward, positive indicates upward."
                ),
                "pan": get_camera_control_input_config(
                    "Controls camera's rotation in vertical plane (x-axis). Negative indicates downward rotation, positive indicates upward rotation.",
                    default=0.5,
                ),
                "tilt": get_camera_control_input_config(
                    "Controls camera's rotation in horizontal plane (y-axis). Negative indicates left rotation, positive indicates right rotation.",
                ),
                "roll": get_camera_control_input_config(
                    "Controls camera's rolling amount (z-axis). Negative indicates counterclockwise, positive indicates clockwise.",
                ),
                "zoom": get_camera_control_input_config(
                    "Controls change in camera's focal length. Negative indicates narrower field of view, positive indicates wider field of view.",
                ),
            }
        }

    DESCRIPTION = "Kling Camera Controls Node. Not all model and mode combinations support camera control. Please refer to the Kling API documentation for more information."
    RETURN_TYPES = ("CAMERA_CONTROL",)
    RETURN_NAMES = ("camera_control",)
    FUNCTION = "main"

    @classmethod
    def VALIDATE_INPUTS(
        cls,
        horizontal_movement: float,
        vertical_movement: float,
        pan: float,
        tilt: float,
        roll: float,
        zoom: float,
    ) -> bool | str:
        if not is_valid_camera_control_configs(
            [
                horizontal_movement,
                vertical_movement,
                pan,
                tilt,
                roll,
                zoom,
            ]
        ):
            return "Invalid camera control configs: at least one of the values must be non-zero"
        return True

    def main(
        self,
        camera_control_type: str,
        horizontal_movement: float,
        vertical_movement: float,
        pan: float,
        tilt: float,
        roll: float,
        zoom: float,
    ) -> tuple[CameraControl]:
        return (
            CameraControl(
                type=CameraType(camera_control_type),
                config=CameraConfig(
                    horizontal=horizontal_movement,
                    vertical=vertical_movement,
                    pan=pan,
                    roll=roll,
                    tilt=tilt,
                    zoom=zoom,
                ),
            ),
        )
```


# Kling Image to Video - ComfyUI Built-in Node
Source: https://docs.comfy.org/built-in-nodes/partner-node/video/kwai_vgi/kling-image-to-video

A node that converts static images to dynamic videos using Kling's AI technology

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-image-to-video.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=d4c1f57cbb7824acee3d6c584252dbdd" alt="ComfyUI Built-in Kling Image to Video Node" data-og-width="1731" width="1731" data-og-height="1759" height="1759" data-path="images/built-in-nodes/api_nodes/kwai_vgi/kling-image-to-video.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-image-to-video.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=3fea62b8125d97e1bc9120ce31807737 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-image-to-video.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=7c8fcda5ec3e60b804a72c518e56b485 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-image-to-video.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=fcce6e7ca1ed6ee44ade5d680bd8e693 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-image-to-video.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=fe8e7a3a27e0a61d99a2f638d00a67bd 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-image-to-video.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=9647385514ef8a88ac0679146625bc9f 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-image-to-video.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=dcd995989590d5bb17f04696b2c26c74 2500w" />

The Kling Image to Video node converts static images into dynamic video content using Kling's image-to-video API.

## Parameters

### Basic Parameters

All parameters below are required:

| Parameter        | Type   | Default      | Description                                     |
| ---------------- | ------ | ------------ | ----------------------------------------------- |
| start\_frame     | Image  | -            | Input source image                              |
| prompt           | String | ""           | Text prompt describing video action and content |
| negative\_prompt | String | ""           | Elements to avoid in the video                  |
| cfg\_scale       | Float  | 7.0          | Controls how closely to follow the prompt       |
| model\_name      | Select | "kling-v1-5" | Model type to use                               |
| aspect\_ratio    | Select | "16:9"       | Output video aspect ratio                       |
| duration         | Select | "5s"         | Generated video duration                        |
| mode             | Select | "pro"        | Video generation mode                           |

### Output

| Output    | Type   | Description             |
| --------- | ------ | ----------------------- |
| VIDEO     | Video  | Generated video         |
| video\_id | String | Unique video identifier |
| duration  | String | Actual video duration   |

## Source Code

\[Node Source Code (Updated 2025-05-03)]

```python  theme={null}

class KlingImage2VideoNode(KlingNodeBase):
    """Kling Image to Video Node"""

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "start_frame": model_field_to_node_input(
                    IO.IMAGE, KlingImage2VideoRequest, "image"
                ),
                "prompt": model_field_to_node_input(
                    IO.STRING, KlingImage2VideoRequest, "prompt", multiline=True
                ),
                "negative_prompt": model_field_to_node_input(
                    IO.STRING,
                    KlingImage2VideoRequest,
                    "negative_prompt",
                    multiline=True,
                ),
                "model_name": model_field_to_node_input(
                    IO.COMBO,
                    KlingImage2VideoRequest,
                    "model_name",
                    enum_type=KlingVideoGenModelName,
                ),
                "cfg_scale": model_field_to_node_input(
                    IO.FLOAT, KlingImage2VideoRequest, "cfg_scale"
                ),
                "mode": model_field_to_node_input(
                    IO.COMBO,
                    KlingImage2VideoRequest,
                    "mode",
                    enum_type=KlingVideoGenMode,
                ),
                "aspect_ratio": model_field_to_node_input(
                    IO.COMBO,
                    KlingImage2VideoRequest,
                    "aspect_ratio",
                    enum_type=KlingVideoGenAspectRatio,
                ),
                "duration": model_field_to_node_input(
                    IO.COMBO,
                    KlingImage2VideoRequest,
                    "duration",
                    enum_type=KlingVideoGenDuration,
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = ("VIDEO", "STRING", "STRING")
    RETURN_NAMES = ("VIDEO", "video_id", "duration")
    DESCRIPTION = "Kling Image to Video Node"

    def get_response(self, task_id: str, auth_token: str) -> KlingImage2VideoResponse:
        return poll_until_finished(
            auth_token,
            ApiEndpoint(
                path=f"{PATH_IMAGE_TO_VIDEO}/{task_id}",
                method=HttpMethod.GET,
                request_model=KlingImage2VideoRequest,
                response_model=KlingImage2VideoResponse,
            ),
        )

    def api_call(
        self,
        start_frame: torch.Tensor,
        prompt: str,
        negative_prompt: str,
        model_name: str,
        cfg_scale: float,
        mode: str,
        aspect_ratio: str,
        duration: str,
        camera_control: Optional[KlingCameraControl] = None,
        end_frame: Optional[torch.Tensor] = None,
        auth_token: Optional[str] = None,
    ) -> tuple[VideoFromFile]:
        validate_prompts(prompt, negative_prompt, MAX_PROMPT_LENGTH_I2V)
        initial_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path=PATH_IMAGE_TO_VIDEO,
                method=HttpMethod.POST,
                request_model=KlingImage2VideoRequest,
                response_model=KlingImage2VideoResponse,
            ),
            request=KlingImage2VideoRequest(
                model_name=KlingVideoGenModelName(model_name),
                image=tensor_to_base64_string(start_frame),
                image_tail=(
                    tensor_to_base64_string(end_frame)
                    if end_frame is not None
                    else None
                ),
                prompt=prompt,
                negative_prompt=negative_prompt if negative_prompt else None,
                cfg_scale=cfg_scale,
                mode=KlingVideoGenMode(mode),
                aspect_ratio=KlingVideoGenAspectRatio(aspect_ratio),
                duration=KlingVideoGenDuration(duration),
                camera_control=camera_control,
            ),
            auth_token=auth_token,
        )

        task_creation_response = initial_operation.execute()
        validate_task_creation_response(task_creation_response)
        task_id = task_creation_response.data.task_id

        final_response = self.get_response(task_id, auth_token)
        validate_video_result_response(final_response)

        video = get_video_from_response(final_response)
        return video_result_to_node_output(video)

```


# Kling Start-End Frame to Video - ComfyUI Built-in Node
Source: https://docs.comfy.org/built-in-nodes/partner-node/video/kwai_vgi/kling-start-end-frame-to-video

A node that creates smooth video transitions between start and end frames using Kling's AI technology

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-start-end-frame-to-video.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=211ba371516fd05518ad7e503ae9e283" alt="ComfyUI Built-in Kling Start-End Frame to Video Node" data-og-width="1731" width="1731" data-og-height="1454" height="1454" data-path="images/built-in-nodes/api_nodes/kwai_vgi/kling-start-end-frame-to-video.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-start-end-frame-to-video.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=4a27bb295b87e9d826db3ccd1c98f2ea 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-start-end-frame-to-video.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=29e1964cadf33ca7569b45b30d333701 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-start-end-frame-to-video.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=099d809688e7dd785a33953c22900ca5 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-start-end-frame-to-video.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=a47bdff39db3e5a2659881e158067e06 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-start-end-frame-to-video.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=7165a82e58b4587f588459fb6bc837ae 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-start-end-frame-to-video.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=8bc1fa5cbf23a3d62a600dd4f279b5e6 2500w" />

The Kling Start-End Frame to Video node lets you create smooth video transitions between two images. It automatically generates all the intermediate frames to produce a fluid transformation.

## Parameters

### Required Parameters

| Parameter        | Type   | Description                                     |
| ---------------- | ------ | ----------------------------------------------- |
| start\_frame     | Image  | Starting image for the video                    |
| end\_frame       | Image  | Ending image for the video                      |
| prompt           | String | Text describing video content and transition    |
| negative\_prompt | String | Elements to avoid in the video                  |
| cfg\_scale       | Float  | Controls how closely to follow the prompt       |
| aspect\_ratio    | Select | Output video aspect ratio                       |
| mode             | Select | Video generation settings (mode/duration/model) |

### Mode Options

Available mode combinations:

* standard mode / 5s duration / kling-v1
* standard mode / 5s duration / kling-v1-5
* pro mode / 5s duration / kling-v1
* pro mode / 5s duration / kling-v1-5
* pro mode / 5s duration / kling-v1-6
* pro mode / 10s duration / kling-v1-5
* pro mode / 10s duration / kling-v1-6

Default: "pro mode / 5s duration / kling-v1"

### Output

| Output | Type  | Description     |
| ------ | ----- | --------------- |
| VIDEO  | Video | Generated video |

## How It Works

The node analyzes the start and end frames to create a smooth transition sequence between them. It sends the images and parameters to Kling's API server, which generates all necessary intermediate frames for a fluid transformation.

The transition style and content can be guided using prompts, while negative prompts help avoid unwanted elements.

## Source Code

\[Node Source Code (Updated 2025-05-03)]

```python  theme={null}


class KlingStartEndFrameNode(KlingImage2VideoNode):
    """
    Kling First Last Frame Node. This node allows creation of a video from a first and last frame. It calls the normal image to video endpoint, but only allows the subset of input options that support the `image_tail` request field.
    """

    @staticmethod
    def get_mode_string_mapping() -> dict[str, tuple[str, str, str]]:
        """
        Returns a mapping of mode strings to their corresponding (mode, duration, model_name) tuples.
        Only includes config combos that support the `image_tail` request field.
        """
        return {
            "standard mode / 5s duration / kling-v1": ("std", "5", "kling-v1"),
            "standard mode / 5s duration / kling-v1-5": ("std", "5", "kling-v1-5"),
            "pro mode / 5s duration / kling-v1": ("pro", "5", "kling-v1"),
            "pro mode / 5s duration / kling-v1-5": ("pro", "5", "kling-v1-5"),
            "pro mode / 5s duration / kling-v1-6": ("pro", "5", "kling-v1-6"),
            "pro mode / 10s duration / kling-v1-5": ("pro", "10", "kling-v1-5"),
            "pro mode / 10s duration / kling-v1-6": ("pro", "10", "kling-v1-6"),
        }

    @classmethod
    def INPUT_TYPES(s):
        modes = list(KlingStartEndFrameNode.get_mode_string_mapping().keys())
        return {
            "required": {
                "start_frame": model_field_to_node_input(
                    IO.IMAGE, KlingImage2VideoRequest, "image"
                ),
                "end_frame": model_field_to_node_input(
                    IO.IMAGE, KlingImage2VideoRequest, "image_tail"
                ),
                "prompt": model_field_to_node_input(
                    IO.STRING, KlingImage2VideoRequest, "prompt", multiline=True
                ),
                "negative_prompt": model_field_to_node_input(
                    IO.STRING,
                    KlingImage2VideoRequest,
                    "negative_prompt",
                    multiline=True,
                ),
                "cfg_scale": model_field_to_node_input(
                    IO.FLOAT, KlingImage2VideoRequest, "cfg_scale"
                ),
                "aspect_ratio": model_field_to_node_input(
                    IO.COMBO,
                    KlingImage2VideoRequest,
                    "aspect_ratio",
                    enum_type=AspectRatio,
                ),
                "mode": (
                    modes,
                    {
                        "default": modes[2],
                        "tooltip": "The configuration to use for the video generation following the format: mode / duration / model_name.",
                    },
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    DESCRIPTION = "Generate a video sequence that transitions between your provided start and end images. The node creates all frames in between, producing a smooth transformation from the first frame to the last."

    def parse_inputs_from_mode(self, mode: str) -> tuple[str, str, str]:
        """Parses the mode input into a tuple of (model_name, duration, mode)."""
        return KlingStartEndFrameNode.get_mode_string_mapping()[mode]

    def api_call(
        self,
        start_frame: torch.Tensor,
        end_frame: torch.Tensor,
        prompt: str,
        negative_prompt: str,
        cfg_scale: float,
        aspect_ratio: str,
        mode: str,
        auth_token: Optional[str] = None,
    ):
        mode, duration, model_name = self.parse_inputs_from_mode(mode)
        return super().api_call(
            prompt=prompt,
            negative_prompt=negative_prompt,
            model_name=model_name,
            start_frame=start_frame,
            cfg_scale=cfg_scale,
            mode=mode,
            aspect_ratio=aspect_ratio,
            duration=duration,
            end_frame=end_frame,
            auth_token=auth_token,
        )


```


# Kling Text to Video - ComfyUI Built-in Node
Source: https://docs.comfy.org/built-in-nodes/partner-node/video/kwai_vgi/kling-text-to-video

A node that converts text descriptions into videos using Kling's AI technology

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-text-to-video.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=7ddc3d180854e9a1e836154b3f36a58d" alt="ComfyUI Built-in Kling Text to Video Node" data-og-width="1731" width="1731" data-og-height="1754" height="1754" data-path="images/built-in-nodes/api_nodes/kwai_vgi/kling-text-to-video.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-text-to-video.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=cfb616771ecd124d2ea552d2d09b13d8 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-text-to-video.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=8d751c1208fd40972b584bc03d13272f 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-text-to-video.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=806374902d147d57dd7e7a83ec596706 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-text-to-video.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=355b23eced44dfaaef83039be2eab4ef 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-text-to-video.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=87fb90578eb9f7974474937f4d0feb16 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/kwai_vgi/kling-text-to-video.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=a1a8a546c809016b9413d5f9a728aca2 2500w" />

The Kling Text to Video node connects to Kling's API service to generate videos from text descriptions. Users simply provide descriptive text to create corresponding video content.

## Parameters

### Required Parameters

| Parameter        | Type   | Default           | Description                                  |
| ---------------- | ------ | ----------------- | -------------------------------------------- |
| prompt           | String | ""                | Text prompt describing desired video content |
| negative\_prompt | String | ""                | Elements to avoid in the video               |
| cfg\_scale       | Float  | 7.0               | Controls how closely to follow the prompt    |
| model\_name      | Select | "kling-v2-master" | Video generation model to use                |
| aspect\_ratio    | Select | AspectRatio enum  | Output video aspect ratio                    |
| duration         | Select | Duration enum     | Length of generated video                    |
| mode             | Select | Mode enum         | Video generation mode                        |

### Output

| Output         | Type   | Description             |
| -------------- | ------ | ----------------------- |
| VIDEO          | Video  | Generated video         |
| Kling ID       | String | Task identifier         |
| Duration (sec) | String | Video length in seconds |

## How It Works

The node sends text prompts to Kling's API server, which processes and returns the generated video. The process includes initial request and status polling. When complete, the node downloads and outputs the video.

Users can control the generation by adjusting parameters like negative prompts, configuration scale, and video properties. The system validates prompt length to ensure API compliance.

## Source Code

\[Node Source Code (Updated 2025-05-03)]

```python  theme={null}

class KlingTextToVideoNode(KlingNodeBase):
    """Kling Text to Video Node"""

    @staticmethod
    def poll_for_task_status(task_id: str, auth_token: str) -> KlingText2VideoResponse:
        """Polls the Kling API endpoint until the task reaches a terminal state."""
        polling_operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path=f"{PATH_TEXT_TO_VIDEO}/{task_id}",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=KlingText2VideoResponse,
            ),
            completed_statuses=[
                TaskStatus.succeed.value,
            ],
            failed_statuses=[TaskStatus.failed.value],
            status_extractor=lambda response: (
                response.data.task_status.value
                if response.data and response.data.task_status
                else None
            ),
            auth_token=auth_token,
        )
        return polling_operation.execute()

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": model_field_to_node_input(
                    IO.STRING, KlingText2VideoRequest, "prompt", multiline=True
                ),
                "negative_prompt": model_field_to_node_input(
                    IO.STRING, KlingText2VideoRequest, "negative_prompt", multiline=True
                ),
                "model_name": model_field_to_node_input(
                    IO.COMBO,
                    KlingText2VideoRequest,
                    "model_name",
                    enum_type=ModelName,
                    default="kling-v2-master",
                ),
                "cfg_scale": model_field_to_node_input(
                    IO.FLOAT, KlingText2VideoRequest, "cfg_scale"
                ),
                "mode": model_field_to_node_input(
                    IO.COMBO, KlingText2VideoRequest, "mode", enum_type=Mode
                ),
                "duration": model_field_to_node_input(
                    IO.COMBO, KlingText2VideoRequest, "duration", enum_type=Duration
                ),
                "aspect_ratio": model_field_to_node_input(
                    IO.COMBO,
                    KlingText2VideoRequest,
                    "aspect_ratio",
                    enum_type=AspectRatio,
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = ("VIDEO", "STRING", "STRING")
    RETURN_NAMES = ("VIDEO", "Kling ID", "Duration (sec)")
    DESCRIPTION = "Kling Text to Video Node"

    def api_call(
        self,
        prompt: str,
        negative_prompt: str,
        model_name: str,
        cfg_scale: float,
        mode: str,
        duration: int,
        aspect_ratio: str,
        camera_control: Optional[CameraControl] = None,
        auth_token: Optional[str] = None,
    ) -> tuple[VideoFromFile, str, str]:
        validate_prompts(prompt, negative_prompt, MAX_PROMPT_LENGTH_T2V)
        initial_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path=PATH_TEXT_TO_VIDEO,
                method=HttpMethod.POST,
                request_model=KlingText2VideoRequest,
                response_model=KlingText2VideoResponse,
            ),
            request=KlingText2VideoRequest(
                prompt=prompt if prompt else None,
                negative_prompt=negative_prompt if negative_prompt else None,
                duration=Duration(duration),
                mode=Mode(mode),
                model_name=ModelName(model_name),
                cfg_scale=cfg_scale,
                aspect_ratio=AspectRatio(aspect_ratio),
                camera_control=camera_control,
            ),
            auth_token=auth_token,
        )

        initial_response = initial_operation.execute()
        if not is_valid_initial_response(initial_response):
            error_msg = f"Kling initial request failed. Code: {initial_response.code}, Message: {initial_response.message}, Data: {initial_response.data}"
            logging.error(error_msg)
            raise KlingApiError(error_msg)

        task_id = initial_response.data.task_id
        final_response = self.poll_for_task_status(task_id, auth_token)
        if not is_valid_video_response(final_response):
            error_msg = (
                f"Kling task {task_id} succeeded but no video data found in response."
            )
            logging.error(error_msg)
            raise KlingApiError(error_msg)

        video = final_response.data.task_result.videos[0]
        logging.debug("Kling task %s succeeded. Video URL: %s", task_id, video.url)
        return (
            download_url_to_video_output(video.url),
            str(video.id),
            str(video.duration),
        )

```


# Luma Concepts - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/video/luma/luma-concepts

A helper node that provides concept guidance for Luma image generation

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-concepts.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=4b90a3fe60374751cdb7750a3afd1feb" alt="ComfyUI Native Luma Concepts Node" data-og-width="1731" width="1731" data-og-height="880" height="880" data-path="images/built-in-nodes/api_nodes/luma/luma-concepts.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-concepts.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=79424adbc8f8ccaa50d65af596aec5f9 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-concepts.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=c51ef6cf53760ff4f96eebf00968352d 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-concepts.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=78fb97a11e9ae8a1ab96a896e5ad29c6 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-concepts.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=415802a6edcbb1d6747b02a378867d4f 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-concepts.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=0b9148b064336c2063c29ccfdf6530a6 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-concepts.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=6da52abf5b7f252c0bcc263803abdc69 2500w" />

The Luma Concepts node allows you to apply predefined camera concepts to the Luma generation process, providing precise control over camera angles and perspectives without complex prompt descriptions.

## Node Function

This node serves as a helper tool for Luma generation nodes, enabling users to select and apply predefined camera concepts. These concepts include different shooting angles (like overhead or low angle), camera distances (like close-up or long shot), and movement styles (like push-in or follow). It simplifies the creative workflow by providing an intuitive way to control camera effects in the generated output.

## Parameters

### Basic Parameters

| Parameter | Type   | Description                                                       |
| --------- | ------ | ----------------------------------------------------------------- |
| concept1  | select | First camera concept choice, includes various presets and "none"  |
| concept2  | select | Second camera concept choice, includes various presets and "none" |
| concept3  | select | Third camera concept choice, includes various presets and "none"  |
| concept4  | select | Fourth camera concept choice, includes various presets and "none" |

### Optional Parameters

| Parameter      | Type           | Description                                              |
| -------------- | -------------- | -------------------------------------------------------- |
| luma\_concepts | LUMA\_CONCEPTS | Optional Camera Concepts to merge with selected concepts |

### Output

| Output         | Type          | Description                                      |
| -------------- | ------------- | ------------------------------------------------ |
| luma\_concepts | LUMA\_CONCEPT | Combined object containing all selected concepts |

## Usage Examples

<Card title="Luma Text to Video Workflow Example" icon="book" href="/tutorials/partner-nodes/luma/luma-text-to-video">
  Luma Text to Video Workflow Example
</Card>

<Card title="Luma Image to Video Workflow Example" icon="book" href="/tutorials/partner-nodes/luma/luma-image-to-video">
  Luma Image to Video Workflow Example
</Card>

## How It Works

The Luma Concepts node offers a variety of predefined camera concepts including:

* Camera distances (close-up, medium shot, long shot)
* View angles (eye level, overhead, low angle)
* Movement types (push-in, follow, orbit)
* Special effects (handheld, stabilized, floating)

Users can select up to 4 concepts to use together. The node creates an object containing the selected camera concepts, which is then passed to Luma generation nodes. During generation, Luma AI uses these camera concepts to influence the viewpoint and composition of the output, ensuring the results reflect the chosen photographic effects.

By combining multiple camera concepts, users can create complex camera guidance without writing detailed prompt descriptions. This is particularly useful when specific camera angles or compositions are needed.

## Source Code

\[Node source code (Updated on 2025-05-03)]

```python  theme={null}

class LumaConceptsNode(ComfyNodeABC):
    """
    Holds one or more Camera Concepts for use with Luma Text to Video and Luma Image to Video nodes.
    """

    RETURN_TYPES = (LumaIO.LUMA_CONCEPTS,)
    RETURN_NAMES = ("luma_concepts",)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "create_concepts"
    CATEGORY = "api node/image/Luma"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "concept1": (get_luma_concepts(include_none=True),),
                "concept2": (get_luma_concepts(include_none=True),),
                "concept3": (get_luma_concepts(include_none=True),),
                "concept4": (get_luma_concepts(include_none=True),),
            },
            "optional": {
                "luma_concepts": (
                    LumaIO.LUMA_CONCEPTS,
                    {
                        "tooltip": "Optional Camera Concepts to add to the ones chosen here."
                    },
                ),
            },
        }

    def create_concepts(
        self,
        concept1: str,
        concept2: str,
        concept3: str,
        concept4: str,
        luma_concepts: LumaConceptChain = None,
    ):
        chain = LumaConceptChain(str_list=[concept1, concept2, concept3, concept4])
        if luma_concepts is not None:
            chain = luma_concepts.clone_and_merge(chain)
        return (chain,)


```


# Luma Image to Video - ComfyUI Native API Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/video/luma/luma-image-to-video

A node that converts static images to dynamic videos using Luma AI

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-image-to-video.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=c424ef4de613563d402de2ab97b919f3" alt="ComfyUI Native Luma Image to Video Node" data-og-width="1731" width="1731" data-og-height="1687" height="1687" data-path="images/built-in-nodes/api_nodes/luma/luma-image-to-video.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-image-to-video.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=b43e784839d4cf1a548a94513db3c2ce 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-image-to-video.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=5b98a3c185f678d0372376e50fb5101e 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-image-to-video.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=2dcf471fd4b988d86480d4d72b89bfba 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-image-to-video.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=6379ff541d4bdca23a599b8f5fce3c6a 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-image-to-video.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=da8e06b6ff28fd499145495787bb6b8f 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-image-to-video.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=fa8899176b3a464afcb9c4bb5b62a441 2500w" />

The Luma Image to Video node uses Luma AI's technology to transform static images into smooth, dynamic videos, bringing your images to life.

## Node Function

This node connects to Luma AI's image-to-video API, allowing users to create dynamic videos from input images. It understands the image content and generates natural, coherent motion while maintaining the original visual style. Combined with text prompts, users can precisely control the video's dynamic effects.

## Parameters

### Basic Parameters

| Parameter  | Type    | Default | Description                                             |
| ---------- | ------- | ------- | ------------------------------------------------------- |
| prompt     | string  | ""      | Text prompt describing video motion and content         |
| model      | select  | -       | Video generation model to use                           |
| resolution | select  | "540p"  | Output video resolution                                 |
| duration   | select  | -       | Video length options                                    |
| loop       | boolean | False   | Whether to loop the video                               |
| seed       | integer | 0       | Seed value for node rerun, results are nondeterministic |

### Optional Parameters

| Parameter      | Type           | Description                                           |
| -------------- | -------------- | ----------------------------------------------------- |
| first\_image   | image          | First frame of video (required if no last\_image)     |
| last\_image    | image          | Last frame of video (required if no first\_image)     |
| luma\_concepts | LUMA\_CONCEPTS | Concepts for controlling camera motion and shot style |

### Requirements

* Either **first\_image** or **last\_image** must be provided
* Each image input (first\_image and last\_image) accepts only 1 image

### Output

| Output | Type  | Description     |
| ------ | ----- | --------------- |
| VIDEO  | video | Generated video |

## Usage Example

<Card title="Luma Image to Video Workflow Example" icon="book" href="/tutorials/partner-nodes/luma/luma-image-to-video">
  Luma Image to Video Workflow Tutorial
</Card>

## Source Code

\[Node Source Code (Updated 2025-05-03)]

```python  theme={null}

class LumaImageToVideoGenerationNode(ComfyNodeABC):
    """
    Generates videos synchronously based on prompt, input images, and output_size.
    """

    RETURN_TYPES = (IO.VIDEO,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/video/Luma"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the video generation",
                    },
                ),
                "model": ([model.value for model in LumaVideoModel],),
                # "aspect_ratio": ([ratio.value for ratio in LumaAspectRatio], {
                #     "default": LumaAspectRatio.ratio_16_9,
                # }),
                "resolution": (
                    [resolution.value for resolution in LumaVideoOutputResolution],
                    {
                        "default": LumaVideoOutputResolution.res_540p,
                    },
                ),
                "duration": ([dur.value for dur in LumaVideoModelOutputDuration],),
                "loop": (
                    IO.BOOLEAN,
                    {
                        "default": False,
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {
                "first_image": (
                    IO.IMAGE,
                    {"tooltip": "First frame of generated video."},
                ),
                "last_image": (IO.IMAGE, {"tooltip": "Last frame of generated video."}),
                "luma_concepts": (
                    LumaIO.LUMA_CONCEPTS,
                    {
                        "tooltip": "Optional Camera Concepts to dictate camera motion via the Luma Concepts node."
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        prompt: str,
        model: str,
        resolution: str,
        duration: str,
        loop: bool,
        seed,
        first_image: torch.Tensor = None,
        last_image: torch.Tensor = None,
        luma_concepts: LumaConceptChain = None,
        auth_token=None,
        **kwargs,
    ):
        if first_image is None and last_image is None:
            raise Exception(
                "At least one of first_image and last_image requires an input."
            )
        keyframes = self._convert_to_keyframes(first_image, last_image, auth_token)
        duration = duration if model != LumaVideoModel.ray_1_6 else None
        resolution = resolution if model != LumaVideoModel.ray_1_6 else None

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/luma/generations",
                method=HttpMethod.POST,
                request_model=LumaGenerationRequest,
                response_model=LumaGeneration,
            ),
            request=LumaGenerationRequest(
                prompt=prompt,
                model=model,
                aspect_ratio=LumaAspectRatio.ratio_16_9,  # ignored, but still needed by the API for some reason
                resolution=resolution,
                duration=duration,
                loop=loop,
                keyframes=keyframes,
                concepts=luma_concepts.create_api_model() if luma_concepts else None,
            ),
            auth_token=auth_token,
        )
        response_api: LumaGeneration = operation.execute()

        operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path=f"/proxy/luma/generations/{response_api.id}",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=LumaGeneration,
            ),
            completed_statuses=[LumaState.completed],
            failed_statuses=[LumaState.failed],
            status_extractor=lambda x: x.state,
            auth_token=auth_token,
        )
        response_poll = operation.execute()

        vid_response = requests.get(response_poll.assets.video)
        return (VideoFromFile(BytesIO(vid_response.content)),)

    def _convert_to_keyframes(
        self,
        first_image: torch.Tensor = None,
        last_image: torch.Tensor = None,
        auth_token=None,
    ):
        if first_image is None and last_image is None:
            return None
        frame0 = None
        frame1 = None
        if first_image is not None:
            download_urls = upload_images_to_comfyapi(
                first_image, max_images=1, auth_token=auth_token
            )
            frame0 = LumaImageReference(type="image", url=download_urls[0])
        if last_image is not None:
            download_urls = upload_images_to_comfyapi(
                last_image, max_images=1, auth_token=auth_token
            )
            frame1 = LumaImageReference(type="image", url=download_urls[0])
        return LumaKeyframes(frame0=frame0, frame1=frame1)

```


# Luma Text to Video - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/video/luma/luma-text-to-video

A node that converts text descriptions to videos using Luma AI

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-text-to-video.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=223f31f422e31ef39bac52410e704fcb" alt="ComfyUI Native Luma Text to Video Node" data-og-width="1731" width="1731" data-og-height="1255" height="1255" data-path="images/built-in-nodes/api_nodes/luma/luma-text-to-video.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-text-to-video.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=c7c66e979c18a3a3c132c3e369dab948 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-text-to-video.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=4bd89c005e94d9efa4925c661fc45eae 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-text-to-video.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=c2d3a42cae2e773294553d7ab6890250 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-text-to-video.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=f7b064caa5ebed4996ca5634da32a792 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-text-to-video.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=816235faa145d6f759e4e5e98f768b6d 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/luma/luma-text-to-video.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=d9458a565cd462c1817a52e218fb181a 2500w" />

The Luma Text to Video node lets you create high-quality, smooth videos from text descriptions using Luma AI's video generation technology.

## Node Function

This node connects to Luma AI's text-to-video API, allowing users to generate dynamic video content from detailed text prompts.

## Parameters

### Basic Parameters

| Parameter     | Type    | Default        | Description                                             |
| ------------- | ------- | -------------- | ------------------------------------------------------- |
| prompt        | string  | ""             | Text prompt describing the video content to generate    |
| model         | select  | -              | Video generation model to use                           |
| aspect\_ratio | select  | "ratio\_16\_9" | Video aspect ratio                                      |
| resolution    | select  | "res\_540p"    | Video resolution                                        |
| duration      | select  | -              | Video length options                                    |
| loop          | boolean | False          | Whether to loop the video                               |
| seed          | integer | 0              | Seed value for node rerun, results are nondeterministic |

<Note>
  When using Ray 1.6 model, duration and resolution parameters will not take effect.
</Note>

### Optional Parameters

| Parameter      | Type           | Description                                              |
| -------------- | -------------- | -------------------------------------------------------- |
| luma\_concepts | LUMA\_CONCEPTS | Camera concepts to control motion via Luma Concepts node |

### Output

| Output | Type  | Description     |
| ------ | ----- | --------------- |
| VIDEO  | video | Generated video |

## Usage Example

<Card title="Luma Text to Video Workflow Example" icon="book" href="/tutorials/partner-nodes/luma/luma-text-to-video">
  Luma Text to Video Workflow Example
</Card>

## Source Code

\[Node source code (Updated on 2025-05-03)]

```python  theme={null}

class LumaTextToVideoGenerationNode(ComfyNodeABC):
    """
    Generates videos synchronously based on prompt and output_size.
    """

    RETURN_TYPES = (IO.VIDEO,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/video/Luma"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the video generation",
                    },
                ),
                "model": ([model.value for model in LumaVideoModel],),
                "aspect_ratio": (
                    [ratio.value for ratio in LumaAspectRatio],
                    {
                        "default": LumaAspectRatio.ratio_16_9,
                    },
                ),
                "resolution": (
                    [resolution.value for resolution in LumaVideoOutputResolution],
                    {
                        "default": LumaVideoOutputResolution.res_540p,
                    },
                ),
                "duration": ([dur.value for dur in LumaVideoModelOutputDuration],),
                "loop": (
                    IO.BOOLEAN,
                    {
                        "default": False,
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {
                "luma_concepts": (
                    LumaIO.LUMA_CONCEPTS,
                    {
                        "tooltip": "Optional Camera Concepts to dictate camera motion via the Luma Concepts node."
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        prompt: str,
        model: str,
        aspect_ratio: str,
        resolution: str,
        duration: str,
        loop: bool,
        seed,
        luma_concepts: LumaConceptChain = None,
        auth_token=None,
        **kwargs,
    ):
        duration = duration if model != LumaVideoModel.ray_1_6 else None
        resolution = resolution if model != LumaVideoModel.ray_1_6 else None

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/luma/generations",
                method=HttpMethod.POST,
                request_model=LumaGenerationRequest,
                response_model=LumaGeneration,
            ),
            request=LumaGenerationRequest(
                prompt=prompt,
                model=model,
                resolution=resolution,
                aspect_ratio=aspect_ratio,
                duration=duration,
                loop=loop,
                concepts=luma_concepts.create_api_model() if luma_concepts else None,
            ),
            auth_token=auth_token,
        )
        response_api: LumaGeneration = operation.execute()

        operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path=f"/proxy/luma/generations/{response_api.id}",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=LumaGeneration,
            ),
            completed_statuses=[LumaState.completed],
            failed_statuses=[LumaState.failed],
            status_extractor=lambda x: x.state,
            auth_token=auth_token,
        )
        response_poll = operation.execute()

        vid_response = requests.get(response_poll.assets.video)
        return (VideoFromFile(BytesIO(vid_response.content)),)

```


# MiniMax Image to Video - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/video/minimax/minimax-image-to-video

A node that converts static images to dynamic videos using MiniMax AI

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/minimax/minimax-image-to-video.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=20daea4b1e99477334f1a7be3d3cb4d3" alt="ComfyUI Native MiniMax Image to Video Node" data-og-width="1731" width="1731" data-og-height="1360" height="1360" data-path="images/built-in-nodes/api_nodes/minimax/minimax-image-to-video.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/minimax/minimax-image-to-video.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=1bf0e97a0a4ba3d78b5bb31179834feb 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/minimax/minimax-image-to-video.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=0e8b54168d3722db035c60ccd51eaa31 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/minimax/minimax-image-to-video.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=063762bc777a56217d88e984d85c07b9 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/minimax/minimax-image-to-video.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=848e10dd87fc30b16072f3ba41c23175 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/minimax/minimax-image-to-video.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=fbe894102020f7b8e27b5f15ebc250e2 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/minimax/minimax-image-to-video.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=34e4fda31f26d1e28a63c600a84d4145 2500w" />

The MiniMax Image to Video node uses MiniMax's API to generate videos from input images and text prompts.

## Parameters

### Required Parameters

| Parameter    | Type   | Default  | Description                                                  |
| ------------ | ------ | -------- | ------------------------------------------------------------ |
| image        | image  | -        | Input image used as the first frame of video                 |
| prompt\_text | string | ""       | Text prompt to guide video generation                        |
| model        | select | "I2V-01" | Available models: "I2V-01-Director", "I2V-01", "I2V-01-live" |

### Optional Parameters

| Parameter | Type    | Description                      |
| --------- | ------- | -------------------------------- |
| seed      | integer | Random seed for noise generation |

### Output

| Output | Type  | Description     |
| ------ | ----- | --------------- |
| VIDEO  | video | Generated video |

## Source Code

\[Node source code (Updated on 2025-05-03)]

```python  theme={null}

class MinimaxImageToVideoNode(MinimaxTextToVideoNode):
    """
    Generates videos synchronously based on an image and prompt, and optional parameters using Minimax's API.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (
                    IO.IMAGE,
                    {
                        "tooltip": "Image to use as first frame of video generation"
                    },
                ),
                "prompt_text": (
                    "STRING",
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Text prompt to guide the video generation",
                    },
                ),
                "model": (
                    [
                        "I2V-01-Director",
                        "I2V-01",
                        "I2V-01-live",
                    ],
                    {
                        "default": "I2V-01",
                        "tooltip": "Model to use for video generation",
                    },
                ),
            },
            "optional": {
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "The random seed used for creating the noise.",
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    RETURN_TYPES = ("VIDEO",)
    DESCRIPTION = "Generates videos from an image and prompts using Minimax's API"
    FUNCTION = "generate_video"
    CATEGORY = "api node/video/Minimax"
    API_NODE = True
    OUTPUT_NODE = True
```


# MiniMax Text to Video - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/video/minimax/minimax-text-to-video

A node that converts text descriptions into videos using MiniMax AI

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/minimax/minimax-text-to-video.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=c141952741306e4733795f063cfe4f66" alt="ComfyUI Native MiniMax Text to Video Node" data-og-width="1731" width="1731" data-og-height="1340" height="1340" data-path="images/built-in-nodes/api_nodes/minimax/minimax-text-to-video.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/minimax/minimax-text-to-video.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=662c1ad1c99ed77cdb75ab187f55958f 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/minimax/minimax-text-to-video.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=3c84d2b9752eb1a2a52aebd63b9219a7 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/minimax/minimax-text-to-video.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=cdc9d9573d192435eba48c1e1834e37c 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/minimax/minimax-text-to-video.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=98340910fe791ede0fe6abdc02991d2e 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/minimax/minimax-text-to-video.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=11c603f036d3d9614d73a42818e7b299 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/minimax/minimax-text-to-video.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=34db8feeb5134426e5268f5159a440ad 2500w" />

The MiniMax Text to Video node connects to MiniMax's API to generate high-quality, smooth videos from text prompts. It supports different video generation models to create short video clips in various styles.

## Parameters

### Required Parameters

| Parameter    | Type    | Default  | Description                                                    |
| ------------ | ------- | -------- | -------------------------------------------------------------- |
| prompt\_text | String  | ""       | Text prompt that guides the video generation                   |
| model        | Select  | "T2V-01" | Video model to use, options are "T2V-01" and "T2V-01-Director" |
| seed         | Integer | 0        | Random seed for noise generation, defaults to 0                |

### Output

| Output | Type  | Description     |
| ------ | ----- | --------------- |
| VIDEO  | Video | Generated video |

## Source Code

\[Node Source Code (Updated 2025-05-03)]

```python  theme={null}

class MinimaxTextToVideoNode:
    """
    Generates videos synchronously based on a prompt, and optional parameters using Minimax's API.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt_text": (
                    "STRING",
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Text prompt to guide the video generation",
                    },
                ),
                "model": (
                    [
                        "T2V-01",
                        "T2V-01-Director",
                    ],
                    {
                        "default": "T2V-01",
                        "tooltip": "Model to use for video generation",
                    },
                ),
            },
            "optional": {
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "The random seed used for creating the noise.",
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    RETURN_TYPES = ("VIDEO",)
    DESCRIPTION = "Generates videos from prompts using Minimax's API"
    FUNCTION = "generate_video"
    CATEGORY = "api node/video/Minimax"
    API_NODE = True
    OUTPUT_NODE = True

    def generate_video(
        self,
        prompt_text,
        seed=0,
        model="T2V-01",
        image: torch.Tensor=None, # used for ImageToVideo
        subject: torch.Tensor=None, # used for SubjectToVideo
        auth_token=None,
    ):
        '''
        Function used between Minimax nodes - supports T2V, I2V, and S2V, based on provided arguments.
        '''
        # upload image, if passed in
        image_url = None
        if image is not None:
            image_url = upload_images_to_comfyapi(image, max_images=1, auth_token=auth_token)[0]

        # TODO: figure out how to deal with subject properly, API returns invalid params when using S2V-01 model
        subject_reference = None
        if subject is not None:
            subject_url = upload_images_to_comfyapi(subject, max_images=1, auth_token=auth_token)[0]
            subject_reference = [SubjectReferenceItem(image=subject_url)]


        video_generate_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/minimax/video_generation",
                method=HttpMethod.POST,
                request_model=MinimaxVideoGenerationRequest,
                response_model=MinimaxVideoGenerationResponse,
            ),
            request=MinimaxVideoGenerationRequest(
                model=Model(model),
                prompt=prompt_text,
                callback_url=None,
                first_frame_image=image_url,
                subject_reference=subject_reference,
                prompt_optimizer=None,
            ),
            auth_token=auth_token,
        )
        response = video_generate_operation.execute()

        task_id = response.task_id
        if not task_id:
            raise Exception(f"Minimax generation failed: {response.base_resp}")

        video_generate_operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path="/proxy/minimax/query/video_generation",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=MinimaxTaskResultResponse,
                query_params={"task_id": task_id},
            ),
            completed_statuses=["Success"],
            failed_statuses=["Fail"],
            status_extractor=lambda x: x.status.value,
            auth_token=auth_token,
        )
        task_result = video_generate_operation.execute()

        file_id = task_result.file_id
        if file_id is None:
            raise Exception("Request was not successful. Missing file ID.")
        file_retrieve_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/minimax/files/retrieve",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=MinimaxFileRetrieveResponse,
                query_params={"file_id": int(file_id)},
            ),
            request=EmptyRequest(),
            auth_token=auth_token,
        )
        file_result = file_retrieve_operation.execute()

        file_url = file_result.file.download_url
        if file_url is None:
            raise Exception(
                f"No video was found in the response. Full response: {file_result.model_dump()}"
            )
        logging.info(f"Generated video URL: {file_url}")

        video_io = download_url_to_bytesio(file_url)
        if video_io is None:
            error_msg = f"Failed to download video from {file_url}"
            logging.error(error_msg)
            raise Exception(error_msg)
        return (VideoFromFile(video_io),)
```


# Pika 2.2 Image to Video - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/video/pika/pika-image-to-video

A node that converts static images to dynamic videos using Pika AI

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pika/pika-2-2-image-to-video.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=4d84ed0f286826f853aeb820bf01c473" alt="ComfyUI Native Pika 2.2 Image to Video Node" data-og-width="1731" width="1731" data-og-height="1610" height="1610" data-path="images/built-in-nodes/api_nodes/pika/pika-2-2-image-to-video.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pika/pika-2-2-image-to-video.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=6231adab3c17f6b259419ea1ca56851b 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pika/pika-2-2-image-to-video.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=f68712438d1dc3b57282825a276e55b1 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pika/pika-2-2-image-to-video.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=414c952553bf355df8f624badafa6651 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pika/pika-2-2-image-to-video.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=1e9d7bf229f464467877a9e3545f6fdb 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pika/pika-2-2-image-to-video.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=24a0c522a83a0f2034931f7b4fb86dd7 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pika/pika-2-2-image-to-video.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=3c88a91bbb2117ae69e7724b3a70ad24 2500w" />

The Pika 2.2 Image to Video node connects to Pika's latest 2.2 API to transform static images into dynamic videos. It preserves the visual features of the original image while adding natural motion based on text prompts.

## Parameters

### Required Parameters

| Parameter        | Type    | Default | Description                                     |
| ---------------- | ------- | ------- | ----------------------------------------------- |
| image            | Image   | -       | Input image to convert to video                 |
| prompt\_text     | String  | ""      | Text prompt describing video motion and content |
| negative\_prompt | String  | ""      | Elements to avoid in the video                  |
| seed             | Integer | 0       | Random seed for generation                      |
| resolution       | Select  | "1080p" | Output video resolution                         |
| duration         | Select  | "5s"    | Length of generated video                       |

### Output

| Output | Type  | Description     |
| ------ | ----- | --------------- |
| VIDEO  | Video | Generated video |

## How It Works

The node sends the input image and parameters (prompts, resolution, duration, etc.) to Pika's API server as multipart form data. The API processes this and returns the generated video. Users can control the output by adjusting the prompts, negative prompts, random seed and other parameters.

## Source Code

\[Node Source Code (Updated 2025-05-05)]

```python  theme={null}

class PikaImageToVideoV2_2(PikaNodeBase):
    """Pika 2.2 Image to Video Node."""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": (
                    IO.IMAGE,
                    {"tooltip": "The image to convert to video"},
                ),
                **cls.get_base_inputs_types(PikaBodyGenerate22I2vGenerate22I2vPost),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    DESCRIPTION = "Sends an image and prompt to the Pika API v2.2 to generate a video."
    RETURN_TYPES = ("VIDEO",)

    def api_call(
        self,
        image: torch.Tensor,
        prompt_text: str,
        negative_prompt: str,
        seed: int,
        resolution: str,
        duration: int,
        auth_token: Optional[str] = None,
    ) -> tuple[VideoFromFile]:
        """API call for Pika 2.2 Image to Video."""
        # Convert image to BytesIO
        image_bytes_io = tensor_to_bytesio(image)
        image_bytes_io.seek(0)  # Reset stream position

        # Prepare file data for multipart upload
        pika_files = {"image": ("image.png", image_bytes_io, "image/png")}

        # Prepare non-file data using the Pydantic model
        pika_request_data = PikaBodyGenerate22I2vGenerate22I2vPost(
            promptText=prompt_text,
            negativePrompt=negative_prompt,
            seed=seed,
            resolution=resolution,
            duration=duration,
        )

        initial_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path=PATH_IMAGE_TO_VIDEO,
                method=HttpMethod.POST,
                request_model=PikaBodyGenerate22I2vGenerate22I2vPost,
                response_model=PikaGenerateResponse,
            ),
            request=pika_request_data,
            files=pika_files,
            content_type="multipart/form-data",
            auth_token=auth_token,
        )

        return self.execute_task(initial_operation, auth_token)

```


# Pika 2.2 Scenes - ComfyUI Built-in Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/video/pika/pika-scenes

A node that creates coherent scene videos from multiple images using Pika AI

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pika/pika-2-2-scenes.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=84326ffddf831516f76705979410d289" alt="ComfyUI Built-in Pika 2.2 Scenes Node" data-og-width="1731" width="1731" data-og-height="2121" height="2121" data-path="images/built-in-nodes/api_nodes/pika/pika-2-2-scenes.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pika/pika-2-2-scenes.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=0d107ed6ac35bd07543a7fc4149b2bfa 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pika/pika-2-2-scenes.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=c68b392aab37de8e75eb0f0edd927dce 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pika/pika-2-2-scenes.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=74cf062e793f2b5a11630569501a0c51 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pika/pika-2-2-scenes.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=849a17a65abc8ebcf590cfa44fd957d3 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pika/pika-2-2-scenes.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=e82e31b75398fa3d05be6f4b109478ed 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pika/pika-2-2-scenes.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=43ce716712a858cb97a3aacb6021904f 2500w" />

The Pika 2.2 Scenes node allows you to upload multiple images and generate a high-quality video incorporating these elements. It uses Pika's 2.2 API to create smooth scene transitions between the images.

## Parameters

### Required Parameters

| Parameter         | Type    | Default                   | Description                                     |
| ----------------- | ------- | ------------------------- | ----------------------------------------------- |
| prompt\_text      | string  | ""                        | Text prompt describing video content and scenes |
| negative\_prompt  | string  | ""                        | Elements to exclude from the video              |
| seed              | integer | 0                         | Random seed for generation                      |
| ingredients\_mode | select  | "creative"                | Image combination mode                          |
| resolution        | select  | API default               | Output video resolution                         |
| duration          | select  | API default               | Output video length                             |
| aspect\_ratio     | float   | 1.7777777777777777 (16:9) | Video aspect ratio, range 0.4-2.5               |

### Optional Parameters

| Parameter            | Type  | Description        |
| -------------------- | ----- | ------------------ |
| image\_ingredient\_1 | image | First scene image  |
| image\_ingredient\_2 | image | Second scene image |
| image\_ingredient\_3 | image | Third scene image  |
| image\_ingredient\_4 | image | Fourth scene image |
| image\_ingredient\_5 | image | Fifth scene image  |

### Output

| Output | Type  | Description     |
| ------ | ----- | --------------- |
| VIDEO  | video | Generated video |

## How It Works

The Pika 2.2 Scenes node analyzes all input images and creates a video containing these image elements. The node sends the images and parameters to Pika's API server, which processes them and returns the generated video.

Users can guide the video style and content through prompts, and exclude unwanted elements using negative prompts. The node supports up to 5 input images as ingredients and generates the final video based on the specified combination mode, resolution, duration, and aspect ratio.

## Source Code

```python  theme={null}

class PikaScenesV2_2(PikaNodeBase):
    """Pika 2.2 Scenes Node."""

    @classmethod
    def INPUT_TYPES(cls):
        image_ingredient_input = (
            IO.IMAGE,
            {"tooltip": "Image that will be used as ingredient to create a video."},
        )
        return {
            "required": {
                **cls.get_base_inputs_types(
                    PikaBodyGenerate22C2vGenerate22PikascenesPost,
                ),
                "ingredients_mode": model_field_to_node_input(
                    IO.COMBO,
                    PikaBodyGenerate22C2vGenerate22PikascenesPost,
                    "ingredientsMode",
                    enum_type=IngredientsMode,
                    default="creative",
                ),
                "aspect_ratio": model_field_to_node_input(
                    IO.FLOAT,
                    PikaBodyGenerate22C2vGenerate22PikascenesPost,
                    "aspectRatio",
                    step=0.001,
                    min=0.4,
                    max=2.5,
                    default=1.7777777777777777,
                ),
            },
            "optional": {
                "image_ingredient_1": image_ingredient_input,
                "image_ingredient_2": image_ingredient_input,
                "image_ingredient_3": image_ingredient_input,
                "image_ingredient_4": image_ingredient_input,
                "image_ingredient_5": image_ingredient_input,
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    DESCRIPTION = "Combine your images to create a video with the objects in them. Upload multiple images as ingredients and generate a high-quality video that incorporates all of them."
    RETURN_TYPES = ("VIDEO",)

    def api_call(
        self,
        prompt_text: str,
        negative_prompt: str,
        seed: int,
        resolution: str,
        duration: int,
        ingredients_mode: str,
        aspect_ratio: float,
        image_ingredient_1: Optional[torch.Tensor] = None,
        image_ingredient_2: Optional[torch.Tensor] = None,
        image_ingredient_3: Optional[torch.Tensor] = None,
        image_ingredient_4: Optional[torch.Tensor] = None,
        image_ingredient_5: Optional[torch.Tensor] = None,
        auth_token: Optional[str] = None,
    ) -> tuple[VideoFromFile]:
        """API call for Pika Scenes 2.2."""
        all_image_bytes_io = []
        for image in [
            image_ingredient_1,
            image_ingredient_2,
            image_ingredient_3,
            image_ingredient_4,
            image_ingredient_5,
        ]:
            if image is not None:
                image_bytes_io = tensor_to_bytesio(image)
                image_bytes_io.seek(0)
                all_image_bytes_io.append(image_bytes_io)

        # Prepare files data for multipart upload
        pika_files = [
            ("images", (f"image_{i}.png", image_bytes_io, "image/png"))
            for i, image_bytes_io in enumerate(all_image_bytes_io)
        ]

        # Prepare non-file data using the Pydantic model
        pika_request_data = PikaBodyGenerate22C2vGenerate22PikascenesPost(
            ingredientsMode=ingredients_mode,
            promptText=prompt_text,
            negativePrompt=negative_prompt,
            seed=seed,
            resolution=resolution,
            duration=duration,
            aspectRatio=aspect_ratio,
        )

        initial_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path=PATH_PIKASCENES,
                method=HttpMethod.POST,
                request_model=PikaBodyGenerate22C2vGenerate22PikascenesPost,
                response_model=PikaGenerateResponse,
            ),
            request=pika_request_data,
            files=pika_files,
            content_type="multipart/form-data",
            auth_token=auth_token,
        )

        return self.execute_task(initial_operation, auth_token)


```


# Pika 2.2 Text to Video - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/video/pika/pika-text-to-video

A node that converts text descriptions into videos using Pika AI

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pika/pika-2-2-text-to-video.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=3eea0be5229f14d73d79d17a1b9872ea" alt="ComfyUI Native Pika 2.2 Text to Video Node" data-og-width="1731" width="1731" data-og-height="1712" height="1712" data-path="images/built-in-nodes/api_nodes/pika/pika-2-2-text-to-video.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pika/pika-2-2-text-to-video.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=d12e35717b758b3fe6edee67ea6bf3f1 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pika/pika-2-2-text-to-video.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=01e926f28ab200aa7ecd60eeed6bb791 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pika/pika-2-2-text-to-video.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=ceaced8c4a6712925c5506eac3fc00af 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pika/pika-2-2-text-to-video.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=b62485e36320061e636d7b34ee9747a4 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pika/pika-2-2-text-to-video.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=9c1bd0cd3ab168fd6f92de1d3dc0a9ab 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pika/pika-2-2-text-to-video.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=03cd041ab170254cc97f90fed682169b 2500w" />

The Pika 2.2 Text to Video node uses Pika's 2.2 API to create videos from text descriptions. It connects to Pika's text-to-video API, allowing users to generate videos using text prompts with various control parameters.

## Parameters

### Required Parameters

| Parameter        | Type    | Default            | Description                                   |
| ---------------- | ------- | ------------------ | --------------------------------------------- |
| prompt\_text     | String  | ""                 | Text prompt describing the video content      |
| negative\_prompt | String  | ""                 | Elements to exclude from the video            |
| seed             | Integer | 0                  | Random seed for generation                    |
| resolution       | Select  | "1080p"            | Output video resolution                       |
| duration         | Select  | "5s"               | Length of generated video                     |
| aspect\_ratio    | Float   | 1.7777777777777777 | Video aspect ratio, range 0.4-2.5, step 0.001 |

### Output

| Output | Type  | Description     |
| ------ | ----- | --------------- |
| VIDEO  | Video | Generated video |

## Source Code

\[Node Source Code (Updated 2025-05-05)]

```python  theme={null}

class PikaTextToVideoNodeV2_2(PikaNodeBase):
    """Pika 2.2 Text to Video Node."""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                **cls.get_base_inputs_types(PikaBodyGenerate22T2vGenerate22T2vPost),
                "aspect_ratio": model_field_to_node_input(
                    IO.FLOAT,
                    PikaBodyGenerate22T2vGenerate22T2vPost,
                    "aspectRatio",
                    step=0.001,
                    min=0.4,
                    max=2.5,
                    default=1.7777777777777777,
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    RETURN_TYPES = ("VIDEO",)
    DESCRIPTION = "Sends a text prompt to the Pika API v2.2 to generate a video."

    def api_call(
        self,
        prompt_text: str,
        negative_prompt: str,
        seed: int,
        resolution: str,
        duration: int,
        aspect_ratio: float,
        auth_token: Optional[str] = None,
    ) -> tuple[VideoFromFile]:
        """API call for Pika 2.2 Text to Video."""
        initial_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path=PATH_TEXT_TO_VIDEO,
                method=HttpMethod.POST,
                request_model=PikaBodyGenerate22T2vGenerate22T2vPost,
                response_model=PikaGenerateResponse,
            ),
            request=PikaBodyGenerate22T2vGenerate22T2vPost(
                promptText=prompt_text,
                negativePrompt=negative_prompt,
                seed=seed,
                resolution=resolution,
                duration=duration,
                aspectRatio=aspect_ratio,
            ),
            auth_token=auth_token,
            content_type="application/x-www-form-urlencoded",
        )

        return self.execute_task(initial_operation, auth_token)


```


# PixVerse Image to Video - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/video/pixverse/pixverse-image-to-video

A node that converts static images to dynamic videos using PixVerse AI

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pixverse/pixverse-image-to-video.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=5dad32a76df9845caf721b255b39df09" alt="ComfyUI Native PixVerse Image to Video Node" data-og-width="1731" width="1731" data-og-height="1659" height="1659" data-path="images/built-in-nodes/api_nodes/pixverse/pixverse-image-to-video.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pixverse/pixverse-image-to-video.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=3155afe1969bafa8920123e9917e54e0 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pixverse/pixverse-image-to-video.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=6a8e5c91f4d608ef3dde687fc9c43541 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pixverse/pixverse-image-to-video.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=03241f98cc99bc9b3bd6a41e361a1a73 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pixverse/pixverse-image-to-video.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=a2ebe2cb1fb508cf2b1d2c3c1d5b2c1b 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pixverse/pixverse-image-to-video.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=1035165aa7732d5e36faa262d6ede118 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pixverse/pixverse-image-to-video.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=951bf8ba9400bbb5c25471ef63ff0fa4 2500w" />

The PixVerse Image to Video node uses PixVerse's API to transform static images into dynamic videos. It preserves the visual features of the original image while adding natural motion based on text prompts.

## Parameters

### Required Parameters

| Parameter        | Type    | Default      | Description                                 |
| ---------------- | ------- | ------------ | ------------------------------------------- |
| image            | Image   | -            | Input image to convert to video             |
| prompt           | String  | ""           | Text prompt describing video motion/content |
| negative\_prompt | String  | ""           | Elements to avoid in the video              |
| seed             | Integer | -1           | Random seed (-1 for random)                 |
| quality          | Select  | "high"       | Output video quality level                  |
| aspect\_ratio    | Select  | "r16\_9"     | Output video aspect ratio                   |
| duration         | Select  | "seconds\_4" | Length of generated video                   |
| motion\_mode     | Select  | "standard"   | Video motion style                          |

### Optional Parameters

| Parameter          | Type               | Default | Description                |
| ------------------ | ------------------ | ------- | -------------------------- |
| pixverse\_template | PIXVERSE\_TEMPLATE | None    | Optional PixVerse template |

### Output

| Output | Type  | Description     |
| ------ | ----- | --------------- |
| VIDEO  | Video | Generated video |

## Source Code

\[Node Source Code (Updated 2025-05-05)]

```python  theme={null}
class PixverseImageToVideoNode(ComfyNodeABC):
    """
    Pixverse Image to Video

    Generates videos from an image and prompts.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": ("IMAGE",),
                "prompt": ("STRING", {"multiline": True, "default": ""}),
                "negative_prompt": ("STRING", {"multiline": True, "default": ""}),
                "seed": ("INT", {"default": -1, "min": -1, "max": 0xffffffffffffffff}),
                "quality": (list(PixverseQuality.__members__.keys()), {"default": "high"}),
                "aspect_ratio": (list(PixverseAspectRatio.__members__.keys()), {"default": "r16_9"}),
                "duration": (list(PixverseDuration.__members__.keys()), {"default": "seconds_4"}),
                "motion_mode": (list(PixverseMotionMode.__members__.keys()), {"default": "standard"}),
            },
            "optional": {
                "pixverse_template": ("PIXVERSE_TEMPLATE",),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    RETURN_TYPES = ("VIDEO",)
    DESCRIPTION = "Generates videos from an image and prompts using Pixverse's API"
    FUNCTION = "generate_video"
    CATEGORY = "api node/video/Pixverse"
    API_NODE = True
    OUTPUT_NODE = True
```


# PixVerse Template - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/video/pixverse/pixverse-template

A helper node that provides preset templates for PixVerse video generation

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pixverse/pixverse-template.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=9995576d223575123de26cc31872a7f1" alt="ComfyUI Native PixVerse Template Node" data-og-width="1731" width="1731" data-og-height="694" height="694" data-path="images/built-in-nodes/api_nodes/pixverse/pixverse-template.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pixverse/pixverse-template.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=603937102f495f8a3fb3ff83676cca70 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pixverse/pixverse-template.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=4628eb175c018d421ce9b721e117d080 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pixverse/pixverse-template.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=ee169c282b5862fe461e93cf38e0c0c9 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pixverse/pixverse-template.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=f957824eb713681eb4e792466067b8b5 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pixverse/pixverse-template.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=8fde847a6014189746bb77b83948bd06 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pixverse/pixverse-template.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=022f8f0a2edf3ddc7f6a760c42dfa4c0 2500w" />

The PixVerse Template node lets you choose from predefined video generation templates to control the style and effects of PixVerse video generation nodes.
This helper node connects to PixVerse video generation nodes, allowing users to quickly apply preset video styles without manually adjusting complex parameter combinations.

## Parameters

### Required Parameters

| Parameter | Type   | Description                                    |
| --------- | ------ | ---------------------------------------------- |
| template  | Select | Choose a template from available video presets |

### Output

| Output             | Type                | Description                                              |
| ------------------ | ------------------- | -------------------------------------------------------- |
| pixverse\_template | PixverseIO.TEMPLATE | Configuration object containing the selected template ID |

## Source Code

\[Node Source Code (Updated 2025-05-05)]

```python  theme={null}

class PixverseTemplateNode:
    """
    Select template for Pixverse Video generation.
    """

    RETURN_TYPES = (PixverseIO.TEMPLATE,)
    RETURN_NAMES = ("pixverse_template",)
    FUNCTION = "create_template"
    CATEGORY = "api node/video/Pixverse"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "template": (list(pixverse_templates.keys()), ),
            }
        }

    def create_template(self, template: str):
        template_id = pixverse_templates.get(template, None)
        if template_id is None:
            raise Exception(f"Template '{template}' is not recognized.")
        # just return the integer
        return (template_id,)

```


# PixVerse Text to Video - ComfyUI Built-in Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/video/pixverse/pixverse-text-to-video

A node that converts text descriptions into videos using PixVerse AI technology

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pixverse/pixverse-text-to-video.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=b565631bd9ca69e804d839e8d79a799c" alt="ComfyUI Built-in PixVerse Text to Video Node" data-og-width="1731" width="1731" data-og-height="1689" height="1689" data-path="images/built-in-nodes/api_nodes/pixverse/pixverse-text-to-video.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pixverse/pixverse-text-to-video.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=8e6098d1ef620e31633c0b4a47e0c82b 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pixverse/pixverse-text-to-video.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=d9432ec30c5d2af16fb70aceaf5202ae 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pixverse/pixverse-text-to-video.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=66543725029b6c747ad4ef61953cef49 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pixverse/pixverse-text-to-video.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=6864d606bcf6ebc5245445d75cecef75 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pixverse/pixverse-text-to-video.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=9f510cffe2825d9855cbbf8588f80d92 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pixverse/pixverse-text-to-video.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=a439e4b778c50a5af9d3a0b72b070fca 2500w" />

The PixVerse Text to Video node connects to PixVerse's text-to-video API, allowing users to generate high-quality videos from text descriptions. Users can customize their creations by adjusting various parameters like video quality, duration, and motion mode.

## Parameters

### Required Parameters

| Parameter         | Type    | Default                   | Description                                   |
| ----------------- | ------- | ------------------------- | --------------------------------------------- |
| prompt            | string  | ""                        | Text prompt describing the video content      |
| aspect\_ratio     | select  | -                         | Output video aspect ratio                     |
| quality           | select  | PixverseQuality.res\_540p | Video quality level                           |
| duration\_seconds | select  | -                         | Video duration                                |
| motion\_mode      | select  | -                         | Video motion mode                             |
| seed              | integer | 0                         | Random seed for consistent generation results |

### Optional Parameters

| Parameter          | Type               | Default | Description                          |
| ------------------ | ------------------ | ------- | ------------------------------------ |
| negative\_prompt   | string             | ""      | Elements to exclude from the video   |
| pixverse\_template | PIXVERSE\_TEMPLATE | None    | Optional template for style settings |

### Limitations

* 1080p quality only supports normal motion mode with 5-second duration
* Non 5-second durations only support normal motion mode

### Output

| Output | Type  | Description     |
| ------ | ----- | --------------- |
| VIDEO  | video | Generated video |

## Source Code

\[Node Source Code (Updated 2025-05-05)]

```python  theme={null}

class PixverseTextToVideoNode(ComfyNodeABC):
    """
    Generates videos synchronously based on prompt and output_size.
    """

    RETURN_TYPES = (IO.VIDEO,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/video/Pixverse"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the video generation",
                    },
                ),
                "aspect_ratio": (
                    [ratio.value for ratio in PixverseAspectRatio],
                ),
                "quality": (
                    [resolution.value for resolution in PixverseQuality],
                    {
                        "default": PixverseQuality.res_540p,
                    },
                ),
                "duration_seconds": ([dur.value for dur in PixverseDuration],),
                "motion_mode": ([mode.value for mode in PixverseMotionMode],),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2147483647,
                        "control_after_generate": True,
                        "tooltip": "Seed for video generation.",
                    },
                ),
            },
            "optional": {
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "An optional text description of undesired elements on an image.",
                    },
                ),
                "pixverse_template": (
                    PixverseIO.TEMPLATE,
                    {
                        "tooltip": "An optional template to influence style of generation, created by the Pixverse Template node."
                    }
                )
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        prompt: str,
        aspect_ratio: str,
        quality: str,
        duration_seconds: int,
        motion_mode: str,
        seed,
        negative_prompt: str=None,
        pixverse_template: int=None,
        auth_token=None,
        **kwargs,
    ):
        # 1080p is limited to 5 seconds duration
        # only normal motion_mode supported for 1080p or for non-5 second duration
        if quality == PixverseQuality.res_1080p:
            motion_mode = PixverseMotionMode.normal
            duration_seconds = PixverseDuration.dur_5
        elif duration_seconds != PixverseDuration.dur_5:
            motion_mode = PixverseMotionMode.normal

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/pixverse/video/text/generate",
                method=HttpMethod.POST,
                request_model=PixverseTextVideoRequest,
                response_model=PixverseVideoResponse,
            ),
            request=PixverseTextVideoRequest(
                prompt=prompt,
                aspect_ratio=aspect_ratio,
                quality=quality,
                duration=duration_seconds,
                motion_mode=motion_mode,
                negative_prompt=negative_prompt if negative_prompt else None,
                template_id=pixverse_template,
                seed=seed,
            ),
            auth_token=auth_token,
        )
        response_api = operation.execute()

        if response_api.Resp is None:
            raise Exception(f"Pixverse request failed: '{response_api.ErrMsg}'")

        operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path=f"/proxy/pixverse/video/result/{response_api.Resp.video_id}",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=PixverseGenerationStatusResponse,
            ),
            completed_statuses=[PixverseStatus.successful],
            failed_statuses=[PixverseStatus.contents_moderation, PixverseStatus.failed, PixverseStatus.deleted],
            status_extractor=lambda x: x.Resp.status,
            auth_token=auth_token,
        )
        response_poll = operation.execute()

        vid_response = requests.get(response_poll.Resp.url)
        return (VideoFromFile(BytesIO(vid_response.content)),)

```


# PixVerse Transition Video - ComfyUI Native Node Documentation
Source: https://docs.comfy.org/built-in-nodes/partner-node/video/pixverse/pixverse-transition-video

Create smooth transition videos between start and end frames using PixVerse AI

<img src="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pixverse/pixverse-transition-video.jpg?fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=13d19c52ceb204b914331211525aca82" alt="ComfyUI Native PixVerse Transition Video Node" data-og-width="1731" width="1731" data-og-height="1659" height="1659" data-path="images/built-in-nodes/api_nodes/pixverse/pixverse-transition-video.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pixverse/pixverse-transition-video.jpg?w=280&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=76907b3f70d9154dec1e893a684a786b 280w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pixverse/pixverse-transition-video.jpg?w=560&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=d6ce25f5e717234c0e05e58ce16ad4fd 560w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pixverse/pixverse-transition-video.jpg?w=840&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=e43117de110af376e5f6ee9226f77933 840w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pixverse/pixverse-transition-video.jpg?w=1100&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=cd15acee6531d79c546762e9d824088d 1100w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pixverse/pixverse-transition-video.jpg?w=1650&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=f73c102b783260a78ef68551789a3a29 1650w, https://mintcdn.com/dripart/5003JSxULDwNImme/images/built-in-nodes/api_nodes/pixverse/pixverse-transition-video.jpg?w=2500&fit=max&auto=format&n=5003JSxULDwNImme&q=85&s=57c484331df6355691cd0201e4206dcc 2500w" />

The Pixverse Transition Video node connects to PixVerse's API to generate smooth video transitions between two images. It automatically creates all intermediate frames to produce fluid transformations, perfect for morphing effects, scene transitions, and object evolution.

## Parameters

### Required Parameters

| Parameter         | Type    | Default                     | Description                                 |
| ----------------- | ------- | --------------------------- | ------------------------------------------- |
| first\_frame      | Image   | -                           | Starting frame image                        |
| last\_frame       | Image   | -                           | Ending frame image                          |
| prompt            | String  | ""                          | Text prompt describing video and transition |
| quality           | Select  | "PixverseQuality.res\_540p" | Output video quality                        |
| duration\_seconds | Select  | -                           | Length of generated video                   |
| motion\_mode      | Select  | -                           | Video motion style                          |
| seed              | Integer | 0                           | Random seed (range: 0-2147483647)           |

### Optional Parameters

| Parameter          | Type               | Default | Description                |
| ------------------ | ------------------ | ------- | -------------------------- |
| negative\_prompt   | String             | ""      | Elements to avoid in video |
| pixverse\_template | PIXVERSE\_TEMPLATE | None    | Optional style preset      |

### Parameter Constraints

* When quality is set to 1080p, motion\_mode is forced to normal and duration\_seconds to 5 seconds
* When duration\_seconds is not 5 seconds, motion\_mode is forced to normal

### Output

| Output | Type  | Description     |
| ------ | ----- | --------------- |
| VIDEO  | Video | Generated video |

## Source Code

```python  theme={null}

class PixverseTransitionVideoNode(ComfyNodeABC):
    """
    Generates videos synchronously based on prompt and output_size.
    """

    RETURN_TYPES = (IO.VIDEO,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/video/Pixverse"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "first_frame": (
                    IO.IMAGE,
                ),
                "last_frame": (
                    IO.IMAGE,
                ),
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the video generation",
                    },
                ),
                "quality": (
                    [resolution.value for resolution in PixverseQuality],
                    {
                        "default": PixverseQuality.res_540p,
                    },
                ),
                "duration_seconds": ([dur.value for dur in PixverseDuration],),
                "motion_mode": ([mode.value for mode in PixverseMotionMode],),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2147483647,
                        "control_after_generate": True,
                        "tooltip": "Seed for video generation.",
                    },
                ),
            },
            "optional": {
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "An optional text description of undesired elements on an image.",
                    },
                ),
                "pixverse_template": (
                    PixverseIO.TEMPLATE,
                    {
                        "tooltip": "An optional template to influence style of generation, created by the Pixverse Template node."
                    }
                )
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        first_frame: torch.Tensor,
        last_frame: torch.Tensor,
        prompt: str,
        quality: str,
        duration_seconds: int,
        motion_mode: str,
        seed,
        negative_prompt: str=None,
        pixverse_template: int=None,
        auth_token=None,
        **kwargs,
    ):
        first_frame_id = upload_image_to_pixverse(first_frame, auth_token=auth_token)
        last_frame_id = upload_image_to_pixverse(last_frame, auth_token=auth_token)

        # 1080p is limited to 5 seconds duration
        # only normal motion_mode supported for 1080p or for non-5 second duration
        if quality == PixverseQuality.res_1080p:
            motion_mode = PixverseMotionMode.normal
            duration_seconds = PixverseDuration.dur_5
        elif duration_seconds != PixverseDuration.dur_5:
            motion_mode = PixverseMotionMode.normal

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/pixverse/video/transition/generate",
                method=HttpMethod.POST,
                request_model=PixverseTransitionVideoRequest,
                response_model=PixverseVideoResponse,
            ),
            request=PixverseTransitionVideoRequest(
                first_frame_img=first_frame_id,
                last_frame_img=last_frame_id,
                prompt=prompt,
                quality=quality,
                duration=duration_seconds,
                motion_mode=motion_mode,
                negative_prompt=negative_prompt if negative_prompt else None,
                template_id=pixverse_template,
                seed=seed,
            ),
            auth_token=auth_token,
        )
        response_api = operation.execute()

        if response_api.Resp is None:
            raise Exception(f"Pixverse request failed: '{response_api.ErrMsg}'")

        operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path=f"/proxy/pixverse/video/result/{response_api.Resp.video_id}",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=PixverseGenerationStatusResponse,
            ),
            completed_statuses=[PixverseStatus.successful],
            failed_statuses=[PixverseStatus.contents_moderation, PixverseStatus.failed, PixverseStatus.deleted],
            status_extractor=lambda x: x.Resp.status,
            auth_token=auth_token,
        )
        response_poll = operation.execute()

        vid_response = requests.get(response_poll.Resp.url)
        return (VideoFromFile(BytesIO(vid_response.content)),)
```


# Ksampler - ComfyUI Built-in Node Documentation
Source: https://docs.comfy.org/built-in-nodes/sampling/ksampler

The Ksampler node is a commonly used sampling node in ComfyUI.

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/sampling/ksampler.jpg?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=40ba407ee8ed6f5322d7c5990a6d8d80" alt="Ksampler" data-og-width="1608" width="1608" data-og-height="1739" height="1739" data-path="images/built-in-nodes/sampling/ksampler.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/sampling/ksampler.jpg?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=cfaee96d09869966fa7bdf2c04569c1c 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/sampling/ksampler.jpg?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=2c9d3d238c3e0b117150852b62a86542 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/sampling/ksampler.jpg?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=94a0c54d37a97f36ce2f667bba8cba05 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/sampling/ksampler.jpg?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=f5a40c984b0d1889254eb313905b7b1d 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/sampling/ksampler.jpg?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=378d714be3c13b3443ff2e4424709225 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/built-in-nodes/sampling/ksampler.jpg?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=38fc34e0eb77b321d85344e5ee424c57 2500w" />

The KSampler node performs multi-step denoising sampling on latent images. It combines positive and negative conditions (prompts) and uses specified sampling algorithms and schedulers to generate high-quality latent images. It is commonly used in AI image generation workflows like text-to-image and image-to-image.

## Parameter Description

### Input Parameters

| Parameter     | Type         | Required | Default | Description                                                                                                        |
| ------------- | ------------ | -------- | ------- | ------------------------------------------------------------------------------------------------------------------ |
| model         | MODEL        | Yes      | None    | Model used for denoising (e.g. Stable Diffusion model)                                                             |
| seed          | INT          | Yes      | 0       | Random seed to ensure reproducible results                                                                         |
| steps         | INT          | Yes      | 20      | Number of denoising steps - more steps mean finer details but slower generation                                    |
| cfg           | FLOAT        | Yes      | 8.0     | Classifier-Free Guidance scale - higher values better match prompts but too high impacts quality                   |
| sampler\_name | Enum         | Yes      | None    | Name of sampling algorithm, affects generation speed, style and quality                                            |
| scheduler     | Enum         | Yes      | None    | Scheduler that controls the noise removal process                                                                  |
| positive      | CONDITIONING | Yes      | None    | Positive conditions describing desired image content                                                               |
| negative      | CONDITIONING | Yes      | None    | Negative conditions describing content to exclude                                                                  |
| latent\_image | LATENT       | Yes      | None    | Latent image to denoise, usually noise or output from previous step                                                |
| denoise       | FLOAT        | Yes      | 1.0     | Denoising strength - 1.0 for full denoising, lower values preserve original structure, suitable for image-to-image |

### Output Parameters

| Output  | Type   | Description                                              |
| ------- | ------ | -------------------------------------------------------- |
| samples | LATENT | Denoised latent image that can be decoded to final image |

## Usage Examples

<Card title="Stable diffusion 1.5 Text-to-Image Workflow Example" icon="book" href="/tutorials/basic/text-to-image">
  Stable diffusion 1.5 Text-to-Image Workflow Example
</Card>

<Card title="Stable diffusion 1.5 Image-to-Image Workflow Example" icon="book" href="/tutorials/basic/image-to-image">
  Stable diffusion 1.5 Image-to-Image Workflow Example
</Card>

## Source Code

\[Updated on May 15, 2025]

```Python  theme={null}

def common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent, denoise=1.0, disable_noise=False, start_step=None, last_step=None, force_full_denoise=False):
    latent_image = latent["samples"]
    latent_image = comfy.sample.fix_empty_latent_channels(model, latent_image)

    if disable_noise:
        noise = torch.zeros(latent_image.size(), dtype=latent_image.dtype, layout=latent_image.layout, device="cpu")
    else:
        batch_inds = latent["batch_index"] if "batch_index" in latent else None
        noise = comfy.sample.prepare_noise(latent_image, seed, batch_inds)

    noise_mask = None
    if "noise_mask" in latent:
        noise_mask = latent["noise_mask"]

    callback = latent_preview.prepare_callback(model, steps)
    disable_pbar = not comfy.utils.PROGRESS_BAR_ENABLED
    samples = comfy.sample.sample(model, noise, steps, cfg, sampler_name, scheduler, positive, negative, latent_image,
                                  denoise=denoise, disable_noise=disable_noise, start_step=start_step, last_step=last_step,
                                  force_full_denoise=force_full_denoise, noise_mask=noise_mask, callback=callback, disable_pbar=disable_pbar, seed=seed)
    out = latent.copy()
    out["samples"] = samples
    return (out, )


class KSampler:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "model": ("MODEL", {"tooltip": "The model used for denoising the input latent."}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffffffffffff, "control_after_generate": True, "tooltip": "The random seed used for creating the noise."}),
                "steps": ("INT", {"default": 20, "min": 1, "max": 10000, "tooltip": "The number of steps used in the denoising process."}),
                "cfg": ("FLOAT", {"default": 8.0, "min": 0.0, "max": 100.0, "step":0.1, "round": 0.01, "tooltip": "The Classifier-Free Guidance scale balances creativity and adherence to the prompt. Higher values result in images more closely matching the prompt however too high values will negatively impact quality."}),
                "sampler_name": (comfy.samplers.KSampler.SAMPLERS, {"tooltip": "The algorithm used when sampling, this can affect the quality, speed, and style of the generated output."}),
                "scheduler": (comfy.samplers.KSampler.SCHEDULERS, {"tooltip": "The scheduler controls how noise is gradually removed to form the image."}),
                "positive": ("CONDITIONING", {"tooltip": "The conditioning describing the attributes you want to include in the image."}),
                "negative": ("CONDITIONING", {"tooltip": "The conditioning describing the attributes you want to exclude from the image."}),
                "latent_image": ("LATENT", {"tooltip": "The latent image to denoise."}),
                "denoise": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 1.0, "step": 0.01, "tooltip": "The amount of denoising applied, lower values will maintain the structure of the initial image allowing for image to image sampling."}),
            }
        }

    RETURN_TYPES = ("LATENT",)
    OUTPUT_TOOLTIPS = ("The denoised latent.",)
    FUNCTION = "sample"

    CATEGORY = "sampling"
    DESCRIPTION = "Uses the provided model, positive and negative conditioning to denoise the latent image."

    def sample(self, model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=1.0):
        return common_ksampler(model, seed, steps, cfg, sampler_name, scheduler, positive, negative, latent_image, denoise=denoise)

```


# Changelog
Source: https://docs.comfy.org/changelog/index

Track ComfyUI's latest features, improvements, and bug fixes. For detailed release notes, see the [Github releases](https://github.com/comfyanonymous/ComfyUI/releases) page.

<Update label="v0.3.71" description="November 21, 2025">
  **Model Compatibility & Enhancements**

  * **HunyuanVideo 1.5 Support**: Added compatibility for the latest version of HunyuanVideo models, expanding video generation capabilities
  * **LLAMA Text Encoder Improvements**: Added ability to disable final normalization in LLAMA-based text encoder models for better workflow customization
  * **HunyuanV3D Schema Migration**: Updated Hunyuan3D nodes to V3 schema for improved performance and compatibility

  **API Node Expansions**

  * **New Topaz API Nodes**: Added support for Topaz video enhancement workflows directly within ComfyUI
  * **Nano Banana Pro Integration**: Expanded API node collection with Nano Banana Pro for enhanced processing capabilities
  * **Kling Lip Sync Improvements**: Fixed audio format conversion issues in KlingLipSyncAudioToVideoNode, ensuring proper MP3 format handling

  **Image Processing & Workflow Improvements**

  * **Enhanced Image Batching**: Fixed ImageBatch node to handle images with different channel counts and automatically add alpha channels when needed
  * **Preview Node Refinement**: Renamed PreviewAny node to "Preview as Text" for clearer workflow organization
  * **Workflow Template Updates**: Enhanced server template handler to support multi-package distribution for better workflow management

  **Performance & System Optimizations**

  * **CUDA Optimization**: Disabled unnecessary workarounds on newer CUDNN versions for improved GPU performance
  * Fixed workflow naming issues and improved overall stability for complex processing pipelines
</Update>

<Update label="v0.3.70" description="November 19, 2025">
  **CUDA 12.6 Support & Distribution**

  * Added official CUDA 12.6 release workflow and portable download support for enhanced GPU compatibility
  * Updated README with corrected portable download links for streamlined installation

  **Model Compatibility & Fixes**

  * **HunYuan 3D 2.0 Support**: Fixed compatibility issues for improved 3D model generation workflows
  * **EasyCache Improvements**: Resolved input/output channel mismatches that affected certain model configurations
  * Enhanced block swap functionality by removing potentially harmful native custom node implementations

  **API Node Enhancements**

  * **New Gemini Models Added**: Expanded AI model options for text and multimodal generation workflows
  * Improved API node development infrastructure with updated PR templates and Python 3.10 minimum version requirements

  **Development & Infrastructure**

  * Enhanced pylint configuration for better code quality in custom node development
  * Improved release automation and distribution processes for more reliable updates
</Update>

<Update label="v0.3.69" description="November 18, 2025">
  **Memory & Performance Optimizations**

  * **Pinned Memory Enabled by Default** for NVIDIA and AMD GPUs
  * **Reduced VRAM Usage** for Flux, Qwen, and LTX-Video models
  * Smart model unloading that automatically frees memory when VRAM usage increases
  * Improved weight casting performance on offload streams

  **New Features**

  * **ScaleROPE Node Now Works with Flux Models**
  * Added left padding support to tokenizers
  * Added `create_time` field to `/history` and `/queue` endpoints

  **Bug Fixes**

  * Fixed custom node import errors for SingleStreamBlock/DoubleStreamBlock (temporary fix)
  * Fixed Qwen ControlNet regression
  * Enhanced quantized operations with offload support and stability improvements
  * Unified RoPE function implementation across models
</Update>

<Update label="v0.3.68" description="November 5, 2025">
  **Performance & Memory Optimizations**

  * Introduced **Mixed Precision Quantization System** for optimized model loading
  * Added **RAM Pressure Cache Mode** for intelligent memory management under resource constraints
  * Accelerated model offloading using pinned memory with automatic low-RAM hardware detection
  * Enhanced FP8 operations: reduced memory usage and fixed torch.compile performance regressions
  * Improved async offload speeds and resolved race conditions

  **New Nodes & Execution Features**

  * **ScaleROPE Node**: Rope scaling support for WAN and Lumina models
  * Enhanced subgraph execution allowing multiple runs within single workflows
  * Improved caching system with proper handling of bytes data and None outputs

  **API Node Enhancements**

  * Migrated API nodes to V3 client architecture: Luma, Minimax, Pixverse, Ideogram, StabilityAI, Pika, Recraft, Hypernetwork, OpenAI
  * Extended LTXV API nodes with 12s-20s duration options
  * Fixed img2img operations in DALL-E 2 node
  * Enhanced Rodin3D nodes to return proper relative paths

  **Updates**

  * Embedded documentation updated to v0.3.1
  * Workflow templates updated to v0.2.11
  * Fixed Windows pinned memory allocation issues
</Update>

<Update label="v0.3.67" description="October 28, 2025">
  **API Nodes**

  * **LTXV API Integration**: Added new LTXV API nodes for Lightricks LTX video generation
  * Network Client V2 upgrade with async operations and cancellation support
  * Converted Tripo and Gemini API nodes to V3 schema

  **Performance & Compatibility**

  * Improved AMD GPU support by only disabling cudnn on newer AMD GPUs
  * Fixed Windows-specific network issues in API nodes for better retry handling

  **Core Improvements**

  * Enhanced dependency-aware caching system that fixes --cache-none behavior with loops
  * Added support for multi-dimensional latents
  * Added custom node published subgraphs endpoint

  **Updates**

  * Frontend bumped to version 1.28.8
  * Template updates to version 0.2.4
</Update>

<Update label="v0.3.66" description="October 21, 2025">
  **Frontend Updates**

  * **Subgraph Widget Editing**: Edit subgraph parameters directly from new Parameters panel without entering the subgraph
  * **Template Modal Redesign**: New template browser with advanced filtering by model tags and categories

  **Performance Optimizations**

  * Iimproved workflow cancellation speed
  * Fixed VAE memory issue consuming 3x more on PyTorch 2.9 with NVIDIA GPUs
  * Enhanced chroma radiance processing speed and fixed batch size issues above 1

  **API Nodes**

  * Added Veo 3.1 model support
  * Added TemporalScoreRescaling node for advanced temporal control in video workflows

  **Hardware & Compatibility**

  * Disabled FP8 operations for AMD gfx942 GPUs
  * Improved CUDA memory management in --fast autotune mode

  **Execution & Schema**

  * Converted ControlNet nodes to V3 schema
  * Enhanced EasyCache with proper batch\_slice handling
  * Improved merge\_nested\_dicts functionality with proper input ordering
  * Added deprecation warnings for unused files
</Update>

<Update label="v0.3.65" description="October 14, 2025">
  **Node Schema Migration (V3)**

  * Migrated core node categories to V3 schema including model downscale, LoRA extraction, compositing, latent ops, SD3/SLG, Flux, upscale models, and HunyuanVideo nodes

  **Audio & Model Improvements**

  * Added MMaudio 16K VAE support for high-fidelity audio workflows
  * Fixed mono audio incorrectly saving as stereo
  * Refactored model sampling sigmas code and fixed FP8 scaled LoRA issues
  * Fixed loading of older Stable Diffusion checkpoints on newer NumPy versions

  **AMD GPU Optimizations**

  * Better memory estimation for SD/Flux VAE operations
  * Enabled RDNA4 PyTorch attention on ROCm 7.0+

  **API Node Updates**

  * Added price extractor and improvements to Kling/Pika API nodes
  * Enhanced Gemini Image API with aspect\_ratio support

  **Updates**

  * Template v0.1.95, node docs v0.3.0
  * Fixed WAN2.2 cache VRAM leak
</Update>

<Update label="v0.3.64" description="October 8, 2025">
  **API Nodes**

  * Added Sora2 API node for OpenAI's video generation API
</Update>

<Update label="v0.3.63" description="October 6, 2025">
  **Model Compatibility Enhancements**

  * **HunyuanVAE Support**: Added support for the new HunyuanVAE, expanding model compatibility for advanced image generation workflows
  * **Epsilon Scaling Node**: Introduced new Epsilon Scaling node that reduces exposure bias in diffusion models by scaling predicted noise, improving generation quality (based on the paper [Elucidating the Exposure Bias in Diffusion Models](https://arxiv.org/abs/2308.15321))

  **Memory and Performance Optimizations**

  * **VAE Memory Leak Fix**: Fixed VRAM leak caused by Python call stack holding tensor references during VAE OOM exception handling, significantly improving tiled fallback reliability on low VRAM devices
  * **AMD Support**: Enabled TORCH\_ROCM\_AOTRITON\_ENABLE\_EXPERIMENTAL by default

  **API Node Updates**

  * **Kling 2.5 Turbo**: Added support for kling-2-5-turbo in both txt2video and img2video nodes, with proper mode configuration fixes
  * **API Node Fixes**: Improved Gemini node base64 handling and fixed indentation issues in Recraft API node functions

  **Node Schema Migration (V3)**

  * **Extensive V3 Conversion**: Migrated numerous node categories to V3 schema including audio encoder, GITS, differential diffusion, optimal steps, PAG, LT, IP2P, morphology, torch compile, EPS, Pixverse, TomeSD, edit model, Rodin, and Stable3D nodes for improved workflow compatibility

  **Developer Experience Improvements**

  * **Code Quality**: Added pylint support for comfy\_api\_nodes folder and updated example\_node.py to use V3 schema, making custom node development more consistent
  * **Documentation Updates**: Enhanced AMD installation instructions with nightly PyTorch commands for Windows users

  **Frontend Updates**

  * **Subgraph Publish**: Allows publishing subgraphs to the node library
  * **Node Selection Toolbox Redesign**: Redesigned the node selection toolbox
</Update>

<Update label="v0.3.62" description="September 30, 2025">
  **API Node**

  * **Rodin3D-Gen2 Parameter Fix**
  * **Seedance Pro Model Support**
</Update>

<Update label="v0.3.61" description="September 30, 2025">
  **API Node**

  * **Rodin3D Gen-2**: Rodin's most powerful image-to-3D tool, is now live in ComfyUI!
  * **WAN Image-to-Image**: Wan2.5 Image-to-Image API node, support image editing.

  **Enhanced Audio Capabilities**

  * **New Audio Nodes**: Added new audio nodes for enhanced audio-driven workflows and multimodal content creation

  **Model Compatibility Fixes**

  * **Qwen2.5VL Template Handling**: Improved template management for Qwen2.5VL models when templates are already present in prompts
  * **HuMo View Operation**: Fixed .view() operation issues in HuMo models for more stable video generation

  **Memory and Performance Optimizations**

  * **Memory Leak Fix**: Resolved memory leaks by explicitly detaching model finalizers, improving long-running workflow stability
  * **Sampler CFG Enhancement**: Added 'input\_cond' and 'input\_uncond' parameters to sampler CFG function arguments for more flexible conditioning control
</Update>

<Update label="v0.3.60" description="September 23, 2025">
  **New Model Support**

  * **Wan2.2 Animate**: Support for Wan2.2 Animate video generation model with character replacement and motion transfer capabilities
  * **Qwen Image Edit 2509 Support**: Support for the updated Qwen Image Edit 2509 with multi-image editing, higher consistency, and native ControlNet support
  * **HuMo Models**: Added support for both 1.7B and 17B HuMo models that use audio to drive video generation while maintaining lip sync
  * **Chroma Radiance**: A model that performs image generation in pixel space, reducing losses during the image generation process
  * **Omnigen2 UMO LoRA**: Added support for Omnigen2 UMO LoRA models

  **API Node Additions**

  * **Kling v2.1 Support**: Added kling-v2-1 model to KlingStartEndFrame node
  * **Seedream4 Fixes**: Fixed the flag that ignores errors on partial success, making workflows more robust

  **Node Schema Migration (V3)**

  * **Core Node Updates**: Migrated multiple node categories to V3 schema including Minimax API, Cosmos, conditioning, CFG, and Canny nodes

  **Performance & Technical Improvements**

  * **FP8 Operations**: Enabled FP8 operations by default on gfx1200 hardware for faster processing
  * **LoRA Trainer Fixes**: Resolved bugs with FP8 model compatibility in LoRA training workflows

  **Frontend Updates**

  * **Frontend Version Update**: Updated to version 1.26.13
</Update>

<Update label="v0.3.59" description="September 10, 2025">
  **ByteDance Seedream 4.0 Integration**

  * **New Seedream Node**: Added ByteDanceSeedream (4.0) node.
</Update>

<Update label="v0.3.58" description="September 6, 2025">
  **New Model Support**

  * Hunyuan Image 2.1 regular model
  * Hunyuan 3D 2.1

  **New API Nodes**

  * Stable Audio 2.5 API
  * Seedance Video API
</Update>

<Update label="v0.3.57" description="September 4, 2025">
  **ByteDance USO Model Support**

  * **UXO Subject Identity LoRA Support**: It's a subject identity LoRA model based on FLUX architecture
  * **Related Workflow**: Please find the workflow in the template `Flux` -> `Flux.1 Dev USO Reference Image Generation`

  **Workflow Utilities**

  * **ImageScaleToMaxDimension Node**: New utility for intelligent image scaling
  * **SEEDS Noise System**: Updated noise decomposition with improved algorithms
  * **Enhanced Prompt Control**: Interrupt handler now accepts prompt\_id parameters

  **Performance & Architecture**

  * **V3 Schema Migration**: Converted some core nodes to V3 schema
  * **Convolution AutoTuning**: Enabled automatic convolution optimization

  **New API Integration**

  * **ByteDance Image Nodes**: Added support for ByteDance image generation services
  * **Ideogram Character Reference**: Ideogram v3 API now supports character reference
</Update>

<Update label="v0.3.56" description="August 30, 2025">
  **Performance Enhancement**

  * **Reduced RAM Usage on Windows**
</Update>

<Update label="v0.3.55" description="August 29, 2025">
  **Wan2.2 S2V Workflow Enhancements & Model Support Expansion**

  This release focuses on Wan2.2 S2V related video workflow capabilities and model support expansion:

  **Wan2.2 S2V Workflow Control**

  * **WanSoundImageToVideoExtend Node**: New manual video extension node for audio-driven video workflows, giving creators precise control over generated video length and timing. This enables fine-tuned control over how audio content translates to video sequences.
  * **Audio-Video Synchronization**: Fixed critical issue where extending video past audio length caused workflow failures, ensuring reliable sound-to-video generation regardless of audio duration.
  * **Automatic Audio Trimming**: Video saves now automatically trim audio to match video length, eliminating audio-video sync issues in final output files.

  **Advanced Latent Processing**

  * **LatentCut Node**: New node for cutting latents at precise points, enabling more granular control over latent space manipulation in complex generation workflows. This is particularly useful for batch processing and temporal video workflows, such as removing specific frames from videos.

  **Wan2.2 5B Model Integration**

  * **Fun Control Model Support**: Added support for Wan2.2 5B fun control model.
  * **Fun Inpaint Model Support**: Integrated Wan2.2 5B fun inpaint model.
</Update>

<Update label="v0.3.54" description="August 28, 2025">
  **Node Model Patch Improvements**

  This focused update improves the core node model patching system that underpins ComfyUI's flexible architecture:

  **Core Infrastructure Enhancement**

  * **Node Model Patch Updates**: Enhanced nodes\_model\_patch.py with improvements to the underlying model patching mechanism, making ComfyUI extensions for Qwen-Image ControlNet easier

  **Workflow Benefits**

  * **Enhanced Stability**: Core model patching improvements contribute to more reliable node execution and model handling across different workflow configurations
</Update>

<Update label="v0.3.53" description="August 28, 2025">
  **Audio Workflow Integration & Enhanced Performance Optimizations**

  This release adds ComfyUI audio processing capabilities and includes performance improvements and model compatibility updates:

  **Audio Processing Updates**

  * **Wav2vec2 Audio Encoder**: Added native wav2vec2 implementation as an audio encoder model, enabling audio-to-embedding workflows for multimodal applications
  * **Audio Encoders Directory**: Added models/audio\_encoders directory, which is the audio encoder directory for Wan2.2 S2V
  * **AudioEncoderOutput V3 Support**: Made AudioEncoderOutput compatible with V3 node schema, ensuring seamless integration with modern workflow architectures

  **Google Gemini API Integration**

  * **Gemini Image API Node**: Added new Google Gemini Image API node, the "nano-Nano-banana" image editing model API with high consistency

  **Video Generation Performance & Memory Optimizations**

  * **WAN 2.2 S2V Model Support**: Work-in-progress implementation of WAN 2.2 Sound-to-Video model with optimized memory usage and performance
  * **Enhanced S2V Performance**: Performance improvements for video generation longer than 120 frames, improving extended video workflows
  * **Better Memory Estimation**: Improved memory usage estimation for S2V workflows prevents out-of-memory errors during long video generation
  * **Negative Audio Handling**: Fixed negative audio input handling in S2V workflows to use proper zero values

  **Sampling & Node Enhancements**

  * **DPM++ 2M SDE Heun (RES) Sampler**: New advanced sampler by @Balladie provides additional sampling options for fine-tuned generation control
  * **LatentConcat Node**: New node for concatenating latent tensors, enabling advanced latent space manipulation workflows
  * **EasyCache/LazyCache Stability**: Fixed critical crashes when tensor properties (shape/dtype/device) change during sampling, ensuring workflow reliability

  **Model Compatibility Improvements**

  * **ControlNet Type Models**: Enhanced compatibility fixes for ControlNet-type models working with Qwen Edit and Kontext workflows
  * **Flux Memory Optimization**: Adjusted Flux model memory usage factors for better resource utilization

  **Infrastructure & Reliability**

  * **Template Updates**: Updated to versions 0.1.66 and 0.1.68
  * **Documentation Cleanup**: Removed incompletely implemented models from readme to avoid user confusion
</Update>

<Update label="v0.3.52" description="August 23, 2025">
  **Enhanced Model Support & Qwen Image ControlNet Integration**

  This release significantly expands ControlNet capabilities and improves model compatibility, making ComfyUI workflows more versatile and reliable:

  **Qwen ControlNet Ecosystem**

  * **Diffsynth ControlNet Support**: Added support for Qwen Diffsynth ControlNets with Canny and depth conditioning, enabling precise edge and depth-based image control
  * **InstantX Qwen ControlNet**: Integrated InstantX Qwen ControlNet for expanded creative control options
  * **Inpaint ControlNet/Model Patches**: Enhanced inpainting capabilities with dedicated Diffsynth inpaint ControlNet support

  **Node Architecture & API Evolution**

  * **V3 Architecture Migration**: String nodes, Google Veo API, and Ideogram API nodes upgraded to V3 architecture for better performance and consistency
  * **Enhanced API Nodes**: OpenAI Chat node renamed to "OpenAI ChatGPT" for clarity, Gemini Chat node now includes copy button functionality
  * **Improved Usability**: API nodes now provide better user experience with clearer labeling and enhanced interaction features

  **Workflow Reliability & Performance**

  * **LTXV Noise Mask Fix**: Resolved key frame noise mask dimension issues when real noise masks exist, ensuring stable video workflow execution
  * **3D Latent Conditioning Control**: Fixed conditioning masks on 3D latents, enabling proper depth-aware conditioning control in advanced workflows
  * **Invalid Filename Handling**: Improved workflow save functionality with proper handling of invalid filenames, preventing save failures
  * **EasyCache & LazyCache**: Implemented advanced caching systems for improved workflow execution performance

  **Platform & Development Improvements**

  * **Python 3.13 Support**: Full compatibility with Python 3.13, keeping ComfyUI current with latest Python developments
  * **Frontend Update**: Updated to v1.25.10 with improved navigation and user interface enhancements
  * **Elementwise Fusions**: Added performance optimizations through elementwise operations fusion
  * **Navigation Mode Rollback**: Rolled back navigation default to traditional legacy mode, avoiding user experience issues caused by default enabled standard navigation mode. Users can still enable standard navigation mode in settings
</Update>

<Update label="v0.3.51" description="August 20, 2025">
  **Model Support**

  * **Qwen-Image-Edit Model**: Native support for Qwen-Image-Edit
  * **FluxKontextMultiReferenceLatentMethod Node**: Multi-reference input node for Flux workflows
  * **WAN 2.2 Fun Camera Model Support**: Support for video generation through camera control
  * **Template Updates**: Upgraded to version 0.1.62, added Wan2.2 Fun Camera and Qwen Image Edit templates

  **Core Function Improvements**

  * **Context Windows Support**: Enhanced sampling code to support longer sequence generation tasks
  * **SDPA Backend Optimization**: Improved Scaled Dot Product Attention backend settings for better performance

  **Multimedia Node Support**

  * **Audio Recording Node**: New native audio recording node, now you can record audio directly in ComfyUI
  * **Audio Video Integration**: Complete audio-video dependency integration

  **API Node Support Updates**

  * **GPT-5 Series Models**: Support for the latest GPT-5 models
  * **Kling V2-1 and V2-1-Master**: Updated video generation model functionality
  * **Minimax Hailuo Video Node**: New video generation node
  * **Vidu Video Node**: Vidu API node support
  * **Google Model Updates**: Added new Google Gemini models
  * **OpenAI API Fix**: Fixed MIME type errors in OpenAI API node input images

  **Performance Optimization**

  * **Intel GPU Compatibility**: Fixed Intel integrated GPU compatibility issues
  * **PyTorch Compatibility**: Enhanced compatibility with older PyTorch versions
  * **Torch Compile Optimization**: Improved torch compile behavior
  * **Memory Management**: Optimized installation size and memory efficiency

  **Frontend Changes**

  * **Subgraph Support**: Subgraph functionality support
  * **Shortcut Panel**: Added bottom shortcut panel
  * **UI Layout Modifications**: Modified terminal entry layout, added template, log panel and other entries
  * **Standard Canvas Mode**: Added standard canvas mode, can be switched in `Lite Graph` > `Canvas` > `Canvas Navigation Mode`
  * **Mini Map**: Added workflow mini map
  * **Tab Preview**: Added workflow tab preview
  * **Top Tab Menu Layout Adjustments**
</Update>

<Update label="v0.3.50" description="August 13, 2025">
  **Model Integration & Performance Enhancements**

  This release expands ComfyUI's model ecosystem with enhanced Qwen support, async API capabilities, and stability improvements for complex workflows:

  **Qwen Model Ecosystem**

  * **Qwen Image Model Support**: Improved integration including proper LoRA loading and model merging capabilities for sophisticated vision workflows
  * **Qwen Model Merging Node**: New dedicated node for merging Qwen image models, allowing creators to combine different model strengths
  * **SimpleTuner Lycoris LoRA Support**: Extended compatibility with SimpleTuner-trained Lycoris LoRAs for Qwen-Image models

  **API & Performance Infrastructure**

  * **Async API Nodes**: Introduction of asynchronous API nodes, enabling non-blocking workflow execution for better performance
  * **Memory Handling**: Enhanced RepeatLatentBatch node now properly handles multi-dimensional latents, fixing workflow interruptions
  * **WAN 2.2 Fun Control Support**: Added support for WAN 2.2 fun control features, expanding creative control for video workflows

  **Hardware Optimization & Compatibility**

  * **AMD GPU Improvements**: Enhanced AMD Radeon support with improved FP16 accuracy handling and performance optimization
  * **RDNA3 Architecture Fixes**: Resolved issues with gfx1201 GPUs when using Flux models with PyTorch attention
  * **Updated PyTorch Support**: Bumped CUDA and ROCM PyTorch versions with testing on Python 3.13 and CUDA 12.9

  **Developer Experience Enhancements**

  * **Cleaner Logging**: Feature flags now only display in verbose mode, reducing console noise
  * **Audio Processing Safety**: Enhanced torchaudio import safety checks prevent crashes when audio dependencies are unavailable
  * **Kling API Improvements**: Fixed image type parameter handling in Kling Image API nodes

  **Workflow Benefits**

  * **Async Workflow Execution**: New async API capabilities enable more responsive workflows when integrating external services
  * **Model Flexibility**: Expanded Qwen support allows for more diverse vision-language workflows with improved LoRA compatibility
  * **Hardware Utilization**: AMD GPU optimizations and updated PyTorch support improve performance across hardware configurations
  * **Batch Processing**: Fixed RepeatLatentBatch ensures reliable operation with complex multi-dimensional data structures
  * **Video Control**: WAN 2.2 fun control features provide advanced creative control for video generation workflows
</Update>

<Update label="v0.3.49" description="August 5, 2025">
  **UI Experience & Model Support**

  This release brings user experience improvements and model support that enhance workflow creation and performance:

  **User Interface Enhancements**

  * **Recently Used Items API**: New API for tracking recently used items in the interface, streamlining workflow creation
  * **Workflow Navigation**: Enhanced user experience with better organization of commonly accessed elements

  **Model Integration**

  * **Qwen Vision Model Support**: Initial support for Qwen image models with configuration options
  * **Image Processing**: Enhanced Qwen model integration allows for more versatile image analysis and generation workflows

  **Video Generation**

  * **Veo3 Video Generation**: Added Veo3 video generation node with integrated audio support
  * **Audio-Visual Synthesis**: Capability combining video and audio generation in a single node

  **Performance & Stability Improvements**

  * **Memory Management**: Optimized conditional VRAM usage through improved casting and device transfer operations
  * **Device Consistency**: Fixes ensuring all conditioning data and context remain on correct devices
  * **ControlNet Stability**: Resolved ControlNet compatibility issues, restoring functionality for image control workflows

  **Developer & System Enhancements**

  * **Error Handling**: Added warnings and crash prevention when conditioning devices don't match
  * **Template Updates**: Multiple template version updates (0.1.47, 0.1.48, 0.1.51) maintaining compatibility

  **Workflow Benefits**

  * **Faster Iteration**: Recently used items API enables quicker workflow assembly and modification
  * **Enhanced Creativity**: Qwen vision models open new possibilities for image understanding and manipulation workflows
  * **Video Production**: Veo3 integration transforms ComfyUI into a comprehensive multimedia creation platform
  * **Reliability**: Memory optimizations and device management fixes ensure stable operation with complex workflows
  * **Performance**: Optimized VRAM usage allows for more ambitious projects on systems with limited resources
</Update>

<Update label="v0.3.48" description="August 2, 2025">
  **API Enhancement & Performance Optimizations**

  This release introduces backend improvements and performance optimizations that enhance workflow execution and node development:

  **ComfyAPI Core Framework**

  * **ComfyAPI Core v0.0.2**: Update to the core API framework, providing improved stability and extensibility
  * **Partial Execution Support**: New backend support for partial workflow execution, enabling efficient processing of multi-stage workflows

  **Video Processing Improvements**

  * **WAN Camera Memory Optimization**: Enhanced memory management for WAN-based camera workflows, reducing VRAM usage
  * **WanFirstLastFrameToVideo Fix**: Resolved issue preventing proper video generation when clip vision components are not available

  **Performance & Model Optimizations**

  * **VAE Nonlinearity Enhancement**: Replaced manual activation functions with optimized torch.silu in VAE operations
  * **WAN VAE Optimizations**: Fine-tuning optimizations for WAN VAE operations, improving processing speed and memory efficiency

  **Node Schema Evolution**

  * **V3 Node Schema Definition**: Implementation of next-generation node schema system
  * **Template Updates**: Multiple template version updates (0.1.44, 0.1.45) ensuring compatibility

  **Workflow Development Benefits**

  * **Video Workflows**: Improved stability and performance for video generation pipelines
  * **Memory Management**: Optimized memory usage patterns enable more complex workflows on systems with limited VRAM
  * **API Reliability**: Core API enhancements provide more stable foundation for custom node development
  * **Execution Flexibility**: New partial execution capabilities allow for more efficient debugging and development
</Update>

<Update label="v0.3.47" description="July 30, 2025">
  **Memory Optimization & Large Model Performance**

  This release focuses on memory optimizations for large model workflows, improving performance with WAN 2.2 models and VRAM management:

  **WAN 2.2 Model Optimizations**

  * **Reduced Memory Footprint**: Eliminated unnecessary memory clones in WAN 2.2 VAE operations, reducing memory usage
  * **5B I2V Model Support**: Memory optimization for WAN 2.2 5B image-to-video models, making these models more accessible

  **Enhanced VRAM Management**

  * **Windows Large Card Support**: Added reserved VRAM allocation for high-end graphics cards on Windows
  * **Memory Allocation**: Improved memory management for users working with multiple large models simultaneously

  **Workflow Performance Benefits**

  * **VAE Processing**: WAN 2.2 VAE operations now run more efficiently with reduced memory overhead
  * **Large Model Inference**: Enhanced stability when working with billion-parameter models
  * **Batch Processing**: Memory optimizations enable better handling of batch operations with large models
</Update>

<Update label="v0.3.46" description="July 28, 2025">
  **Hardware Acceleration & Audio Processing**

  This release expands hardware support and enhances audio processing capabilities:

  **Audio Processing Enhancements**

  * **PyAV Audio Backend**: Replaced torchaudio.load with PyAV for more reliable audio processing in video workflows
  * **Audio Integration**: Enhanced audio handling for multimedia generation workflows

  **Hardware Support**

  * **Iluvatar CoreX Support**: Added native support for Iluvatar CoreX accelerators
  * **Intel XPU Optimization**: XPU support improvements including async offload capabilities
  * **AMD ROCm Enhancements**: Enabled PyTorch attention by default for gfx1201 on Torch 2.8
  * **CUDA Memory Management**: Fixed CUDA malloc to only activate on CUDA-enabled PyTorch installations

  **Sampling Algorithm Improvements**

  * **Euler CFG++ Enhancement**: Separated denoised and noise estimation processes in Euler CFG++ sampler
  * **WAN Model Support**: Added support for WAN (Wavelet-based Attention Network) models

  **Training Features**

  * **Training Nodes**: Added algorithm support, gradient accumulation, and optional gradient checkpointing
  * **Training Flexibility**: Better memory management and performance optimization for custom model training

  **Node & Workflow Enhancements**

  * **Moonvalley V2V Node**: Added Moonvalley Marey V2V node with enhanced input validation
  * **Negative Prompt Updates**: Improved negative prompt handling for Moonvalley nodes
  * **History API Enhancement**: Added map\_function parameter to get\_history API

  **API & System Improvements**

  * **Frontend Version Tracking**: Added required\_frontend\_version parameter in /system\_stats API response
  * **Device Information**: Enhanced XPU device name printing for better hardware identification
  * **Template Updates**: Multiple template updates (0.1.40, 0.1.41) ensuring compatibility

  **Developer Experience**

  * **Documentation Updates**: Enhanced README with examples and updated model integration guides
  * **Line Ending Fixes**: Improved cross-platform compatibility by standardizing line endings
  * **Code Cleanup**: Removed deprecated code and optimized components
</Update>

<Update label="v0.3.45" description="July 21, 2025">
  **Sampling & Training Improvements**

  This release introduces enhancements to sampling algorithms, training capabilities, and node functionality:

  **Sampling & Generation Features**

  * **SA-Solver Sampler**: New reconstructed SA-Solver sampling algorithm providing enhanced numerical stability
  * **Experimental CFGNorm Node**: Classifier-free guidance normalization for improved control over generation quality
  * **Nested Dual CFG Support**: Added nested style configuration to DualCFGGuider node
  * **SamplingPercentToSigma Node**: New utility node for precise sigma calculation from sampling percentages

  **Training Capabilities**

  * **Multi Image-Caption Dataset Support**: LoRA training node now handles multiple image-caption datasets simultaneously
  * **Training Loop Implementation**: Optimized training algorithms for improved convergence and stability
  * **Error Detection**: Added model detection error hints for LoRA operations

  **Platform & Performance Improvements**

  * **Async Node Support**: Full support for asynchronous node functions with earlier execution optimization
  * **Chroma Flexibility**: Un-hardcoded patch\_size parameter in Chroma
  * **LTXV VAE Decoder**: Switched to improved default padding mode for better image quality
  * **Safetensors Memory Management**: Added workaround for mmap issues

  **API & Integration Enhancements**

  * **Custom Prompt IDs**: API now allows specifying prompt IDs for better workflow tracking
  * **Kling API Optimization**: Increased polling timeout to prevent user timeouts
  * **History Token Cleanup**: Removed sensitive tokens from history items
  * **Python 3.9 Compatibility**: Fixed compatibility issues ensuring broader platform support

  **Bug Fixes & Stability**

  * **MaskComposite Fixes**: Resolved errors when destination masks have 2 dimensions
  * **Fresca Input/Output**: Corrected input and output handling for Fresca model workflows
  * **Reference Bug Fixes**: Resolved incorrect reference bugs in Gemini node implementations
  * **Line Ending Standardization**: Automated detection and removal of Windows line endings

  **Developer Experience**

  * **Warning Systems**: Added torch import mistake warnings to catch common configuration issues
  * **Template Updates**: Multiple template version updates (0.1.36, 0.1.37, 0.1.39) for improved custom node development
  * **Documentation**: Enhanced fast\_fp16\_accumulation documentation
</Update>

<Update label="v0.3.44" description="July 8, 2025">
  **Sampling & Model Control Enhancements**

  This release delivers improvements to sampling algorithms and model control systems:

  **Sampling Capabilities**

  * **TCFG Node**: Enhanced classifier-free guidance control for more nuanced generation control
  * **ER-SDE Sampler**: Migrated from VE to VP algorithm with new sampler node
  * **Skip Layer Guidance (SLG)**: Implementation for precise layer-level control during inference

  **Development Tools**

  * **Custom Node Management**: New `--whitelist-custom-nodes` argument pairs with `--disable-all-custom-nodes`
  * **Performance Optimizations**: Dual CFG node now optimizes automatically when CFG is 1.0
  * **GitHub Actions Integration**: Automated release webhook notifications

  **Image Processing Improvements**

  * **Transform Nodes**: Added ImageRotate and ImageFlip nodes for enhanced image manipulation
  * **ImageColorToMask Fix**: Corrected mask value returns for more accurate color-based masking
  * **3D Model Support**: Upload 3D models to custom subfolders for better organization

  **Guidance & Conditioning Enhancements**

  * **PerpNeg Guider**: Updated with improved pre and post-CFG handling
  * **Latent Conditioning Fix**: Resolved issues with conditioning at index > 0 for multi-step workflows
  * **Denoising Steps**: Added denoising step support to several samplers

  **Platform Stability**

  * **PyTorch Compatibility**: Fixed contiguous memory issues with PyTorch nightly builds
  * **FP8 Fallback**: Automatic fallback to regular operations when FP8 operations encounter exceptions
  * **Audio Processing**: Removed deprecated torchaudio.save function dependencies

  **Model Integration**

  * **Moonvalley Nodes**: Added native support for Moonvalley model workflows
  * **Scheduler Reordering**: Simple scheduler now defaults first
  * **Template Updates**: Multiple template version updates (0.1.31-0.1.35)

  **Security & Safety**

  * **Safe Loading**: Added warnings when loading files unsafely
  * **File Validation**: Enhanced checkpoint loading safety measures
</Update>

<Update label="v0.3.43" description="June 30, 2025">
  **Model Support & Workflow Reliability**

  This release brings improvements to model compatibility and workflow stability:

  **Expanded Model Documentation**: Added support documentation for Flux Kontext and Omnigen 2 models
  **VAE Encoding Improvements**: Removed unnecessary random noise injection during VAE encoding
  **Memory Management Fix**: Resolved a memory estimation bug affecting Kontext model usage
</Update>

<Update label="v0.3.41" description="June 17, 2025">
  **Model Support Additions**

  * **Cosmos Predict2 Support**: Implementation for text-to-image (2B and 14B models) and image-to-video generation workflows
  * **Flux Compatibility**: Chroma Text Encoder now works with regular Flux models
  * **LoRA Training Integration**: New native LoRA training node using weight adapter scheme

  **Performance & Hardware Optimizations**

  * **AMD GPU Enhancements**: Enabled FP8 operations and PyTorch attention on AMD GPUs
  * **Apple Silicon Fixes**: Addressed FP16 attention issues on Apple devices
  * **Flux Model Stability**: Resolved black image generation issues with certain Flux models

  **Sampling Improvements**

  * **Rectified Flow Samplers**: Added SEEDS and multistep DPM++ SDE samplers with RF support
  * **ModelSamplingContinuousEDM**: New cosmos\_rflow option for enhanced sampling control
  * **Memory Optimization**: Improved memory estimation for Cosmos models

  **Developer & Integration Features**

  * **SQLite Database Support**: Enhanced data management capabilities for custom nodes
  * **PyProject.toml Integration**: Automatic web folder registration from pyproject files
  * **Frontend Flexibility**: Support for semver suffixes and prerelease frontend versions
  * **Tokenizer Enhancements**: Configurable min\_length settings with tokenizer\_data

  **Quality of Life Improvements**

  * **Kontext Aspect Ratio Fix**: Resolved widget-only limitation
  * **SaveLora Consistency**: Standardized filename format across all save nodes
  * **Python Version Warnings**: Added alerts for outdated Python installations
  * **WebcamCapture Fixes**: Corrected IS\_CHANGED signature
</Update>

<Update label="v0.3.40" description="June 5, 2025">
  **Workflow Tools & Performance Optimizations**

  This release brings new workflow utilities and performance optimizations:

  **Workflow Tools**

  * **ImageStitch Node**: Concatenate multiple images seamlessly in your workflows
  * **GetImageSize Node**: Extract image dimensions with batch processing support
  * **Regex Replace Node**: Advanced text manipulation capabilities for workflows

  **Model Compatibility**

  * **Tensor Handling**: Streamlined list processing makes multi-model workflows more reliable
  * **BFL API Optimization**: Refined support for Kontext models with cleaner node interfaces
  * **Performance Boost**: Fused multiply-add operations in chroma processing for faster generation

  **Developer Experience**

  * **Custom Node Support**: Added pyproject.toml support for better dependency management
  * **Help Menu Integration**: New help system in the Node Library sidebar
  * **API Documentation**: Enhanced API nodes documentation

  **Frontend & UI Enhancements**

  * **Frontend Updated to v1.21.7**: Stability fixes and performance improvements
  * **Custom API Base Support**: Better subpath handling for custom deployment configurations
  * **Security Hardening**: XSS vulnerability fixes

  **Bug Fixes & Stability**

  * **Pillow Compatibility**: Updated deprecated API calls
  * **ROCm Support**: Improved version detection for AMD GPU users
  * **Template Updates**: Enhanced project templates for custom node development
</Update>


# Getting Started
Source: https://docs.comfy.org/comfy-cli/getting-started



### Overview

`comfy-cli` is a [command line tool](https://github.com/Comfy-Org/comfy-cli) that makes it easier to install and manage Comfy.

### Install CLI

<CodeGroup>
  ```bash pip theme={null}
  pip install comfy-cli
  ```

  ```bash homebrew theme={null}
  brew tap Comfy-Org/comfy-cli
  brew install comfy-org/comfy-cli/comfy-cli
  ```
</CodeGroup>

To get shell completion hints:

```bash  theme={null}
comfy --install-completion
```

### Install ComfyUI

Create a virtual environment with any Python version greater than 3.9.

<CodeGroup>
  ```bash conda theme={null}
  conda create -n comfy-env python=3.11
  conda activate comfy-env
  ```

  ```bash venv theme={null}
  python3 -m venv comfy-env
  source comfy-env/bin/activate
  ```
</CodeGroup>

Install ComfyUI

```bash  theme={null}
comfy install
```

<Warning>You still need to install CUDA, or ROCm depending on your GPU.</Warning>

### Run ComfyUI

```bash  theme={null}
comfy launch
```

### Manage Custom Nodes

```bash  theme={null}
comfy node install <NODE_NAME>
```

We use `cm-cli` for installing custom nodes. See the [docs](https://github.com/ltdrdata/ComfyUI-Manager/blob/main/docs/en/cm-cli.md) for more information.

### Manage Models

Downloading models with `comfy-cli` is easy. Just run:

```bash  theme={null}
comfy model download <url> models/checkpoints
```

### Contributing

We encourage contributions to comfy-cli! If you have suggestions, ideas, or bug reports, please open an issue on our [GitHub repository](https://github.com/Comfy-Org/comfy-cli/issues). If you want to contribute code, fork the repository and submit a pull request.

Refer to the [Dev Guide](https://github.com/Comfy-Org/comfy-cli/blob/main/DEV_README.md) for further details.

### Analytics

We track usage of the CLI to improve the user experience. You can disable this by running:

```bash  theme={null}
comfy tracking disable
```

To re-enable tracking, run:

```bash  theme={null}
comfy tracking enable
```


# Reference
Source: https://docs.comfy.org/comfy-cli/reference



# CLI

## Nodes

**Usage**:

```console  theme={null}
$ comfy node [OPTIONS] COMMAND [ARGS]...
```

**Options**:

* `--install-completion`: Install completion for the current shell.
* `--show-completion`: Show completion for the current shell, to copy it or customize the installation.
* `--help`: Show this message and exit.

**Commands**:

* `deps-in-workflow`
* `disable`
* `enable`
* `fix`
* `install`
* `install-deps`
* `reinstall`
* `restore-dependencies`
* `restore-snapshot`
* `save-snapshot`: Save a snapshot of the current ComfyUI...
* `show`
* `simple-show`
* `uninstall`
* `update`

### `deps-in-workflow`

**Usage**:

```console  theme={null}
$ deps-in-workflow [OPTIONS]
```

**Options**:

* `--workflow TEXT`: Workflow file (.json/.png)  \[required]
* `--output TEXT`: Workflow file (.json/.png)  \[required]
* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `disable`

**Usage**:

```console  theme={null}
$ disable [OPTIONS] ARGS...
```

**Arguments**:

* `ARGS...`: disable custom nodes  \[required]

**Options**:

* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `enable`

**Usage**:

```console  theme={null}
$ enable [OPTIONS] ARGS...
```

**Arguments**:

* `ARGS...`: enable custom nodes  \[required]

**Options**:

* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `fix`

**Usage**:

```console  theme={null}
$ fix [OPTIONS] ARGS...
```

**Arguments**:

* `ARGS...`: fix dependencies for specified custom nodes  \[required]

**Options**:

* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `install`

**Usage**:

```console  theme={null}
$ install [OPTIONS] ARGS...
```

**Arguments**:

* `ARGS...`: install custom nodes  \[required]

**Options**:

* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `install-deps`

**Usage**:

```console  theme={null}
$ install-deps [OPTIONS]
```

**Options**:

* `--deps TEXT`: Dependency spec file (.json)
* `--workflow TEXT`: Workflow file (.json/.png)
* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `reinstall`

**Usage**:

```console  theme={null}
$ reinstall [OPTIONS] ARGS...
```

**Arguments**:

* `ARGS...`: reinstall custom nodes  \[required]

**Options**:

* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `restore-dependencies`

**Usage**:

```console  theme={null}
$ restore-dependencies [OPTIONS]
```

**Options**:

* `--help`: Show this message and exit.

### `restore-snapshot`

**Usage**:

```console  theme={null}
$ restore-snapshot [OPTIONS] PATH
```

**Arguments**:

* `PATH`: \[required]

**Options**:

* `--help`: Show this message and exit.

### `save-snapshot`

Save a snapshot of the current ComfyUI environment

**Usage**:

```console  theme={null}
$ save-snapshot [OPTIONS]
```

**Options**:

* `--output TEXT`: Specify the output file path. (.json/.yaml)
* `--help`: Show this message and exit.

### `show`

**Usage**:

```console  theme={null}
$ show [OPTIONS] ARGS...
```

**Arguments**:

* `ARGS...`: \[installed|enabled|not-installed|disabled|all|snapshot|snapshot-list]  \[required]

**Options**:

* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `simple-show`

**Usage**:

```console  theme={null}
$ simple-show [OPTIONS] ARGS...
```

**Arguments**:

* `ARGS...`: \[installed|enabled|not-installed|disabled|all|snapshot|snapshot-list]  \[required]

**Options**:

* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `uninstall`

**Usage**:

```console  theme={null}
$ uninstall [OPTIONS] ARGS...
```

**Arguments**:

* `ARGS...`: uninstall custom nodes  \[required]

**Options**:

* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

### `update`

**Usage**:

```console  theme={null}
$ update [OPTIONS] ARGS...
```

**Arguments**:

* `ARGS...`: update custom nodes  \[required]

**Options**:

* `--channel TEXT`: Specify the operation mode
* `--mode TEXT`: \[remote|local|cache]
* `--help`: Show this message and exit.

## Models

**Usage**:

```console  theme={null}
$ comfy model [OPTIONS] COMMAND [ARGS]...
```

**Options**:

* `--install-completion`: Install completion for the current shell.
* `--show-completion`: Show completion for the current shell, to copy it or customize the installation.
* `--help`: Show this message and exit.

**Commands**:

* `download`: Download a model to a specified relative...
* `list`: Display a list of all models currently...
* `remove`: Remove one or more downloaded models,...

### `download`

Download a model to a specified relative path if it is not already downloaded.

**Usage**:

```console  theme={null}
$ download [OPTIONS]
```

**Options**:

* `--url TEXT`: The URL from which to download the model  \[required]
* `--relative-path TEXT`: The relative path from the current workspace to install the model.  \[default: models/checkpoints]
* `--help`: Show this message and exit.

### `list`

Display a list of all models currently downloaded in a table format.

**Usage**:

```console  theme={null}
$ list [OPTIONS]
```

**Options**:

* `--relative-path TEXT`: The relative path from the current workspace where the models are stored.  \[default: models/checkpoints]
* `--help`: Show this message and exit.

### `remove`

Remove one or more downloaded models, either by specifying them directly or through an interactive selection.

**Usage**:

```console  theme={null}
$ remove [OPTIONS]
```

**Options**:

* `--relative-path TEXT`: The relative path from the current workspace where the models are stored.  \[default: models/checkpoints]
* `--model-names TEXT`: List of model filenames to delete, separated by spaces
* `--help`: Show this message and exit.


# Getting Started
Source: https://docs.comfy.org/comfy-cli/troubleshooting



### Prerequisites

You need to have git installed on your system. Install it [here](https://git-scm.com/downloads).


# Datatypes
Source: https://docs.comfy.org/custom-nodes/backend/datatypes



These are the most important built in datatypes. You can also [define your own](./more_on_inputs#custom-datatypes).

Datatypes are used on the client side to prevent a workflow from passing the wrong form of data into a node - a bit like strong typing.
The JavaScript client side code will generally not allow a node output to be connected to an input of a different datatype,
although a few exceptions are noted below.

## Comfy datatypes

### COMBO

* No additional parameters in `INPUT_TYPES`

* Python datatype: defined as `list[str]`, output value is `str`

Represents a dropdown menu widget.
Unlike other datatypes, `COMBO` it is not specified in `INPUT_TYPES` by a `str`, but by a `list[str]`
corresponding to the options in the dropdown list, with the first option selected by default.

`COMBO` inputs are often dynamically generated at run time. For instance, in the built-in `CheckpointLoaderSimple` node, you find

```
"ckpt_name": (folder_paths.get_filename_list("checkpoints"), )
```

or they might just be a fixed list of options,

```
"play_sound": (["no","yes"], {}),
```

### Primitive and reroute

Primitive and reroute nodes only exist on the client side. They do not have an intrinsic datatype, but when connected they take on
the datatype of the input or output to which they have been connected (which is why they can't connect to a `*` input...)

## Python datatypes

### INT

* Additional parameters in `INPUT_TYPES`:

  * `default` is required

  * `min` and `max` are optional

* Python datatype `int`

### FLOAT

* Additional parameters in `INPUT_TYPES`:

  * `default` is required

  * `min`, `max`, `step` are optional

* Python datatype `float`

### STRING

* Additional parameters in `INPUT_TYPES`:

  * `default` is required

* Python datatype `str`

### BOOLEAN

* Additional parameters in `INPUT_TYPES`:

  * `default` is required

* Python datatype `bool`

## Tensor datatypes

### IMAGE

* No additional parameters in `INPUT_TYPES`

* Python datatype `torch.Tensor` with *shape* \[B,H,W,C]

A batch of `B` images, height `H`, width `W`, with `C` channels (generally `C=3` for `RGB`).

### LATENT

* No additional parameters in `INPUT_TYPES`

* Python datatype `dict`, containing a `torch.Tensor` with *shape* \[B,C,H,W]

The `dict` passed contains the key `samples`, which is a `torch.Tensor` with *shape* \[B,C,H,W] representing
a batch of `B` latents, with `C` channels (generally `C=4` for existing stable diffusion models), height `H`, width `W`.

The height and width are 1/8 of the corresponding image size (which is the value you set in the Empty Latent Image node).

Other entries in the dictionary contain things like latent masks.

{/* TODO new SD models might have different C values? */}

### MASK

* No additional parameters in `INPUT_TYPES`

* Python datatype `torch.Tensor` with *shape* \[H,W] or \[B,C,H,W]

### AUDIO

* No additional parameters in `INPUT_TYPES`

* Python datatype `dict`, containing a `torch.Tensor` with *shape* \[B, C, T] and a sample rate.

The `dict` passed contains the key `waveform`, which is a `torch.Tensor` with *shape* \[B, C, T] representing a batch of `B` audio samples, with `C` channels (`C=2` for stereo and `C=1` for mono), and `T` time steps (i.e., the number of audio samples).

The `dict` contains another key `sample_rate`, which indicates the sampling rate of the audio.

## Custom Sampling datatypes

### Noise

The `NOISE` datatype represents a *source* of noise (not the actual noise itself). It can be represented by any Python object
that provides a method to generate noise, with the signature `generate_noise(self, input_latent:Tensor) -> Tensor`, and a
property, `seed:Optional[int]`.

<Tip>The `seed` is passed into `sample` guider in the `SamplerCustomAdvanced`, but does not appear to be used in any of the standard guiders.
It is Optional, so you can generally set it to None.</Tip>

When noise is to be added, the latent is passed into this method, which should return a `Tensor` of the same shape containing the noise.

See the [noise mixing example](./snippets#creating-noise-variations)

### Sampler

The `SAMPLER` datatype represents a sampler, which is represented as a Python object providing a `sample` method.
Stable diffusion sampling is beyond the scope of this guide; see `comfy/samplers.py` if you want to dig into this part of the code.

### Sigmas

The `SIGMAS` datatypes represents the values of sigma before and after each step in the sampling process, as produced by a scheduler.
This is represented as a one-dimensional tensor, of length `steps+1`, where each element represents the noise expected to be present
before the corresponding step, with the final value representing the noise present after the final step.

A `normal` scheduler, with 20 steps and denoise of 1, for an SDXL model, produces:

```
tensor([14.6146, 10.7468,  8.0815,  6.2049,  4.8557,  
         3.8654,  3.1238,  2.5572,  2.1157,  1.7648,  
         1.4806,  1.2458,  1.0481,  0.8784,  0.7297,  
         0.5964,  0.4736,  0.3555,  0.2322,  0.0292,  0.0000])
```

<Tip>The starting value of sigma depends on the model, which is why a scheduler node requires a `MODEL` input to produce a SIGMAS output</Tip>

### Guider

A `GUIDER` is a generalisation of the denoising process, as 'guided' by a prompt or any other form of conditioning. In Comfy the guider is
represented by a `callable` Python object providing a `__call__(*args, **kwargs)` method which is called by the sample.

The `__call__` method takes (in `args[0]`) a batch of noisy latents (tensor `[B,C,H,W]`), and returns a prediction of the noise (a tensor of the same shape).

## Model datatypes

There are a number of more technical datatypes for stable diffusion models. The most significant ones are `MODEL`, `CLIP`, `VAE` and `CONDITIONING`.
Working with these is (for the time being) beyond the scope of this guide! {/* TODO but maybe not forever */}

## Additional Parameters

Below is a list of officially supported keys that can be used in the 'extra options' portion of an input definition.

<Warning>You can use additional keys for your own custom widgets, but should *not* reuse any of the keys below for other purposes.</Warning>

| Key              | Description                                                                                                                                                                                      |
| ---------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `default`        | The default value of the widget                                                                                                                                                                  |
| `min`            | The minimum value of a number (`FLOAT` or `INT`)                                                                                                                                                 |
| `max`            | The maximum value of a number (`FLOAT` or `INT`)                                                                                                                                                 |
| `step`           | The amount to increment or decrement a widget                                                                                                                                                    |
| `label_on`       | The label to use in the UI when the bool is `True` (`BOOL`)                                                                                                                                      |
| `label_off`      | The label to use in the UI when the bool is `False` (`BOOL`)                                                                                                                                     |
| `defaultInput`   | Defaults to an input socket rather than a supported widget                                                                                                                                       |
| `forceInput`     | `defaultInput` and also don't allow converting to a widget                                                                                                                                       |
| `multiline`      | Use a multiline text box (`STRING`)                                                                                                                                                              |
| `placeholder`    | Placeholder text to display in the UI when empty (`STRING`)                                                                                                                                      |
| `dynamicPrompts` | Causes the front-end to evaluate dynamic prompts                                                                                                                                                 |
| `lazy`           | Declares that this input uses [Lazy Evaluation](./lazy_evaluation)                                                                                                                               |
| `rawLink`        | When a link exists, rather than receiving the evaluated value, you will receive the link (i.e. `["nodeId", <outputIndex>]`). Primarily useful when your node uses [Node Expansion](./expansion). |


# Node Expansion
Source: https://docs.comfy.org/custom-nodes/backend/expansion



## Node Expansion

Normally, when a node is executed, that execution function immediately returns the output results of that node. "Node Expansion" is a relatively advanced technique that allows nodes to return a new subgraph of nodes that should take its place in the graph. This technique is what allows custom nodes to implement loops.

### A Simple Example

First, here's a simple example of what node expansion looks like:

<Tip>We highly recommend using the `GraphBuilder` class when creating subgraphs. It isn't mandatory, but it prevents you from making many easy mistakes.</Tip>

```python  theme={null}
def load_and_merge_checkpoints(self, checkpoint_path1, checkpoint_path2, ratio):
    from comfy_execution.graph_utils import GraphBuilder # Usually at the top of the file
    graph = GraphBuilder()
    checkpoint_node1 = graph.node("CheckpointLoaderSimple", checkpoint_path=checkpoint_path1)
    checkpoint_node2 = graph.node("CheckpointLoaderSimple", checkpoint_path=checkpoint_path2)
    merge_model_node = graph.node("ModelMergeSimple", model1=checkpoint_node1.out(0), model2=checkpoint_node2.out(0), ratio=ratio)
    merge_clip_node = graph.node("ClipMergeSimple", clip1=checkpoint_node1.out(1), clip2=checkpoint_node2.out(1), ratio=ratio)
    return {
        # Returning (MODEL, CLIP, VAE) outputs
        "result": (merge_model_node.out(0), merge_clip_node.out(0), checkpoint_node1.out(2)),
        "expand": graph.finalize(),
    }
```

While this same node could previously be implemented by manually calling into ComfyUI internals, using expansion means that each subnode will be cached separately (so if you change `model2`, you don't have to reload `model1`).

### Requirements

In order to perform node expansion, a node must return a dictionary with the following keys:

1. `result`: A tuple of the outputs of the node. This may be a mix of finalized values (like you would return from a normal node) and node outputs.
2. `expand`: The finalized graph to perform expansion on. See below if you are not using the `GraphBuilder`.

#### Additional Requirements if not using GraphBuilder

The format expected from the `expand` key is the same as the ComfyUI API format. The following requirements are handled by the `GraphBuilder`, but must be handled manually if you choose to forego it:

1. Node IDs must be unique across the entire graph. (This includes between multiple executions of the same node due to the use of lists.)
2. Node IDs must be deterministic and consistent between multiple executions of the graph (including partial executions due to caching).

Even if you don't want to use the `GraphBuilder` for actually building the graph (e.g. because you're loading the raw json of the graph from a file), you can use the `GraphBuilder.alloc_prefix()` function to generate a prefix and `comfy.graph_utils.add_graph_prefix` to fix existing graphs to meet these requirements.

### Efficient Subgraph Caching

While you can pass non-literal inputs to nodes within the subgraph (like torch tensors), this can inhibit caching *within* the subgraph. When possible, you should pass links to subgraph objects rather than the node itself. (You can declare an input as a `rawLink` within the input's [Additional Parameters](./datatypes#additional-parameters) to do this easily.)


# Images, Latents, and Masks
Source: https://docs.comfy.org/custom-nodes/backend/images_and_masks



When working with these datatypes, you will need to know about the `torch.Tensor` class.
Complete documentation is [here](https://pytorch.org/docs/stable/tensors.html), or
an introduction to the key concepts required for Comfy [here](./tensors).

<Warning>If your node has a single output which is a tensor, remember to return `(image,)` not `(image)`</Warning>

Most of the concepts below are illustrated in the [example code snippets](./snippets).

## Images

An IMAGE is a `torch.Tensor` with shape `[B,H,W,C]`, `C=3`. If you are going to save or load images, you will
need to convert to and from `PIL.Image` format - see the code snippets below! Note that some `pytorch` operations
offer (or expect) `[B,C,H,W]`, known as 'channel first', for reasons of computational efficiency. Just be careful.

### Working with PIL.Image

If you want to load and save images, you'll want to use PIL:

```python  theme={null}
from PIL import Image, ImageOps
```

## Masks

A MASK is a `torch.Tensor` with shape `[B,H,W]`.
In many contexts, masks have binary values (0 or 1), which are used to indicate which pixels should undergo specific operations.
In some cases values between 0 and 1 are used indicate an extent of masking, (for instance, to alter transparency, adjust filters, or composite layers).

### Masks from the Load Image Node

The `LoadImage` node uses an image's alpha channel (the "A" in "RGBA") to create MASKs.
The values from the alpha channel are normalized to the range \[0,1] (torch.float32) and then inverted.
The `LoadImage` node always produces a MASK output when loading an image. Many images (like JPEGs) don't have an alpha channel.
In these cases, `LoadImage` creates a default mask with the shape `[1, 64, 64]`.

### Understanding Mask Shapes

In libraries like `numpy`, `PIL`, and many others, single-channel images (like masks) are typically represented as 2D arrays, shape `[H,W]`.
This means the `C` (channel) dimension is implicit, and thus unlike IMAGE types, batches of MASKs have only three dimensions: `[B, H, W]`.
It is not uncommon to encounter a mask which has had the `B` dimension implicitly squeezed, giving a tensor `[H,W]`.

To use a MASK, you will often have to match shapes by unsqueezing to produce a shape `[B,H,W,C]` with `C=1`
To unsqueezing the `C` dimension, so you should `unsqueeze(-1)`, to unsqueeze `B`, you `unsqueeze(0)`.
If your node receives a MASK as input, you would be wise to always check `len(mask.shape)`.

## Latents

A LATENT is a `dict`; the latent sample is referenced by the key `samples` and has shape `[B,C,H,W]`, with `C=4`.

<Tip>LATENT is channel first, IMAGE is channel last</Tip>


# Lazy Evaluation
Source: https://docs.comfy.org/custom-nodes/backend/lazy_evaluation



## Lazy Evaluation

By default, all `required` and `optional` inputs are evaluated before a node can be run. Sometimes, however, an
input won't necessarily be used and evaluating it would result in unnecessary processing. Here are some examples
of nodes where lazy evaluation may be beneficial:

1. A `ModelMergeSimple` node where the ratio is either `0.0` (in which case the first model doesn't need to be loaded)
   or `1.0` (in which case the second model doesn't need to be loaded).
2. Interpolation between two images where the ratio (or mask) is either entirely `0.0` or entirely `1.0`.
3. A Switch node where one input determines which of the other inputs will be passed through.

<Tip>There is very little cost in making an input lazy. If it's something you can do, you generally should.</Tip>

### Creating Lazy Inputs

There are two steps to making an input a "lazy" input. They are:

1. Mark the input as lazy in the dictionary returned by `INPUT_TYPES`
2. Define a method named `check_lazy_status` (note: *not* a class method) that will be called prior to evaluation to determine if any more inputs are necessary.

To demonstrate these, we'll make a "MixImages" node that interpolates between two images according to a mask. If the entire mask is `0.0`, we don't need to evaluate any part of the tree leading up to the second image. If the entire mask is `1.0`, we can skip evaluating the first image.

#### Defining `INPUT_TYPES`

Declaring that an input is lazy is as simple as adding a `lazy: True` key-value pair to the input's options dictionary.

```python  theme={null}
@classmethod
def INPUT_TYPES(cls):
    return {
        "required": {
            "image1": ("IMAGE",{"lazy": True}),
            "image2": ("IMAGE",{"lazy": True}),
            "mask": ("MASK",),
        },
    }
```

In this example, `image1` and `image2` are both marked as lazy inputs, but `mask` will always be evaluated.

#### Defining `check_lazy_status`

A `check_lazy_status` method is called if there are one or more lazy inputs that are not yet available. This method receives the same arguments as the standard execution function. All available inputs are passed in with their final values while unavailable lazy inputs have a value of `None`.

The responsibility of the `check_lazy_status` function is to return a list of the names of any lazy inputs that are needed to proceed. If all lazy inputs are available, the function should return an empty list.

Note that `check_lazy_status` may be called multiple times. (For example, you might find after evaluating one lazy input that you need to evaluate another.)

<Tip>Note that because the function uses actual input values, it is *not* a class method.</Tip>

```python  theme={null}
def check_lazy_status(self, mask, image1, image2):
    mask_min = mask.min()
    mask_max = mask.max()
    needed = []
    if image1 is None and (mask_min != 1.0 or mask_max != 1.0):
        needed.append("image1")
    if image2 is None and (mask_min != 0.0 or mask_max != 0.0):
        needed.append("image2")
    return needed
```

### Full Example

```python  theme={null}
class LazyMixImages:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image1": ("IMAGE",{"lazy": True}),
                "image2": ("IMAGE",{"lazy": True}),
                "mask": ("MASK",),
            },
        }

    RETURN_TYPES = ("IMAGE",)
    FUNCTION = "mix"

    CATEGORY = "Examples"

    def check_lazy_status(self, mask, image1, image2):
        mask_min = mask.min()
        mask_max = mask.max()
        needed = []
        if image1 is None and (mask_min != 1.0 or mask_max != 1.0):
            needed.append("image1")
        if image2 is None and (mask_min != 0.0 or mask_max != 0.0):
            needed.append("image2")
        return needed

    # Not trying to handle different batch sizes here just to keep the demo simple
    def mix(self, mask, image1, image2):
        mask_min = mask.min()
        mask_max = mask.max()
        if mask_min == 0.0 and mask_max == 0.0:
            return (image1,)
        elif mask_min == 1.0 and mask_max == 1.0:
            return (image2,)

        result = image1 * (1. - mask) + image2 * mask,
        return (result[0],)
```

## Execution Blocking

While Lazy Evaluation is the recommended way to "disable" part of a graph, there are times when you want to disable an `OUTPUT` node that doesn't implement lazy evaluation itself. If it's an output node that you developed yourself, you should just add lazy evaluation as follows:

1. Add a required (if this is a new node) or optional (if you care about backward compatibility) input for `enabled` that defaults to `True`
2. Make all other inputs `lazy` inputs
3. Only evaluate the other inputs if `enabled` is `True`

If it's not a node you control, you can make use of a `comfy_execution.graph.ExecutionBlocker`. This special object can be returned as an output from any socket. Any nodes which receive an `ExecutionBlocker` as input will skip execution and return that `ExecutionBlocker` for any outputs.

<Tip>**There is intentionally no way to stop an ExecutionBlocker from propagating forward.** If you think you want this, you should really be using Lazy Evaluation.</Tip>

### Usage

There are two ways to construct and use an `ExecutionBlocker`

1. Pass `None` into the constructor to silently block execution. This is useful for cases where blocking execution is part of a successful run -- like disabling an output.

```python  theme={null}
def silent_passthrough(self, passthrough, blocked):
    if blocked:
        return (ExecutionBlocker(None),)
    else:
        return (passthrough,)
```

2. Pass a string into the constructor to display an error message when a node is blocked due to receiving the object. This can be useful if you want to display a meaningful error message if someone uses a meaningless output -- for example, the `VAE` output when loading a model that doesn't contain VAEs.

```python  theme={null}
def load_checkpoint(self, ckpt_name):
    ckpt_path = folder_paths.get_full_path("checkpoints", ckpt_name)
    model, clip, vae = load_checkpoint(ckpt_path)
    if vae is None:
        # This error is more useful than a "'NoneType' has no attribute" error
        # in a later node
        vae = ExecutionBlocker(f"No VAE contained in the loaded model {ckpt_name}")
    return (model, clip, vae)
```


# Lifecycle
Source: https://docs.comfy.org/custom-nodes/backend/lifecycle



## How Comfy loads custom nodes

When Comfy starts, it scans the directory `custom_nodes` for Python modules, and attempts to load them.
If the module exports `NODE_CLASS_MAPPINGS`, it will be treated as a custom node.
<Tip>A python module is a directory containing an `__init__.py` file.
The module exports whatever is listed in the `__all__` attribute defined in `__init__.py`.</Tip>

### **init**.py

`__init__.py` is executed when Comfy attempts to import the module. For a module to be recognized as containing
custom node definitions, it needs to export `NODE_CLASS_MAPPINGS`. If it does (and if nothing goes wrong in the import),
the nodes defined in the module will be available in Comfy. If there is an error in your code,
Comfy will continue, but will report the module as having failed to load. So check the Python console!

A very simple `__init__.py` file would look like this:

```python  theme={null}
from .python_file import MyCustomNode
NODE_CLASS_MAPPINGS = { "My Custom Node" : MyCustomNode }
__all__ = ["NODE_CLASS_MAPPINGS"]
```

#### NODE\_CLASS\_MAPPINGS

`NODE_CLASS_MAPPINGS` must be a `dict` mapping custom node names (unique across the Comfy install)
to the corresponding node class.

#### NODE\_DISPLAY\_NAME\_MAPPINGS

`__init__.py` may also export `NODE_DISPLAY_NAME_MAPPINGS`, which maps the same unique name to a display name for the node.
If `NODE_DISPLAY_NAME_MAPPINGS` is not provided, Comfy will use the unique name as the display name.

#### WEB\_DIRECTORY

If you are deploying client side code, you will also need to export the path, relative to the module, in which the
JavaScript files are to be found. It is conventional to place these in a subdirectory of your custom node named `js`.
<Tip>*Only* `.js` files will be served; you can't deploy `.css` or other types in this way</Tip>

<Warning>In previous versions of Comfy, `__init__.py` was required to copy the JavaScript files into the main Comfy web
subdirectory. You will still see code that does this. Don't.</Warning>


# Data lists
Source: https://docs.comfy.org/custom-nodes/backend/lists



## Length one processing

Internally, the Comfy server represents data flowing from one node to the next as a Python `list`, normally length 1, of the relevant datatype.
In normal operation, when a node returns an output, each element in the output `tuple` is separately wrapped in a list (length 1); then when the
next node is called, the data is unwrapped and passed to the main function.

<Tip>You generally don't need to worry about this, since Comfy does the wrapping and unwrapping.</Tip>

<Tip>This isn't about batches. A batch (of, for instance, latents, or images) is a *single entry* in the list (see [tensor datatypes](./images_and_masks))</Tip>

## List processing

In some circumstance, multiple data instances are processed in a single workflow, in which case the internal data will be a list containing the data instances.
An example of this might be processing a series of images one at a time to avoid running out of VRAM, or handling images of different sizes.

By default, Comfy will process the values in the list sequentially:

* if the inputs are `list`s of different lengths, the shorter ones are padded by repeating the last value
* the main method is called once for each value in the input lists
* the outputs are `list`s, each of which is the same length as the longest input

The relevant code can be found in the method `map_node_over_list` in `execution.py`.

However, as Comfy wraps node outputs into a `list` of length one, if the `tuple` returned by
a custom node contains a `list`, that `list` will be wrapped, and treated as a single piece of data.
In order to tell Comfy that the list being returned should not be wrapped, but treated as a series of data for sequential processing,
the node should provide a class attribute `OUTPUT_IS_LIST`, which is a `tuple[bool]`, of the same length as `RETURN_TYPES`, specifying
which outputs which should be so treated.

A node can also override the default input behaviour and receive the whole list in a single call. This is done by setting a class attribute
`INPUT_IS_LIST` to `True`.

Here's a (lightly annotated) example from the built in nodes - `ImageRebatch` takes one or more batches of images (received as a list, because `INPUT_IS_LIST - True`)
and rebatches them into batches of the requested size.

<Tip>`INPUT_IS_LIST` is node level - all inputs get the same treatment. So the value of the `batch_size` widget is given by `batch_size[0]`.</Tip>

```Python  theme={null}

class ImageRebatch:
    @classmethod
    def INPUT_TYPES(s):
        return {"required": { "images": ("IMAGE",),
                              "batch_size": ("INT", {"default": 1, "min": 1, "max": 4096}) }}
    RETURN_TYPES = ("IMAGE",)
    INPUT_IS_LIST = True
    OUTPUT_IS_LIST = (True, )
    FUNCTION = "rebatch"
    CATEGORY = "image/batch"

    def rebatch(self, images, batch_size):
        batch_size = batch_size[0]    # everything comes as a list, so batch_size is list[int]

        output_list = []
        all_images = []
        for img in images:                    # each img is a batch of images
            for i in range(img.shape[0]):     # each i is a single image
                all_images.append(img[i:i+1])

        for i in range(0, len(all_images), batch_size): # take batch_size chunks and turn each into a new batch
            output_list.append(torch.cat(all_images[i:i+batch_size], dim=0))  # will die horribly if the image batches had different width or height!

        return (output_list,)
```

#### INPUT\_IS\_LIST


# Hidden and Flexible inputs
Source: https://docs.comfy.org/custom-nodes/backend/more_on_inputs



## Hidden inputs

Alongside the `required` and `optional` inputs, which create corresponding inputs or widgets on the client-side,
there are three `hidden` input options which allow the custom node to request certain information from the server.

These are accessed by returning a value for `hidden` in the `INPUT_TYPES` `dict`, with the signature `dict[str,str]`,
containing one or more of `PROMPT`, `EXTRA_PNGINFO`, or `UNIQUE_ID`

```python  theme={null}
@classmethod
def INPUT_TYPES(s):
    return {
        "required": {...},
        "optional": {...},
        "hidden": {
            "unique_id": "UNIQUE_ID",
            "prompt": "PROMPT", 
            "extra_pnginfo": "EXTRA_PNGINFO",
        }
    }
```

### UNIQUE\_ID

`UNIQUE_ID` is the unique identifier of the node, and matches the `id` property of the node on the client side.
It is commonly used in client-server communications (see [messages](/development/comfyui-server/comms_messages#getting-node-id)).

### PROMPT

`PROMPT` is the complete prompt sent by the client to the server.
See [the prompt object](/custom-nodes/js/javascript_objects_and_hijacking#prompt) for a full description.

### EXTRA\_PNGINFO

`EXTRA_PNGINFO` is a dictionary that will be copied into the metadata of any `.png` files saved. Custom nodes can store additional
information in this dictionary for saving (or as a way to communicate with a downstream node).

<Tip>Note that if Comfy is started with the `disable_metadata` option, this data won't be saved.</Tip>

### DYNPROMPT

`DYNPROMPT` is an instance of `comfy_execution.graph.DynamicPrompt`. It differs from `PROMPT` in that it may mutate during the course of execution in response to [Node Expansion](/custom-nodes/backend/expansion).
<Tip>`DYNPROMPT` should only be used for advanced cases (like implementing loops in custom nodes).</Tip>

## Flexible inputs

### Custom datatypes

If you want to pass data between your own custom nodes, you may find it helpful to define a custom datatype. This is (almost) as simple as
just choosing a name for the datatype, which should be a unique string in upper case, such as `CHEESE`.

You can then use `CHEESE` in your node `INPUT_TYPES` and `RETURN_TYPES`, and the Comfy client will only allow `CHEESE` outputs to connect to a `CHEESE` input.
`CHEESE` can be any python object.

The only point to note is that because the Comfy client doesn't know about `CHEESE` you need (unless you define a custom widget for `CHEESE`,
which is a topic for another day), to force it to be an input rather than a widget. This can be done with the `forceInput` option in the input options dictionary:

```python  theme={null}
@classmethod
def INPUT_TYPES(s):
    return {
        "required": { "my_cheese": ("CHEESE", {"forceInput":True}) }
    }
```

### Wildcard inputs

```python  theme={null}
@classmethod
def INPUT_TYPES(s):
    return {
        "required": { "anything": ("*",{})},
    }

@classmethod
def VALIDATE_INPUTS(s, input_types):
    return True
```

The frontend allows `*` to indicate that an input can be connected to any source. Because this is not officially supported by the backend, you can skip the backend validation of types by accepting a parameter named `input_types` in your `VALIDATE_INPUTS` function. (See [VALIDATE\_INPUTS](./server_overview#validate-inputs) for more information.)
It's up to the node to make sense of the data that is passed.

### Dynamically created inputs

If inputs are dynamically created on the client side, they can't be defined in the Python source code.
In order to access this data we need an `optional` dictionary that allows Comfy to pass data with
arbitrary names. Since the Comfy server

```python  theme={null}
class ContainsAnyDict(dict):
    def __contains__(self, key):
        return True
...

@classmethod
def INPUT_TYPES(s):
    return {
        "required": {},
        "optional": ContainsAnyDict()
    }
...

def main_method(self, **kwargs):
    # the dynamically created input data will be in the dictionary kwargs

```

<Tip>Hat tip to rgthree for this pythonic trick!</Tip>


# Properties
Source: https://docs.comfy.org/custom-nodes/backend/server_overview

Properties of a custom node

### Simple Example

Here's the code for the Invert Image Node, which gives an overview of the key concepts in custom node development.

```python  theme={null}
class InvertImageNode:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": { "image_in" : ("IMAGE", {}) },
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image_out",)
    CATEGORY = "examples"
    FUNCTION = "invert"

    def invert(self, image_in):
        image_out = 1 - image_in
        return (image_out,)
```

### Main properties

Every custom node is a Python class, with the following key properties:

#### INPUT\_TYPES

`INPUT_TYPES`, as the name suggests, defines the inputs for the node. The method returns a `dict`
which *must* contain the key `required`, and *may* also include the keys `optional` and/or `hidden`. The only difference
between `required` and `optional` inputs is that `optional` inputs can be left unconnected.
For more information on `hidden` inputs, see [Hidden Inputs](./more_on_inputs#hidden-inputs).

Each key has, as its value, another `dict`, in which key-value pairs specify the names and types of the inputs.
The types are defined by a `tuple`, the first element of which defines the data type,
and the second element of which is a `dict` of additional parameters.

Here we have just one required input, named `image_in`, of type `IMAGE`, with no additional parameters.

Note that unlike the next few attributes, this `INPUT_TYPES` is a `@classmethod`. This is so
that the options in dropdown widgets (like the name of the checkpoint to be loaded) can be
computed by Comfy at run time. We'll go into this more later. {/* TODO link when written */}

#### RETURN\_TYPES

A `tuple` of `str` defining the data types returned by the node.
If the node has no outputs this must still be provided `RETURN_TYPES = ()`
<Warning>If you have exactly one output, remember the trailing comma: `RETURN_TYPES = ("IMAGE",)`.
This is required for Python to make it a `tuple`</Warning>

#### RETURN\_NAMES

The names to be used to label the outputs. This is optional; if omitted, the names are simply the `RETURN_TYPES` in lowercase.

#### CATEGORY

Where the node will be found in the ComfyUI **Add Node** menu. Submenus can be specified as a path, eg. `examples/trivial`.

#### FUNCTION

The name of the Python function in the class that should be called when the node is executed.

The function is called with named arguments. All `required` (and `hidden`) inputs will be included;
`optional` inputs will be included only if they are connected, so you should provide default values for them in the function
definition (or capture them with `**kwargs`).

The function returns a tuple corresponding to the `RETURN_TYPES`. This is required even if nothing is returned (`return ()`).
Again, if you only have one output, remember that trailing comma `return (image_out,)`!

### Execution Control Extras

A great feature of Comfy is that it caches outputs,
and only executes nodes that might produce a different result than the previous run.
This can greatly speed up lots of workflows.

In essence this works by identifying which nodes produce an output (these, notably the Image Preview and Save Image nodes, are always executed), and then working
backwards to identify which nodes provide data that might have changed since the last run.

Two optional features of a custom node assist in this process.

#### OUTPUT\_NODE

By default, a node is not considered an output. Set `OUTPUT_NODE = True` to specify that it is.

#### IS\_CHANGED

By default, Comfy considers that a node has changed if any of its inputs or widgets have changed.
This is normally correct, but you may need to override this if, for instance, the node uses a random
number (and does not specify a seed - it's best practice to have a seed input in this case so that
the user can control reproducibility and avoid unnecessary execution), or loads an input that may have
changed externally, or sometimes ignores inputs (so doesn't need to execute just because those inputs changed).

<Warning>Despite the name, IS\_CHANGED should not return a `bool`</Warning>

`IS_CHANGED` is passed the same arguments as the main function defined by `FUNCTION`, and can return any
Python object. This object is compared with the one returned in the previous run (if any) and the node
will be considered to have changed if `is_changed != is_changed_old` (this code is in `execution.py` if you need to dig).

Since `True == True`, a node that returns `True` to say it has changed will be considered not to have! I'm sure this would
be changed in the Comfy code if it wasn't for the fact that it might break existing nodes to do so.

To specify that your node should always be considered to have changed (which you should avoid if possible, since it
stops Comfy optimising what gets run), `return float("NaN")`. This returns a `NaN` value, which is not equal
to anything, even another `NaN`.

A good example of actually checking for changes is the code from the built-in LoadImage node, which loads the image and returns a hash

```python  theme={null}
    @classmethod
    def IS_CHANGED(s, image):
        image_path = folder_paths.get_annotated_filepath(image)
        m = hashlib.sha256()
        with open(image_path, 'rb') as f:
            m.update(f.read())
        return m.digest().hex()
```

### Other attributes

There are three other attributes that can be used to modify the default Comfy treatment of a node.

#### INPUT\_IS\_LIST, OUTPUT\_IS\_LIST

These are used to control sequential processing of data, and are described [later](./lists).

### VALIDATE\_INPUTS

If a class method `VALIDATE_INPUTS` is defined, it will be called before the workflow begins execution.
`VALIDATE_INPUTS` should return `True` if the inputs are valid, or a message (as a `str`) describing the error (which will prevent execution).

#### Validating Constants

<Warning>Note that `VALIDATE_INPUTS` will only receive inputs that are defined as constants within the workflow. Any inputs that are received from other nodes will *not* be available in `VALIDATE_INPUTS`.</Warning>

`VALIDATE_INPUTS` is called with only the inputs that its signature requests (those returned by `inspect.getfullargspec(obj_class.VALIDATE_INPUTS).args`). Any inputs which are received in this way will *not* run through the default validation rules. For example, in the following snippet, the front-end will use the specified `min` and `max` values of the `foo` input, but the back-end will not enforce it.

```python  theme={null}
class CustomNode:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": { "foo" : ("INT", {"min": 0, "max": 10}) },
        }

    @classmethod
    def VALIDATE_INPUTS(cls, foo):
        # YOLO, anything goes!
        return True
```

Additionally, if the function takes a `**kwargs` input, it will receive *all* available inputs and all of them will skip validation as if specified explicitly.

#### Validating Types

If the `VALIDATE_INPUTS` method receives an argument named `input_types`, it will be passed a dictionary in which the key is the name of each input which is connected to an output from another node and the value is the type of that output.

When this argument is present, all default validation of input types is skipped. Here's an example making use of the fact that the front-end allows for the specification of multiple types:

```python  theme={null}
class AddNumbers:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "input1" : ("INT,FLOAT", {"min": 0, "max": 1000})
                "input2" : ("INT,FLOAT", {"min": 0, "max": 1000})
            },
        }

    @classmethod
    def VALIDATE_INPUTS(cls, input_types):
        # The min and max of input1 and input2 are still validated because
        # we didn't take `input1` or `input2` as arguments
        if input_types["input1"] not in ("INT", "FLOAT"):
            return "input1 must be an INT or FLOAT type"
        if input_types["input2"] not in ("INT", "FLOAT"):
            return "input2 must be an INT or FLOAT type"
        return True
```


# Annotated Examples
Source: https://docs.comfy.org/custom-nodes/backend/snippets



A growing collection of fragments of example code...

## Images and Masks

### Load an image

Load an image into a batch of size 1 (based on `LoadImage` source code in `nodes.py`)

```python  theme={null}
i = Image.open(image_path)
i = ImageOps.exif_transpose(i)
if i.mode == 'I':
    i = i.point(lambda i: i * (1 / 255))
image = i.convert("RGB")
image = np.array(image).astype(np.float32) / 255.0
image = torch.from_numpy(image)[None,]
```

### Save an image batch

Save a batch of images (based on `SaveImage` source code in `nodes.py`)

```python  theme={null}
for (batch_number, image) in enumerate(images):
    i = 255. * image.cpu().numpy()
    img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))
    filepath = # some path that takes the batch number into account
    img.save(filepath)
```

### Invert a mask

Inverting a mask is a straightforward process. Since masks are normalised to the range \[0,1]:

```python  theme={null}
mask = 1.0 - mask
```

### Convert a mask to Image shape

```Python  theme={null}
# We want [B,H,W,C] with C = 1
if len(mask.shape)==2: # we have [H,W], so insert B and C as dimension 1
    mask = mask[None,:,:,None]
elif len(mask.shape)==3 and mask.shape[2]==1: # we have [H,W,C]
    mask = mask[None,:,:,:]
elif len(mask.shape)==3:                      # we have [B,H,W]
    mask = mask[:,:,:,None]
```

### Using Masks as Transparency Layers

When used for tasks like inpainting or segmentation, the MASK's values will eventually be rounded to the nearest integer so that they are binary  0 indicating regions to be ignored and 1 indicating regions to be targeted. However, this doesn't happen until the MASK is passed to those nodes. This flexibility allows you to use MASKs as you would in digital photography contexts as a transparency layer:

```python  theme={null}
# Invert mask back to original transparency layer
mask = 1.0 - mask

# Unsqueeze the `C` (channels) dimension
mask = mask.unsqueeze(-1)

# Concatenate ("cat") along the `C` dimension
rgba_image = torch.cat((rgb_image, mask), dim=-1)
```

## Noise

### Creating noise variations

Here's an example of creating a noise object which mixes the noise from two sources. This could be used to create slight noise variations by varying `weight2`.

```python  theme={null}
class Noise_MixedNoise:
    def __init__(self, nosie1, noise2, weight2):
        self.noise1  = noise1
        self.noise2  = noise2
        self.weight2 = weight2

    @property
    def seed(self): return self.noise1.seed

    def generate_noise(self, input_latent:torch.Tensor) -> torch.Tensor:
        noise1 = self.noise1.generate_noise(input_latent)
        noise2 = self.noise2.generate_noise(input_latent)
        return noise1 * (1.0-self.weight2) + noise2 * (self.weight2)
```


# Working with torch.Tensor
Source: https://docs.comfy.org/custom-nodes/backend/tensors



## pytorch, tensors, and torch.Tensor

All the core number crunching in Comfy is done by [pytorch](https://pytorch.org/). If your custom nodes are going
to get into the guts of stable diffusion you will need to become familiar with this library, which is way beyond
the scope of this introduction.

However, many custom nodes will need to manipulate images, latents and masks, each of which are represented internally
as `torch.Tensor`, so you'll want to bookmark the
[documentation for torch.Tensor](https://pytorch.org/docs/stable/tensors.html).

### What is a Tensor?

`torch.Tensor` represents a tensor, which is the mathematical generalization of a vector or matrix to any number of dimensions.
A tensor's *rank* is the number of dimensions it has (so a vector has *rank* 1, a matrix *rank* 2); its *shape* describes the
size of each dimension.

So an RGB image (of height H and width W) might be thought of as three arrays (one for each color channel), each measuring H x W,
which could be represented as a tensor with *shape* `[H,W,3]`. In Comfy images almost always come in a batch (even if the batch
only contains a single image). `torch` always places the batch dimension first, so Comfy images have *shape* `[B,H,W,3]`, generally
written as `[B,H,W,C]` where C stands for Channels.

### squeeze, unsqueeze, and reshape

If a tensor has a dimension of size 1 (known as a collapsed dimension), it is equivalent to the same tensor with that dimension removed
(a batch with 1 image is just an image). Removing such a collapsed dimension is referred to as squeezing, and
inserting one is known as unsqueezing.

<Warning>Some torch code, and some custom node authors, will return a squeezed tensor when a dimension is collapsed - such
as when a batch has only one member. This is a common cause of bugs!</Warning>

To represent the same data in a different shape is referred to as reshaping. This often requires you to know
the underlying data structure, so handle with care!

### Important notation

`torch.Tensor` supports most Python slice notation, iteration, and other common list-like operations. A tensor
also has a `.shape` attribute which returns its size as a `torch.Size` (which is a subclass of `tuple` and can
be treated as such).

There are some other important bits of notation you'll often see (several of these are less common
standard Python notation, seen much more frequently when dealing with tensors)

* `torch.Tensor` supports the use of `None` in slice notation
  to indicate the insertion of a dimension of size 1.

* `:` is frequently used when slicing a tensor; this simply means 'keep the whole dimension'.
  It's like using `a[start:end]` in Python, but omitting the start point and end point.

* `...` represents 'the whole of an unspecified number of dimensions'. So `a[0, ...]` would extract the first
  item from a batch regardless of the number of dimensions.

* in methods which require a shape to be passed, it is often passed as a `tuple` of the dimensions, in
  which a single dimension can be given the size `-1`, indicating that the size of this dimension should
  be calculated based on the total size of the data.

```python  theme={null}
>>> a = torch.Tensor((1,2))
>>> a.shape
torch.Size([2])
>>> a[:,None].shape 
torch.Size([2, 1])
>>> a.reshape((1,-1)).shape
torch.Size([1, 2])
```

### Elementwise operations

Many binary on `torch.Tensor` (including '+', '-', '\*', '/' and '==') are applied elementwise (independently applied to each element).
The operands must be *either* two tensors of the same shape, *or* a tensor and a scalar. So:

```python  theme={null}
>>> import torch
>>> a = torch.Tensor((1,2))
>>> b = torch.Tensor((3,2))
>>> a*b
tensor([3., 4.])
>>> a/b
tensor([0.3333, 1.0000])
>>> a==b
tensor([False,  True])
>>> a==1
tensor([ True, False])
>>> c = torch.Tensor((3,2,1)) 
>>> a==c
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0
```

### Tensor truthiness

<Warning>The 'truthiness' value of a Tensor is not the same as that of Python lists.</Warning>

You may be familiar with the truthy value of a Python list as `True` for any non-empty list, and `False` for `None` or `[]`.
By contrast A `torch.Tensor` (with more than one elements) does not have a defined truthy value. Instead you need to use
`.all()` or `.any()` to combine the elementwise truthiness:

```python  theme={null}
>>> a = torch.Tensor((1,2))
>>> print("yes" if a else "no")
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
RuntimeError: Boolean value of Tensor with more than one value is ambiguous
>>> a.all()
tensor(False)
>>> a.any()
tensor(True)
```

This also means that you need to use `if a is not None:` not `if a:` to determine if a tensor variable has been set.


# About Panel Badges
Source: https://docs.comfy.org/custom-nodes/js/javascript_about_panel_badges



The About Panel Badges API allows extensions to add custom badges to the ComfyUI about page. These badges can display information about your extension and contain links to documentation, source code, or other resources.

## Basic Usage

```javascript  theme={null}
app.registerExtension({
  name: "MyExtension",
  aboutPageBadges: [
    {
      label: "Documentation",
      url: "https://example.com/docs",
      icon: "pi pi-file"
    },
    {
      label: "GitHub",
      url: "https://github.com/username/repo",
      icon: "pi pi-github"
    }
  ]
});
```

## Badge Configuration

Each badge requires all of these properties:

```javascript  theme={null}
{
  label: string,           // Text to display on the badge
  url: string,             // URL to open when badge is clicked
  icon: string             // Icon class (e.g., PrimeVue icon)
}
```

## Icon Options

Badge icons use PrimeVue's icon set. Here are some commonly used icons:

* Documentation: `pi pi-file` or `pi pi-book`
* GitHub: `pi pi-github`
* External link: `pi pi-external-link`
* Information: `pi pi-info-circle`
* Download: `pi pi-download`
* Website: `pi pi-globe`
* Discord: `pi pi-discord`

For a complete list of available icons, refer to the [PrimeVue Icons documentation](https://primevue.org/icons/).

## Example

```javascript  theme={null}
app.registerExtension({
  name: "BadgeExample",
  aboutPageBadges: [
    {
      label: "Website",
      url: "https://example.com",
      icon: "pi pi-home"
    },
    {
      label: "Donate",
      url: "https://example.com/donate",
      icon: "pi pi-heart"
    },
    {
      label: "Documentation",
      url: "https://example.com/docs",
      icon: "pi pi-book"
    }
  ]
});
```

Badges appear in the About panel of the Settings dialog, which can be accessed via the gear icon in the top-right corner of the ComfyUI interface.


# Dialog API
Source: https://docs.comfy.org/custom-nodes/js/javascript_dialog



The Dialog API provides standardized dialogs that work consistently across desktop and web environments. Extension authors will find the prompt and confirm methods most useful.

## Basic Usage

### Prompt Dialog

```javascript  theme={null}
// Show a prompt dialog
app.extensionManager.dialog.prompt({
  title: "User Input",
  message: "Please enter your name:",
  defaultValue: "User"
}).then(result => {
  if (result !== null) {
    console.log(`Input: ${result}`);
  }
});
```

### Confirm Dialog

```javascript  theme={null}
// Show a confirmation dialog
app.extensionManager.dialog.confirm({
  title: "Confirm Action",
  message: "Are you sure you want to continue?",
  type: "default"
}).then(result => {
  console.log(result ? "User confirmed" : "User cancelled");
});
```

## API Reference

### Prompt

```javascript  theme={null}
app.extensionManager.dialog.prompt({
  title: string,             // Dialog title
  message: string,           // Message/question to display
  defaultValue?: string      // Initial value in the input field (optional)
}).then((result: string | null) => {
  // result is the entered text, or null if cancelled
});
```

### Confirm

```javascript  theme={null}
app.extensionManager.dialog.confirm({
  title: string,             // Dialog title
  message: string,           // Message to display
  type?: "default" | "overwrite" | "delete" | "dirtyClose" | "reinstall", // Dialog type (optional)
  itemList?: string[],       // List of items to display (optional)
  hint?: string              // Hint text to display (optional)
}).then((result: boolean | null) => {
  // result is true if confirmed, false if denied, null if cancelled
});
```

For other specialized dialogs available in ComfyUI, extension authors can refer to the `dialogService.ts` file in the source code.


# Comfy Hooks
Source: https://docs.comfy.org/custom-nodes/js/javascript_hooks



## Extension hooks

At various points during Comfy execution, the application calls
`#invokeExtensionsAsync` or `#invokeExtensions` with the name of a hook.
These invoke, on all registered extensions, the appropriately named method (if present), such as `setup`
in the example above.

Comfy provides a variety of hooks for custom extension code to use to modify client behavior.

<Tip>These hooks are called during creation and modification of the Comfy client side elements.
<br />Events during workflow execution are handled by
the `apiUpdateHandlers`</Tip> {/* TODO link when written */}

A few of the most significant hooks are described below.
As Comfy is being actively developed, from time to time additional hooks are added, so
search for `#invokeExtensions` in `app.js` to find all available hooks.

See also the [sequence](#call-sequences) in which hooks are invoked.

### Commonly used hooks

Start with `beforeRegisterNodeDef`, which is used by the majority of extensions, and is often the only one needed.

#### beforeRegisterNodeDef()

Called once for each node type (the list of nodes available in the `AddNode` menu), and is used to
modify the behaviour of the node.

```Javascript  theme={null}
async beforeRegisterNodeDef(nodeType, nodeData, app) 
```

The object passed in the `nodeType` parameter essentially serves as a template
for all nodes that will be created of this type, so modifications made to `nodeType.prototype` will apply
to all nodes of this type. `nodeData` is an encapsulation of aspects of the node defined in the Python code,
such as its category, inputs, and outputs. `app` is a reference to the main Comfy app object (which you
have already imported anyway!)

<Tip>This method is called, on each registered extension, for *every* node type, not just the ones added by that extension.</Tip>

The usual idiom is to check `nodeType.ComfyClass`, which holds the Python class name corresponding to this node,
to see if you need to modify the node. Often this means modifying the custom nodes that you have added,
although you may sometimes need to modify the behavior of other nodes (or other custom nodes
might modify yours!), in which case care should be taken to ensure interoperability.

<Tip>Since other extensions may also modify nodes, aim to write code that makes as few assumptions as possible.
And play nicely - isolate your changes wherever possible.</Tip>

A very common idiom in `beforeRegisterNodeDef` is to 'hijack' an existing method:

```Javascript  theme={null}
async beforeRegisterNodeDef(nodeType, nodeData, app) {
	if (nodeType.comfyClass=="MyNodeClass") { 
		const onConnectionsChange = nodeType.prototype.onConnectionsChange;
		nodeType.prototype.onConnectionsChange = function (side,slot,connect,link_info,output) {     
			const r = onConnectionsChange?.apply(this, arguments);   
			console.log("Someone changed my connection!");
			return r;
		}
	}
}
```

In this idiom the existing prototype method is stored, and then replaced. The replacement calls the
original method (the `?.apply` ensures that if there wasn't one this is still safe) and then
performs additional operations. Depending on your code logic, you may need to place the `apply` elsewhere in your replacement code,
or even make calling it conditional.

When hijacking a method in this way, you will want to look at the core comfy code (breakpoints are your friend) to check
and conform with the method signature.

#### nodeCreated()

```Javascript  theme={null}
async nodeCreated(node)
```

Called when a specific instance of a node gets created
(right at the end of the `ComfyNode()` function on `nodeType` which serves as a constructor).
In this hook you can make modifications to individual instances of your node.

<Tip>Changes that apply to all instances are better added to the prototype in `beforeRegisterNodeDef` as described above.</Tip>

#### init()

```Javascript  theme={null}
async init()
```

Called when the Comfy webpage is loaded (or reloaded). The call is made after the graph object has been created, but before any
nodes are registered or created. It can be used to modify core Comfy behavior by hijacking methods of the app, or of the
graph (a `LiteGraph` object). This is discussed further in [Comfy Objects](./javascript_objects_and_hijacking).

<Warning>With great power comes great responsibility. Hijacking core behavior makes it more likely your nodes
will be incompatible with other custom nodes, or future Comfy updates</Warning>

#### setup()

```Javascript  theme={null}
async setup()
```

Called at the end of the startup process. A good place to add event listeners (either for Comfy events, or DOM events),
or adding to the global menus, both of which are discussed elsewhere. {/* TODO link when written */}

<Tip>To do something when a workflow has loaded, use `afterConfigureGraph`, not `setup`</Tip>

### Call sequences

These sequences were obtained by insert logging code into the Comfy `app.js` file. You may find similar code helpful
in understanding the execution flow.

```Javascript  theme={null}
/* approx line 220 at time of writing: */
	#invokeExtensions(method, ...args) {
		console.log(`invokeExtensions      ${method}`) // this line added
		// ...
	}
/* approx line 250 at time of writing: */
	async #invokeExtensionsAsync(method, ...args) {
		console.log(`invokeExtensionsAsync ${method}`) // this line added
		// ...
	}
```

#### Web page load

```
invokeExtensionsAsync init
invokeExtensionsAsync addCustomNodeDefs
invokeExtensionsAsync getCustomWidgets
invokeExtensionsAsync beforeRegisterNodeDef    [repeated multiple times]
invokeExtensionsAsync registerCustomNodes
invokeExtensionsAsync beforeConfigureGraph
invokeExtensionsAsync nodeCreated
invokeExtensions      loadedGraphNode
invokeExtensionsAsync afterConfigureGraph
invokeExtensionsAsync setup
```

#### Loading workflow

```
invokeExtensionsAsync beforeConfigureGraph
invokeExtensionsAsync beforeRegisterNodeDef   [zero, one, or multiple times]
invokeExtensionsAsync nodeCreated             [repeated multiple times]
invokeExtensions      loadedGraphNode         [repeated multiple times]
invokeExtensionsAsync afterConfigureGraph
```

#### Adding new node

```
invokeExtensionsAsync nodeCreated
```


# Comfy Objects
Source: https://docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking



## LiteGraph

The Comfy UI is built on top of [LiteGraph](https://github.com/jagenjo/litegraph.js).
Much of the Comfy functionality is provided by LiteGraph, so if developing more complex
nodes you will probably find it helpful to clone that repository and browse the documentation,
which can be found at `doc/index.html`.

## ComfyApp

The `app` object (always accessible by `import { app } from "../../scripts/app.js";`) represents the Comfy application running in the browser,
and contains a number of useful properties and functions, some of which are listed below.

<Warning>Hijacking functions on `app` is not recommended, as Comfy is under constant development, and core behavior may change.</Warning>

### Properties

Important properties of `app` include (this is not an exhaustive list):

| property        | contents                                                                                                                                                        |
| --------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `canvas`        | An LGraphCanvas object, representing the current user interface. It contains some potentially interesting properties, such as `node_over` and `selected_nodes`. |
| `canvasEl`      | The DOM `<canvas>` element                                                                                                                                      |
| `graph`         | A reference to the LGraph object describing the current graph                                                                                                   |
| `runningNodeId` | During execution, the node currently being executed                                                                                                             |
| `ui`            | Provides access to some UI elements, such as the queue, menu, and dialogs                                                                                       |

`canvas` (for graphical elements) and `graph` (for logical connections) are probably the ones you are most likely to want to access.

### Functions

Again, there are many. A few significant ones are:

| function          | notes                                                                 |
| ----------------- | --------------------------------------------------------------------- |
| graphToPrompt     | Convert the graph into a prompt that can be sent to the Python server |
| loadGraphData     | Load a graph                                                          |
| queuePrompt       | Submit a prompt to the queue                                          |
| registerExtension | You've seen this one - used to add an extension                       |

## LGraph

The `LGraph` object is part of the LiteGraph framework, and represents the current logical state of the graph (nodes and links).
If you want to manipulate the graph, the LiteGraph documentation (at `doc/index.html` if you clone `https://github.com/jagenjo/litegraph.js`)
describes the functions you will need.

You can use `graph` to obtain details of nodes and links, for example:

```Javascript  theme={null}
const ComfyNode_object_for_my_node = app.graph._nodes_by_id(my_node_id) 
ComfyNode_object_for_my_node.inputs.forEach(input => {
    const link_id = input.link;
    if (link_id) {
        const LLink_object = app.graph.links[link_id]
        const id_of_upstream_node = LLink_object.origin_id
        // etc
    }
});
```

## LLink

The `LLink` object, accessible through `graph.links`, represents a single link in the graph, from node `link.origin_id` output slot `link.origin_slot`
to node `link.target_id` slot `link.target_slot`. It also has a string representing the data type, in `link.type`, and `link.id`.

`LLink`s are created in the `connect` method of a `LGraphNode` (of which `ComfyNode` is a subclass).

<Tip>Avoid creating your own LLink objects - use the LiteGraph functions instead.</Tip>

## ComfyNode

`ComfyNode` is a subclass of `LGraphNode`, and the LiteGraph documentation is therefore helpful for more generic
operations. However, Comfy has significantly extended the LiteGraph core behavior, and also does not make
use of all LiteGraph functionality.

<Tip>The description that follows applies to a normal node.
Group nodes, primitive nodes, notes, and redirect nodes have different properties.</Tip>

A `ComfyNode` object represents a node in the current workflow. It has a number of important properties
that you may wish to make use of, a very large number of functions that you may wish to use, or hijack to
modify behavior.

To get a more complete sense of the node object, you may find it helpful to insert the following
code into your extension and place a breakpoint on the `console.log` command. When you then create a new node
you can use your favorite debugger to interrogate the node.

```Javascript  theme={null}
async nodeCreated(node) {
    console.log("nodeCreated")
}
```

### Properties

| property          | contents                                                                                                                            |
| ----------------- | ----------------------------------------------------------------------------------------------------------------------------------- |
| `bgcolor`         | The background color of the node, or undefined for the default                                                                      |
| `comfyClass`      | The Python class representing the node                                                                                              |
| `flags`           | A dictionary that may contain flags related to the state of the node. In particular, `flags.collapsed` is true for collapsed nodes. |
| `graph`           | A reference to the LGraph object                                                                                                    |
| `id`              | A unique id                                                                                                                         |
| `input_type`      | A list of the input types (eg "STRING", "MODEL", "CLIP" etc). Generally matches the Python INPUT\_TYPES                             |
| `inputs`          | A list of inputs (discussed below)                                                                                                  |
| `mode`            | Normally 0, set to 2 if the node is muted and 4 if the node is bypassed. Values of 1 and 3 are not used by Comfy                    |
| `order`           | The node's position in the execution order. Set by `LGraph.computeExecutionOrder()` when the prompt is submitted                    |
| `pos`             | The \[x,y] position of the node on the canvas                                                                                       |
| `properties`      | A dictionary containing `"Node name for S&R"`, used by LiteGraph                                                                    |
| `properties_info` | The type and default value of entries in `properties`                                                                               |
| `size`            | The width and height of the node on the canvas                                                                                      |
| `title`           | Display Title                                                                                                                       |
| `type`            | The unique name (from Python) of the node class                                                                                     |
| `widgets`         | A list of widgets (discussed below)                                                                                                 |
| `widgets_values`  | A list of the current values of widgets                                                                                             |

### Functions

There are a very large number of functions (85, last time I counted). A selection are listed below.
Most of these functions are unmodified from the LiteGraph core code.

#### Inputs, Outputs, Widgets

| function               | notes                                                                                              |
| ---------------------- | -------------------------------------------------------------------------------------------------- |
| Inputs / Outputs       | Most have output methods with the equivalent names: s/In/Out/                                      |
| `addInput`             | Create a new input, defined by name and type                                                       |
| `addInputs`            | Array version of `addInput`                                                                        |
| `findInputSlot`        | Find the slot index from the input name                                                            |
| `findInputSlotByType`  | Find an input matching the type. Options to prefer, or only use, free slots                        |
| `removeInput`          | By slot index                                                                                      |
| `getInputNode`         | Get the node connected to this input. The output equivalent is `getOutputNodes` and returns a list |
| `getInputLink`         | Get the LLink connected to this input. No output equivalent                                        |
| Widgets                |                                                                                                    |
| `addWidget`            | Add a standard Comfy widget                                                                        |
| `addCustomWidget`      | Add a custom widget (defined in the `getComfyWidgets` hook)                                        |
| `addDOMWidget`         | Add a widget defined by a DOM element                                                              |
| `convertWidgetToInput` | Convert a widget to an input if allowed by `isConvertableWidget` (in `widgetInputs.js`)            |

#### Connections

| function              | notes                                                                                             |
| --------------------- | ------------------------------------------------------------------------------------------------- |
| `connect`             | Connect this node's output to another node's input                                                |
| `connectByType`       | Connect output to another node by specifying the type - connects to first available matching slot |
| `connectByTypeOutput` | Connect input to another node output by type                                                      |
| `disconnectInput`     | Remove any link into the input (specified by name or index)                                       |
| `disconnectOutput`    | Disconnect an output from a specified node's input                                                |
| `onConnectionChange`  | Called on each node. `side==1` if it's an input on this node                                      |
| `onConnectInput`      | Called *before* a connection is made. If this returns `false`, the connection is refused          |

#### Display

| function           | notes                                                                                                  |
| ------------------ | ------------------------------------------------------------------------------------------------------ |
| `setDirtyCanvas`   | Specify that the foreground (nodes) and/or background (links and images) need to be redrawn            |
| `onDrawBackground` | Called with a `CanvasRenderingContext2D` object to draw the background. Used by Comfy to render images |
| `onDrawForeground` | Called with a `CanvasRenderingContext2D` object to draw the node.                                      |
| `getTitle`         | The title to be displayed.                                                                             |
| `collapse`         | Toggles the collapsed state of the node.                                                               |

<Warning>`collapse` is badly named; it *toggles* the collapsed state.
It takes a boolean parameter, which can be used to override
`node.collapsable === false`.</Warning>

#### Other

| function     | notes                                                              |
| ------------ | ------------------------------------------------------------------ |
| `changeMode` | Use to set the node to bypassed (`mode == 4`) or not (`mode == 0`) |

## Inputs and Widgets

Inputs and Widgets represent the two ways that data can be fed into a node. In general a widget can be
converted to an input, but not all inputs can be converted to a widget (as many datatypes can't be
entered through a UI element).

`node.inputs` is a list of the current inputs (colored dots on the left hand side of the node),
specifying their `.name`, `.type`, and `.link` (a reference to the connected `LLink` in `app.graph.links`).

If an input is a widget which has been converted, it also holds a reference to the, now inactive, widget in `.widget`.

`node.widgets` is a list of all widgets, whether or not they have been converted to an input. A widget has:

| property/function | notes                                                                     |
| ----------------- | ------------------------------------------------------------------------- |
| `callback`        | A function called when the widget value is changed                        |
| `last_y`          | The vertical position of the widget in the node                           |
| `name`            | The (unique within a node) widget name                                    |
| `options`         | As specified in the Python code (such as default, min, and max)           |
| `type`            | The name of the widget type (see below) in lowercase                      |
| `value`           | The current widget value. This is a property with `get` and `set` methods |

### Widget Types

`app.widgets` is a dictionary of currently registered widget types, keyed in the UPPER CASE version of the name of the type.
Build in Comfy widgets types include the self explanatory `BOOLEAN`, `INT`, and `FLOAT`,
as well as `STRING` (which comes in two flavours, single line and multiline),
`COMBO` for dropdown selection from a list, and `IMAGEUPLOAD`, used in Load Image nodes.

Custom widget types can be added by providing a `getCustomWidgets` method in your extension.

### Linked widgets

Widgets can also be linked - the built in behavior of `seed` and `control_after_generate`, for example.
A linked widget has `.type = 'base_widget_type:base_widget_name'`; so `control_after_generate` may have
type `int:seed`.

## Prompt

When you press the `Queue Prompt` button in Comfy, the `app.graphToPrompt()` method is called to convert the
current graph into a prompt that can be sent to the server.

`app.graphToPrompt` returns an object (referred to herein as `prompt`) with two properties, `output` and `workflow`.

### output

`prompt.output` maps from the `node_id` of each node in the graph to an object with two properties.

* `prompt.output[node_id].class_type`, the unique name of the custom node class, as defined in the Python code
* `prompt.output[node_id].inputs`, which contains the value of each input (or widget) as a map from the input name to:
  * the selected value, if it is a widget, or
  * an array containing (`upstream_node_id`, `upstream_node_output_slot`) if there is a link connected to the input, or
  * undefined, if it is a widget that has been converted to an input and is not connected
  * other unconnected inputs are not included in `.inputs`

<Tip>Note that the `upstream_node_id` in the array describing a connected input is represented as a string, not an integer.</Tip>

### workflow

`prompt.workflow` contains the following properties:

* `config` - a dictionary of additional configuration options (empty by default)
* `extra` - a dictionary containing extra information about the workflow. By default it contains:
  * `extra.ds` - describes the current view of the graph (`scale` and `offset`)
* `groups` - all groups in the workflow
* `last_link_id` - the id of the last link added
* `last_node_id` - the id of the last node added
* `links` - a list of all links in the graph. Each entry is an array of five integers and one string:
  * (`link_id`, `upstream_node_id`, `upstream_node_output_slot`, `downstream_node_id`, `downstream_node_input_slot`, `data type`)
* `nodes` - a list of all nodes in the graph. Each entry is a map of a subset of the properties of the node as described [above](#comfynode)
  * The following properties are included: `flags`, `id`, `inputs`, `mode`, `order`, `pos`, `properties`, `size`, `type`, `widgets_values`
  * In addition, unless a node has no outputs, there is an `outputs` property, which is a list of the outputs of the node, each of which contains:
    * `name` - the name of the output
    * `type` - the data type of the output
    * `links` - a list of the `link_id` of all links from this output (if there are no connections, may be an empty list, or null),
    * `shape` - the shape used to draw the output (default 3 for a dot)
    * `slot_index` - the slot number of the output
* `version` - the LiteGraph version number (at time of writing, `0.4`)

<Tip>`nodes.output` is absent for nodes with no outputs, not an empty list.</Tip>


# Javascript Extensions
Source: https://docs.comfy.org/custom-nodes/js/javascript_overview



## Extending the Comfy Client

Comfy can be modified through an extensions mechanism. To add an extension you need to:

* Export `WEB_DIRECTORY` from your Python module,
* Place one or more `.js` files into that directory,
* Use `app.registerExtension` to register your extension.

These three steps are below. Once you know how to add an extension, look
through the [hooks](/custom-nodes/js/javascript_hooks) available to get your code called,
a description of various [Comfy objects](/custom-nodes/js/javascript_objects_and_hijacking) you might need,
or jump straight to some [example code snippets](/custom-nodes/js/javascript_examples).

### Exporting `WEB_DIRECTORY`

The Comfy web client can be extended by creating a subdirectory in your custom node directory, conventionally called `js`, and
exporting `WEB_DIRECTORY` - so your `__init_.py` will include something like:

```python  theme={null}
WEB_DIRECTORY = "./js"
__all__ = ["NODE_CLASS_MAPPINGS", "NODE_DISPLAY_NAME_MAPPINGS", "WEB_DIRECTORY"]
```

### Including `.js` files

<Tip>All Javascript `.js` files will be loaded by the browser as the Comfy webpage loads. You don't need to specify the file
your extension is in.</Tip>

*Only* `.js` files will be added to the webpage. Other resources (such as `.css` files) can be accessed
at `extensions/custom_node_subfolder/the_file.css` and added programmatically.

<Warning>That path does *not* include the name of the subfolder. The value of `WEB_DIRECTORY` is inserted by the server.</Warning>

### Registering an extension

The basic structure of an extension follows is to import the main Comfy `app` object, and call `app.registerExtension`,
passing a dictionary that contains a unique `name`,
and one or more functions to be called by hooks in the Comfy code.

A complete, trivial, and annoying, extension might look like this:

```Javascript  theme={null}
import { app } from "../../scripts/app.js";
app.registerExtension({ 
	name: "a.unique.name.for.a.useless.extension",
	async setup() { 
		alert("Setup complete!")
	},
})
```


# Settings
Source: https://docs.comfy.org/custom-nodes/js/javascript_settings



You can provide a settings object to ComfyUI that will show up when the user
opens the ComfyUI settings panel.

## Basic operation

### Add a setting

```javascript  theme={null}
import { app } from "../../scripts/app.js";

app.registerExtension({
    name: "My Extension",
    settings: [
        {
            id: "example.boolean",
            name: "Example boolean setting",
            type: "boolean",
            defaultValue: false,
        },
    ],
});
```

The `id` must be unique across all extensions and will be used to fetch values.

If you do not [provide a category](#categories), then the `id` will be split by
`.` to determine where it appears in the settings panel.

* If your `id` doesn't contain any `.` then it will appear in the "Other"
  category and your `id` will be used as the section heading.
* If your `id` contains at least one `.` then the leftmost part will be used
  as the setting category and the second part will be used as the section
  heading. Any further parts are ignored.

### Read a setting

```javascript  theme={null}
import { app } from "../../scripts/app.js";

if (app.extensionManager.setting.get('example.boolean')) {
    console.log("Setting is enabled.");
} else {
    console.log("Setting is disabled.");
}
```

### React to changes

The `onChange()` event handler will be called as soon as the user changes the
setting in the settings panel.

This will also be called when the extension is registered, on every page load.

```javascript  theme={null}
{
    id: "example.boolean",
    name: "Example boolean setting",
    type: "boolean",
    defaultValue: false,
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### Write a setting

```javascript  theme={null}
import { app } from "../../scripts/app.js";

try {
    await app.extensionManager.setting.set("example.boolean", true);
} catch (error) {
    console.error(`Error changing setting: ${error}`);
}
```

### Extra configuration

The setting types are based on [PrimeVue](https://primevue.org/) components.
Props described in the PrimeVue documentation can be defined for ComfyUI
settings by adding them in an `attrs` field.

For instance, this adds increment/decrement buttons to a number input:

```javascript  theme={null}
{
    id: "example.number",
    name: "Example number setting",
    type: "number",
    defaultValue: 0,
    attrs: {
        showButtons: true,
    },
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

## Types

### Boolean

This shows an on/off toggle.

Based on the [ToggleSwitch PrimeVue
component](https://primevue.org/toggleswitch/).

```javascript  theme={null}
{
    id: "example.boolean",
    name: "Example boolean setting",
    type: "boolean",
    defaultValue: false,
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### Text

This is freeform text.

Based on the [InputText PrimeVue component](https://primevue.org/inputtext/).

```javascript  theme={null}
{
    id: "example.text",
    name: "Example text setting",
    type: "text",
    defaultValue: "Foo",
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### Number

This for entering numbers.

To allow decimal places, set the `maxFractionDigits` attribute to a number greater than zero.

Based on the [InputNumber PrimeVue
component](https://primevue.org/inputnumber/).

```javascript  theme={null}
{
    id: "example.number",
    name: "Example number setting",
    type: "number",
    defaultValue: 42,
    attrs: {
        showButtons: true,
        maxFractionDigits: 1,
    },
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### Slider

This lets the user enter a number directly or via a slider.

Based on the [Slider PrimeVue component](https://primevue.org/slider/). Ranges
are not supported.

```javascript  theme={null}
{
    id: "example.slider",
    name: "Example slider setting",
    type: "slider",
    attrs: {
        min: -10,
        max: 10,
        step: 0.5,
    },
    defaultValue: 0,
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### Combo

This lets the user pick from a drop-down list of values.

You can provide options either as a plain string or as an object with `text`
and `value` fields. If you only provide a plain string, then it will be used
for both.

You can let the user enter freeform text by supplying the `editable: true`
attribute, or search by supplying the `filter: true` attribute.

Based on the [Select PrimeVue component](https://primevue.org/select/). Groups
are not supported.

```javascript  theme={null}
{
    id: "example.combo",
    name: "Example combo setting",
    type: "combo",
    defaultValue: "first",
    options: [
        { text: "My first option", value: "first" },
        "My second option",
    ],
    attrs: {
        editable: true,
        filter: true,
    },
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### Color

This lets the user select a color from a color picker or type in a hex
reference.

Note that the format requires six full hex digits - three digit shorthand does
not work.

Based on the [ColorPicker PrimeVue
component](https://primevue.org/colorpicker/).

```javascript  theme={null}
{
    id: "example.color",
    name: "Example color setting",
    type: "color",
    defaultValue: "ff0000",
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### Image

This lets the user upload an image.

The setting will be saved as a [data
URL](https://developer.mozilla.org/en-US/docs/Web/URI/Schemes/data).

Based on the [FileUpload PrimeVue
component](https://primevue.org/fileupload/).

```javascript  theme={null}
{
    id: "example.image",
    name: "Example image setting",
    type: "image",
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### Hidden

Hidden settings aren't displayed in the settings panel, but you can read and
write to them from your code.

```javascript  theme={null}
{
    id: "example.hidden",
    name: "Example hidden setting",
    type: "hidden",
}
```

## Other

### Categories

You can specify the categorisation of your setting separately to the `id`.
This means you can change the categorisation and naming without changing the
`id` and losing the values that have already been set by users.

```javascript  theme={null}
{
    id: "example.boolean",
    name: "Example boolean setting",
    type: "boolean",
    defaultValue: false,
    category: ["Category name", "Section heading", "Setting label"],
}
```

### Tooltips

You can add extra contextual help with the `tooltip` field. This adds a small 
icon after the field name that will show the help text when the user hovers
over it.

```javascript  theme={null}
{
    id: "example.boolean",
    name: "Example boolean setting",
    type: "boolean",
    defaultValue: false,
    tooltip: "This is some helpful information",
}
```


# Toast API
Source: https://docs.comfy.org/custom-nodes/js/javascript_toast



The Toast API provides a way to display non-blocking notification messages to users. These are useful for providing feedback without interrupting workflow.

## Basic Usage

### Simple Toast

```javascript  theme={null}
// Display a simple info toast
app.extensionManager.toast.add({
  severity: "info",
  summary: "Information",
  detail: "Operation completed successfully",
  life: 3000
});
```

### Toast Types

```javascript  theme={null}
// Success toast
app.extensionManager.toast.add({
  severity: "success",
  summary: "Success",
  detail: "Data saved successfully",
  life: 3000
});

// Warning toast
app.extensionManager.toast.add({
  severity: "warn",
  summary: "Warning",
  detail: "This action may cause problems",
  life: 5000
});

// Error toast
app.extensionManager.toast.add({
  severity: "error",
  summary: "Error",
  detail: "Failed to process request",
  life: 5000
});
```

### Alert Helper

```javascript  theme={null}
// Shorthand for creating an alert toast
app.extensionManager.toast.addAlert("This is an important message");
```

## API Reference

### Toast Message

```javascript  theme={null}
app.extensionManager.toast.add({
  severity?: "success" | "info" | "warn" | "error" | "secondary" | "contrast", // Message severity level (default: "info")
  summary?: string,         // Short title for the toast
  detail?: any,             // Detailed message content
  closable?: boolean,       // Whether user can close the toast (default: true)
  life?: number,            // Duration in milliseconds before auto-closing
  group?: string,           // Group identifier for managing related toasts
  styleClass?: any,         // Style class of the message
  contentStyleClass?: any   // Style class of the content
});
```

### Alert Helper

```javascript  theme={null}
app.extensionManager.toast.addAlert(message: string);
```

### Additional Methods

```javascript  theme={null}
// Remove a specific toast
app.extensionManager.toast.remove(toastMessage);

// Remove all toasts
app.extensionManager.toast.removeAll();
```


# Overview
Source: https://docs.comfy.org/custom-nodes/overview



Custom nodes allow you to implement new features and share them with the wider community.

A custom node is like any Comfy node: it takes input, does something to it, and produces an output.
While some custom nodes perform highly complex tasks, many just do one thing. Here's an example of a
simple node that takes an image and inverts it.

<img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/invert_image_node.png?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=8088073e29e8af1bc700937ecb3b77e9" alt="Unique Images Node" data-og-width="564" width="564" data-og-height="279" height="279" data-path="images/invert_image_node.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/invert_image_node.png?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=6fa9f912f38ec608ee43f4e456cedf42 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/invert_image_node.png?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=aeae6dd8bc6393935ebabe3de4b8ed85 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/invert_image_node.png?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=3dbe92afa58705aca5f218df9f5c5a52 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/invert_image_node.png?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=dfda0a7b67616219ac9b7ec9742cb0d4 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/invert_image_node.png?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=a3b84be0acf1b9248b9ac0086a49bd53 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/invert_image_node.png?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=2113625a7743f680050980330f4f928d 2500w" />

Custom node examples

* [cookiecutter-comfy-extension](https://github.com/Comfy-Org/cookiecutter-comfy-extension)
* [ComfyUI-React-Extension-Template](https://github.com/Comfy-Org/ComfyUI-React-Extension-Template)
* [ComfyUI\_frontend\_vue\_basic](https://github.com/jtydhr88/ComfyUI_frontend_vue_basic)

## Client-Server Model

Comfy runs in a client-server model. The server, written in Python, handles all the real work: data-processing, models, image diffusion etc. The client, written in Javascript, handles the user interface.

Comfy can also be used in API mode, in which a workflow is sent to the server by a non-Comfy client (such as another UI, or a command line script).

Custom nodes can be placed into one of four categories:

### Server side only

The majority of Custom Nodes run purely on the server side, by defining a Python class that specifies the input and output types, and provides a function that can be called to process inputs and produce an output.

### Client side only

A few Custom Nodes provide a modification to the client UI, but do not add core functionality. Despite the name, they may not even add new nodes to the system.

### Independent Client and Server

Custom nodes may provide additional server features, and additional (related) UI features (such as a new widget to deal with a new data type). In most cases, communication between the client and server can be handled by the Comfy data flow control.

### Connected Client and Server

In a small number of cases, the UI features and the server need to interact with each other directly.

<Warning>Any node that requires Client-Server communication will not be compatible with use through the API.</Warning>


# Getting Started
Source: https://docs.comfy.org/custom-nodes/walkthrough



This page will take you step-by-step through the process of creating a custom node.

Our example will take a batch of images, and return one of the images. Initially, the node
will return the image which is, on average, the lightest in color; we'll then extend
it to have a range of selection criteria, and then finally add some client side code.

This page assumes very little knowledge of Python or Javascript.

After this walkthrough, dive into the details of [backend code](./backend/server_overview), and
[frontend code](./backend/server_overview).

## Write a basic node

### Prerequisites

* A working ComfyUI [installation](/installation/manual_install). For development, we recommend installing ComfyUI manually.
* A working comfy-cli [installation](/comfy-cli/getting-started).

### Setting up

```bash  theme={null}
cd ComfyUI/custom_nodes
comfy node scaffold
```

After answering a few questions, you'll have a new directory set up.

```bash  theme={null}
 ~  % comfy node scaffold
You've downloaded .cookiecutters/cookiecutter-comfy-extension before. Is it okay to delete and re-download it? [y/n] (y): y
  [1/9] full_name (): Comfy
  [2/9] email (you@gmail.com): me@comfy.org
  [3/9] github_username (your_github_username): comfy
  [4/9] project_name (My Custom Nodepack): FirstComfyNode
  [5/9] project_slug (firstcomfynode): 
  [6/9] project_short_description (A collection of custom nodes for ComfyUI): 
  [7/9] version (0.0.1): 
  [8/9] Select open_source_license
    1 - GNU General Public License v3
    2 - MIT license
    3 - BSD license
    4 - ISC license
    5 - Apache Software License 2.0
    6 - Not open source
    Choose from [1/2/3/4/5/6] (1): 1
  [9/9] include_web_directory_for_custom_javascript [y/n] (n): y
Initialized empty Git repository in firstcomfynode/.git/
 Custom node project created successfully!
```

### Defining the node

Add the following code to the end of `src/nodes.py`:

```Python src/nodes.py theme={null}
class ImageSelector:
    CATEGORY = "example"
    @classmethod    
    def INPUT_TYPES(s):
        return { "required":  { "images": ("IMAGE",), } }
    RETURN_TYPES = ("IMAGE",)
    FUNCTION = "choose_image"
```

<Info>The basic structure of a custom node is described in detail [here](/custom-nodes/backend/server_overview). </Info>

A custom node is defined using a Python class, which must include these four things: `CATEGORY`,
which specifies where in the add new node menu the custom node will be located,
`INPUT_TYPES`, which is a class method defining what inputs the node will take
(see [later](/custom-nodes/backend/server_overview#input-types) for details of the dictionary returned),
`RETURN_TYPES`, which defines what outputs the node will produce, and `FUNCTION`, the name
of the function that will be called when the node is executed.

<Tip>Notice that the data type for input and output is `IMAGE` (singular) even though
we expect to receive a batch of images, and return just one. In Comfy, `IMAGE` means
image batch, and a single image is treated as a batch of size 1.</Tip>

### The main function

The main function, `choose_image`, receives named arguments as defined in `INPUT_TYPES`, and
returns a `tuple` as defined in `RETURN_TYPES`. Since we're dealing with images, which are internally
stored as `torch.Tensor`,

```Python  theme={null}
import torch
```

Then add the function to your class. The datatype for image is `torch.Tensor` with shape `[B,H,W,C]`,
where `B` is the batch size and `C` is the number of channels - 3, for RGB. If we iterate over such
a tensor, we will get a series of `B` tensors of shape `[H,W,C]`. The `.flatten()` method turns
this into a one dimensional tensor, of length `H*W*C`, `torch.mean()` takes the mean, and `.item()`
turns a single value tensor into a Python float.

```Python  theme={null}
def choose_image(self, images):
    brightness = list(torch.mean(image.flatten()).item() for image in images)
    brightest = brightness.index(max(brightness))
    result = images[brightest].unsqueeze(0)
    return (result,)
```

Notes on those last two lines:

* `images[brightest]` will return a Tensor of shape `[H,W,C]`. `unsqueeze` is used to insert a (length 1) dimension at, in this case, dimension zero, to give
  us `[B,H,W,C]` with `B=1`: a single image.
* in `return (result,)`, the trailing comma is essential to ensure you return a tuple.

### Register the node

To make Comfy recognize the new node, it must be available at the package level. Modify the `NODE_CLASS_MAPPINGS` variable at the end of `src/nodes.py`. You must restart ComfyUI to see any changes.

```Python src/nodes.py theme={null}

NODE_CLASS_MAPPINGS = {
    "Example" : Example,
    "Image Selector" : ImageSelector,
}

# Optionally, you can rename the node in the `NODE_DISPLAY_NAME_MAPPINGS` dictionary.
NODE_DISPLAY_NAME_MAPPINGS = {
    "Example": "Example Node",
    "Image Selector": "Image Selector",
}
```

<Info>For a detailed explanation of how ComfyUI discovers and loads custom nodes, see the [node lifecycle documentation](/custom-nodes/backend/lifecycle).</Info>

## Add some options

That node is maybe a bit boring, so we might add some options; a widget that allows you to
choose the brightest image, or the reddest, bluest, or greenest. Edit your `INPUT_TYPES` to look like:

```Python  theme={null}
@classmethod    
def INPUT_TYPES(s):
    return { "required":  { "images": ("IMAGE",), 
                            "mode": (["brightest", "reddest", "greenest", "bluest"],)} }
```

Then update the main function. We'll use a fairly naive definition of 'reddest' as being the average
`R` value of the pixels divided by the average of all three colors. So:

```Python  theme={null}
def choose_image(self, images, mode):
    batch_size = images.shape[0]
    brightness = list(torch.mean(image.flatten()).item() for image in images)
    if (mode=="brightest"):
        scores = brightness
    else:
        channel = 0 if mode=="reddest" else (1 if mode=="greenest" else 2)
        absolute = list(torch.mean(image[:,:,channel].flatten()).item() for image in images)
        scores = list( absolute[i]/(brightness[i]+1e-8) for i in range(batch_size) )
    best = scores.index(max(scores))
    result = images[best].unsqueeze(0)
    return (result,)
```

## Tweak the UI

Maybe we'd like a bit of visual feedback, so let's send a little text message to be displayed.

### Send a message from server

This requires two lines to be added to the Python code:

```Python  theme={null}
from server import PromptServer
```

and, at the end of the `choose_image` method, add a line to send a message to the front end (`send_sync` takes a message
type, which should be unique, and a dictionary)

```Python  theme={null}
PromptServer.instance.send_sync("example.imageselector.textmessage", {"message":f"Picked image {best+1}"})
return (result,)
```

### Write a client extension

To add some Javascript to the client, create a subdirectory, `web/js` in your custom node directory, and modify the end of `__init__.py`
to tell Comfy about it by exporting `WEB_DIRECTORY`:

```Python  theme={null}
WEB_DIRECTORY = "./web/js"
__all__ = ['NODE_CLASS_MAPPINGS', 'WEB_DIRECTORY']
```

The client extension is saved as a `.js` file in the `web/js` subdirectory, so create `image_selector/web/js/imageSelector.js` with the
code below. (For more, see [client side coding](./js/javascript_overview)).

```Javascript  theme={null}
import { app } from "../../scripts/app.js";
app.registerExtension({
	name: "example.imageselector",
    async setup() {
        function messageHandler(event) { alert(event.detail.message); }
        app.api.addEventListener("example.imageselector.textmessage", messageHandler);
    },
})
```

All we've done is register an extension and add a listener for the message type we are sending in the `setup()` method. This reads the dictionary we sent (which is stored in `event.detail`).

Stop the Comfy server, start it again, reload the webpage, and run your workflow.

### The complete example

The complete example is available [here](https://gist.github.com/robinjhuang/fbf54b7715091c7b478724fc4dffbd03). You can download the example workflow [JSON file](https://github.com/Comfy-Org/docs/blob/main/public/workflow.json) or view it below:

<div align="center">
  <img src="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/firstnodeworkflow.png?fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=a6f6c26f0dba136f864c59044909c55c" alt="Image Selector Workflow" width="100%" data-og-width="2264" data-og-height="1202" data-path="images/firstnodeworkflow.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/firstnodeworkflow.png?w=280&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=f2f67b0396e9d25543aeb3f3e3bb520c 280w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/firstnodeworkflow.png?w=560&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=9559b4d018dd5853f8a94bccf8056f45 560w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/firstnodeworkflow.png?w=840&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=0ce3e3b832cfe24004f61109ed025930 840w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/firstnodeworkflow.png?w=1100&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=2e82d92552ce1ae1315d35724f81db67 1100w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/firstnodeworkflow.png?w=1650&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=a61bedab96b65b0ef4c1d80650720821 1650w, https://mintcdn.com/dripart/EgZuQyCGLVUEw53Z/images/firstnodeworkflow.png?w=2500&fit=max&auto=format&n=EgZuQyCGLVUEw53Z&q=85&s=b5363f7ee4b36b0c132f1691e886c6d1 2500w" />
</div>


# ComfyUI Account API Key Integration
Source: https://docs.comfy.org/development/comfyui-server/api-key-integration

This article explains how to use ComfyUI Account API Key to call paid API nodes in headless mode

Starting from [PR #8041](https://github.com/comfyanonymous/ComfyUI/pull/8041), ComfyUI supports directly using built-in paid API nodes through your ComfyUI Account API Key, without requiring a specific frontend interface (you can even run without a frontend).

This means you can create workflows that combine:

* local OS models
* tools from the custom node community
* popular paid models

Then run everything together by simply sending the prompt to the Comfy webserver API, letting it handle all the orchestration.

This is helpful for users who want to use Comfy as a backend service, via the command line, with their own frontend, etc.

## Prerequisites

Using your ComfyUI Account API Key to call paid API nodes requires:

* A ComfyUI Account API Key
* Sufficient account credits

<Note>
  **Important:** This page describes the **ComfyUI Account API Key** used for accessing paid API nodes in workflows. If you're looking to publish custom nodes to the registry instead, see [Publishing Nodes](/registry/publishing).
</Note>

To use your ComfyUI Account API Key to call paid API nodes, you need to first register an account on [ComfyUI Platform](https://platform.comfy.org/login) and create an API key

<Card title="Login with API Key" icon="link" href="/interface/user#logging-in-with-an-api-key">
  Please refer to the User Interface section to learn how to login with API Key
</Card>

You need to ensure your ComfyUI account has sufficient credits to test the corresponding features.

<Card title="Credits" icon="link" href="/interface/credits">
  Please refer to the Credits section to learn how to purchase credits for your account
</Card>

## Python Example

Here is an example of how to send a workflow containing API nodes to the ComfyUI API using Python code:

```python  theme={null}
"""Using API nodes when running ComfyUI headless or with alternative frontend

You can execute a ComfyUI workflow that contains API nodes by including an API key in the prompt.
The API key should be added to the `extra_data` field of the payload.
Below we show an example of how to do this.

See more:

- API nodes overview: https://docs.comfy.org/tutorials/partner-nodes/overview
- To generate an API key, login here: https://platform.comfy.org/login
"""

import json
from urllib import request

SERVER_URL = "http://127.0.0.1:8188"

# We have a prompt/job (workflow in "API format") that contains API nodes.
workflow_with_api_nodes = """{
  "11": {
    "inputs": {
      "prompt": "A dreamy, surreal half-body portrait of a young woman meditating. She has a short, straight bob haircut dyed in pastel pink, with soft bangs covering her forehead. Her eyes are gently closed, and her hands are raised in a calm, open-palmed meditative pose, fingers slightly curved, as if levitating or in deep concentration. She wears a colorful dress made of patchwork-like pastel tiles, featuring clouds, stars, and rainbows. Around her float translucent, iridescent soap bubbles reflecting the rainbow hues. The background is a fantastical sky filled with cotton-candy clouds and vivid rainbow waves, giving the entire scene a magical, dreamlike atmosphere. Emphasis on youthful serenity, whimsical ambiance, and vibrant soft lighting.",
      "prompt_upsampling": false,
      "seed": 589991183902375,
      "aspect_ratio": "1:1",
      "raw": false,
      "image_prompt_strength": 0.4000000000000001,
      "image_prompt": [
        "14",
        0
      ]
    },
    "class_type": "FluxProUltraImageNode",
    "_meta": {
      "title": "Flux 1.1 [pro] Ultra Image"
    }
  },
  "12": {
    "inputs": {
      "filename_prefix": "ComfyUI",
      "images": [
        "11",
        0
      ]
    },
    "class_type": "SaveImage",
    "_meta": {
      "title": "Save Image"
    }
  },
  "14": {
    "inputs": {
      "image": "example.png"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image"
    }
  }
}"""


prompt = json.loads(workflow_with_api_nodes)
payload = {
    "prompt": prompt,
    # Add the `api_key_comfy_org` to the payload.
    # You can first get the key from the associated user if handling multiple clients.
    "extra_data": {
        "api_key_comfy_org": "comfyui-87d01e28d*******************************************************"  # replace with actual key
    },
}
data = json.dumps(payload).encode("utf-8")
req = request.Request(f"{SERVER_URL}/prompt", data=data)
request.urlopen(req)

```

## Related Documentation

* [API nodes overview](https://docs.comfy.org/tutorials/partner-nodes/overview)
* [Account management](https://docs.comfy.org/interface/user)
* [Credits](https://docs.comfy.org/interface/credits)


# Messages
Source: https://docs.comfy.org/development/comfyui-server/comms_messages



## Messages

During execution (or when the state of the queue changes), the `PromptExecutor` sends messages back to the client
through the `send_sync` method of `PromptServer`.

These messages are received by a socket event listener defined in `api.js` (at time of writing around line 90, or search for `this.socket.addEventListener`),
which creates a `CustomEvent` object for any known message type, and dispatches it to any registered listeners.

An extension can register to receive events (normally done in the `setup()` function) following the standard Javascript idiom:

```Javascript  theme={null}
api.addEventListener(message_type, messageHandler);
```

If the `message_type` is not one of the built in ones, it will be added to the list of known message types automatically. The message `messageHandler`
will be called with a `CustomEvent` object, which extends the event raised by the socket to add a `.detail` property, which is a dictionary of
the data sent by the server. So usage is generally along the lines of:

```Javascript  theme={null}
function messageHandler(event) {
    if (event.detail.node == aNodeIdThatIsInteresting) {
        // do something with event.detail.other_things
    }
}
```

### Built in message types

During execution (or when the state of the queue changes), the `PromptExecutor` sends the following messages back to the client
through the `send_sync` method of `PromptServer`. An extension can register as a listener for any of these.

| event                   | when                                                                       | data                                                                                                    |
| ----------------------- | -------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------- |
| `execution_start`       | When a prompt is about to run                                              | `prompt_id`                                                                                             |
| `execution_error`       | When an error occurs during execution                                      | `prompt_id`, plus additional information                                                                |
| `execution_interrupted` | When execution is stopped by a node raising `InterruptProcessingException` | `prompt_id`, `node_id`, `node_type` and `executed` (a list of executed nodes)                           |
| `execution_cached`      | At the start of execution                                                  | `prompt_id`, `nodes` (a list of nodes which are being skipped because their cached outputs can be used) |
| `execution_success`     | When all nodes from the prompt have been successfully executed             | `prompt_id`, `timestamp`                                                                                |
| `executing`             | When a new node is about to be executed                                    | `node` (node id or `None` to indicate completion), `prompt_id`                                          |
| `executed`              | When a node returns a ui element                                           | `node` (node id), `prompt_id`, `output`                                                                 |
| `progress`              | During execution of a node that implements the required hook               | `node` (node id), `prompt_id`, `value`, `max`                                                           |
| `status`                | When the state of the queue changes                                        | `exec_info`, a dictionary holding `queue_remaining`, the number of entries in the queue                 |

### Using executed

Despite the name, an `executed` message is not sent whenever a node completes execution (unlike `executing`), but only when the node
returns a ui update.

To do this, the main function needs to return a dictionary instead of a tuple:

```python  theme={null}
# at the end of my main method
        return { "ui":a_new_dictionary, "result": the_tuple_of_output_values }
```

`a_new_dictionary` will then be sent as the value of `output` in an `executed` message.
The `result` key can be omitted if the node has no outputs (see, for instance, the code for `SaveImage` in `nodes.py`)

### Custom message types

As indicated above, on the client side, a custom message type can be added simply by registering as a listener for a unique message type name.

```Javascript  theme={null}
api.addEventListener("my.custom.message", messageHandler);
```

On the server, the code is equally simple:

```Python  theme={null}
from server import PromptServer
# then, in your main execution function (normally)
        PromptServer.instance.send_sync("my.custom.message", a_dictionary)
```

#### Getting node\_id

Most of the built-in messages include the current node id in the value of `node`. It's likely that you will want to do the same.

The node\_id is available on the server side through a hidden input, which is obtained with the `hidden` key in the `INPUT_TYPES` dictionary:

```Python  theme={null}
    @classmethod    
    def INPUT_TYPES(s):
        return {"required" : { }, # whatever your required inputs are 
                "hidden": { "node_id": "UNIQUE_ID" } } # Add the hidden key

    def my_main_function(self, required_inputs, node_id):
        # do some things
        PromptServer.instance.send_sync("my.custom.message", {"node": node_id, "other_things": etc})
```


# Server Overview
Source: https://docs.comfy.org/development/comfyui-server/comms_overview



## Overview

The Comfy server runs on top of the [aiohttp framework](https://docs.aiohttp.org/), which in turn uses [asyncio](https://pypi.org/project/asyncio/).

Messages from the server to the client are sent by socket messages through the `send_sync` method of the server,
which is an instance of `PromptServer` (defined in `server.py`). They are processed
by a socket event listener registered in `api.js`. See [messages](/development/comfyui-server/comms_messages).

Messages from the client to the server are sent by the `api.fetchApi()` method defined in `api.js`,
and are handled by http routes defined by the server. See [routes](/development/comfyui-server/comms_routes).

<Tip>The client submits the whole workflow (widget values and all) when you queue a request.
The server does not receive any changes you make after you send a request to the queue.
If you want to modify server behavior during execution, you'll need routes.</Tip>


# Routes
Source: https://docs.comfy.org/development/comfyui-server/comms_routes



## Routes

The server defines a series of `get` and `post` methods
which can be found by searching for `@routes` in `server.py`. When you submit a workflow
in the web client, it is posted to `/prompt` which validates the prompt and adds it to an execution queue,
returning either a `prompt_id` and `number` (the position in the queue), or `error` and `node_errors` if validation fails.
The prompt queue is defined in `execution.py`, which also defines the `PromptExecutor` class.

### Built in routes

`server.py` defines the following routes:

#### Core API Routes

| path                           | get/post/ws | purpose                                                                   |
| ------------------------------ | ----------- | ------------------------------------------------------------------------- |
| `/`                            | get         | load the comfy webpage                                                    |
| `/ws`                          | websocket   | WebSocket endpoint for real-time communication with the server            |
| `/embeddings`                  | get         | retrieve a list of the names of embeddings available                      |
| `/extensions`                  | get         | retrieve a list of the extensions registering a `WEB_DIRECTORY`           |
| `/features`                    | get         | retrieve server features and capabilities                                 |
| `/models`                      | get         | retrieve a list of available model types                                  |
| `/models/{folder}`             | get         | retrieve models in a specific folder                                      |
| `/workflow_templates`          | get         | retrieve a map of custom node modules and associated template workflows   |
| `/upload/image`                | post        | upload an image                                                           |
| `/upload/mask`                 | post        | upload a mask                                                             |
| `/view`                        | get         | view an image. Lots of options, see `@routes.get("/view")` in `server.py` |
| `/view_metadata`/{folder_name} | get         | retrieve metadata for a model                                             |
| `/system_stats`                | get         | retrieve information about the system (python version, devices, vram etc) |
| `/prompt`                      | get         | retrieve current queue status and execution information                   |
| `/prompt`                      | post        | submit a prompt to the queue                                              |
| `/object_info`                 | get         | retrieve details of all node types                                        |
| `/object_info/{node_class}`    | get         | retrieve details of one node type                                         |
| `/history`                     | get         | retrieve the queue history                                                |
| `/history/{prompt_id}`         | get         | retrieve the queue history for a specific prompt                          |
| `/history`                     | post        | clear history or delete history item                                      |
| `/queue`                       | get         | retrieve the current state of the execution queue                         |
| `/queue`                       | post        | manage queue operations (clear pending/running)                           |
| `/interrupt`                   | post        | stop the current workflow execution                                       |
| `/free`                        | post        | free memory by unloading specified models                                 |
| `/userdata`                    | get         | list user data files in a specified directory                             |
| `/v2/userdata`                 | get         | enhanced version that lists files and directories in structured format    |
| `/userdata/{file}`             | get         | retrieve a specific user data file                                        |
| `/userdata/{file}`             | post        | upload or update a user data file                                         |
| `/userdata/{file}`             | delete      | delete a specific user data file                                          |
| `/userdata/{file}/move/{dest}` | post        | move or rename a user data file                                           |
| `/users`                       | get         | get user information                                                      |
| `/users`                       | post        | create a new user (multi-user mode only)                                  |

### WebSocket Communication

The `/ws` endpoint provides real-time bidirectional communication between the client and server. This is used for:

* Receiving execution progress updates
* Getting node execution status in real-time
* Receiving error messages and debugging information
* Live updates when queue status changes

The WebSocket connection sends JSON messages with different types such as:

* `status` - Overall system status updates
* `execution_start` - When a prompt execution begins
* `execution_cached` - When cached results are used
* `executing` - Updates during node execution
* `progress` - Progress updates for long-running operations
* `executed` - When a node completes execution

### Custom routes

If you want to send a message from the client to the server during execution, you will need to add a custom route to the server.
For anything complicated, you will need to dive into the [aiohttp framework docs](https://docs.aiohttp.org/), but most cases can
be handled as follows:

```Python  theme={null}
from server import PromptServer
from aiohttp import web
routes = PromptServer.instance.routes
@routes.post('/my_new_path')
async def my_function(request):
    the_data = await request.post()
    # the_data now holds a dictionary of the values sent
    MyClass.handle_my_message(the_data)
    return web.json_response({})
```

<Tip>Unless you know what you are doing, don't try to define `my_function` within a class.
The `@routes.post` decorator does a lot of work! Instead, define the function as above
and then call a classmethod.</Tip>

<Tip>You can also define a `@routes.get` if you aren't changing anything.</Tip>

The client can use this new route by sending a `FormData` object with code something like this,
which would result in `the_data`, in the above code, containing `message` and `node_id` keys:

```Javascript  theme={null}
import { api } from "../../scripts/api.js";
function send_message(node_id, message) {
    const body = new FormData();
    body.append('message',message);
    body.append('node_id', node_id);
    api.fetchApi("/my_new_path", { method: "POST", body, });
}
```


# Execution Model Inversion Guide
Source: https://docs.comfy.org/development/comfyui-server/execution_model_inversion_guide



[PR #2666](https://github.com/comfyanonymous/ComfyUI/pull/2666) inverts the execution model from a back-to-front recursive model to a front-to-back topological sort. While most custom nodes should continue to "just work", this page is intended to serve as a guide for custom node creators to the things that *could* break.

## Breaking Changes

### Monkey Patching

Any code that monkey patched the execution model is likely to stop working. Note that the performance of execution with this PR exceeds that with the most popular monkey patches, so many of them will be unnecessary.

### Optional Input Validation

Prior to this PR, only nodes that were connected to outputs exclusively through a string of `"required"` inputs were actually validated. If you had custom nodes that were only ever connected to `"optional"` inputs, you previously wouldn't have been seeing that they failed validation.

<Tip>If your nodes' outputs could already be connected to `"required"` inputs, it is unlikely that anything in this section applies to you. It will primarily apply to custom node authors who use custom types and exclusively use `"optional"` inputs.</Tip>

Here are some of the things that could cause you to fail validation along with recommended solutions:

* Use of reserved [Additional Parameters](/custom-nodes/backend/datatypes#additional-parameters) like `min` and `max` on types that aren't comparable (e.g. dictionaries) in order to configure custom widgets.
  * Change the additional parameters used to non-reserved keys like `uiMin` and `uiMax`. *(Recommended Solution)*
    ```python  theme={null}
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "my_size": ("VEC2", {"uiMin": 0.0, "uiMax": 1.0}),
            }
        }
    ```

  * Define a custom [VALIDATE\_INPUTS](/custom-nodes/backend/server_overview#validate-inputs) function with this input so validation of it is skipped. *(Quick Solution)*
    ```python  theme={null}
    @classmethod
    def VALIDATE_INPUTS(cls, my_size):
        return True
    ```

* Use of composite types (e.g. `CUSTOM_A,CUSTOM_B`)
  * (When used as output) Define and use a wrapper like `MakeSmartType` [seen here in the PR's unit tests](https://github.com/comfyanonymous/ComfyUI/pull/2666/files#diff-714643f1fdb6f8798c45f77ab10d212ca7f41dd71bbe55069f1f9f146a8f0cb9R2)
    ```python  theme={null}
    class MyCustomNode:

        @classmethod
        def INPUT_TYPES(cls):
            return {
                "required": {
                    "input": (MakeSmartType("FOO,BAR"), {}),
                }
            }

        RETURN_TYPES = (MakeSmartType("FOO,BAR"),)

        # ...
    ```
  * (When used as input) Define a custom[VALIDATE\_INPUTS](/custom-nodes/backend/server_overview#validate-inputs) function that takes a `input_types` argument so type validation is skipped.
    ```python  theme={null}
    @classmethod
    def VALIDATE_INPUTS(cls, input_types):
        return True
    ```
  * (Supports both, convenient) Define and use the `@VariantSupport` decorator [seen here in the PR's unit tests](https://github.com/comfyanonymous/ComfyUI/pull/2666/files#diff-714643f1fdb6f8798c45f77ab10d212ca7f41dd71bbe55069f1f9f146a8f0cb9R15)
    ```python  theme={null}
    @VariantSupport
    class MyCustomNode:

        @classmethod
        def INPUT_TYPES(cls):
            return {
                "required": {
                    "input": ("FOO,BAR", {}),
                }
            }
        
        RETURN_TYPES = (MakeSmartType("FOO,BAR"),)

        # ...
    ```

* The use of lists (e.g. `[1, 2, 3]`) as constants in the graph definition (e.g. to represent a const `VEC3` input). This would have required a front-end extension before. Previously, lists of size exactly `2` would have failed anyway -- they would have been treated as broken links.
  * Wrap the lists in a dictionary like `{ "value": [1, 2, 3] }`

### Execution Order

Execution order has always changed depending on which nodes happen to have which IDs, but it may now change depending on which values are cached as well. In general, the execution order should be considered non-deterministic and subject to change (beyond what is enforced by the graph's structure).

Don't rely on the execution order.

*HIC SUNT DRACONES*

## New Functionality

### Validation Changes

A number of features were added to the `VALIDATE_INPUTS` function in order to lessen the impact of the [Optional Input Validation](#optional-input-validation) mentioned above.

* Default validation will now be skipped for inputs which are received by the `VALIDATE_INPUTS` function.
* The `VALIDATE_INPUTS` function can now take `**kwargs` which causes all inputs to be treated as validated by the node creator.
* The `VALIDATE_INPUTS` function can take an input named `input_types`. This input will be a dict mapping each input (connected via a link) to the type of the connected output. When this argument exists, type validation for the node's inputs is skipped.

You can read more at [VALIDATE\_INPUTS](/custom-nodes/backend/server_overview#validate-inputs).

### Lazy Evaluation

Inputs can be evaluated lazily (i.e. you can wait to see if they are needed before evaluating the attached node and all its ancestors). See [Lazy Evaluation](/custom-nodes/backend/lazy_evaluation) for more information.

### Node Expansion

At runtime, nodes can expand into a subgraph of nodes. This is what allows loops to be implemented (via tail-recursion). See [Node Expansion](/custom-nodes/backend/expansion) for more information.


# Overview
Source: https://docs.comfy.org/development/overview

Using ComfyUI as a Developer

ComfyUI is a powerful GenAI inference engine that can be used to run AI models locally, create workflows, develop custom nodes, and be deployed as a server.

ComfyUI's key capabilities are:

* **[Creating Workflows](/development/core-concepts/workflow)**: Workflows are a way to orchestrate AI models and automate tasks. They are a series of nodes that are connected together to form a pipeline.
* **[Custom Nodes](/custom-nodes/overview)**: Custom nodes can be written by anyone to extend the capabilities of ComfyUI for your own use. Nodes are written in Python and are published by the community.
* **Extensions**: Extensions are 3rd party applications that improve the UI of ComfyUI.
* **[Deployment](/development/comfyui-server/comms_overview)**: ComfyUI can be deployed in your own environment as an API endpoint. \[Learn more]


# OpenAI Chat API Node ComfyUI Official Example
Source: https://docs.comfy.org/tutorials/partner-nodes/openai/chat

This article will introduce how to use OpenAI Chat Partner nodes in ComfyUI to complete conversational functions

OpenAI is a company focused on generative AI, providing powerful conversational capabilities. Currently, ComfyUI has integrated the OpenAI API, allowing you to directly use the related nodes in ComfyUI to complete conversational functions.

In this guide, we will walk you through completing the corresponding conversational functionality.

<Tip>
  To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](/tutorials/partner-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.
</Tip>

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## OpenAI Chat Workflow

### 1. Workflow File Download

Please download the Json file below and drag it into ComfyUI to load the corresponding workflow.

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/openai/api_openai_chat.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download Json Format Workflow File</p>
</a>

### 2. Complete the Workflow Execution Step by Step

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/openai_chat_step_guide.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=a101638c18cc6656457c72f47bf0307c" alt="OpenAI Chat Step Guide" data-og-width="3834" width="3834" data-og-height="2148" height="2148" data-path="images/tutorial/api_nodes/openai/openai_chat_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/openai_chat_step_guide.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=fa3d9c490702f1376543a6d1717c117d 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/openai_chat_step_guide.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=f0072e6b06444e5be5b5d1a146458ef6 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/openai_chat_step_guide.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=c7febc5dd35adb937dd39a3771da0559 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/openai_chat_step_guide.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=cfdd9933ef732e40cdf1ac7bbd91a39f 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/openai_chat_step_guide.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=8343e98d4e6ad3f5edc919136c681cd1 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/openai/openai_chat_step_guide.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=62799060fa2a14fdbc8980dca0fe45a7 2500w" />

<Note>
  In the corresponding template, we have built a role setting for analyzing prompt generation.
</Note>

You can refer to the numbers in the image to complete the basic text-to-image workflow execution:

1. In the `Load Image` node, load the image you need AI to interpret
2. (Optional) If needed, you can modify the settings in `OpenAI Chat Advanced Options` to have AI execute specific tasks
3. In the `OpenAI Chat` node, you can modify `Prompt` to set the conversation prompt, or modify `model` to select different models
4. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the conversation.
5. After waiting for the API to return results, you can view the corresponding AI returned content in the `Preview Any` node.

### 3. Additional Notes

* Currently, the file input node `OpenAI Chat Input Files` requires files to be uploaded to the `ComfyUI/input/` directory first. This node is being improved, and we will modify the template after updates
* The workflow provides an example using `Batch Images` for input. If you have multiple images that need AI interpretation, you can refer to the step diagram and use right-click to set the corresponding node mode to `Always` to enable it


# Recraft Text to Image API Node ComfyUI Official Example
Source: https://docs.comfy.org/tutorials/partner-nodes/recraft/recraft-text-to-image

Learn how to use the Recraft Text to Image Partner node in ComfyUI

The [Recraft Text to Image](/built-in-nodes/partner-node/image/recraft/recraft-text-to-image) node allows you to create high-quality images in various styles using Recraft AI's image generation technology based on text descriptions.

In this guide, we'll show you how to set up a text-to-image workflow using this node.

<Tip>
  To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](/tutorials/partner-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.
</Tip>

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Recraft Text to Image API Node Workflow

### 1. Download the Workflow File

The workflow information is included in the metadata of the image below. Download and drag it into ComfyUI to load the workflow.

![Recraft Text to Image Workflow](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/api_nodes/recraft/t2i/recraft_t2i.png)

### 2. Follow the Steps to Run the Workflow

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/recraft/recraft_t2v_step_guide.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=26e3744a4e7c44a1d72840e8ffef5c3c" alt="Recraft Text to Image Workflow Steps" data-og-width="2822" width="2822" data-og-height="1505" height="1505" data-path="images/tutorial/api_nodes/recraft/recraft_t2v_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/recraft/recraft_t2v_step_guide.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=dc89e07d6e583587d024f85b99626261 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/recraft/recraft_t2v_step_guide.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=10cce0cedd135dbfef4efe38ea61eeab 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/recraft/recraft_t2v_step_guide.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=46b3c2bbad43ffa402380f256c2e899d 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/recraft/recraft_t2v_step_guide.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=4617e18965b44fb37f0c6652a1b8961c 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/recraft/recraft_t2v_step_guide.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=02b68dffb02846a5793b0487a2a34173 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/recraft/recraft_t2v_step_guide.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=b341b7afdec336aabccf223c6312f31f 2500w" />

Follow these numbered steps to run the basic workflow:

1. (Optional) Change the `Recraft Color RGB` in the `Color` node to your desired color
2. (Optional) Modify the `Recraft Style` node to control the visual style, such as digital art, realistic photo, or logo design. This group includes other style nodes you can enable as needed
3. (Optional) Edit the `prompt` parameter in the `Recraft Text to Image` node. You can also change the `size` parameter
4. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to generate the image
5. After the API returns the result, you can view the generated image in the `Save Image` node. The image will also be saved to the `ComfyUI/output/` directory

> (Optional) We've included a **Convert to SVG** group in the workflow. Since the `Recraft Vectorize Image` node in this group consumes additional credits, enable it only when you need to convert the generated image to SVG format

### 3. Additional Notes

* **Recraft Style**: Offers various preset styles like realistic photos, digital art, and logo designs
* **Seed Parameter**: Only used to determine if the node should run again, the actual generation result is not affected by the seed value

## Related Node Documentation

Check the following documentation for detailed parameter settings of the nodes

<Card title="Recraft Text to Image Node Documentation" icon="book" href="/built-in-nodes/partner-node/image/recraft/recraft-text-to-image">
  Documentation for the Recraft Text to Image Partner node
</Card>

<Card title="Recraft Style Node Documentation" icon="book" href="/built-in-nodes/partner-node/image/recraft/recraft-style-realistic-image">
  Documentation for the Recraft Style - Realistic Image Partner node
</Card>

<Card title="Recraft Controls Node Documentation" icon="book" href="/built-in-nodes/partner-node/image/recraft/recraft-controls">
  Documentation for the Recraft Controls Partner node
</Card>


# Rodin API Node Model Generation ComfyUI Official Example
Source: https://docs.comfy.org/tutorials/partner-nodes/rodin/model-generation

This article will introduce how to use Rodin node's API in ComfyUI for model generation

Hyper3D Rodin (hyper3d.ai) is a platform focused on rapidly generating high-quality, production-ready 3D models and materials through artificial intelligence.
ComfyUI has now natively integrated the corresponding Rodin model generation API, allowing you to conveniently use the related nodes in ComfyUI for model generation.

Currently, ComfyUI's Partner nodes support the following Rodin model generation capabilities:

* Single-view model generation
* Multi-view model generation
* Model generation with different levels of detail

<Tip>
  To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](/tutorials/partner-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.
</Tip>

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Single-view Model Generation Workflow

### 1. Workflow File Download

Download the file below and drag it into ComfyUI to load the corresponding workflow.

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/rodin_image_to_model.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download Json Format Workflow File</p>
</a>

Download the image below as input image

![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/doll.jpg)

### 2. Complete the Workflow Execution Step by Step

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/rodin/rodin_image_to_model_step_guide.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=ed79f1fb8546ed41a3d157cece818ee9" alt="ComfyUI Rodin Image to Model Step Guide" data-og-width="3184" width="3184" data-og-height="1966" height="1966" data-path="images/tutorial/api_nodes/rodin/rodin_image_to_model_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/rodin/rodin_image_to_model_step_guide.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=bb2b6e3ac6ed70eacf7eeac89fd00f52 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/rodin/rodin_image_to_model_step_guide.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=19a1db47eea31a4666f42bb6607cb513 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/rodin/rodin_image_to_model_step_guide.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=75cbf45620b8886b60a396330a9bb382 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/rodin/rodin_image_to_model_step_guide.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=c33ffd2c8306ea9d2d9ba27191374af7 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/rodin/rodin_image_to_model_step_guide.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=bf0689795ccfaa241823dcf6ae4bd2e3 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/rodin/rodin_image_to_model_step_guide.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=03f0404c646e21eaac437e9d2959c174 2500w" />

You can refer to the numbers in the image to complete the basic text-to-image workflow execution:

1. In the `Load Image` node, load the provided input image
2. (Optional) In `Rodin 3D Generate - Regular Generate` adjust the corresponding parameters
   * polygon\_count: You can set different polygon counts, the higher the value, the smoother and more detailed the model
3. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute model generation. After the workflow completes, the corresponding model will be automatically saved to the `ComfyUI/output/Rodin` directory
4. In the `Preview 3D` node, click to expand the menu
5. Select `Export` to directly export the corresponding model

## Multi-view Model Generation Workflow

The corresponding `Rodin 3D Generate - Regular Generate` allows up to 5 image inputs

### 1. Workflow File Download

You can modify the single-view workflow to a multi-view workflow, or directly download the workflow file below

Download the file below and drag it into ComfyUI to load the corresponding workflow.

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/multiview_to_model/api_rodin_multiview_to_model.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download Json Format Workflow File</p>
</a>

Download the images below as input images

![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/multiview_to_model/front.jpg)
![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/multiview_to_model/back.jpg)
![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/rodin/multiview_to_model/left.jpg)

### 2. Complete the Workflow Execution Step by Step

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/rodin/rodin_multiview_to_model_step_guide.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=6241527775e83bd480811631ef4d88ed" alt="ComfyUI Rodin Image to Model Step Guide" data-og-width="3062" width="3062" data-og-height="1862" height="1862" data-path="images/tutorial/api_nodes/rodin/rodin_multiview_to_model_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/rodin/rodin_multiview_to_model_step_guide.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=e3bdf978dce46f6db8ab13a0437f8af6 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/rodin/rodin_multiview_to_model_step_guide.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=a867df65f89c9cfd6bb6ce6d84de998f 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/rodin/rodin_multiview_to_model_step_guide.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=4a64175900800ea8ed9c10834b1a659a 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/rodin/rodin_multiview_to_model_step_guide.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=85a75906989c2303477a70a57bc2a749 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/rodin/rodin_multiview_to_model_step_guide.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=24ca3f810e13d8e7eab70ceb2ec96dfd 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/rodin/rodin_multiview_to_model_step_guide.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=c82bea8ae4fbf55442bdc61abfd3dc8c 2500w" />

You can refer to the numbers in the image to complete the basic text-to-image workflow execution:

1. In the `Load Image` node, load the provided input images
2. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute model generation. After the workflow completes, the corresponding model will be automatically saved to the `ComfyUI/output/Rodin` directory
3. In the `Preview 3D` node, click to expand the menu
4. Select `Export` to directly export the corresponding model

## Other Related Nodes

Currently, Rodin provides different types of model generation nodes in ComfyUI, since the corresponding input conditions are the same as the workflow introduced in this article, you can enable them as needed. In addition, we have provided corresponding nodes in the corresponding templates, you can also modify the corresponding node mode as needed to enable them

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/rodin/other_nodes.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=7caf60a6b994156e71476564c856fc15" alt="Rodin Other Related Nodes" data-og-width="975" width="975" data-og-height="1170" height="1170" data-path="images/tutorial/api_nodes/rodin/other_nodes.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/rodin/other_nodes.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=37c61a8000c0d3bfd5db85417a4ef98a 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/rodin/other_nodes.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=ed2d06928db9041c249ab44b780c0127 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/rodin/other_nodes.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=d4faca003f5b868e16be6f45b03d8a8f 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/rodin/other_nodes.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=52067d9b1de876f176083b6b2c0cc8f5 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/rodin/other_nodes.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=86fffeaf114a2532b28a95a75d5f5df7 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/rodin/other_nodes.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=945fdf363130d1e3bad049c2062dd231 2500w" />


# Runway API Node Image Generation ComfyUI Official Example
Source: https://docs.comfy.org/tutorials/partner-nodes/runway/image-generation

This article will introduce how to use Runway nodes in ComfyUI for text-to-image and reference-to-image generation

Runway is a company focused on generative AI, providing powerful image generation capabilities. Its models support features such as style transfer, image extension, and detail control. Currently, ComfyUI has integrated the Runway API, allowing you to directly use the related nodes in ComfyUI for image generation.

In this guide, we will walk you through the following workflows:

* Text-to-image
* Reference-to-image

<Tip>
  To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](/tutorials/partner-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.
</Tip>

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Runway Image Text-to-Image Workflow

### 1. Workflow File Download

The image below contains workflow information in its `metadata`. Please download and drag it into ComfyUI to load the corresponding workflow.

![ComfyUI Runway Image Text to Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/image/text_to_image.png)

### 2. Complete the Workflow Execution Step by Step

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_text_to_image_step_guide.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=66f4bf5de975a5f0302d930e352605ff" alt="ComfyUI Runway Image Text to Image Step Guide" data-og-width="2842" width="2842" data-og-height="1260" height="1260" data-path="images/tutorial/api_nodes/runway/runway_text_to_image_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_text_to_image_step_guide.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=126eb9ab59ac3fa69464dcca17d562c9 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_text_to_image_step_guide.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=4a77fa7bbaf5dc855339285e728bc27a 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_text_to_image_step_guide.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=73094b95981662d3901cd4510050449b 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_text_to_image_step_guide.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=a604909a3fa461af35535f68ead78758 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_text_to_image_step_guide.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=4951668dc502679fe8d2c0e229330b28 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_text_to_image_step_guide.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=1c41ecbce818d39424610c0a5ee21efa 2500w" />

You can refer to the numbers in the image to complete the basic text-to-image workflow execution:

1. In the `Runway Text to Image` node, input your prompt in the `prompt` field
2. (Optional) Adjust the `ratio` setting to set different output aspect ratios
3. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute image generation.
4. After waiting for the API to return results, you can view the generated image in the `Save Image` node (right-click to save). The corresponding image will also be saved to the `ComfyUI/output/` directory.

## Runway Image Reference-to-Image Workflow

### 1. Workflow and Input Image Download

The image below contains workflow information in its `metadata`. Please download and drag it into ComfyUI to load the corresponding workflow.

![ComfyUI Runway Image Reference to Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/image/reference_to_image/runway_reference_to_image.png)

Download the image below for input

![ComfyUI Runway Image Reference to Image Input](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/image/reference_to_image/input.png)

### 2. Complete the Workflow Execution Step by Step

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_reference_to_image_step_guide.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=f0ce702cc2a6a03d887072416d78c241" alt="ComfyUI Runway Image Reference to Image Step Guide" data-og-width="2842" width="2842" data-og-height="1260" height="1260" data-path="images/tutorial/api_nodes/runway/runway_reference_to_image_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_reference_to_image_step_guide.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=696ef8ecc46427bd46c7830c11b24f49 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_reference_to_image_step_guide.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=1956bc38e424e3e757f63ee07fa88957 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_reference_to_image_step_guide.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=8d03e7a6eca51b29251bf9606b86e351 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_reference_to_image_step_guide.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=894ba54fbd13795bac0e847936fed65c 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_reference_to_image_step_guide.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=423127ac47ecb8310a3a451a35b4e522 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_reference_to_image_step_guide.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=c97748843c91858030259f11b2ad526e 2500w" />

You can refer to the numbers in the image to complete the basic reference-to-image workflow execution:

1. In the `Load Image` node, load the provided input image
2. In the `Runway Text to Image` node, input your prompt in the `prompt` field and adjust dimensions
3. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute image generation.
4. After waiting for the API to return results, you can view the generated image in the `Save Image` node (right-click to save). The corresponding image will also be saved to the `ComfyUI/output/` directory.


# Runway API Node Video Generation ComfyUI Official Example
Source: https://docs.comfy.org/tutorials/partner-nodes/runway/video-generation

This article will introduce how to use Runway nodes in ComfyUI for video generation workflows

Runway is a company focused on generative AI, providing powerful video generation capabilities. Currently, ComfyUI has integrated the Runway API, allowing you to directly use the related nodes in ComfyUI for video generation.

Currently, ComfyUI natively integrates the following Runway video generation models:

* Runway Gen3a turbo
* Runway Gen4 turbo
* Runway First Last Frame to video

<Tip>
  To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](/tutorials/partner-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.
</Tip>

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Gen3a turbo Image-to-Video Workflow

### 1. Workflow File Download

The video below contains workflow information in its `metadata`. Please download and drag it into ComfyUI to load the corresponding workflow.

<video controls className="w-full aspect-video" src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/gen3a_turbo_image_to_video/runway_image_to_video_gen3a_turbo.mp4" />

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/gen3a_turbo_image_to_video/runway_image_to_video_gen3a_turbo.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download Json Format Workflow File</p>
</a>

Download the image below as input image

![ComfyUI Runway gen3a turbo image to video Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/gen3a_turbo_image_to_video/steampunk.png)

### 2. Complete the Workflow Execution Step by Step

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_gen3a_turbo_image_to_video_step_guide.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=c173269c427a4c3fcc08f5b0ead4ffab" alt="ComfyUI Runway gen3a turbo image to video Step Guide" data-og-width="2202" width="2202" data-og-height="1188" height="1188" data-path="images/tutorial/api_nodes/runway/runway_gen3a_turbo_image_to_video_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_gen3a_turbo_image_to_video_step_guide.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=0a59187c4a18d53b4568d662092f12de 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_gen3a_turbo_image_to_video_step_guide.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=23df8ac507adbadca3e4e49883e7ce8b 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_gen3a_turbo_image_to_video_step_guide.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=5c511ccab9bbfdde11116edfe184999b 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_gen3a_turbo_image_to_video_step_guide.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=770b47e6170450ce300b163f8ed77ccc 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_gen3a_turbo_image_to_video_step_guide.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=58f31be12e93ece370302cae526092a9 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_gen3a_turbo_image_to_video_step_guide.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=3ced3310fde20979a1159def906fa388 2500w" />

You can refer to the numbers in the image to complete the basic image-to-video workflow execution:

1. In the `Load Image` node, load the provided input image
2. In the `Runway Gen3a turbo` node, set the `prompt` to describe video content, modify the `duration` parameter to set video length, modify the `ratio` parameter to set video aspect ratio
3. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute video generation.
4. After waiting for the API to return results, you can view the generated video in the `Save Video` node (right-click to save). The corresponding video will also be saved to the `ComfyUI/output/` directory.

## Gen4 turbo Image-to-Video Workflow

### 1. Workflow File Download

The video below contains workflow information in its `metadata`. Please download and drag it into ComfyUI to load the corresponding workflow.

<video controls className="w-full aspect-video" src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/gen4_turbo_image_to_video/runway_gen4_turo_image_to_video.mp4" />

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/gen4_turbo_image_to_video/runway_gen4_turo_image_to_video.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download Json Format Workflow File</p>
</a>

Download the image below as input image

![ComfyUI Runway gen4 turbo image to video Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/gen4_turbo_image_to_video/godfather.jpg)

### 2. Complete the Workflow Execution Step by Step

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_gen4_turbo_image_to_video_step_guide.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=1c2df6f00cd1a0fc221a058621cd1449" alt="ComfyUI Runway gen4 turbo image to video Step Guide" data-og-width="2202" width="2202" data-og-height="1152" height="1152" data-path="images/tutorial/api_nodes/runway/runway_gen4_turbo_image_to_video_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_gen4_turbo_image_to_video_step_guide.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=676ee2e7986565c3e482f153871858de 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_gen4_turbo_image_to_video_step_guide.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=49184313bdcc3931cfdf6aec01fd7170 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_gen4_turbo_image_to_video_step_guide.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=11dc5ed72c9e578e83484ff3a24ed8f3 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_gen4_turbo_image_to_video_step_guide.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=ada028966a71fed3c695a5fafc5ba08b 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_gen4_turbo_image_to_video_step_guide.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=9344d3b599961ce42bd43e7f5a9dd23f 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_gen4_turbo_image_to_video_step_guide.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=81a52c64571c1a3c61410e63f9183a72 2500w" />

You can refer to the numbers in the image to complete the basic image-to-video workflow execution:

1. In the `Load Image` node, load the provided input image
2. In the `Runway Gen4 turbo` node, set the `prompt` to describe video content, modify the `duration` parameter to set video length, modify the `ratio` parameter to set video aspect ratio
3. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute video generation.
4. After waiting for the API to return results, you can view the generated video in the `Save Video` node (right-click to save). The corresponding video will also be saved to the `ComfyUI/output/` directory.

## First-Last Frame Video Generation Workflow

### 1. Workflow File Download

The video below contains workflow information in its `metadata`. Please download and drag it into ComfyUI to load the corresponding workflow.

<video controls className="w-full aspect-video" src="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/first_last_frame_to_video/runway_first_last_frame.mp4" />

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/first_last_frame_to_video/runway_first_last_frame.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download Json Format Workflow File</p>
</a>

Download the images below as input images

![ComfyUI Runway gen4 turbo image to video Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/first_last_frame_to_video/first.jpg)
![ComfyUI Runway gen4 turbo image to video Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/runway/first_last_frame_to_video/last.jpg)

### 2. Complete the Workflow Execution Step by Step

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_first_last_frame_step_guide.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=4cf592284e4ef7d48a1780e10483f03c" alt="ComfyUI Runway gen4 turbo image to video Step Guide" data-og-width="2082" width="2082" data-og-height="1154" height="1154" data-path="images/tutorial/api_nodes/runway/runway_first_last_frame_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_first_last_frame_step_guide.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=d39249726e6ff98915ca48dbd010c81e 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_first_last_frame_step_guide.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=ad3e00b1647cfa32b12682f69ecc004d 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_first_last_frame_step_guide.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=0570bf102ba92d8b823caf6cf604126f 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_first_last_frame_step_guide.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=d969338e625f09c6e54a07eb5c6f1ce1 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_first_last_frame_step_guide.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=bcd44d0a555ee1c9893d5ddb154f1266 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/runway/runway_first_last_frame_step_guide.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=cb6a1b59c2f2904f1c7d7e0fdcde23d0 2500w" />

You can refer to the numbers in the image to complete the basic first-last frame to video workflow execution:

1. In the `Load Image` node, load the starting frame
2. In the `Load Image` node, load the ending frame
3. In the `Runway First-Last-Frame to Video` node, set the `prompt` to describe video content, modify the `duration` parameter to set video length, modify the `ratio` parameter to set video aspect ratio
4. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute video generation.
5. After waiting for the API to return results, you can view the generated video in the `Save Video` node (right-click to save). The corresponding video will also be saved to the `ComfyUI/output/` directory.


# Tripo API Node Model Generation ComfyUI Official Example
Source: https://docs.comfy.org/tutorials/partner-nodes/tripo/model-generation

This article will introduce how to use Tripo node's API in ComfyUI for model generation

Tripo AI is a company focused on generative AI 3D modeling. It provides user-friendly platforms and API services that can quickly convert text prompts or 2D images (single or multiple) into high-quality 3D models.
ComfyUI has now natively integrated the corresponding Tripo API, allowing you to conveniently use the related nodes in ComfyUI for model generation.

Currently, ComfyUI's Partner nodes support the following Tripo model generation capabilities:

* Text-to-model
* Image-to-model
* Multi-view model generation
* Rig model
* Retarget rigged model

<Tip>
  To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](/tutorials/partner-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.
</Tip>

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Text-to-Model Workflow

### 1. Workflow File Download

Download the file below and drag it into ComfyUI to load the corresponding workflow.

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/api_tripo_text_to_model.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download Json Format Workflow File</p>
</a>

### 2. Complete the Workflow Execution Step by Step

<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/api_nodes/tripo/tripo_text_to_model_step_guide.jpg?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=b25199fbdd5c00f3fd8a1b587586bbed" alt="ComfyUI Tripo Text to Model Step Guide" data-og-width="2318" width="2318" data-og-height="1564" height="1564" data-path="images/tutorial/api_nodes/tripo/tripo_text_to_model_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/api_nodes/tripo/tripo_text_to_model_step_guide.jpg?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=808ef38a541ba42c3b620b3e55ff56f3 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/api_nodes/tripo/tripo_text_to_model_step_guide.jpg?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=ca327bac2f1d7a11cc6d8433607e85ed 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/api_nodes/tripo/tripo_text_to_model_step_guide.jpg?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=432f2214d0abbb7709ec85850f76b9c9 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/api_nodes/tripo/tripo_text_to_model_step_guide.jpg?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=f34d08594b7a1720ae80afa4c91b3142 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/api_nodes/tripo/tripo_text_to_model_step_guide.jpg?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=a68421325e3a559a58e8a1a84395cf17 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/api_nodes/tripo/tripo_text_to_model_step_guide.jpg?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=1c434abf6d478d1f86f1385614cc29a8 2500w" />

You can refer to the numbers in the image to complete the basic text-to-model workflow execution:

1. In the `Tripo: Text to Model` node, input your prompt in the `prompt` field
   * model: You can select different models, currently only v1.4 model supports subsequent optimization with `Tripo: Refine Draft model`
   * style: You can set different styles
   * texture\_quality: You can set different texture qualities
2. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute model generation. After the workflow completes, the corresponding model will be automatically saved to the `ComfyUI/output/` directory
3. In the `Preview 3D` node, click to expand the menu
4. Select `Export` to directly export the corresponding model

## Image-to-Model Workflow

### 1. Workflow File Download

Download the file below and drag it into ComfyUI to load the corresponding workflow.

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/image_to_model/api_tripo_image_to_model.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download Json Format Workflow File</p>
</a>

Download the image below as input image

![Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/image_to_model/panda.jpg)

### 2. Complete the Workflow Execution Step by Step

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/tripo/tripo_image_to_model_step_guide.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=302a46095bbb73ea478f441fa489b763" alt="ComfyUI Tripo Text to Model Step Guide" data-og-width="2242" width="2242" data-og-height="2610" height="2610" data-path="images/tutorial/api_nodes/tripo/tripo_image_to_model_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/tripo/tripo_image_to_model_step_guide.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=d8e92cb71153fceb6c2f0dbb4c1274f3 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/tripo/tripo_image_to_model_step_guide.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=0a94c80971b9c0af74607418869d99e5 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/tripo/tripo_image_to_model_step_guide.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=c52fda410d968df9b53b62282bd0af75 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/tripo/tripo_image_to_model_step_guide.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=f38d19355d3a8bdb3f1c7d9754f2e186 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/tripo/tripo_image_to_model_step_guide.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=e2d4f2d0e7592eddc65143626053ebe4 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/tripo/tripo_image_to_model_step_guide.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=88b1cbf8fce76b0e1810bf6ec9aa2a6c 2500w" />

You can refer to the numbers in the image to complete the basic image-to-model workflow execution:

1. In the `Load Image` node, load the provided input image
2. In the `Tripo: Image to Model` node, modify the corresponding parameter settings
   * model: You can select different models, currently only v1.4 model supports subsequent optimization with `Tripo: Refine Draft model`
   * style: You can set different styles
   * texture\_quality: You can set different texture qualities
3. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute model generation. After the workflow completes, the corresponding model will be automatically saved to the `ComfyUI/output/` directory
4. For model download, please refer to the instructions in the text-to-model section

## Multi-view Model Generation Workflow

### 1. Workflow File Download

Download the file below and drag it into ComfyUI to load the corresponding workflow.

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/multiview_to_image/api_tripo_multiview_to_model.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download Json Format Workflow File</p>
</a>

Download the images below as input images

![Front View](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/multiview_to_image/front.jpg)
![Back View](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/api_nodes/tripo/multiview_to_image/back.jpg)

### 2. Complete the Workflow Execution Step by Step

<img src="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/api_nodes/tripo/tripo_multiview_to_model_step_guide.jpg?fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=5c4abf93f36e2ea96202c7cdb92d9917" alt="ComfyUI Tripo Text to Model Step Guide" data-og-width="4474" width="4474" data-og-height="2210" height="2210" data-path="images/tutorial/api_nodes/tripo/tripo_multiview_to_model_step_guide.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/api_nodes/tripo/tripo_multiview_to_model_step_guide.jpg?w=280&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=1652f6fdc87287b6740f8ba562ffe58d 280w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/api_nodes/tripo/tripo_multiview_to_model_step_guide.jpg?w=560&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=dc97a91541029fac8049a1b9d8b54d8b 560w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/api_nodes/tripo/tripo_multiview_to_model_step_guide.jpg?w=840&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=f2ff33370148a64dcf2aa4866a3f5e19 840w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/api_nodes/tripo/tripo_multiview_to_model_step_guide.jpg?w=1100&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=e377ddfcb814d22fb8f6254725a9d806 1100w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/api_nodes/tripo/tripo_multiview_to_model_step_guide.jpg?w=1650&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=c8ac321431fb9f399eed6cad35629cd9 1650w, https://mintcdn.com/dripart/OltlUSVBSNcJsDMs/images/tutorial/api_nodes/tripo/tripo_multiview_to_model_step_guide.jpg?w=2500&fit=max&auto=format&n=OltlUSVBSNcJsDMs&q=85&s=701f03c24907864e48309ac5fa5eed11 2500w" />

You can refer to the numbers in the image to complete the basic multi-view to model workflow execution:

1. In the `Load Image` nodes, load the provided input images respectively
2. In the `Tripo: Image to Model` node, modify the corresponding parameter settings
   * model: You can select different models, currently only v1.4 model supports subsequent optimization with `Tripo: Refine Draft model`
   * style: You can set different styles
   * texture\_quality: You can set different texture qualities
3. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute model generation. After the workflow completes, the corresponding model will be automatically saved to the `ComfyUI/output/` directory
4. For other view inputs, you can refer to the step diagram and set the corresponding node mode to `Always` to enable it
5. For model download, please refer to the instructions in the text-to-model section

## Subsequent Task Processing for the Same Task

Tripo's corresponding nodes provide subsequent processing for the same task, you only need to input the corresponding `model_task_id` in the relevant nodes, and we have also provided the corresponding nodes in the relevant templates, you can also modify the corresponding node mode as needed to enable it

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/tripo/other_nodes.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=ad7966d458e1d327c98d2b21eccdd7a4" alt="Tripo Task Processing" data-og-width="904" width="904" data-og-height="696" height="696" data-path="images/tutorial/api_nodes/tripo/other_nodes.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/tripo/other_nodes.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=de8dda0c9a610962374d6d45ef9ef81b 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/tripo/other_nodes.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=aa635a07bf3de940c377e07e9bb9b323 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/tripo/other_nodes.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=853f4de5683d2adaf09fc590ee0afda1 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/tripo/other_nodes.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=b673f11e9724c3bb397d3a13b71fcc47 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/tripo/other_nodes.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=58e9b87b84f3d2edcdda3ce067e52aa1 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/api_nodes/tripo/other_nodes.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=57cc00957d0d65bf950a8ffcf014fc6a 2500w" />

<Error>
  The `Tripo: Refine Draft model` node only supports V1.4 model, other models do not support it
</Error>


# Create a Comfy account
Source: https://docs.comfy.org/account/create-account

Learn how to create a new Comfy account for ComfyUI to access all features and services.

Your Comfy account gives you access to [partner nodes (API node)](/tutorials/partner-nodes/overview) and [cloud subscriptions](https://www.comfy.org/cloud), enabling you to use premium features and services across the ComfyUI platform.

## Creating a Comfy account on Comfy Cloud

You can create a Comfy account for ComfyUI directly on Comfy Cloud:

1. Navigate to [Comfy Cloud](https://www.comfy.org)
2. Click **Sign up** or **Create account**
3. Choose one of the following login methods:
   * **Email**: Enter your email address and create a password
   * **Google**: Sign up with your Google account
   * **GitHub**: Sign up with your GitHub account
4. Complete the registration process
5. Verify your email if using email registration

## Creating a Comfy account locally

If you have ComfyUI installed locally, you can create a Comfy account through the application:

1. Open ComfyUI on your local machine
2. Navigate to **Settings** in the interface
3. Go to the **User** section (see [User settings](/interface/user) for details)
4. Click **Create account** or **Sign up**
5. Choose one of the following login methods:
   * **Email**: Enter your email address and create a password
   * **Google**: Sign up with your Google account
   * **GitHub**: Sign up with your GitHub account
6. Complete the registration process

<img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=066170b38e0b9ead026029685e00fa65" alt="User settings interface" data-og-width="3358" width="3358" data-og-height="1828" height="1828" data-path="images/interface/setting/user.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=c7f1a017e9b00c6224a440f83d121a59 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=92b830f85f4393d802f7f33bdce81634 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=e294573cc054158fb3a108d10bc67087 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=5160ea18d19dfdde201bbf41dbb1af0b 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=463f4645799b84ea5dcd4879ed3b87ca 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=e52e827f35eddf444e91e5ed4f11b331 2500w" />

## Next steps

Once your account is created and verified:

* [Log in to your account](/account/login)
* Set up your profile preferences
* Start using ComfyUI features
* Explore tutorials and documentation

## Troubleshooting

If you encounter issues during account creation:

* Ensure your email address is valid and not already registered
* Check that your password meets the minimum requirements
* Clear your browser cache and try again
* Contact [support](/support/contact-support) if problems persist


# Delete your Comfy account
Source: https://docs.comfy.org/account/delete-account

Learn how to permanently delete your Comfy account for ComfyUI and associated data.

<Warning>
  Account deletion is permanent and cannot be undone. All your data, workflows, and settings will be permanently removed.
</Warning>

## Before you delete

Before proceeding with account deletion, consider:

* **Backup your data**: For Comfy Cloud users, your assets are stored under your account, so back up any data you want to keep. If you don't use Comfy Cloud, your API usage can be found at [https://platform.comfy.org/](https://platform.comfy.org/)
* **Cancel subscriptions**: Ensure all active subscriptions are cancelled to avoid future charges
* **Download invoices**: Save copies of any payment history or invoices you may need
* **Alternative options**: Consider deactivating your account temporarily instead of permanent deletion

## Deleting your Comfy account

To permanently delete your Comfy account for ComfyUI:

1. Log in to your account
2. Navigate to **Settings** > **Account**

<Frame>
  <img src="https://mintcdn.com/dripart/i-QzV1V5IIRkC9tt/images/interface/setting/menu-user-logged.jpg?fit=max&auto=format&n=i-QzV1V5IIRkC9tt&q=85&s=114285c984763119766644a35dab4002" alt="Account settings menu showing delete account option" data-og-width="4266" width="4266" data-og-height="3230" height="3230" data-path="images/interface/setting/menu-user-logged.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/i-QzV1V5IIRkC9tt/images/interface/setting/menu-user-logged.jpg?w=280&fit=max&auto=format&n=i-QzV1V5IIRkC9tt&q=85&s=5b42fd498d1796c9962de0aa1635e957 280w, https://mintcdn.com/dripart/i-QzV1V5IIRkC9tt/images/interface/setting/menu-user-logged.jpg?w=560&fit=max&auto=format&n=i-QzV1V5IIRkC9tt&q=85&s=f8306bcbf6161cc9e1983be741f689a6 560w, https://mintcdn.com/dripart/i-QzV1V5IIRkC9tt/images/interface/setting/menu-user-logged.jpg?w=840&fit=max&auto=format&n=i-QzV1V5IIRkC9tt&q=85&s=54745182bde2201633475e6a3839aa4a 840w, https://mintcdn.com/dripart/i-QzV1V5IIRkC9tt/images/interface/setting/menu-user-logged.jpg?w=1100&fit=max&auto=format&n=i-QzV1V5IIRkC9tt&q=85&s=00fb3349320582961915354a6e167394 1100w, https://mintcdn.com/dripart/i-QzV1V5IIRkC9tt/images/interface/setting/menu-user-logged.jpg?w=1650&fit=max&auto=format&n=i-QzV1V5IIRkC9tt&q=85&s=55e09b7785d7ae4d00c9a1aedff6e8d7 1650w, https://mintcdn.com/dripart/i-QzV1V5IIRkC9tt/images/interface/setting/menu-user-logged.jpg?w=2500&fit=max&auto=format&n=i-QzV1V5IIRkC9tt&q=85&s=18259c2d6696a984912e03d4a4889102 2500w" />
</Frame>

3. Scroll to the bottom of the page
4. Click **Delete account**
5. Read the warning message carefully
6. Enter your password to confirm
7. Type "DELETE" in the confirmation field
8. Click **Permanently delete my account**

## What gets deleted

When you delete your account, the following data is permanently removed:

* Your user profile and account information
* All saved workflows and projects
* Generated images and outputs
* Custom settings and preferences
* Payment history and billing information
* API keys and access tokens


# Log in to your Comfy account
Source: https://docs.comfy.org/account/login

Access your Comfy account for ComfyUI to use all platform features and services.

Your Comfy account gives you access to [partner nodes (API node)](/tutorials/partner-nodes/overview) and [cloud subscriptions](https://www.comfy.org/cloud), enabling you to use premium features and services across the ComfyUI platform.

## Supported login methods

ComfyUI supports the following login methods:

* **Email**: Log in with your email address and password
* **Google**: Log in with your Google account
* **GitHub**: Log in with your GitHub account

## Logging in on Comfy Cloud

To access your Comfy account for ComfyUI on Comfy Cloud:

1. Navigate to [Comfy Cloud](https://www.comfy.org)
2. Click **Log in** or **Sign in**
3. Choose your login method:
   * **Email**: Enter your email address and password, then click **Log in**
   * **Google**: Click the Google login button and authenticate
   * **GitHub**: Click the GitHub login button and authenticate

## Logging in locally

If you have ComfyUI installed locally:

1. Open ComfyUI on your local machine
2. Navigate to **Settings** in the interface
3. Go to the **User** section (see [User settings](/interface/user) for details)
4. Choose your login method:
   * **Email**: Enter your email address and password
   * **Google**: Click the Google login button and authenticate
   * **GitHub**: Click the GitHub login button and authenticate
   * **API Key**: Use an API Key for non-whitelisted deployments (see below)

<img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=066170b38e0b9ead026029685e00fa65" alt="User settings interface" data-og-width="3358" width="3358" data-og-height="1828" height="1828" data-path="images/interface/setting/user.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=c7f1a017e9b00c6224a440f83d121a59 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=92b830f85f4393d802f7f33bdce81634 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=e294573cc054158fb3a108d10bc67087 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=5160ea18d19dfdde201bbf41dbb1af0b 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=463f4645799b84ea5dcd4879ed3b87ca 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=e52e827f35eddf444e91e5ed4f11b331 2500w" />

### Logging in with an API Key

Since not all ComfyUI deployments are on our domain authorization whitelist, we have provided API Key login in a recent update (2025-05-10) for logging in through non-whitelisted sites. Below are the steps for logging in with an API Key:

<video controls className="w-full aspect-video" src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/api_login.mp4?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=66faa87880cf3961efc7849e2052a48a" data-path="images/interface/setting/user/api_login.mp4" />

<Tabs>
  <Tab title="Have an API Key">
    <Steps>
      <Step title="Select Comfy API Key Login on the Login Screen">
        Select `Comfy API Key` login in the login popup
        <img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-1.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=bdb8d99a4bd52a6cc1de1b3453e8cfda" alt="Select Comfy API Key Login" data-og-width="3450" width="3450" data-og-height="1914" height="1914" data-path="images/interface/setting/user/user-login-api-1.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-1.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=84683bcf8e9b1f53885f54175cd83b87 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-1.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=ae30f89ae6d9a7b97e41f69d3ae0e9f6 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-1.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=4dce9815e1729742abd819ce400429ad 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-1.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=148e2ee529690c7985999f64841f8fcd 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-1.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=dca8a17b4e613ba65ee0d1dca67f4b28 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-1.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=0d1ab01184bd504d62531abfb88abb57 2500w" />
      </Step>

      <Step title="Enter Your API Key">
                <img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-2.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=df2f26ee4d89a8296496d2f1b4eff825" alt="Enter API Key" data-og-width="3452" width="3452" data-og-height="1914" height="1914" data-path="images/interface/setting/user/user-login-api-2.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-2.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=7d3f07cb20fc3bc7d08f26a2ddc28837 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-2.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=ace907529f3fa6c521f2b522764733a9 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-2.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=a13216f376e836aaf4cc504354a9329b 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-2.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=b4c62c53a3e625906ffa04c2d913fa5e 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-2.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=3d3732ba2ff4492e85d6c414b9e5b051 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-2.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=c680949c8ff3df421b4c6628f782f5c4 2500w" />

        1. Enter your API Key and save it
        2. If you don't have an API Key, click the `Get one here` link to go to [https://platform.comfy.org/login](https://platform.comfy.org/login) and log in to obtain it.
      </Step>

      <Step title="Login Successful">
        After a successful login, you can see the corresponding API Key login information in the settings menu
        <img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-api-logged.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=c5bac79e062f6b2225e2b10675559356" alt="Logged In" data-og-width="2348" width="2348" data-og-height="1440" height="1440" data-path="images/interface/setting/user/user-api-logged.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-api-logged.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=bcf452934a17269a672bd4a4476697de 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-api-logged.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=7dd75b61e1180970de94cd1ac5b84840 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-api-logged.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=e0dc7715fb8308e5248d9c4d2455a67b 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-api-logged.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=28006e88122590c2e9e71d18a7446e16 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-api-logged.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=77c247a7c702f54402fe4c4f387ba50f 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-api-logged.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=7a80faee6a21e915828c17a11ca99c05 2500w" />
      </Step>
    </Steps>
  </Tab>

  <Tab title="No API Key, Apply for an API Key First">
    Please refer to the following steps to apply for and obtain an API Key:

    <Steps>
      <Step title="Visit https://platform.comfy.org/login and Log In">
        Please visit [https://platform.comfy.org/login](https://platform.comfy.org/login) and log in with the corresponding account
        <img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-1.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=ca2af02d83d8ecebb12c7a445dae9cc5" alt="Visit Platform Login Page" data-og-width="2294" width="2294" data-og-height="1430" height="1430" data-path="images/interface/setting/user/user-login-api-key-1.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-1.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=b36af0e411001467177b981fd9af215e 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-1.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=df33eced88532a16dbb8a8efadd9b6a9 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-1.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=46d58d30d1c26d919157b37cb57b846b 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-1.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=b0210e40457c4bfe880a7e3a49450b6d 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-1.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=54bb3a7c7398ede7f0232311e4d67204 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-1.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=e5bde7a1835bd77cfd986f42fd1ebd36 2500w" />
      </Step>

      <Step title="Click `+ New` in API Keys to Create an API Key">
        Click `+ New` in API Keys to create an API Key
        <img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-2.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=5e5ce94f749cdda502ac72046af360ad" alt="Create API Key" data-og-width="2298" width="2298" data-og-height="1432" height="1432" data-path="images/interface/setting/user/user-login-api-key-2.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-2.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=219b758fcfae2790a6bf24deab145b7f 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-2.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=e7ce9bee1e52b80cc3239ff283212606 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-2.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=e835a0b114b854fc215cdd79b0f96861 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-2.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=b7f8df2c20785d4aa9e94abe42f9ef2e 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-2.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=c030c3ae3dea6789d6c3318a7080f5e9 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-2.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=9d04076b968a9d1ad238a2fdcbcddf4b 2500w" />
      </Step>

      <Step title="Enter API Key Name">
                <img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-3.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=851631f097e1fd2e833cbf9ba53043aa" alt="Enter API Key Name" data-og-width="2298" width="2298" data-og-height="1432" height="1432" data-path="images/interface/setting/user/user-login-api-key-3.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-3.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=5536a9c0d08dde6e4c21e9fda7f69856 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-3.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=737a284eb79fb6606b11b3f1bd18183f 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-3.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=b651fb8f64fe43e19f6bf7016b233786 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-3.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=e2885aa2885e8cff85fba874552059b9 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-3.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=6afe2029a1346e57f8cbb9805f6cad16 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-3.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=24eef5fd81be84f555f95a67cf5594eb 2500w" />

        1. (Required) Enter the API Key name,
        2. Click `Generate` to create
      </Step>

      <Step title="Save the Obtained API Key">
                <img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-4.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=ee235cef22e5ae1b5328b0ccf416b823" alt="Obtain API Key" data-og-width="2298" width="2298" data-og-height="1432" height="1432" data-path="images/interface/setting/user/user-login-api-key-4.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-4.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=9682b7089faffce97c953738aada4e20 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-4.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=fc7e0463b9e068555afe15cf9360f937 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-4.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=5532bbbd9a26d1e290b99a0cccd3c055 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-4.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=3f9b9cd0565ed6da47ed105a354021b3 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-4.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=9d9a7bbf68feb1c125b98d9dd9248737 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-4.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=9ae82323091fc06101550bfad0dfb02e 2500w" />

        <Warning>
          Since the API Key is only visible upon first creation, please save it immediately after creation. It cannot be viewed later, so please keep it safe.
          Please note that you should not share your API Key with others.Once it leaked, you can delete it and create a new one.
        </Warning>
      </Step>

      <Step title="API Key Management">
        <img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-5.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=7ad65e12a293e4ddc3ad962e986871da" alt="API Key Management" data-og-width="2298" width="2298" data-og-height="1432" height="1432" data-path="images/interface/setting/user/user-login-api-key-5.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-5.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=3a4e374cb24d27524a5aaa75638b1ed1 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-5.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=301ad0bf4ebd6ffe3381d07e9ba312f3 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-5.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=ceb156cfa577945c5ab38862e3f33f4a 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-5.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=48528443658153f9744b970444a1a18b 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-5.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=c0d147abd07a62c5f7492bddc79f1d4b 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-5.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=01c437b2f999314141d7c91cfd513c19 2500w" />
        For unused API Keys or those at risk of being leaked, you can click `Delete` to remove them to prevent unnecessary losses.
      </Step>

      <Step title="(Optional) Log Out">
        <img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-6.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=c1ec69545e8cd8cbecf7fbb8ee9660e0" alt="Log Out" data-og-width="2298" width="2298" data-og-height="1432" height="1432" data-path="images/interface/setting/user/user-login-api-key-6.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-6.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=368ce1f5adb7197da582c12e7daf87f9 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-6.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=6e034eaec88f03b5d2d5cdf1c694266d 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-6.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=23310c2a7095814a8c37e37bbff4850a 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-6.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=c4a9898f6504b583f134ea60e0d6b3c4 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-6.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=27f853c30be8e0fec719b40e309db21d 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/interface/setting/user/user-login-api-key-6.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=7a21181502f55039615c1fc5a699e0fc 2500w" />
        If you have obtained an API Key and are logged in on a public device, please log out promptly.
      </Step>
    </Steps>
  </Tab>
</Tabs>

## Forgot your password

If you can't remember your password (for email login):

1. Click the **Forgot password?** link on the login page
2. Enter your registered email address
3. Check your email for password reset instructions
4. Click the reset link in the email
5. Create a new password
6. Log in with your new password

## Troubleshooting login issues

If you're having trouble logging in:

* Verify your email address is correct
* Check that Caps Lock is not enabled when entering your password
* Clear your browser cookies and cache
* Try using a different browser or incognito mode
* Ensure your account email has been verified
* Contact [support](/support/contact-support) if issues persist

## Security tips

To keep your account secure:

* Use a strong, unique password
* Don't share your login credentials
* Log out when using shared devices
* Regularly update your password


# Get proxyvidutasks creations
Source: https://docs.comfy.org/api-reference/api-nodes/get-proxyvidutasks-creations

https://api.comfy.org/openapi get /proxy/vidu/tasks/{id}/creations



# Post proxyviduimg2video
Source: https://docs.comfy.org/api-reference/api-nodes/post-proxyviduimg2video

https://api.comfy.org/openapi post /proxy/vidu/img2video



# Post proxyvidureference2video
Source: https://docs.comfy.org/api-reference/api-nodes/post-proxyvidureference2video

https://api.comfy.org/openapi post /proxy/vidu/reference2video



# Post proxyvidustart end2video
Source: https://docs.comfy.org/api-reference/api-nodes/post-proxyvidustart-end2video

https://api.comfy.org/openapi post /proxy/vidu/start-end2video



# Post proxyvidutext2video
Source: https://docs.comfy.org/api-reference/api-nodes/post-proxyvidutext2video

https://api.comfy.org/openapi post /proxy/vidu/text2video



# Add review to a specific version of a node
Source: https://docs.comfy.org/api-reference/registry/add-review-to-a-specific-version-of-a-node

https://api.comfy.org/openapi post /nodes/{nodeId}/reviews



# Claim nodeId into publisherId for the authenticated publisher
Source: https://docs.comfy.org/api-reference/registry/claim-nodeid-into-publisherid-for-the-authenticated-publisher

https://api.comfy.org/openapi post /publishers/{publisherId}/nodes/{nodeId}/claim-my-node
This endpoint allows a publisher to claim an unclaimed node that they own the repo, which is identified by the nodeId. The unclaimed node's repository must be owned by the authenticated user.




# Create a new custom node
Source: https://docs.comfy.org/api-reference/registry/create-a-new-custom-node

https://api.comfy.org/openapi post /publishers/{publisherId}/nodes



# Create a new personal access token
Source: https://docs.comfy.org/api-reference/registry/create-a-new-personal-access-token

https://api.comfy.org/openapi post /publishers/{publisherId}/tokens



# Create a new publisher
Source: https://docs.comfy.org/api-reference/registry/create-a-new-publisher

https://api.comfy.org/openapi post /publishers



# create comfy-nodes for certain node
Source: https://docs.comfy.org/api-reference/registry/create-comfy-nodes-for-certain-node

https://api.comfy.org/openapi post /nodes/{nodeId}/versions/{version}/comfy-nodes



# Create Node Translations
Source: https://docs.comfy.org/api-reference/registry/create-node-translations

https://api.comfy.org/openapi post /nodes/{nodeId}/translations



# Delete a publisher
Source: https://docs.comfy.org/api-reference/registry/delete-a-publisher

https://api.comfy.org/openapi delete /publishers/{publisherId}



# Delete a specific node
Source: https://docs.comfy.org/api-reference/registry/delete-a-specific-node

https://api.comfy.org/openapi delete /publishers/{publisherId}/nodes/{nodeId}



# Delete a specific personal access token
Source: https://docs.comfy.org/api-reference/registry/delete-a-specific-personal-access-token

https://api.comfy.org/openapi delete /publishers/{publisherId}/tokens/{tokenId}



# Get information about the calling user.
Source: https://docs.comfy.org/api-reference/registry/get-information-about-the-calling-user

https://api.comfy.org/openapi get /users



# get specify comfy-node based on its id
Source: https://docs.comfy.org/api-reference/registry/get-specify-comfy-node-based-on-its-id

https://api.comfy.org/openapi get /nodes/{nodeId}/versions/{version}/comfy-nodes/{comfyNodeName}



# list all comfy-nodes
Source: https://docs.comfy.org/api-reference/registry/list-all-comfy-nodes

https://api.comfy.org/openapi get /comfy-nodes



# List all node versions given some filters.
Source: https://docs.comfy.org/api-reference/registry/list-all-node-versions-given-some-filters

https://api.comfy.org/openapi get /versions



# List all versions of a node
Source: https://docs.comfy.org/api-reference/registry/list-all-versions-of-a-node

https://api.comfy.org/openapi get /nodes/{nodeId}/versions



# list comfy-nodes for node version
Source: https://docs.comfy.org/api-reference/registry/list-comfy-nodes-for-node-version

https://api.comfy.org/openapi get /nodes/{nodeId}/versions/{version}/comfy-nodes



# Publish a new version of a node
Source: https://docs.comfy.org/api-reference/registry/publish-a-new-version-of-a-node

https://api.comfy.org/openapi post /publishers/{publisherId}/nodes/{nodeId}/versions



# Retrieve a node by ComfyUI node name
Source: https://docs.comfy.org/api-reference/registry/retrieve-a-node-by-comfyui-node-name

https://api.comfy.org/openapi get /comfy-nodes/{comfyNodeName}/node
Returns the node that contains a ComfyUI node with the specified name



# Retrieve a publisher by ID
Source: https://docs.comfy.org/api-reference/registry/retrieve-a-publisher-by-id

https://api.comfy.org/openapi get /publishers/{publisherId}



# Retrieve a specific node by ID
Source: https://docs.comfy.org/api-reference/registry/retrieve-a-specific-node-by-id

https://api.comfy.org/openapi get /nodes/{nodeId}
Returns the details of a specific node.



# Retrieve a specific version of a node
Source: https://docs.comfy.org/api-reference/registry/retrieve-a-specific-version-of-a-node

https://api.comfy.org/openapi get /nodes/{nodeId}/versions/{versionId}



# Retrieve all nodes
Source: https://docs.comfy.org/api-reference/registry/retrieve-all-nodes

https://api.comfy.org/openapi get /publishers/{publisherId}/nodes



# Retrieve all nodes
Source: https://docs.comfy.org/api-reference/registry/retrieve-all-nodes-1

https://api.comfy.org/openapi get /publishers/{publisherId}/nodes/v2



# Retrieve all publishers
Source: https://docs.comfy.org/api-reference/registry/retrieve-all-publishers

https://api.comfy.org/openapi get /publishers



# Retrieve all publishers for a given user
Source: https://docs.comfy.org/api-reference/registry/retrieve-all-publishers-for-a-given-user

https://api.comfy.org/openapi get /users/publishers/



# Retrieve multiple node versions in a single request
Source: https://docs.comfy.org/api-reference/registry/retrieve-multiple-node-versions-in-a-single-request

https://api.comfy.org/openapi post /bulk/nodes/versions



# Retrieve permissions the user has for a given publisher
Source: https://docs.comfy.org/api-reference/registry/retrieve-permissions-the-user-has-for-a-given-publisher

https://api.comfy.org/openapi get /publishers/{publisherId}/nodes/{nodeId}/permissions



# Retrieve permissions the user has for a given publisher
Source: https://docs.comfy.org/api-reference/registry/retrieve-permissions-the-user-has-for-a-given-publisher-1

https://api.comfy.org/openapi get /publishers/{publisherId}/permissions



# Retrieves a list of nodes
Source: https://docs.comfy.org/api-reference/registry/retrieves-a-list-of-nodes

https://api.comfy.org/openapi get /nodes
Returns a paginated list of nodes across all publishers.



# Retrieves a list of nodes
Source: https://docs.comfy.org/api-reference/registry/retrieves-a-list-of-nodes-1

https://api.comfy.org/openapi get /nodes/search
Returns a paginated list of nodes across all publishers.



# Returns a node version to be installed.
Source: https://docs.comfy.org/api-reference/registry/returns-a-node-version-to-be-installed

https://api.comfy.org/openapi get /nodes/{nodeId}/install
Retrieves the node data for installation, either the latest or a specific version.



# Unpublish (delete) a specific version of a node
Source: https://docs.comfy.org/api-reference/registry/unpublish-delete-a-specific-version-of-a-node

https://api.comfy.org/openapi delete /publishers/{publisherId}/nodes/{nodeId}/versions/{versionId}



# Update a publisher
Source: https://docs.comfy.org/api-reference/registry/update-a-publisher

https://api.comfy.org/openapi put /publishers/{publisherId}



# Update a specific comfy-node
Source: https://docs.comfy.org/api-reference/registry/update-a-specific-comfy-node

https://api.comfy.org/openapi put /nodes/{nodeId}/versions/{version}/comfy-nodes/{comfyNodeName}



# Update a specific node
Source: https://docs.comfy.org/api-reference/registry/update-a-specific-node

https://api.comfy.org/openapi put /publishers/{publisherId}/nodes/{nodeId}



# Update changelog and deprecation status of a node version
Source: https://docs.comfy.org/api-reference/registry/update-changelog-and-deprecation-status-of-a-node-version

https://api.comfy.org/openapi put /publishers/{publisherId}/nodes/{nodeId}/versions/{versionId}
Update only the changelog and deprecated status of a specific version of a node.



# Validate if a publisher username is available
Source: https://docs.comfy.org/api-reference/registry/validate-if-a-publisher-username-is-available

https://api.comfy.org/openapi get /publishers/validate
Checks if the publisher username is already taken.



# Get release notes
Source: https://docs.comfy.org/api-reference/releases/get-release-notes

https://api.comfy.org/openapi get /releases
Fetch release notes from Strapi with caching



# Contributing
Source: https://docs.comfy.org/community/contributing



### How to contribute

We welcome contributions of all kinds. Check out the various repositories we support on our [Github organization](https://github.com/Comfy-Org).

You can also contribute by sharing workflows or developing [custom nodes](/custom-nodes/overview).


# Community links
Source: https://docs.comfy.org/community/links

Connect with the ComfyUI community through various platforms

Join the ComfyUI community and get help, share your work, and stay updated with the latest developments.

<CardGroup cols={2}>
  <Card title="Discord" icon="discord" href="https://discord.gg/comfyorg">
    Join our Discord community
  </Card>

  <Card title="Support Portal" icon="life-ring" href="https://support.comfy.org/">
    Get help and support
  </Card>

  <Card title="Forum" icon="comments" href="https://forum.comfy.org/">
    Join community discussions
  </Card>

  <Card title="Matrix" icon="hashtag" href="https://app.element.io/#/room/%23comfyui_space%3Amatrix.org">
    Chat on Matrix
  </Card>

  <Card title="GitHub" icon="github" href="https://github.com/comfyanonymous/ComfyUI/">
    View source code
  </Card>

  <Card title="YouTube" icon="youtube" href="https://www.youtube.com/@comfyorg">
    Watch tutorials
  </Card>

  <Card title="X (Twitter)" icon="x-twitter" href="https://x.com/ComfyUI">
    Follow us on X
  </Card>

  <Card title="LinkedIn" icon="linkedin" href="https://www.linkedin.com/company/comfyui/">
    Connect on LinkedIn
  </Card>

  <Card title="Reddit" icon="reddit" href="https://www.reddit.com/r/comfyui/">
    Join our subreddit
  </Card>
</CardGroup>


# Add node docs for your ComfyUI custom node
Source: https://docs.comfy.org/custom-nodes/help_page

How to create rich documentation for your custom nodes

## Node Documentation with Markdown

Custom nodes can include rich markdown documentation that will be displayed in the UI instead of the generic node description. This provides users with detailed information about your node's functionality, parameters, and usage examples.

If you have already added tooltips for each parameter in the node definition, this basic information can be directly accessed via the node documentation panel.
No additional node documentation needs to be added, and you can refer to the related implementation of [ContextWindowsManualNode](https://github.com/comfyanonymous/ComfyUI/blob/master/comfy_extras/nodes_context_windows.py#L7)

## Setup

To add documentation for your custom nodes or multi-language support documentation:

1. Create a `docs` folder inside your `WEB_DIRECTORY`
2. Add markdown files named after your nodes (the names of your nodes are the dictionary keys in the `NODE_CLASS_MAPPINGS` dictionary used to register the nodes):
   * `WEB_DIRECTORY/docs/NodeName.md` - Default documentation
   * `WEB_DIRECTORY/docs/NodeName/en.md` - English documentation
   * `WEB_DIRECTORY/docs/NodeName/zh.md` - Chinese documentation
   * Add other locales as needed (e.g., `fr.md`, `de.md`, etc.)

The system will automatically load the appropriate documentation based on the user's locale, falling back to `NodeName.md` if a localized version is not available.

## Supported Markdown Features

* Standard markdown syntax (headings, lists, code blocks, etc.)
* Images using markdown syntax: `![alt text](image.png)`
* HTML media elements with specific attributes:
  * `<video>` and `<source>` tags
  * Allowed attributes: `controls`, `autoplay`, `loop`, `muted`, `preload`, `poster`

## Example Structure

```
my-custom-node/
 __init__.py
 web/              # WEB_DIRECTORY
    js/
       my-node.js
    docs/
        MyNode.md           # Fallback documentation
        MyNode/
            en.md           # English version
            zh.md           # Chinese version
```

## Example Markdown File

```markdown  theme={null}
# My Custom Node

This node processes images using advanced algorithms.

## Parameters

- **image**: Input image to process
- **strength**: Processing strength (0.0 - 1.0)

## Usage

![example usage](example.png)

<video controls loop muted>
  <source src="demo.mp4" type="video/mp4">
</video>
```


# ComfyUI Custom Nodes i18n Support
Source: https://docs.comfy.org/custom-nodes/i18n

Learn how to add multi-language support for ComfyUI custom nodes

If you want to add support for multiple languages, you can refer to this document to learn how to implement multi-language support.

Currently, ComfyUI supports the following languages:

* Englishen
* Chinese (Simplified)zh
* Chinese (Traditional)zh-TW
* Frenchfr
* Koreanko
* Russianru
* Spanishes
* Japaneseja
* Arabicar

Custom node i18n demo: [comfyui-wiki/ComfyUI-i18n-demo](https://github.com/comfyui-wiki/ComfyUI-i18n-demo)

## Directory Structure

Create a `locales` folder under your custom node, which supports multiple types of translation files:

```bash  theme={null}
your_custom_node/
 __init__.py
 your_node.py
 locales/                   # i18n support folder
     en/                    # English translations (recommended as the base)
        main.json          # General English translation content
        nodeDefs.json      # English node definition translations
        settings.json      # Optional: settings interface translations
        commands.json      # Optional: command translations
     zh/                    # Chinese translation files
        nodeDefs.json      # Chinese node definition translations
        main.json          # General Chinese translation content
        settings.json      # Chinese settings interface translations
        commands.json      # Chinese command translations
    ...

```

## nodeDefs.json - Node Definition Translation

For example, here is a Python definition example of an [i18n-demo](https://github.com/comfyui-wiki/ComfyUI-i18n-demo) node:

```python  theme={null}
class I18nTextProcessor:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "text": ("STRING", {
                    "multiline": True,
                    "default": "Hello World!",
                    "tooltip": "The original text content to be processed"
                }),
                "operation": (["uppercase", "lowercase", "reverse", "add_prefix"], {
                    "default": "uppercase",
                    "tooltip": "The text processing operation to be executed"
                }),
                "count": ("INT", {
                    "default": 1,
                    "min": 1,
                    "max": 10,
                    "step": 1,
                    "tooltip": "The number of times to repeat the operation"
                }),
            },
            "optional": {
                "prefix": ("STRING", {
                    "default": "[I18N] ",
                    "multiline": False,
                    "tooltip": "The prefix to add to the text"
                }),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("processed_text",)
    FUNCTION = "process_text"
    CATEGORY = "I18n Demo"
    DESCRIPTION = "A simple i18n demo node that demonstrates text processing with internationalization support"

    def process_text(self, text, operation, count, prefix=""):
        try:
            result = text
            
            for _ in range(count):
                if operation == "uppercase":
                    result = result.upper()
                elif operation == "lowercase":
                    result = result.lower()
                elif operation == "reverse":
                    result = result[::-1]
                elif operation == "add_prefix":
                    result = prefix + result
            
            return (result,)
        except Exception as e:
            print(f"I18nTextProcessor error: {e}")
            return (f"Error: {str(e)}",)
```

Then the corresponding localized support nodeDefs.json file content should include:

```json  theme={null}
{
  "I18nTextProcessor": {
    "display_name": "I18n Text Processor",
    "description": "A simple i18n demo node that demonstrates text processing with internationalization support",
    "inputs": {
      "text": {
        "name": "Text Input",
        "tooltip": "The original text content to be processed"
      },
      "operation": {
        "name": "Operation Type",
        "tooltip": "The text processing operation to be executed",
        "options": {
          "uppercase": "To Uppercase",
          "lowercase": "To Lowercase",
          "reverse": "Reverse Text",
          "add_prefix": "Add Prefix"
        }
      },
      "count": {
        "name": "Repeat Count",
        "tooltip": "The number of times to repeat the operation"
      },
      "prefix": {
        "name": "Prefix Text",
        "tooltip": "The prefix to add to the text"
      }
    },
    "outputs": {
      "0": {
        "name": "Processed Text",
        "tooltip": "The final processed text result"
      }
    }
  }
}
```

For the node output part, the corresponding output index is used instead of the output name, for example, the first output should be `0`, the second output should be `1`, and so on.

## Menu Settings

For example, in the i18n-demo custom node, we registered the following two menu settings:

```javascript  theme={null}
app.registerExtension({
    name: "I18nDemo",
    settings: [
        {
            id: "I18nDemo.EnableDebugMode",
            category: ["I18nDemo","DebugMode"], // This matches the settingsCategories key in main.json
            name: "Enable Debug Mode", // Will be overridden by translation
            tooltip: "Show debug information in console for i18n demo nodes", // Will be overridden by translation
            type: "boolean",
            defaultValue: false,
            experimental: true,
            onChange: (value) => {
                console.log("I18n Demo:", value ? "Debug mode enabled" : "Debug mode disabled");
            }
        },
        {
            id: "I18nDemo.DefaultTextOperation",
            category: ["I18nDemo","DefaultTextOperation"], // This matches the settingsCategories key in main.json
            name: "Default Text Operation", // Will be overridden by translation
            tooltip: "Default operation for text processor node", // Will be overridden by translation
            type: "combo",
            options: ["uppercase", "lowercase", "reverse", "add_prefix"],
            defaultValue: "uppercase",
            experimental: true
        }
    ],
    })
```

If you need to add corresponding internationalization support, for the menu category, you need to add it in the `main.json` file:

```json  theme={null}
{
  "settingsCategories": {
    "I18nDemo": "I18n Demo",
    "DebugMode": "Debug Mode",
    "DefaultTextOperation": "Default Text Operation"
  }
}
```

For the translation of the corresponding setting items, it is recommended to update them separately in the `settings.json` file, for example:

```json  theme={null}
{
  "I18nDemo_EnableDebugMode": {
    "name": "Enable Debug Mode",
    "tooltip": "Show debug information in console for i18n demo nodes"
  },
  "I18nDemo_DefaultTextOperation": {
    "name": "Default Text Operation",
    "tooltip": "Default operation for text processor node",
    "options": {
      "uppercase": "Uppercase",
      "lowercase": "Lowercase",
      "reverse": "Reverse",
      "add_prefix": "Add Prefix"
    }
  }
}
```

Note that the name of the corresponding translation key should replace the `.` in the original id with `_`, for example:

```
"I18nDemo.EnableDebugMode" -> "I18nDemo_EnableDebugMode"
```

## Custom Frontend Component Localization Support

\[To be updated]


# Context Menu Migration Guide
Source: https://docs.comfy.org/custom-nodes/js/context-menu-migration



This guide helps you migrate from the deprecated monkey-patching approach to the new context menu extension API.

The old approach of monkey-patching `LGraphCanvas.prototype.getCanvasMenuOptions` and `nodeType.prototype.getExtraMenuOptions` is deprecated:

<Tip>If you see deprecation warnings in your browser console, your extension is using the old API and should be migrated.</Tip>

## Migrating Canvas Menus

### Old Approach (Deprecated)

The old approach modified the prototype during extension setup:

```javascript  theme={null}
import { app } from "../../scripts/app.js"

app.registerExtension({
  name: "MyExtension",
  async setup() {
    //  OLD: Monkey-patching the prototype
    const original = LGraphCanvas.prototype.getCanvasMenuOptions
    LGraphCanvas.prototype.getCanvasMenuOptions = function() {
      const options = original.apply(this, arguments)

      options.push(null) // separator
      options.push({
        content: "My Custom Action",
        callback: () => {
          console.log("Action triggered")
        }
      })

      return options
    }
  }
})
```

### New Approach (Recommended)

The new approach uses a dedicated extension hook:

```javascript  theme={null}
import { app } from "../../scripts/app.js"

app.registerExtension({
  name: "MyExtension",
  //  NEW: Use the getCanvasMenuItems hook
  getCanvasMenuItems(canvas) {
    return [
      null, // separator
      {
        content: "My Custom Action",
        callback: () => {
          console.log("Action triggered")
        }
      }
    ]
  }
})
```

### Key Differences

| Old Approach               | New Approach                     |
| -------------------------- | -------------------------------- |
| Modified in `setup()`      | Uses `getCanvasMenuItems()` hook |
| Wraps existing function    | Returns menu items directly      |
| Modifies `options` array   | Returns new array                |
| Canvas accessed via `this` | Canvas passed as parameter       |

## Migrating Node Menus

### Old Approach (Deprecated)

The old approach modified the node type prototype:

```javascript  theme={null}
import { app } from "../../scripts/app.js"

app.registerExtension({
  name: "MyExtension",
  async beforeRegisterNodeDef(nodeType, nodeData, app) {
    if (nodeType.comfyClass === "KSampler") {
      //  OLD: Monkey-patching the node prototype
      const original = nodeType.prototype.getExtraMenuOptions
      nodeType.prototype.getExtraMenuOptions = function(canvas, options) {
        original?.apply(this, arguments)

        options.push({
          content: "Randomize Seed",
          callback: () => {
            const seedWidget = this.widgets.find(w => w.name === "seed")
            if (seedWidget) {
              seedWidget.value = Math.floor(Math.random() * 1000000)
            }
          }
        })
      }
    }
  }
})
```

### New Approach (Recommended)

The new approach uses a dedicated extension hook:

```javascript  theme={null}
import { app } from "../../scripts/app.js"

app.registerExtension({
  name: "MyExtension",
  //  NEW: Use the getNodeMenuItems hook
  getNodeMenuItems(node) {
    const items = []

    // Add items only for specific node types
    if (node.comfyClass === "KSampler") {
      items.push({
        content: "Randomize Seed",
        callback: () => {
          const seedWidget = node.widgets.find(w => w.name === "seed")
          if (seedWidget) {
            seedWidget.value = Math.floor(Math.random() * 1000000)
          }
        }
      })
    }

    return items
  }
})
```

### Key Differences

| Old Approach                          | New Approach                         |
| ------------------------------------- | ------------------------------------ |
| Modified in `beforeRegisterNodeDef()` | Uses `getNodeMenuItems()` hook       |
| Type-specific via `if` check          | Type-specific via `if` check in hook |
| Modifies `options` array              | Returns new array                    |
| Node accessed via `this`              | Node passed as parameter             |

## Common Patterns

### Conditional Menu Items

Both approaches support conditional items, but the new API is cleaner:

```javascript  theme={null}
//  NEW: Clean conditional logic
getCanvasMenuItems(canvas) {
  const items = []

  if (canvas.selectedItems.size > 0) {
    items.push({
      content: `Process ${canvas.selectedItems.size} Selected Nodes`,
      callback: () => {
        // Process nodes
      }
    })
  }

  return items
}
```

### Adding Separators

Separators are added the same way in both approaches:

```javascript  theme={null}
getCanvasMenuItems(canvas) {
  return [
    null, // Separator (horizontal line)
    {
      content: "My Action",
      callback: () => {}
    }
  ]
}
```

### Creating Submenus

The recommended way to create submenus is using the declarative `submenu` property:

```javascript  theme={null}
getNodeMenuItems(node) {
  return [
    {
      content: "Advanced Options",
      submenu: {
        options: [
          { content: "Option 1", callback: () => {} },
          { content: "Option 2", callback: () => {} }
        ]
      }
    }
  ]
}
```

This declarative approach is cleaner and matches the patterns used throughout the ComfyUI codebase.

<Tip>While a callback-based approach with `has_submenu: true` and `new LiteGraph.ContextMenu()` is also supported, the declarative `submenu` property is preferred for better maintainability.</Tip>

### Accessing State

```javascript  theme={null}
//  NEW: State access is clearer
getCanvasMenuItems(canvas) {
  // Access canvas properties
  const selectedCount = canvas.selectedItems.size
  const graphMousePos = canvas.graph_mouse

  return [/* menu items */]
}

getNodeMenuItems(node) {
  // Access node properties
  const nodeType = node.comfyClass
  const isDisabled = node.mode === 2
  const widgets = node.widgets

  return [/* menu items */]
}
```

## Troubleshooting

### How to Identify Old API Usage

Look for these patterns in your code:

```javascript  theme={null}
//  Signs of old API:
LGraphCanvas.prototype.getCanvasMenuOptions = function() { /* ... */ }
nodeType.prototype.getExtraMenuOptions = function() { /* ... */ }
```

### Understanding Deprecation Warnings

If you see this warning in the console:

```
[DEPRECATED] Monkey-patching getCanvasMenuOptions is deprecated. (Extension: "MyExtension")
Please use the new context menu API instead.
See: https://docs.comfy.org/custom-nodes/js/context-menu-migration
```

Your extension is using the old approach and should be migrated.

### Verifying Migration Success

After migration:

1. Remove all prototype modifications from `setup()` and `beforeRegisterNodeDef()`
2. Add `getCanvasMenuItems()` and/or `getNodeMenuItems()` hooks
3. Test that your menu items still appear correctly
4. Verify no deprecation warnings appear in the console

### Complete Migration Example

**Before:**

```javascript  theme={null}
app.registerExtension({
  name: "MyExtension",
  async setup() {
    const original = LGraphCanvas.prototype.getCanvasMenuOptions
    LGraphCanvas.prototype.getCanvasMenuOptions = function() {
      const options = original.apply(this, arguments)
      options.push({ content: "Action", callback: () => {} })
      return options
    }
  },
  async beforeRegisterNodeDef(nodeType) {
    if (nodeType.comfyClass === "KSampler") {
      const original = nodeType.prototype.getExtraMenuOptions
      nodeType.prototype.getExtraMenuOptions = function(_, options) {
        original?.apply(this, arguments)
        options.push({ content: "Node Action", callback: () => {} })
      }
    }
  }
})
```

**After:**

```javascript  theme={null}
app.registerExtension({
  name: "MyExtension",
  getCanvasMenuItems(canvas) {
    return [
      { content: "Action", callback: () => {} }
    ]
  },
  getNodeMenuItems(node) {
    if (node.comfyClass === "KSampler") {
      return [
        { content: "Node Action", callback: () => {} }
      ]
    }
    return []
  }
})
```

## Additional Resources

* [Annotated Examples](./javascript_examples) - More examples using the new API
* [Extension Hooks](./javascript_hooks) - Complete list of available extension hooks
* [Commands and Keybindings](./javascript_commands_keybindings) - Add keyboard shortcuts to your menu actions


# Bottom Panel Tabs
Source: https://docs.comfy.org/custom-nodes/js/javascript_bottom_panel_tabs



The Bottom Panel Tabs API allows extensions to add custom tabs to the bottom panel of the ComfyUI interface. This is useful for adding features like logs, debugging tools, or custom panels.

## Basic Usage

```javascript  theme={null}
app.registerExtension({
  name: "MyExtension",
  bottomPanelTabs: [
    {
      id: "customTab",
      title: "Custom Tab",
      type: "custom",
      render: (el) => {
        el.innerHTML = '<div>This is my custom tab content</div>';
      }
    }
  ]
});
```

## Tab Configuration

Each tab requires an `id`, `title`, and `type`, along with a render function:

```javascript  theme={null}
{
  id: string,              // Unique identifier for the tab
  title: string,           // Display title shown on the tab
  type: string,            // Tab type (usually "custom")
  icon?: string,           // Icon class (optional)
  render: (element) => void // Function that populates the tab content
}
```

The `render` function receives a DOM element where you should insert your tab's content.

## Interactive Elements

You can add interactive elements like buttons:

```javascript  theme={null}
app.registerExtension({
  name: "InteractiveTabExample",
  bottomPanelTabs: [
    {
      id: "controlsTab",
      title: "Controls",
      type: "custom",
      render: (el) => {
        el.innerHTML = `
          <div style="padding: 10px;">
            <button id="runBtn">Run Workflow</button>
          </div>
        `;
        
        // Add event listeners
        el.querySelector('#runBtn').addEventListener('click', () => {
          app.queuePrompt();
        });
      }
    }
  ]
});
```

## Using React Components

You can mount React components in bottom panel tabs:

```javascript  theme={null}
// Import React dependencies in your extension
import React from "react";
import ReactDOM from "react-dom/client";

// Simple React component
function TabContent() {
  const [count, setCount] = React.useState(0);
  
  return (
    <div style={{ padding: "10px" }}>
      <h3>React Component</h3>
      <p>Count: {count}</p>
      <button onClick={() => setCount(count + 1)}>Increment</button>
    </div>
  );
}

// Register the extension with React content
app.registerExtension({
  name: "ReactTabExample",
  bottomPanelTabs: [
    {
      id: "reactTab",
      title: "React Tab",
      type: "custom",
      render: (el) => {
        const container = document.createElement("div");
        container.id = "react-tab-container";
        el.appendChild(container);
        
        // Mount React component
        ReactDOM.createRoot(container).render(
          <React.StrictMode>
            <TabContent />
          </React.StrictMode>
        );
      }
    }
  ]
});
```

## Standalone Registration

You can also register tabs outside of `registerExtension`:

```javascript  theme={null}
app.extensionManager.registerBottomPanelTab({
  id: "standAloneTab",
  title: "Stand-Alone Tab",
  type: "custom",
  render: (el) => {
    el.innerHTML = '<div>This tab was registered independently</div>';
  }
});
```


# Commands and Keybindings
Source: https://docs.comfy.org/custom-nodes/js/javascript_commands_keybindings



The Commands and Keybindings API allows extensions to register custom commands and associate them with keyboard shortcuts. This enables users to quickly trigger actions without using the mouse.

## Basic Usage

```javascript  theme={null}
app.registerExtension({
  name: "MyExtension",
  // Register commands
  commands: [
    {
      id: "myCommand",
      label: "My Command",
      function: () => {
        console.log("Command executed!");
      }
    }
  ],
  // Associate keybindings with commands
  keybindings: [
    {
      combo: { key: "k", ctrl: true },
      commandId: "myCommand"
    }
  ]
});
```

## Command Configuration

Each command requires an `id`, `label`, and `function`:

```javascript  theme={null}
{
  id: string,              // Unique identifier for the command
  label: string,           // Display name for the command
  function: () => void     // Function to execute when command is triggered
}
```

## Keybinding Configuration

Each keybinding requires a `combo` and `commandId`:

```javascript  theme={null}
{
  combo: {                 // Key combination
    key: string,           // The main key (single character or special key)
    ctrl?: boolean,        // Require Ctrl key (optional)
    shift?: boolean,       // Require Shift key (optional)
    alt?: boolean,         // Require Alt key (optional)
    meta?: boolean         // Require Meta/Command key (optional)
  },
  commandId: string        // ID of the command to trigger
}
```

### Special Keys

For non-character keys, use one of these values:

* Arrow keys: `"ArrowUp"`, `"ArrowDown"`, `"ArrowLeft"`, `"ArrowRight"`
* Function keys: `"F1"` through `"F12"`
* Other special keys: `"Escape"`, `"Tab"`, `"Enter"`, `"Backspace"`, `"Delete"`, `"Home"`, `"End"`, `"PageUp"`, `"PageDown"`

## Command Examples

```javascript  theme={null}
app.registerExtension({
  name: "CommandExamples",
  commands: [
    {
      id: "runWorkflow",
      label: "Run Workflow",
      function: () => {
        app.queuePrompt();
      }
    },
    {
      id: "clearWorkflow",
      label: "Clear Workflow",
      function: () => {
        if (confirm("Clear the workflow?")) {
          app.graph.clear();
        }
      }
    },
    {
      id: "saveWorkflow",
      label: "Save Workflow",
      function: () => {
        app.graphToPrompt().then(workflow => {
          const blob = new Blob([JSON.stringify(workflow)], {type: "application/json"});
          const url = URL.createObjectURL(blob);
          const a = document.createElement("a");
          a.href = url;
          a.download = "workflow.json";
          a.click();
          URL.revokeObjectURL(url);
        });
      }
    }
  ]
});
```

## Keybinding Examples

```javascript  theme={null}
app.registerExtension({
  name: "KeybindingExamples",
  commands: [
    /* Commands defined above */
  ],
  keybindings: [
    // Ctrl+R to run workflow
    {
      combo: { key: "r", ctrl: true },
      commandId: "runWorkflow"
    },
    // Ctrl+Shift+C to clear workflow
    {
      combo: { key: "c", ctrl: true, shift: true },
      commandId: "clearWorkflow"
    },
    // Ctrl+S to save workflow
    {
      combo: { key: "s", ctrl: true },
      commandId: "saveWorkflow"
    },
    // F5 to run workflow (alternative)
    {
      combo: { key: "F5" },
      commandId: "runWorkflow"
    }
  ]
});
```

## Notes and Limitations

* Keybindings defined in the ComfyUI core cannot be overwritten by extensions. Check the core keybindings in these source files:
  * [Core Commands](https://github.com/Comfy-Org/ComfyUI_frontend/blob/e76e9ec61a068fd2d89797762f08ee551e6d84a0/src/composables/useCoreCommands.ts)
  * [Core Menu Commands](https://github.com/Comfy-Org/ComfyUI_frontend/blob/e76e9ec61a068fd2d89797762f08ee551e6d84a0/src/constants/coreMenuCommands.ts)
  * [Core Keybindings](https://github.com/Comfy-Org/ComfyUI_frontend/blob/e76e9ec61a068fd2d89797762f08ee551e6d84a0/src/constants/coreKeybindings.ts)
  * [Reserved Key Combos](https://github.com/Comfy-Org/ComfyUI_frontend/blob/e76e9ec61a068fd2d89797762f08ee551e6d84a0/src/constants/reservedKeyCombos.ts)

* Some key combinations are reserved by the browser (like Ctrl+F for search) and cannot be overridden

* If multiple extensions register the same keybinding, the behavior is undefined


# Annotated Examples
Source: https://docs.comfy.org/custom-nodes/js/javascript_examples



A growing collection of fragments of example code...

## Right click menus

### Background menu

The main background menu (right-click on the canvas) is generated by a call to
`LGraphCanvas.getCanvasMenuOptions`. The standard way of editing this is to implement the `getCanvasMenuItems` method on your extension:

```javascript  theme={null}
app.registerExtension({
  name: "MyExtension",
  getCanvasMenuItems(canvas) {
    const items = []
    items.push(null) // inserts a divider
    items.push({
      content: "The text for the menu",
      callback: async () => {
        // do whatever
      }
    })
    return items
  }
});
```

### Node menu

When you right click on a node, the menu is similarly generated by `node.getExtraMenuOptions`.
The standard way is to implement the `getNodeMenuItems` method on your extension:

```javascript  theme={null}
app.registerExtension({
  name: "MyExtension",
  getNodeMenuItems(node) {
    const items = []

    // You can filter by node type if needed
    if (node.comfyClass === "MyNodeClass") {
      items.push({
        content: "Do something fun",
        callback: async () => {
          // fun thing
        }
      })
    }

    return items
  }
});
```

### Submenus

If you want a submenu, use the `submenu` property with an `options` array:

```javascript  theme={null}
app.registerExtension({
  name: "MyExtension",
  getCanvasMenuItems(canvas) {
    const items = []
    items.push({
      content: "Menu with options",
      submenu: {
        options: [
          {
            content: "option 1",
            callback: (v) => {
              // do something with v
            }
          },
          {
            content: "option 2",
            callback: (v) => {
              // do something with v
            }
          },
          {
            content: "option 3",
            callback: (v) => {
              // do something with v
            }
          }
        ]
      }
    })
    return items
  }
});
```

## Capture UI events

This works just like you'd expect - find the UI element in the DOM and
add an eventListener. `setup()` is a good place to do this, since the page
has fully loaded. For instance, to detect a click on the 'Queue' button:

```Javascript  theme={null}
function queue_button_pressed() { console.log("Queue button was pressed!") }
document.getElementById("queue-button").addEventListener("click", queue_button_pressed);
```

## Detect when a workflow starts

This is one of many `api` events:

```javascript  theme={null}
import { api } from "../../scripts/api.js";
/* in setup() */
    function on_execution_start() { 
        /* do whatever */
    }
    api.addEventListener("execution_start", on_execution_start);
```

## Detect an interrupted workflow

A simple example of hijacking the api:

```Javascript  theme={null}
import { api } from "../../scripts/api.js";
/* in setup() */
    const original_api_interrupt = api.interrupt;
    api.interrupt = function () {
        /* Do something before the original method is called */
        original_api_interrupt.apply(this, arguments);
        /* Or after */
    }
```

## Catch clicks on your node

`node` has a mouseDown method you can hijack.
This time we're careful to pass on any return value.

```javascript  theme={null}
async nodeCreated(node) {
    if (node?.comfyClass === "My Node Name") {
        const original_onMouseDown = node.onMouseDown;
        node.onMouseDown = function( e, pos, canvas ) {
            alert("ouch!");
            return original_onMouseDown?.apply(this, arguments);
        }        
    }
}
```


# Selection Toolbox
Source: https://docs.comfy.org/custom-nodes/js/javascript_selection_toolbox



The Selection Toolbox API allows extensions to add custom action buttons that appear when nodes are selected on the canvas. This provides quick access to context-sensitive commands for selected items (nodes, groups, etc.).

## Basic Usage

To add commands to the selection toolbox, your extension needs to:

1. Define commands with the standard [command interface](https://docs.comfy.org/custom-nodes/js/javascript_commands_keybindings)
2. Implement the `getSelectionToolboxCommands` method to specify which commands appear in the toolbox

Note: The `getSelectionToolboxCommands` method is called for each item in the selection set whenever a new selection is made.

```javascript  theme={null}
app.registerExtension({
  name: "MyExtension",
  commands: [
    {
      id: "my-extension.duplicate-special",
      label: "Duplicate Special",
      icon: "pi pi-copy",
      function: (selectedItem) => {
        // Your command logic here
        console.log("Duplicating selected nodes with special behavior");
      }
    }
  ],
  getSelectionToolboxCommands: (selectedItem) => {
    // Return array of command IDs to show in the toolbox
    return ["my-extension.duplicate-special"];
  }
});
```

## Command Definition

Commands for the selection toolbox use the standard ComfyUI command interface:

```javascript  theme={null}
{
  id: string,          // Unique identifier for the command
  label: string,       // Display text for the button tooltip
  icon?: string,       // Icon class for the button (optional)
  function: (selectedItem) => void  // Function executed when clicked
}
```

The `function` receives the selected item(s) as a parameter, allowing you to perform actions on the current selection.

## Icon Options

Selection toolbox buttons support the same icon libraries as other UI elements:

* PrimeVue icons: `pi pi-[icon-name]` (e.g., `pi pi-star`)
* Material Design icons: `mdi mdi-[icon-name]` (e.g., `mdi mdi-content-copy`)

## Dynamic Command Visibility

The `getSelectionToolboxCommands` method is called each time the selection changes, allowing you to show different commands based on what's selected:

```javascript  theme={null}
app.registerExtension({
  name: "ContextualCommands",
  commands: [
    {
      id: "my-ext.align-nodes",
      label: "Align Nodes",
      icon: "pi pi-align-left",
      function: () => {
        // Align multiple nodes
      }
    },
    {
      id: "my-ext.configure-single",
      label: "Configure",
      icon: "pi pi-cog",
      function: () => {
        // Configure single node
      }
    }
  ],
  getSelectionToolboxCommands: (selectedItem) => {
    const selectedItems = app.canvas.selectedItems;
    const itemCount = selectedItems ? selectedItems.size : 0;
    
    if (itemCount > 1) {
      // Show alignment command for multiple items
      return ["my-ext.align-nodes"];
    } else if (itemCount === 1) {
      // Show configuration for single item
      return ["my-ext.configure-single"];
    }
    
    return [];
  }
});
```

## Working with Selected Items

Access information about selected items through the app's canvas object. The `selectedItems` property is a Set that includes nodes, groups, and other canvas elements:

```javascript  theme={null}
app.registerExtension({
  name: "SelectionInfo",
  commands: [
    {
      id: "my-ext.show-info",
      label: "Show Selection Info",
      icon: "pi pi-info-circle",
      function: () => {
        const selectedItems = app.canvas.selectedItems;
        
        if (selectedItems && selectedItems.size > 0) {
          console.log(`Selected ${selectedItems.size} items`);
          
          // Iterate through selected items
          selectedItems.forEach(item => {
            if (item.type) {
              console.log(`Item: ${item.type} (ID: ${item.id})`);
            }
          });
        }
      }
    }
  ],
  getSelectionToolboxCommands: () => ["my-ext.show-info"]
});
```

## Complete Example

Here's a simple example showing various selection toolbox features:

```javascript  theme={null}
app.registerExtension({
  name: "SelectionTools",
  commands: [
    {
      id: "selection-tools.count",
      label: "Count Selection",
      icon: "pi pi-hashtag",
      function: () => {
        const count = app.canvas.selectedItems?.size || 0;
        app.extensionManager.toast.add({
          severity: "info",
          summary: "Selection Count",
          detail: `You have ${count} item${count !== 1 ? 's' : ''} selected`,
          life: 3000
        });
      }
    },
    {
      id: "selection-tools.copy-ids",
      label: "Copy IDs",
      icon: "pi pi-copy",
      function: () => {
        const items = Array.from(app.canvas.selectedItems || []);
        const ids = items.map(item => item.id).filter(id => id !== undefined);
        
        if (ids.length > 0) {
          navigator.clipboard.writeText(ids.join(', '));
          app.extensionManager.toast.add({
            severity: "success",
            summary: "Copied",
            detail: `Copied ${ids.length} IDs to clipboard`,
            life: 2000
          });
        }
      }
    },
    {
      id: "selection-tools.log-types",
      label: "Log Types",
      icon: "pi pi-info-circle",
      function: () => {
        const items = Array.from(app.canvas.selectedItems || []);
        const typeCount = {};
        
        items.forEach(item => {
          const type = item.type || 'unknown';
          typeCount[type] = (typeCount[type] || 0) + 1;
        });
        
        console.log("Selection types:", typeCount);
      }
    }
  ],
  
  getSelectionToolboxCommands: (selectedItem) => {
    const selectedItems = app.canvas.selectedItems;
    const itemCount = selectedItems ? selectedItems.size : 0;
    
    if (itemCount === 0) return [];
    
    const commands = ["selection-tools.count", "selection-tools.log-types"];
    
    // Only show copy command if items have IDs
    const hasIds = Array.from(selectedItems).some(item => item.id !== undefined);
    if (hasIds) {
      commands.push("selection-tools.copy-ids");
    }
    
    return commands;
  }
});
```

## Notes

* The selection toolbox must be enabled in settings: `Comfy.Canvas.SelectionToolbox`
* Commands must be defined in the `commands` array before being referenced in `getSelectionToolboxCommands`
* The toolbox automatically updates when the selection changes
* The `getSelectionToolboxCommands` method is called for each item in the selection set whenever a new selection is made
* Use `app.canvas.selectedItems` (a Set) to access all selected items including nodes, groups, and other canvas elements
* For backward compatibility, `app.canvas.selected_nodes` still exists but only contains nodes


# Sidebar Tabs
Source: https://docs.comfy.org/custom-nodes/js/javascript_sidebar_tabs



The Sidebar Tabs API allows extensions to add custom tabs to the sidebar of the ComfyUI interface. This is useful for adding features that require persistent visibility and quick access.

## Basic Usage

```javascript  theme={null}
app.extensionManager.registerSidebarTab({
  id: "customSidebar",
  icon: "pi pi-compass",
  title: "Custom Tab",
  tooltip: "My Custom Sidebar Tab",
  type: "custom",
  render: (el) => {
    el.innerHTML = '<div>This is my custom sidebar content</div>';
  }
});
```

## Tab Configuration

Each tab requires several properties:

```javascript  theme={null}
{
  id: string,              // Unique identifier for the tab
  icon: string,            // Icon class for the tab button
  title: string,           // Title text for the tab
  tooltip?: string,        // Tooltip text on hover (optional)
  type: string,            // Tab type (usually "custom")
  render: (element) => void // Function that populates the tab content
}
```

The `render` function receives a DOM element where you should insert your tab's content.

## Icon Options

Sidebar tab icons can use various icon sets:

* PrimeVue icons: `pi pi-[icon-name]` (e.g., `pi pi-home`)
* Material Design icons: `mdi mdi-[icon-name]` (e.g., `mdi mdi-robot`)
* Font Awesome icons: `fa-[style] fa-[icon-name]` (e.g., `fa-solid fa-star`)

Ensure the corresponding icon library is loaded before using these icons.

## Stateful Tab Example

You can create tabs that maintain state:

```javascript  theme={null}
app.extensionManager.registerSidebarTab({
  id: "statefulTab",
  icon: "pi pi-list",
  title: "Notes",
  type: "custom",
  render: (el) => {
    // Create elements
    const container = document.createElement('div');
    container.style.padding = '10px';
    
    const notepad = document.createElement('textarea');
    notepad.style.width = '100%';
    notepad.style.height = '200px';
    notepad.style.marginBottom = '10px';
    
    // Load saved content if available
    const savedContent = localStorage.getItem('comfyui-notes');
    if (savedContent) {
      notepad.value = savedContent;
    }
    
    // Auto-save content
    notepad.addEventListener('input', () => {
      localStorage.setItem('comfyui-notes', notepad.value);
    });
    
    // Assemble the UI
    container.appendChild(notepad);
    el.appendChild(container);
  }
});
```

## Using React Components

You can mount React components in sidebar tabs:

```javascript  theme={null}
// Import React dependencies in your extension
import React from "react";
import ReactDOM from "react-dom/client";

// Register sidebar tab with React content
app.extensionManager.registerSidebarTab({
  id: "reactSidebar",
  icon: "mdi mdi-react",
  title: "React Tab",
  type: "custom",
  render: (el) => {
    const container = document.createElement("div");
    container.id = "react-sidebar-container";
    el.appendChild(container);
    
    // Define a simple React component
    function SidebarContent() {
      const [count, setCount] = React.useState(0);
      
      return (
        <div style={{ padding: "10px" }}>
          <h3>React Sidebar</h3>
          <p>Count: {count}</p>
          <button onClick={() => setCount(count + 1)}>
            Increment
          </button>
        </div>
      );
    }
    
    // Mount React component
    ReactDOM.createRoot(container).render(
      <React.StrictMode>
        <SidebarContent />
      </React.StrictMode>
    );
  }
});
```

For a real-world example of a React application integrated as a sidebar tab, check out the [ComfyUI-Copilot project on GitHub](https://github.com/AIDC-AI/ComfyUI-Copilot).

## Dynamic Content Updates

You can update sidebar content in response to graph changes:

```javascript  theme={null}
app.extensionManager.registerSidebarTab({
  id: "dynamicSidebar",
  icon: "pi pi-chart-line",
  title: "Stats",
  type: "custom",
  render: (el) => {
    const container = document.createElement('div');
    container.style.padding = '10px';
    el.appendChild(container);
    
    // Function to update stats
    function updateStats() {
      const stats = {
        nodes: app.graph._nodes.length,
        connections: Object.keys(app.graph.links).length
      };
      
      container.innerHTML = `
        <h3>Workflow Stats</h3>
        <ul>
          <li>Nodes: ${stats.nodes}</li>
          <li>Connections: ${stats.connections}</li>
        </ul>
      `;
    }
    
    // Initial update
    updateStats();
    
    // Listen for graph changes
    const api = app.api;
    api.addEventListener("graphChanged", updateStats);
    
    // Clean up listeners when tab is destroyed
    return () => {
      api.removeEventListener("graphChanged", updateStats);
    };
  }
});
```


# Topbar Menu
Source: https://docs.comfy.org/custom-nodes/js/javascript_topbar_menu



The Topbar Menu API allows extensions to add custom menu items to the ComfyUI's top menu bar. This is useful for providing access to advanced features or less frequently used commands.

## Basic Usage

```javascript  theme={null}
app.registerExtension({
  name: "MyExtension",
  // Define commands
  commands: [
    { 
      id: "myCommand", 
      label: "My Command", 
      function: () => { alert("Command executed!"); } 
    }
  ],
  // Add commands to menu
  menuCommands: [
    { 
      path: ["Extensions", "My Extension"], 
      commands: ["myCommand"] 
    }
  ]
});
```

Command definitions follow the same pattern as in the [Commands and Keybindings API](./javascript_commands_keybindings). See that page for more detailed information about defining commands.

## Command Configuration

Each command requires an `id`, `label`, and `function`:

```javascript  theme={null}
{
  id: string,              // Unique identifier for the command
  label: string,           // Display name for the command
  function: () => void     // Function to execute when command is triggered
}
```

## Menu Configuration

The `menuCommands` array defines where to place commands in the menu structure:

```javascript  theme={null}
{
  path: string[],          // Array representing menu hierarchy
  commands: string[]       // Array of command IDs to add at this location
}
```

The `path` array specifies the menu hierarchy. For example, `["File", "Export"]` would add commands to the "Export" submenu under the "File" menu.

## Menu Examples

### Adding to Existing Menus

```javascript  theme={null}
app.registerExtension({
  name: "MenuExamples",
  commands: [
    { 
      id: "saveAsImage", 
      label: "Save as Image", 
      function: () => { 
        // Code to save canvas as image
      } 
    },
    { 
      id: "exportWorkflow", 
      label: "Export Workflow", 
      function: () => { 
        // Code to export workflow
      } 
    }
  ],
  menuCommands: [
    // Add to File menu
    { 
      path: ["File"], 
      commands: ["saveAsImage", "exportWorkflow"] 
    }
  ]
});
```

### Creating Submenu Structure

```javascript  theme={null}
app.registerExtension({
  name: "SubmenuExample",
  commands: [
    { 
      id: "option1", 
      label: "Option 1", 
      function: () => { console.log("Option 1"); } 
    },
    { 
      id: "option2", 
      label: "Option 2", 
      function: () => { console.log("Option 2"); } 
    },
    { 
      id: "suboption1", 
      label: "Sub-option 1", 
      function: () => { console.log("Sub-option 1"); } 
    }
  ],
  menuCommands: [
    // Create a nested menu structure
    { 
      path: ["Extensions", "My Tools"], 
      commands: ["option1", "option2"] 
    },
    { 
      path: ["Extensions", "My Tools", "Advanced"], 
      commands: ["suboption1"] 
    }
  ]
});
```

### Multiple Menu Locations

You can add the same command to multiple menu locations:

```javascript  theme={null}
app.registerExtension({
  name: "MultiLocationExample",
  commands: [
    { 
      id: "helpCommand", 
      label: "Get Help", 
      function: () => { window.open("https://docs.example.com", "_blank"); } 
    }
  ],
  menuCommands: [
    // Add to Help menu
    { 
      path: ["Help"], 
      commands: ["helpCommand"] 
    },
    // Also add to Extensions menu
    { 
      path: ["Extensions"], 
      commands: ["helpCommand"] 
    }
  ]
});
```

Commands can work with other ComfyUI APIs like settings. For more information about the Settings API, see the [Settings API](./javascript_settings) documentation.


# V3 Migration
Source: https://docs.comfy.org/custom-nodes/v3_migration

How to migrate your existing V1 nodes to the new V3 schema.

## Overview

The ComfyUI V3 schema introduces a more organized way of defining nodes, and future extensions to node features will only be added to V3 schema. You can use this guide to help you migrate your existing V1 nodes to the new V3 schema.

## Core Concepts

The V3 schema is kept on the new versioned Comfy API, meaning future revisions to the schema will be backwards compatible. `comfy_api.latest` will point to the latest numbered API that is still under development; the version before latest is what can be considered 'stable'. Version `v0_0_2` is the current (and first) API version so more changes will be made to it without warning. Once it is considered stable, a new version `v0_0_3` will be created for `latest` to point at.

```python  theme={null}
# use latest ComfyUI API
from comfy_api.latest import ComfyExtension, io, ui

# use a specific version of ComfyUI API
from comfy_api.v0_0_2 import ComfyExtension, io, ui
```

### V1 vs V3 Architecture

The biggest changes in V3 schema are:

* Inputs and Outputs defined by objects instead of a dictionary.
* The execution method is fixed to the name 'execute' and is a class method.
* `def comfy_entrypoint()` function that returns a ComfyExtension object defines exposed nodes instead of NODE\_CLASS\_MAPPINGS/NODE\_DISPLAY\_NAME\_MAPPINGS
* Node objects do not expose 'state' - `def __init__(self)` will have no effect on what is exposed in the node's functions, as all of them are class methods. The node class is sanitized before execution as well.

#### V1 (Legacy)

```python  theme={null}
class MyNode:
    @classmethod
    def INPUT_TYPES(s):
        return {"required": {...}}

    RETURN_TYPES = ("IMAGE",)
    FUNCTION = "execute"
    CATEGORY = "my_category"

    def execute(self, ...):
        return (result,)

NODE_CLASS_MAPPINGS = {"MyNode": MyNode}
```

#### V3 (Modern)

```python  theme={null}
from comfy_api.latest import ComfyExtension, io

class MyNode(io.ComfyNode):
    @classmethod
    def define_schema(cls) -> io.Schema:
        return io.Schema(
            node_id="MyNode",
            display_name="My Node",
            category="my_category",
            inputs=[...],
            outputs=[...]
        )

    @classmethod
    def execute(cls, ...) -> io.NodeOutput:
        return io.NodeOutput(result)

class MyExtension(ComfyExtension):
    async def get_node_list(self) -> list[type[io.ComfyNode]]:
        return [MyNode]

async def comfy_entrypoint() -> ComfyExtension:
    return MyExtension()
```

## Migration Steps

Going from V1 to V3 should be simple in most cases and is simply a syntax change.

### Step 1: Change Base Class

All V3 Schema nodes should inherit from `ComfyNode`. Multiple layers of inheritance are okay as long as at the top of the chain there is a `ComfyNode` parent.

**V1:**

```python  theme={null}
class Example:
    def __init__(self):
        pass
```

**V3:**

```python  theme={null}
from comfy_api.latest import io

class Example(io.ComfyNode):
    # No __init__ needed
```

### Step 2: Convert INPUT\_TYPES to define\_schema

Node properties like node id, display name, category, etc. that were assigned in different places in code such as dictionaries and class properties are now kept together via the `Schema` class.

The `define_schema(cls)` function is expected to return a `Schema` object in much the same way INPUT\_TYPES(s) worked in V1.

Supported core Input/Output types are stored and documented in `comfy_api/{version}` in `_io.py`, which is namespaced as `io` by default. Since Inputs/Outputs are defined by classes now instead of dictionaries or strings, custom types are supported by either defining your own class or using the helper function `Custom` in `io`.

Custom types are elaborated on in a section further below.

A type class has the following properties:

* `class Input` for Inputs (i.e. `Model.Input(...)`)
* `class Output` for Outputs (i.e. `Model.Output(...)`). Note that all types may not support being an output.
* `Type` for getting a typehint of the type (i.e. `Model.Type`). Note that some typehints are just `any`, which may be updated in the future. These typehints are not enforced and just act as useful documentation.

**V1:**

```python  theme={null}
@classmethod
def INPUT_TYPES(s):
    return {
        "required": {
            "image": ("IMAGE",),
            "int_field": ("INT", {
                "default": 0,
                "min": 0,
                "max": 4096,
                "step": 64,
                "display": "number"
            }),
            "string_field": ("STRING", {
                "multiline": False,
                "default": "Hello"
            }),
            # V1 handling of arbitrary types
            "custom_field": ("MY_CUSTOM_TYPE",),
        },
        "optional": {
            "mask": ("MASK",)
        }
    }
```

**V3:**

```python  theme={null}
@classmethod
def define_schema(cls) -> io.Schema:
    return io.Schema(
        node_id="Example",
        display_name="Example Node",
        category="examples",
        description="Node description here",
        inputs=[
            io.Image.Input("image"),
            io.Int.Input("int_field",
                default=0,
                min=0,
                max=4096,
                step=64,
                display_mode=io.NumberDisplay.number
            ),
            io.String.Input("string_field",
                default="Hello",
                multiline=False
            ),
            # V3 handling of arbitrary types
            io.Custom("my_custom_type").Input("custom_input"),
            io.Mask.Input("mask", optional=True)
        ],
        outputs=[
            io.Image.Output()
        ]
    )
```

### Step 3: Update Execute Method

All execution functions in v3 are named `execute` and are class methods.

**V1:**

```python  theme={null}
def test(self, image, string_field, int_field):
    # Process
    image = 1.0 - image
    return (image,)
```

**V3:**

```python  theme={null}
@classmethod
def execute(cls, image, string_field, int_field) -> io.NodeOutput:
    # Process
    image = 1.0 - image

    # Return with optional UI preview
    return io.NodeOutput(image, ui=ui.PreviewImage(image, cls=cls))
```

### Step 4: Convert Node Properties

Here are some examples of property names; see the source code in `comfy_api.latest._io` for more details.

| V1 Property    | V3 Schema Field             | Notes                       |
| -------------- | --------------------------- | --------------------------- |
| `RETURN_TYPES` | `outputs` in Schema         | List of Output objects      |
| `RETURN_NAMES` | `display_name` in Output    | Per-output display names    |
| `FUNCTION`     | Always `execute`            | Method name is standardized |
| `CATEGORY`     | `category` in Schema        | String value                |
| `OUTPUT_NODE`  | `is_output_node` in Schema  | Boolean flag                |
| `DEPRECATED`   | `is_deprecated` in Schema   | Boolean flag                |
| `EXPERIMENTAL` | `is_experimental` in Schema | Boolean flag                |

### Step 5: Handle Special Methods

The same special methods are supported as in v1, but either lowercased or renamed entirely to be more clear. Their usage remains the same.

#### Validation (V1  V3)

The input validation function was renamed to `validate_inputs`.

**V1:**

```python  theme={null}
@classmethod
def VALIDATE_INPUTS(s, **kwargs):
    # Validation logic
    return True
```

**V3:**

```python  theme={null}
@classmethod
def validate_inputs(cls, **kwargs) -> bool | str:
    # Return True if valid, error string if not
    if error_condition:
        return "Error message"
    return True
```

#### Lazy Evaluation (V1  V3)

The `check_lazy_status` function is class method, remains the same otherwise.

**V1:**

```python  theme={null}
def check_lazy_status(self, image, string_field, ...):
    if condition:
        return ["string_field"]
    return []
```

**V3:**

```python  theme={null}
@classmethod
def check_lazy_status(cls, image, string_field, ...):
    if condition:
        return ["string_field"]
    return []
```

#### Cache Control (V1  V3)

The functionality of cache control remains the same as in V1, but the original name was very misleading as to how it operated.

V1's `IS_CHANGED` function signals execution not to trigger rerunning the node if the return value is the SAME as the last time the node was ran.

Thus, the function `IS_CHANGED` was renamed to `fingerprint_inputs`. One of the most common mistakes by developers was thinking if you return `True`, the node would always re-run. Because `True` would always be returned, it would have the opposite effect of only making the node run once and reuse cached values.

An example of using this function is the LoadImage node. It returns the hash of the selected file, so that if the file changes, the node will be forced to rerun.

**V1:**

```python  theme={null}
@classmethod
def IS_CHANGED(s, **kwargs):
    return "unique_value"
```

**V3:**

```python  theme={null}
@classmethod
def fingerprint_inputs(cls, **kwargs):
    return "unique_value"
```

### Step 6: Create Extension and Entry Point

Instead of defining dictionaries to link node id to node class/display name, there is now a `ComfyExtension` class and an expected `comfy_entrypoint` function to be defined.

In the future, more functions may be added to ComfyExtension to register more than just nodes via `get_node_list`.

`comfy_entrypoint` can be either async or not, but `get_node_list` must be defined as async.

**V1:**

```python  theme={null}
NODE_CLASS_MAPPINGS = {
    "Example": Example
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "Example": "Example Node"
}
```

**V3:**

```python  theme={null}
from comfy_api.latest import ComfyExtension

class MyExtension(ComfyExtension):
    # must be declared as async
    async def get_node_list(self) -> list[type[io.ComfyNode]]:
        return [
            Example,
            # Add more nodes here
        ]

# can be declared async or not, both will work
async def comfy_entrypoint() -> MyExtension:
    return MyExtension()
```

## Input Type Reference

Already explained in step 2, but here are some type reference comparisons in V1 vs V3. See `comfy_api.latest._io` for the full type declarations.

### Basic Types

| V1 Type     | V3 Type              | Example                                                      |
| ----------- | -------------------- | ------------------------------------------------------------ |
| `"INT"`     | `io.Int.Input()`     | `io.Int.Input("count", default=1, min=0, max=100)`           |
| `"FLOAT"`   | `io.Float.Input()`   | `io.Float.Input("strength", default=1.0, min=0.0, max=10.0)` |
| `"STRING"`  | `io.String.Input()`  | `io.String.Input("text", multiline=True)`                    |
| `"BOOLEAN"` | `io.Boolean.Input()` | `io.Boolean.Input("enabled", default=True)`                  |

### ComfyUI Types

| V1 Type          | V3 Type                   | Example                                          |
| ---------------- | ------------------------- | ------------------------------------------------ |
| `"IMAGE"`        | `io.Image.Input()`        | `io.Image.Input("image", tooltip="Input image")` |
| `"MASK"`         | `io.Mask.Input()`         | `io.Mask.Input("mask", optional=True)`           |
| `"LATENT"`       | `io.Latent.Input()`       | `io.Latent.Input("latent")`                      |
| `"CONDITIONING"` | `io.Conditioning.Input()` | `io.Conditioning.Input("positive")`              |
| `"MODEL"`        | `io.Model.Input()`        | `io.Model.Input("model")`                        |
| `"VAE"`          | `io.VAE.Input()`          | `io.VAE.Input("vae")`                            |
| `"CLIP"`         | `io.CLIP.Input()`         | `io.CLIP.Input("clip")`                          |

### Combo (Dropdowns/Selection Lists)

Combo types in V3 require explicit class definition.

**V1:**

```python  theme={null}
"mode": (["option1", "option2", "option3"],)
```

**V3:**

```python  theme={null}
io.Combo.Input("mode", options=["option1", "option2", "option3"])
```

## Advanced Features

### UI Integration

V3 provides built-in UI helpers to avoid common boilerplate of saving files.

```python  theme={null}
from comfy_api.latest import ui

@classmethod
def execute(cls, images) -> io.NodeOutput:
    # Show preview in node
    return io.NodeOutput(images, ui=ui.PreviewImage(images, cls=cls))
```

### Output Nodes

For nodes that produce side effects (like saving files). Same as in V1, marking a node as output will display a `run` play button in the node's context window, allowing for partial execution of the graph.

```python  theme={null}
@classmethod
def define_schema(cls) -> io.Schema:
    return io.Schema(
        node_id="SaveNode",
        inputs=[...],
        outputs=[],  # Does not need to be empty.
        is_output_node=True  # Mark as output node
    )
```

### Custom Types

Create custom input/output types either via class definition of `Custom` helper function.

```python  theme={null}
from comfy_api.latest import io

# Method 1: Using decorator with class
@io.comfytype(io_type="MY_CUSTOM_TYPE")
class MyCustomType:
    Type = torch.Tensor  # Python type hint

    class Input(io.Input):
        def __init__(self, id: str, **kwargs):
            super().__init__(id, **kwargs)

    class Output(io.Output):
        def __init__(self, **kwargs):
            super().__init__(**kwargs)

# Method 2: Using Custom helper
# The helper can be used directly without saving to a variable first for convenience as well
MyCustomType = io.Custom("MY_CUSTOM_TYPE")
```


# Workflow templates
Source: https://docs.comfy.org/custom-nodes/workflow_templates



If you have example workflow files associated with your custom nodes
then ComfyUI can show these to the user in the template browser (`Workflow`/`Browse Templates` menu).
Workflow templates are a great way to support people getting started with your nodes.

All you have to do as a node developer is to create an `example_workflows` folder and place the `json` files there.
Optionally you can place `jpg` files with the same name to be shown as the template thumbnail.

Under the hood ComfyUI statically serves these files along with an endpoint (`/api/workflow_templates`)
that returns the collection of workflow templates.

<Note>
  The following folder names are also accepted, but we still recommend using `example_workflows`

  * workflow
  * workflows
  * example
  * examples
</Note>

## Example

Under `ComfyUI-MyCustomNodeModule/example_workflows/` directory:

* `My_example_workflow_1.json`
* `My_example_workflow_1.jpg`
* `My_example_workflow_2.json`

In this example ComfyUI's template browser shows a category called `ComfyUI-MyCustomNodeModule` with two items, one of which has a thumbnail.


# Custom Node CI/CD
Source: https://docs.comfy.org/registry/cicd



## Introduction

When making changes to custom nodes, it's not uncommon to break things in Comfy or other custom nodes. It is often unrealistic to test on every operating system and different configurations of Pytorch.

### Run Comfy Workflows using Github Actions

[Comfy-Action](https://github.com/Comfy-Org/comfy-action) allows you to run a Comfy workflow\.json file on Github Actions. It supports downloading models, custom nodes, and runs on Linux/Mac/Windows.

### Results

Output files are uploaded to the [CI/CD Dashboard](https://comfyci.org) and can be viewed as a last step before committing new changes or publishing new versions of the custom node.

<img src="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfyci.png?fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=443608968123247168b89b026ed2ae0e" alt="ComfyCI" data-og-width="2526" width="2526" data-og-height="1722" height="1722" data-path="images/comfyci.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfyci.png?w=280&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=bf00166a6e417a12b3afce2a3b70caac 280w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfyci.png?w=560&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=8e6a0bfe6492622ae7c3b61c2946fb01 560w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfyci.png?w=840&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=4381e7cb42aa617fdfaf2f0f51abd09c 840w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfyci.png?w=1100&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=64dd33fa30c0810d93e6c4415e847d6d 1100w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfyci.png?w=1650&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=ac6ecc93ae6dfc064445e7ca61e8351c 1650w, https://mintcdn.com/dripart/Rig0_LOInmwVbVSB/images/comfyci.png?w=2500&fit=max&auto=format&n=Rig0_LOInmwVbVSB&q=85&s=c458db2f48fa97f12f368c2adcd3de76 2500w" />


# Claim My Node
Source: https://docs.comfy.org/registry/claim-my-node



## Overview

The **Claim My Node** feature allows developers to claim ownership of custom nodes in the ComfyUI Registry. This system ensures that only the rightful authors can manage and update their published nodes, providing security and accountability within the community.

## What are Unclaimed Nodes?

As we migrate from ComfyUI Manager to the Comfy Registry with new standards, many custom nodes that were previously listed in the ComfyUI Manager legacy system now appear as "unclaimed" in the registry. These are nodes that:

* Were originally published in the ComfyUI Manager legacy system
* Have been migrated to the Comfy Registry to meet the latest standards
* Are waiting for their original authors to claim ownership

We provide an easy way for developers to claim these migrated nodes, ensuring a smooth transition from the legacy system to the new registry standard while maintaining proper ownership and control.

## Getting Started

To claim your nodes:

1. **Navigate to Your Unclaimed Node Page**: Visit your unclaimed node page, Click "Claim my node!" button.

2. **Create a Publisher (if you don't have one yet)**:
   * If you haven't created a publisher yet, you'll be prompted to create one. A publisher is required to claim nodes and manage them effectively.

3. **Select a Publisher**: Choose the publisher under which you want to claim the node. this step will redirect you to the claim page.

<img className="block dark:hidden" src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/registry/claim-my-node/choose-a-publisher.png?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=d39535c83d234287192d81e822a3e57b" alt="Choose a Publisher - Light Mode" data-og-width="763" width="763" data-og-height="433" height="433" data-path="images/registry/claim-my-node/choose-a-publisher.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/registry/claim-my-node/choose-a-publisher.png?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=2ec391c5d706c46147d9b151d3a576f1 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/registry/claim-my-node/choose-a-publisher.png?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=dc59831c2302f81388d531b22298efb9 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/registry/claim-my-node/choose-a-publisher.png?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=de460789f899a3122da7f4bde42db9cd 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/registry/claim-my-node/choose-a-publisher.png?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=1aa2e046d275a4ca3987ac8a57d48ae0 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/registry/claim-my-node/choose-a-publisher.png?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=8ef76ed2acdf78e146a78e3835176341 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/registry/claim-my-node/choose-a-publisher.png?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=4ecdde28e4243f4799e8a6ca5e3d5a3c 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/registry/claim-my-node/choose-a-publisher.png?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=d39535c83d234287192d81e822a3e57b" alt="Choose a Publisher - Dark Mode" data-og-width="763" width="763" data-og-height="433" height="433" data-path="images/registry/claim-my-node/choose-a-publisher.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/registry/claim-my-node/choose-a-publisher.png?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=2ec391c5d706c46147d9b151d3a576f1 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/registry/claim-my-node/choose-a-publisher.png?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=dc59831c2302f81388d531b22298efb9 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/registry/claim-my-node/choose-a-publisher.png?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=de460789f899a3122da7f4bde42db9cd 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/registry/claim-my-node/choose-a-publisher.png?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=1aa2e046d275a4ca3987ac8a57d48ae0 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/registry/claim-my-node/choose-a-publisher.png?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=8ef76ed2acdf78e146a78e3835176341 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/registry/claim-my-node/choose-a-publisher.png?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=4ecdde28e4243f4799e8a6ca5e3d5a3c 2500w" />

To claim the node under the choosed publisher, follow these steps:

1. **Review Node Information:**
   * Check the node details, including the name, repository link, and publisher status as shown on the screen.

<img className="block dark:hidden" src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/registry/claim-my-node/claim-a-sample-custom-node-under-my-publisher-stage-1.png?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=fe25ebbfe7db2c36b1166841e64adddb" alt="Claim Node Process Stage 1 - Light Mode" data-og-width="774" width="774" data-og-height="501" height="501" data-path="images/registry/claim-my-node/claim-a-sample-custom-node-under-my-publisher-stage-1.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/registry/claim-my-node/claim-a-sample-custom-node-under-my-publisher-stage-1.png?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=bb76caa1153e21bf3a23a6f0a1f22c8c 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/registry/claim-my-node/claim-a-sample-custom-node-under-my-publisher-stage-1.png?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=214e11c2f9ddbb73766190b7eb3e3831 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/registry/claim-my-node/claim-a-sample-custom-node-under-my-publisher-stage-1.png?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=9a309365092aa80294a539bdf079542f 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/registry/claim-my-node/claim-a-sample-custom-node-under-my-publisher-stage-1.png?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=c3acbf9e19fa320d96e05cf347c46523 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/registry/claim-my-node/claim-a-sample-custom-node-under-my-publisher-stage-1.png?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=330a5dc0cd28a8c8c231e290067bf62d 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/registry/claim-my-node/claim-a-sample-custom-node-under-my-publisher-stage-1.png?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=08cd4437f5a9db3bfd3c442109038250 2500w" />

<img className="hidden dark:block" src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/registry/claim-my-node/claim-a-sample-custom-node-under-my-publisher-stage-1.png?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=fe25ebbfe7db2c36b1166841e64adddb" alt="Claim Node Process Stage 1 - Dark Mode" data-og-width="774" width="774" data-og-height="501" height="501" data-path="images/registry/claim-my-node/claim-a-sample-custom-node-under-my-publisher-stage-1.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/registry/claim-my-node/claim-a-sample-custom-node-under-my-publisher-stage-1.png?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=bb76caa1153e21bf3a23a6f0a1f22c8c 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/registry/claim-my-node/claim-a-sample-custom-node-under-my-publisher-stage-1.png?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=214e11c2f9ddbb73766190b7eb3e3831 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/registry/claim-my-node/claim-a-sample-custom-node-under-my-publisher-stage-1.png?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=9a309365092aa80294a539bdf079542f 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/registry/claim-my-node/claim-a-sample-custom-node-under-my-publisher-stage-1.png?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=c3acbf9e19fa320d96e05cf347c46523 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/registry/claim-my-node/claim-a-sample-custom-node-under-my-publisher-stage-1.png?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=330a5dc0cd28a8c8c231e290067bf62d 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/registry/claim-my-node/claim-a-sample-custom-node-under-my-publisher-stage-1.png?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=08cd4437f5a9db3bfd3c442109038250 2500w" />

2. **GitHub Authentication:**
   * Click the "Continue with GitHub" button to start the authentication process. Ensure you're logged in to the correct GitHub account with admin rights to the repository.

3. **Verify Admin Access:**
   * Once logged in, the system will verify if you have admin privileges for the specified GitHub repository. This step is crucial to ensure you have the necessary permissions.

4. **Claim the Node:**
   * If the verification is successful, Click "Claim" to claim the node. The publisher status will change, indicating ownership.

5. **Complete!:**
   * After successfully claiming the node, you can continue on [publishing](./publishing) and managing your node as the rightful owner.

## Frequently Asked Questions

<AccordionGroup>
  <Accordion title="Who can claim nodes?">
    Any GitHub user can submit a claim request, but claims are verified against the original node repository and author information.
  </Accordion>

  <Accordion title="How long does the claim process take?">
    Claims are automatically approved once GitHub admin permissions to the repository are verified. This happens instantly upon submitting your claim.
  </Accordion>

  <Accordion title="What happens after my claim is approved?">
    Once approved, you'll have full control over your node's management, including publish versions, updates meta, deprecation, and policy settings.
  </Accordion>

  <Accordion title="Can I claim nodes I didn't create?">
    No, claims are automatically validated against GitHub repository admin permissions. Only users with admin access to the original repository can successfully claim nodes.
  </Accordion>

  <Accordion title="What if my claim is denied?">
    Claims are automatically processed based on GitHub admin permissions. If you don't have admin access to the repository, the claim will be automatically denied. Contact the repository owner to request admin access if you believe you should have ownership rights.
  </Accordion>
</AccordionGroup>


# Overview
Source: https://docs.comfy.org/registry/overview



## Introduction

The Registry is a public collection of custom nodes that powers [ComfyUI-Manager](https://github.com/ltdrdata/ComfyUI-Manager). Developers can publish, version, deprecate, and track metrics related to their custom nodes. ComfyUI users can discover, install, and rate custom nodes from the registry through ComfyUI-Manager.

## Why use the Registry?

The Comfy Registry helps the community by standardizing the development of custom nodes:

<Icon icon="timeline" iconType="solid" size={20} /> **Node Versioning:** Developers frequently publish new versions of their custom nodes which often break workflows that rely on them. With registry nodes being [semantically versioned](https://semver.org/), users can now choose to safely upgrade, deprecate, or lock their node versions in place, knowing in advance how their actions will impact their workflows. The workflow JSON will store the version of the node used, so you can always reliably reproduce your workflows.

<Icon icon="shield" iconType="solid" size={20} /> **Node Security:** The registry will serve as a backend for the [ComfyUI-manager](https://github.com/ltdrdata/ComfyUI-Manager). All nodes will be scanned for malicious behaviour such as custom pip wheels, arbitrary system calls, etc. Nodes that pass these checks will have a verification flag (<Icon icon="check" iconType="solid" />) beside their name on the UI-manager. For a list of security standards, see the [standards](/registry/standards).

<Icon icon="magnifying-glass" iconType="solid" size={20} /> **Search:** Search across all nodes on the Registry to find existing nodes for your workflow\.x

## Publishing Nodes

Get started publishing your first node by following the [tutorial](/registry/publishing).

## Frequently Asked Questions

<AccordionGroup>
  <Accordion title="Do registry nodes have unique identifiers?">
    Yes, a custom node on the Registry has a globally unique name and this allows Comfy Workflow JSON files to uniquely identify any custom node without collisions.
  </Accordion>

  <Accordion title="Are there any restrictions on what I can publish?">
    Check the [standards](/registry/standards) for more information.
  </Accordion>

  <Accordion title="How do you ensure node stability?">
    Once a custom node version is published, it cannot be changed. This ensures that users can rely on the stability of the custom node over time.
  </Accordion>

  <Accordion title="How are nodes versioned?">
    Custom nodes are versioned using [semantic versioning](https://semver.org/). This allows users to understand the impact of upgrading to a new version.
  </Accordion>

  <Accordion title="How do I deprecate a node version?">
    You can deprecate a version in the Comfy Registry website by clicking **More Actions > Deprecate**. Users who installed this version will be shown the deprecation message and be encouraged to upgrade to a newer version.

    Deprecating versions is useful when an issue is discovered after publishing.
  </Accordion>
</AccordionGroup>


# Publishing Nodes
Source: https://docs.comfy.org/registry/publishing



## Set up a Registry Account

Follow the steps below to set up a registry account and publish your first node.

### Watch a Tutorial

<iframe height="415" src="https://www.youtube.com/embed/WhOZZOgBggU?si=6TyvhJJadmQ65uXC" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen style={{ width: "100%", borderRadius: "0.5rem" }} />

### Create a Publisher

A publisher is an identity that can publish custom nodes to the registry. Every custom node needs to include a publisher identifier in the pyproject.toml [file]().

Go to [Comfy Registry](https://registry.comfy.org), and create a publisher account. Your publisher id is globally unique, and cannot be changed later because it is used in the URL of your custom node.

Your publisher id is found after the `@` symbol on your profile page.

<img className="block" src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/publisherid.png?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=95e3271276938f1ec1226f172ffc020f" alt="Hero Dark" data-og-width="534" width="534" data-og-height="342" height="342" data-path="images/publisherid.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/publisherid.png?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=aafd181c16f25d66961b717fb46b77e8 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/publisherid.png?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=618357651a7a64a0f90d31227ba4295b 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/publisherid.png?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=743c9a3f2b3c387aee9f668169e8b21f 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/publisherid.png?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=a2bd945881111904b91bde24daa733f3 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/publisherid.png?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=511f7245307d7569caeedfd0781864cc 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/publisherid.png?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=f2cc85d730f5fd90c59d8d198c2e623d 2500w" />

### Create a Registry Publishing API Key

<Note>
  **Important:** This API key is specifically for **publishing custom nodes to the Registry and ComfyUI-Manager**. If you're looking to use paid API nodes in your workflows instead, see [API Nodes Overview](/tutorials/partner-nodes/overview).
</Note>

Go [here](https://registry.comfy.org/nodes) and click on the publisher you want to create an API key for. This key will be used to publish custom nodes to the Registry (which powers ComfyUI-Manager) via the CLI or GitHub Actions.

<img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/pat-1.png?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=6f9c94583f78457419cfde793f49f387" alt="Create key for Specific Publisher" data-og-width="295" width="295" data-og-height="159" height="159" data-path="images/pat-1.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/pat-1.png?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=11fb3f0cf7bb78bee430c7f46acdd856 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/pat-1.png?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=ab9b38ebab6988131e095f953b453b23 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/pat-1.png?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=f1ab93bd727e4867871910d15dfcd8ac 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/pat-1.png?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=593cbe8448b237f88b299862bb93603d 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/pat-1.png?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=d16bea3a955600aa59999dfdd0abaa01 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/pat-1.png?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=1367f355a0557b74b236392c35521a3e 2500w" />

Name the API key and save it somewhere safe. If you lose it, you'll have to create a new key.

<img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/pat-2.png?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=490347389e83b978f4c65b0e6a7b5d33" alt="Create API Key" data-og-width="526" width="526" data-og-height="474" height="474" data-path="images/pat-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/pat-2.png?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=9b69fd6494a9bb03f31ec26e4360ef88 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/pat-2.png?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=b0aa3c4774cd80af6018e52f3decff9f 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/pat-2.png?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=3a7dd425166cabb9abbdcda3687a9dad 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/pat-2.png?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=0bbe8642a1be3bd3d96659723590c5f9 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/pat-2.png?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=994cdedbfa26a483463ccbaf65871e87 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/pat-2.png?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=be07aee07dc86c5a196c5babb2da69ec 2500w" />

### Add Metadata

<Tip>Have you installed the comfy-cli? [Do that first](/comfy-cli/getting-started).</Tip>

```bash  theme={null}
comfy node init
```

This command will generate the following metadata:

```toml  theme={null}
# pyproject.toml
[project]
name = "" # Unique identifier for your node. Immutable after creation.
description = ""
version = "1.0.0" # Custom Node version. Must be semantically versioned.
license = { file = "LICENSE.txt" }
dependencies  = [] # Filled in from requirements.txt

[project.urls]
Repository = "https://github.com/..."

[tool.comfy]
PublisherId = "" # TODO (fill in Publisher ID from Comfy Registry Website).
DisplayName = "" # Display name for the Custom Node. Can be changed later.
Icon = "https://example.com/icon.png" # SVG, PNG, JPG or GIF (MAX. 800x400px)
```

Add this file to your repository. Check the [specifications](/registry/specifications) for more information on the pyproject.toml file.

## Publish to the Registry

### Option 1: Comfy CLI

Run the command below to manually publish your node to the registry.

```bash  theme={null}
comfy node publish
```

You'll be prompted for the API key.

```bash  theme={null}
API Key for publisher '<publisher id>': ****************************************************

...Version 1.0.0 Published. 
See it here: https://registry.comfy.org/publisherId/your-node
```

<Warning>
  Keep in mind that the API key is hidden by default.
</Warning>

<Warning>
  When copy-pasting, your API key might have an additional \x16 at the back when using CTRL+V (for Windows), eg: \*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\x16.

  It is recommended to copy-paste your API key via right-clicking instead.
</Warning>

### Option 2: Github Actions

Automatically publish your node through github actions.

<Steps>
  <Step title="Set up a Github Secret">
    Go to Settings -> Secrets and Variables -> Actions -> Under Secrets Tab and Repository secrets -> New Repository Secret.

    Create a secret called `REGISTRY_ACCESS_TOKEN` and store your API key as the value.
  </Step>

  <Step title="Create a Github Action">
    Copy the code below and paste it here `/.github/workflows/publish_action.yml`

    ```bash  theme={null}
    name: Publish to Comfy registry
    on:
      workflow_dispatch:
      push:
        branches:
          - main
        paths:
          - "pyproject.toml"

    jobs:
      publish-node:
        name: Publish Custom Node to registry
        runs-on: ubuntu-latest
        steps:
          - name: Check out code
            uses: actions/checkout@v4
          - name: Publish Custom Node
            uses: Comfy-Org/publish-node-action@main
            with:
              personal_access_token: ${{ secrets.REGISTRY_ACCESS_TOKEN }} ## Add your own personal access token to your Github Repository secrets and reference it here.
    ```

    <Warning>If your working branch is named something besides `main`, such as `master`, add the name under the branches section.</Warning>
  </Step>

  <Step title="Test the Github Action">
    Push an update to your `pyproject.toml`'s version number. You should see your updated node on the registry.

    <Tip>The github action will automatically run every time you push an update to your `pyproject.toml` file</Tip>
  </Step>
</Steps>


# pyproject.toml
Source: https://docs.comfy.org/registry/specifications



# Specifications

The `pyproject.toml` file contains two main sections for ComfyUI custom nodes: `[project]` and `[tool.comfy]`. Below are the specifications for each section.

## \[project] Section

### name (required)

The node id uniquely identifies the custom node and will be used in URLs from the registry. Users can install the node by referencing this name:

```bash  theme={null}
comfy node install <node-id>
```

**Requirements:**

* Must be less than 100 characters
* Can only contain alphanumeric characters, hyphens, underscores, and periods
* Cannot have consecutive special characters
* Cannot start with a number or special character
* Case-insensitive comparison

**Best Practices:**

* Use a short, descriptive name
* Don't include "ComfyUI" in the name
* Make it memorable and easy to type

**Examples:**

```toml  theme={null}
name = "image-processor"      #  Good: Simple and clear
name = "super-resolution"     #  Good: Describes functionality
name = "ComfyUI-enhancer"    #  Bad: Includes ComfyUI
name = "123-tool"            #  Bad: Starts with number
```

See the official [python documentation](https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#name) for more details.

### version (required)

Uses [semantic versioning](https://semver.org/) with a three-digit version number X.Y.Z:

* X (**MAJOR**): Breaking changes
* Y (**MINOR**): New features (backwards compatible)
* Z (**PATCH**): Bug fixes

**Examples:**

```toml  theme={null}
version = "1.0.0"    # Initial release
version = "1.1.0"    # Added new features
version = "1.1.1"    # Bug fix
version = "2.0.0"    # Breaking changes
```

### license (optional)

Specifies the license for your custom node. Can be specified in two ways:

1. **File Reference:**

```toml  theme={null}
license = { file = "LICENSE" }     #  Points to LICENSE file
license = { file = "LICENSE.txt" } #  Points to LICENSE.txt
license = "LICENSE"                #  Incorrect format
```

2. **License Name:**

```toml  theme={null}
license = { text = "MIT License" }  #  Correct format
license = { text = "Apache-2.0" }   #  Correct format
license = "MIT LICENSE"             #  Incorrect format
```

Common licenses: [MIT](https://opensource.org/license/mit), [GPL](https://www.gnu.org/licenses/gpl-3.0.en.html), [Apache](https://www.apache.org/licenses/LICENSE-2.0)

### description (recommended)

A brief description of what your custom node does.

```toml  theme={null}
description = "A super resolution node for enhancing image quality"
```

### repository (required)

Links to related resources:

```toml  theme={null}
[project.urls]
Repository = "https://github.com/username/repository"
```

### urls (recommended)

Links to related resources:

```toml  theme={null}
[project.urls]
Documentation = "https://github.com/username/repository/wiki"
"Bug Tracker" = "https://github.com/username/repository/issues"
```

### requires-python (recommended)

Specifies the Python versions that your node supports:

```toml  theme={null}
requires-python = ">=3.8"        # Python 3.8 or higher
requires-python = ">=3.8,<3.11"  # Python 3.8 up to (but not including) 3.11
```

### Frontend Version Compatibility (optional)

If your node has specific requirements for which ComfyUI frontend versions it supports, you can specify this using the `comfyui-frontend-package` dependency. This package is published on [PyPI](https://pypi.org/project/comfyui-frontend-package/).

For example, use this field when:

* Your custom node uses frontend APIs that were introduced in a specific version
* You've identified incompatibilities between your node and certain frontend versions
* Your node requires specific UI features only available in newer frontend versions

```toml  theme={null}
[project]
dependencies = [
    "comfyui-frontend-package>=1.20.0"       # Requires frontend 1.20.0 or newer
    "comfyui-frontend-package<=1.21.6"       # Restricts to frontend versions up to 1.21.6
    "comfyui-frontend-package>=1.19,<1.22"   # Works with frontend 1.19 to 1.21.x
    "comfyui-frontend-package~=1.20.0"       # Compatible with 1.20.x but not 1.21.0
    "comfyui-frontend-package!=1.21.3"       # Works with any version except 1.21.3
]
```

### classifiers (recommended)

Use classifiers to specify operating system compatibility and GPU accelerators. This information is used to help users find the right node for their system.

```toml  theme={null}
[project]
classifiers = [
    # For OS-independent nodes (works on all operating systems)
    "Operating System :: OS Independent",

    # OR for OS-specific nodes, specify the supported systems:
    "Operating System :: Microsoft :: Windows",  # Windows specific
    "Operating System :: POSIX :: Linux",  # Linux specific
    "Operating System :: MacOS",  # macOS specific
    
    # GPU Accelerator support
    "Environment :: GPU :: NVIDIA CUDA",    # NVIDIA CUDA support
    "Environment :: GPU :: AMD ROCm",       # AMD ROCm support
    "Environment :: GPU :: Intel Arc",      # Intel Arc support
    "Environment :: NPU :: Huawei Ascend",  # Huawei Ascend support
    "Environment :: GPU :: Apple Metal",    # Apple Metal support
]
```

## \[tool.comfy] Section

### PublisherId (required)

Your unique publisher identifier, typically matching your GitHub username.

**Examples:**

```toml  theme={null}
PublisherId = "john-doe"        #  Matches GitHub username
PublisherId = "image-wizard"    #  Unique identifier
```

### DisplayName (optional)

A user-friendly name for your custom node.

```toml  theme={null}
DisplayName = "Super Resolution Node"
```

### Icon (optional)

URL to your custom node's icon that will be displayed on the ComfyUI Registry and ComfyUI-Manager.

**Requirements:**

* File types: SVG, PNG, JPG, or GIF
* Maximum resolution: 400px  400px
* Aspect ratio should be square

```toml  theme={null}
Icon = "https://raw.githubusercontent.com/username/repo/main/icon.png"
```

### Banner (optional)

URL to a larger banner image that will be displayed on the ComfyUI Registry and ComfyUI-Manager.

**Requirements:**

* File types: SVG, PNG, JPG, or GIF
* Aspect ratio: 21:9

```toml  theme={null}
Banner = "https://raw.githubusercontent.com/username/repo/main/banner.png"
```

### requires-comfyui (optional)

Specifies which version of ComfyUI your node is compatible with. This helps users ensure they have the correct version of ComfyUI installed.

**Supported operators:** `<`, `>`, `<=`, `>=`, `~=`, `<>`, `!=` and ranges

```toml  theme={null}
requires-comfyui = ">=1.0.0"        # ComfyUI 1.0.0 or higher
requires-comfyui = ">=1.0.0,<2.0.0"  # ComfyUI 1.0.0 up to (but not including) 2.0.0
requires-comfyui = "~=1.0.0"         # Compatible release: version 1.0.0 or newer, but not version 2.0.0
requires-comfyui = "!=1.2.3"         # Any version except 1.2.3
requires-comfyui = ">0.1.3,<1.0.0"   # Greater than 0.1.3 and less than 1.0.0
```

### includes (optional)

Specifies whether to force include certain specific folders. For some situations, such as custom nodes in frontend projects, the final packaged output folder might be included in .gitignore. In such cases, we need to force include it for registry use.

```toml  theme={null}
includes = ['dist']
```

## Complete Example

```toml  theme={null}
[project]
name = "super-resolution-node"
version = "1.0.0"
description = "Enhance image quality using advanced super resolution techniques"
license = { file = "LICENSE" }
requires-python = ">=3.8"
dependencies = [
    "comfyui-frontend-package<=1.21.6"  # Frontend version compatibility
]
classifiers = [
    "Operating System :: OS Independent"  # Works on all operating systems
]
dynamic = ["dependencies"]

[tool.setuptools.dynamic]
dependencies = {file = ["requirements.txt"]}

[project.urls]
Repository = "https://github.com/username/super-resolution-node"
Documentation = "https://github.com/username/super-resolution-node/wiki"
"Bug Tracker" = "https://github.com/username/super-resolution-node/issues"

[tool.comfy]
PublisherId = "image-wizard"
DisplayName = "Super Resolution Node"
Icon = "https://raw.githubusercontent.com/username/super-resolution-node/main/icon.png"
Banner = "https://raw.githubusercontent.com/username/super-resolution-node/main/banner.png"
requires-comfyui = ">=1.0.0"  # ComfyUI version compatibility
```


# Standards
Source: https://docs.comfy.org/registry/standards

Security and other standards for publishing to the Registry

## Base Standards

### 1. Community Value

Custom nodes must provide valuable functionality to the ComfyUI community

Avoid:

* Excessive self-promotion
* Impersonation or misleading behavior
* Malicious behavior
* Self-promotion is permitted only within your designated settings menu section
* Top and side menus should contain only useful functionality

### 2. Node Compatibility

Do not interfere with other custom nodes' operations (installation, updates, removal)

* For dependencies on other custom nodes:
  * Display clear warnings when dependent functionality is used
  * Provide example workflows demonstrating required nodes

### 3. Legal Compliance

Must comply with all applicable laws and regulations

### 5. Quality Requirements

Nodes must be fully functional, well documented, and actively maintained.

### 6. Fork Guidelines

Forked nodes must:

* Have clearly distinct names from original
* Provide significant differences in functionality or code

Below are standards that must be met to publish custom nodes to the registry.

## Security Standards

Custom nodes should be secure. We will start working with custom nodes that violate these standards to be rewritten. If there is some major functionality that should be exposed by core, please request it in the [rfcs repo](https://github.com/comfy-org/rfcs).

### eval/exec Calls

#### Policy

The use of `eval` and `exec` functions is prohibited in custom nodes due to security concerns.

#### Reasoning

These functions can enable arbitrary code execution, creating potential Remote Code Execution (RCE) vulnerabilities when processing user inputs. Workflows containing nodes that pass user inputs into `eval` or `exec` could be exploited for various cyberattacks, including:

* Keylogging
* Ransomware
* Other malicious code execution

### subprocess for pip install

#### Policy

Runtime package installation through subprocess calls is not permitted.

#### Reasoning

* First item
  ComfyUI manager will ship with ComfyUI and lets the user install dependencies
* Centralized dependency management improves security and user experience
* Helps prevent potential supply chain attacks
* Eliminates need for multiple ComfyUI reloads

### Code Obfuscation

#### Policy

Code obfuscation is prohibited in custom nodes.

#### Reasoning

Obfuscated code:

* Impossible to review and likely to be malicious


# Node Definition JSON
Source: https://docs.comfy.org/specs/nodedef_json

JSON schema for a ComfyUI Node.

The node definition JSON is defined using [JSON Schema](https://json-schema.org/). Changes to this schema will be discussed in the [rfcs repo](https://github.com/comfy-org/rfcs).

## v2.0 (Latest)

```json Node Definition v2.0 theme={null}
{
  "$ref": "#/definitions/ComfyNodeDefV2",
  "definitions": {
    "ComfyNodeDefV2": {
      "type": "object",
      "properties": {
        "inputs": {
          "type": "object",
          "additionalProperties": {
            "anyOf": [
              {
                "type": "object",
                "properties": {
                  "default": {
                    "anyOf": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "array",
                        "items": {
                          "type": "number"
                        }
                      }
                    ]
                  },
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "min": {
                    "type": "number"
                  },
                  "max": {
                    "type": "number"
                  },
                  "step": {
                    "type": "number"
                  },
                  "display": {
                    "type": "string",
                    "enum": [
                      "slider",
                      "number",
                      "knob"
                    ]
                  },
                  "control_after_generate": {
                    "type": "boolean"
                  },
                  "type": {
                    "type": "string",
                    "const": "INT"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              },
              {
                "type": "object",
                "properties": {
                  "default": {
                    "anyOf": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "array",
                        "items": {
                          "type": "number"
                        }
                      }
                    ]
                  },
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "min": {
                    "type": "number"
                  },
                  "max": {
                    "type": "number"
                  },
                  "step": {
                    "type": "number"
                  },
                  "display": {
                    "type": "string",
                    "enum": [
                      "slider",
                      "number",
                      "knob"
                    ]
                  },
                  "round": {
                    "anyOf": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "boolean",
                        "const": false
                      }
                    ]
                  },
                  "type": {
                    "type": "string",
                    "const": "FLOAT"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              },
              {
                "type": "object",
                "properties": {
                  "default": {
                    "type": "boolean"
                  },
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "label_on": {
                    "type": "string"
                  },
                  "label_off": {
                    "type": "string"
                  },
                  "type": {
                    "type": "string",
                    "const": "BOOLEAN"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              },
              {
                "type": "object",
                "properties": {
                  "default": {
                    "type": "string"
                  },
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "multiline": {
                    "type": "boolean"
                  },
                  "dynamicPrompts": {
                    "type": "boolean"
                  },
                  "defaultVal": {
                    "type": "string"
                  },
                  "placeholder": {
                    "type": "string"
                  },
                  "type": {
                    "type": "string",
                    "const": "STRING"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              },
              {
                "type": "object",
                "properties": {
                  "default": {},
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "control_after_generate": {
                    "type": "boolean"
                  },
                  "image_upload": {
                    "type": "boolean"
                  },
                  "image_folder": {
                    "type": "string",
                    "enum": [
                      "input",
                      "output",
                      "temp"
                    ]
                  },
                  "allow_batch": {
                    "type": "boolean"
                  },
                  "video_upload": {
                    "type": "boolean"
                  },
                  "remote": {
                    "type": "object",
                    "properties": {
                      "route": {
                        "anyOf": [
                          {
                            "type": "string",
                            "format": "uri"
                          },
                          {
                            "type": "string",
                            "pattern": "^\\/"
                          }
                        ]
                      },
                      "refresh": {
                        "anyOf": [
                          {
                            "type": "number",
                            "minimum": -9007199254740991,
                            "maximum": 9007199254740991
                          },
                          {
                            "type": "number",
                            "maximum": 9007199254740991,
                            "minimum": -9007199254740991
                          }
                        ]
                      },
                      "response_key": {
                        "type": "string"
                      },
                      "query_params": {
                        "type": "object",
                        "additionalProperties": {
                          "type": "string"
                        }
                      },
                      "refresh_button": {
                        "type": "boolean"
                      },
                      "control_after_refresh": {
                        "type": "string",
                        "enum": [
                          "first",
                          "last"
                        ]
                      },
                      "timeout": {
                        "type": "number",
                        "minimum": 0
                      },
                      "max_retries": {
                        "type": "number",
                        "minimum": 0
                      }
                    },
                    "required": [
                      "route"
                    ],
                    "additionalProperties": false
                  },
                  "options": {
                    "type": "array",
                    "items": {
                      "type": [
                        "string",
                        "number"
                      ]
                    }
                  },
                  "type": {
                    "type": "string",
                    "const": "COMBO"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              },
              {
                "type": "object",
                "properties": {
                  "default": {},
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "type": {
                    "type": "string"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              }
            ]
          }
        },
        "outputs": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "index": {
                "type": "number"
              },
              "name": {
                "type": "string"
              },
              "type": {
                "type": "string"
              },
              "is_list": {
                "type": "boolean"
              },
              "options": {
                "type": "array"
              },
              "tooltip": {
                "type": "string"
              }
            },
            "required": [
              "index",
              "name",
              "type",
              "is_list"
            ],
            "additionalProperties": false
          }
        },
        "hidden": {
          "type": "object",
          "additionalProperties": {}
        },
        "name": {
          "type": "string"
        },
        "display_name": {
          "type": "string"
        },
        "description": {
          "type": "string"
        },
        "category": {
          "type": "string"
        },
        "output_node": {
          "type": "boolean"
        },
        "python_module": {
          "type": "string"
        },
        "deprecated": {
          "type": "boolean"
        },
        "experimental": {
          "type": "boolean"
        }
      },
      "required": [
        "inputs",
        "outputs",
        "name",
        "display_name",
        "description",
        "category",
        "output_node",
        "python_module"
      ],
      "additionalProperties": false
    }
  },
  "$schema": "http://json-schema.org/draft-07/schema#"
}
```


# Node Definition JSON 1.0
Source: https://docs.comfy.org/specs/nodedef_json_1_0

JSON schema for a ComfyUI Node.

## v1.0

```json Node Definition v1.0 theme={null}
{
  "$ref": "#/definitions/ComfyNodeDefV1",
  "definitions": {
    "ComfyNodeDefV1": {
      "type": "object",
      "properties": {
        "input": {
          "type": "object",
          "properties": {
            "required": {
              "type": "object",
              "additionalProperties": {
                "anyOf": [
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "INT"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "array",
                                    "items": {
                                      "type": "number"
                                    }
                                  }
                                ]
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "min": {
                                "type": "number"
                              },
                              "max": {
                                "type": "number"
                              },
                              "step": {
                                "type": "number"
                              },
                              "display": {
                                "type": "string",
                                "enum": [
                                  "slider",
                                  "number",
                                  "knob"
                                ]
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "FLOAT"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "array",
                                    "items": {
                                      "type": "number"
                                    }
                                  }
                                ]
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "min": {
                                "type": "number"
                              },
                              "max": {
                                "type": "number"
                              },
                              "step": {
                                "type": "number"
                              },
                              "display": {
                                "type": "string",
                                "enum": [
                                  "slider",
                                  "number",
                                  "knob"
                                ]
                              },
                              "round": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "boolean",
                                    "const": false
                                  }
                                ]
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "BOOLEAN"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "type": "boolean"
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "label_on": {
                                "type": "string"
                              },
                              "label_off": {
                                "type": "string"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "STRING"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "type": "string"
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "multiline": {
                                "type": "boolean"
                              },
                              "dynamicPrompts": {
                                "type": "boolean"
                              },
                              "defaultVal": {
                                "type": "string"
                              },
                              "placeholder": {
                                "type": "string"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "array",
                        "items": {
                          "type": [
                            "string",
                            "number"
                          ]
                        }
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              },
                              "image_upload": {
                                "type": "boolean"
                              },
                              "image_folder": {
                                "type": "string",
                                "enum": [
                                  "input",
                                  "output",
                                  "temp"
                                ]
                              },
                              "allow_batch": {
                                "type": "boolean"
                              },
                              "video_upload": {
                                "type": "boolean"
                              },
                              "remote": {
                                "type": "object",
                                "properties": {
                                  "route": {
                                    "anyOf": [
                                      {
                                        "type": "string",
                                        "format": "uri"
                                      },
                                      {
                                        "type": "string",
                                        "pattern": "^\\/"
                                      }
                                    ]
                                  },
                                  "refresh": {
                                    "anyOf": [
                                      {
                                        "type": "number",
                                        "minimum": -9007199254740991,
                                        "maximum": 9007199254740991
                                      },
                                      {
                                        "type": "number",
                                        "maximum": 9007199254740991,
                                        "minimum": -9007199254740991
                                      }
                                    ]
                                  },
                                  "response_key": {
                                    "type": "string"
                                  },
                                  "query_params": {
                                    "type": "object",
                                    "additionalProperties": {
                                      "type": "string"
                                    }
                                  },
                                  "refresh_button": {
                                    "type": "boolean"
                                  },
                                  "control_after_refresh": {
                                    "type": "string",
                                    "enum": [
                                      "first",
                                      "last"
                                    ]
                                  },
                                  "timeout": {
                                    "type": "number",
                                    "minimum": 0
                                  },
                                  "max_retries": {
                                    "type": "number",
                                    "minimum": 0
                                  }
                                },
                                "required": [
                                  "route"
                                ],
                                "additionalProperties": false
                              },
                              "options": {
                                "type": "array",
                                "items": {
                                  "type": [
                                    "string",
                                    "number"
                                  ]
                                }
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "COMBO"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              },
                              "image_upload": {
                                "type": "boolean"
                              },
                              "image_folder": {
                                "type": "string",
                                "enum": [
                                  "input",
                                  "output",
                                  "temp"
                                ]
                              },
                              "allow_batch": {
                                "type": "boolean"
                              },
                              "video_upload": {
                                "type": "boolean"
                              },
                              "remote": {
                                "type": "object",
                                "properties": {
                                  "route": {
                                    "anyOf": [
                                      {
                                        "type": "string",
                                        "format": "uri"
                                      },
                                      {
                                        "type": "string",
                                        "pattern": "^\\/"
                                      }
                                    ]
                                  },
                                  "refresh": {
                                    "anyOf": [
                                      {
                                        "type": "number",
                                        "minimum": -9007199254740991,
                                        "maximum": 9007199254740991
                                      },
                                      {
                                        "type": "number",
                                        "maximum": 9007199254740991,
                                        "minimum": -9007199254740991
                                      }
                                    ]
                                  },
                                  "response_key": {
                                    "type": "string"
                                  },
                                  "query_params": {
                                    "type": "object",
                                    "additionalProperties": {
                                      "type": "string"
                                    }
                                  },
                                  "refresh_button": {
                                    "type": "boolean"
                                  },
                                  "control_after_refresh": {
                                    "type": "string",
                                    "enum": [
                                      "first",
                                      "last"
                                    ]
                                  },
                                  "timeout": {
                                    "type": "number",
                                    "minimum": 0
                                  },
                                  "max_retries": {
                                    "type": "number",
                                    "minimum": 0
                                  }
                                },
                                "required": [
                                  "route"
                                ],
                                "additionalProperties": false
                              },
                              "options": {
                                "type": "array",
                                "items": {
                                  "type": [
                                    "string",
                                    "number"
                                  ]
                                }
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  }
                ]
              }
            },
            "optional": {
              "type": "object",
              "additionalProperties": {
                "anyOf": [
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "INT"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "array",
                                    "items": {
                                      "type": "number"
                                    }
                                  }
                                ]
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "min": {
                                "type": "number"
                              },
                              "max": {
                                "type": "number"
                              },
                              "step": {
                                "type": "number"
                              },
                              "display": {
                                "type": "string",
                                "enum": [
                                  "slider",
                                  "number",
                                  "knob"
                                ]
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "FLOAT"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "array",
                                    "items": {
                                      "type": "number"
                                    }
                                  }
                                ]
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "min": {
                                "type": "number"
                              },
                              "max": {
                                "type": "number"
                              },
                              "step": {
                                "type": "number"
                              },
                              "display": {
                                "type": "string",
                                "enum": [
                                  "slider",
                                  "number",
                                  "knob"
                                ]
                              },
                              "round": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "boolean",
                                    "const": false
                                  }
                                ]
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "BOOLEAN"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "type": "boolean"
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "label_on": {
                                "type": "string"
                              },
                              "label_off": {
                                "type": "string"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "STRING"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "type": "string"
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "multiline": {
                                "type": "boolean"
                              },
                              "dynamicPrompts": {
                                "type": "boolean"
                              },
                              "defaultVal": {
                                "type": "string"
                              },
                              "placeholder": {
                                "type": "string"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "array",
                        "items": {
                          "type": [
                            "string",
                            "number"
                          ]
                        }
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              },
                              "image_upload": {
                                "type": "boolean"
                              },
                              "image_folder": {
                                "type": "string",
                                "enum": [
                                  "input",
                                  "output",
                                  "temp"
                                ]
                              },
                              "allow_batch": {
                                "type": "boolean"
                              },
                              "video_upload": {
                                "type": "boolean"
                              },
                              "remote": {
                                "type": "object",
                                "properties": {
                                  "route": {
                                    "anyOf": [
                                      {
                                        "type": "string",
                                        "format": "uri"
                                      },
                                      {
                                        "type": "string",
                                        "pattern": "^\\/"
                                      }
                                    ]
                                  },
                                  "refresh": {
                                    "anyOf": [
                                      {
                                        "type": "number",
                                        "minimum": -9007199254740991,
                                        "maximum": 9007199254740991
                                      },
                                      {
                                        "type": "number",
                                        "maximum": 9007199254740991,
                                        "minimum": -9007199254740991
                                      }
                                    ]
                                  },
                                  "response_key": {
                                    "type": "string"
                                  },
                                  "query_params": {
                                    "type": "object",
                                    "additionalProperties": {
                                      "type": "string"
                                    }
                                  },
                                  "refresh_button": {
                                    "type": "boolean"
                                  },
                                  "control_after_refresh": {
                                    "type": "string",
                                    "enum": [
                                      "first",
                                      "last"
                                    ]
                                  },
                                  "timeout": {
                                    "type": "number",
                                    "minimum": 0
                                  },
                                  "max_retries": {
                                    "type": "number",
                                    "minimum": 0
                                  }
                                },
                                "required": [
                                  "route"
                                ],
                                "additionalProperties": false
                              },
                              "options": {
                                "type": "array",
                                "items": {
                                  "type": [
                                    "string",
                                    "number"
                                  ]
                                }
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "COMBO"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              },
                              "image_upload": {
                                "type": "boolean"
                              },
                              "image_folder": {
                                "type": "string",
                                "enum": [
                                  "input",
                                  "output",
                                  "temp"
                                ]
                              },
                              "allow_batch": {
                                "type": "boolean"
                              },
                              "video_upload": {
                                "type": "boolean"
                              },
                              "remote": {
                                "type": "object",
                                "properties": {
                                  "route": {
                                    "anyOf": [
                                      {
                                        "type": "string",
                                        "format": "uri"
                                      },
                                      {
                                        "type": "string",
                                        "pattern": "^\\/"
                                      }
                                    ]
                                  },
                                  "refresh": {
                                    "anyOf": [
                                      {
                                        "type": "number",
                                        "minimum": -9007199254740991,
                                        "maximum": 9007199254740991
                                      },
                                      {
                                        "type": "number",
                                        "maximum": 9007199254740991,
                                        "minimum": -9007199254740991
                                      }
                                    ]
                                  },
                                  "response_key": {
                                    "type": "string"
                                  },
                                  "query_params": {
                                    "type": "object",
                                    "additionalProperties": {
                                      "type": "string"
                                    }
                                  },
                                  "refresh_button": {
                                    "type": "boolean"
                                  },
                                  "control_after_refresh": {
                                    "type": "string",
                                    "enum": [
                                      "first",
                                      "last"
                                    ]
                                  },
                                  "timeout": {
                                    "type": "number",
                                    "minimum": 0
                                  },
                                  "max_retries": {
                                    "type": "number",
                                    "minimum": 0
                                  }
                                },
                                "required": [
                                  "route"
                                ],
                                "additionalProperties": false
                              },
                              "options": {
                                "type": "array",
                                "items": {
                                  "type": [
                                    "string",
                                    "number"
                                  ]
                                }
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  }
                ]
              }
            },
            "hidden": {
              "type": "object",
              "additionalProperties": {}
            }
          },
          "additionalProperties": false
        },
        "output": {
          "type": "array",
          "items": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "array",
                "items": {
                  "type": [
                    "string",
                    "number"
                  ]
                }
              }
            ]
          }
        },
        "output_is_list": {
          "type": "array",
          "items": {
            "type": "boolean"
          }
        },
        "output_name": {
          "type": "array",
          "items": {
            "type": "string"
          }
        },
        "output_tooltips": {
          "type": "array",
          "items": {
            "type": "string"
          }
        },
        "name": {
          "type": "string"
        },
        "display_name": {
          "type": "string"
        },
        "description": {
          "type": "string"
        },
        "category": {
          "type": "string"
        },
        "output_node": {
          "type": "boolean"
        },
        "python_module": {
          "type": "string"
        },
        "deprecated": {
          "type": "boolean"
        },
        "experimental": {
          "type": "boolean"
        }
      },
      "required": [
        "name",
        "display_name",
        "description",
        "category",
        "output_node",
        "python_module"
      ],
      "additionalProperties": false
    }
  },
  "$schema": "http://json-schema.org/draft-07/schema#"
}
```


# Workflow JSON
Source: https://docs.comfy.org/specs/workflow_json

JSON schema for a ComfyUI workflow.

The workflow JSON is defined using [JSON Schema](https://json-schema.org/). Changes to this schema will be discussed in the [rfcs repo](https://github.com/comfy-org/rfcs).

## Version 1.0 (Latest)

```json ComfyUI Workflow v1.0 theme={null}
{
  "$ref": "#/definitions/ComfyWorkflow1_0",
  "definitions": {
    "ComfyWorkflow1_0": {
      "type": "object",
      "properties": {
        "version": {
          "type": "number",
          "const": 1
        },
        "config": {
          "anyOf": [
            {
              "anyOf": [
                {
                  "not": {}
                },
                {
                  "type": "object",
                  "properties": {
                    "links_ontop": {
                      "type": "boolean"
                    },
                    "align_to_grid": {
                      "type": "boolean"
                    }
                  },
                  "additionalProperties": true
                }
              ]
            },
            {
              "type": "null"
            }
          ]
        },
        "state": {
          "type": "object",
          "properties": {
            "lastGroupid": {
              "type": "number"
            },
            "lastNodeId": {
              "type": "number"
            },
            "lastLinkId": {
              "type": "number"
            },
            "lastRerouteId": {
              "type": "number"
            }
          },
          "additionalProperties": true
        },
        "groups": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "title": {
                "type": "string"
              },
              "bounding": {
                "type": "array",
                "minItems": 4,
                "maxItems": 4,
                "items": [
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  }
                ]
              },
              "color": {
                "type": "string"
              },
              "font_size": {
                "type": "number"
              },
              "locked": {
                "type": "boolean"
              }
            },
            "required": [
              "title",
              "bounding"
            ],
            "additionalProperties": true
          }
        },
        "nodes": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "id": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "type": {
                "type": "string"
              },
              "pos": {
                "anyOf": [
                  {
                    "type": "object",
                    "properties": {
                      "0": {
                        "type": "number"
                      },
                      "1": {
                        "type": "number"
                      }
                    },
                    "required": [
                      "0",
                      "1"
                    ],
                    "additionalProperties": true
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "number"
                      }
                    ]
                  }
                ]
              },
              "size": {
                "anyOf": [
                  {
                    "type": "object",
                    "properties": {
                      "0": {
                        "type": "number"
                      },
                      "1": {
                        "type": "number"
                      }
                    },
                    "required": [
                      "0",
                      "1"
                    ],
                    "additionalProperties": true
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "number"
                      }
                    ]
                  }
                ]
              },
              "flags": {
                "type": "object",
                "properties": {
                  "collapsed": {
                    "type": "boolean"
                  },
                  "pinned": {
                    "type": "boolean"
                  },
                  "allow_interaction": {
                    "type": "boolean"
                  },
                  "horizontal": {
                    "type": "boolean"
                  },
                  "skip_repeated_outputs": {
                    "type": "boolean"
                  }
                },
                "additionalProperties": true
              },
              "order": {
                "type": "number"
              },
              "mode": {
                "type": "number"
              },
              "inputs": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "name": {
                      "type": "string"
                    },
                    "type": {
                      "anyOf": [
                        {
                          "type": "string"
                        },
                        {
                          "type": "array",
                          "items": {
                            "type": "string"
                          }
                        },
                        {
                          "type": "number"
                        }
                      ]
                    },
                    "link": {
                      "type": [
                        "number",
                        "null"
                      ]
                    },
                    "slot_index": {
                      "anyOf": [
                        {
                          "type": "integer"
                        },
                        {
                          "type": "string"
                        }
                      ]
                    }
                  },
                  "required": [
                    "name",
                    "type"
                  ],
                  "additionalProperties": true
                }
              },
              "outputs": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "name": {
                      "type": "string"
                    },
                    "type": {
                      "anyOf": [
                        {
                          "type": "string"
                        },
                        {
                          "type": "array",
                          "items": {
                            "type": "string"
                          }
                        },
                        {
                          "type": "number"
                        }
                      ]
                    },
                    "links": {
                      "anyOf": [
                        {
                          "type": "array",
                          "items": {
                            "type": "number"
                          }
                        },
                        {
                          "type": "null"
                        }
                      ]
                    },
                    "slot_index": {
                      "anyOf": [
                        {
                          "type": "integer"
                        },
                        {
                          "type": "string"
                        }
                      ]
                    }
                  },
                  "required": [
                    "name",
                    "type"
                  ],
                  "additionalProperties": true
                }
              },
              "properties": {
                "type": "object",
                "properties": {
                  "Node name for S&R": {
                    "type": "string"
                  }
                },
                "additionalProperties": true
              },
              "widgets_values": {
                "anyOf": [
                  {
                    "type": "array"
                  },
                  {
                    "type": "object",
                    "additionalProperties": {}
                  }
                ]
              },
              "color": {
                "type": "string"
              },
              "bgcolor": {
                "type": "string"
              }
            },
            "required": [
              "id",
              "type",
              "pos",
              "size",
              "flags",
              "order",
              "mode",
              "properties"
            ],
            "additionalProperties": true
          }
        },
        "links": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "id": {
                "type": "number"
              },
              "origin_id": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "origin_slot": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "target_id": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "target_slot": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "type": {
                "anyOf": [
                  {
                    "type": "string"
                  },
                  {
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  {
                    "type": "number"
                  }
                ]
              },
              "parentId": {
                "type": "number"
              }
            },
            "required": [
              "id",
              "origin_id",
              "origin_slot",
              "target_id",
              "target_slot",
              "type"
            ],
            "additionalProperties": true
          }
        },
        "reroutes": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "id": {
                "type": "number"
              },
              "parentId": {
                "type": "number"
              },
              "pos": {
                "anyOf": [
                  {
                    "type": "object",
                    "properties": {
                      "0": {
                        "type": "number"
                      },
                      "1": {
                        "type": "number"
                      }
                    },
                    "required": [
                      "0",
                      "1"
                    ],
                    "additionalProperties": true
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "number"
                      }
                    ]
                  }
                ]
              },
              "linkIds": {
                "anyOf": [
                  {
                    "type": "array",
                    "items": {
                      "type": "number"
                    }
                  },
                  {
                    "type": "null"
                  }
                ]
              }
            },
            "required": [
              "id",
              "pos"
            ],
            "additionalProperties": true
          }
        },
        "extra": {
          "anyOf": [
            {
              "anyOf": [
                {
                  "not": {}
                },
                {
                  "type": "object",
                  "properties": {
                    "ds": {
                      "type": "object",
                      "properties": {
                        "scale": {
                          "type": "number"
                        },
                        "offset": {
                          "anyOf": [
                            {
                              "type": "object",
                              "properties": {
                                "0": {
                                  "type": "number"
                                },
                                "1": {
                                  "type": "number"
                                }
                              },
                              "required": [
                                "0",
                                "1"
                              ],
                              "additionalProperties": true
                            },
                            {
                              "type": "array",
                              "minItems": 2,
                              "maxItems": 2,
                              "items": [
                                {
                                  "type": "number"
                                },
                                {
                                  "type": "number"
                                }
                              ]
                            }
                          ]
                        }
                      },
                      "required": [
                        "scale",
                        "offset"
                      ],
                      "additionalProperties": true
                    },
                    "info": {
                      "type": "object",
                      "properties": {
                        "name": {
                          "type": "string"
                        },
                        "author": {
                          "type": "string"
                        },
                        "description": {
                          "type": "string"
                        },
                        "version": {
                          "type": "string"
                        },
                        "created": {
                          "type": "string"
                        },
                        "modified": {
                          "type": "string"
                        },
                        "software": {
                          "type": "string"
                        }
                      },
                      "required": [
                        "name",
                        "author",
                        "description",
                        "version",
                        "created",
                        "modified",
                        "software"
                      ],
                      "additionalProperties": true
                    },
                    "linkExtensions": {
                      "type": "array",
                      "items": {
                        "type": "object",
                        "properties": {
                          "id": {
                            "type": "number"
                          },
                          "parentId": {
                            "type": "number"
                          }
                        },
                        "required": [
                          "id",
                          "parentId"
                        ],
                        "additionalProperties": true
                      }
                    },
                    "reroutes": {
                      "type": "array",
                      "items": {
                        "type": "object",
                        "properties": {
                          "id": {
                            "type": "number"
                          },
                          "parentId": {
                            "type": "number"
                          },
                          "pos": {
                            "anyOf": [
                              {
                                "type": "object",
                                "properties": {
                                  "0": {
                                    "type": "number"
                                  },
                                  "1": {
                                    "type": "number"
                                  }
                                },
                                "required": [
                                  "0",
                                  "1"
                                ],
                                "additionalProperties": true
                              },
                              {
                                "type": "array",
                                "minItems": 2,
                                "maxItems": 2,
                                "items": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "number"
                                  }
                                ]
                              }
                            ]
                          },
                          "linkIds": {
                            "anyOf": [
                              {
                                "type": "array",
                                "items": {
                                  "type": "number"
                                }
                              },
                              {
                                "type": "null"
                              }
                            ]
                          }
                        },
                        "required": [
                          "id",
                          "pos"
                        ],
                        "additionalProperties": true
                      }
                    }
                  },
                  "additionalProperties": true
                }
              ]
            },
            {
              "type": "null"
            }
          ]
        },
        "models": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "name": {
                "type": "string"
              },
              "url": {
                "type": "string",
                "format": "uri"
              },
              "hash": {
                "type": "string"
              },
              "hash_type": {
                "type": "string"
              },
              "directory": {
                "type": "string"
              }
            },
            "required": [
              "name",
              "url",
              "directory"
            ],
            "additionalProperties": false
          }
        }
      },
      "required": [
        "version",
        "state",
        "nodes"
      ],
      "additionalProperties": true
    }
  },
  "$schema": "http://json-schema.org/draft-07/schema#"
}
```

## Older versions

* [0.4](./workflow_json_0.4)


# Workflow JSON 0.4
Source: https://docs.comfy.org/specs/workflow_json_0.4

JSON schema for a ComfyUI workflow.

## v0.4

```json  theme={null}
{
  "$ref": "#/definitions/ComfyWorkflow0_4",
  "definitions": {
    "ComfyWorkflow0_4": {
      "type": "object",
      "properties": {
        "last_node_id": {
          "anyOf": [
            {
              "type": "integer"
            },
            {
              "type": "string"
            }
          ]
        },
        "last_link_id": {
          "type": "number"
        },
        "nodes": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "id": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "type": {
                "type": "string"
              },
              "pos": {
                "anyOf": [
                  {
                    "type": "object",
                    "properties": {
                      "0": {
                        "type": "number"
                      },
                      "1": {
                        "type": "number"
                      }
                    },
                    "required": [
                      "0",
                      "1"
                    ],
                    "additionalProperties": true
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "number"
                      }
                    ]
                  }
                ]
              },
              "size": {
                "anyOf": [
                  {
                    "type": "object",
                    "properties": {
                      "0": {
                        "type": "number"
                      },
                      "1": {
                        "type": "number"
                      }
                    },
                    "required": [
                      "0",
                      "1"
                    ],
                    "additionalProperties": true
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "number"
                      }
                    ]
                  }
                ]
              },
              "flags": {
                "type": "object",
                "properties": {
                  "collapsed": {
                    "type": "boolean"
                  },
                  "pinned": {
                    "type": "boolean"
                  },
                  "allow_interaction": {
                    "type": "boolean"
                  },
                  "horizontal": {
                    "type": "boolean"
                  },
                  "skip_repeated_outputs": {
                    "type": "boolean"
                  }
                },
                "additionalProperties": true
              },
              "order": {
                "type": "number"
              },
              "mode": {
                "type": "number"
              },
              "inputs": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "name": {
                      "type": "string"
                    },
                    "type": {
                      "anyOf": [
                        {
                          "type": "string"
                        },
                        {
                          "type": "array",
                          "items": {
                            "type": "string"
                          }
                        },
                        {
                          "type": "number"
                        }
                      ]
                    },
                    "link": {
                      "type": [
                        "number",
                        "null"
                      ]
                    },
                    "slot_index": {
                      "anyOf": [
                        {
                          "type": "integer"
                        },
                        {
                          "type": "string"
                        }
                      ]
                    }
                  },
                  "required": [
                    "name",
                    "type"
                  ],
                  "additionalProperties": true
                }
              },
              "outputs": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "name": {
                      "type": "string"
                    },
                    "type": {
                      "anyOf": [
                        {
                          "type": "string"
                        },
                        {
                          "type": "array",
                          "items": {
                            "type": "string"
                          }
                        },
                        {
                          "type": "number"
                        }
                      ]
                    },
                    "links": {
                      "anyOf": [
                        {
                          "type": "array",
                          "items": {
                            "type": "number"
                          }
                        },
                        {
                          "type": "null"
                        }
                      ]
                    },
                    "slot_index": {
                      "anyOf": [
                        {
                          "type": "integer"
                        },
                        {
                          "type": "string"
                        }
                      ]
                    }
                  },
                  "required": [
                    "name",
                    "type"
                  ],
                  "additionalProperties": true
                }
              },
              "properties": {
                "type": "object",
                "properties": {
                  "Node name for S&R": {
                    "type": "string"
                  }
                },
                "additionalProperties": true
              },
              "widgets_values": {
                "anyOf": [
                  {
                    "type": "array"
                  },
                  {
                    "type": "object",
                    "additionalProperties": {}
                  }
                ]
              },
              "color": {
                "type": "string"
              },
              "bgcolor": {
                "type": "string"
              }
            },
            "required": [
              "id",
              "type",
              "pos",
              "size",
              "flags",
              "order",
              "mode",
              "properties"
            ],
            "additionalProperties": true
          }
        },
        "links": {
          "type": "array",
          "items": {
            "type": "array",
            "minItems": 6,
            "maxItems": 6,
            "items": [
              {
                "type": "number"
              },
              {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              {
                "anyOf": [
                  {
                    "type": "string"
                  },
                  {
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  {
                    "type": "number"
                  }
                ]
              }
            ]
          }
        },
        "groups": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "title": {
                "type": "string"
              },
              "bounding": {
                "type": "array",
                "minItems": 4,
                "maxItems": 4,
                "items": [
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  }
                ]
              },
              "color": {
                "type": "string"
              },
              "font_size": {
                "type": "number"
              },
              "locked": {
                "type": "boolean"
              }
            },
            "required": [
              "title",
              "bounding"
            ],
            "additionalProperties": true
          }
        },
        "config": {
          "anyOf": [
            {
              "anyOf": [
                {
                  "not": {}
                },
                {
                  "type": "object",
                  "properties": {
                    "links_ontop": {
                      "type": "boolean"
                    },
                    "align_to_grid": {
                      "type": "boolean"
                    }
                  },
                  "additionalProperties": true
                }
              ]
            },
            {
              "type": "null"
            }
          ]
        },
        "extra": {
          "anyOf": [
            {
              "anyOf": [
                {
                  "not": {}
                },
                {
                  "type": "object",
                  "properties": {
                    "ds": {
                      "type": "object",
                      "properties": {
                        "scale": {
                          "type": "number"
                        },
                        "offset": {
                          "anyOf": [
                            {
                              "type": "object",
                              "properties": {
                                "0": {
                                  "type": "number"
                                },
                                "1": {
                                  "type": "number"
                                }
                              },
                              "required": [
                                "0",
                                "1"
                              ],
                              "additionalProperties": true
                            },
                            {
                              "type": "array",
                              "minItems": 2,
                              "maxItems": 2,
                              "items": [
                                {
                                  "type": "number"
                                },
                                {
                                  "type": "number"
                                }
                              ]
                            }
                          ]
                        }
                      },
                      "required": [
                        "scale",
                        "offset"
                      ],
                      "additionalProperties": true
                    },
                    "info": {
                      "type": "object",
                      "properties": {
                        "name": {
                          "type": "string"
                        },
                        "author": {
                          "type": "string"
                        },
                        "description": {
                          "type": "string"
                        },
                        "version": {
                          "type": "string"
                        },
                        "created": {
                          "type": "string"
                        },
                        "modified": {
                          "type": "string"
                        },
                        "software": {
                          "type": "string"
                        }
                      },
                      "required": [
                        "name",
                        "author",
                        "description",
                        "version",
                        "created",
                        "modified",
                        "software"
                      ],
                      "additionalProperties": true
                    },
                    "linkExtensions": {
                      "type": "array",
                      "items": {
                        "type": "object",
                        "properties": {
                          "id": {
                            "type": "number"
                          },
                          "parentId": {
                            "type": "number"
                          }
                        },
                        "required": [
                          "id",
                          "parentId"
                        ],
                        "additionalProperties": true
                      }
                    },
                    "reroutes": {
                      "type": "array",
                      "items": {
                        "type": "object",
                        "properties": {
                          "id": {
                            "type": "number"
                          },
                          "parentId": {
                            "type": "number"
                          },
                          "pos": {
                            "anyOf": [
                              {
                                "type": "object",
                                "properties": {
                                  "0": {
                                    "type": "number"
                                  },
                                  "1": {
                                    "type": "number"
                                  }
                                },
                                "required": [
                                  "0",
                                  "1"
                                ],
                                "additionalProperties": true
                              },
                              {
                                "type": "array",
                                "minItems": 2,
                                "maxItems": 2,
                                "items": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "number"
                                  }
                                ]
                              }
                            ]
                          },
                          "linkIds": {
                            "anyOf": [
                              {
                                "type": "array",
                                "items": {
                                  "type": "number"
                                }
                              },
                              {
                                "type": "null"
                              }
                            ]
                          }
                        },
                        "required": [
                          "id",
                          "pos"
                        ],
                        "additionalProperties": true
                      }
                    }
                  },
                  "additionalProperties": true
                }
              ]
            },
            {
              "type": "null"
            }
          ]
        },
        "version": {
          "type": "number"
        },
        "models": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "name": {
                "type": "string"
              },
              "url": {
                "type": "string",
                "format": "uri"
              },
              "hash": {
                "type": "string"
              },
              "hash_type": {
                "type": "string"
              },
              "directory": {
                "type": "string"
              }
            },
            "required": [
              "name",
              "url",
              "directory"
            ],
            "additionalProperties": false
          }
        }
      },
      "required": [
        "last_node_id",
        "last_link_id",
        "nodes",
        "links",
        "version"
      ],
      "additionalProperties": true
    }
  },
  "$schema": "http://json-schema.org/draft-07/schema#"
}
```


# Contact support
Source: https://docs.comfy.org/support/contact-support

Get help and report issues for ComfyUI

Get help with ComfyUI through our support channels and report issues in the appropriate repository.

## Support channels

<CardGroup cols={2}>
  <Card title="Zendesk Support" icon="life-ring" href="https://support.comfy.org/">
    Get help from our support team
  </Card>
</CardGroup>

## Report issues

Report bugs and issues in the corresponding GitHub repository:

<CardGroup cols={2}>
  <Card title="UI Issues" icon="browser" href="https://github.com/Comfy-Org/ComfyUI_frontend/issues">
    Report UI-related issues in ComfyUI\_frontend
  </Card>

  <Card title="Core Issues" icon="code" href="https://github.com/comfyanonymous/ComfyUI/issues">
    Report core functionality issues
  </Card>

  <Card title="Desktop Issues" icon="desktop" href="https://github.com/Comfy-Org/desktop/issues">
    Report issues with the desktop version
  </Card>
</CardGroup>


# Accepted payment methods
Source: https://docs.comfy.org/support/payment/accepted-payment-methods

Learn about the payment methods accepted by Comfy Organization Inc

Comfy Organization Inc uses Stripe as our payment processor, which means you can only use payment methods that Stripe accepts. Generally, you can pay with major credit and debit cards like Visa, Mastercard, and American Express, as well as some digital wallets such as Google Pay and Link in some regions. The options you see during checkout are the ones available to you.

## Credit and debit cards

We accept the following major credit and debit cards:

* Visa
* Mastercard
* American Express

## Digital wallets

You can also pay using digital wallet services:

* Google Pay
* Link

## Alternative payment methods

We also support the following payment methods, but only in USD:

* Alipay
* WeChat Pay

<Note>
  WeChat Pay and Alipay are only available in specific regions. Check the `Business locations` section in the Stripe documentation to verify availability in your country:

  * [Stripe WeChat Pay documentation](https://docs.stripe.com/payments/wechat-pay#get-started)
  * [Stripe Alipay documentation](https://docs.stripe.com/payments/alipay)
</Note>

<img src="https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/active-wechat-and-alipay.jpg?fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=d43ddcb60a99cb5d33822e900003c997" alt="Checkout screen showing USD selected and Alipay/WeChat Pay options available" data-og-width="1640" width="1640" data-og-height="1294" height="1294" data-path="images/support/payment/active-wechat-and-alipay.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/active-wechat-and-alipay.jpg?w=280&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=f54535c6c1a8e625f59aa9b3a945c40f 280w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/active-wechat-and-alipay.jpg?w=560&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=7f6db4e1b2c33ce730f3e0a40feda94d 560w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/active-wechat-and-alipay.jpg?w=840&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=71e5cca88841e884f54ccc0f5d5f70bc 840w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/active-wechat-and-alipay.jpg?w=1100&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=18e4b5802e8ff9963031db99a6efecd9 1100w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/active-wechat-and-alipay.jpg?w=1650&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=66aaaf12d29ceb43513f4d86515e454b 1650w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/active-wechat-and-alipay.jpg?w=2500&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=3873782a23bcb7c0028e47d5f0d5968e 2500w" />
*Select **USD** as your payment currency (1) to unlock the Alipay and WeChat Pay options (2) during checkout.*

<Note>
  Alipay and WeChat Pay are not yet supported for Comfy Cloud Subscription.
</Note>

To enable these options, choose **USD** in the Stripe billing portal currency selector. Once USD is selected, Alipay and WeChat Pay appear in the payment method list.

<img src="https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/active-wechat-and-alipay.jpg?fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=d43ddcb60a99cb5d33822e900003c997" alt="Stripe payment form showing USD currency selection and Alipay/WeChat options" data-og-width="1640" width="1640" data-og-height="1294" height="1294" data-path="images/support/payment/active-wechat-and-alipay.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/active-wechat-and-alipay.jpg?w=280&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=f54535c6c1a8e625f59aa9b3a945c40f 280w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/active-wechat-and-alipay.jpg?w=560&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=7f6db4e1b2c33ce730f3e0a40feda94d 560w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/active-wechat-and-alipay.jpg?w=840&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=71e5cca88841e884f54ccc0f5d5f70bc 840w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/active-wechat-and-alipay.jpg?w=1100&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=18e4b5802e8ff9963031db99a6efecd9 1100w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/active-wechat-and-alipay.jpg?w=1650&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=66aaaf12d29ceb43513f4d86515e454b 1650w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/active-wechat-and-alipay.jpg?w=2500&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=3873782a23bcb7c0028e47d5f0d5968e 2500w" />

## Requirements

To successfully process your payment:

* Your card must have sufficient funds or available credit
* Your billing address must match the address on file with your card issuer
* International cards are accepted, but may be subject to currency conversion fees from your bank

## Security

Stripe is a PCI Service Provider Level 1, the most stringent level of certification available in the payments industry. All payment information is processed securely using industry-standard encryption. Comfy Organization Inc does not store your complete card details on our servers.

## Payment processing

Payments are processed immediately upon subscription or renewal. You will receive a confirmation email once your payment has been successfully processed.

If you experience any issues with payment processing, please contact our [support team](https://support.comfy.org/) for assistance.


# Editing your payment information
Source: https://docs.comfy.org/support/payment/editing-payment-information

Learn how to update your payment method and billing details

You can update your payment information at any time through your account settings.

## How to edit payment information

1. Log in to your ComfyUI account and open the profile menu
2. Click **User Settings**
3. Select the **Credits** tab in the settings panel
4. Choose **Invoice History** to open the Stripe billing portal in a new tab
5. Click **Update information** and edit your billing or payment details
6. Save your changes in the Stripe portal

### Visual walkthrough

<img src="https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-1-account.png?fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=7c4c0333b413dfe0042c639c05a613e7" alt="Profile menu highlighting the User Settings option" data-og-width="671" width="671" data-og-height="878" height="878" data-path="images/support/payment/billing-1-account.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-1-account.png?w=280&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=f2acac78bf7b27fbb7f615304dd3cabe 280w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-1-account.png?w=560&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=13e09fcc47881a6260cd0304a2415a44 560w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-1-account.png?w=840&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=deb33f98765a74fd122a135a0ce74df5 840w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-1-account.png?w=1100&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=6ca7960d66f0c98c7e82e06d2fe4ea06 1100w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-1-account.png?w=1650&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=2cfd5ee9f2218bd16ea94a924f73dbb6 1650w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-1-account.png?w=2500&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=812cf61e231cfb121cc032363bda02fa 2500w" />
*Open the profile menu and choose **User Settings** to manage billing.*

<img src="https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-2-credits.png?fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=ab359343637977e08e19743d1ffe6691" alt="Credits tab in settings with Invoice History button highlighted" data-og-width="1640" width="1640" data-og-height="838" height="838" data-path="images/support/payment/billing-2-credits.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-2-credits.png?w=280&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=2c62abe6cb097cd75d3bf5cf1c0de245 280w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-2-credits.png?w=560&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=d8c4a786bd3bf06d18a39e8b57ba6145 560w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-2-credits.png?w=840&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=8f3a829cf25e4124f220097d4909f975 840w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-2-credits.png?w=1100&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=24085f467f806b297024d97fab3b4091 1100w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-2-credits.png?w=1650&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=dd2488fdfaaa38d1684af0d7dbf8c1f5 1650w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-2-credits.png?w=2500&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=6b05b4f152b7de8cf80651ee295659d0 2500w" />
*Go to the **Credits** tab and select **Invoice History**.*

<img src="https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-3-update-ingformation.png?fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=7e23190c02b7f49b25b928924585446d" alt="Stripe billing information form ready for updates" data-og-width="1640" width="1640" data-og-height="1271" height="1271" data-path="images/support/payment/billing-3-update-ingformation.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-3-update-ingformation.png?w=280&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=01a36084ec45521d30a1e3f46d84336c 280w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-3-update-ingformation.png?w=560&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=ba144722dbb5976b6ae416e1d35cbed5 560w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-3-update-ingformation.png?w=840&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=5f2b3a9cfc8ab99c5ef876fcaeacbb1b 840w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-3-update-ingformation.png?w=1100&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=aa0036044c8f577dcd5d0ea99de7c141 1100w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-3-update-ingformation.png?w=1650&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=b5b6c38f281adf6d44e8ee6a56b2a357 1650w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-3-update-ingformation.png?w=2500&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=dfda46c760c62a2983d8fc388c484034 2500w" />
*Review the billing information fields in the Stripe portal.*

<img src="https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-4-update-ingformation.png?fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=31fbfdc473967348a335d1f080276006" alt="Stripe portal with Update information button emphasized" data-og-width="1640" width="1640" data-og-height="1271" height="1271" data-path="images/support/payment/billing-4-update-ingformation.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-4-update-ingformation.png?w=280&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=9f893a0a28b8612d2b3da820f2a24190 280w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-4-update-ingformation.png?w=560&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=1b26159c5f9fab4c9cd343fe53780372 560w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-4-update-ingformation.png?w=840&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=c19ef53a01ade89ea3f01771c207df6f 840w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-4-update-ingformation.png?w=1100&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=fb91ff69a0843c52a246a5f38bcf5fbf 1100w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-4-update-ingformation.png?w=1650&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=a62c7aeead2002be1017d349f87acede 1650w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-4-update-ingformation.png?w=2500&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=c4b9a5cd7bd257da917ccef1d8f9e03a 2500w" />
*Click **Update information** to edit payment details and save your changes.*

## What you can update

You can modify the following payment details:

* Credit or debit card number
* Card expiration date
* CVV/security code
* Billing address
* Payment method type (switch between card and digital wallet)

## When changes take effect

* Changes to your payment method take effect immediately
* Your next billing cycle will use the updated payment information
* Updates to billing details (name, address, tax ID) only apply to future invoices; existing invoices cannot be modified
* Active subscriptions will not be interrupted when you update payment details

## Important notes

* You must have an active payment method on file to maintain your subscription
* If you remove a payment method, you must add a new one before the next billing date
* Keep a copy of past invoices before making changes if you need them for record keeping
* Updating your payment information does not change your billing date or subscription plan

## Troubleshooting

If you encounter issues updating your payment information:

* Verify that all information is entered correctly
* Ensure your card has not expired
* Check that your billing address matches your card issuer's records
* Try using a different browser or clearing your cache
* Contact our [support team](https://support.comfy.org/) if problems persist


# Invoice information
Source: https://docs.comfy.org/support/payment/invoice-information

Learn how your invoice information is managed

Your invoice is based on the information you provide to Stripe during checkout. The details you enter will appear on all your invoices.

<Card title="Update your payment information" icon="credit-card" href="/support/payment/editing-payment-information">
  Learn how to update your billing details and payment information
</Card>

## Important notes

* Changes to your payment information only apply to **future invoices**
* Unfortunately, we cannot change invoices that have already been issued
* Ensure that the payment details you enter at checkout are the ones you want to see on your invoice


# Payment currency
Source: https://docs.comfy.org/support/payment/payment-currency

Information about supported currencies and currency conversion

Comfy Organization Inc processes payments in multiple currencies to serve our global user base.

## Primary currency

All subscription prices are listed in **US Dollars (USD)** by default.

## Payment method availability

### Alipay and WeChat Pay

* **Alipay** and **WeChat Pay** are currently only available for local API credits
* These payment methods are not yet supported for Comfy Cloud subscriptions
* Alipay support for Cloud subscriptions will be available soon

## Currency selection

Your payment currency is typically determined by:

* Your account location
* Your billing address
* Your payment method's default currency

## Currency conversion

### Automatic conversion

If your card's currency differs from the billing currency:

* Your bank or card issuer will automatically convert the amount
* Conversion rates are set by your financial institution
* Additional currency conversion fees may apply

### Conversion fees

* Currency conversion fees are charged by your bank, not by Comfy Organization Inc
* These fees typically range from 1-3% of the transaction amount
* Check with your bank for specific conversion fee rates

## Viewing charges in your currency

To see the approximate charge in your local currency:

1. Check your bank or card statement
2. Use your bank's currency converter
3. Note that exchange rates fluctuate daily

## Changing your billing currency

To change your billing currency:

1. Contact our [support team](https://support.comfy.org/)
2. Provide your preferred currency
3. Note that currency changes may affect your subscription price due to exchange rates

## Price variations by region

Subscription prices may vary slightly by region due to:

* Local tax requirements
* Currency exchange rates
* Regional pricing adjustments

## Tax and VAT

Depending on your location, additional charges may apply:

* Sales tax (US customers)
* VAT (European customers)
* GST (Australian, Canadian, and other applicable regions)

These taxes are calculated based on your billing address and added to your total.


# Viewing your payment history
Source: https://docs.comfy.org/support/payment/payment-history

Access and review your past payments and invoices

You can view your complete payment history and download invoices through your account dashboard.

## Accessing payment history

1. Log in to your ComfyUI account and open the profile menu
2. Click **User Settings**
3. Select the **Credits** tab in the settings panel
4. Click **Invoice History** to open the Stripe billing portal and view your payment history

### Visual walkthrough

<img src="https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-1-account.png?fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=7c4c0333b413dfe0042c639c05a613e7" alt="Profile menu highlighting the User Settings option" data-og-width="671" width="671" data-og-height="878" height="878" data-path="images/support/payment/billing-1-account.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-1-account.png?w=280&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=f2acac78bf7b27fbb7f615304dd3cabe 280w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-1-account.png?w=560&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=13e09fcc47881a6260cd0304a2415a44 560w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-1-account.png?w=840&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=deb33f98765a74fd122a135a0ce74df5 840w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-1-account.png?w=1100&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=6ca7960d66f0c98c7e82e06d2fe4ea06 1100w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-1-account.png?w=1650&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=2cfd5ee9f2218bd16ea94a924f73dbb6 1650w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-1-account.png?w=2500&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=812cf61e231cfb121cc032363bda02fa 2500w" />
*Open the profile menu and choose **User Settings** to manage billing.*

<img src="https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-2-credits.png?fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=ab359343637977e08e19743d1ffe6691" alt="Credits tab in settings with Invoice History button highlighted" data-og-width="1640" width="1640" data-og-height="838" height="838" data-path="images/support/payment/billing-2-credits.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-2-credits.png?w=280&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=2c62abe6cb097cd75d3bf5cf1c0de245 280w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-2-credits.png?w=560&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=d8c4a786bd3bf06d18a39e8b57ba6145 560w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-2-credits.png?w=840&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=8f3a829cf25e4124f220097d4909f975 840w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-2-credits.png?w=1100&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=24085f467f806b297024d97fab3b4091 1100w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-2-credits.png?w=1650&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=dd2488fdfaaa38d1684af0d7dbf8c1f5 1650w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-2-credits.png?w=2500&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=6b05b4f152b7de8cf80651ee295659d0 2500w" />
*Go to the **Credits** tab and select **Invoice History**.*

<img src="https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-4-history.jpg?fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=9cfb803d2910a5eefbf006fea5d33495" alt="Stripe billing portal showing invoice history table" data-og-width="1640" width="1640" data-og-height="1271" height="1271" data-path="images/support/payment/billing-4-history.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-4-history.jpg?w=280&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=38fd58db94d458f2c1b34d0ec319879b 280w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-4-history.jpg?w=560&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=cd012193ba8640eb8494efb5d5999a53 560w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-4-history.jpg?w=840&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=b983e9f792bc7ab30694050e92eae3f3 840w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-4-history.jpg?w=1100&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=023a93cb6dabbbb5dc6f72d67fdcd810 1100w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-4-history.jpg?w=1650&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=8a1a1602442fb3a8bab6b99b81909bd6 1650w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-4-history.jpg?w=2500&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=1b0cb8a6bdd798028042b5fb3cd5431f 2500w" />
*Review your invoice list in the Stripe billing portal.*

## Downloading invoices

In the Stripe portal, open any entry in the invoice history list to view its details and download the PDF copy.

<img src="https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-5-download-invoice.jpg?fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=dd229361d56c7696a963964d8a172098" alt="Invoice history details with download option highlighted" data-og-width="1780" width="1780" data-og-height="1212" height="1212" data-path="images/support/payment/billing-5-download-invoice.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-5-download-invoice.jpg?w=280&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=ad78d8505c20a24ec21a940d535ed9db 280w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-5-download-invoice.jpg?w=560&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=257196c01a686b5f2df49553e9e96848 560w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-5-download-invoice.jpg?w=840&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=f5cbf52c787e2afef3bcc5805d002bcc 840w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-5-download-invoice.jpg?w=1100&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=569123bff4fe706970db7febc22c8b4b 1100w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-5-download-invoice.jpg?w=1650&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=65f2917735174a06b2b020f390482e62 1650w, https://mintcdn.com/dripart/-4dXqBBjO7AVe0YV/images/support/payment/billing-5-download-invoice.jpg?w=2500&fit=max&auto=format&n=-4dXqBBjO7AVe0YV&q=85&s=1177a680721729ae3020cbbbd74751a3 2500w" />

Make sure your billing details are correct before new invoices are generated. Update them from [Editing your payment information](/support/payment/editing-payment-information).


# Unsuccessful payments
Source: https://docs.comfy.org/support/payment/unsuccessful-payments

Understand why payments fail and how to resolve payment issues

If your payment fails, your subscription may be interrupted. This guide helps you understand and resolve payment issues.

## Common reasons for payment failure

### Insufficient funds

* Your card does not have enough available balance or credit
* **Solution**: Add funds to your account or use a different payment method

### Expired card

* Your credit or debit card has passed its expiration date
* **Solution**: Update your payment information with a valid card

### Incorrect information

* Card number, CVV, or billing address is entered incorrectly
* **Solution**: Verify and correct your payment details

### Card declined by issuer

* Your bank or card issuer has declined the transaction
* **Solution**: Contact your bank to authorize the payment

### International restrictions

* Your card does not support international transactions
* **Solution**: Enable international payments or use a different card

### Security holds

* Your bank has flagged the transaction as suspicious
* **Solution**: Contact your bank to verify and approve the charge

## What happens when payment fails

## Resolving payment issues

### Update your payment method

1. Log in to your account
2. Go to **Account Settings** > **Billing & Payment**
3. Update your payment information
4. Click **Retry Payment** if available

### Contact your bank

* Verify that your card is active and has sufficient funds
* Ensure international transactions are enabled
* Confirm that the charge from Comfy Organization Inc is authorized

### Try a different payment method

If your current payment method continues to fail, consider using:

* A different credit or debit card
* A digital wallet (Apple Pay, Google Pay, PayPal)
* An alternative payment method

## Preventing future payment failures

* Keep your payment information up to date
* Ensure your card has sufficient funds before billing dates
* Update your card details before expiration
* Whitelist charges from Comfy Organization Inc with your bank
* Enable email notifications for billing reminders

## Grace period and account suspension

* You typically have 7-14 days to resolve payment issues
* During this period, you may have limited access to services
* If payment is not resolved, your subscription will be cancelled
* You can reactivate your subscription at any time by updating payment information

## Need assistance?

If you continue to experience payment issues after trying these solutions, please contact our [support team](https://support.comfy.org/) with:

* Your account email
* The error message you received
* The payment method you're attempting to use
* Any relevant transaction IDs

We're here to help resolve your payment issues quickly.


# Canceling your Comfy Cloud subscription
Source: https://docs.comfy.org/support/subscription/canceling

Learn how to cancel your Comfy Cloud subscription

## How to cancel

To cancel your Comfy Cloud subscription:

1. Visit and log in to [cloud.comfy.org](https://cloud.comfy.org/)
2. Navigate to the settings menu
3. Go to **Plan & Credits**
4. Click **Manage subscription**

<img src="https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/subscription.jpg?fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=5fe2f1c68668ceaa5b6555749ae09eaa" alt="Subscription settings menu" data-og-width="1200" width="1200" data-og-height="1023" height="1023" data-path="images/support/subscription/subscription.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/subscription.jpg?w=280&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=4d34b81bbfc68da20f9c8befa56484d7 280w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/subscription.jpg?w=560&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=962d5d1cfcb4c3b11e81ce6beb67f046 560w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/subscription.jpg?w=840&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=e84f945616384bd8734bfa02563ec5fe 840w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/subscription.jpg?w=1100&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=fc6f0dfa1924ce03f4b3821df6d5dbac 1100w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/subscription.jpg?w=1650&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=ffcef99f86f9e682e1818ef58791462c 1650w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/subscription.jpg?w=2500&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=b5df8339cc4af67564c4db0d3b920bec 2500w" />

5. Click `Cancel subscription` button in Stripe to cancel your subscription

<img src="https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/payment/comfyui-billing-portal.jpg?fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=60aa8ec725601f1c218cc42ae9584311" alt="Stripe billing portal" data-og-width="2004" width="2004" data-og-height="1324" height="1324" data-path="images/support/payment/comfyui-billing-portal.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/payment/comfyui-billing-portal.jpg?w=280&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=db4a02e9229974a8da4747e402cef64d 280w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/payment/comfyui-billing-portal.jpg?w=560&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=b2f117c9ba924bc931d6206eead12dce 560w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/payment/comfyui-billing-portal.jpg?w=840&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=5203997fa9a06dfd160ea0ea3c5e7635 840w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/payment/comfyui-billing-portal.jpg?w=1100&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=4c79be94ae76250fac40af9948dd384d 1100w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/payment/comfyui-billing-portal.jpg?w=1650&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=6206c52db84488d144ebfd2a75fbba5f 1650w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/payment/comfyui-billing-portal.jpg?w=2500&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=b54ca7825155a6ff799624d1a0f12bb5 2500w" />

## Important notes

* All Comfy Cloud subscription plans automatically renew unless you set your plan to cancel
* Your subscription will remain active until the end of your current billing period
* You can resubscribe at any time by visiting [cloud.comfy.org](https://cloud.comfy.org/)


# Managing your Comfy Cloud subscription
Source: https://docs.comfy.org/support/subscription/managing

Learn how to manage your Comfy Cloud subscription

## Accessing subscription management

To manage your Comfy Cloud subscription:

1. Visit and log in to [cloud.comfy.org](https://cloud.comfy.org/)
2. Navigate to the settings menu
3. Go to **Plan & Credits**

<img src="https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/subscription.jpg?fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=5fe2f1c68668ceaa5b6555749ae09eaa" alt="Subscription settings menu" data-og-width="1200" width="1200" data-og-height="1023" height="1023" data-path="images/support/subscription/subscription.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/subscription.jpg?w=280&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=4d34b81bbfc68da20f9c8befa56484d7 280w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/subscription.jpg?w=560&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=962d5d1cfcb4c3b11e81ce6beb67f046 560w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/subscription.jpg?w=840&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=e84f945616384bd8734bfa02563ec5fe 840w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/subscription.jpg?w=1100&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=fc6f0dfa1924ce03f4b3821df6d5dbac 1100w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/subscription.jpg?w=1650&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=ffcef99f86f9e682e1818ef58791462c 1650w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/subscription.jpg?w=2500&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=b5df8339cc4af67564c4db0d3b920bec 2500w" />

4. Click **Manage subscription**

This will take you to Stripe, where you can manage your subscription details.


# Subscribing to Comfy Cloud
Source: https://docs.comfy.org/support/subscription/subscribing

Learn how to subscribe to Comfy Cloud

## How to subscribe

You can browse and use Comfy Cloud normally without a subscription, but you need to subscribe to run workflows.

To subscribe to Comfy Cloud:

1. Visit [cloud.comfy.org](https://cloud.comfy.org/) and log in to your account.
2. Click the **Subscribe** button.

<img src="https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/subscribe.jpg?fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=5562418c93728e927291b0bd352cec47" alt="Subscribe button location" data-og-width="2400" width="2400" data-og-height="1561" height="1561" data-path="images/support/subscription/subscribe.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/subscribe.jpg?w=280&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=bfad9d16bc645617cac672d51efc76b1 280w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/subscribe.jpg?w=560&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=4c4465126e0f1688ab044139988617af 560w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/subscribe.jpg?w=840&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=2645aead6ebb4a7f9fb567dba2cb39c1 840w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/subscribe.jpg?w=1100&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=95d78ea0cb4db543cef2b31af670cce9 1100w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/subscribe.jpg?w=1650&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=1ee655c1b324bb210241c4eebc2758c9 1650w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/subscribe.jpg?w=2500&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=87baacbc6e3efd27a1dd489aa5af34fc 2500w" />

3. Select your desired subscription plan.

For detailed information about Comfy Cloud subscription plans and pricing, visit [comfy.org/cloud/pricing](https://www.comfy.org/cloud/pricing).

Comfy Cloud subscriptions are managed through Stripe, our payment processor.

## Applying a promotion code

If you have a promotion code, you can apply it during checkout to receive a discount on your subscription:

### Step 1: Activate the promotion code field

During checkout, click the **Add promotion code** button to reveal the input field.

<img src="https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/apply-coupon-1.jpg?fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=235c91aefcc3a463b8d284b1074c65ff" alt="Click Add promotion code button" data-og-width="2400" width="2400" data-og-height="1575" height="1575" data-path="images/support/subscription/apply-coupon-1.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/apply-coupon-1.jpg?w=280&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=e4e95846eab8f0bc06e15e4a3f84ad94 280w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/apply-coupon-1.jpg?w=560&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=e8675c7a32da87da7c9ad34ecb309d80 560w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/apply-coupon-1.jpg?w=840&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=5b56ae9c1a2790b56ab2b6a988f7cd41 840w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/apply-coupon-1.jpg?w=1100&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=4fcfad5c547e43545259b361cd5b5ed9 1100w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/apply-coupon-1.jpg?w=1650&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=bec21ae9b172f299d26ee5801268aef6 1650w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/apply-coupon-1.jpg?w=2500&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=3a40db8d3b23c957ec577ca48470ff38 2500w" />

### Step 2: Enter and apply your code

Type your promotion code into the field and click **Apply**.

<img src="https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/apply-coupon-2.jpg?fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=87cbd13a6dcf9ae5f27514622e9d6a25" alt="Enter promotion code and click Apply" data-og-width="2400" width="2400" data-og-height="1575" height="1575" data-path="images/support/subscription/apply-coupon-2.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/apply-coupon-2.jpg?w=280&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=d24e6dd6008963ed24e0cd59bc9b6bdc 280w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/apply-coupon-2.jpg?w=560&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=84db7e03663057f081756a1275aaef77 560w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/apply-coupon-2.jpg?w=840&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=30b9c86cd86accf6080d0a7980552456 840w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/apply-coupon-2.jpg?w=1100&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=e3373948300358f254f4831bee5c21b4 1100w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/apply-coupon-2.jpg?w=1650&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=00c2e08d427ef6c169d2c87148ab5e3d 1650w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/apply-coupon-2.jpg?w=2500&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=1a17cdcbb38e5a0740059900def3dbfd 2500w" />

### Step 3: Verify the discount

If the code is valid, you'll see the updated price reflecting your discount.

<img src="https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/apply-coupon-3.jpg?fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=dd27b5121f2fbd917f253fd3b3bf5291" alt="Discount applied successfully" data-og-width="2400" width="2400" data-og-height="1590" height="1590" data-path="images/support/subscription/apply-coupon-3.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/apply-coupon-3.jpg?w=280&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=065039e872a7e3f9278d7090c162fb31 280w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/apply-coupon-3.jpg?w=560&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=f6fa84fcb1ae269393e881abffe9bfe2 560w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/apply-coupon-3.jpg?w=840&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=288ed87bf03bc1106cc461dab3dfa545 840w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/apply-coupon-3.jpg?w=1100&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=ffe6d2847a60f1ec69012c23eb4408f7 1100w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/apply-coupon-3.jpg?w=1650&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=453646313f2cb1c1ec79b7b4b4c177dd 1650w, https://mintcdn.com/dripart/0Mzzo7Lf2E6P6bS3/images/support/subscription/apply-coupon-3.jpg?w=2500&fit=max&auto=format&n=0Mzzo7Lf2E6P6bS3&q=85&s=437ef0e3c9978203cbb9cece51fec976 2500w" />

Complete the payment process to activate your subscription with the applied discount.

<Note>
  Promotion codes may have specific terms and conditions, such as expiration dates or usage limits. Make sure to review the details of your promotion code before applying it.
</Note>

## Subscription renewal

Your subscription renews automatically on the same day and time each month as when you first subscribed.

All Comfy Cloud subscription plans automatically renew unless you set your plan to cancel.


# How to Troubleshoot and Solve ComfyUI Issues
Source: https://docs.comfy.org/troubleshooting/custom-node-issues

Troubleshoot and fix problems caused by custom nodes and extensions

Here is the overall approach for troubleshooting custom node issues:

```mermaid  theme={null}
flowchart TD
   A[Issue Encountered] --> B{Does the issue disappear after <a href="#how-to-disable-all-custom-nodes%3F">disabling all custom nodes</a>?}
   B -- Yes --> C[Issue caused by custom nodes]
   B -- No --> D[Issue not caused by custom nodes, refer to other troubleshooting docs]
   C --> E{<a href="#1-troubleshooting-the-custom-nodes-frontend-extensions">Check frontend extensions first</a>?}
   E -- Yes --> F[<a href="#1-troubleshooting-the-custom-nodes-frontend-extensions">Troubleshoot in ComfyUI frontend</a><br/><li>Only need reload the frontend</li>]
   E -- No --> G[<a href="#2-general-custom-node-troubleshooting">Use general binary search method</a><li>Need to restart ComfyUI multiple times</li>]
   F --> H[Use binary search to locate problematic node]
   G --> H
   H --> I[<a href="#how-to-fix-the-issue">Fix, replace, report or remove problematic node</a>]
   I --> J[Issue solved]
```

## How to disable all custom nodes?

<Tabs>
  <Tab title="Desktop Users">
    Start ComfyUI Desktop with custom nodes disabled from the settings menu
    <img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/desktop-diable-custom-node.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=81e3ec0e178c9d0852ae4d97efc1232f" alt="Settings menu - Disable custom nodes" data-og-width="3276" width="3276" data-og-height="2332" height="2332" data-path="images/troubleshooting/desktop-diable-custom-node.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/desktop-diable-custom-node.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=b81ba39ae093d8ba58e8e9440bde3e9e 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/desktop-diable-custom-node.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=bf69f5e7d17412f1a0f3ec5b130da8a1 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/desktop-diable-custom-node.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=aef1fe68a458bcb761aef6fd70336e12 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/desktop-diable-custom-node.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=e4867e54f415d11f54d03e6096b220d8 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/desktop-diable-custom-node.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=ca1ef11def0631f4b2aa0511249422db 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/desktop-diable-custom-node.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=cc994b34e2608748a89d9c277ddebab2 2500w" />
    or run the server manually:

    ```bash  theme={null}
    cd path/to/your/comfyui
    python main.py --disable-all-custom-nodes
    ```
  </Tab>

  <Tab title="Manual Install">
    ```bash  theme={null}
    cd ComfyUI
    python main.py --disable-all-custom-nodes
    ```
  </Tab>

  <Tab title="Portable">
    <Tabs>
      <Tab title="Modify `.bat` file">
        Open the folder where the portable version is located, and find the `run_nvidia_gpu.bat` or `run_cpu.bat` file
        <img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/Portable-disable-custom-nodes.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=e5e98abac0f8b4525791042bc4727f3b" alt="Modify .bat file" data-og-width="2448" width="2448" data-og-height="1468" height="1468" data-path="images/troubleshooting/Portable-disable-custom-nodes.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/Portable-disable-custom-nodes.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=bfc884259862dca2b64fc083122a8db0 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/Portable-disable-custom-nodes.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=f521ff89ca396ccdb2db7912a778d149 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/Portable-disable-custom-nodes.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=111888b841b94fe47c6517b6c8a1406b 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/Portable-disable-custom-nodes.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=30a723a395d4be3178353e6f7ba72f2d 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/Portable-disable-custom-nodes.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=1fe5eb32a5666672deb209395e6d782e 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/Portable-disable-custom-nodes.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=8e0225d4f84eb194e9ec6c52d269618c 2500w" />

        1. Copy `run_nvidia_gpu.bat` or `run_cpu.bat` file and rename it to `run_nvidia_gpu_disable_custom_nodes.bat`
        2. Open the copied file with Notepad
        3. Add the `--disable-all-custom-nodes` parameter to the file, or copy the parameters below into a `.txt` file and rename the file to `run_nvidia_gpu_disable_custom_nodes.bat`

        ```bash  theme={null}
        .\python_embeded\python.exe -s ComfyUI\main.py --disable-all-custom-nodes  --windows-standalone-build
        pause
        ```

        4. Save the file and close it
        5. Double-click the file to run it. If everything is normal, you should see ComfyUI start and custom nodes disabled
      </Tab>

      <Tab title="Through Command Line">
                <img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/portable-disable-custom-nodes-cml-1.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=168ab7a2fa8c72a324b13e7aa0605721" alt="ComfyUI troubleshooting" data-og-width="1224" width="1224" data-og-height="726" height="726" data-path="images/troubleshooting/portable-disable-custom-nodes-cml-1.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/portable-disable-custom-nodes-cml-1.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=84539a288939656e739a5656f099e146 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/portable-disable-custom-nodes-cml-1.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=bf1d603c671bac9956c6c4faf0a03f0f 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/portable-disable-custom-nodes-cml-1.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=4da410291291b9df901364e6e056fba8 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/portable-disable-custom-nodes-cml-1.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=f35e0a934c2ff9fadcfa5dcfbba6e7af 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/portable-disable-custom-nodes-cml-1.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=dede74104694e9ce2f2cb2c8bd18a515 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/portable-disable-custom-nodes-cml-1.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=333a75528558d15a301bcb725f25bda3 2500w" />

        1. Enter the folder where the portable version is located
        2. Open the terminal by right-clicking the menu  Open terminal
           <img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/portable-disable-custom-nodes-cml-2.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=e94076e9febdd73b763aa4ad31eff68b" alt="ComfyUI troubleshooting" data-og-width="1224" width="1224" data-og-height="761" height="761" data-path="images/troubleshooting/portable-disable-custom-nodes-cml-2.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/portable-disable-custom-nodes-cml-2.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=b50a25cd8fe0f5a47dc6fc29c29af812 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/portable-disable-custom-nodes-cml-2.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=93405b88f71da021748ea246a0a01c89 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/portable-disable-custom-nodes-cml-2.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=18c64bafff772eb4fc02bbe9f13087e5 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/portable-disable-custom-nodes-cml-2.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=f2eb4fb92d6891a641f1bd2279803079 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/portable-disable-custom-nodes-cml-2.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=57f35a798df8ab4f705370026241e124 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/portable-disable-custom-nodes-cml-2.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=1f1f8f7766ceee802baf09af09d39232 2500w" />
        3. Ensure that the folder name is the current directory of the portable version
        4. Enter the following command to start ComfyUI through the portable python and disable custom nodes

        ```
        .\python_embeded\python.exe -s ComfyUI\main.py --disable-all-custom-nodes
        ```
      </Tab>
    </Tabs>
  </Tab>
</Tabs>

**Results:**

*  **Issue disappears**: A custom node is causing the problem  Continue to Step 2
*  **Issue persists**: Not a custom node issue  [Report the issue](#reporting-issues)

## What is Binary Search?

In this document, we will introduce the binary search approach for troubleshooting custom node issues, which involves checking half of the custom nodes at a time until we locate the problematic node.

Please refer to the flowchart below for the specific approach - enable half of the currently disabled nodes each time and check if the issue appears, until we identify which custom node is causing the issue

```mermaid  theme={null}
flowchart TD
   A[Start] --> B{Split all custom nodes in half}
   B --> C[Enable first half of custom nodes]
   C --> D[Restart ComfyUI and test]
   D --> E{Does the issue appear?}
   E --> |Yes| F[Issue is in enabled custom nodes]
   E --> |No| G[Issue is in disabled custom nodes]
   F --> H{Enabled custom nodes > 1?}
   G --> I{Disabled custom nodes > 1?}
   H --> |Yes| J[Continue binary search on enabled nodes]
   I --> |Yes| K[Continue binary search on disabled nodes]
   H --> |No| L[Found problematic custom node]
   I --> |No| L
   J --> B
   K --> B
   L --> M[End]
```

## Two Troubleshooting Methods

In this document, we categorize custom nodes into two types for troubleshooting:

<img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/custom_node_type.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=de35f5f9b582df8a3825b1103e8500fa" alt="Custom node types" data-og-width="1650" width="1650" data-og-height="416" height="416" data-path="images/troubleshooting/custom_node_type.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/custom_node_type.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=cc587e7a49fad74f85f54a764abc57e0 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/custom_node_type.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=08238cc924c009b6e16e21bd1a46f8e5 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/custom_node_type.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=50af03f0e0811b83d6f4cedd08b11f05 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/custom_node_type.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=828ebb57ba9eb8c40920eb8330e330a9 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/custom_node_type.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=b01b10bb0c92c61e3e9c6d65e2d71d64 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/custom_node_type.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=85249f6053606cd82244211fd445ff0e 2500w" />

* A: Custom nodes with frontend extensions
* B: Regular custom nodes

Let's first understand the potential issues and causes for different types of custom nodes:

<Tabs>
  <Tab title="Custom Nodes with Frontend Extensions">
    For custom nodes, we can prioritize troubleshooting those with frontend extensions, as they cause the most issues. Their main conflicts arise from incompatibilities with ComfyUI frontend version updates.

    Common issues include:

    * Workflows not executing
    * Some nodes can't show preview images(such as save image node)
    * Misaligned UI elements
    * Unable to access ComfyUI frontend
    * Completely broken UI or blank screen
    * Unable to communicate normally with ComfyUI backend
    * Node connections not working properly
    * And more

    Common causes for these issues:

    * Frontend modifications during updates that custom nodes haven't adapted to yet
    * Users updating ComfyUI without synchronously upgrading custom nodes, even though authors have released compatible versions
    * Authors stopping maintenance, leading to incompatibility between custom node extensions and the ComfyUI frontend
  </Tab>

  <Tab title="Regular Custom Nodes">
    If the problem isn't caused by custom nodes' frontend extensions, issues often relate to dependencies. Common problems include:

    * "Failed to import" errors in console/logs
    * Missing nodes still showing as missing after installation and restart
    * ComfyUI crashes or fails to start
    * And more

    Common causes for these errors:

    * Custom nodes requiring additional wheels like ComfyUI-Nunchaku
    * Custom nodes using strict dependency versions (e.g., `torch==2.4.1`) while other plugins use different versions (e.g., `torch>=2.4.2`), causing conflicts after installation
    * Network issues preventing successful dependency installation

    When problems involve Python environment interdependencies and versions, troubleshooting becomes more complex and requires knowledge of Python environment management, including how to install and uninstall dependencies
  </Tab>
</Tabs>

## Using Binary Search for Troubleshooting

Among these two different types of custom node issues, conflicts between custom node frontend extensions and ComfyUI are more common. We'll prioritize troubleshooting these nodes first. Here's the overall troubleshooting approach:

### 1. Troubleshooting the Custom Nodes' Frontend Extensions

<Steps>
  <Step title="Disable All Third-Party Frontend Extensions">
    <img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/disable_3rd_party.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=ae3bfc32cbe98dce9fdce7874cdf6516" alt="Disable all plugin frontend extensions" data-og-width="2404" width="2404" data-og-height="1514" height="1514" data-path="images/troubleshooting/disable_3rd_party.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/disable_3rd_party.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=76024aa7cdd685366ae14af404ffc3f2 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/disable_3rd_party.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=0e8ef55218480aa6ceed79d6fd197299 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/disable_3rd_party.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=8ced516c3ddac4432a3315d2a83ae2ae 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/disable_3rd_party.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=2bbe6f83502c4d050dc92c590655c83e 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/disable_3rd_party.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=0a81b08084a02f70644fba1af6bce67c 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/disable_3rd_party.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=af4d3c3de59790429968243afd83fbce 2500w" />
    After starting ComfyUI, find the `Extensions` menu in settings and follow the steps shown in the image to disable all third-party extensions

    <Tip>
      If you can't enter ComfyUI frontend, just skip the frontend extensions troubleshooting section and continue to [General Custom Node Troubleshooting Approach](#2-general-custom-node-troubleshooting-approach)
    </Tip>
  </Step>

  <Step title="Restart ComfyUI">
    After disabling frontend extensions for the first time, it's recommended to restart ComfyUI to ensure all frontend extensions are properly disabled

    * If the problem disappears, then it was caused by custom node frontend extensions, and we can proceed with binary search troubleshooting
    * If the problem persists, then it's not caused by frontend extensions - please refer to the other troubleshooting approaches in this document
  </Step>

  <Step title="Use Binary Search to Locate Problem Nodes">
    Use the method mentioned at the beginning of this document to troubleshoot, enabling half of the custom nodes at a time until you find the problematic node
    <img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/enable_extensions.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=9ba859462bdd883cdecd0ab8df40ad62" alt="Enable frontend extensions" data-og-width="3117" width="3117" data-og-height="950" height="950" data-path="images/troubleshooting/enable_extensions.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/enable_extensions.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=5c2c96d97b1608e78c86733f333eebf7 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/enable_extensions.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=4acf0406ce914d239b3999f1fa6e2b31 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/enable_extensions.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=9be438ef4fda2f241593ccf401a5ca12 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/enable_extensions.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=14285caeffc2ae86a89579ec354f0472 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/enable_extensions.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=8846d10f9ddef6c4dec35bd5b09849bb 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/enable_extensions.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=fb3890c22fffcd25e484410ea7ea3c65 2500w" />
    Refer to the image to enable half of the frontend extensions. Note that if extension names are similar, they likely come from the same custom node's frontend extensions
  </Step>

  <Step title="Follow-up Actions">
    If you find the problematic custom node, please refer to the problem fixing section of this document to resolve the custom node issues
  </Step>
</Steps>

Using this method, you don't need to restart ComfyUI multiple times - just reload ComfyUI after enabling/disabling custom node frontend extensions. Plus, your troubleshooting scope is limited to nodes with frontend extensions, which greatly narrows down the search range.

### 2. General Custom Node Troubleshooting

<Steps>
  <Step title="Use Binary Search to Locate Custom Nodes">
    For the binary search localization method, in addition to manual search, we also have automated binary search using comfy-cli, as detailed below:

    <Tabs>
      <Tab title="Using Comfy CLI (Recommended)">
        Using Comfy CLI requires some command line experience. If you're not comfortable with it, use manual binary search instead.

        If you have [Comfy CLI](/comfy-cli/getting-started) installed, you can use the automated bisect tool to find the problematic node:

        ```bash  theme={null}
        # Start a bisect session
        comfy-cli node bisect start

        # Follow the prompts:
        # - Test ComfyUI with the current set of enabled nodes
        # - Mark as 'good' if the issue is gone: comfy-cli node bisect good
        # - Mark as 'bad' if the issue persists: comfy-cli node bisect bad
        # - Repeat until the problematic node is identified

        # Reset when done
        comfy-cli node bisect reset
        ```

        The bisect tool will automatically enable/disable nodes and guide you through the process.
      </Tab>

      <Tab title="Manual Binary Search">
        <Warning>
          Before starting, please **create a backup** of your custom\_nodes folder in case something goes wrong.
        </Warning>

        If you prefer to do the process manually or don't have Comfy CLI installed, follow the steps below:

        <Steps>
          <Step title="Create Temporary Folders">
            Before starting, enter the `<YOUR_COMFYUI_FOLDER>\ComfyUI\` folder

            <Tabs>
              <Tab title="Windows">
                * **Backup all custom nodes**: Copy and rename `custom_nodes` to `custom_nodes_backup`
                * **Create a temporary folder**: Create a folder named `custom_nodes_temp`

                Or use the following command line to backup:

                ```bash  theme={null}
                # Create backup and temporary folder
                mkdir "%USERPROFILE%\custom_nodes_backup"
                mkdir "%USERPROFILE%\custom_nodes_temp"

                # Backup all content
                xcopy "custom_nodes\*" "%USERPROFILE%\custom_nodes_backup\" /E /H /Y
                ```
              </Tab>

              <Tab title="macOS/Linux">
                Manually backup custom\_nodes folder
                Or use the following command line to backup:

                ```bash  theme={null}
                # Create backup and temporary folder
                mkdir ~/custom_nodes_backup
                mkdir ~/custom_nodes_temp

                # Backup all content
                cp -r custom_nodes/* ~/custom_nodes_backup/
                ```
              </Tab>

              <Tab title="Cloud/Colab">
                ```bash  theme={null}
                # Create backup and temporary folder
                mkdir /content/custom_nodes_backup
                mkdir /content/custom_nodes_temp

                # Backup all content
                cp -r /content/ComfyUI/custom_nodes/* /content/custom_nodes_backup/
                ```
              </Tab>
            </Tabs>
          </Step>

          <Step title="List all custom nodes">
            <Tabs>
              <Tab title="Windows">
                Since Windows has a visual interface, you can skip this step unless you're only using the command line

                ```bash  theme={null}
                dir custom_nodes
                ```
              </Tab>

              <Tab title="macOS/Linux">
                ```bash  theme={null}
                ls custom_nodes/
                ```
              </Tab>

              <Tab title="Cloud/Colab">
                ```bash  theme={null}
                ls /content/ComfyUI/custom_nodes/
                ```
              </Tab>
            </Tabs>
          </Step>

          <Step title="Split nodes in half">
            Let's assume that you have 8 custom nodes. Move the first half to temporary storage:

            <Tabs>
              <Tab title="Windows">
                ```bash  theme={null}
                # Move first half (nodes 1-4) to temp
                move "custom_nodes\node1" "%USERPROFILE%\custom_nodes_temp\"
                move "custom_nodes\node2" "%USERPROFILE%\custom_nodes_temp\"
                move "custom_nodes\node3" "%USERPROFILE%\custom_nodes_temp\"
                move "custom_nodes\node4" "%USERPROFILE%\custom_nodes_temp\"
                ```
              </Tab>

              <Tab title="macOS/Linux">
                ```bash  theme={null}
                # Move first half (nodes 1-4) to temp
                mv custom_nodes/node1 ~/custom_nodes_temp/
                mv custom_nodes/node2 ~/custom_nodes_temp/
                mv custom_nodes/node3 ~/custom_nodes_temp/
                mv custom_nodes/node4 ~/custom_nodes_temp/
                ```
              </Tab>

              <Tab title="Cloud/Colab">
                ```bash  theme={null}
                # Move first half (nodes 1-4) to temp
                mv /content/ComfyUI/custom_nodes/node1 /content/custom_nodes_temp/
                mv /content/ComfyUI/custom_nodes/node2 /content/custom_nodes_temp/
                mv /content/ComfyUI/custom_nodes/node3 /content/custom_nodes_temp/
                mv /content/ComfyUI/custom_nodes/node4 /content/custom_nodes_temp/
                ```
              </Tab>
            </Tabs>
          </Step>

          <Step title="Test ComfyUI">
            Start ComfyUI normally

            ```bash  theme={null}
            python main.py
            ```
          </Step>

          <Step title="Interpret results">
            * **Issue persists**: Problem is in the remaining nodes (5-8)
            * **Issue disappears**: Problem was in the moved nodes (1-4)
          </Step>

          <Step title="Narrow it down">
            * If issue persists: Move half of remaining nodes (e.g., nodes 7-8) to temp
            * If issue gone: Move half of temp nodes (e.g., nodes 3-4) back to custom\_nodes
          </Step>

          <Step title="Repeat until you find the single problematic node">
            * Repeat until you find the single problematic node
          </Step>
        </Steps>
      </Tab>
    </Tabs>
  </Step>
</Steps>

## How to Fix the Issue

Once you've identified the problematic custom node:

### Option 1: Update the Node

1. Check if there's an update available in ComfyUI Manager
2. Update the node and test again

### Option 2: Replace the Node

1. Look for alternative custom nodes with similar functionality
2. Check the [ComfyUI Registry](https://registry.comfy.org) for alternatives

### Option 3: Report the Issue

Contact the custom node developer:

1. Find the node's GitHub repository
2. Create an issue with:
   * Your ComfyUI version
   * Error messages/logs
   * Steps to reproduce
   * Your operating system

### Option 4: Remove or Disable the Node

If no fix is available and you don't need the functionality:

1. Remove the problematic node from `custom_nodes/` or disable it in the ComfyUI Manager interface
2. Restart ComfyUI

## Reporting Issues

If the issue isn't caused by custom nodes, refer to the general [troubleshooting overview](/troubleshooting/overview) for other common problems.

### For Custom Node-Specific Issues

Contact the custom node developer:

* Find the node's GitHub repository
* Create an issue with your ComfyUI version, error messages, reproduction steps, and OS
* Check the node's documentation and Issues page for known issues

### For ComfyUI Core Issues

* **GitHub**: [ComfyUI Issues](https://github.com/comfyanonymous/ComfyUI/issues)
* **Forum**: [Official ComfyUI Forum](https://forum.comfy.org/)

### For Desktop App Issues

* **GitHub**: [ComfyUI Desktop Issues](https://github.com/Comfy-Org/desktop/issues)

### For Frontend Issues

* **GitHub**: [ComfyUI Frontend Issues](https://github.com/Comfy-Org/ComfyUI_frontend/issues)

<Note>
  For general installation, model, or performance issues, see our [troubleshooting overview](/troubleshooting/overview) and [model issues](/troubleshooting/model-issues) pages.
</Note>


# How to Troubleshoot and Solve ComfyUI Model Issues
Source: https://docs.comfy.org/troubleshooting/model-issues

Troubleshooting model-related problems including architecture mismatches, missing models, and loading errors

## Model Architecture Mismatch

**Symptoms:** Tensor dimension errors during generation, especially during VAE decode stage

**Common error messages:**

* `Given groups=1, weight of size [64, 4, 3, 3], expected input[1, 16, 128, 128] to have 4 channels, but got 16 channels instead`
* `Given groups=1, weight of size [4, 4, 1, 1], expected input[1, 16, 144, 112] to have 4 channels, but got 16 channels instead`
* `Given groups=1, weight of size [320, 4, 3, 3], expected input[2, 16, 192, 128] to have 4 channels, but got 16 channels instead`
* `The size of tensor a (49) must match the size of tensor b (16) at non-singleton dimension 1`
* `Tensors must have same number of dimensions: got 2 and 3`
* `mat1 and mat2 shapes cannot be multiplied (154x2048 and 768x320)`

**Root cause:** Using models from different architecture families together

### Solutions

1. **Verify model family compatibility:**
   * **Flux models** use 16-channel latent space with dual text encoder conditioning (CLIP-L + T5-XXL)
   * **SD1.5 models** use 4-channel latent space with single CLIP ViT-L/14 text encoder
   * **SDXL models** use 4-channel latent space with dual text encoders (CLIP ViT-L/14 + OpenCLIP ViT-bigG/14)
   * **SD3 models** use 16-channel latent space with triple text encoder conditioning (CLIP-L + OpenCLIP bigG + T5-XXL)
   * **ControlNet models** must match the architecture of the base checkpoint (SD1.5 ControlNets only work with SD1.5 checkpoints, SDXL ControlNets only work with SDXL checkpoints, etc.)

2. **Common mismatch scenarios and fixes:**

   **Flux + wrong VAE:**

   ```
   Problem: Using taesd or sdxl_vae.safetensors with Flux checkpoint
   Fix: Use ae.safetensors (Flux VAE) from Hugging Face Flux releases
   ```

   **Flux + incorrect CLIP configuration:**

   ```
   Problem: Using t5xxl_fp8_e4m3fn.safetensors in both CLIP slots of DualClipLoader
   Fix: Use t5xxl_fp8_e4m3fn.safetensors in one slot and clip_l.safetensors in the other
   ```

   **ControlNet architecture mismatch:**

   ```
   Problem: SD1.5 ControlNet with SDXL checkpoint (or vice versa)
   Error: "mat1 and mat2 shapes cannot be multiplied (154x2048 and 768x320)"
   Fix: Use ControlNet models designed for your checkpoint architecture
        - SD1.5 checkpoints require SD1.5 ControlNets
        - SDXL checkpoints require SDXL ControlNets
   ```

3. **Quick diagnostics:**
   ```bash  theme={null}
   # Check if error occurs at VAE decode stage
   # Look for "expected input[X, Y, Z] to have N channels, but got M channels"
   # Y value indicates channel count: 4 = SD models, 16 = Flux models
   ```

4. **Prevention strategies:**
   * Keep all workflow models within the same architecture family
   * Download complete model packages from same source/release (often all in a Hugging Face repo)
   * When trying new models, start with the template workflows or official ComfyUI workflow examples before customizing

## Missing Models Error

**Example error message:**

```
Prompt execution failed
Prompt outputs failed validation:
CheckpointLoaderSimple:
- Value not in list: ckpt_name: 'model-name.safetensors' not in []
```

### Solutions

1. **Download required models:**
   * Use ComfyUI Manager to auto-download models
   * Verify models are in correct subfolders

2. **Check model paths:**
   * **Checkpoints**: `models/checkpoints/`
   * **VAE**: `models/vae/`
   * **LoRA**: `models/loras/`
   * **ControlNet**: `models/controlnet/`
   * **Embeddings**: `models/embeddings/`

3. **Share models between UIs or use custom paths:**
   * See [ComfyUI Model Sharing and Custom Model Directory Configuration](/installation/comfyui_portable_windows#2-comfyui-model-sharing-and-custom-model-directory-configuration) for detailed instructions
   * Edit `extra_model_paths.yaml` file to add custom model directories

### Model Search Path Configuration

If you have models in custom locations, see the detailed guide for [ComfyUI Model Sharing and Custom Model Directory Configuration](/installation/comfyui_portable_windows#2-comfyui-model-sharing-and-custom-model-directory-configuration) to configure ComfyUI to find them.

## Model Loading Errors

**Error message:** "Error while deserializing header"

### Solutions

1. **Re-download the model** - File may be corrupted during download
2. **Check available disk space** - Ensure enough space for model loading (models can be 2-15GB+)
3. **Check file permissions** - Ensure ComfyUI can read the model files
4. **Test with different model** - Verify if issue is model-specific or system-wide

## Model Performance Issues

### Slow Model Loading

**Symptoms:** Long delays when switching models or starting generation

**Solutions:**

1. **Use faster storage:**
   * Move models to SSD if using HDD
   * Use NVMe SSD for best performance

2. **Adjust caching settings:**
   ```bash  theme={null}
   python main.py --cache-classic       # Use the old style (aggressive) caching. 
   python main.py --cache-lru 10         # Increase size of LRU cache
   ```

### Memory Issues with Large Models

**"RuntimeError: CUDA out of memory":**

```bash  theme={null}
# Progressive memory reduction
python main.py --lowvram          # First try
python main.py --novram           # If lowvram insufficient  
python main.py --cpu              # Last resort
```

**Model-specific memory optimization:**

```bash  theme={null}
# Force lower precision
python main.py --force-fp16

# Reduce attention memory usage
python main.py --use-pytorch-cross-attention
```

<Note>
  For additional model configuration and setup information, see the [Models documentation](/development/core-concepts/models).
</Note>


# How to Troubleshoot and Solve ComfyUI Issues
Source: https://docs.comfy.org/troubleshooting/overview

Common ComfyUI issues, solutions, and how to report bugs effectively

<Tip>
  We receive a lot of feedback issues, and we find that most of the issues submitted are related to custom nodes. So please ensure that you have read the [custom node troubleshooting guide](/troubleshooting/custom-node-issues) before submitting an error report to ensure that the issue is not caused by ComfyUI core issues.
</Tip>

<Card title="Custom Node Troubleshooting Guide" icon="puzzle-piece" href="/troubleshooting/custom-node-issues">
  Check how to troubleshoot issues caused by custom nodes.
</Card>

## Common Issues & Quick Fixes

Before diving into detailed troubleshooting, try these common solutions:

### ComfyUI Won't Start

**Symptoms:** Application crashes on startup, black screen, or fails to load

**Quick fixes:**

1. **Check system requirements** - Ensure your system meets the [minimum requirements](/installation/system_requirements)
2. **Update GPU drivers** - Download latest drivers from NVIDIA/AMD/Intel

### Generation Fails or Produces Errors

**Symptoms:** "Prompt execution failed" dialog with "Show report" button, workflow stops executing

**Quick fixes:**

1. **Click "Show report"** - Read the detailed error message to identify the specific issue
2. **Check if it's a custom node issue** - [Follow our custom node troubleshooting guide](/troubleshooting/custom-node-issues)
3. **Verify model files** - See [Models documentation](/development/core-concepts/models) for model setup
4. **Check VRAM usage** - Close other applications using GPU memory

### Slow Performance

**Symptoms:** Very slow generation times, system freezing, out of memory errors

**Quick fixes:**

1. **Lower resolution/batch size** - Reduce image size or number of images
2. **Use memory optimization flags** - See performance optimization section below
3. **Close unnecessary applications** - Free up RAM and VRAM
4. **Check CPU/GPU usage** - Use Task Manager to identify bottlenecks

**Performance Optimization Commands:**

For low VRAM systems:

```bash  theme={null}
# Low VRAM mode (uses cpu for text encoder)
python main.py --lowvram

# CPU mode (very slow but works with any hardware, only use as absolute last resort)
python main.py --cpu
```

For better performance:

```bash  theme={null}
# Disable previews (saves VRAM and processing)
python main.py --preview-method none

# Use optimized attention mechanisms
python main.py --use-pytorch-cross-attention
python main.py --use-flash-attention

# Async weight offloading
python main.py --async-offload
```

For memory management:

```bash  theme={null}
# Reserve specific VRAM amount for OS (in GB)
python main.py --reserve-vram 2

# Disable smart memory management
python main.py --disable-smart-memory

# Use different caching strategies
python main.py --cache-none      # Less RAM usage, but slower
python main.py --cache-lru 10    # Cache 10 results, faster
python main.py --cache-classic   # Use the old style (aggressive) caching. 
```

## Installation-Specific Issues

### Desktop App Issues

For comprehensive desktop installation troubleshooting, see the [Desktop Installation Guide](/installation/desktop/windows).

<Tabs>
  <Tab title="Windows">
    * **Unsupported device**: ComfyUI Desktop Windows only supports NVIDIA GPUs with CUDA. Use [ComfyUI Portable](/installation/comfyui_portable_windows) or [manual installation](/installation/manual_install) for other GPUs
    * **Installation fails**: Run installer as administrator, ensure at least 15GB disk space
    * **Maintenance page**: Check [mirror settings](/installation/desktop/windows#mirror-settings) if downloads fail
    * **Missing models**: Models are not copied during migration, only linked. Verify model paths
  </Tab>

  <Tab title="macOS">
    * **"App is damaged"**: Allow app in Security & Privacy settings
    * **Performance issues**: Grant Full Disk Access in Privacy settings
    * **Crashes**: Check Console app for crash reports
  </Tab>

  <Tab title="Linux">
    * **Missing libraries**: Install dependencies with package manager
    * **LD\_LIBRARY\_PATH errors**: PyTorch library path issues (see below)
  </Tab>
</Tabs>

### Manual Installation Issues

<Note>
  The documentation may be slightly out of date. If an issue occurs, please manually verify whether a newer stable version of pytorch or any of the listed libraries exists. Refer to resources like the [pytorch installation matrix](https://pytorch.org/get-started/locally/) or the [ROCm website](https://rocm.docs.amd.com/projects/install-on-linux/en/develop/install/3rd-party/pytorch-install.html#using-a-wheels-package).
</Note>

**Python version conflicts:**

```bash  theme={null}
# Check Python version (3.9+ required, 3.12 recommended)
python --version

# Use virtual environment (recommended)
python -m venv comfyui_env
source comfyui_env/bin/activate  # Linux/Mac
comfyui_env\Scripts\activate     # Windows
```

**Package installation failures:**

```bash  theme={null}
# Update pip first
python -m pip install --upgrade pip

# Install dependencies
pip install -r requirements.txt

# For NVIDIA GPUs (CUDA 12.8)
pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu128

# For AMD GPUs (Linux only - ROCm 6.3)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.3
```

### Linux-Specific Issues

**LD\_LIBRARY\_PATH errors:**

Common symptoms:

* "libcuda.so.1: cannot open shared object file"
* "libnccl.so: cannot open shared object file"
* "ImportError: libnvinfer.so.X: cannot open shared object file"

**Solutions:**

1. **Modern PyTorch installations (most common):**

```bash  theme={null}
# For virtual environments with NVIDIA packages
export LD_LIBRARY_PATH=$VIRTUAL_ENV/lib/python3.12/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH

# For conda environments
export LD_LIBRARY_PATH=$CONDA_PREFIX/lib/python3.12/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH

# Or find your Python site-packages automatically
PYTHON_PATH=$(python -c "import site; print(site.getsitepackages()[0])")
export LD_LIBRARY_PATH=$PYTHON_PATH/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH

# You may also need other NVIDIA libraries
export LD_LIBRARY_PATH=$PYTHON_PATH/nvidia/cuda_runtime/lib:$LD_LIBRARY_PATH
export LD_LIBRARY_PATH=$PYTHON_PATH/nvidia/cublas/lib:$LD_LIBRARY_PATH
```

2. **Find what libraries you have:**

```bash  theme={null}
# Check installed NVIDIA packages
python -c "import site; import os; nvidia_path=os.path.join(site.getsitepackages()[0], 'nvidia'); print('NVIDIA libs:', [d for d in os.listdir(nvidia_path) if os.path.isdir(os.path.join(nvidia_path, d))] if os.path.exists(nvidia_path) else 'Not found')"

# Find missing libraries that PyTorch needs
python -c "import torch; print(torch.__file__)"
ldd $(python -c "import torch; print(torch.__file__.replace('__init__.py', 'lib/libtorch_cuda.so'))")
```

3. **Set permanently for your environment:**

```bash  theme={null}
# For virtual environments, add to activation script
echo 'export LD_LIBRARY_PATH=$VIRTUAL_ENV/lib/python*/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH' >> $VIRTUAL_ENV/bin/activate

# For conda environments
conda env config vars set LD_LIBRARY_PATH=$CONDA_PREFIX/lib/python*/site-packages/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH

# For global bashrc (adjust Python version as needed)
echo 'export LD_LIBRARY_PATH=$(python -c "import site; print(site.getsitepackages()[0])")/nvidia/nvjitlink/lib:$LD_LIBRARY_PATH' >> ~/.bashrc
```

4. **Alternative: Use ldconfig:**

```bash  theme={null}
# Check current library cache
ldconfig -p | grep cuda
ldconfig -p | grep nccl

# If missing, add library paths (requires root)
sudo echo "/usr/local/cuda/lib64" > /etc/ld.so.conf.d/cuda.conf
sudo ldconfig
```

5. **Debug library loading:**

```bash  theme={null}
# Verbose library loading to see what's missing
LD_DEBUG=libs python main.py 2>&1 | grep "looking for"

# Check PyTorch CUDA availability
python -c "import torch; print('CUDA available:', torch.cuda.is_available()); print('CUDA version:', torch.version.cuda)"
```

## Model-Related Issues

For comprehensive model troubleshooting including architecture mismatches, missing models, and loading errors, see the dedicated [Model Issues](/troubleshooting/model-issues) page.

## Network & API Issues

### API Nodes Not Working

**Symptoms:** API calls fail, timeout errors, quota exceeded

**Solutions:**

1. **Check API key validity** - Verify keys in [user settings](/interface/user)
2. **Check account credits** - Ensure sufficient [API credits](/interface/credits)
3. **Verify internet connection** - Test with other online services
4. **Check service status** - Provider may be experiencing downtime

### Connection Issues

**Symptoms:** "Failed to connect to server", timeout errors

**Solutions:**

1. **Check firewall settings** - Allow ComfyUI through firewall
2. **Try different port** - Default is 8188, try 8189 or 8190
3. **Disable VPN temporarily** - VPN may be blocking connections
4. **Check proxy settings** - Disable proxy if not required

### Frontend Issues

**"Frontend or Templates Package Not Updated":**

```bash  theme={null}
# After updating ComfyUI via Git, update frontend dependencies
pip install -r requirements.txt
```

**"Can't Find Custom Node":**

* Disable node validation in ComfyUI settings

**"Error Toast About Workflow Failing Validation":**

* Disable workflow validation in settings temporarily
* Report the issue to the ComfyUI team

**Login Issues When Not on Localhost:**

* Normal login only works when accessing from localhost
* For LAN/remote access: Generate API key at [platform.comfy.org/login](https://platform.comfy.org/login)
* Use API key in login dialog or with `--api-key` command line argument

## Hardware-Specific Issues

### NVIDIA GPU Issues

**"Torch not compiled with CUDA enabled" error:**

```bash  theme={null}
# First uninstall torch
pip uninstall torch

# Install stable PyTorch with CUDA 12.8
pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu128

# For nightly builds (might have performance improvements)
pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128

# Verify CUDA support
python -c "import torch; print(torch.cuda.is_available())"
```

**GPU not detected:**

```bash  theme={null}
# Check if GPU is visible
nvidia-smi

# Check driver version and CUDA compatibility
nvidia-smi --query-gpu=driver_version --format=csv
```

### AMD GPU Issues

**ROCm support (Linux only):**

```bash  theme={null}
# Install stable ROCm PyTorch (6.3.1 at the time of writing)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.3

# For nightly builds (ROCm 6.4 at the time of writing), which might have performance improvements)
pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.4
```

**Unsupported AMD GPUs:**

```bash  theme={null}
# For RDNA2 or older (6700, 6600)
HSA_OVERRIDE_GFX_VERSION=10.3.0 python main.py

# For RDNA3 cards (7600)
HSA_OVERRIDE_GFX_VERSION=11.0.0 python main.py
```

**Performance optimization:**

```bash  theme={null}
# Enable experimental memory efficient attention (no longer necessary with PyTorch 2.4)
TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1 python main.py --use-pytorch-cross-attention

# Enable tunable operations (slow first run, but faster subsequent runs)
PYTORCH_TUNABLEOP_ENABLED=1 python main.py
```

### Apple Silicon (M1/M2/M3) Issues

**MPS backend setup:**

```bash  theme={null}
# Install PyTorch nightly for Apple Silicon
# Follow Apple's guide: https://developer.apple.com/metal/pytorch/

# Check MPS availability
python -c "import torch; print(torch.backends.mps.is_available())"

# Launch ComfyUI
python main.py
```

**If MPS causes issues:**

```bash  theme={null}
# Force CPU mode
python main.py --cpu

# With memory optimization
python main.py --force-fp16 --cpu
```

### Intel GPU Issues

**Option 1: Native PyTorch XPU support (Windows/Linux):**

```bash  theme={null}
# Install PyTorch nightly with XPU support
pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/xpu

# Launch ComfyUI
python main.py
```

**Option 2: Intel Extension for PyTorch (IPEX):**

```bash  theme={null}
# For Intel Arc A-Series Graphics
conda install libuv
pip install torch==2.3.1.post0+cxx11.abi torchvision==0.18.1.post0+cxx11.abi torchaudio==2.3.1.post0+cxx11.abi intel-extension-for-pytorch==2.3.110.post0+xpu --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/
```

## Getting Help & Reporting Bugs

### Before Reporting a Bug

1. **Check if it's a known issue:**
   * Search [GitHub Issues](https://github.com/comfyanonymous/ComfyUI/issues)
   * Check [ComfyUI Forum](https://forum.comfy.org/)
   * Review [Discord discussions](https://discord.com/invite/comfyorg)

2. **Try basic troubleshooting:**
   * Test with [default workflow](/get_started/first_generation)
   * Disable all custom nodes (see [custom node troubleshooting](/troubleshooting/custom-node-issues))
   * Check console/terminal for error messages
   * If using comfy-cli, try updating: `comfy node update all`

### How to Report Bugs Effectively

#### For ComfyUI Core Issues

**Where to report:** [GitHub Issues](https://github.com/comfyanonymous/ComfyUI/issues)

#### For Desktop App Issues

**Where to report:** [Desktop GitHub Issues](https://github.com/Comfy-Org/desktop/issues)

#### For Frontend Issues

**Where to report:** [Frontend GitHub Issues](https://github.com/Comfy-Org/ComfyUI_frontend/issues)

#### For Custom Node Issues

**Where to report:** Contact the specific custom node developer

### Required Information

When reporting any issue, include:

<Steps>
  <Step title="System Information">
    <Tabs>
      <Tab title="From ComfyUI Interface">
        **System Information (can be found in the About page in settings):**

        * Operating System (Windows 11, macOS 14.1, Ubuntu 22.04, etc.)
        * ComfyUI version (check About page in settings)
        * Python version: `python --version`
        * PyTorch version: `python -c "import torch; print(torch.__version__)"`
        * GPU model and driver version
        * Installation method (Desktop, Portable, Manual, comfy-cli)

                <img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/menu-about.jpg?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=c5bbb54ba0d54a908d76ea3e863680d8" alt="About page in settings" data-og-width="4002" width="4002" data-og-height="2442" height="2442" data-path="images/troubleshooting/menu-about.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/menu-about.jpg?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=9afd28b32c6f1b69d8fe0c79535e5c18 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/menu-about.jpg?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=81709829c925abaf97975c6d26b791ad 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/menu-about.jpg?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=c36c48d23e08a916f30a89e5111fe838 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/menu-about.jpg?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=5a9007acc524d67de29d7fec90bda3fa 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/menu-about.jpg?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=105a22440f4be0fcafbeaf217e29574f 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/troubleshooting/menu-about.jpg?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=4eb752489fcf8318681103de4486faaf 2500w" />
      </Tab>

      <Tab title="From Command Line">
        <Tabs>
          <Tab title="Windows">
            ```bash  theme={null}
            # System info
            systeminfo | findstr /C:"OS Name" /C:"OS Version"

            # GPU info
            wmic path win32_VideoController get name

            # Python & PyTorch info
            python --version
            python -c "import torch; print(f'PyTorch: {torch.__version__}')"
            python -c "import torch; print(f'CUDA Available: {torch.cuda.is_available()}')"
            ```
          </Tab>

          <Tab title="macOS/Linux">
            ```bash  theme={null}
            # System info
            uname -a

            # GPU info (Linux)
            lspci | grep VGA

            # Python & PyTorch info
            python --version
            python -c "import torch; print(f'PyTorch: {torch.__version__}')"
            python -c "import torch; print(f'CUDA Available: {torch.cuda.is_available()}')"
            ```
          </Tab>
        </Tabs>
      </Tab>
    </Tabs>
  </Step>

  <Step title="Desktop App issues">
    **For Desktop App issues, also include:**

    * Log files from: `C:\Users\<username>\AppData\Roaming\ComfyUI\logs` (Windows)
    * Config files from: `C:\Users\<username>\AppData\Roaming\ComfyUI` (Windows)
  </Step>

  <Step title="Problem Details">
    **Problem Details:**

    * Clear description of the issue
    * Steps to reproduce the problem
    * Expected vs actual behavior
    * Screenshots or videos if applicable

    **Error Messages:**

    * Full error text from console/terminal
    * Browser console errors (F12  Console tab)
    * Any crash logs or error dialogs
  </Step>

  <Step title="Additional Context">
    **Additional Context:**

    * List of installed custom nodes
    * Workflow file (.json) that reproduces the issue
    * Recent changes (new installations, updates, etc.)
  </Step>
</Steps>

## Community Resources

* **Official Forum:** [forum.comfy.org](https://forum.comfy.org/)
* **Discord:** [ComfyUI Discord Server](https://discord.com/invite/comfyorg)
* **Reddit:** [r/comfyui](https://reddit.com/r/comfyui)
* **YouTube:** [ComfyUI Tutorials](https://www.youtube.com/@comfyorg)


# Changelog
Source: https://docs.comfy.org/changelog/index

Track ComfyUI's latest features, improvements, and bug fixes. For detailed release notes, see the [Github releases](https://github.com/comfyanonymous/ComfyUI/releases) page.

<Update label="v0.3.71" description="November 21, 2025">
  **Model Compatibility & Enhancements**

  * **HunyuanVideo 1.5 Support**: Added compatibility for the latest version of HunyuanVideo models, expanding video generation capabilities
  * **LLAMA Text Encoder Improvements**: Added ability to disable final normalization in LLAMA-based text encoder models for better workflow customization
  * **HunyuanV3D Schema Migration**: Updated Hunyuan3D nodes to V3 schema for improved performance and compatibility

  **API Node Expansions**

  * **New Topaz API Nodes**: Added support for Topaz video enhancement workflows directly within ComfyUI
  * **Nano Banana Pro Integration**: Expanded API node collection with Nano Banana Pro for enhanced processing capabilities
  * **Kling Lip Sync Improvements**: Fixed audio format conversion issues in KlingLipSyncAudioToVideoNode, ensuring proper MP3 format handling

  **Image Processing & Workflow Improvements**

  * **Enhanced Image Batching**: Fixed ImageBatch node to handle images with different channel counts and automatically add alpha channels when needed
  * **Preview Node Refinement**: Renamed PreviewAny node to "Preview as Text" for clearer workflow organization
  * **Workflow Template Updates**: Enhanced server template handler to support multi-package distribution for better workflow management

  **Performance & System Optimizations**

  * **CUDA Optimization**: Disabled unnecessary workarounds on newer CUDNN versions for improved GPU performance
  * Fixed workflow naming issues and improved overall stability for complex processing pipelines
</Update>

<Update label="v0.3.70" description="November 19, 2025">
  **CUDA 12.6 Support & Distribution**

  * Added official CUDA 12.6 release workflow and portable download support for enhanced GPU compatibility
  * Updated README with corrected portable download links for streamlined installation

  **Model Compatibility & Fixes**

  * **HunYuan 3D 2.0 Support**: Fixed compatibility issues for improved 3D model generation workflows
  * **EasyCache Improvements**: Resolved input/output channel mismatches that affected certain model configurations
  * Enhanced block swap functionality by removing potentially harmful native custom node implementations

  **API Node Enhancements**

  * **New Gemini Models Added**: Expanded AI model options for text and multimodal generation workflows
  * Improved API node development infrastructure with updated PR templates and Python 3.10 minimum version requirements

  **Development & Infrastructure**

  * Enhanced pylint configuration for better code quality in custom node development
  * Improved release automation and distribution processes for more reliable updates
</Update>

<Update label="v0.3.69" description="November 18, 2025">
  **Memory & Performance Optimizations**

  * **Pinned Memory Enabled by Default** for NVIDIA and AMD GPUs
  * **Reduced VRAM Usage** for Flux, Qwen, and LTX-Video models
  * Smart model unloading that automatically frees memory when VRAM usage increases
  * Improved weight casting performance on offload streams

  **New Features**

  * **ScaleROPE Node Now Works with Flux Models**
  * Added left padding support to tokenizers
  * Added `create_time` field to `/history` and `/queue` endpoints

  **Bug Fixes**

  * Fixed custom node import errors for SingleStreamBlock/DoubleStreamBlock (temporary fix)
  * Fixed Qwen ControlNet regression
  * Enhanced quantized operations with offload support and stability improvements
  * Unified RoPE function implementation across models
</Update>

<Update label="v0.3.68" description="November 5, 2025">
  **Performance & Memory Optimizations**

  * Introduced **Mixed Precision Quantization System** for optimized model loading
  * Added **RAM Pressure Cache Mode** for intelligent memory management under resource constraints
  * Accelerated model offloading using pinned memory with automatic low-RAM hardware detection
  * Enhanced FP8 operations: reduced memory usage and fixed torch.compile performance regressions
  * Improved async offload speeds and resolved race conditions

  **New Nodes & Execution Features**

  * **ScaleROPE Node**: Rope scaling support for WAN and Lumina models
  * Enhanced subgraph execution allowing multiple runs within single workflows
  * Improved caching system with proper handling of bytes data and None outputs

  **API Node Enhancements**

  * Migrated API nodes to V3 client architecture: Luma, Minimax, Pixverse, Ideogram, StabilityAI, Pika, Recraft, Hypernetwork, OpenAI
  * Extended LTXV API nodes with 12s-20s duration options
  * Fixed img2img operations in DALL-E 2 node
  * Enhanced Rodin3D nodes to return proper relative paths

  **Updates**

  * Embedded documentation updated to v0.3.1
  * Workflow templates updated to v0.2.11
  * Fixed Windows pinned memory allocation issues
</Update>

<Update label="v0.3.67" description="October 28, 2025">
  **API Nodes**

  * **LTXV API Integration**: Added new LTXV API nodes for Lightricks LTX video generation
  * Network Client V2 upgrade with async operations and cancellation support
  * Converted Tripo and Gemini API nodes to V3 schema

  **Performance & Compatibility**

  * Improved AMD GPU support by only disabling cudnn on newer AMD GPUs
  * Fixed Windows-specific network issues in API nodes for better retry handling

  **Core Improvements**

  * Enhanced dependency-aware caching system that fixes --cache-none behavior with loops
  * Added support for multi-dimensional latents
  * Added custom node published subgraphs endpoint

  **Updates**

  * Frontend bumped to version 1.28.8
  * Template updates to version 0.2.4
</Update>

<Update label="v0.3.66" description="October 21, 2025">
  **Frontend Updates**

  * **Subgraph Widget Editing**: Edit subgraph parameters directly from new Parameters panel without entering the subgraph
  * **Template Modal Redesign**: New template browser with advanced filtering by model tags and categories

  **Performance Optimizations**

  * Iimproved workflow cancellation speed
  * Fixed VAE memory issue consuming 3x more on PyTorch 2.9 with NVIDIA GPUs
  * Enhanced chroma radiance processing speed and fixed batch size issues above 1

  **API Nodes**

  * Added Veo 3.1 model support
  * Added TemporalScoreRescaling node for advanced temporal control in video workflows

  **Hardware & Compatibility**

  * Disabled FP8 operations for AMD gfx942 GPUs
  * Improved CUDA memory management in --fast autotune mode

  **Execution & Schema**

  * Converted ControlNet nodes to V3 schema
  * Enhanced EasyCache with proper batch\_slice handling
  * Improved merge\_nested\_dicts functionality with proper input ordering
  * Added deprecation warnings for unused files
</Update>

<Update label="v0.3.65" description="October 14, 2025">
  **Node Schema Migration (V3)**

  * Migrated core node categories to V3 schema including model downscale, LoRA extraction, compositing, latent ops, SD3/SLG, Flux, upscale models, and HunyuanVideo nodes

  **Audio & Model Improvements**

  * Added MMaudio 16K VAE support for high-fidelity audio workflows
  * Fixed mono audio incorrectly saving as stereo
  * Refactored model sampling sigmas code and fixed FP8 scaled LoRA issues
  * Fixed loading of older Stable Diffusion checkpoints on newer NumPy versions

  **AMD GPU Optimizations**

  * Better memory estimation for SD/Flux VAE operations
  * Enabled RDNA4 PyTorch attention on ROCm 7.0+

  **API Node Updates**

  * Added price extractor and improvements to Kling/Pika API nodes
  * Enhanced Gemini Image API with aspect\_ratio support

  **Updates**

  * Template v0.1.95, node docs v0.3.0
  * Fixed WAN2.2 cache VRAM leak
</Update>

<Update label="v0.3.64" description="October 8, 2025">
  **API Nodes**

  * Added Sora2 API node for OpenAI's video generation API
</Update>

<Update label="v0.3.63" description="October 6, 2025">
  **Model Compatibility Enhancements**

  * **HunyuanVAE Support**: Added support for the new HunyuanVAE, expanding model compatibility for advanced image generation workflows
  * **Epsilon Scaling Node**: Introduced new Epsilon Scaling node that reduces exposure bias in diffusion models by scaling predicted noise, improving generation quality (based on the paper [Elucidating the Exposure Bias in Diffusion Models](https://arxiv.org/abs/2308.15321))

  **Memory and Performance Optimizations**

  * **VAE Memory Leak Fix**: Fixed VRAM leak caused by Python call stack holding tensor references during VAE OOM exception handling, significantly improving tiled fallback reliability on low VRAM devices
  * **AMD Support**: Enabled TORCH\_ROCM\_AOTRITON\_ENABLE\_EXPERIMENTAL by default

  **API Node Updates**

  * **Kling 2.5 Turbo**: Added support for kling-2-5-turbo in both txt2video and img2video nodes, with proper mode configuration fixes
  * **API Node Fixes**: Improved Gemini node base64 handling and fixed indentation issues in Recraft API node functions

  **Node Schema Migration (V3)**

  * **Extensive V3 Conversion**: Migrated numerous node categories to V3 schema including audio encoder, GITS, differential diffusion, optimal steps, PAG, LT, IP2P, morphology, torch compile, EPS, Pixverse, TomeSD, edit model, Rodin, and Stable3D nodes for improved workflow compatibility

  **Developer Experience Improvements**

  * **Code Quality**: Added pylint support for comfy\_api\_nodes folder and updated example\_node.py to use V3 schema, making custom node development more consistent
  * **Documentation Updates**: Enhanced AMD installation instructions with nightly PyTorch commands for Windows users

  **Frontend Updates**

  * **Subgraph Publish**: Allows publishing subgraphs to the node library
  * **Node Selection Toolbox Redesign**: Redesigned the node selection toolbox
</Update>

<Update label="v0.3.62" description="September 30, 2025">
  **API Node**

  * **Rodin3D-Gen2 Parameter Fix**
  * **Seedance Pro Model Support**
</Update>

<Update label="v0.3.61" description="September 30, 2025">
  **API Node**

  * **Rodin3D Gen-2**: Rodin's most powerful image-to-3D tool, is now live in ComfyUI!
  * **WAN Image-to-Image**: Wan2.5 Image-to-Image API node, support image editing.

  **Enhanced Audio Capabilities**

  * **New Audio Nodes**: Added new audio nodes for enhanced audio-driven workflows and multimodal content creation

  **Model Compatibility Fixes**

  * **Qwen2.5VL Template Handling**: Improved template management for Qwen2.5VL models when templates are already present in prompts
  * **HuMo View Operation**: Fixed .view() operation issues in HuMo models for more stable video generation

  **Memory and Performance Optimizations**

  * **Memory Leak Fix**: Resolved memory leaks by explicitly detaching model finalizers, improving long-running workflow stability
  * **Sampler CFG Enhancement**: Added 'input\_cond' and 'input\_uncond' parameters to sampler CFG function arguments for more flexible conditioning control
</Update>

<Update label="v0.3.60" description="September 23, 2025">
  **New Model Support**

  * **Wan2.2 Animate**: Support for Wan2.2 Animate video generation model with character replacement and motion transfer capabilities
  * **Qwen Image Edit 2509 Support**: Support for the updated Qwen Image Edit 2509 with multi-image editing, higher consistency, and native ControlNet support
  * **HuMo Models**: Added support for both 1.7B and 17B HuMo models that use audio to drive video generation while maintaining lip sync
  * **Chroma Radiance**: A model that performs image generation in pixel space, reducing losses during the image generation process
  * **Omnigen2 UMO LoRA**: Added support for Omnigen2 UMO LoRA models

  **API Node Additions**

  * **Kling v2.1 Support**: Added kling-v2-1 model to KlingStartEndFrame node
  * **Seedream4 Fixes**: Fixed the flag that ignores errors on partial success, making workflows more robust

  **Node Schema Migration (V3)**

  * **Core Node Updates**: Migrated multiple node categories to V3 schema including Minimax API, Cosmos, conditioning, CFG, and Canny nodes

  **Performance & Technical Improvements**

  * **FP8 Operations**: Enabled FP8 operations by default on gfx1200 hardware for faster processing
  * **LoRA Trainer Fixes**: Resolved bugs with FP8 model compatibility in LoRA training workflows

  **Frontend Updates**

  * **Frontend Version Update**: Updated to version 1.26.13
</Update>

<Update label="v0.3.59" description="September 10, 2025">
  **ByteDance Seedream 4.0 Integration**

  * **New Seedream Node**: Added ByteDanceSeedream (4.0) node.
</Update>

<Update label="v0.3.58" description="September 6, 2025">
  **New Model Support**

  * Hunyuan Image 2.1 regular model
  * Hunyuan 3D 2.1

  **New API Nodes**

  * Stable Audio 2.5 API
  * Seedance Video API
</Update>

<Update label="v0.3.57" description="September 4, 2025">
  **ByteDance USO Model Support**

  * **UXO Subject Identity LoRA Support**: It's a subject identity LoRA model based on FLUX architecture
  * **Related Workflow**: Please find the workflow in the template `Flux` -> `Flux.1 Dev USO Reference Image Generation`

  **Workflow Utilities**

  * **ImageScaleToMaxDimension Node**: New utility for intelligent image scaling
  * **SEEDS Noise System**: Updated noise decomposition with improved algorithms
  * **Enhanced Prompt Control**: Interrupt handler now accepts prompt\_id parameters

  **Performance & Architecture**

  * **V3 Schema Migration**: Converted some core nodes to V3 schema
  * **Convolution AutoTuning**: Enabled automatic convolution optimization

  **New API Integration**

  * **ByteDance Image Nodes**: Added support for ByteDance image generation services
  * **Ideogram Character Reference**: Ideogram v3 API now supports character reference
</Update>

<Update label="v0.3.56" description="August 30, 2025">
  **Performance Enhancement**

  * **Reduced RAM Usage on Windows**
</Update>

<Update label="v0.3.55" description="August 29, 2025">
  **Wan2.2 S2V Workflow Enhancements & Model Support Expansion**

  This release focuses on Wan2.2 S2V related video workflow capabilities and model support expansion:

  **Wan2.2 S2V Workflow Control**

  * **WanSoundImageToVideoExtend Node**: New manual video extension node for audio-driven video workflows, giving creators precise control over generated video length and timing. This enables fine-tuned control over how audio content translates to video sequences.
  * **Audio-Video Synchronization**: Fixed critical issue where extending video past audio length caused workflow failures, ensuring reliable sound-to-video generation regardless of audio duration.
  * **Automatic Audio Trimming**: Video saves now automatically trim audio to match video length, eliminating audio-video sync issues in final output files.

  **Advanced Latent Processing**

  * **LatentCut Node**: New node for cutting latents at precise points, enabling more granular control over latent space manipulation in complex generation workflows. This is particularly useful for batch processing and temporal video workflows, such as removing specific frames from videos.

  **Wan2.2 5B Model Integration**

  * **Fun Control Model Support**: Added support for Wan2.2 5B fun control model.
  * **Fun Inpaint Model Support**: Integrated Wan2.2 5B fun inpaint model.
</Update>

<Update label="v0.3.54" description="August 28, 2025">
  **Node Model Patch Improvements**

  This focused update improves the core node model patching system that underpins ComfyUI's flexible architecture:

  **Core Infrastructure Enhancement**

  * **Node Model Patch Updates**: Enhanced nodes\_model\_patch.py with improvements to the underlying model patching mechanism, making ComfyUI extensions for Qwen-Image ControlNet easier

  **Workflow Benefits**

  * **Enhanced Stability**: Core model patching improvements contribute to more reliable node execution and model handling across different workflow configurations
</Update>

<Update label="v0.3.53" description="August 28, 2025">
  **Audio Workflow Integration & Enhanced Performance Optimizations**

  This release adds ComfyUI audio processing capabilities and includes performance improvements and model compatibility updates:

  **Audio Processing Updates**

  * **Wav2vec2 Audio Encoder**: Added native wav2vec2 implementation as an audio encoder model, enabling audio-to-embedding workflows for multimodal applications
  * **Audio Encoders Directory**: Added models/audio\_encoders directory, which is the audio encoder directory for Wan2.2 S2V
  * **AudioEncoderOutput V3 Support**: Made AudioEncoderOutput compatible with V3 node schema, ensuring seamless integration with modern workflow architectures

  **Google Gemini API Integration**

  * **Gemini Image API Node**: Added new Google Gemini Image API node, the "nano-Nano-banana" image editing model API with high consistency

  **Video Generation Performance & Memory Optimizations**

  * **WAN 2.2 S2V Model Support**: Work-in-progress implementation of WAN 2.2 Sound-to-Video model with optimized memory usage and performance
  * **Enhanced S2V Performance**: Performance improvements for video generation longer than 120 frames, improving extended video workflows
  * **Better Memory Estimation**: Improved memory usage estimation for S2V workflows prevents out-of-memory errors during long video generation
  * **Negative Audio Handling**: Fixed negative audio input handling in S2V workflows to use proper zero values

  **Sampling & Node Enhancements**

  * **DPM++ 2M SDE Heun (RES) Sampler**: New advanced sampler by @Balladie provides additional sampling options for fine-tuned generation control
  * **LatentConcat Node**: New node for concatenating latent tensors, enabling advanced latent space manipulation workflows
  * **EasyCache/LazyCache Stability**: Fixed critical crashes when tensor properties (shape/dtype/device) change during sampling, ensuring workflow reliability

  **Model Compatibility Improvements**

  * **ControlNet Type Models**: Enhanced compatibility fixes for ControlNet-type models working with Qwen Edit and Kontext workflows
  * **Flux Memory Optimization**: Adjusted Flux model memory usage factors for better resource utilization

  **Infrastructure & Reliability**

  * **Template Updates**: Updated to versions 0.1.66 and 0.1.68
  * **Documentation Cleanup**: Removed incompletely implemented models from readme to avoid user confusion
</Update>

<Update label="v0.3.52" description="August 23, 2025">
  **Enhanced Model Support & Qwen Image ControlNet Integration**

  This release significantly expands ControlNet capabilities and improves model compatibility, making ComfyUI workflows more versatile and reliable:

  **Qwen ControlNet Ecosystem**

  * **Diffsynth ControlNet Support**: Added support for Qwen Diffsynth ControlNets with Canny and depth conditioning, enabling precise edge and depth-based image control
  * **InstantX Qwen ControlNet**: Integrated InstantX Qwen ControlNet for expanded creative control options
  * **Inpaint ControlNet/Model Patches**: Enhanced inpainting capabilities with dedicated Diffsynth inpaint ControlNet support

  **Node Architecture & API Evolution**

  * **V3 Architecture Migration**: String nodes, Google Veo API, and Ideogram API nodes upgraded to V3 architecture for better performance and consistency
  * **Enhanced API Nodes**: OpenAI Chat node renamed to "OpenAI ChatGPT" for clarity, Gemini Chat node now includes copy button functionality
  * **Improved Usability**: API nodes now provide better user experience with clearer labeling and enhanced interaction features

  **Workflow Reliability & Performance**

  * **LTXV Noise Mask Fix**: Resolved key frame noise mask dimension issues when real noise masks exist, ensuring stable video workflow execution
  * **3D Latent Conditioning Control**: Fixed conditioning masks on 3D latents, enabling proper depth-aware conditioning control in advanced workflows
  * **Invalid Filename Handling**: Improved workflow save functionality with proper handling of invalid filenames, preventing save failures
  * **EasyCache & LazyCache**: Implemented advanced caching systems for improved workflow execution performance

  **Platform & Development Improvements**

  * **Python 3.13 Support**: Full compatibility with Python 3.13, keeping ComfyUI current with latest Python developments
  * **Frontend Update**: Updated to v1.25.10 with improved navigation and user interface enhancements
  * **Elementwise Fusions**: Added performance optimizations through elementwise operations fusion
  * **Navigation Mode Rollback**: Rolled back navigation default to traditional legacy mode, avoiding user experience issues caused by default enabled standard navigation mode. Users can still enable standard navigation mode in settings
</Update>

<Update label="v0.3.51" description="August 20, 2025">
  **Model Support**

  * **Qwen-Image-Edit Model**: Native support for Qwen-Image-Edit
  * **FluxKontextMultiReferenceLatentMethod Node**: Multi-reference input node for Flux workflows
  * **WAN 2.2 Fun Camera Model Support**: Support for video generation through camera control
  * **Template Updates**: Upgraded to version 0.1.62, added Wan2.2 Fun Camera and Qwen Image Edit templates

  **Core Function Improvements**

  * **Context Windows Support**: Enhanced sampling code to support longer sequence generation tasks
  * **SDPA Backend Optimization**: Improved Scaled Dot Product Attention backend settings for better performance

  **Multimedia Node Support**

  * **Audio Recording Node**: New native audio recording node, now you can record audio directly in ComfyUI
  * **Audio Video Integration**: Complete audio-video dependency integration

  **API Node Support Updates**

  * **GPT-5 Series Models**: Support for the latest GPT-5 models
  * **Kling V2-1 and V2-1-Master**: Updated video generation model functionality
  * **Minimax Hailuo Video Node**: New video generation node
  * **Vidu Video Node**: Vidu API node support
  * **Google Model Updates**: Added new Google Gemini models
  * **OpenAI API Fix**: Fixed MIME type errors in OpenAI API node input images

  **Performance Optimization**

  * **Intel GPU Compatibility**: Fixed Intel integrated GPU compatibility issues
  * **PyTorch Compatibility**: Enhanced compatibility with older PyTorch versions
  * **Torch Compile Optimization**: Improved torch compile behavior
  * **Memory Management**: Optimized installation size and memory efficiency

  **Frontend Changes**

  * **Subgraph Support**: Subgraph functionality support
  * **Shortcut Panel**: Added bottom shortcut panel
  * **UI Layout Modifications**: Modified terminal entry layout, added template, log panel and other entries
  * **Standard Canvas Mode**: Added standard canvas mode, can be switched in `Lite Graph` > `Canvas` > `Canvas Navigation Mode`
  * **Mini Map**: Added workflow mini map
  * **Tab Preview**: Added workflow tab preview
  * **Top Tab Menu Layout Adjustments**
</Update>

<Update label="v0.3.50" description="August 13, 2025">
  **Model Integration & Performance Enhancements**

  This release expands ComfyUI's model ecosystem with enhanced Qwen support, async API capabilities, and stability improvements for complex workflows:

  **Qwen Model Ecosystem**

  * **Qwen Image Model Support**: Improved integration including proper LoRA loading and model merging capabilities for sophisticated vision workflows
  * **Qwen Model Merging Node**: New dedicated node for merging Qwen image models, allowing creators to combine different model strengths
  * **SimpleTuner Lycoris LoRA Support**: Extended compatibility with SimpleTuner-trained Lycoris LoRAs for Qwen-Image models

  **API & Performance Infrastructure**

  * **Async API Nodes**: Introduction of asynchronous API nodes, enabling non-blocking workflow execution for better performance
  * **Memory Handling**: Enhanced RepeatLatentBatch node now properly handles multi-dimensional latents, fixing workflow interruptions
  * **WAN 2.2 Fun Control Support**: Added support for WAN 2.2 fun control features, expanding creative control for video workflows

  **Hardware Optimization & Compatibility**

  * **AMD GPU Improvements**: Enhanced AMD Radeon support with improved FP16 accuracy handling and performance optimization
  * **RDNA3 Architecture Fixes**: Resolved issues with gfx1201 GPUs when using Flux models with PyTorch attention
  * **Updated PyTorch Support**: Bumped CUDA and ROCM PyTorch versions with testing on Python 3.13 and CUDA 12.9

  **Developer Experience Enhancements**

  * **Cleaner Logging**: Feature flags now only display in verbose mode, reducing console noise
  * **Audio Processing Safety**: Enhanced torchaudio import safety checks prevent crashes when audio dependencies are unavailable
  * **Kling API Improvements**: Fixed image type parameter handling in Kling Image API nodes

  **Workflow Benefits**

  * **Async Workflow Execution**: New async API capabilities enable more responsive workflows when integrating external services
  * **Model Flexibility**: Expanded Qwen support allows for more diverse vision-language workflows with improved LoRA compatibility
  * **Hardware Utilization**: AMD GPU optimizations and updated PyTorch support improve performance across hardware configurations
  * **Batch Processing**: Fixed RepeatLatentBatch ensures reliable operation with complex multi-dimensional data structures
  * **Video Control**: WAN 2.2 fun control features provide advanced creative control for video generation workflows
</Update>

<Update label="v0.3.49" description="August 5, 2025">
  **UI Experience & Model Support**

  This release brings user experience improvements and model support that enhance workflow creation and performance:

  **User Interface Enhancements**

  * **Recently Used Items API**: New API for tracking recently used items in the interface, streamlining workflow creation
  * **Workflow Navigation**: Enhanced user experience with better organization of commonly accessed elements

  **Model Integration**

  * **Qwen Vision Model Support**: Initial support for Qwen image models with configuration options
  * **Image Processing**: Enhanced Qwen model integration allows for more versatile image analysis and generation workflows

  **Video Generation**

  * **Veo3 Video Generation**: Added Veo3 video generation node with integrated audio support
  * **Audio-Visual Synthesis**: Capability combining video and audio generation in a single node

  **Performance & Stability Improvements**

  * **Memory Management**: Optimized conditional VRAM usage through improved casting and device transfer operations
  * **Device Consistency**: Fixes ensuring all conditioning data and context remain on correct devices
  * **ControlNet Stability**: Resolved ControlNet compatibility issues, restoring functionality for image control workflows

  **Developer & System Enhancements**

  * **Error Handling**: Added warnings and crash prevention when conditioning devices don't match
  * **Template Updates**: Multiple template version updates (0.1.47, 0.1.48, 0.1.51) maintaining compatibility

  **Workflow Benefits**

  * **Faster Iteration**: Recently used items API enables quicker workflow assembly and modification
  * **Enhanced Creativity**: Qwen vision models open new possibilities for image understanding and manipulation workflows
  * **Video Production**: Veo3 integration transforms ComfyUI into a comprehensive multimedia creation platform
  * **Reliability**: Memory optimizations and device management fixes ensure stable operation with complex workflows
  * **Performance**: Optimized VRAM usage allows for more ambitious projects on systems with limited resources
</Update>

<Update label="v0.3.48" description="August 2, 2025">
  **API Enhancement & Performance Optimizations**

  This release introduces backend improvements and performance optimizations that enhance workflow execution and node development:

  **ComfyAPI Core Framework**

  * **ComfyAPI Core v0.0.2**: Update to the core API framework, providing improved stability and extensibility
  * **Partial Execution Support**: New backend support for partial workflow execution, enabling efficient processing of multi-stage workflows

  **Video Processing Improvements**

  * **WAN Camera Memory Optimization**: Enhanced memory management for WAN-based camera workflows, reducing VRAM usage
  * **WanFirstLastFrameToVideo Fix**: Resolved issue preventing proper video generation when clip vision components are not available

  **Performance & Model Optimizations**

  * **VAE Nonlinearity Enhancement**: Replaced manual activation functions with optimized torch.silu in VAE operations
  * **WAN VAE Optimizations**: Fine-tuning optimizations for WAN VAE operations, improving processing speed and memory efficiency

  **Node Schema Evolution**

  * **V3 Node Schema Definition**: Implementation of next-generation node schema system
  * **Template Updates**: Multiple template version updates (0.1.44, 0.1.45) ensuring compatibility

  **Workflow Development Benefits**

  * **Video Workflows**: Improved stability and performance for video generation pipelines
  * **Memory Management**: Optimized memory usage patterns enable more complex workflows on systems with limited VRAM
  * **API Reliability**: Core API enhancements provide more stable foundation for custom node development
  * **Execution Flexibility**: New partial execution capabilities allow for more efficient debugging and development
</Update>

<Update label="v0.3.47" description="July 30, 2025">
  **Memory Optimization & Large Model Performance**

  This release focuses on memory optimizations for large model workflows, improving performance with WAN 2.2 models and VRAM management:

  **WAN 2.2 Model Optimizations**

  * **Reduced Memory Footprint**: Eliminated unnecessary memory clones in WAN 2.2 VAE operations, reducing memory usage
  * **5B I2V Model Support**: Memory optimization for WAN 2.2 5B image-to-video models, making these models more accessible

  **Enhanced VRAM Management**

  * **Windows Large Card Support**: Added reserved VRAM allocation for high-end graphics cards on Windows
  * **Memory Allocation**: Improved memory management for users working with multiple large models simultaneously

  **Workflow Performance Benefits**

  * **VAE Processing**: WAN 2.2 VAE operations now run more efficiently with reduced memory overhead
  * **Large Model Inference**: Enhanced stability when working with billion-parameter models
  * **Batch Processing**: Memory optimizations enable better handling of batch operations with large models
</Update>

<Update label="v0.3.46" description="July 28, 2025">
  **Hardware Acceleration & Audio Processing**

  This release expands hardware support and enhances audio processing capabilities:

  **Audio Processing Enhancements**

  * **PyAV Audio Backend**: Replaced torchaudio.load with PyAV for more reliable audio processing in video workflows
  * **Audio Integration**: Enhanced audio handling for multimedia generation workflows

  **Hardware Support**

  * **Iluvatar CoreX Support**: Added native support for Iluvatar CoreX accelerators
  * **Intel XPU Optimization**: XPU support improvements including async offload capabilities
  * **AMD ROCm Enhancements**: Enabled PyTorch attention by default for gfx1201 on Torch 2.8
  * **CUDA Memory Management**: Fixed CUDA malloc to only activate on CUDA-enabled PyTorch installations

  **Sampling Algorithm Improvements**

  * **Euler CFG++ Enhancement**: Separated denoised and noise estimation processes in Euler CFG++ sampler
  * **WAN Model Support**: Added support for WAN (Wavelet-based Attention Network) models

  **Training Features**

  * **Training Nodes**: Added algorithm support, gradient accumulation, and optional gradient checkpointing
  * **Training Flexibility**: Better memory management and performance optimization for custom model training

  **Node & Workflow Enhancements**

  * **Moonvalley V2V Node**: Added Moonvalley Marey V2V node with enhanced input validation
  * **Negative Prompt Updates**: Improved negative prompt handling for Moonvalley nodes
  * **History API Enhancement**: Added map\_function parameter to get\_history API

  **API & System Improvements**

  * **Frontend Version Tracking**: Added required\_frontend\_version parameter in /system\_stats API response
  * **Device Information**: Enhanced XPU device name printing for better hardware identification
  * **Template Updates**: Multiple template updates (0.1.40, 0.1.41) ensuring compatibility

  **Developer Experience**

  * **Documentation Updates**: Enhanced README with examples and updated model integration guides
  * **Line Ending Fixes**: Improved cross-platform compatibility by standardizing line endings
  * **Code Cleanup**: Removed deprecated code and optimized components
</Update>

<Update label="v0.3.45" description="July 21, 2025">
  **Sampling & Training Improvements**

  This release introduces enhancements to sampling algorithms, training capabilities, and node functionality:

  **Sampling & Generation Features**

  * **SA-Solver Sampler**: New reconstructed SA-Solver sampling algorithm providing enhanced numerical stability
  * **Experimental CFGNorm Node**: Classifier-free guidance normalization for improved control over generation quality
  * **Nested Dual CFG Support**: Added nested style configuration to DualCFGGuider node
  * **SamplingPercentToSigma Node**: New utility node for precise sigma calculation from sampling percentages

  **Training Capabilities**

  * **Multi Image-Caption Dataset Support**: LoRA training node now handles multiple image-caption datasets simultaneously
  * **Training Loop Implementation**: Optimized training algorithms for improved convergence and stability
  * **Error Detection**: Added model detection error hints for LoRA operations

  **Platform & Performance Improvements**

  * **Async Node Support**: Full support for asynchronous node functions with earlier execution optimization
  * **Chroma Flexibility**: Un-hardcoded patch\_size parameter in Chroma
  * **LTXV VAE Decoder**: Switched to improved default padding mode for better image quality
  * **Safetensors Memory Management**: Added workaround for mmap issues

  **API & Integration Enhancements**

  * **Custom Prompt IDs**: API now allows specifying prompt IDs for better workflow tracking
  * **Kling API Optimization**: Increased polling timeout to prevent user timeouts
  * **History Token Cleanup**: Removed sensitive tokens from history items
  * **Python 3.9 Compatibility**: Fixed compatibility issues ensuring broader platform support

  **Bug Fixes & Stability**

  * **MaskComposite Fixes**: Resolved errors when destination masks have 2 dimensions
  * **Fresca Input/Output**: Corrected input and output handling for Fresca model workflows
  * **Reference Bug Fixes**: Resolved incorrect reference bugs in Gemini node implementations
  * **Line Ending Standardization**: Automated detection and removal of Windows line endings

  **Developer Experience**

  * **Warning Systems**: Added torch import mistake warnings to catch common configuration issues
  * **Template Updates**: Multiple template version updates (0.1.36, 0.1.37, 0.1.39) for improved custom node development
  * **Documentation**: Enhanced fast\_fp16\_accumulation documentation
</Update>

<Update label="v0.3.44" description="July 8, 2025">
  **Sampling & Model Control Enhancements**

  This release delivers improvements to sampling algorithms and model control systems:

  **Sampling Capabilities**

  * **TCFG Node**: Enhanced classifier-free guidance control for more nuanced generation control
  * **ER-SDE Sampler**: Migrated from VE to VP algorithm with new sampler node
  * **Skip Layer Guidance (SLG)**: Implementation for precise layer-level control during inference

  **Development Tools**

  * **Custom Node Management**: New `--whitelist-custom-nodes` argument pairs with `--disable-all-custom-nodes`
  * **Performance Optimizations**: Dual CFG node now optimizes automatically when CFG is 1.0
  * **GitHub Actions Integration**: Automated release webhook notifications

  **Image Processing Improvements**

  * **Transform Nodes**: Added ImageRotate and ImageFlip nodes for enhanced image manipulation
  * **ImageColorToMask Fix**: Corrected mask value returns for more accurate color-based masking
  * **3D Model Support**: Upload 3D models to custom subfolders for better organization

  **Guidance & Conditioning Enhancements**

  * **PerpNeg Guider**: Updated with improved pre and post-CFG handling
  * **Latent Conditioning Fix**: Resolved issues with conditioning at index > 0 for multi-step workflows
  * **Denoising Steps**: Added denoising step support to several samplers

  **Platform Stability**

  * **PyTorch Compatibility**: Fixed contiguous memory issues with PyTorch nightly builds
  * **FP8 Fallback**: Automatic fallback to regular operations when FP8 operations encounter exceptions
  * **Audio Processing**: Removed deprecated torchaudio.save function dependencies

  **Model Integration**

  * **Moonvalley Nodes**: Added native support for Moonvalley model workflows
  * **Scheduler Reordering**: Simple scheduler now defaults first
  * **Template Updates**: Multiple template version updates (0.1.31-0.1.35)

  **Security & Safety**

  * **Safe Loading**: Added warnings when loading files unsafely
  * **File Validation**: Enhanced checkpoint loading safety measures
</Update>

<Update label="v0.3.43" description="June 30, 2025">
  **Model Support & Workflow Reliability**

  This release brings improvements to model compatibility and workflow stability:

  **Expanded Model Documentation**: Added support documentation for Flux Kontext and Omnigen 2 models
  **VAE Encoding Improvements**: Removed unnecessary random noise injection during VAE encoding
  **Memory Management Fix**: Resolved a memory estimation bug affecting Kontext model usage
</Update>

<Update label="v0.3.41" description="June 17, 2025">
  **Model Support Additions**

  * **Cosmos Predict2 Support**: Implementation for text-to-image (2B and 14B models) and image-to-video generation workflows
  * **Flux Compatibility**: Chroma Text Encoder now works with regular Flux models
  * **LoRA Training Integration**: New native LoRA training node using weight adapter scheme

  **Performance & Hardware Optimizations**

  * **AMD GPU Enhancements**: Enabled FP8 operations and PyTorch attention on AMD GPUs
  * **Apple Silicon Fixes**: Addressed FP16 attention issues on Apple devices
  * **Flux Model Stability**: Resolved black image generation issues with certain Flux models

  **Sampling Improvements**

  * **Rectified Flow Samplers**: Added SEEDS and multistep DPM++ SDE samplers with RF support
  * **ModelSamplingContinuousEDM**: New cosmos\_rflow option for enhanced sampling control
  * **Memory Optimization**: Improved memory estimation for Cosmos models

  **Developer & Integration Features**

  * **SQLite Database Support**: Enhanced data management capabilities for custom nodes
  * **PyProject.toml Integration**: Automatic web folder registration from pyproject files
  * **Frontend Flexibility**: Support for semver suffixes and prerelease frontend versions
  * **Tokenizer Enhancements**: Configurable min\_length settings with tokenizer\_data

  **Quality of Life Improvements**

  * **Kontext Aspect Ratio Fix**: Resolved widget-only limitation
  * **SaveLora Consistency**: Standardized filename format across all save nodes
  * **Python Version Warnings**: Added alerts for outdated Python installations
  * **WebcamCapture Fixes**: Corrected IS\_CHANGED signature
</Update>

<Update label="v0.3.40" description="June 5, 2025">
  **Workflow Tools & Performance Optimizations**

  This release brings new workflow utilities and performance optimizations:

  **Workflow Tools**

  * **ImageStitch Node**: Concatenate multiple images seamlessly in your workflows
  * **GetImageSize Node**: Extract image dimensions with batch processing support
  * **Regex Replace Node**: Advanced text manipulation capabilities for workflows

  **Model Compatibility**

  * **Tensor Handling**: Streamlined list processing makes multi-model workflows more reliable
  * **BFL API Optimization**: Refined support for Kontext models with cleaner node interfaces
  * **Performance Boost**: Fused multiply-add operations in chroma processing for faster generation

  **Developer Experience**

  * **Custom Node Support**: Added pyproject.toml support for better dependency management
  * **Help Menu Integration**: New help system in the Node Library sidebar
  * **API Documentation**: Enhanced API nodes documentation

  **Frontend & UI Enhancements**

  * **Frontend Updated to v1.21.7**: Stability fixes and performance improvements
  * **Custom API Base Support**: Better subpath handling for custom deployment configurations
  * **Security Hardening**: XSS vulnerability fixes

  **Bug Fixes & Stability**

  * **Pillow Compatibility**: Updated deprecated API calls
  * **ROCm Support**: Improved version detection for AMD GPU users
  * **Template Updates**: Enhanced project templates for custom node development
</Update>


# ComfyUI Hunyuan Video Examples
Source: https://docs.comfy.org/tutorials/video/hunyuan/hunyuan-video

This guide shows how to use Hunyuan Text-to-Video and Image-to-Video workflows in ComfyUI

<video controls className="w-full aspect-video" src="https://github.com/user-attachments/assets/442afb73-3092-454f-bc46-02361c285930" />

Hunyuan Video series is developed and open-sourced by [Tencent](https://huggingface.co/tencent), featuring a hybrid architecture that supports both [Text-to-Video](https://github.com/Tencent/HunyuanVideo) and [Image-to-Video](https://github.com/Tencent/HunyuanVideo-I2V) generation with a parameter scale of 13B.

Technical features:

* **Core Architecture:** Uses a DiT (Diffusion Transformer) architecture similar to Sora, effectively fusing text, image, and motion information to improve consistency, quality, and alignment between generated video frames. A unified full-attention mechanism enables multi-view camera transitions while ensuring subject consistency.
* **3D VAE:** The custom 3D VAE compresses videos into a compact latent space, making image-to-video generation more efficient.
* **Superior Image-Video-Text Alignment:** Utilizing MLLM text encoders that excel in both image and video generation, better following text instructions, capturing details, and performing complex reasoning.

You can learn more through the official repositories: [Hunyuan Video](https://github.com/Tencent/HunyuanVideo) and [Hunyuan Video-I2V](https://github.com/Tencent/HunyuanVideo-I2V).

This guide will walk you through setting up both **Text-to-Video** and **Image-to-Video** workflows in ComfyUI.

<Tip>
  The workflow images in this tutorial contain metadata with model download information.

  Simply drag them into ComfyUI or use the menu `Workflows` -> `Open (ctrl+o)` to load the corresponding workflow, which will prompt you to download the required models.

  Alternatively, this guide provides direct model links if automatic downloads fail or you are not using the Desktop version. All models are available [here](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/tree/main/split_files) for download.
</Tip>

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Common Models for All Workflows

The following models are used in both Text-to-Video and Image-to-Video workflows. Please download and save them to the specified directories:

* [clip\_l.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/clip_l.safetensors?download=true)
* [llava\_llama3\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/llava_llama3_fp8_scaled.safetensors?download=true)
* [hunyuan\_video\_vae\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/vae/hunyuan_video_vae_bf16.safetensors?download=true)

Storage location:

```
ComfyUI/
 models/
    text_encoders/
       clip_l.safetensors
       llava_llama3_fp8_scaled.safetensors
    vae/
       hunyuan_video_vae_bf16.safetensors
```

## Hunyuan Text-to-Video Workflow

Hunyuan Text-to-Video was open-sourced in December 2024, supporting 5-second short video generation through natural language descriptions in both Chinese and English.

### 1. Workflow

Download the image below and drag it into ComfyUI to load the workflow:
![ComfyUI Workflow - Hunyuan Text-to-Video](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/t2v/kitchen.webp)

### 2. Manual Models Installation

Download [hunyuan\_video\_t2v\_720p\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_t2v_720p_bf16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models` folder.

Ensure you have all these model files in the correct locations:

```
ComfyUI/
 models/
    text_encoders/
       clip_l.safetensors                       // Shared model
       llava_llama3_fp8_scaled.safetensors      // Shared model
    vae/
       hunyuan_video_vae_bf16.safetensors       // Shared model
    diffusion_models/
        hunyuan_video_t2v_720p_bf16.safetensors  // T2V model
```

### 3. Steps to Run the Workflow

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_t2v.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=8cf9db23e82c1c31702966878d7d6326" alt="ComfyUI Hunyuan Video T2V Workflow" data-og-width="4004" width="4004" data-og-height="1810" height="1810" data-path="images/tutorial/advanced/hunyuanvideo/flow_diagram_t2v.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_t2v.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=46ecaef5c813cfe73e61b8f9f4cdbe6b 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_t2v.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=5d87f06e2721307cf44973080a3cc4c1 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_t2v.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=351426c249545789f7fddcd6bfdbd545 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_t2v.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=5f7c8c5315be22ad33d24525c7ea75af 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_t2v.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=121b5d422c627eb576f607141c91084c 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_t2v.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=7c1054fb6ba08ec4b3f388ecbd1292c2 2500w" />

1. Ensure the `DualCLIPLoader` node has loaded these models:
   * clip\_name1: clip\_l.safetensors
   * clip\_name2: llava\_llama3\_fp8\_scaled.safetensors
2. Ensure the `Load Diffusion Model` node has loaded `hunyuan_video_t2v_720p_bf16.safetensors`
3. Ensure the `Load VAE` node has loaded `hunyuan_video_vae_bf16.safetensors`
4. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

<Tip>
  When the `length` parameter in the `EmptyHunyuanLatentVideo` node is set to 1, the model can generate a static image.
</Tip>

## Hunyuan Image-to-Video Workflow

Hunyuan Image-to-Video model was open-sourced on March 6, 2025, based on the HunyuanVideo framework. It transforms static images into smooth, high-quality videos and also provides LoRA training code to customize special video effects like hair growth, object transformation, etc.

Currently, the Hunyuan Image-to-Video model has two versions:

* v1 "concat": Better motion fluidity but less adherence to the image guidance
* v2 "replace": Updated the day after v1, with better image guidance but seemingly less dynamic compared to v1

<div class="flex justify-between">
  <div class="text-center">
    <p>v1 "concat"</p>

    <img src="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/hunyuan_video_image_to_video.webp" alt="HunyuanVideo v1" />
  </div>

  <div class="text-center">
    <p>v2 "replace"</p>

    <img src="https://comfyanonymous.github.io/ComfyUI_examples/hunyuan_video/hunyuan_video_image_to_video_v2.webp" alt="HunyuanVideo v2" />
  </div>
</div>

### Shared Model for v1 and v2 Versions

Download the following file and save it to the `ComfyUI/models/clip_vision` directory:

* [llava\_llama3\_vision.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/clip_vision/llava_llama3_vision.safetensors?download=true)

### V1 "concat" Image-to-Video Workflow

#### 1. Workflow and Asset

Download the workflow image below and drag it into ComfyUI to load the workflow:
![ComfyUI Workflow - Hunyuan Image-to-Video v1](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/i2v/v1_robot.webp)

Download the image below, which we'll use as the starting frame for the image-to-video generation:
![Starting Frame](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/hunyuan-video/i2v/robot-ballet.png)

#### 2. Related models manual installation

* [hunyuan\_video\_image\_to\_video\_720p\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_image_to_video_720p_bf16.safetensors?download=true)

Ensure you have all these model files in the correct locations:

```
ComfyUI/
 models/
    clip_vision/
       llava_llama3_vision.safetensors                     // I2V shared model
    text_encoders/
       clip_l.safetensors                                  // Shared model
       llava_llama3_fp8_scaled.safetensors                 // Shared model
    vae/
       hunyuan_video_vae_bf16.safetensors                  // Shared model
    diffusion_models/
        hunyuan_video_image_to_video_720p_bf16.safetensors  // I2V v1 "concat" version model
```

#### 3. Steps to Run the Workflow

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v1.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=66d1fd271cc2a45a4fbcd92850074912" alt="ComfyUI Hunyuan Video I2V v1 Workflow" data-og-width="4604" width="4604" data-og-height="1780" height="1780" data-path="images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v1.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v1.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=c69870b0963cf8328ef655ada46aff2e 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v1.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=b8445274ec2b9c64952eca5d43d80afd 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v1.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=7d2f125a2e28660e4fe97dca2c223951 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v1.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=91eab50c724944dc6cf09f2213360ea4 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v1.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=6aacdffc0aa5c73dc82ca3857f89761b 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v1.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=080cb021fdef78a1b6dbbf5538f4a2fc 2500w" />

1. Ensure that `DualCLIPLoader` has loaded these models:
   * clip\_name1: clip\_l.safetensors
   * clip\_name2: llava\_llama3\_fp8\_scaled.safetensors
2. Ensure that `Load CLIP Vision` has loaded `llava_llama3_vision.safetensors`
3. Ensure that `Load Image Model` has loaded `hunyuan_video_image_to_video_720p_bf16.safetensors`
4. Ensure that `Load VAE` has loaded `vae_name: hunyuan_video_vae_bf16.safetensors`
5. Ensure that `Load Diffusion Model` has loaded `hunyuan_video_image_to_video_720p_bf16.safetensors`
6. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

### v2 "replace" Image-to-Video Workflow

The v2 workflow is essentially the same as the v1 workflow. You just need to download the **replace** model and use it in the `Load Diffusion Model` node.

#### 1. Workflow and Asset

Download the workflow image below and drag it into ComfyUI to load the workflow:
![ComfyUI Workflow - Hunyuan Image-to-Video v2](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/hunyuan-video/i2v/v2_fennec_gril.webp)

Download the image below, which we'll use as the starting frame for the image-to-video generation:
![Starting Frame](https://comfyanonymous.github.io/ComfyUI_examples/flux/flux_dev_example.png)

#### 2. Related models manual installation

* [hunyuan\_video\_v2\_replace\_image\_to\_video\_720p\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors?download=true)

Ensure you have all these model files in the correct locations:

```
ComfyUI/
 models/
    clip_vision/
       llava_llama3_vision.safetensors                                // I2V shared model
    text_encoders/
       clip_l.safetensors                                             // Shared model
       llava_llama3_fp8_scaled.safetensors                            // Shared model
    vae/
       hunyuan_video_vae_bf16.safetensors                             // Shared model
    diffusion_models/
        hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors  // V2 "replace" version model
```

#### 3. Steps to Run the Workflow

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v2.jpg?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=f26ddd8edc837fbf9d2bb4f6459a82ee" alt="ComfyUI Hunyuan Video I2V v2 Workflow" data-og-width="4604" width="4604" data-og-height="1780" height="1780" data-path="images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v2.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v2.jpg?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=d8baea25dfcaf7566c43eee90ddf30e2 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v2.jpg?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=095646b3588b08ac62943b23d4a4a381 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v2.jpg?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=c4fb1b7af59a8236189b723f182384c0 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v2.jpg?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=41e6f324ab148f773a1fbf7a68d1eecc 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v2.jpg?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=a428533a8b26e8a34f7cdc240d60a0c3 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/flow_diagram_i2v_v2.jpg?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=c4af9cc4415d6b5c355bae31a57488ae 2500w" />

1. Ensure the `DualCLIPLoader` node has loaded these models:
   * clip\_name1: clip\_l.safetensors
   * clip\_name2: llava\_llama3\_fp8\_scaled.safetensors
2. Ensure the `Load CLIP Vision` node has loaded `llava_llama3_vision.safetensors`
3. Ensure the `Load Image Model` node has loaded `hunyuan_video_image_to_video_720p_bf16.safetensors`
4. Ensure the `Load VAE` node has loaded `hunyuan_video_vae_bf16.safetensors`
5. Ensure the `Load Diffusion Model` node has loaded `hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors`
6. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## Try it yourself

Here are some images and prompts we provide. Based on that content or make an adjustment to create your own video.

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/humanoid_android_dressed_in_a_flowing.png?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=16d6b0ea15e14c74e8d2e5bfacfe4bf8" alt="example" data-og-width="1024" width="1024" data-og-height="1024" height="1024" data-path="images/tutorial/advanced/hunyuanvideo/humanoid_android_dressed_in_a_flowing.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/humanoid_android_dressed_in_a_flowing.png?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=fb99a02e45e6c7f270d9ce083790314c 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/humanoid_android_dressed_in_a_flowing.png?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=d9a4f1f52d7215e48a374391428822af 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/humanoid_android_dressed_in_a_flowing.png?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=9e906c0b832af401f05c4cc052dc8fcf 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/humanoid_android_dressed_in_a_flowing.png?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=c8da46339d87ae93596c5e61bd5ec0c3 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/humanoid_android_dressed_in_a_flowing.png?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=19badecb5287b9aa7230d6e5610f5d09 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/humanoid_android_dressed_in_a_flowing.png?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=d4a1c639e320e34572b495821a1a9408 2500w" />

```
Futuristic robot dancing ballet, dynamic motion, fast motion, fast shot, moving scene
```

***

<img src="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/samurai.png?fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=f8a76a691a7a9ebda088941350538375" alt="example" data-og-width="1024" width="1024" data-og-height="1024" height="1024" data-path="images/tutorial/advanced/hunyuanvideo/samurai.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/samurai.png?w=280&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=8a06ce9e5abc61680c56b8f50d351b51 280w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/samurai.png?w=560&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=69324963fb61e59c1228c1bc8e74e230 560w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/samurai.png?w=840&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=23bd95f28cf382d0653eabb11f7db077 840w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/samurai.png?w=1100&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=87ce3e4a1a8df22d48cdec2b10c27fdb 1100w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/samurai.png?w=1650&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=ba0bedcdaa6d746d03cea491e9ecd1db 1650w, https://mintcdn.com/dripart/OVkvfOwYrH10fL3Y/images/tutorial/advanced/hunyuanvideo/samurai.png?w=2500&fit=max&auto=format&n=OVkvfOwYrH10fL3Y&q=85&s=766950cfbd96fde9037be7605cf3674e 2500w" />

```
Samurai waving sword and hitting the camera. camera angle movement, zoom in, fast scene, super fast, dynamic
```

***

<img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hunyuanvideo/a_flying_car.png?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=aa2ce4f30c4d367170a8a696e62a7c7e" alt="example" data-og-width="1024" width="1024" data-og-height="1024" height="1024" data-path="images/tutorial/advanced/hunyuanvideo/a_flying_car.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hunyuanvideo/a_flying_car.png?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=6a8b34642f416da105998b377bde9dc8 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hunyuanvideo/a_flying_car.png?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=25f4eb4f74901534255e2b0bf552c218 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hunyuanvideo/a_flying_car.png?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=f68ad26ec9cef9228518da359e75acb8 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hunyuanvideo/a_flying_car.png?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=321b298270f365989f9ff1b39d8f9b97 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hunyuanvideo/a_flying_car.png?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=58b7c90be2e12ad056058c47720e5fbe 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hunyuanvideo/a_flying_car.png?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=e3d9e0536bdf426014c8f8a27de20065 2500w" />

```
flying car fastly moving and flying through the city
```

***

<img src="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hunyuanvideo/cyber_car_race.png?fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=875b086bd33ac131bda4fe6c718b9bc7" alt="example" data-og-width="1024" width="1024" data-og-height="1024" height="1024" data-path="images/tutorial/advanced/hunyuanvideo/cyber_car_race.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hunyuanvideo/cyber_car_race.png?w=280&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=ba4b75e9f735ba0cc5f136bec09360f9 280w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hunyuanvideo/cyber_car_race.png?w=560&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=c4a722a42e56ba171385fc7a78893227 560w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hunyuanvideo/cyber_car_race.png?w=840&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=4a04c0664b3e5bb3d6441243e5a7802c 840w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hunyuanvideo/cyber_car_race.png?w=1100&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=eab9135cbe99a2b42a3bf29cc901736a 1100w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hunyuanvideo/cyber_car_race.png?w=1650&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=2a2173570ea181e0711e6984dd13e3c8 1650w, https://mintcdn.com/dripart/NmGUk_QSXQXRVtZP/images/tutorial/advanced/hunyuanvideo/cyber_car_race.png?w=2500&fit=max&auto=format&n=NmGUk_QSXQXRVtZP&q=85&s=ea36cb8fda2435a47eec887bb130d6d8 2500w" />

```
cyberpunk car race in night city, dynamic, super fast, fast shot
```


# HunyuanVideo 1.5
Source: https://docs.comfy.org/tutorials/video/hunyuan/hunyuan-video-1-5

Learn how to use HunyuanVideo 1.5, a lightweight 8.3B parameter model for high-quality video generation on consumer GPUs

[HunyuanVideo 1.5](https://github.com/Tencent/HunyuanVideo) is a lightweight 8.3B parameter model developed by Tencent's Hunyuan team. It delivers flagship-quality video generation on consumer GPUs (24GB VRAM), dramatically lowering the barrier to entry without compromising on quality.

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release, we will update the Cloud after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Model highlights

* **Compact powerhouse**: Delivers SOTA performance comparable to larger models while running on consumer hardware.
* **Versatile generation**: Supports high-quality Text-to-Video and Image-to-Video (5-10s) with exceptional consistency.
* **Precise control**: Strong instruction following for camera movements, physics, and emotional expressions.
* **Cinematic quality**: Native 720p output (upscalable to 1080p) with professional aesthetics.
* **Rich features**: Supports diverse styles (realistic, anime, 3D) and in-video text rendering (Chinese/English).

## Workflow templates

[video\_hunyuan\_video\_1.5\_720p\_i2v.json](https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/video_hunyuan_video_1.5_720p_i2v.json)

[video\_hunyuan\_video\_1.5\_720p\_t2v.json](https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/video_hunyuan_video_1.5_720p_t2v.json)

## Model links

**text\_encoders**

* [qwen\_2.5\_vl\_7b\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/text_encoders/qwen_2.5_vl_7b_fp8_scaled.safetensors)
* [byt5\_small\_glyphxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/text_encoders/byt5_small_glyphxl_fp16.safetensors)

**diffusion\_models**

* [hunyuanvideo1.5\_1080p\_sr\_distilled\_fp16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/diffusion_models/hunyuanvideo1.5_1080p_sr_distilled_fp16.safetensors)
* [hunyuanvideo1.5\_720p\_t2v\_fp16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/diffusion_models/hunyuanvideo1.5_720p_t2v_fp16.safetensors)

**vae**

* [hunyuanvideo15\_vae\_fp16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_1.5_repackaged/resolve/main/split_files/vae/hunyuanvideo15_vae_fp16.safetensors)

Model Storage Location

```
:open_file_folder: ComfyUI/
 :open_file_folder: models/
    :open_file_folder: text_encoders/
          qwen_2.5_vl_7b_fp8_scaled.safetensors
          byt5_small_glyphxl_fp16.safetensors
    :open_file_folder: diffusion_models/
          hunyuanvideo1.5_1080p_sr_distilled_fp16.safetensors
          hunyuanvideo1.5_720p_t2v_fp16.safetensors
    :open_file_folder: vae/
           hunyuanvideo15_vae_fp16.safetensors
```


# Get proxyvidutasks creations
Source: https://docs.comfy.org/api-reference/api-nodes/get-proxyvidutasks-creations

https://api.comfy.org/openapi get /proxy/vidu/tasks/{id}/creations



# Post proxyviduimg2video
Source: https://docs.comfy.org/api-reference/api-nodes/post-proxyviduimg2video

https://api.comfy.org/openapi post /proxy/vidu/img2video



# Post proxyvidureference2video
Source: https://docs.comfy.org/api-reference/api-nodes/post-proxyvidureference2video

https://api.comfy.org/openapi post /proxy/vidu/reference2video



# Post proxyvidustart end2video
Source: https://docs.comfy.org/api-reference/api-nodes/post-proxyvidustart-end2video

https://api.comfy.org/openapi post /proxy/vidu/start-end2video



# Post proxyvidutext2video
Source: https://docs.comfy.org/api-reference/api-nodes/post-proxyvidutext2video

https://api.comfy.org/openapi post /proxy/vidu/text2video



# Add review to a specific version of a node
Source: https://docs.comfy.org/api-reference/registry/add-review-to-a-specific-version-of-a-node

https://api.comfy.org/openapi post /nodes/{nodeId}/reviews



# Claim nodeId into publisherId for the authenticated publisher
Source: https://docs.comfy.org/api-reference/registry/claim-nodeid-into-publisherid-for-the-authenticated-publisher

https://api.comfy.org/openapi post /publishers/{publisherId}/nodes/{nodeId}/claim-my-node
This endpoint allows a publisher to claim an unclaimed node that they own the repo, which is identified by the nodeId. The unclaimed node's repository must be owned by the authenticated user.




# Create a new custom node
Source: https://docs.comfy.org/api-reference/registry/create-a-new-custom-node

https://api.comfy.org/openapi post /publishers/{publisherId}/nodes



# Create a new personal access token
Source: https://docs.comfy.org/api-reference/registry/create-a-new-personal-access-token

https://api.comfy.org/openapi post /publishers/{publisherId}/tokens



# Create a new publisher
Source: https://docs.comfy.org/api-reference/registry/create-a-new-publisher

https://api.comfy.org/openapi post /publishers



# create comfy-nodes for certain node
Source: https://docs.comfy.org/api-reference/registry/create-comfy-nodes-for-certain-node

https://api.comfy.org/openapi post /nodes/{nodeId}/versions/{version}/comfy-nodes



# Create Node Translations
Source: https://docs.comfy.org/api-reference/registry/create-node-translations

https://api.comfy.org/openapi post /nodes/{nodeId}/translations



# Delete a publisher
Source: https://docs.comfy.org/api-reference/registry/delete-a-publisher

https://api.comfy.org/openapi delete /publishers/{publisherId}



# Delete a specific node
Source: https://docs.comfy.org/api-reference/registry/delete-a-specific-node

https://api.comfy.org/openapi delete /publishers/{publisherId}/nodes/{nodeId}



# Delete a specific personal access token
Source: https://docs.comfy.org/api-reference/registry/delete-a-specific-personal-access-token

https://api.comfy.org/openapi delete /publishers/{publisherId}/tokens/{tokenId}



# Get information about the calling user.
Source: https://docs.comfy.org/api-reference/registry/get-information-about-the-calling-user

https://api.comfy.org/openapi get /users



# get specify comfy-node based on its id
Source: https://docs.comfy.org/api-reference/registry/get-specify-comfy-node-based-on-its-id

https://api.comfy.org/openapi get /nodes/{nodeId}/versions/{version}/comfy-nodes/{comfyNodeName}



# list all comfy-nodes
Source: https://docs.comfy.org/api-reference/registry/list-all-comfy-nodes

https://api.comfy.org/openapi get /comfy-nodes



# List all node versions given some filters.
Source: https://docs.comfy.org/api-reference/registry/list-all-node-versions-given-some-filters

https://api.comfy.org/openapi get /versions



# List all versions of a node
Source: https://docs.comfy.org/api-reference/registry/list-all-versions-of-a-node

https://api.comfy.org/openapi get /nodes/{nodeId}/versions



# list comfy-nodes for node version
Source: https://docs.comfy.org/api-reference/registry/list-comfy-nodes-for-node-version

https://api.comfy.org/openapi get /nodes/{nodeId}/versions/{version}/comfy-nodes



# Publish a new version of a node
Source: https://docs.comfy.org/api-reference/registry/publish-a-new-version-of-a-node

https://api.comfy.org/openapi post /publishers/{publisherId}/nodes/{nodeId}/versions



# Retrieve a node by ComfyUI node name
Source: https://docs.comfy.org/api-reference/registry/retrieve-a-node-by-comfyui-node-name

https://api.comfy.org/openapi get /comfy-nodes/{comfyNodeName}/node
Returns the node that contains a ComfyUI node with the specified name



# Retrieve a publisher by ID
Source: https://docs.comfy.org/api-reference/registry/retrieve-a-publisher-by-id

https://api.comfy.org/openapi get /publishers/{publisherId}



# Retrieve a specific node by ID
Source: https://docs.comfy.org/api-reference/registry/retrieve-a-specific-node-by-id

https://api.comfy.org/openapi get /nodes/{nodeId}
Returns the details of a specific node.



# Retrieve a specific version of a node
Source: https://docs.comfy.org/api-reference/registry/retrieve-a-specific-version-of-a-node

https://api.comfy.org/openapi get /nodes/{nodeId}/versions/{versionId}



# Retrieve all nodes
Source: https://docs.comfy.org/api-reference/registry/retrieve-all-nodes

https://api.comfy.org/openapi get /publishers/{publisherId}/nodes



# Retrieve all nodes
Source: https://docs.comfy.org/api-reference/registry/retrieve-all-nodes-1

https://api.comfy.org/openapi get /publishers/{publisherId}/nodes/v2



# Retrieve all publishers
Source: https://docs.comfy.org/api-reference/registry/retrieve-all-publishers

https://api.comfy.org/openapi get /publishers



# Retrieve all publishers for a given user
Source: https://docs.comfy.org/api-reference/registry/retrieve-all-publishers-for-a-given-user

https://api.comfy.org/openapi get /users/publishers/



# Retrieve multiple node versions in a single request
Source: https://docs.comfy.org/api-reference/registry/retrieve-multiple-node-versions-in-a-single-request

https://api.comfy.org/openapi post /bulk/nodes/versions



# Retrieve permissions the user has for a given publisher
Source: https://docs.comfy.org/api-reference/registry/retrieve-permissions-the-user-has-for-a-given-publisher

https://api.comfy.org/openapi get /publishers/{publisherId}/nodes/{nodeId}/permissions



# Retrieve permissions the user has for a given publisher
Source: https://docs.comfy.org/api-reference/registry/retrieve-permissions-the-user-has-for-a-given-publisher-1

https://api.comfy.org/openapi get /publishers/{publisherId}/permissions



# Retrieves a list of nodes
Source: https://docs.comfy.org/api-reference/registry/retrieves-a-list-of-nodes

https://api.comfy.org/openapi get /nodes
Returns a paginated list of nodes across all publishers.



# Retrieves a list of nodes
Source: https://docs.comfy.org/api-reference/registry/retrieves-a-list-of-nodes-1

https://api.comfy.org/openapi get /nodes/search
Returns a paginated list of nodes across all publishers.



# Returns a node version to be installed.
Source: https://docs.comfy.org/api-reference/registry/returns-a-node-version-to-be-installed

https://api.comfy.org/openapi get /nodes/{nodeId}/install
Retrieves the node data for installation, either the latest or a specific version.



# Unpublish (delete) a specific version of a node
Source: https://docs.comfy.org/api-reference/registry/unpublish-delete-a-specific-version-of-a-node

https://api.comfy.org/openapi delete /publishers/{publisherId}/nodes/{nodeId}/versions/{versionId}



# Update a publisher
Source: https://docs.comfy.org/api-reference/registry/update-a-publisher

https://api.comfy.org/openapi put /publishers/{publisherId}



# Update a specific comfy-node
Source: https://docs.comfy.org/api-reference/registry/update-a-specific-comfy-node

https://api.comfy.org/openapi put /nodes/{nodeId}/versions/{version}/comfy-nodes/{comfyNodeName}



# Update a specific node
Source: https://docs.comfy.org/api-reference/registry/update-a-specific-node

https://api.comfy.org/openapi put /publishers/{publisherId}/nodes/{nodeId}



# Update changelog and deprecation status of a node version
Source: https://docs.comfy.org/api-reference/registry/update-changelog-and-deprecation-status-of-a-node-version

https://api.comfy.org/openapi put /publishers/{publisherId}/nodes/{nodeId}/versions/{versionId}
Update only the changelog and deprecated status of a specific version of a node.



# Validate if a publisher username is available
Source: https://docs.comfy.org/api-reference/registry/validate-if-a-publisher-username-is-available

https://api.comfy.org/openapi get /publishers/validate
Checks if the publisher username is already taken.



# Get release notes
Source: https://docs.comfy.org/api-reference/releases/get-release-notes

https://api.comfy.org/openapi get /releases
Fetch release notes from Strapi with caching



