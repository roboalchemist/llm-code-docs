# Source: https://docs.comfy.org/tutorials/controlnet/pose-controlnet-2-pass.md

# ComfyUI Pose ControlNet Usage Example

> This guide will introduce you to the basic concepts of Pose ControlNet, and demonstrate how to generate large-sized images in ComfyUI using a two-pass generation approach

## Introduction to OpenPose

[OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) is an open-source real-time multi-person pose estimation system developed by Carnegie Mellon University (CMU), representing a significant breakthrough in the field of computer vision. The system can simultaneously detect multiple people in an image, capturing:

* **Body skeleton**: 18 keypoints, including head, shoulders, elbows, wrists, hips, knees, and ankles
* **Facial expressions**: 70 facial keypoints for capturing micro-expressions and facial contours
* **Hand details**: 21 hand keypoints for precisely expressing finger positions and gestures
* **Foot posture**: 6 foot keypoints, recording standing postures and movement details

<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/openpose_example.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=bf85624e63f724a95635801aea0af8d5" alt="OpenPose Example" data-og-width="2048" width="2048" data-og-height="512" height="512" data-path="images/tutorial/controlnet/openpose_example.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/openpose_example.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=b41f289b721359a33ffc37287ac8384a 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/openpose_example.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=d51ebc3cd95276ccefa02559efe695f8 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/openpose_example.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=f2df9f53281cd7a27c4f1ed6a610d924 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/openpose_example.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=6e4256760e75758f7e53e80e7b1a82ad 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/openpose_example.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=974b7c1b9c31f679d4f9b8e13b5f3ef9 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/openpose_example.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=558727680b244ba2a38285e7fd029f25 2500w" />

In AI image generation, skeleton structure maps generated by OpenPose serve as conditional inputs for ControlNet, enabling precise control over the posture, actions, and expressions of generated characters. This allows us to generate realistic human figures with expected poses and actions, greatly improving the controllability and practical value of AI-generated content.
Particularly for early Stable Diffusion 1.5 series models, skeletal maps generated by OpenPose can effectively prevent issues with distorted character actions, limbs, and expressions.

## ComfyUI 2-Pass Pose ControlNet Usage Example

### 1. Pose ControlNet Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:

![ComfyUI Workflow - Pose ControlNet](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/pose_controlnet_2_pass.png)

<Tip>
  Images with workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -> `Open (ctrl+o)`.
  This image already includes download links for the corresponding models, and dragging it into ComfyUI will automatically prompt for downloads.
</Tip>

Please download the image below, which we will use as input:

![ComfyUI Pose Input Image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/controlnet/pose_controlnet_2_pass_input.png)

### 2. Manual Model Installation

<Note>
  If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:
</Note>

* [control\_v11p\_sd15\_openpose\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors?download=true)
* [majicmixRealistic\_v7.safetensors](https://civitai.com/api/download/models/176425?type=Model\&format=SafeTensor\&size=pruned\&fp=fp16)
* [japaneseStyleRealistic\_v20.safetensors](https://civitai.com/api/download/models/85426?type=Model\&format=SafeTensor\&size=pruned\&fp=fp16)
* [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)

```
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── majicmixRealistic_v7.safetensors
│   │   └── japaneseStyleRealistic_v20.safetensors
│   ├── vae/
│   │   └── vae-ft-mse-840000-ema-pruned.safetensors
│   └── controlnet/
│       └── control_v11p_sd15_openpose_fp16.safetensors
```

### 3. Step-by-Step Workflow Execution

<img src="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_pose_controlnet_2_pass.jpg?fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=f953cc0188bb242b6f03389d295bf551" alt="ComfyUI Workflow - Pose ControlNet Flow Diagram" data-og-width="1803" width="1803" data-og-height="1081" height="1081" data-path="images/tutorial/controlnet/flow_diagram_pose_controlnet_2_pass.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_pose_controlnet_2_pass.jpg?w=280&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=e2ca788a11b7d9415b05dfc430258b01 280w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_pose_controlnet_2_pass.jpg?w=560&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=e7132662b1d03152042ca49cafc93d8f 560w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_pose_controlnet_2_pass.jpg?w=840&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=dd612fbf76ab6fe38306afd63a128c17 840w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_pose_controlnet_2_pass.jpg?w=1100&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=9fd99919c6ea83863a61a530ce696148 1100w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_pose_controlnet_2_pass.jpg?w=1650&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=839e819834805cd482efc81a3f2ca6a6 1650w, https://mintcdn.com/dripart/TwfNQ2dEaWQA7tIL/images/tutorial/controlnet/flow_diagram_pose_controlnet_2_pass.jpg?w=2500&fit=max&auto=format&n=TwfNQ2dEaWQA7tIL&q=85&s=f039ec9330579f55a5a958fe8dc291e2 2500w" />

Follow these steps according to the numbered markers in the image:

1. Ensure that `Load Checkpoint` can load **majicmixRealistic\_v7.safetensors**
2. Ensure that `Load VAE` can load **vae-ft-mse-840000-ema-pruned.safetensors**
3. Ensure that `Load ControlNet Model` can load **control\_v11p\_sd15\_openpose\_fp16.safetensors**
4. Click the select button in the `Load Image` node to upload the pose input image provided earlier, or use your own OpenPose skeleton map
5. Ensure that `Load Checkpoint` can load **japaneseStyleRealistic\_v20.safetensors**
6. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## Explanation of the Pose ControlNet 2-Pass Workflow

This workflow uses a two-pass image generation approach, dividing the image creation process into two phases:

### First Phase: Basic Pose Image Generation

In the first phase, the **majicmixRealistic\_v7** model is combined with Pose ControlNet to generate an initial character pose image:

1. First, load the majicmixRealistic\_v7 model via the `Load Checkpoint` node
2. Load the pose control model through the `Load ControlNet Model` node
3. The input pose image is fed into the `Apply ControlNet` node and combined with positive and negative prompt conditions
4. The first `KSampler` node (typically using 20-30 steps) generates a basic character pose image
5. The pixel-space image for the first phase is obtained through `VAE Decode`

This phase primarily focuses on correct character posture, pose, and basic structure, ensuring that the generated character conforms to the input skeletal pose.

### Second Phase: Style Optimization and Detail Enhancement

In the second phase, the output image from the first phase is used as a reference, with the **japaneseStyleRealistic\_v20** model performing stylization and detail enhancement:

1. The image generated in the first phase creates a larger resolution latent space through the `Upscale latent` node
2. The second `Load Checkpoint` loads the japaneseStyleRealistic\_v20 model, which focuses on details and style
3. The second `KSampler` node uses a lower `denoise` strength (typically 0.4-0.6) for refinement, preserving the basic structure from the first phase
4. Finally, a higher quality, larger resolution image is output through the second `VAE Decode` and `Save Image` nodes

This phase primarily focuses on style consistency, detail richness, and enhancing overall image quality.

## Advantages of 2-Pass Image Generation

Compared to single-pass generation, the two-pass image generation method offers the following advantages:

1. **Higher Resolution**: Two-pass processing can generate high-resolution images beyond the capabilities of single-pass generation
2. **Style Blending**: Can combine advantages of different models, such as using a realistic model in the first phase and a stylized model in the second phase
3. **Better Details**: The second phase can focus on optimizing details without having to worry about overall structure
4. **Precise Control**: Once pose control is completed in the first phase, the second phase can focus on refining style and details
5. **Reduced GPU Load**: Generating in two passes allows for high-quality large images with limited GPU resources

<Tip>
  To learn more about techniques for mixing multiple ControlNets, please refer to the [Mixing ControlNet Models](/tutorials/controlnet/mixing-controlnets) tutorial.
</Tip>
