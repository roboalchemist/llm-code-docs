# Source: https://docs.comfy.org/tutorials/partner-nodes/kling/kling-motion-control.md

> ## Documentation Index
> Fetch the complete documentation index at: https://docs.comfy.org/llms.txt
> Use this file to discover all available pages before exploring further.

# Kling 2.6 Motion Control API Node ComfyUI Official Example

> Learn how to use the Kling 2.6 Motion Control Partner node in ComfyUI for precise motion transfer from reference videos to character images

Kling 2.6 Motion Control is a specialized multimodal model developed by Kuaishou that enables precise motion transfer from reference videos to character images. By combining a **Reference Image** (your character) and a **Motion Reference Video** (the action), the AI applies the movement, expression, and pacing of the video to your static character while maintaining their identity.

Unlike standard image-to-video generation that guesses motion from text prompts, Motion Control uses a reference video as the movement blueprint. The model acts as a digital puppeteer, extracting choreography from your reference video and applying it to your character with cinema-grade fidelity.

## Product highlights

* **Complex motion handling**: Execute complicated sequences like dance routines, martial arts, or athletic movements without losing character coherence. The model understands weight transfer and momentum for realistic physical impact.
* **Precision hand and finger performance**: Hands have traditionally been a weak point for AI video. This feature specifically improves finger articulation and hand movements by mimicking real footage, making it ideal for presentations and demonstrations.
* **Scene and environment flexibility**: Use text prompts to change the environment while the character continues their referenced motion. You're not limited to the reference video's background.
* **Advanced camera and perspective modes**: Granular control over how the camera interprets your reference with distinct orientation modes.
* **30-second one-shot support**: Generate continuous actions up to 30 seconds in a single generation, enabling uninterrupted character motion for narrative scenes.

## Character orientation modes

The `character_orientation` parameter determines how the model interprets spatial information and constrains output duration:

| Mode    | Description                                                                                                                                        | Max Duration |
| ------- | -------------------------------------------------------------------------------------------------------------------------------------------------- | ------------ |
| `video` | Output character orientation matches the reference video. Best for complex full-body performances like dance sequences and elaborate choreography. | 30 seconds   |
| `image` | Output character orientation matches the reference image. Best for portrait animations with camera movement like pans, tilts, and tracking shots.  | 10 seconds   |

## Model tiers

| Tier         | Resolution | Best For                                                                                                      |
| ------------ | ---------- | ------------------------------------------------------------------------------------------------------------- |
| **Standard** | 720p       | Simple animations, social media content, memes, and quick tests. Faster and more credit-efficient.            |
| **Pro**      | 1080p      | Complex choreography, intricate hand movements, professional marketing assets, and broadcast-quality content. |

<Tip>
  To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](/tutorials/partner-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.
</Tip>

<Tip>
  <Tabs>
    <Tab title="Portable or self deployed users">
      Make sure your ComfyUI is updated.

      * [Download ComfyUI](https://www.comfy.org/download)
      * [Update Guide](/installation/update_comfyui)

      Workflows in this guide can be found in the [Workflow Templates](/interface/features/template).
      If you can't find them in the template, your ComfyUI may be outdated. (Desktop version's update will delay sometime)

      If nodes are missing when loading a workflow, possible reasons:

      1. You are not using the latest ComfyUI version (Nightly version)
      2. Some nodes failed to import at startup
    </Tab>

    <Tab title="Desktop or Cloud users">
      * The Desktop is base on ComfyUI stable release, it will auto-update when there is a new Desktop stable release available.
      * [Cloud](https://cloud.comfy.org) will update after ComfyUI stable release.

      So, if you find any core node missing in this document, it might be because the new core nodes have not yet been released in the latest stable version. Please wait for the next stable release.
    </Tab>
  </Tabs>
</Tip>

## Kling 2.6 Motion Control workflow

<a className="prose" target="_blank" href="https://raw.githubusercontent.com/Comfy-Org/workflow_templates/refs/heads/main/templates/api_kling_motion_control.json" style={{ display: 'inline-block', backgroundColor: '#0078D6', color: '#ffffff', padding: '10px 20px', borderRadius: '8px', borderColor: "transparent", textDecoration: 'none', fontWeight: 'bold'}}>
  <p className="prose" style={{ margin: 0, fontSize: "0.8rem" }}>Download the workflow file in JSON format</p>
</a>

## Input requirements

* **Image formats**: JPG, PNG, WEBP, GIF, AVIF (max 10MB)
* **Video formats**: MP4, MOV, WEBM, M4V, GIF (max 100MB)
* **Video duration**: 3-30 seconds depending on orientation mode
* **Minimum resolution**: 720px width and height
* **Subject visibility**: Character must clearly show head, shoulders, and torso

## Tips for better results

* **Match aspect ratios**: Ensure your character image and reference video have similar aspect ratios (e.g., both 16:9 or both 9:16) to prevent awkward stretching or cropping.
* **Clean backgrounds**: Use reference videos with simple or static backgrounds for best motion extraction. High-contrast videos where the actor's silhouette is distinct work best.
* **Clear character angles**: If your reference video shows rotation, use 3D-style characters or realistic photos that handle rotation better. Flat 2D cartoons may struggle with back views.
* **Visible limbs**: Ensure the character's limbs are visible in the source image. If a character has hands in pockets but the motion requires waving, the AI will hallucinate hands, often leading to artifacts.
* **Leave breathing room**: Leave negative space around the subject. If the character will dance or move arms wide, they need space within the frame to avoid clipping.
* **Framing alignment**: Match the framing between your image and reference video. Use a close-up reference for face animations, or a full-body reference for walking/dancing sequences.
