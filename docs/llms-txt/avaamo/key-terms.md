# Source: https://docs.avaamo.com/user-guide/how-to/build-agents/create-universal-agent/key-terms.md

# Source: https://docs.avaamo.com/user-guide/llamb/llamb-filters/key-terms.md

# Source: https://docs.avaamo.com/user-guide/llamb/key-terms.md

# Source: https://docs.avaamo.com/user-guide/how-to/build-agents/create-universal-agent/key-terms.md

# Source: https://docs.avaamo.com/user-guide/llamb/llamb-filters/key-terms.md

# Source: https://docs.avaamo.com/user-guide/llamb/key-terms.md

# Key terms

This article introduces a few key terms specific to the LLaMB product in the Avaamo Conversational AI Platform:

* **LLaMB**: `LLaMB - Large Language Model for Business` is an exclusive product offering from the Avaamo Conversational AI Platform to harness the power of enterprise content coupled with Generative AI in your agents. It is a full stack, low code toolset to build, deploy, and maintain LLM ([Large Language Model](https://en.wikipedia.org/wiki/Large_language_model)) applications at enterprise scale. See [Overview - Key features](https://docs.avaamo.com/user-guide/llamb/overview-key-features), for more information on LLaMB.
* **Generations**: An agent can either respond to users by referencing a previously stored answer (in its cache), or by using an LLM. Only the LLM-based responses count towards your generation quota. Generations are displayed in the `LLaMB -> Usage` page at the account level. See [Generation Usage](https://docs.avaamo.com/user-guide/how-to/manage-platform-settings/usage-reports/llamb-usage#generation-usage), for more information.
* **Cached responses**: For faster results and better efficiency the responses generated by LLaMB are cached. If a user asks the same or similar query, then instead of generating a response from LLM, the cached response is displayed to the user. See [Generation Usage](https://docs.avaamo.com/user-guide/how-to/manage-platform-settings/usage-reports/llamb-usage#generation-usage), for more information.
