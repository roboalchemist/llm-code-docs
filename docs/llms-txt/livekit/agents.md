# Source: https://docs.livekit.io/reference/python/v1/livekit/agents.md

# Source: https://docs.livekit.io/deploy/agents.md

# Source: https://docs.livekit.io/frontends/telephony/agents.md

# Source: https://docs.livekit.io/agents.md

# Source: https://docs.livekit.io/intro/basics/agents.md

LiveKit docs › Understanding LiveKit › Building AI agents

---

# Building AI agents

> Build AI agents that interact with users through realtime media and data streams.

## Overview

Build AI agents that join LiveKit rooms as participants, process realtime media and data streams, and interact with users through voice, text, and vision. The [LiveKit Agents framework](https://docs.livekit.io/agents.md) provides everything you need to build production-ready voice AI agents and programmatic participants.

When you build agents with the Agents framework, they join rooms as participants just like users from frontend apps. Agents can process audio, video, and data streams in realtime, making them ideal for voice assistants, multimodal AI applications, and custom programmatic participants.

The framework allows you to add Python or Node.js programs to any LiveKit room as full realtime participants. It includes tools and abstractions that make it easy to feed realtime media and data through an AI pipeline that works with any provider, and to publish realtime results back to the room.

## Getting started

Build your first agent with these resources:

- **[Voice AI quickstart](https://docs.livekit.io/agents/start/voice-ai.md)**: Build and deploy a simple voice assistant with Python or Node.js in less than 10 minutes.

- **[LiveKit Agent Builder](https://docs.livekit.io/agents/start/builder.md)**: Prototype and deploy voice agents directly in your browser, without writing any code.

## Learn more

For complete documentation on building agents:

- **[Agents framework](https://docs.livekit.io/agents.md)**: Learn how to build AI agents and programmatic participants with the LiveKit Agents framework.

- **[Multimodality](https://docs.livekit.io/agents/multimodality.md)**: Learn how to configure agents to process speech, text, and vision inputs.

- **[Logic & structure](https://docs.livekit.io/agents/logic.md)**: Learn how to structure your agent's logic and behavior with sessions, tasks, and workflows.

- **[Agent server](https://docs.livekit.io/agents/server.md)**: Learn how agent servers manage your agents' lifecycle and deployment.

- **[Models](https://docs.livekit.io/agents/models.md)**: Explore the full list of AI models and providers available for your agents.

---

This document was rendered at 2026-02-03T03:24:53.412Z.
For the latest version of this document, see [https://docs.livekit.io/intro/basics/agents.md](https://docs.livekit.io/intro/basics/agents.md).

To explore all LiveKit documentation, see [llms.txt](https://docs.livekit.io/llms.txt).