# Source: https://exa.ai/docs/reference/contents-retrieval.md

> ## Documentation Index
> Fetch the complete documentation index at: https://exa.ai/docs/llms.txt
> Use this file to discover all available pages before exploring further.

# Contents Retrieval

***

When using the Exa API, you can request different types of content to be returned for each search result.

## Text (text=True)

Returns the full text content of the result, formatted as markdown. It extracts the main content (like article body text) while filtering out navigation elements, pop-ups, and other peripheral text. This is extractive content taken directly from the page's source.

### Content Filtering Options

<Note>
  **Important**: Content filtering options (`verbosity`, `includeSections`, `excludeSections`) require `livecrawl: "always"` to take effect. These filters are applied during the live crawling process.
</Note>

You can control the level of detail and which page sections are included using these options:

1. **Verbosity** - Controls overall content detail level:
   * `compact` (default): Most concise output, main content only
   * `standard`: Balanced content with more detail
   * `full`: Complete content including all sections

2. **Section Filtering** - Include or exclude specific semantic sections:

   * `includeSections`: Only include content from specified sections
   * `excludeSections`: Remove content from specified sections

   Available section tags:

   * `header` - Page header content
   * `navigation` - Navigation menus
   * `banner` - Banner/hero sections
   * `body` - Main body content
   * `sidebar` - Sidebar content
   * `footer` - Page footer
   * `metadata` - Page metadata

Example configuration:

```json  theme={null}
{
  "contents": {
    "text": {
      "verbosity": "standard",
      "includeSections": ["body", "header"]
    },
    "livecrawl": "always"
  }
}
```

Or to exclude noisy sections:

```json  theme={null}
{
  "contents": {
    "text": {
      "excludeSections": ["navigation", "footer", "sidebar"]
    },
    "livecrawl": "always"
  }
}
```

## Summary (summary=True)

Provides a concise summary generated from the text, tailored to a specific query you provide. This is abstractive content created by processing the source text using Gemini Flash.

### Structured Summaries

You can also request structured summaries by providing a JSON schema:

```json  theme={null}
{
  "summary": {
    "query": "Provide company information",
    "schema": {
      "$schema": "http://json-schema.org/draft-07/schema#",
      "title": "Company Information",
      "type": "object",
      "properties": {
        "name": { "type": "string", "description": "The name of the company" },
        "industry": { "type": "string", "description": "The industry the company operates in" },
        "foundedYear": { "type": "number", "description": "The year the company was founded" }
      },
      "required": ["name", "industry"]
    }
  }
}
```

The API will return the summary as a JSON string that matches your schema structure, which you can parse to access the structured data.

## Highlights

Delivers key excerpts from the text that are most relevant to your search query, emphasizing important information within the content. This is also extractive content from the source.

You can configure highlights in two ways:

1. **Simple boolean** (`highlights=True`): Returns default highlights based on the search query

2. **Detailed configuration** (pass as an object):

```json  theme={null}
{
  "contents": {
    "highlights": {
      "query": "Your specific highlight query",
      "maxCharacters": 2000
    }
  }
}
```

* `query`: The specific query to use for generating highlights (if different from search query)
* `maxCharacters`: Maximum number of characters to return for highlights

## Context String (Deprecated)

<Warning>
  The `context` parameter is deprecated and will be removed in a future version. Use `highlights` or `text` instead.
</Warning>

Returns page contents as a single combined string. When you set `context=True`, all result contents are joined together into one text block.

### Configuration:

1. **Simple boolean** (`context=True`): Returns all content combined with no character limit
2. **With character limit** (pass as an object):

```json  theme={null}
{
  "contents": {
    "context": {
      "maxCharacters": 10000
    }
  }
}
```

## Images and favicons

You can get images from webpages by setting `imageLinks` (under `contents.extras.imageLinks`) to specify how many images you want per result. Each result also includes the website's `favicon` URL and a representative `image` URL when available.

## Crawl Errors

The contents endpoint provides detailed status information for each URL through the `statuses` field in the response. The endpoint only returns an error if there's an internal issue on Exa's end - all other cases are reported through individual URL statuses.

Each response includes a `statuses` array with status information for each requested URL:

```json  theme={null}
{
  "results": [...],
  "statuses": [
    {
      "id": "https://example.com",
      "status": "success" | "error",
      "error": {
        "tag": "CRAWL_NOT_FOUND" | "CRAWL_TIMEOUT" | "CRAWL_LIVECRAWL_TIMEOUT" | "SOURCE_NOT_AVAILABLE" | "CRAWL_UNKNOWN_ERROR",
        "httpStatusCode": 404 | 408 | 403 | 500
      }
    }
  ]
}
```

The error tags correspond to different failure scenarios:

* `CRAWL_NOT_FOUND`: Content not found (HTTP 404)
* `CRAWL_TIMEOUT`: The target page returned a timeout error (HTTP 408)
* `CRAWL_LIVECRAWL_TIMEOUT`: The `livecrawlTimeout` parameter limit was reached during crawling
* `SOURCE_NOT_AVAILABLE`: Access forbidden or source unavailable (HTTP 403)
* `CRAWL_UNKNOWN_ERROR`: Other errors (HTTP 500+)

To handle errors, check the `statuses` field for each URL:

```python  theme={null}
result = exa.get_contents(["https://example.com"])
for status in result.statuses:
    if status.status == "error":
        print(f"Error for {status.id}: {status.error.tag} ({status.error.httpStatusCode})")
```

This allows you to handle different failure scenarios appropriately for each URL in your request.
