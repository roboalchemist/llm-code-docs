# Source: https://developers.openai.com/cookbook/articles/gpt-oss/fine-tune-korean.md

ì´ ë…¸íŠ¸ë¶ì€ OpenAIì˜  **gpt-oss (openâ€‘weight)** ëª¨ë¸ì„ **í•œêµ­ ë‰´ìŠ¤ ë¬¸ì²´ + ìµœì‹  ëŒ€í™”ì²´**ë¡œ ì„¸ë°€ íŠœë‹í•˜ëŠ” ë°©ë²•ì„
í•œêµ­ì–´/ì˜ì–´ **ì´ì¤‘ ì–¸ì–´**ë¡œ ì œê³µí•©ë‹ˆë‹¤.  
This notebook shows how to fineâ€‘tune OpenAI's **gpt-oss (openâ€‘weight)** models for **Korean news style + modern chat tone**, in **Korean & English**.

---

### MXFP4 workflow clarifications Â· MXFP4 ì›Œí¬í”Œë¡œ ì •ë¦¬

**EN:**  
- Training or fine-tuning **directly in MXFP4 is not supported** by public frameworks today.  
- Recommended path: train in **BF16** (or **QLoRA 4â€‘bit nf4**) â†’ **merge LoRA** â†’ **postâ€‘training quantize to MXFP4** â†’ `save_pretrained()` for deployment.  
- If you need an MXFP4 artifact, you must **reâ€‘quantize from BF16** after merging adapters. (Export utilities are evolving; if your toolchain already supports MXFP4 serialization, thatâ€™s ideal.)

**KR:**  
- í˜„ì¬ ê³µê°œ í”„ë ˆì„ì›Œí¬ì—ì„œëŠ” **MXFP4ë¡œ ì§ì ‘ í•™ìŠµ/íŒŒì¸íŠœë‹**ì´ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.  
- ê¶Œì¥ ê²½ë¡œ: **BF16**(ë˜ëŠ” **QLoRA 4â€‘bit nf4**)ë¡œ í•™ìŠµ â†’ **LoRA ë³‘í•©** â†’ **ì‚¬í›„(MXFP4) ì–‘ìí™”** â†’ ë°°í¬ìš©ìœ¼ë¡œ `save_pretrained()` ì €ì¥.  
- MXFP4 ì•„í‹°íŒ©íŠ¸ê°€ í•„ìš”í•˜ë©´, ì–´ëŒ‘í„° ë³‘í•© í›„ **BF16 â†’ MXFP4 ì¬ì–‘ìí™”**ê°€ í•„ìš”í•©ë‹ˆë‹¤. (ì§ë ¬í™” ìœ í‹¸ì€ ì§„í™” ì¤‘ì´ë©°, íˆ´ì²´ì¸ì—ì„œ MXFP4 ì €ì¥ì„ ì§€ì›í•˜ë©´ ê°€ì¥ ì¢‹ìŠµë‹ˆë‹¤.)

---

### LoRA targets (MoE) Â· LoRA íƒ€ê¹ƒ(MoE í¬í•¨)

**EN:**  
- Minimal config (fast, low VRAM): target attention only, e.g. `["q_proj","v_proj"]`.  
- MoEâ€‘aware config (better domain adaptation, more VRAM/time): include **expert projection layers** in addition to attention.  

```python
from peft import LoraConfig

TARGET_MODULES = ["q_proj", "v_proj"]  # baseline
MOE_TARGET_PARAMETERS = [
    # example expert layers; adjust indices to your model depth
    "mlp.experts.gate_up_proj",
    "mlp.experts.down_proj",
]

lora_cfg = LoraConfig(
    r=16, lora_alpha=32, lora_dropout=0.05,
    target_modules="all-linear",              # cover all linear layers
    target_parameters=MOE_TARGET_PARAMETERS,  # add expert projections
    bias="none", task_type="CAUSAL_LM",
)
```

- Start with attentionâ€‘only; if KR domain fit is insufficient, enable MoE targets and reâ€‘eval.

**KR:**  
- ìµœì†Œ êµ¬ì„±(ë¹ ë¥´ê³  VRAM ì ˆì•½): `["q_proj","v_proj"]` ë“± **ì–´í…ì…˜ë§Œ** ì ìš©.  
- **MoE ì¸ì§€ êµ¬ì„±**(ë„ë©”ì¸ ì í•©ì„±â†‘, ìì› ì†Œëª¨â†‘): ì–´í…ì…˜ì— **ì „ë¬¸ê°€(Expert) íˆ¬ì˜ ë ˆì´ì–´**ë¥¼ ì¶”ê°€ë¡œ í¬í•¨.  
- ë¨¼ì € ì–´í…ì…˜ë§Œìœ¼ë¡œ ì‹œë„í•œ ë’¤, í•œêµ­ì–´ ë„ë©”ì¸ ì í•©ì„±ì´ ë¶€ì¡±í•˜ë©´ MoE íƒ€ê¹ƒì„ ì¼œê³  ì¬í‰ê°€í•˜ì„¸ìš”.

## Contents Â· ëª©ì°¨
0) Goals & Scope Â· ëª©í‘œ & ë²”ìœ„  
1) Environment check Â· í™˜ê²½ ì ê²€  
2) ì„¤ì •ê°’ Â· Config  
3) íŒ¨í‚¤ì§€ ì„¤ì¹˜ Â· Install Deps  
4) ë°ì´í„° ì†Œì‹±(í•œêµ­í˜•) Â· KRâ€‘Context Data Sourcing  
5) ìƒ˜í”Œ ë°ì´í„° ìƒì„± Â· Create Sample Data  
6) ì „ì²˜ë¦¬(PIPA) & ìŠ¤íƒ€ì¼ ë¼ë²¨ Â· PII Scrubbing & Style Tags  
7) ë°ì´í„° ë¡œë”©/í¬ë§·íŒ… Â· Load & Format  
8) ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë“œ Â· Load Model & Tokenizer  
9) Fineâ€‘Tuning (LoRA/QLoRA) Â· ì„¸ë°€ íŠœë‹  
   9a) Data curation & splits  
   9b) Hyperparameters (r/alpha/dropout)  
   9c) Merge adapters (BF16)  
   9d) Save merged BF16 (`save_pretrained`)  
   9e) Export & Quantize (BF16 â†’ MXFP4) Â· ë‚´ë³´ë‚´ê¸° & ì–‘ìí™”  
10) í‰ê°€(ë‰´ìŠ¤/ëŒ€í™”) Â· Evaluation (News/Chat)  
11) Inference Prompt Templates Â· ì¶”ë¡  í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿  
12) ìµœì‹ ì„± ìœ ì§€ Â· Freshness Strategy  
13) ì•ˆì „/ì»´í”Œë¼ì´ì–¸ìŠ¤ Â· Safety & Compliance  
14) ë¬¸ì œí•´ê²° & ë‹¤ìŒ ë‹¨ê³„ Â· Troubleshooting & Next Steps


### âš™ï¸ Training vs Quantization â€” Whatâ€™s supported
- **Do:** Train with BF16/FP16 or QLoRA; export merged weights.
- **Then:** Quantize to **MXFP4** for inference using provided conversion scripts/utilities.
- **Donâ€™t:** Attempt to run an endâ€‘toâ€‘end â€œtrain in MXFP4â€ pipeline â€” not supported today.

> **PII & Compliance Reminder:** For KR data, follow your enterprise policy (mask RRN/phone/account IDs, remove emails) **before** training & logging. Keep train/val/test splits stratified by source and style tags.

### ğŸ§ª MoE adapters (optional)
You can target MoE layers with adapters, but treat this as **advanced/experimental**. Start with attention projections first and validate KR benchmarks before expanding scope.

> **Note:** Keep `transformers`, `peft`, `accelerate`, and `trl` at versions known to support BF16/4â€‘bit LoRA.  
If you pin `safetensors`, remember that **native MXFP4 serialization is not yet standardized**; loaders may upcast internally.

### ğŸ” Support Matrix â€” At a glance
- **Fineâ€‘tuning precision:** BF16/FP16 âœ… Â· QLoRA 4â€‘bit âœ… Â· **MXFP4 FT âŒ**
- **Quantization target:** MXFP4 âœ… (postâ€‘training)
- **API FT (hosted) for OSS models:** âŒ
- **Openâ€‘source FT (Transformers/TRL/PEFT):** âœ…
- **LoRA targets:** `q_proj`, `k_proj`, `v_proj`, `o_proj` âœ…; MoE expert adapters **experimental** âš ï¸

---

## 0) Goals & Scope Â· ëª©í‘œ & ë²”ìœ„
- **KR**: í•œêµ­ì–´ ì¼ë°˜ ë‰´ìŠ¤ + ì¼ìƒ/ìƒë‹´ ëŒ€í™”ì²´ì— ìµœì í™”. `style=news_headline|news_lead|news_body|kakao_casual|kakao_formal` ì œì–´.
- **EN**: Optimize for Korean news writing and modern chat tone; control output via style tags above.
- **Stack**: `transformers`, `trl(SFTTrainer)`, `peft(LoRA/QLoRA)`, `datasets`.
- **Hardware**: Single/few GPUs (BF16 preferred). CPU/Mac for lightweight tests.

## 1) Environment check Â· í™˜ê²½ ì ê²€

```python
import os, sys, platform
print("Python:", sys.version)
print("OS/Platform:", platform.platform())
print("CUDA_VISIBLE_DEVICES:", os.environ.get("CUDA_VISIBLE_DEVICES", ""))

try:
    import torch
    print("Torch:", torch.__version__, "CUDA:", torch.cuda.is_available())
    if torch.cuda.is_available():
        print("GPU:", torch.cuda.get_device_name(0))
except Exception as e:
    print("Torch not installed or GPU not detected:", e)
```

```text
Python: 3.10.12 (main, May 27 2025, 17:12:29) [GCC 11.4.0]
OS/Platform: Linux-6.8.0-60-generic-x86_64-with-glibc2.35
CUDA_VISIBLE_DEVICES: 
Torch: 2.7.1+cu126 CUDA: True
GPU: NVIDIA H100 80GB HBM3
```

## 2) ì„¤ì •ê°’ Â· Config

```python
from pathlib import Path
import os

# === Model & Training Params ===
BASE_URL = "http://localhost:8000/v1"     # vLLM OpenAI-compatible endpoint
API_KEY  = "dummy-key"                     # vLLM ignores; SDK requires a value
MODEL    = "openai/gpt-oss-120b"           # must match the model vLLM loaded
OUTPUT_DIR = "ft-oss-kr-news-chat-bilingual"

# Data mix (news : chat)
MIX_NEWS = 0.6
MIX_CHAT = 0.4

# LoRA
LORA_R = 8
LORA_ALPHA = 16
LORA_DROPOUT = 0.05
TARGET_MODULES = ["q_proj", "v_proj"]  # adjust per model

# Training
EPOCHS = 1
PER_DEVICE_BS = 2
GRAD_ACCUM = 8
LEARNING_RATE = 2e-4
BF16 = True
LOG_STEPS = 20
SAVE_STEPS = 200
SAVE_TOTAL_LIMIT = 2

print("Config ready.")
```

```text
Config ready.
```

## 3) íŒ¨í‚¤ì§€ ì„¤ì¹˜ Â· Install Deps

```python
# %pip install --upgrade pip
# %pip install transformers accelerate datasets peft trl bitsandbytes sentencepiece
# (optional) serving/runtimes
# %pip install vllm
# %pip install llama-cpp-python

import importlib, pip

for dep in ["transformers","accelerate","datasets","peft","trl",
            "bitsandbytes","sentencepiece","vllm","llama_cpp"]:
    try:
        print(f"{dep}: {importlib.import_module(dep).__version__}")
    except Exception:
        print(f"{dep}: not installed")

print(f"pip: {pip.__version__}")

print("Install cells are commented. Un-comment in your environment.")
```

```text
transformers: 4.55.3
accelerate: 1.10.0
datasets: 4.0.0
peft: not installed
trl: 0.21.0
bitsandbytes: not installed
sentencepiece: 0.2.1
vllm: 0.10.1
llama_cpp: 0.3.16
pip: 25.2
Install cells are commented. Un-comment in your environment.
```

## 4) ë°ì´í„° ì†Œì‹±(í•œêµ­í˜•) Â· KRâ€‘Context Data Sourcing

**KR**  
- ê³µê°œ ë²¤ì¹˜ë§ˆí¬(ì£¼ì œ ë¶„ë¥˜/ìš”ì•½/QA) + **í—ˆìš©ëœ ë‰´ìŠ¤ APIì˜ ë©”íƒ€ë°ì´í„°(ì œëª©/ìš”ì•½/ì„¹ì…˜)** ì¤‘ì‹¬ìœ¼ë¡œ ìŠ¤íƒ€ì¼ ë³´ì •.
- ê¸°ì‚¬ **ì›ë¬¸ ëŒ€ëŸ‰ ì¬í•™ìŠµì€ ì €ì‘ê¶Œ/ì•½ê´€ ì´ìŠˆ** â†’ ë©”íƒ€ë°ì´í„°Â·ê³µê°œ ì½”í¼ìŠ¤ ìœ„ì£¼.
- ëŒ€í™”ì²´ëŠ” í•©ë²• ê³µê°œ ì½”í¼ìŠ¤(ë°˜ë§/ì¡´ëŒ“ë§/ì´ëª¨í‹°ì½˜/ì¶•ì•½ì–´ ë¼ë²¨ í¬í•¨) ìš°ì„ .
- PIPA: ì£¼ë¯¼ë²ˆí˜¸/ì—°ë½ì²˜/ì´ë©”ì¼/ê³„ì¢Œ ë“± ê°œì¸ì •ë³´ëŠ” **í›ˆë ¨ ì „/ë¡œê·¸ ì „** ìŠ¤í¬ëŸ¬ë¹™.

**EN**  
- Prefer public KR benchmarks (topic classification / summarization / QA) and **allowed news API metadata** for style calibration.
- Avoid mass training on news full texts due to license/ToS constraints; use metadata + open corpora.
- For chat, use lawful open corpora with tone/emoji/informalâ€‘formal annotations.
- Scrub PII (phone, RRNs, emails, accounts) before training/logging.

## 5) ìƒ˜í”Œ ë°ì´í„° ìƒì„± Â· Create Sample Data

```python
import json, pathlib
pathlib.Path("data").mkdir(exist_ok=True)

news_samples = [
  {"style":"news_lead","topic":"ê²½ì œ","title":"ë°˜ë„ì²´ ìˆ˜ì¶œ í˜¸ì¡°â€¦ 7ì›” ìˆ˜ì¶œì•¡ 20% ì¦ê°€","summary":"ìˆ˜ì¶œ ê°œì„ ì„¸ê°€ ì´ì–´ì§€ë©° ê²½ê¸° íšŒë³µ ê¸°ëŒ€ê°€ ì»¤ì¡Œë‹¤."},
  {"style":"news_headline","topic":"ì •ì¹˜","title":"êµ­íšŒ, ë°ì´í„° ì‚°ì—… ìœ¡ì„±ë²• ë³¸íšŒì˜ í†µê³¼","summary":"ë°ì´í„° í™œìš© ì´‰ì§„ê³¼ ê°œì¸ì •ë³´ ë³´í˜¸ë¥¼ ê°•í™”í•˜ëŠ” ë‚´ìš©."},
  {
    "style": "news_lead",
    "topic": "ê²½ì œ",
    "title": "ì¹´ì¹´ì˜¤í˜ì´ ë³´ì•ˆ ì ê²€â€¦ ê³ ê°ë¬¸ì˜: help+vip@corp.co.kr",
    "summary": "ê³ ê°ì„¼í„° 010-1234-5678ë¡œ ë¬¸ì˜ í­ì£¼. ê³„ì¢Œ 110-123-456789 ê´€ë ¨ ê²°ì œ ì˜¤ë¥˜ ë…¼ë€."
  },
  {
    "style": "news_headline",
    "topic": "ì‚¬íšŒ",
    "title": "ê°œì¸ì •ë³´ ìœ ì¶œ ì˜í˜¹â€¦ ì£¼ë¯¼ë²ˆí˜¸ 901010-1234567 ìœ í†µ ì£¼ì¥",
    "summary": "ì„œìš¸íŠ¹ë³„ì‹œ ê°•ë‚¨êµ¬ í…Œí—¤ë€ë¡œ 123ì—ì„œ ìë£Œ í™•ë³´â€¦ ë‹´ë‹¹ì john.doe+news@example.com"
  }
]

chat_samples = [
  {"style":"kakao_casual","dialog":["ì£¼ë§ì— ë¹„ ì˜¨ëŒ€?","ì‘ ì¼ìš”ì¼ì— ê½¤ ì˜¨ë‹¤ë”ë¼ â˜”","í— ìš°ì‚° ì±™ê²¨ì•¼ê² ë‹¤"]},
  {"style":"kakao_formal","dialog":["ì•ˆë…•í•˜ì„¸ìš”. ë°°ì†¡ ì¼ì • í™•ì¸ ë¶€íƒë“œë¦½ë‹ˆë‹¤.","ë‚´ì¼ ì¤‘ ë„ì°© ì˜ˆì •ì…ë‹ˆë‹¤.","ì•ˆë‚´ ê°ì‚¬í•©ë‹ˆë‹¤."]},
  {
    "style": "kakao_formal",
    "dialog": [
      "ë°°ì†¡ í™•ì¸ ë¶€íƒë“œë¦½ë‹ˆë‹¤. ì£¼ë¬¸ë²ˆí˜¸ ORD-2025-0001 ì…ë‹ˆë‹¤.",
      "ì—°ë½ì²˜ëŠ” 010-2222-3333 ì…ë‹ˆë‹¤. (ìœ ë‹ˆì½”ë“œ í•˜ì´í”ˆ)",
      "ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸ëŠ” ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    ]
  }
]

with open("data/news.jsonl","w",encoding="utf-8") as f:
    for ex in news_samples: f.write(json.dumps(ex, ensure_ascii=False)+"\n")
with open("data/chat.jsonl","w",encoding="utf-8") as f:
    for ex in chat_samples: f.write(json.dumps(ex, ensure_ascii=False)+"\n")

print("Created: data/news.jsonl, data/chat.jsonl")
```

```text
Created: data/news.jsonl, data/chat.jsonl
```

## 6) ì „ì²˜ë¦¬(PIPA) & ìŠ¤íƒ€ì¼ ë¼ë²¨ Â· PII Scrubbing & Style Tags

```python
# Step 6 â€” PII scrubbing + style tags (no Harmony here)
import json, re, unicodedata
from pathlib import Path

# --- Normalization helpers ---
HYPHENS = dict.fromkeys(map(ord, "â€-â€’â€“â€”â€•ï¹˜ï¹£ï¼"), ord("-"))  # map unicode hyphens â†’ ASCII
def normalize(s: str) -> str:
    if not isinstance(s, str): return s
    s = unicodedata.normalize("NFKC", s)
    s = s.translate(HYPHENS)
    return s

# --- PII patterns (illustrative; tune for production) ---
RE_EMAIL = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")
# KR mobile numbers with spaces/hyphens: 010-1234-5678, 010 1234 5678, etc.
RE_PHONE = re.compile(r"\b01[016789][-\s]?\d{3,4}[-\s]?\d{4}\b")
# Korean RRN (ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸) basic pattern
RE_RRN = re.compile(r"\b\d{6}-\d{7}\b")
# Bank-ish account numbers: strictly digits in groups (avoid codes with letters)
RE_ACCOUNT = re.compile(r"\b\d{2,3}-\d{2,4}-\d{3,6}\b")
# Very simple postal address cue (city names) â€“ conservative, just redact the token (optional)
RE_CITY = re.compile(r"(ì„œìš¸íŠ¹ë³„ì‹œ|ë¶€ì‚°ê´‘ì—­ì‹œ|ëŒ€êµ¬ê´‘ì—­ì‹œ|ì¸ì²œê´‘ì—­ì‹œ|ê´‘ì£¼ê´‘ì—­ì‹œ|ëŒ€ì „ê´‘ì—­ì‹œ|ìš¸ì‚°ê´‘ì—­ì‹œ|ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ|ê²½ê¸°ë„|ê°•ì›ë„|ì¶©ì²­ë¶ë„|ì¶©ì²­ë‚¨ë„|ì „ë¼ë¶ë„|ì „ë¼ë‚¨ë„|ê²½ìƒë¶ë„|ê²½ìƒë‚¨ë„|ì œì£¼íŠ¹ë³„ìì¹˜ë„)")

# Allowlist: things that look like PII but arenâ€™t (e.g., bill/order codes w/ letters)
def looks_like_code(s: str) -> bool:
    return bool(re.search(r"[A-Za-z]", s))  # if letters present, treat as code, not account/phone

# Order of application matters (longest/most specific first sometimes helps)
SCRUBBERS = [
    ("[RRN]", RE_RRN),
    ("[EMAIL]", RE_EMAIL),
    ("[PHONE]", RE_PHONE),
    ("[ACCOUNT]", RE_ACCOUNT),
    ("[CITY]", RE_CITY),  # optional; comment out if you don't want to redact city tokens
]

def scrub_text(text: str) -> tuple[str, dict]:
    """Return (scrubbed_text, hits_dict). Avoid false positives with basic allowlisting."""
    if not isinstance(text, str) or not text:
        return text, {}
    orig = text
    text = normalize(text)
    hits = {}

    # Guard account-like and phone-like strings that contain letters (likely codes)
    guarded = set()
    for m in RE_ACCOUNT.finditer(text):
        if looks_like_code(m.group(0)):
            guarded.add(m.span())
    for m in RE_PHONE.finditer(text):
        if looks_like_code(m.group(0)):
            guarded.add(m.span())

    # Apply scrubs
    for label, pattern in SCRUBBERS:
        out = []
        last = 0
        count = 0
        for m in pattern.finditer(text):
            span = m.span()
            if pattern in (RE_ACCOUNT, RE_PHONE) and span in guarded:
                continue
            out.append(text[last:span[0]])
            out.append(label)
            last = span[1]
            count += 1
        out.append(text[last:])
        text = "".join(out)
        if count:
            hits[label] = hits.get(label, 0) + count

    return text, hits if text != orig else {}

def scrub_record(rec: dict, kind: str) -> tuple[dict, dict]:
    """Scrub fields in a news/chat record; return (new_rec, hits)."""
    rec = dict(rec)  # shallow copy
    total_hits = {}

    def scrub_field(key):
        val = rec.get(key)
        new, hits = scrub_text(val) if isinstance(val, str) else (val, {})
        rec[key] = new
        for k, v in hits.items():
            total_hits[k] = total_hits.get(k, 0) + v

    if kind == "news":
        for key in ("title", "summary", "topic"):
            scrub_field(key)
    elif kind == "chat":
        scrub_field("style")
        if isinstance(rec.get("dialog"), list):
            cleaned_dialog = []
            for turn in rec["dialog"]:
                new, hits = scrub_text(turn) if isinstance(turn, str) else (turn, {})
                cleaned_dialog.append(new)
                for k, v in hits.items():
                    total_hits[k] = total_hits.get(k, 0) + v
            rec["dialog"] = cleaned_dialog

    return rec, total_hits

# --- Style tagger (lightweight labels for later routing/metrics) ---
def build_style_tags(rec: dict, kind: str) -> list[str]:
    tags = []
    if kind == "news":
        tags.append("domain:" + (rec.get("topic") or "unknown"))
        tags.append("style:" + (rec.get("style") or "news"))
        tags.append("tone:formal")
        tags.append("medium:news")
    elif kind == "chat":
        style = (rec.get("style") or "").lower()
        tags.append("style:" + (style or "chat"))
        tags.append("tone:" + ("formal" if "formal" in style else "casual"))
        tags.append("medium:kakao")
    return [t.replace(" ", "_") for t in tags]

# --- Process files ---
def process_file(src: str, dst: str, kind: str):
    total = 0
    redacted = 0
    counters = {}
    with open(src, encoding="utf-8") as fin, open(dst, "w", encoding="utf-8") as fout:
        for line in fin:
            if not line.strip(): continue
            rec = json.loads(line)
            total += 1
            cleaned, hits = scrub_record(rec, kind)
            cleaned["style_tags"] = build_style_tags(cleaned, kind)
            cleaned["_pii_hits"] = hits  # keep for inspection; drop later if you want
            if hits: redacted += 1
            for k, v in hits.items():
                counters[k] = counters.get(k, 0) + v
            fout.write(json.dumps(cleaned, ensure_ascii=False) + "\n")
    print(f"{src} -> {dst} | rows: {total}, redacted_rows: {redacted}, hits: {counters}")

process_file("data/news.jsonl", "data/news_clean.jsonl", kind="news")
process_file("data/chat.jsonl", "data/chat_clean.jsonl", kind="chat")
```

```text
data/news.jsonl -> data/news_clean.jsonl | rows: 4, redacted_rows: 2, hits: {'[EMAIL]': 2, '[ACCOUNT]': 1, '[RRN]': 1, '[CITY]': 1}
data/chat.jsonl -> data/chat_clean.jsonl | rows: 3, redacted_rows: 1, hits: {'[PHONE]': 1}
```

## 7) ë°ì´í„° ë¡œë”©/í¬ë§·íŒ… Â· Load & Format

```python
# Step 7 â€” Harmony conversion + dataset loading & tokenization
import json, math
from pathlib import Path
from datasets import load_dataset, Dataset, concatenate_datasets
from transformers import AutoTokenizer

DATA = Path("data")
assert (DATA / "news_clean.jsonl").exists(), "Run Step 6 first"
assert (DATA / "chat_clean.jsonl").exists(), "Run Step 6 first"

# ---------- 7A) Convert cleaned â†’ Harmony messages ----------

def news_to_messages(rec):
    # system style from Step 6 tags; default to KR news tone
    system = "í•œêµ­ ë‰´ìŠ¤ ë¬¸ì²´ë¡œ ê°„ê²°í•˜ê³  ì‚¬ì‹¤ ìœ„ì£¼ë¡œ ì‘ì„±."
    # user asks for a headline+lead from topic; assistant is the expected formatted answer
    user = f"ì£¼ì œ: {rec.get('topic','ì•Œìˆ˜ì—†ìŒ')}. ê¸°ì‚¬ ì œëª©ê³¼ ìš”ì•½ì„ ìƒì„±í•´ì¤˜."
    assistant = f"{rec.get('title','')} â€” {rec.get('summary','')}"
    return [{"role":"system","content":system},
            {"role":"user","content":user},
            {"role":"assistant","content":assistant}]

def chat_to_messages(rec):
    # Keep style hint (casual/formal) in system
    style = (rec.get("style") or "").lower()
    system = f"ì¹´ì¹´ì˜¤í†¡ ëŒ€í™” ìŠ¤íƒ€ì¼. style={style or 'chat'}"
    dialog = rec.get("dialog") or []
    msgs = [{"role":"system","content":system}]
    # Alternate user/assistant turns; if odd length, last user stays without assistant label
    roles = ["user","assistant"]
    for i, turn in enumerate(dialog[:6]):  # cap tiny demos to avoid runaway
        msgs.append({"role": roles[i % 2], "content": str(turn)})
    # Ensure there is at least one assistant turn for SFT
    if not any(m["role"]=="assistant" for m in msgs):
        msgs.append({"role":"assistant","content":"ë„¤, í™•ì¸í–ˆìŠµë‹ˆë‹¤."})
    return msgs

def write_harmony(src, dst, kind):
    convert = news_to_messages if kind=="news" else chat_to_messages
    with open(src, encoding="utf-8") as fin, open(dst, "w", encoding="utf-8") as fout:
        for line in fin:
            if not line.strip(): continue
            rec = json.loads(line)
            msgs = convert(rec)
            fout.write(json.dumps({"messages": msgs}, ensure_ascii=False) + "\n")

write_harmony(DATA/"news_clean.jsonl", DATA/"news_harmony.jsonl", "news")
write_harmony(DATA/"chat_clean.jsonl", DATA/"chat_harmony.jsonl", "chat")
print("Created:", DATA/"news_harmony.jsonl", DATA/"chat_harmony.jsonl")

# ---------- 7B) Load Harmony JSONL with ğŸ¤— Datasets ----------
raw = load_dataset(
    "json",
    data_files={"news": str(DATA/"news_harmony.jsonl"),
                "chat": str(DATA/"chat_harmony.jsonl")}
)

# Mix train split using your Step-2 mix ratios
news = raw["news"]
chat = raw["chat"]

def take_portion(ds, frac):
    n = max(1, int(round(len(ds) * frac)))
    return ds.select(range(n)) if n < len(ds) else ds

news_part = take_portion(news, MIX_NEWS if 'MIX_NEWS' in globals() else 0.5)
chat_part = take_portion(chat, MIX_CHAT if 'MIX_CHAT' in globals() else 0.5)
train_ds = concatenate_datasets([news_part, chat_part]).shuffle(seed=42)

# Tiny validation built from remaining examples (if any)
remaining_news = news.select(range(len(news_part), len(news))) if len(news) > len(news_part) else news_part
remaining_chat = chat.select(range(len(chat_part), len(chat))) if len(chat) > len(chat_part) else chat_part
val_candidates = concatenate_datasets([remaining_news, remaining_chat])
val_ds = val_candidates.shuffle(seed=43).select(range(min(64, len(val_candidates)))) if len(val_candidates) else train_ds.select(range(min(32, len(train_ds))))

dataset = {"train": train_ds, "validation": val_ds}
print({k: len(v) for k, v in dataset.items()})
```

```text
Created: data/news_harmony.jsonl data/chat_harmony.jsonl
```

```text
Generating news split: 0 examples [00:00, ? examples/s]
```

```text
Generating chat split: 0 examples [00:00, ? examples/s]
```

```text
{'train': 3, 'validation': 4}
```

## 8) ëª¨ë¸/í† í¬ë‚˜ì´ì € ë¡œë“œ Â· Load Model & Tokenizer

```python
# ---------- 7C) Tokenizer + Harmony template fallback ----------
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    MODEL,
    use_fast=True,          # required if only tokenizer.json exists
    trust_remote_code=True,
    force_download=True     # ensures a fresh pull
)

if not getattr(tokenizer, "chat_template", None):
    # Minimal Harmony-style fallback (server already knows Harmony; this is ONLY for training tokenization)
    tokenizer.chat_template = """{% for m in messages -%}
{%- if m['role'] == 'system' -%}<|system|>
{{ m['content'] }}<|end|>
{%- elif m['role'] == 'user' -%}<|user|>
{{ m['content'] }}<|end|>
{%- elif m['role'] == 'assistant' -%}<|assistant|>
{{ m['content'] }}<|end|>
{%- endif -%}
{%- endfor -%}"""

# Ensure pad/eos are sane
tokenizer.pad_token = tokenizer.eos_token or tokenizer.pad_token

# ---------- 7D) Tokenize with assistant-only labels ----------
ASST_TOKEN = None
END_TOKEN = None
try:
    ASST_TOKEN = tokenizer.convert_tokens_to_ids("<|assistant|>")
    END_TOKEN = tokenizer.convert_tokens_to_ids("<|end|>")
except Exception:
    # If the base vocab lacks these tokens, it's okay; masking fallback below will still work heuristically
    pass

MAX_LEN = 2048  # you can raise this if you have room

def tokenize_with_labels(example):
    # 1) Render with chat template (includes assistant answer)
    text = tokenizer.apply_chat_template(example["messages"], tokenize=False, add_generation_prompt=False)
    # 2) Tokenize
    enc = tokenizer(text, truncation=True, max_length=MAX_LEN)
    input_ids = enc["input_ids"]
    labels = [-100] * len(input_ids)

    # 3) Label only assistant content
    if ASST_TOKEN is not None and END_TOKEN is not None:
        start = None
        for i, tid in enumerate(input_ids):
            if tid == ASST_TOKEN:
                start = i + 1  # learn after the tag
            elif start is not None and tid == END_TOKEN:
                start = None
            elif start is not None:
                labels[i] = input_ids[i]
    else:
        # Heuristic fallback: learn on the last third of tokens (crude but avoids total silence)
        start = int(len(input_ids) * 0.66)
        for i in range(start, len(input_ids)):
            labels[i] = input_ids[i]

    return {"input_ids": input_ids, "attention_mask": enc["attention_mask"], "labels": labels}

tokenized_train = dataset["train"].map(tokenize_with_labels, remove_columns=["messages"])
tokenized_val   = dataset["validation"].map(tokenize_with_labels, remove_columns=["messages"])

print("Tokenization done.",
      "train:", len(tokenized_train),
      "val:", len(tokenized_val),
      "example lens:", tokenized_train[0]["input_ids"][:12], "...")
```

```text
tokenizer_config.json: 0.00B [00:00, ?B/s]
```

```text
tokenizer_config.json: 0.00B [00:00, ?B/s]
```

```text
tokenizer.json:   0%|          | 0.00/27.9M [00:00<?, ?B/s]
```

```text
special_tokens_map.json:   0%|          | 0.00/98.0 [00:00<?, ?B/s]
```

```text
tokenizer_config.json: 0.00B [00:00, ?B/s]
```

```text
chat_template.jinja: 0.00B [00:00, ?B/s]
```

```text
Map:   0%|          | 0/3 [00:00<?, ? examples/s]
```

```text
Map:   0%|          | 0/4 [00:00<?, ? examples/s]
```

```text
Tokenization done. train: 3 val: 4 example lens: [200006, 17360, 200008, 3575, 553, 17554, 162016, 11, 261, 4410, 6439, 2359] ...
```

## 9) Fineâ€‘Tuning (LoRA/QLoRA) Â· ì„¸ë°€ íŠœë‹
### 9a) Data curation & splits
_(See Section 7/8 for dataset prep; move relevant snippets here if needed.)_
### 9b) Hyperparameters (r/alpha/dropout)
```python
# Example LoRA hyperparameters
LORA_R = 8
LORA_ALPHA = 16
LORA_DROPOUT = 0.05
```

### 9c) Merge adapters (BF16)
```python
# Example merge step (after training)
# model = PeftModel.from_pretrained(base_model, adapter_path)
# merged_model = model.merge_and_unload()
```

### 9d) Save merged BF16 (`save_pretrained`)
```python
# merged_model.save_pretrained(OUTPUT_DIR)
```


### 9e) Export & Quantize (BF16 â†’ MXFP4) Â· ë‚´ë³´ë‚´ê¸° & ì–‘ìí™”

**EN (neutral, framework-agnostic):**  
Public libraries currently do **not** support training/fineâ€‘tuning *directly* in MXFP4. The common pipeline is:
1) **Train/SFT** in **BF16** (or **QLoRA 4â€‘bit nf4**).  
2) **Merge LoRA adapters** into the base model (BF16).  
3) **Save** the merged BF16 checkpoint with `save_pretrained()`.  
4) **Postâ€‘training quantize** the merged BF16 tensors to **MXFP4** using a **vendor/toolchainâ€‘provided packer**.  
5) **Save/export** the MXFP4 artifact (same shape as Hugging Face `save_pretrained()` output) for deployment/serving.

> Notes:  
> - If your serving stack supports **LoRA at inference**, you may skip merging and quantization and ship: **base (MXFP4 or BF16) + LoRA adapters**.  
> - If your runtime requires **merged MXFP4**, you must run a **BF16 â†’ MXFP4** quantization step after merging adapters.  
> - Keep **tokenizer/config** files aligned across BF16 and MXFP4 exports.

**KR (ì¤‘ë¦½ì , ë„êµ¬ ë¹„ì˜ì¡´):**  
í˜„ì¬ ê³µê°œ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” MXFP4ì—ì„œ **ì§ì ‘ í•™ìŠµ/íŒŒì¸íŠœë‹ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤**. ì¼ë°˜ì ì¸ íŒŒì´í”„ë¼ì¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:  
1) **BF16**(ë˜ëŠ” **QLoRA 4â€‘bit nf4**)ë¡œ **í•™ìŠµ/íŒŒì¸íŠœë‹**  
2) **LoRA ì–´ëŒ‘í„° ë³‘í•©**(BF16 ê¸°ì¤€)  
3) `save_pretrained()`ë¡œ **ë³‘í•©ëœ BF16 ì²´í¬í¬ì¸íŠ¸ ì €ì¥**  
4) ë²¤ë”/íˆ´ì²´ì¸ì—ì„œ ì œê³µí•˜ëŠ” **ì–‘ìí™” ë„êµ¬**ë¡œ **BF16 â†’ MXFP4 ì‚¬í›„ ì–‘ìí™”**  
5) ë°°í¬/ì„œë¹™ìš© **MXFP4 ì•„í‹°íŒ©íŠ¸ ì €ì¥/ë‚´ë³´ë‚´ê¸°** (Hugging Face `save_pretrained()` êµ¬ì¡°ì™€ ë™ì¼)

> ì°¸ê³ :  
> - **ì„œë¹™ì—ì„œ LoRAë¥¼ ì§€ì›**í•œë‹¤ë©´, ë³‘í•©Â·ì–‘ìí™”ë¥¼ ìƒëµí•˜ê³  **ê¸°ì €( MXFP4 ë˜ëŠ” BF16 ) + LoRA ì–´ëŒ‘í„°**ë¡œ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
> - **ë³‘í•©ëœ MXFP4**ê°€ í•„ìš”í•œ ëŸ°íƒ€ì„ì˜ ê²½ìš°, ì–´ëŒ‘í„° ë³‘í•© í›„ **BF16 â†’ MXFP4 ì¬ì–‘ìí™”** ë‹¨ê³„ê°€ í•„ìš”í•©ë‹ˆë‹¤.  
> - **tokenizer/config** íŒŒì¼ì€ BF16ê³¼ MXFP4 ì•„í‹°íŒ©íŠ¸ ê°„ì— ì¼ê´€ë˜ê²Œ ìœ ì§€í•˜ì„¸ìš”.


```python
from trl import SFTTrainer, SFTConfig
from peft import LoraConfig, get_peft_model

lora_cfg = LoraConfig(
    task_type="CAUSAL_LM",
    r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT,
    target_modules=TARGET_MODULES
)

# base_model = get_peft_model(base_model, lora_cfg)

sft_args = SFTConfig(
    output_dir=OUTPUT_DIR,
    num_train_epochs=EPOCHS,
    per_device_train_batch_size=PER_DEVICE_BS,
    gradient_accumulation_steps=GRAD_ACCUM,
    learning_rate=LEARNING_RATE,
    lr_scheduler_type="cosine",
    bf16=BF16,
    logging_steps=LOG_STEPS,
    save_steps=SAVE_STEPS,
    save_total_limit=SAVE_TOTAL_LIMIT
)

# trainer = SFTTrainer(model=base_model, args=sft_args, train_dataset=combined, tokenizer=tokenizer)
# trainer.train()
# trainer.save_model(OUTPUT_DIR)
print("Fineâ€‘tuning skeleton ready. Unâ€‘comment on your machine.")
```

```text
Fineâ€‘tuning skeleton ready. Unâ€‘comment on your machine.
```

## 10) í‰ê°€(ë‰´ìŠ¤/ëŒ€í™”) Â· Evaluation (News/Chat)

**KR ì§€í‘œ Â· KR Metrics**  
- ë‰´ìŠ¤ì„±: ì£¼ì œ ë¶„ë¥˜ ì í•©ë„(F1), ìš”ì•½ í’ˆì§ˆ(ROUGEâ€‘1/2/L), ë…í•´ QA(EM/F1).  
- ëŒ€í™”ì„±: ìì—°ì„±/ë§¥ë½ ìœ ì§€, ê²½ì–´/ë°˜ë§ ì „í™˜ ì •í™•ë„, ì´ëª¨í‹°ì½˜/ì¶•ì•½ì–´ ì ì ˆì„±.

**EN Notes**  
- Use public KR benchmarks (e.g., topic classification, KorQuADâ€‘like QA) where licenses permit.
- Mix automatic metrics (F1/ROUGE) with human eval for tone & politeness.

```python
# Example helpers (stub)
def simple_accuracy(preds, labels):
    return sum(int(p==g) for p,g in zip(preds, labels)) / max(1, len(labels))

# For ROUGE:
# import evaluate
# rouge = evaluate.load("rouge")
# result = rouge.compute(predictions=pred_texts, references=ref_texts)
# print(result)

print("Eval stubs ready.")
```

```text
Eval stubs ready.
```

## 11) Inference Prompt Templates Â· ì¶”ë¡  í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿

```python
from openai_harmony import Message, ChatFormatter

# Example prompt construction using Harmony
messages = [
    Message(role="system", content="ë„ˆëŠ” í•œêµ­ ê³ ê°ì„ ë•ëŠ” ìœ ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤."),
    Message(role="user", content="êµ­ë‚´ PIPA ê·œì •ì„ ì¤€ìˆ˜í•˜ë©´ì„œ ì‚¬ë‚´ ë¬¸ì„œ ìš”ì•½ê¸°ë¥¼ êµ¬ì„±í•˜ë ¤ë©´ ì–´ë–¤ ì•„í‚¤í…ì²˜ê°€ ì¢‹ì„ê¹Œ?")
]

prompt = ChatFormatter.to_chat_prompt(messages)
print(prompt)  # For preview; pass to tokenizer when running inference
```

```text
<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2025-08-21

Reasoning: medium

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions

ë„ˆëŠ” í•œêµ­ ê³ ê°ì„ ë•ëŠ” ìœ ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤.

<|end|><|start|>user<|message|>êµ­ë‚´ PIPA ê·œì •ì„ ì¤€ìˆ˜í•˜ë©´ì„œ ì‚¬ë‚´ ë¬¸ì„œ ìš”ì•½ê¸°ë¥¼ êµ¬ì„±í•˜ë ¤ë©´ ì–´ë–¤ ì•„í‚¤í…ì²˜ê°€ ì¢‹ì„ê¹Œ?<|end|><|start|>assistant
```

## 12) ìµœì‹ ì„± ìœ ì§€ Â· Freshness Strategy

- **ì£¼ê°„ ë³´ì • SFT**: í—ˆìš©ëœ ë‰´ìŠ¤ API **ë©”íƒ€ë°ì´í„°(ì œëª©/ìš”ì•½/ì„¹ì…˜)** ìƒ˜í”Œë§ â†’ ìŠ¤íƒ€ì¼ ë³´ì •.  
- **ëŒ€í™”ì²´ ì—…ë°ì´íŠ¸**: ìµœì‹  ì¶•ì•½ì–´/ì‹ ì¡°ì–´/ì´ëª¨í‹°ì½˜ ì‚¬ì „ ë°˜ì˜(ì˜ˆ: ã„±ã„±, ã…‡ã…‹, ã…‹ã…‹, ã„¹ã…‡).  
- **íšŒê·€ í‰ê°€**: ë™ì¼ ì§€í‘œë¡œ before/after ë¹„êµ â†’ í˜¼í•©ë¹„/ì˜¨ë„/íŒ¨ë„í‹° íŠœë‹.

- Weekly calibration SFT using **allowed news API metadata** for style;  
- Update slang/emoji lexicons;  
- Regression evals to track drift and adjust data mix/decoding.

## 13) ì•ˆì „/ì»´í”Œë¼ì´ì–¸ìŠ¤ Â· Safety & Compliance

- ë°ì´í„° ì¶œì²˜/ë¼ì´ì„ ìŠ¤ í™•ì¸(ë²¤ì¹˜ë§ˆí¬, API, ë‚´ë¶€ ë°ì´í„°) Â· Verify dataset/API licenses.
- ê°œì¸ì •ë³´ ìŠ¤í¬ëŸ¬ë¹™(í›ˆë ¨/ë¡œê·¸/í‰ê°€ ì „) Â· Scrub PII before training/logging/eval.
- ì €ì‘ê¶Œ/ì•½ê´€ ì¤€ìˆ˜(ê¸°ì‚¬ **ì›ë¬¸ ëŒ€ëŸ‰ ì¬í•™ìŠµ ê¸ˆì§€**) Â· Avoid mass training on full news articles.
- ì¶œë ¥ ê²€ì¦(ìŠ¤í‚¤ë§ˆ/ê¸ˆì¹™ì–´/ë¯¼ê°ë„ ê·œì¹™) Â· Output validation & forbiddenâ€‘term filters.
- ë²„ì „/í‰ê°€ ë¦¬í¬íŠ¸ ê´€ë¦¬ Â· Version datasets/models and keep eval reports.

## 14) ë¬¸ì œí•´ê²° & ë‹¤ìŒ ë‹¨ê³„ Â· Troubleshooting & Next Steps

- í˜¼í•© ë¹„ìœ¨ íŠœë‹: (ë‰´ìŠ¤:ëŒ€í™”) 6:4 â†’ 7:3 ë˜ëŠ” 5:5ë¡œ ì¡°ì •  
- LoRA í•˜ì´í¼íŒŒë¼ë¯¸í„°: r=8~16, Î±=16~32, dropout=0.05~0.1  
- ì„œë¹„ìŠ¤í™”: vLLM/llama.cpp ì„œë¹™ + í† í”½/ìŠ¤íƒ€ì¼ ë¼ìš°íŒ…  
- RAG ê²°í•©: ìµœì‹  ì‚¬ì‹¤ì„± ë³´ê°•ì„ ìœ„í•´ ë‰´ìŠ¤/ë¬¸ì„œ ì¸ë±ìŠ¤ ê²°í•©  
- A/B í…ŒìŠ¤íŠ¸: í†¤/ê¸¸ì´/ì´ëª¨í‹°ì½˜ ì‚¬ìš©ëŸ‰ ë“± ì‚¬ìš©ì ë§Œì¡±ë„ ì¸¡ì •

- Tune mix ratios, run A/B tests, consider vLLM serving, and pair with RAG for factuality.