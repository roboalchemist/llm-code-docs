# Any Llm Documentation

Source: https://mozilla-ai.github.io/any-llm/llms-full.txt

---

# any-llm Documentation

> Complete documentation for any-llm - A Python library providing a single interface to different llm providers.

This file contains all documentation pages concatenated for easy consumption by AI systems.

---

## index.md

<!-- Source: index.md -->

<p align="center">
  <picture>
    <img src="./images/any-llm-logo.png" width="20%" alt="any-llm logo"/>
  </picture>
      <p align="center">  <b>Stop rewriting code for every LLM provider </b></p>
</p>

`any-llm` is a Python library providing a single interface to different llm providers.

```python
from any_llm import completion

# Using the messages format
response = completion(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "What is Python?"}],
    provider="openai"
)
print(response)

# Switch providers without changing your code
response = completion(
    model="claude-sonnet-4-5-20250929",
    messages=[{"role": "user", "content": "What is Python?"}],
    provider="anthropic"
)
print(response)
```

### Why any-llm
  - Switch providers in one line
  - Consistent error handling across providers
  - Simple API, powerful features

[View supported providers â†’](#./providers.md)

### Getting Started

**[Get started in 5 minutes â†’](#./quickstart.md)** - Install the library and run your first API call.


### Demo

Try `any-llm` in action with our interactive chat demo:

**[ðŸ“‚ Run the Demo](https://github.com/mozilla-ai/any-llm/tree/main/demos/chat#readme)**

Features: real-time streaming responses, multiple provider support, and collapsible "thinking" content display.

### API Documentation

`any-llm` provides two main interfaces:

**Direct API Functions** (recommended for simple use cases):
- [completion](#./api/completion.md) - Chat completions with any provider
- [embedding](#./api/embedding.md) - Text embeddings
- [responses](#./api/responses.md) - OpenAI-style Responses API

**AnyLLM Class** (recommended for advanced use cases):
- [Provider API](#./api/any_llm.md) - Lower-level provider interface with metadata access and reusability

### Error Handling

`any-llm` provides custom exceptions to indicate common errors like missing API keys
and parameters that are unsupported by a specific provider.

For more details on exceptions, see the [exceptions API documentation](#./api/exceptions.md).

## For AI Systems

This documentation is available in two AI-friendly formats:

- **[llms.txt](https://mozilla-ai.github.io/any-llm/llms.txt)** - A structured overview with curated links to key documentation sections
- **[llms-full.txt](https://mozilla-ai.github.io/any-llm/llms-full.txt)** - Complete documentation content concatenated into a single file


---

## quickstart.md

<!-- Source: quickstart.md -->

## Quickstart

### Requirements

- Python 3.11 or newer
- API keys for your chosen LLM provider

### Installation
```bash
pip install any-llm-sdk[all]  # Install with all provider support
```

#### Installing Specific Providers

If you want to install a specific provider from our [supported providers](#./providers.md):

```bash
pip install any-llm-sdk[mistral]  # For Mistral provider
pip install any-llm-sdk[ollama]   # For Ollama provider
# install multiple providers
pip install any-llm-sdk[mistral,ollama]
```

#### Library Integration

If you're building a library, install just the base package (`pip install any-llm-sdk`) and let your users install provider dependencies.

> **API Keys:** Set your provider's API key as an environment variable (e.g., `export MISTRAL_API_KEY="your-key"`) or pass it directly using the `api_key` parameter.

### Your First API Call 

```python
import os

from any_llm import completion

# Make sure you have the appropriate API key set
api_key = os.environ.get('MISTRAL_API_KEY')
if not api_key:
    raise ValueError("Please set MISTRAL_API_KEY environment variable")

# Recommended: separate provider and model parameters
response = completion(
    model="mistral-small-latest",
    provider="mistral",
    messages=[{"role": "user", "content": "Hello!"}]
)
print(response.choices[0].message.content)
```

### Advanced: Using the AnyLLM Class

For applications making multiple requests with the same provider, use the `AnyLLM` class to avoid repeated provider instantiation:

```python
import os

from any_llm import AnyLLM

# Make sure you have the appropriate API key set
api_key = os.environ.get('MISTRAL_API_KEY')
if not api_key:
    raise ValueError("Please set MISTRAL_API_KEY environment variable")

llm = AnyLLM.create("mistral")

response = llm.completion(
    model="mistral-small-latest",
    messages=[{"role": "user", "content": "Hello!"}]
)
print(response.choices[0].message.content)

metadata = llm.get_provider_metadata()
print(f"Supports streaming: {metadata.streaming}")
print(f"Supports tools: {metadata.completion}")
```

#### When to Choose Which Approach

**Use Direct API Functions (`completion`, `acompletion`) when:**

- Making simple, one-off requests
- Prototyping or writing quick scripts
- You want the simplest possible interface

**Use Provider Class (`AnyLLM.create`) when:**

- Building applications that make multiple requests with the same provider
- You want to avoid repeated provider instantiation overhead

**Finding model names:** Check the [providers page](#./providers.md) for provider IDs, or use the [`list_models`](#./api/list_models.md) API to see available models for your provider.

### Streaming

For the [providers that support streaming](#./providers.md), you can enable it by passing `stream=True`:

```python
output = ""
for chunk in completion(
    model="mistral-small-latest",
    provider="mistral",
    messages=[{"role": "user", "content": "Hello!"}],
    stream=True
):
    chunk_content = chunk.choices[0].delta.content or ""
    print(chunk_content)
    output += chunk_content
```

### Embeddings

[`embedding`][any_llm.embedding] and [`aembedding`][any_llm.aembedding] allow you to create vector embeddings from text using the same unified interface across providers.

Not all providers support embeddings - check the [providers documentation](#./providers.md) to see which ones do.

```python
from any_llm import embedding

result = embedding(
    model="text-embedding-3-small",
    provider="openai",
    inputs="Hello, world!" # can be either string or list of strings
)

# Access the embedding vector
embedding_vector = result.data[0].embedding
print(f"Embedding vector length: {len(embedding_vector)}")
print(f"Tokens used: {result.usage.total_tokens}")
```

### Tools

`any-llm` supports tool calling for providers that support it. You can pass a list of tools where each tool is either:

1. **Python callable** - Functions with proper docstrings and type annotations
2. **OpenAI Format tool dict** - Already in OpenAI tool format

```python
from any_llm import completion

def get_weather(location: str, unit: str = "F") -> str:
    """Get weather information for a location.

    Args:
        location: The city or location to get weather for
        unit: Temperature unit, either 'C' or 'F'

    Returns: 
        Current weather description
    """
    return f"Weather in {location} is sunny and 75{unit}!"

response = completion(
    model="mistral-small-latest",
    provider="mistral",
    messages=[{"role": "user", "content": "What's the weather in Pittsburgh PA?"}],
    tools=[get_weather]
)
```

any-llm automatically converts your Python functions to OpenAI tools format. Functions must have:
- A docstring describing what the function does
- Type annotations for all parameters
- A return type annotation


---

## providers.md

<!-- Source: providers.md -->

# Supported Providers

`any-llm` supports the below providers. In order to discover information about what models are supported by a provider
as well as what features the provider supports for each model, refer to the provider documentation.

Provider source code can be found in the [`src/any_llm/providers/`](https://github.com/mozilla-ai/any-llm/tree/main/src/any_llm/providers) directory of the repository.

!!! note "Legend"

    - **Reasoning (Completions)**: Provider can return reasoning traces alongside the assistant message via the completions and/or streaming endpoints. This does not indicate whether the provider offers separate "reasoning models".See [this](https://github.com/mozilla-ai/any-llm/issues/95)
    - **Streaming (Completions)**: Provider can stream completion results back as an iterator.
    discussion for more information.
    - **Image (Completions)**: Provider supports passing an `image_data` parameter for vision capabilities, as defined by the OpenAI spec [here](https://platform.openai.com/docs/api-reference/chat/create#chat_create-messages).
    - **Responses API**: Provider supports the Responses API variant for text generation.  See [this](https://github.com/mozilla-ai/any-llm/issues/26) to follow along with our implementation effort.
    - **List Models API**: Provider supports listing available models programmatically via the `list_models()` function. This allows you to discover what models are available from the provider at runtime, which can be useful for dynamic model selection or validation.


<!-- The below table is auto-generated by the mkdocs build hook. It will display in the generated site -->
<!-- AUTO-GENERATED TABLE START -->
<!-- AUTO-GENERATED TABLE END -->


---

## api/any_llm.md

<!-- Source: api/any_llm.md -->

## AnyLLM

::: any_llm.AnyLLM


---

## api/responses.md

<!-- Source: api/responses.md -->

## Responses


!!! warning

    This API is experimental and subject to changes based upon our experience as we integrate additional providers.
    Use with caution.

::: any_llm.api.responses
::: any_llm.api.aresponses


---

## api/completion.md

<!-- Source: api/completion.md -->

## Completion

::: any_llm.api.completion
::: any_llm.api.acompletion


---

## api/embedding.md

<!-- Source: api/embedding.md -->

## Embedding

::: any_llm.api.embedding
::: any_llm.api.aembedding


---

## api/exceptions.md

<!-- Source: api/exceptions.md -->

## Exceptions

::: any_llm.exceptions


---

## api/list_models.md

<!-- Source: api/list_models.md -->

## Models

::: any_llm.api.list_models
::: any_llm.api.alist_models


---

## api/batch.md

<!-- Source: api/batch.md -->

# Batch

!!! warning "Experimental API"
    The Batch API is experimental and subject to breaking changes in future versions. Use with caution in production environments.

The Batch API allows you to process multiple requests asynchronously at a lower cost.

## File Path Interface

The `any-llm` batch API requires you to pass a **path to a local JSONL file** containing your batch requests. The provider implementation automatically handles uploading and file management as needed.

Different providers handle batch processing differently:

- **OpenAI**: Requires uploading a file first, then creating a batch with the file ID
- **Anthropic** (future): Expects file content passed directly in the request
- **Other providers**: May have their own unique requirements

By accepting a local file path, `any-llm` abstracts these provider differences and handles the implementation details automatically.

::: any_llm.api.create_batch
::: any_llm.api.acreate_batch
::: any_llm.api.retrieve_batch
::: any_llm.api.aretrieve_batch
::: any_llm.api.cancel_batch
::: any_llm.api.acancel_batch
::: any_llm.api.list_batches
::: any_llm.api.alist_batches


---

## api/types/completion.md

<!-- Source: api/types/completion.md -->

## Completion Types

Data models and types for completion operations.

::: any_llm.types.completion


---

## api/types/responses.md

<!-- Source: api/types/responses.md -->

## Response Types

Data models and types for API responses.

::: any_llm.types.responses


---

## api/types/model.md

<!-- Source: api/types/model.md -->

## Model Types

Data models and types for model operations.

::: any_llm.types.model


---

## api/types/provider.md

<!-- Source: api/types/provider.md -->

## Provider Types

Data models and types for provider operations.

::: any_llm.types.provider


---

## api/types/batch.md

<!-- Source: api/types/batch.md -->

## Batch Types

Data models and types for batch operations.

::: any_llm.types.batch


---

## gateway/overview.md

<!-- Source: gateway/overview.md -->

# Gateway Overview

## What is any-llm-gateway?

any-llm-gateway is a FastAPI-based proxy server that adds production-grade budget enforcement, API key management, and usage analytics on top of any-llm's multi-provider foundation. It sits between your applications and LLM providers, giving you complete control over costs, access, and observability.

## Why use the gateway?

Managing LLM costs and access at scale is challenging. Give users unrestricted access and you risk runaway costs. Lock it down too much and you slow down innovation. any-llm-gateway solves this by providing:

- **Cost Control**: Set budgets that automatically enforce or track spending limits
- **Access Management**: Issue, revoke, and monitor API keys generated for user access without exposing provider credentials
- **Complete Visibility**: Track every request with full token counts, costs, and metadata
- **Production-Ready**: Deploy with Docker and Postgres, Kubernetes-ready

## How it works

The gateway acts as a transparent proxy between your applications and LLM providers. Here's the request flow:

1. **Your application** sends a request to the gateway (instead of directly to OpenAI, Anthropic, etc.)
2. **The gateway** authenticates the request, checks budget limits, and tracks usage
3. **The gateway** routes to the appropriate provider based on the model format
4. **The provider** processes the request and returns the response
5. **The gateway** logs the usage and returns the response to your application

    ```bash
    curl -X POST http://localhost:8000/v1/chat/completions \
      -H "X-AnyLLM-Key: Bearer your-secure-master-key" \
      -H "Content-Type: application/json" \
      -d '{
        "model": "openai:gpt-5",
        "messages": [{"role": "user", "content": "Hello!"}]
      }'
    ```
  > Learn how to set up your secure master key [here](#authentication.md)  

<p align="center" width="100%">
  <img src="../../images/gateway.png" alt="Diagram showing application connecting to gateway, which then routes to multiple LLM providers (OpenAI, Anthropic, Google, etc). The gateway interfaces with a PostgreSQL database for storing usage, budgets, and keys." width="70%" align="center"/>
</p>

## Key Features

### Smart Budget Management

Create shared budget tiers with automatic daily, weekly, or monthly resets. Budgets can be:

- **Shared across multiple users** - Perfect for team or organization-wide limits
- **Automatically enforced** - Requests are rejected when budgets are exceeded
- **Tracking-only mode** - Monitor spending without blocking requests
- **Auto-resetting** - No manual intervention required for recurring budgets

[Set up your first budget â†’](#budget-management.md)

### Flexible API Key System

Choose between two authentication patterns:

**Master Key Authentication**
- Ideal for trusted services and internal tools
- Full access to all gateway features

**Virtual API Keys**
- Create scoped keys with fine-grained control
- Set expiration dates for time-limited access
- Associate with users for spend tracking
- Add custom metadata for tracking
- Activate, deactivate, or revoke on demand

[Set up your keys â†’](#authentication.md)

### Complete Usage Analytics

Every request is logged with comprehensive details:

- Full token counts (prompt, completion, total)
- Per-request costs based on admin-configured per-token pricing
- Request metadata and timestamps
- User and API key attribution

Track spending per user, view detailed usage history, and get the observability you need for cost attribution and chargebacks.

### Production-Ready Deployment

- **Quick Start**: Deploy with Docker in minutes
- **Flexible Configuration**: Configure via YAML or environment variables
- **Database**: Designed for PostgreSQL
- **Kubernetes Ready**: Built-in liveness and readiness probes

### Performance Impact
The gateway adds minimal latency (<50ms) to requests while providing complete observability.

## Getting Started

For comprehensive setup instructions, see the [Quick Start Guide](#quickstart.md).

## Next Steps

- **[Quick Start](#quickstart.md)** - Deploy and configure your first gateway
- **[Authentication](#authentication.md)** - Set up master keys and virtual API keys
- **[Budget Management](#budget-management.md)** - Configure spending limits and tracking
- **[Configuration](#configuration.md)** - Learn about all configuration options
- **[API Reference](#api-reference.md)** - Explore the complete API


---

## gateway/quickstart.md

<!-- Source: gateway/quickstart.md -->

# Quick Start

This guide will help you set up any-llm-gateway and make your first LLM completion request. The gateway acts as a proxy between your applications and LLM providers, providing cost control, usage tracking, and API key management.

By the end of this guide, you will:  

1. Configure provider credentials and model pricing (e.g., OpenAI API key)  
1. Run the gateway   
1. Authenticate requests using a master key  
1. Make completion requests through the gateway  

> **Note:** for the purposes of this quickstart we will utilize the docker-compose and config.yml file, but alternative configuration designs are available and detailed [here](#./configuration.md)

## Pre-Requisites

1. Docker
1. Access to at least one LLM provider

## Configure and run the Gateway

When running any-llm-gateway, it must have a few things configured:

1. `GATEWAY_MASTER_KEY`. This master key has admin access to manage budgets, users, virtual keys, etc.
1. `DATABASE_URL`. The gateway relies upon a postgres database for storage.
1. Provider Keys. The gateway connects to providers (Mistral, AWS, Vertex, Azure, etc) using credentials that must be set.

For the purposes of the quickstart we will use the included `docker/docker-compose.yml`, but you can customize the file to your own requirements.

### Generate  master key

First, generate a secure master key: 
```python 
python -c "import secrets; print(secrets.token_urlsafe(32))"
```

Save the output of this command, you'll need it in the next steps. 

### Configure providers 

Copy the example `docker/config.example.yml` file to `docker/config.yml`

```bash
cp docker/config.example.yml docker/config.yml
```

At a minimum you'll need to fill out the value for the master_key, and also enter credential information for at least one provider. You can browse supported providers [here](https://mozilla-ai.github.io/any-llm/providers/). If you would like to track usage cost, you'll also need to configure model pricing, as explained in the config template file.

```yaml
master_key: 09kS0xTiz6JqO....

providers:
  openai:
    api_key: YOUR_OPENAI_API_KEY_HERE
    api_base: "https://api.openai.com/v1"  # optional, useful when you want to use a specific version of the API

models:
  openai:gpt-4:
    input_price_per_million: 0.15
    output_price_per_million: 0.60
```


### Start the gateway

Run the docker-compose file, ensuring that your config.yml is located in the same directory as the docker-compose.yml file (`docker/config.yml`).

The default setting is to build the gateway from source, but see the docker-compose.yml file comment to see how to use a published version of any-llm-gateway instead of the source code.

```bash
# From project root directory
docker-compose -f docker/docker-compose.yml up -d --build
```

````bash
# Verify the gateway is running
curl http://localhost:8000/health

# Expected response:
# {"status": "healthy"}
````

### View Logs 

```bash
docker-compose -f docker/docker-compose.yml logs -f
```


## Create a user and make your first request

Now that it's running, clients can make requests! The gateway supports two authentication patterns: use of the master key, or virtual keys. See the [authentication doc](#./authentication.md) for more information. For this guide we will use the master key for both administration and client requests.

To make the below commands easier to run, you can set the key as an env var in your terminal:

```bash
export GATEWAY_MASTER_KEY=YOUR_MASTER_KEY
```

> **tip**: for the below `curl` commands, append `| jq` in order for it be pretty-printed in the console.

### Create a user

In order to track usage, we must first create a user so that we can associate our completion request with them.

```bash
curl -s -X POST http://localhost:8000/v1/users \
  -H "X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}" \
  -H "Content-Type: application/json" \
  -d '{"user_id": "user-123", "alias": "Bob"}'
```

<details>
<summary>Sample Response</summary>

```bash
{
    "user_id": "user-123",
    "alias": "Bob",
    "spend": 0,
    "budget_id": null,
    "budget_started_at": null,
    "next_budget_reset_at": null,
    "blocked": false,
    "created_at": "2025-11-07T16:41:44.429258+00:00",
    "updated_at": "2025-11-07T16:41:44.429261+00:00",
    "metadata": {}
}
```
</details>

### Make a request

Make a completion request using the master key and specify that the completion should be attached to the user you just created. This is only required when authenticating using the master key, if a user has a virtual key they do not need to specify a user id. You may also need to adjust the model to match one of the providers that you configured when running the gateway.

```bash
curl -s -X POST http://localhost:8000/v1/chat/completions \
  -H "X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai:gpt-4",
    "messages": [{"role": "user", "content": "Write a haiku on Uranus!"}],
    "user": "user-123"
  }'
```

<details>  
<summary>Sample Response </summary>

```json 
{
    "id": "chatcmpl-CZJvdiwHSdCZ2TfIPhutgPY4WYP46",
    "choices": [
        {
            "finish_reason": "stop",
            "index": 0,
            "logprobs": null,
            "message": {
                "content": "Gleaming ice-blue sphere,\nCircling in celestial dance,\nUranus, so clear.",
                "refusal": null,
                "role": "assistant",
                "annotations": [],
                "audio": null,
                "function_call": null,
                "tool_calls": null,
                "reasoning": null
            }
        }
    ],
    "created": 1762534121,
    "model": "gpt-4-0613",
    "object": "chat.completion",
    "service_tier": "default",
    "system_fingerprint": null,
    "usage": {
        "completion_tokens": 21,
        "prompt_tokens": 15,
        "total_tokens": 36,
        "completion_tokens_details": {
            "accepted_prediction_tokens": 0,
            "audio_tokens": 0,
            "reasoning_tokens": 0,
            "rejected_prediction_tokens": 0
        },
        "prompt_tokens_details": {
            "audio_tokens": 0,
            "cached_tokens": 0
        }
    }
}
```

</details>

Alternatively, if you are using the any-llm python sdk, you can access using the gateway client.

```python
import os
from any_llm import completion

completion(
  provider="gateway",
  model="openai:gpt-4",
  api_base="http://localhost:8000/v1",
  api_key=os.environ['GATEWAY_MASTER_KEY'],
  messages=[{"role": "user", "content": "Write a haiku on Uranus!"}],
  user="user-123",
)
```

### View metrics

Now using the master key, we can access the usage information for the user.

```bash
curl -s http://localhost:8000/v1/users/user-123 \
  -H "X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}" \
  -H "Content-Type: application/json"
```

<details>
<summary>Sample Response </summary>

```json
{
    "user_id": "user-123",
    "alias": "Bob",
    "spend": 0.0000216,
    "budget_id": null,
    "budget_started_at": null,
    "next_budget_reset_at": null,
    "blocked": false,
    "created_at": "2025-11-07T16:41:44.429258+00:00",
    "updated_at": "2025-11-07T16:48:42.972327+00:00",
    "metadata": {}
}
```
</details>  

You'll notice that the user does not have a budget attached, which means that we track their usage but do not limit them! For more information on creating and managing budgets and budget reset cycles, see the [Budget Management docs](#budget-management.md)

## Next Steps



- **[Configuration](#configuration.md)** - Configure providers, pricing, and other settings
- **[Authentication](#authentication.md)** - Learn about master keys and virtual API keys
- **[Budget Management](#budget-management.md)** - Set spending limits and track costs
- **[API Reference](#api-reference.md)** - Explore the complete API


---

## gateway/authentication.md

<!-- Source: gateway/authentication.md -->

# Authentication

any-llm-gateway offers two authentication methods, each designed for different use cases. Understanding when to use each approach will help you secure your gateway effectively.
## Authentication Methods Overview

| Method | Best For | Key Management | Usage Tracking |
|--------|----------|----------------|----------------|
| **Master Key** | Internal services, admin operations, trusted environments | Single key with full access | Requires manual user specification |
| **Virtual API Keys** | External apps, per-user access, customer-facing services | Multiple scoped keys | Automatic per-key tracking |


## Master Key 
The master key is the root credential for your gateway installation. It has unrestricted access to all gateway operations and should be treated with the same security as your production database credentials.

### Generating a Master Key

Generate a cryptographically secure master key (minimum 32 characters recommended):

```bash
python -c "import secrets; print(secrets.token_urlsafe(32))"
```

**Example output:**
```
Zx8Q_vKm3nR7wP2sT9yU5iO1eA6hD4fG0bN8cL3jM5k
```

Set the generated key in your configuration:

**Using environment variables:**
```bash
export GATEWAY_MASTER_KEY="Zx8Q_vKm3nR7wP2sT9yU5iO1eA6hD4fG0bN8cL3jM5k"
```

**Using config.yml:**
```yaml
master_key: "Zx8Q_vKm3nR7wP2sT9yU5iO1eA6hD4fG0bN8cL3jM5k"
```

### Creating a User

```bash
curl -X POST http://localhost:8000/v1/users \
  -H "X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}" \
  -H "Content-Type: application/json" \
  -d '{"user_id": "user-123", "alias": "Alice"}'
```
<details>
<summary>With optional metadata</summary>

```bash
curl -X POST http://localhost:8000/v1/users \
  -H "X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}" \
  -H "Content-Type: application/json" \
  -d ' { 
    "user_id": "user-123", 
    "alias": "Alice",
    "metadata": {
      "department": "Engineering",
      "team": "ML",
      "email": "alice@example.com"
    }
  }'
```
</details>

### Making Requests with Master Key
When using the master key, you **must** specify which user is making the request using the `user` field:

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai:gpt-4o-mini",
    "messages": [{"role": "user", "content": "Write a haiku on Jupiter"}],
    "user": "user-123"
  }'
```
The `user` field tells the gateway which user's budget and spend tracking to update. Without this field, the request will be rejected.

## Virtual API Keys

Virtual API keys provide scoped access for making completion requests without exposing the master key. Each virtual key can have expiration dates, metadata, and associated users for automatic usage tracking.

### Creating a Virtual API Key
Create a virtual key with a descriptive name : 

```bash
curl -X POST http://localhost:8000/v1/keys \
  -H "X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}" \
  -H "Content-Type: application/json" \
  -d '{"key_name": "mobile-app"}'
```
> **Important:** Save the `key` value immediatelyâ€”it's only shown once and cannot be retrieved later.

<details>
<summary>Example Response</summary>

```json
{
  "id": "abc-123",
  "key": "gw-...",
  "key_name": "mobile-app",
  "created_at": "2025-10-20T10:00:00",
  "expires_at": null,
  "is_active": true,
  "metadata": {}
}
```
</details>

#### Key with Expiration

Create a key that automatically expires on a specific date:

```bash
curl -X POST http://localhost:8000/v1/keys \
  -H "X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}" \
  -H "Content-Type: application/json" \
  -d '{
    "key_name": "trial-access",
    "expires_at": "2025-12-31T23:59:59Z"
  }'
```

### Using Virtual API Keys
Making requests with a virtual key is simpler than using the master keyâ€”no `user` field is required:

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "X-AnyLLM-Key: Bearer gw-..." \
  -H "Content-Type: application/json" \
  -d '{"model": "openai:gpt-5-mini", "messages": [{"role": "user", "content": "Write a haiku on Saturn"}]}'
```

The gateway automatically tracks usage based on the virtual key used.
### Managing Virtual Keys

#### List All Keys
**List all keys:**
```bash
curl http://localhost:8000/v1/keys \
  -H "X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}"
```

**Deactivate a key:**
```bash
curl -X PATCH http://localhost:8000/v1/keys/<virtual_key_id>\
  -H "X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}" \
  -H "Content-Type: application/json" \
  -d '{"is_active": false}'
```

**Delete a key:**
```bash
curl -X DELETE http://localhost:8000/v1/keys/<virtual_key_id> \
  -H "X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}"
```

> See [API Reference](#api-reference.md) for complete key management operations.


Note: The actual key values are never returned in list or get operations for security reasons.

## Next Steps

Now that you understand authentication, explore these related topics:

- **[Budget Management](#budget-management.md)** - Set spending limits for users and enforce budgets
- **[Configuration](#configuration.md)** - Learn about provider setup and pricing configuration
- **[API Reference](#api-reference.md)** - Explore all available endpoints for managing keys and users
- **[Quick Start](#quickstart.md)** - Complete walkthrough of setting up your first gateway

For questions or issues, refer to the [troubleshooting guide](#troubleshooting.md) or check the project's issue tracker.


---

## gateway/budget-management.md

<!-- Source: gateway/budget-management.md -->

# Budget Management

Budgets provide shared spending limits that can be assigned to multiple users. This allows you to create budget tiers (like "Free", "Pro", "Enterprise") and enforce spending limits across groups of users.

## Creating a Budget

```bash
# Create a budget with a $10.00 spending limit and monthly resets (30 days = 2592000 seconds)
curl -X POST http://localhost:8000/v1/budgets \
  -H "X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}" \
  -H "Content-Type: application/json" \
  -d '{
    "max_budget": 10.0,
    "budget_duration_sec": 2592000
  }'
```

<details> 
<summary> Sample Response</summary>

```json
{
  "budget_id": "abc-123",
  "max_budget": 10.0,
  "budget_duration_sec": 2592000,
  "created_at": "2025-10-22T10:00:00Z",
  "updated_at": "2025-10-22T10:00:00Z"
}
```
</details>

## Assigning Budgets to Users

When creating or updating a user, specify the `budget_id`:

**Warning: If you don't create and set a budget, budget is unlimited**

```bash
# Create a user with a budget
curl -X POST http://localhost:8000/v1/users \
  -H "X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}" \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "user-456",
    "alias": "Bob",
    "budget_id": "abc-123"
  }'

# Update an existing user's budget
curl -X PATCH http://localhost:8000/v1/users/user-123 \
  -H "X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}" \
  -H "Content-Type: application/json" \
  -d '{"budget_id": "abc-123"}'
```

## Per-User Budget Resets

Budget resets are **per-user**, not global. Each user tracks their own budget period based on when they were assigned the budget.

**Example:**
1. Create a budget with `budget_duration_sec: 604800` (1 week)
2. Assign User A to the budget on Monday
3. Assign User B to the budget on Tuesday
4. User A's budget resets every Monday
5. User B's budget resets every Tuesday

This allows you to create budget tiers (like "Free", "Pro", "Enterprise") without worrying about all users resetting at the same time.

## Automatic Reset Behavior

Budget resets happen automatically using a "lazy reset" approach:
- When a user makes a request, the system checks if their `next_budget_reset_at` has passed
- If yes, the user's `spend` is reset to $0.00 and a new reset date is calculated
- A log entry is created in `budget_reset_logs` for audit purposes
- The request then proceeds normally


---

## gateway/configuration.md

<!-- Source: gateway/configuration.md -->

# Configuration

The any-llm-gateway requires configuration to connect to your database, authenticate requests, and route to LLM providers. This guide covers the two main configuration approaches and how to set up model pricing for cost tracking.

You can configure the gateway using either a YAML configuration file or environment variables:

- **Config File (Recommended)**: Best for development and when managing multiple providers with complex settings. Easier to version control and share across teams.
- **Environment Variables**: Best for production deployments, containerized environments, or when following 12-factor app principles.

Both methods can also be combinedâ€”environment variables will override config file values.

## Option 1: Config File

Create a `config.yml` file with your database connection, master key, and provider credentials:

> **Generating a secure master key:**
> ```bash
>  python -c "import secrets; print(secrets.token_urlsafe(32))"
> ```

```yaml
#Database connection
database_url: "postgresql://gateway:gateway@localhost:5432/gateway_db"

#Master key for admin access
master_key: "your-secure-master-key"

## LLM Provider Credentials 
providers:
  openai:
    api_key: "${OPENAI_API_KEY}"
  gemini:
    api_key: "${GEMINI_API_KEY}"
  vertexai:
    credentials: "/path/to/service_account.json"
    project: "your-gcp-project-id"
    location: "us-central1"

# Model pricing for cost-tracking (optional)
pricing:
  openai:gpt-4:
    input_price_per_million: 0.15
    output_price_per_million: 0.6
```

Start the gateway with your config file:

```bash
any-llm-gateway serve --config config.yml
```

## Option 2: Environment Variables
Configure the gateway entirely through environment variablesâ€”useful for containerized deployments:

```bash
#Required settings
export DATABASE_URL="postgresql://gateway:gateway@localhost:5432/gateway_db"
export GATEWAY_MASTER_KEY="your-secure-master-key"
export GATEWAY_HOST="0.0.0.0"
export GATEWAY_PORT=8000

any-llm-gateway serve
```
> **Note**: Model pricing cannot be set via environment variables. Use the config file or the [Pricing API](#dynamic-pricing-via-api) instead.


## Model Pricing Configuration

Configure model pricing in your config file to automatically track costs. Pricing can be set via config file or dynamically via the API.

### Config File Pricing

Add pricing for models in your config file using the format `provider:model-name`:

```yaml
pricing:
  openai:gpt-3.5-turbo:
    input_price_per_million: 0.5
    output_price_per_million: 1.5
```

### Dynamic Pricing via API

You can also set or update pricing dynamically using the API:
```bash
curl -X POST http://localhost:8000/v1/pricing \
  -H "X-AnyLLM-Key: Bearer ${GATEWAY_MASTER_KEY}" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "openai:gpt-4",
    "input_price_per_million": 30.0,
    "output_price_per_million": 60.0
  }'
```

This is useful for:  
- Updating pricing without restarting the gateway  
- Managing pricing in production environments  
- Adjusting rates as provider pricing changes  

**Important notes:**  
- Database pricing takes precedence - config only sets initial values  
- If pricing for the model already exists in the database, config values are ignored (with a warning logged)

## Next Steps

- See [supported providers](https://mozilla-ai.github.io/any-llm/providers/) for provider-specific configuration
- Learn about [authentication methods](#./authentication.md) for managing access
- Set up [budget management](#./budget-management.md) to enforce spending limits


---

## gateway/api-reference.md

<!-- Source: gateway/api-reference.md -->

# API Reference

<swagger-ui src="openapi.json"/>


---

## gateway/troubleshooting.md

<!-- Source: gateway/troubleshooting.md -->

# Troubleshooting

## Database connection errors

Make sure the database URL is correct and the database is accessible:

```bash
python -c "from sqlalchemy import create_engine; engine = create_engine('postgresql://user:pass@host/db'); print('OK')"
```

## Common Issues

### Authentication Errors

- Ensure you're using the correct master key format: `Bearer your-secure-master-key`
- Check that the `X-AnyLLM-Key` header is properly set
- Verify that virtual API keys are active and not expired

### Configuration Issues

- Verify your `config.yml` file is properly formatted
- Check that environment variables are set correctly
- Ensure provider API keys are valid and have proper permissions

### Budget Enforcement

- Check that budgets are properly assigned to users
- Verify budget limits are set correctly
- Monitor user spending to ensure limits are being enforced

## Getting Help

- Check the logs for detailed error messages
- Verify your configuration matches the examples in the documentation
- Ensure all required environment variables are set


---

## gateway/docker-deployment.md

<!-- Source: gateway/docker-deployment.md -->

# Docker Deployment Guide

This guide walks you through deploying `any-llm-gateway` using Docker and Docker Compose. Whether you're setting up a local development environment or deploying to production, this guide covers the essential steps and best practices for a secure, reliable deployment.

## Quick Start with Docker Compose

Docker Compose is the recommended deployment method for most users. It automatically sets up both the gateway application and a PostgreSQL database with proper networking and dependencies.

**Prerequisites:**
- Docker Engine 20.10 or newer
- Docker Compose 2.0 or newer
- At least one LLM provider API key (OpenAI, Anthropic, Mistral, etc.)

### Configure the Gateway

First, prepare your configuration file with credentials and settings:

Copy the example configuration file:

```bash
cp docker/config.example.yml docker/config.yml
```

Generate a secure master key (minimum 32 characters recommended):

```bash
python -c "import secrets; print(secrets.token_urlsafe(32))"
```

Save the output of this command for the next step. [Learn more about keys here](#authentication.md).

Edit `docker/config.yml` with your master key and provider credentials. See the [Configuration Guide](#configuration.md) for detailed options.

### Start the Services

Launch the gateway and database with a single command:

```bash
docker-compose -f docker/docker-compose.yml up -d
```

This command will:
- Pull the PostgreSQL 16 Alpine image
- Build the gateway Docker image from source (or pull from GHCR if configured)
- Create a dedicated network for service communication
- Start PostgreSQL with automatic health checks
- Wait for the database to be healthy before starting the gateway
- Initialize database tables and schema automatically

The `-d` flag runs services in detached mode (background).

### Verify Deployment

Confirm everything is running correctly:

```bash
# Test the health endpoint
curl http://localhost:8000/health
# Expected: {"status": "healthy"}

# Check service status
docker-compose -f docker/docker-compose.yml ps

# View real-time logs
docker-compose -f docker/docker-compose.yml logs -f gateway
```

If the health check returns successfully, your gateway is ready to accept requests!

## Standalone Docker Deployment

For scenarios where you have an existing PostgreSQL database or prefer more control over your deployment architecture, you can run the gateway as a standalone container.

### Using Pre-built Image

Pull and run the official image from GitHub Container Registry:

```bash
docker pull ghcr.io/mozilla-ai/any-llm/gateway:latest

docker run -d \
  --name any-llm-gateway \
  -p 8000:8000 \
  -v $(pwd)/config.yml:/app/config.yml \
  -e DATABASE_URL="postgresql://user:pass@host:5432/dbname" \
  ghcr.io/mozilla-ai/any-llm/gateway:latest \
  any-llm-gateway serve --config /app/config.yml
```

Replace the `DATABASE_URL` with your actual PostgreSQL connection string. The format is: `postgresql://username:password@hostname:port/database_name`

### Building from Source

If you need to customize the image or test local changes:

```bash
docker build -t any-llm-gateway:local -f docker/Dockerfile .

docker run -d \
  --name any-llm-gateway \
  -p 8000:8000 \
  -v $(pwd)/config.yml:/app/config.yml \
  -e DATABASE_URL="postgresql://user:pass@host:5432/dbname" \
  any-llm-gateway:local
```

## Production Deployment

Production deployments require additional considerations for reliability, security, and performance.

### Production Configuration

Enhance your docker-compose.yml with production-grade settings:

```yaml
services:
  gateway:
    image: ghcr.io/mozilla-ai/any-llm/gateway:latest
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
```

### Nginx Reverse Proxy

For production, always use a reverse proxy with HTTPS:

```nginx
server {
    listen 443 ssl http2;
    server_name gateway.yourdomain.com;

    ssl_certificate /etc/ssl/certs/gateway.crt;
    ssl_certificate_key /etc/ssl/private/gateway.key;

    # Security headers
    add_header Strict-Transport-Security "max-age=31536000" always;

    location / {
        proxy_pass http://localhost:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        # Timeouts for LLM streaming
        proxy_read_timeout 300s;
        proxy_connect_timeout 75s;
    }
}
```

## Environment Variables

The gateway can be configured using environment variables instead of or in addition to a config file. This is useful for Docker deployments and follows 12-factor app principles.

For a complete list of environment variables and configuration options, see the [Configuration Guide](#configuration.md).

**Docker Compose example with .env file:**

```yaml
services:
  gateway:
    env_file:
      - .env
```

## Database Backups

```bash
# Backup
docker-compose -f docker/docker-compose.yml exec postgres \
  pg_dump -U gateway gateway > backup.sql

# Restore
docker-compose -f docker/docker-compose.yml exec -T postgres \
  psql -U gateway gateway < backup.sql
```

## Security Best Practices

1. **Never commit secrets** - Use `.env` files (gitignored) or Docker secrets
2. **Use read-only volumes** - Mount configs with `:ro` flag
3. **Enable HTTPS** - Use a reverse proxy with SSL certificates
4. **Isolate networks** - Keep database on internal network only
5. **Update regularly** - Use tagged versions and update containers periodically

## Monitoring and Logging

### Health Checks

```bash
# Test health endpoint
curl http://localhost:8000/health

# Check container health status
docker inspect --format='{{.State.Health.Status}}' container-name
```

### Logging

```bash
# View logs
docker-compose logs -f gateway

# Last 100 lines
docker-compose logs --tail=100 gateway
```

Configure log rotation:

```yaml
services:
  gateway:
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
```

## Troubleshooting

**Container won't start:**
```bash
docker-compose logs gateway
```
Common issues: Database connection failed, port in use, missing config

**Database connection issues:**
```bash
docker-compose exec postgres psql -U gateway -c "SELECT version();"
```

**Permission errors:**
```bash
chmod 644 docker/config.yml
chmod 600 docker/service_account.json
```

**Rebuild after changes:**
```bash
docker-compose -f docker/docker-compose.yml up -d --build
```

## Next Steps

- [Configuration Guide](#configuration.md) - Advanced configuration options
- [Authentication](#authentication.md) - Set up API keys and user management
- [Budget Management](#budget-management.md) - Configure spending limits
- [API Reference](#api-reference.md) - Explore the complete API
- [Troubleshooting](#troubleshooting.md) - Common issues and solutions


---
