# Source: https://www.promptfoo.dev/docs/category/red-teaming/

# Red teaming

## Red teaming

[ğŸ“„ï¸ Intro](/docs/red-team/)  
Red team LLM systems through systematic adversarial testing to detect content policy violations, information leakage, and API misuse before production deployment

[ğŸ“„ï¸ Quickstart](/docs/red-team/quickstart/)  
Start red teaming LLMs in minutes by scanning 50+ vulnerabilities including jailbreaks, prompt injection, and data exfiltration

[ğŸ“„ï¸ Configuration](/docs/red-team/configuration/)  
Red team your LLM configuration settings using automated vulnerability scanning to detect misconfigurations and prevent unauthorized access to AI system parameters

[ğŸ“„ï¸ Architecture](/docs/red-team/architecture/)  
Red team AI systems by analyzing architecture components and attack surfaces to protect LLM applications through systematic vulnerability assessment and threat modeling

[ğŸ“„ï¸ Types of LLM vulnerabilities](/docs/red-team/llm-vulnerability-types/)  
Red team LLM systems for security, privacy, and criminal vulnerabilities using modular testing plugins to protect AI applications from exploitation and data breaches

[ğŸ“„ï¸ Risk Scoring](/docs/red-team/risk-scoring/)  
Promptfoo provides a risk scoring system that quantifies the severity and likelihood of vulnerabilities in your LLM application. Each vulnerability is assigned a risk score between 0 and 10 that helps you prioritize remediation efforts.

[ğŸ—ƒï¸ Plugins](/docs/red-team/plugins/)  
5 items

[ğŸ—ƒï¸ Strategies](/docs/red-team/strategies/)  
6 items

[ğŸ—ƒï¸ Frameworks](/docs/red-team/nist-ai-rmf/)  
8 items

[ğŸ—ƒï¸ Tools](/docs/red-team/discovery/)  
3 items

[ğŸ—ƒï¸ Troubleshooting](/docs/red-team/troubleshooting/overview/)  
12 items

[ğŸ—ƒï¸ Guides](/docs/guides/llm-redteaming/)  
9 items

[Previous: Travis CI](/docs/integrations/travis-ci/)  
[Next: Intro](/docs/red-team/)