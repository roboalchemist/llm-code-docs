# configuration

Configuration
=============

*   [ğŸ“„ï¸ Guide](/docs/configuration/guide/)
    Complete guide to configuring promptfoo for LLM evaluation. Learn prompts, providers, test cases, assertions, and advanced features with examples.
*   [ğŸ“„ï¸ Reference](/docs/configuration/reference/)
    Comprehensive reference for all promptfoo configuration options, properties, and settings. Complete API documentation for evaluation setup.
*   [ğŸ“„ï¸ Prompts](/docs/configuration/prompts/)
    Configure prompts for LLM evaluation including text prompts, chat conversations, file-based prompts, and dynamic prompt generation with variables.
*   [ğŸ“„ï¸ Test Cases](/docs/configuration/test-cases/)
    Configure test cases for LLM evaluation with variables, assertions, CSV data, and dynamic generation. Learn inline tests, external files, and media support.
*   [ğŸ“„ï¸ Output Formats](/docs/configuration/outputs/)
    Configure output formats for LLM evaluation results. Export to HTML, JSON, CSV, and YAML formats for analysis, reporting, and data processing.
*   [ğŸ“„ï¸ Chat threads](/docs/configuration/chat/)
    Configure chat conversations and multi-turn threads for LLM evaluation. Learn conversation history, multi-shot prompts, and chat flow testing.
*   [ğŸ“„ï¸ Dataset generation](/docs/configuration/datasets/)
    Generate comprehensive test datasets automatically using promptfoo. Create diverse test cases, personas, and edge cases for thorough LLM evaluation.
*   [ğŸ“„ï¸ Scenarios](/docs/configuration/scenarios/)
    Configure scenarios to group test data with evaluation tests. Learn how to organize and run multiple test combinations efficiently in promptfoo.
*   [ğŸ“„ï¸ Caching](/docs/configuration/caching/)
    Configure caching for faster LLM evaluations. Learn cache strategies, storage options, and performance optimization for prompt testing workflows.
*   [ğŸ“„ï¸ Telemetry](/docs/configuration/telemetry/)
    Configure telemetry and analytics for promptfoo usage monitoring. Learn data collection settings, privacy controls, and usage tracking options.
*   [ğŸ“„ï¸ Tracing](/docs/tracing/)
    Implement OpenTelemetry tracing in your LLM evaluations to monitor provider performance, debug workflows, and visualize execution traces directly in Promptfoo's web UI.

[Previous: Getting Started](/docs/getting-started/)
[Next: Guide](/docs/configuration/guide/)