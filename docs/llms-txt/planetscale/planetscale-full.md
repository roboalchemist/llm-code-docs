# Planetscale Documentation

Source: https://planetscale.com/docs/llms-full.txt

---

# OpenAPI Spec
Source: https://planetscale.com/docs/api/openapi-spec

Download the PlanetScale API OpenAPI 3.0 specification

The following specification is for `https://api.planetscale.com/v1`.

<Card title="Download OpenAPI Spec" icon="download" href="/docs/openapi.yaml" arrow="true" cta="Download OpenAPI Spec">
  Download the PlanetScale API OpenAPI 3.0 specification in YAML format.
</Card>

The spec is updated weekly to ensure it is up to date with the latest API changes.


# PlanetScale API and OAuth applications
Source: https://planetscale.com/docs/api/planetscale-api-oauth-applications

The PlanetScale API allows you to manage your PlanetScale databases programmatically.

## PlanetScale API overview

The API is currently in **beta**. The API supports many actions you can take in the PlanetScale web app or CLI and can be used to integrate PlanetScale into your existing workflows and tools.

The PlanetScale API does **not** include direct access to the data in the database. Some endpoints will consist of database schema information or connection information. You will still need to use a [client library, object–relational mapping tool (ORM)](/docs/vitess/tutorials/connect-any-application), or the [PlanetScale serverless driver](/docs/vitess/tutorials/planetscale-serverless-driver) to read from and write data to your PlanetScale database.

Learn more about the API and how to use it in the [PlanetScale API documentation](/docs/api/reference/getting-started-with-planetscale-api).

## Use cases

Some examples of what you can do with the PlanetScale API:

* Automatically [create and delete database branches](/docs/api/reference/create_branch) from CI/CD pipelines or data migration tooling
* Programmatically build out new environments that connect to PlanetScale database branches for testing
* Get information about a PlanetScale user, database, branch, organization, and deploy request
* [Check the status of deploy requests](/docs/api/reference/get_deploy_request) in the deploy queue
* Automate [creating and deleting database connection strings](/docs/api/reference/create_password) for internal users or tools
* [Create, update, approve, deploy, and delete deploy requests](/docs/api/reference/create_deploy_request) programmatically from tooling outside of PlanetScale

Anywhere that can programmatically use an HTTP API can be integrated with PlanetScale. This includes CLI tools, build scripts, desktop applications, and more.

## OAuth applications

The PlanetScale platform also allows you to create OAuth applications. Creating an OAuth application within PlanetScale allows your application to access your users’ PlanetScale accounts.

With PlanetScale OAuth applications, you can choose what access your application needs, and a user will allow (or deny) your application those accesses on their PlanetScale account. The organization that you create the OAuth application in is the "owner" of the application.

By using PlanetScale OAuth as a PlanetScale partner or developer, your organization is agreeing to prominently display a privacy policy and obtain consent to your terms of use from all users of your products and services.

<Note>
  PlanetScale OAuth applications are in beta. Opt-in to the beta's terms of service when creating an OAuth application in your PlanetScale organization's [**Settings > OAuth applications**](https://app.planetscale.com/~/settings/oauth-applications) page.
</Note>

You can read more about creating an OAuth application in PlanetScale in the [PlanetScale OAuth documentation](/docs/api/reference/oauth).

If you build something you would like to share with us, please email us at `education (at) planetscale.com`. We would love to hear about your experience building the application, and we may even feature your application in future blog posts, videos, or social media.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Cancel a resize request
Source: https://planetscale.com/docs/api/reference/cancel_bouncer_resize_request

delete /organizations/{organization}/databases/{database}/branches/{branch}/bouncers/{bouncer}/resizes

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `write_database`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `write_databases` |
| Database | `write_database` |



# Cancel a change request
Source: https://planetscale.com/docs/api/reference/cancel_branch_change_request

delete /organizations/{organization}/databases/{database}/branches/{branch}/resizes

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `write_database`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `write_databases` |
| Database | `write_database` |



# Cancel a queued deploy request
Source: https://planetscale.com/docs/api/reference/cancel_deploy_request

post /organizations/{organization}/databases/{database}/deploy-requests/{number}/cancel

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_deploy_request`, `create_deploy_request`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `deploy_deploy_requests` |
| Database | `deploy_deploy_requests` |



# Close a deploy request
Source: https://planetscale.com/docs/api/reference/close_deploy_request

patch /organizations/{organization}/databases/{database}/deploy-requests/{number}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_deploy_request`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `write_deploy_requests` |
| Database | `write_deploy_requests` |



# Complete an errored deploy
Source: https://planetscale.com/docs/api/reference/complete_errored_deploy

post /organizations/{organization}/databases/{database}/deploy-requests/{number}/complete-deploy

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_deploy_request`, `create_deploy_request`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `deploy_deploy_requests` |
| Database | `deploy_deploy_requests` |



# Complete a gated deploy request
Source: https://planetscale.com/docs/api/reference/complete_gated_deploy_request

post /organizations/{organization}/databases/{database}/deploy-requests/{number}/apply-deploy

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_deploy_request`, `create_deploy_request`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `deploy_deploy_requests` |
| Database | `deploy_deploy_requests` |



# Complete a revert
Source: https://planetscale.com/docs/api/reference/complete_revert

post /organizations/{organization}/databases/{database}/deploy-requests/{number}/revert

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_deploy_request`, `create_deploy_request`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `deploy_deploy_requests` |
| Database | `deploy_deploy_requests` |



# Create a backup
Source: https://planetscale.com/docs/api/reference/create_backup

post /organizations/{organization}/databases/{database}/branches/{branch}/backups

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `write_backups`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `write_backups` |
| Database | `write_backups` |
| Branch | `write_backups` |



# Create a bouncer
Source: https://planetscale.com/docs/api/reference/create_bouncer

post /organizations/{organization}/databases/{database}/branches/{branch}/bouncers

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `write_database`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `write_databases` |
| Database | `write_database` |



# Create a branch
Source: https://planetscale.com/docs/api/reference/create_branch

post /organizations/{organization}/databases/{database}/branches

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `create_branch`, `restore_production_branch_backup`, `restore_backup`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `write_branches`, `restore_production_branch_backups`, `restore_backups` |
| Database | `write_branches`, `restore_production_branch_backups`, `restore_backups` |
| Branch | `restore_backups` |



# Create a database
Source: https://planetscale.com/docs/api/reference/create_database

post /organizations/{organization}/databases

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `create_databases`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `create_databases` |



# Create an IP restriction entry
Source: https://planetscale.com/docs/api/reference/create_database_postgres_cidr

post /organizations/{organization}/databases/{database}/cidrs

### Authorization
A   OAuth token must have at least one of the following   scopes in order to use this API endpoint:

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `write_databases` |
| Database | `write_database` |



# Create a deploy request
Source: https://planetscale.com/docs/api/reference/create_deploy_request

post /organizations/{organization}/databases/{database}/deploy-requests

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_deploy_request`, `create_deploy_requests`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `write_deploy_requests` |
| Database | `write_deploy_requests` |



# Create a keyspace
Source: https://planetscale.com/docs/api/reference/create_keyspace

post /organizations/{organization}/databases/{database}/branches/{branch}/keyspaces

### Authorization
A service token   must have at least one of the following access   in order to use this API endpoint:

**Service Token Accesses**
 `create_branch`





# Create or renew an OAuth token
Source: https://planetscale.com/docs/api/reference/create_oauth_token

post /organizations/{organization}/oauth-applications/{id}/token
Create an OAuth token from an authorization grant code, or refresh an OAuth token from a refresh token
### Authorization
A service token   must have at least one of the following access   in order to use this API endpoint:

**Service Token Accesses**
 `write_oauth_tokens`





# Create an organization team
Source: https://planetscale.com/docs/api/reference/create_organization_team

post /organizations/{organization}/teams

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `write_teams`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `write_organization` |



# Create a password
Source: https://planetscale.com/docs/api/reference/create_password

post /organizations/{organization}/databases/{database}/branches/{branch}/passwords

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `connect_production_branch`, `connect_branch`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `manage_passwords`, `manage_production_branch_passwords` |
| Database | `manage_passwords`, `manage_production_branch_passwords` |
| Branch | `manage_passwords` |



# Create a new query patterns report
Source: https://planetscale.com/docs/api/reference/create_query_patterns_report

post /organizations/{organization}/databases/{database}/branches/{branch}/query-patterns

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_branch`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_branches` |
| Database | `read_branches` |
| Branch | `read_branch` |



# Create role credentials
Source: https://planetscale.com/docs/api/reference/create_role

post /organizations/{organization}/databases/{database}/branches/{branch}/roles

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `create_production_branch_password`, `create_branch_password`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `manage_passwords`, `manage_production_branch_passwords` |
| Database | `manage_passwords`, `manage_production_branch_passwords` |
| Branch | `manage_passwords` |



# Create a service token
Source: https://planetscale.com/docs/api/reference/create_service_token

post /organizations/{organization}/service-tokens
Create a new service token for the organization.
### Authorization
A service token   must have at least one of the following access   in order to use this API endpoint:

**Service Token Accesses**
 `write_service_tokens`





# Create a webhook
Source: https://planetscale.com/docs/api/reference/create_webhook

post /organizations/{organization}/databases/{database}/webhooks

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `write_database`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `write_databases` |
| Database | `write_database` |



# Create a workflow
Source: https://planetscale.com/docs/api/reference/create_workflow

post /organizations/{organization}/databases/{database}/workflows





# Delete a backup
Source: https://planetscale.com/docs/api/reference/delete_backup

delete /organizations/{organization}/databases/{database}/branches/{branch}/backups/{id}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `delete_backups`, `delete_production_branch_backups`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `delete_backups`, `delete_production_branch_backups` |
| Database | `delete_backups`, `delete_production_branch_backups` |
| Branch | `delete_backups` |



# Delete a bouncer
Source: https://planetscale.com/docs/api/reference/delete_bouncer

delete /organizations/{organization}/databases/{database}/branches/{branch}/bouncers/{bouncer}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `write_database`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `write_databases` |
| Database | `write_database` |



# Delete a branch
Source: https://planetscale.com/docs/api/reference/delete_branch

delete /organizations/{organization}/databases/{database}/branches/{branch}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `delete_branch`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `delete_branches`, `delete_production_branches` |
| Database | `delete_branches`, `delete_production_branches` |
| Branch | `delete_branch` |



# Delete a database
Source: https://planetscale.com/docs/api/reference/delete_database

delete /organizations/{organization}/databases/{database}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `delete_database`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `delete_databases` |
| Database | `delete_database` |



# Delete an IP restriction entry
Source: https://planetscale.com/docs/api/reference/delete_database_postgres_cidr

delete /organizations/{organization}/databases/{database}/cidrs/{id}

### Authorization
A   OAuth token must have at least one of the following   scopes in order to use this API endpoint:

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `write_databases` |
| Database | `write_database` |



# Delete a keyspace
Source: https://planetscale.com/docs/api/reference/delete_keyspace

delete /organizations/{organization}/databases/{database}/branches/{branch}/keyspaces/{keyspace}

### Authorization
A service token   must have at least one of the following access   in order to use this API endpoint:

**Service Token Accesses**
 `delete_branch`, `delete_production_branch`





# Delete an OAuth token
Source: https://planetscale.com/docs/api/reference/delete_oauth_token

delete /organizations/{organization}/oauth-applications/{application_id}/tokens/{token_id}

### Authorization
A service token   must have at least one of the following access   in order to use this API endpoint:

**Service Token Accesses**
 `delete_oauth_tokens`





# Delete an organization team
Source: https://planetscale.com/docs/api/reference/delete_organization_team

delete /organizations/{organization}/teams/{team}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `write_teams`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `write_organization` |



# Delete a password
Source: https://planetscale.com/docs/api/reference/delete_password

delete /organizations/{organization}/databases/{database}/branches/{branch}/passwords/{id}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `delete_production_branch_password`, `delete_branch_password`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `manage_passwords`, `manage_production_branch_passwords` |
| Database | `manage_passwords`, `manage_production_branch_passwords` |
| Branch | `manage_passwords` |



# Delete a query patterns report
Source: https://planetscale.com/docs/api/reference/delete_query_patterns_report

delete /organizations/{organization}/databases/{database}/branches/{branch}/query-patterns/{id}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_branch`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_branches` |
| Database | `read_branches` |
| Branch | `read_branch` |



# Delete role credentials
Source: https://planetscale.com/docs/api/reference/delete_role

delete /organizations/{organization}/databases/{database}/branches/{branch}/roles/{id}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `delete_production_branch_password`, `delete_branch_password`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `manage_passwords`, `manage_production_branch_passwords` |
| Database | `manage_passwords`, `manage_production_branch_passwords` |
| Branch | `manage_passwords` |



# Delete a service token
Source: https://planetscale.com/docs/api/reference/delete_service_token

delete /organizations/{organization}/service-tokens/{id}
Delete a service token from the organization.
### Authorization
A service token   must have at least one of the following access   in order to use this API endpoint:

**Service Token Accesses**
 `delete_service_tokens`





# Delete a webhook
Source: https://planetscale.com/docs/api/reference/delete_webhook

delete /organizations/{organization}/databases/{database}/webhooks/{id}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `write_database`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `write_databases` |
| Database | `write_database` |



# Demote a branch
Source: https://planetscale.com/docs/api/reference/demote_branch

post /organizations/{organization}/databases/{database}/branches/{branch}/demote
Demotes a branch from production to development
### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `connect_production_branch`, `demote_branches`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `demote_branches` |
| Database | `demote_branches` |



# Disable safe migrations for a branch
Source: https://planetscale.com/docs/api/reference/disable_safe_migrations

delete /organizations/{organization}/databases/{database}/branches/{branch}/safe-migrations





# Enable safe migrations for a branch
Source: https://planetscale.com/docs/api/reference/enable_safe_migrations

post /organizations/{organization}/databases/{database}/branches/{branch}/safe-migrations





# Get a backup
Source: https://planetscale.com/docs/api/reference/get_backup

get /organizations/{organization}/databases/{database}/branches/{branch}/backups/{id}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_backups`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_backups` |
| Database | `read_backups` |
| Branch | `read_backups` |



# Get a bouncer
Source: https://planetscale.com/docs/api/reference/get_bouncer

get /organizations/{organization}/databases/{database}/branches/{branch}/bouncers/{bouncer}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_branch`, `delete_branch`, `create_branch`, `connect_production_branch`, `connect_branch`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_branches` |
| Database | `read_branches` |
| Branch | `read_branch` |



# Get a branch
Source: https://planetscale.com/docs/api/reference/get_branch

get /organizations/{organization}/databases/{database}/branches/{branch}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_branch`, `delete_branch`, `create_branch`, `connect_production_branch`, `connect_branch`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_branches` |
| Database | `read_branches` |
| Branch | `read_branch` |



# Get a branch change request
Source: https://planetscale.com/docs/api/reference/get_branch_change_request

get /organizations/{organization}/databases/{database}/branches/{branch}/changes/{id}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_branch`, `delete_branch`, `create_branch`, `connect_production_branch`, `connect_branch`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_branches` |
| Database | `read_branches` |
| Branch | `read_branch` |



# Get a branch schema
Source: https://planetscale.com/docs/api/reference/get_branch_schema

get /organizations/{organization}/databases/{database}/branches/{branch}/schema

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_branch`, `delete_branch`, `create_branch`, `connect_production_branch`, `connect_branch`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_branches` |
| Database | `read_branches` |
| Branch | `read_branch` |



# Get current user
Source: https://planetscale.com/docs/api/reference/get_current_user

get /user
Get the user associated with this service token
### Authorization
A   OAuth token must have at least one of the following   scopes in order to use this API endpoint:

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| User | `read_user` |



# Get a database
Source: https://planetscale.com/docs/api/reference/get_database

get /organizations/{organization}/databases/{database}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_database`, `delete_database`, `write_database`, `read_branch`, `delete_branch`, `create_branch`, `delete_production_branch`, `connect_branch`, `connect_production_branch`, `delete_branch_password`, `delete_production_branch_password`, `read_deploy_request`, `create_deploy_request`, `approve_deploy_request`, `read_schema_recommendations`, `close_schema_recommendations`, `read_comment`, `create_comment`, `restore_backup`, `restore_production_branch_backup`, `read_backups`, `write_backups`, `delete_backups`, `delete_production_branch_backups`, `write_branch_vschema`, `write_production_branch_vschema`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_databases` |
| Database | `read_database` |



# Get an IP restriction entry
Source: https://planetscale.com/docs/api/reference/get_database_postgres_cidr

get /organizations/{organization}/databases/{database}/cidrs/{id}

### Authorization
A   OAuth token must have at least one of the following   scopes in order to use this API endpoint:

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_databases` |
| Database | `read_database` |



# Get database throttler configurations
Source: https://planetscale.com/docs/api/reference/get_database_throttler

get /organizations/{organization}/databases/{database}/throttler

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_deploy_request`, `create_deploy_request`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_databases` |
| Database | `read_database` |



# Get the default postgres role
Source: https://planetscale.com/docs/api/reference/get_default_role

get /organizations/{organization}/databases/{database}/branches/{branch}/roles/default

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_branch`, `delete_branch`, `create_branch`, `connect_production_branch`, `connect_branch`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `manage_passwords`, `manage_production_branch_passwords` |
| Database | `manage_passwords`, `manage_production_branch_passwords` |
| Branch | `manage_passwords` |



# Get the deploy queue
Source: https://planetscale.com/docs/api/reference/get_deploy_queue

get /organizations/{organization}/databases/{database}/deploy-queue
The deploy queue returns the current list of deploy requests in the order they will be deployed.




# Get a deploy request
Source: https://planetscale.com/docs/api/reference/get_deploy_request

get /organizations/{organization}/databases/{database}/deploy-requests/{number}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_deploy_request`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_deploy_requests` |
| Database | `read_deploy_requests` |



# Get deploy request throttler configurations
Source: https://planetscale.com/docs/api/reference/get_deploy_request_throttler

get /organizations/{organization}/databases/{database}/deploy-requests/{number}/throttler

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_deploy_request`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_deploy_requests` |
| Database | `read_deploy_requests` |



# Get a deployment
Source: https://planetscale.com/docs/api/reference/get_deployment

get /organizations/{organization}/databases/{database}/deploy-requests/{number}/deployment
Get the deployment for a deploy request
### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_deploy_request`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_deploy_requests` |
| Database | `read_deploy_requests` |



# Get an invoice
Source: https://planetscale.com/docs/api/reference/get_invoice

get /organizations/{organization}/invoices/{id}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_invoices`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_invoices` |



# Get invoice line items
Source: https://planetscale.com/docs/api/reference/get_invoice_line_items

get /organizations/{organization}/invoices/{id}/line-items
Get the line items for an invoice
### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_invoices`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_invoices` |



# Get a keyspace
Source: https://planetscale.com/docs/api/reference/get_keyspace

get /organizations/{organization}/databases/{database}/branches/{branch}/keyspaces/{keyspace}

### Authorization
A service token   must have at least one of the following access   in order to use this API endpoint:

**Service Token Accesses**
 `read_branch`





# Get keyspace rollout status
Source: https://planetscale.com/docs/api/reference/get_keyspace_rollout_status

get /organizations/{organization}/databases/{database}/branches/{branch}/keyspaces/{keyspace}/rollout-status

### Authorization
A service token   must have at least one of the following access   in order to use this API endpoint:

**Service Token Accesses**
 `read_branch`





# Get the VSchema for the keyspace
Source: https://planetscale.com/docs/api/reference/get_keyspace_vschema

get /organizations/{organization}/databases/{database}/branches/{branch}/keyspaces/{keyspace}/vschema

### Authorization
A service token   must have at least one of the following access   in order to use this API endpoint:

**Service Token Accesses**
 `read_branch`, `delete_branch`, `create_branch`, `connect_production_branch`, `connect_branch`





# Get an OAuth application
Source: https://planetscale.com/docs/api/reference/get_oauth_application

get /organizations/{organization}/oauth-applications/{application_id}

### Authorization
A service token   must have at least one of the following access   in order to use this API endpoint:

**Service Token Accesses**
 `read_oauth_applications`





# Get an OAuth token
Source: https://planetscale.com/docs/api/reference/get_oauth_token

get /organizations/{organization}/oauth-applications/{application_id}/tokens/{token_id}

### Authorization
A service token   must have at least one of the following access   in order to use this API endpoint:

**Service Token Accesses**
 `read_oauth_tokens`





# Get an organization
Source: https://planetscale.com/docs/api/reference/get_organization

get /organizations/{organization}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_organization`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| User | `read_organizations` |
| Organization | `read_organization` |



# Get an organization member
Source: https://planetscale.com/docs/api/reference/get_organization_membership

get /organizations/{organization}/members/{id}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_organization`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_organization` |



# Get an organization team
Source: https://planetscale.com/docs/api/reference/get_organization_team

get /organizations/{organization}/teams/{team}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_organization`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_organization` |



# Get a password
Source: https://planetscale.com/docs/api/reference/get_password

get /organizations/{organization}/databases/{database}/branches/{branch}/passwords/{id}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_branch`, `delete_branch`, `create_branch`, `connect_production_branch`, `connect_branch`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `manage_passwords`, `manage_production_branch_passwords` |
| Database | `manage_passwords`, `manage_production_branch_passwords` |
| Branch | `manage_passwords` |



# Download a finished query patterns report
Source: https://planetscale.com/docs/api/reference/get_query_patterns_report

get /organizations/{organization}/databases/{database}/branches/{branch}/query-patterns/{id}/download

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_branch`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_branches` |
| Database | `read_branches` |
| Branch | `read_branch` |



# Show the status of a query patterns report
Source: https://planetscale.com/docs/api/reference/get_query_patterns_report_status

get /organizations/{organization}/databases/{database}/branches/{branch}/query-patterns/{id}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_branch`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_branches` |
| Database | `read_branches` |
| Branch | `read_branch` |



# Get a role
Source: https://planetscale.com/docs/api/reference/get_role

get /organizations/{organization}/databases/{database}/branches/{branch}/roles/{id}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_branch`, `delete_branch`, `create_branch`, `connect_production_branch`, `connect_branch`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `manage_passwords`, `manage_production_branch_passwords` |
| Database | `manage_passwords`, `manage_production_branch_passwords` |
| Branch | `manage_passwords` |



# Get a service token
Source: https://planetscale.com/docs/api/reference/get_service_token

get /organizations/{organization}/service-tokens/{id}
Get information about a service token.
### Authorization
A service token   must have at least one of the following access   in order to use this API endpoint:

**Service Token Accesses**
 `read_service_tokens`





# Get a webhook
Source: https://planetscale.com/docs/api/reference/get_webhook

get /organizations/{organization}/databases/{database}/webhooks/{id}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_database`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_databases` |
| Database | `read_database` |



# Get a workflow
Source: https://planetscale.com/docs/api/reference/get_workflow

get /organizations/{organization}/databases/{database}/workflows/{number}





# Getting started with PlanetScale API
Source: https://planetscale.com/docs/api/reference/getting-started-with-planetscale-api

Learn how to start using the PlanetScale API.

## Overview

You can use the PlanetScale API to manage your PlanetScale databases programmatically.

The PlanetScale API does **not** include direct access to the data in the database. Some endpoints will consist of database schema information or connection information.

## Authentication

Before making your first API call, set up the proper authentication for the PlanetScale API using the `Authorization` header. There are two API authentication types: **Service tokens** and **OAuth**.

### Service tokens

Most endpoints only need a service token for authentication, but some organization-specific endpoints also need OAuth. Each endpoint will state what types of authentication are allowed. See the [Service tokens documentation](/docs/api/reference/service-tokens) for creating a service token and making your first API call with the PlanetScale API.

### OAuth applications

All OAuth applications have a comprehensive list of scopes that the application can request from the PlanetScale user. See the [OAuth documentation](/docs/api/reference/oauth) for more info.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Lint a branch schema
Source: https://planetscale.com/docs/api/reference/lint_branch_schema

get /organizations/{organization}/databases/{database}/branches/{branch}/schema/lint

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_branch`, `delete_branch`, `create_branch`, `connect_production_branch`, `connect_branch`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_branches` |
| Database | `read_branches` |
| Branch | `read_branch` |



# List audit logs
Source: https://planetscale.com/docs/api/reference/list_audit_logs

get /organizations/{organization}/audit-log

### Authorization
A service token   must have at least one of the following access   in order to use this API endpoint:

**Service Token Accesses**
 `read_audit_logs`





# List backups
Source: https://planetscale.com/docs/api/reference/list_backups

get /organizations/{organization}/databases/{database}/branches/{branch}/backups

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_backups`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_backups` |
| Database | `read_backups` |
| Branch | `read_backups` |



# Get bouncer resize requests
Source: https://planetscale.com/docs/api/reference/list_bouncer_resize_requests

get /organizations/{organization}/databases/{database}/branches/{branch}/bouncers/{bouncer}/resizes

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_branch`, `delete_branch`, `create_branch`, `connect_production_branch`, `connect_branch`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_branches` |
| Database | `read_branches` |
| Branch | `read_branch` |



# List bouncers
Source: https://planetscale.com/docs/api/reference/list_bouncers

get /organizations/{organization}/databases/{database}/branches/{branch}/bouncers

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_branch`, `delete_branch`, `create_branch`, `connect_production_branch`, `connect_branch`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_branches` |
| Database | `read_branches` |
| Branch | `read_branch` |



# Get bouncer resize requests
Source: https://planetscale.com/docs/api/reference/list_branch_bouncer_resize_requests

get /organizations/{organization}/databases/{database}/branches/{branch}/bouncer-resizes

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_branch`, `delete_branch`, `create_branch`, `connect_production_branch`, `connect_branch`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_branches` |
| Database | `read_branches` |
| Branch | `read_branch` |



# Get branch change requests
Source: https://planetscale.com/docs/api/reference/list_branch_change_requests

get /organizations/{organization}/databases/{database}/branches/{branch}/changes

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_branch`, `delete_branch`, `create_branch`, `connect_production_branch`, `connect_branch`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_branches` |
| Database | `read_branches` |
| Branch | `read_branch` |



# List branches
Source: https://planetscale.com/docs/api/reference/list_branches

get /organizations/{organization}/databases/{database}/branches

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_branch`, `delete_branch`, `create_branch`, `connect_production_branch`, `connect_branch`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_branches` |
| Database | `read_branches` |
| Branch | `read_branch` |



# List available cluster sizes
Source: https://planetscale.com/docs/api/reference/list_cluster_size_skus

get /organizations/{organization}/cluster-size-skus
List available cluster sizes for an organization
### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_organization`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| User | `read_organizations` |
| Organization | `read_organization` |



# List IP restriction entries
Source: https://planetscale.com/docs/api/reference/list_database_postgres_cidrs

get /organizations/{organization}/databases/{database}/cidrs

### Authorization
A   OAuth token must have at least one of the following   scopes in order to use this API endpoint:

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_databases` |
| Database | `read_database` |



# List database regions
Source: https://planetscale.com/docs/api/reference/list_database_regions

get /organizations/{organization}/databases/{database}/regions

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_database`, `delete_database`, `write_database`, `read_branch`, `delete_branch`, `create_branch`, `delete_production_branch`, `connect_branch`, `connect_production_branch`, `delete_branch_password`, `delete_production_branch_password`, `read_deploy_request`, `create_deploy_request`, `approve_deploy_request`, `read_schema_recommendations`, `close_schema_recommendations`, `read_comment`, `create_comment`, `restore_backup`, `restore_production_branch_backup`, `read_backups`, `write_backups`, `delete_backups`, `delete_production_branch_backups`, `write_branch_vschema`, `write_production_branch_vschema`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_databases` |
| Database | `read_database` |



# List databases
Source: https://planetscale.com/docs/api/reference/list_databases

get /organizations/{organization}/databases

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_database`, `delete_database`, `write_database`, `read_branch`, `delete_branch`, `create_branch`, `delete_production_branch`, `connect_branch`, `connect_production_branch`, `delete_branch_password`, `delete_production_branch_password`, `read_deploy_request`, `create_deploy_request`, `approve_deploy_request`, `read_schema_recommendations`, `close_schema_recommendations`, `read_comment`, `create_comment`, `restore_backup`, `restore_production_branch_backup`, `read_backups`, `write_backups`, `delete_backups`, `delete_production_branch_backups`, `write_branch_vschema`, `write_production_branch_vschema`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_databases` |



# List deploy operations
Source: https://planetscale.com/docs/api/reference/list_deploy_operations

get /organizations/{organization}/databases/{database}/deploy-requests/{number}/operations
List deploy operations for a deploy request
### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_deploy_request`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_deploy_requests` |
| Database | `read_deploy_requests` |



# List deploy request reviews
Source: https://planetscale.com/docs/api/reference/list_deploy_request_reviews

get /organizations/{organization}/databases/{database}/deploy-requests/{number}/reviews

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_deploy_request`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_deploy_requests` |
| Database | `read_deploy_requests` |



# List deploy requests
Source: https://planetscale.com/docs/api/reference/list_deploy_requests

get /organizations/{organization}/databases/{database}/deploy-requests
List deploy requests for a database
### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_deploy_request`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_deploy_requests` |
| Database | `read_deploy_requests` |



# List cluster extensions
Source: https://planetscale.com/docs/api/reference/list_extensions

get /organizations/{organization}/databases/{database}/branches/{branch}/extensions

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_branch`, `delete_branch`, `create_branch`, `connect_production_branch`, `connect_branch`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_branches` |
| Database | `read_branches` |
| Branch | `read_branch` |



# List generated query patterns reports
Source: https://planetscale.com/docs/api/reference/list_generated_query_patterns_reports

get /organizations/{organization}/databases/{database}/branches/{branch}/query-patterns

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_branch`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_branches` |
| Database | `read_branches` |
| Branch | `read_branch` |



# Get invoices
Source: https://planetscale.com/docs/api/reference/list_invoices

get /organizations/{organization}/invoices
Get the invoices for an organization
### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_invoices`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_invoices` |



# Get keyspaces
Source: https://planetscale.com/docs/api/reference/list_keyspaces

get /organizations/{organization}/databases/{database}/branches/{branch}/keyspaces

### Authorization
A service token   must have at least one of the following access   in order to use this API endpoint:

**Service Token Accesses**
 `read_branch`





# List OAuth applications
Source: https://planetscale.com/docs/api/reference/list_oauth_applications

get /organizations/{organization}/oauth-applications

### Authorization
A service token   must have at least one of the following access   in order to use this API endpoint:

**Service Token Accesses**
 `read_oauth_applications`





# List OAuth tokens
Source: https://planetscale.com/docs/api/reference/list_oauth_tokens

get /organizations/{organization}/oauth-applications/{application_id}/tokens
List OAuth tokens created by an OAuth application
### Authorization
A service token   must have at least one of the following access   in order to use this API endpoint:

**Service Token Accesses**
 `read_oauth_tokens`





# List organization members
Source: https://planetscale.com/docs/api/reference/list_organization_members

get /organizations/{organization}/members

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_organization`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_organization` |



# List teams in an organization
Source: https://planetscale.com/docs/api/reference/list_organization_teams

get /organizations/{organization}/teams

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_organization`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_organization` |



# List organizations
Source: https://planetscale.com/docs/api/reference/list_organizations

get /organizations
When using a service token, returns the list of organizations the service token has access to. When using an OAuth token, returns the list of organizations the user has access to.
### Authorization
A   OAuth token must have at least one of the following   scopes in order to use this API endpoint:

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| User | `read_organizations` |



# List cluster parameters
Source: https://planetscale.com/docs/api/reference/list_parameters

get /organizations/{organization}/databases/{database}/branches/{branch}/parameters
        Returns the parameters for a branch. To update the parameters, use the "Upsert a change request" endpoint.

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_branch`, `delete_branch`, `create_branch`, `connect_production_branch`, `connect_branch`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_branches` |
| Database | `read_branches` |
| Branch | `read_branch` |



# List passwords
Source: https://planetscale.com/docs/api/reference/list_passwords

get /organizations/{organization}/databases/{database}/branches/{branch}/passwords

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_branch`, `delete_branch`, `create_branch`, `connect_production_branch`, `connect_branch`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `manage_passwords`, `manage_production_branch_passwords` |
| Database | `manage_passwords`, `manage_production_branch_passwords` |
| Branch | `manage_passwords` |



# List public regions
Source: https://planetscale.com/docs/api/reference/list_public_regions

get /regions
Endpoint is available without authentication.




# List read-only regions
Source: https://planetscale.com/docs/api/reference/list_read_only_regions

get /organizations/{organization}/databases/{database}/read-only-regions
List read-only regions for the database's default branch
### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_database`, `delete_database`, `write_database`, `read_branch`, `delete_branch`, `create_branch`, `delete_production_branch`, `connect_branch`, `connect_production_branch`, `delete_branch_password`, `delete_production_branch_password`, `read_deploy_request`, `create_deploy_request`, `approve_deploy_request`, `read_schema_recommendations`, `close_schema_recommendations`, `read_comment`, `create_comment`, `restore_backup`, `restore_production_branch_backup`, `read_backups`, `write_backups`, `delete_backups`, `delete_production_branch_backups`, `write_branch_vschema`, `write_production_branch_vschema`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_branches` |
| Database | `read_branches` |



# List regions for an organization
Source: https://planetscale.com/docs/api/reference/list_regions_for_organization

get /organizations/{organization}/regions

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_organization`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| User | `read_organizations` |
| Organization | `read_organization` |



# List roles
Source: https://planetscale.com/docs/api/reference/list_roles

get /organizations/{organization}/databases/{database}/branches/{branch}/roles

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_branch`, `delete_branch`, `create_branch`, `connect_production_branch`, `connect_branch`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `manage_passwords`, `manage_production_branch_passwords` |
| Database | `manage_passwords`, `manage_production_branch_passwords` |
| Branch | `manage_passwords` |



# List service tokens
Source: https://planetscale.com/docs/api/reference/list_service_tokens

get /organizations/{organization}/service-tokens
List service tokens for an organization.
### Authorization
A service token   must have at least one of the following access   in order to use this API endpoint:

**Service Token Accesses**
 `read_service_tokens`





# List webhooks
Source: https://planetscale.com/docs/api/reference/list_webhooks

get /organizations/{organization}/databases/{database}/webhooks
List webhooks for a database
### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_database`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `read_databases` |
| Database | `read_database` |



# List workflows
Source: https://planetscale.com/docs/api/reference/list_workflows

get /organizations/{organization}/databases/{database}/workflows





# OAuth
Source: https://planetscale.com/docs/api/reference/oauth

Using PlanetScale OAuth enables your users to connect their accounts to PlanetScale.

## Overview

Creating an OAuth application within PlanetScale allows your application to access your users’ PlanetScale accounts.

With PlanetScale OAuth applications, you can choose what access your application needs, and a user will allow (or deny) your application those accesses on their PlanetScale account. The organization that you create the OAuth application in is the "owner" of the application.

## Getting started

### 1. Creating an OAuth application in PlanetScale

1. To create a new OAuth application, log into your organization and click **Settings > OAuth applications**.
2. Create a new OAuth application by clicking **Create new application**.
3. You will need to fill out the following fields:

* **Name**: A user-friendly name for your OAuth application.
* **Domain**: The full URL to your application's domain.
* **Redirect URI**: The full URL PlanetScale should redirect users on completion of the authorization flow, also known as the callback URL. It must have the same domain as the domain above.
* **Avatar**: An image that represents your OAuth application. (Optional but recommended.)

<Warning>
  You will also be agreeing, on behalf of your organization, to prominently display a privacy policy and obtain consent to your organization's terms of use from all users of your products and services.
</Warning>

### 2. Credentials to copy to your application code

Once you have created your OAuth application in PlanetScale, you will need the following credentials to use the OAuth authorization flow:

* **ID**: Your OAuth application's ID.
* **Client ID**: Your OAuth application's client ID.
* **Redirect URL**: The full URL PlanetScale should redirect users on completion of the authorization flow, also known as the callback URL.
* **Client secret**: Your OAuth application's client secret, used to exchange *access grants* for service tokens. (This will only be shown once, make sure to save it!)

Later in this document, we will go through how you use each of these credentials. We recommend saving them as environment variables.

### 3. OAuth application access scopes

Every OAuth application in PlanetScale will request from its users a specific set of permissions in the users' databases. We call these permissions "access scopes." They are broken into:

* User access
* Organization access
* Database access
* Branch access

Access is scoped to a resource. For example, selecting `write_branches` on an organization allows you to write branches across all databases in organizations the user gives permission to, while `write_branches` on a database enables you to only write branches in databases the user gives permission to.

The API reference for each endpoint will say what scope is needed.

In this step, select the access scopes you think your application will need on a user's account and click the **Save access scopes** button.

<Frame caption="This is only a partial list of the OAuth access scopes. For a full list of scopes, see the OAuth access scopes documentation.">
  <a href="/docs/api/reference/oauth-access-scopes">
    <img src="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/31ea0ef-CleanShot_2024-02-01_at_15.32.27.jpg?fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=1b1552118e5f4a3fa1ee4731c5cd19fb" alt="OAuth access scopes selection interface" data-og-width="1075" width="1075" data-og-height="893" height="893" data-path="docs/images/reference/31ea0ef-CleanShot_2024-02-01_at_15.32.27.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/31ea0ef-CleanShot_2024-02-01_at_15.32.27.jpg?w=280&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=1eb76259611eb188489f77652598c55f 280w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/31ea0ef-CleanShot_2024-02-01_at_15.32.27.jpg?w=560&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=d5ef28c0b11d1100e2627e917811f413 560w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/31ea0ef-CleanShot_2024-02-01_at_15.32.27.jpg?w=840&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=2688c3f22c9e5db3b7a115b70d536404 840w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/31ea0ef-CleanShot_2024-02-01_at_15.32.27.jpg?w=1100&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=f87dd4ccb484aa72e8a3602aedde8937 1100w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/31ea0ef-CleanShot_2024-02-01_at_15.32.27.jpg?w=1650&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=b7ef6e925aab9cb3f3d502dbf6ea288c 1650w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/31ea0ef-CleanShot_2024-02-01_at_15.32.27.jpg?w=2500&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=3271bedac3736ad0eed765875d0db16c 2500w" />

    <span className="sr-only">View OAuth access scopes documentation</span>
  </a>
</Frame>

## OAuth Authorization Flow

PlanetScale's OAuth implementation supports the [Authorization Code grant type](https://oauth.net/2/grant-types/authorization-code/). The following diagram walks through the flow.

<Frame caption="OAuth authorization flow diagram">
  <img src="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/c46b041-oauth_diagram.png?fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=af734aec19f3da002b77cb6a561b4485" alt="OAuth authorization flow diagram" data-og-width="2000" width="2000" data-og-height="1321" height="1321" data-path="docs/images/reference/c46b041-oauth_diagram.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/c46b041-oauth_diagram.png?w=280&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=ad8f2e1e7b8886a96b93850255c9e3ca 280w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/c46b041-oauth_diagram.png?w=560&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=6d6b73206c39d80e0cda34dbc9b48934 560w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/c46b041-oauth_diagram.png?w=840&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=afa4c8c62431393d44171afdee3a62a4 840w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/c46b041-oauth_diagram.png?w=1100&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=18f2167435e9430288533aadc1dfeb3b 1100w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/c46b041-oauth_diagram.png?w=1650&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=8dc62c4c8d7ad43a9035669ed5285cd3 1650w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/c46b041-oauth_diagram.png?w=2500&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=a7137c6b769a5a1085d0c585273309b1 2500w" />
</Frame>

### 0. Prerequisites

You must have [created a service token](/docs/api/reference/service-tokens) in your OAuth application's organization.

Copy and paste the ID and service token into your code, where the rest of your important credentials are stored.

A service token is needed to use the PlanetScale API to create OAuth tokens as a part of the OAuth authorization flow. You will need the following organization-level accesses on the service token in order to complete the flow: `read_oauth_applications`, `write_oauth_tokens`, `read_oauth_tokens`.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/e3a5f65-CleanShot_2024-02-01_at_15.43.31.jpg?fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=ba4bfc9acde96505e0f6033a8fd7f529" alt="Shows the read_oauth_applications, write_oauth_tokens, read_oauth_tokens scopes selected." data-og-width="644" width="644" data-og-height="504" height="504" data-path="docs/images/reference/e3a5f65-CleanShot_2024-02-01_at_15.43.31.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/e3a5f65-CleanShot_2024-02-01_at_15.43.31.jpg?w=280&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=213fb94f3b80dc95938ab3c0a6d84c4c 280w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/e3a5f65-CleanShot_2024-02-01_at_15.43.31.jpg?w=560&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=95aa170974763e1d335372793a28fd93 560w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/e3a5f65-CleanShot_2024-02-01_at_15.43.31.jpg?w=840&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=eab7cac792f05293885c23ce4d027464 840w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/e3a5f65-CleanShot_2024-02-01_at_15.43.31.jpg?w=1100&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=b69ac5bbdcee567db1df54143940a595 1100w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/e3a5f65-CleanShot_2024-02-01_at_15.43.31.jpg?w=1650&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=5854f20ea18280aab61ea22502fd0d31 1650w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/e3a5f65-CleanShot_2024-02-01_at_15.43.31.jpg?w=2500&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=536f86901690369fffad4fe5fc0ad9cc 2500w" />
</Frame>

### 1. User authorizes your OAuth application on their account

Your application should direct your users to the PlanetScale authorization page (see URL below) so that they can grant your application access to their PlanetScale account:

<CodeGroup>
  ```text Text theme={null}
  http://app.planetscale.com/oauth/authorize?client_id=CLIENT_ID&redirect_uri=REDIRECT_URI&state=STATE
  ```
</CodeGroup>

### Query parameters:

| Name               | Type   | Description                                                                                                                                                                                                            |
| :----------------- | :----- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `client_id`        | string | Your OAuth application's client id                                                                                                                                                                                     |
| `redirect_uri`     | string | The full URL PlanetScale should redirect users on completion of the authorization flow, also known as the callback URL.                                                                                                |
| `state` (optional) | string | You may also optionally pass a state parameter, which exists to prevent third-party attacks. Pass a random string, and PlanetScale will return it in step 2. Compare to ensure the request came from your application. |

## 2. The authorization code returns to your application

Upon authorization, PlanetScale will redirect the user to your `redirect_uri` with an authorization code in the query parameters. The authorization code is only good for one use. It will look like the following URL:

```
https://my-redirect-uri.com?code=AUTHORIZATION_CODE&state=STATE
```

### Query parameters:

| Name                          | Type   | Description                                                                                                                                              |
| :---------------------------- | :----- | :------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `code`                        | string | An authorization code to be exchanged for an access token.                                                                                               |
| `state` (if you provided one) | string | Compare with the original `state` parameter to ensure they match. Abort the process if they do not because the request may have come from a third party. |

### 3a. Exchange authorization code for an OAuth token

Your application can now exchange the authorization code for an access token.

**POST**

```
https://api.planetscale.com/v1/organizations/:organization_name/oauth-applications/:application_id/token?client_id=CLIENT_ID&client_secret=CLIENT_SECRET&code=CODE&grant_type=authorization_code&redirect_uri=REDIRECT_URI
```

The `POST` request will need the following:

### Headers:

`Authorization: <SERVICE_TOKEN_ID>:<SERVICE_TOKEN>`

### Path parameters:

| Name                | Type   | Description                                         |
| :------------------ | :----- | :-------------------------------------------------- |
| `organization_name` | string | Your organization name.                             |
| `application_id`    | string | Your OAuth application's ID (12 character long ID). |

### Query parameters:

| Name            | Type   | Description                                                                                                             |
| :-------------- | :----- | :---------------------------------------------------------------------------------------------------------------------- |
| `client_id`     | string | Your OAuth application's client ID.                                                                                     |
| `client_secret` | string | Your OAuth application's client secret.                                                                                 |
| `code`          | string | The code located in the query parameters of the previous step.                                                          |
| `grant_type`    | string | Set to `authorization_code`.                                                                                            |
| `redirect_uri`  | string | The full URL PlanetScale should redirect users on completion of the authorization flow, also known as the callback URL. |

The response will look similar to the following:

<CodeGroup>
  ```json JSON theme={null}
  {
    "id": "cv4d3zi653gv",
    "type": "ServiceToken",
    "display_name": "My OAuth App's Service Token cv4d3zi653gv",
    "avatar_url": "https://my-oauth-app.com/avatar.png",
    "created_at": "2022-08-01T20:19:41.886Z",
    "updated_at": "2022-08-01T20:19:41.886Z",
    "expires_at": "2022-09-01T20:19:41.887Z",
    "last_used_at": null,
    "name": "my-oauth-app",
    "token": "pscale_tkn_O4KbFjH97uOz2bLWJtQXjYgDsqkGgC8bNNlrzgo6YUY",
    "plain_text_refresh_token": "pscale_tkn_W_zjmZ1a14sczj15bxJdsW_kiv063OrHG4CBh0IXR9M",
    "actor_id": "r80q66antldo",
    "actor_display_name": "[email protected]",
    "actor_type": "User",
    "service_token_accesses": [...]
  }
  ```
</CodeGroup>

The three most essential credentials in the response are:

* `id`: Your access token's ID.
* `token`: Your access token.
* `plain_text_refresh_token`: A refresh token that can refresh your access token when it expires. See step 3b for more info.

You will need both the `id` and `token` to make API calls on behalf of the user.

### 3b. Refreshing an OAuth token

When an OAuth token expires, you can refresh it:

**POST**

```
https://api.planetscale.com/v1/organizations/:organization_name/oauth-applications/:application_id/token?client_id=CLIENT_ID&client_secret=CLIENT_SECRET&grant_type=refresh_token&refresh_token=REFRESH_TOKEN
```

The `POST` request will need the following:

### Headers:

`Authorization: <SERVICE_TOKEN_ID>:<SERVICE_TOKEN>`

#### Path parameters:

| Name                | Type   | Description                  |
| :------------------ | :----- | :--------------------------- |
| `organization_name` | string | Your organization name.      |
| `application_id`    | string | Your OAuth application's ID. |

#### Query parameters:

| Name            | Type   | Description                                                      |
| :-------------- | :----- | :--------------------------------------------------------------- |
| `client_id`     | string | Your OAuth application's client ID.                              |
| `client_secret` | string | Your OAuth application's client secret.                          |
| `refresh_token` | string | The refresh token sent in the initial token response in step 3a. |
| `grant_type`    | string | Set to `refresh_token`.                                          |

The response will look similar to the response in 3a.

### 4. Using the OAuth token

Now your application can make requests to the PlanetScale API on behalf of the user. To make requests to the API, add the `id` and `token` in the `Authorization` header in your HTTP API request using the following format:

<CodeGroup>
  ```text Text theme={null}
  Authorization: <OAUTH_TOKEN_ID>:<OAUTH_TOKEN>
  ```
</CodeGroup>

Now your application can make requests to the PlanetScale API on behalf of the user. To make requests to the API, add the `id` and `token` in the `Authorization` header in your HTTP API request using the following format:

<CodeGroup>
  ```text Text theme={null}
  Authorization: <OAUTH_TOKEN_ID>:<OAUTH_TOKEN>
  ```
</CodeGroup>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# OAuth access scopes
Source: https://planetscale.com/docs/api/reference/oauth-access-scopes

The full list of OAuth application access scopes.

See the [OAuth documentation](/docs/api/reference/oauth) for more information on creating OAuth applications on PlanetScale.

## User access

| Access permissions  | Description                 |
| :------------------ | :-------------------------- |
| read\_user          | Read user                   |
| write\_user         | Write user                  |
| read\_organizations | Read a user's organizations |

## Organization access

| Access permissions                    | Description                                         |
| :------------------------------------ | :-------------------------------------------------- |
| write\_organization                   | Write organization                                  |
| read\_organization                    | Read organization                                   |
| read\_invoices                        | Read organization invoices                          |
| delete\_organization                  | Delete organization                                 |
| read\_databases                       | Read organization databases                         |
| create\_databases                     | Create organization databases                       |
| write\_databases                      | Write organization databases                        |
| delete\_databases                     | Delete organization databases                       |
| write\_members                        | Write members                                       |
| read\_members                         | Read members                                        |
| delete\_members                       | Delete members                                      |
| read\_branches                        | Read database branches                              |
| write\_branches                       | Write database branches                             |
| delete\_branches                      | Delete database branches                            |
| promote\_branches                     | Promote database branches                           |
| delete\_production\_branches          | Delete a production database branch                 |
| manage\_production\_branch\_passwords | Read, write, and delete production branch passwords |
| write\_deploy\_requests               | Create and update deploy requests in a database     |
| read\_deploy\_requests                | Read deploy requests in a database                  |
| deploy\_deploy\_requests              | Deploy deploy requests in a database                |
| approve\_deploy\_requests             | Approve deploy requests in a database               |
| write\_comments                       | Create deploy request comments in a database        |
| read\_comments                        | Read deploy request comments in a database          |
| manage\_passwords                     | Read, write, and delete branch passwords            |
| write\_backups                        | Create and update backups                           |
| read\_backups                         | Read backups                                        |
| delete\_backups                       | Delete backups                                      |
| delete\_production\_branch\_backups   | Delete production backups                           |
| restore\_backups                      | Restore this branch's backups to new branches       |
| restore\_production\_branch\_backups  | Restore production branch backups to new            |

## Database access

| Access permissions                    | Description                                         |
| :------------------------------------ | :-------------------------------------------------- |
| write\_members                        | Write members                                       |
| read\_members                         | Read members                                        |
| delete\_members                       | Delete members                                      |
| read\_branches                        | Read database branches                              |
| write\_branches                       | Write database branches                             |
| delete\_branches                      | Delete database branches                            |
| promote\_branches                     | Promote database branches                           |
| delete\_production\_branches          | Delete a production database branch                 |
| manage\_production\_branch\_passwords | Read, write, and delete production branch passwords |
| write\_deploy requests                | Create and update deploy requests in a database     |
| read\_deploy\_requests                | Read deploy requests in a database                  |
| deploy\_deploy\_requests              | Deploy deploy requests in a database                |
| approve\_deploy\_requests             | Approve deploy requests in a database               |
| write\_comments                       | Create deploy request comments in a database        |
| read\_comments                        | Read deploy request comments in a database          |
| read\_database                        | Read database information                           |
| delete\_database                      | Delete a database                                   |
| write\_database                       | Write database                                      |
| manage\_passwords                     | Read, write, and delete branch passwords            |
| write\_backups                        | Create and update backups                           |
| read\_backups                         | Read backups                                        |
| delete\_backups                       | Delete backups                                      |
| delete\_production\_branch\_backups   | Delete production backups                           |
| restore\_backups                      | Restore this branch's backups to new branches       |
| restore\_production\_branch\_backups  | Restore production branch backups to new branches   |

## Branch access

| Access permissions | Description                                   |
| :----------------- | :-------------------------------------------- |
| manage\_passwords  | Read, write, and delete branch passwords      |
| write\_branch      | Write a database branch                       |
| read\_branch       | Read a database branch                        |
| delete\_branch     | Delete a database branch                      |
| write\_backups     | Create and update backups                     |
| read\_backups      | Read backups                                  |
| delete\_backups    | Delete backups                                |
| restore\_backups   | Restore this branch's backups to new branches |

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Pagination
Source: https://planetscale.com/docs/api/reference/pagination

How to paginate API responses

## Page-based pagination

The majority of our API endpoints use standard page-based pagination.

* `page`: the page number
* `per_page`: the number of items returned per page (default: 25, max: 100)

To set these values, pass them as query parameters in your request. For example, [in a list organizations `GET` request](/docs/api/reference/list_organizations), to return the first page at 50 items per page:

<CodeGroup>
  ```curl cURL theme={null}
  curl --request GET \
       --url 'https://api.planetscale.com/v1/organizations?page=1&per_page=50' \
       --header 'Authorization: TOKEN_ID:TOKEN_SECRET' \
       --header 'accept: application/json'
  ```
</CodeGroup>

## Cursor-based pagination

For API endpoints with large result sets, we use cursor-based pagination.

Cursor endpoints will return the following in their payload.

* `cursor_start`: The ID of the first item returned.
* `cursor_end`: The ID of the last item returned.
* `has_next`: Whether there is a next page of results.
* `has_prev`: Whether there is a previous page of results.

For example:

<CodeGroup>
  ```json json theme={null}
  {
  "type": "list",
  "has_next": true,
  "has_prev": false,
  "cursor_start": "b34zxm9mkz7g",
  "cursor_end": "eeq8f2lwrlum",
  "data": []
  }
  ```
</CodeGroup>

### Cursor-based query parameters

To pagination records, the following query parameters are available:

* `starting_after`: The public\_id of the last item in the previous page.
* `ending_before`: The public\_id of the first item in the next page.
* `limit`: The number of items to return. Default DEFAULT\_LIMIT.

For example, use the following to retrieve to items after `eeq8f2lwrlum`.

<CodeGroup>
  ```curl cURL theme={null}
  curl --request GET \
       --url 'https://api.planetscale.com/v1/organizations/my-org/audit-logs?starting_after=eeq8f2lwrlum&limit=50' \
       --header 'Authorization: TOKEN_ID:TOKEN_SECRET' \
       --header 'accept: application/json'
  ```
</CodeGroup>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Promote a branch
Source: https://planetscale.com/docs/api/reference/promote_branch

post /organizations/{organization}/databases/{database}/branches/{branch}/promote
Promotes a branch from development to production
### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `connect_production_branch`, `promote_branches`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `promote_branches` |
| Database | `promote_branches` |



# Queue a deploy request
Source: https://planetscale.com/docs/api/reference/queue_deploy_request

post /organizations/{organization}/databases/{database}/deploy-requests/{number}/deploy

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_deploy_request`, `create_deploy_request`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `deploy_deploy_requests` |
| Database | `deploy_deploy_requests` |



# Rate limits
Source: https://planetscale.com/docs/api/reference/rate-limits

The current rate limit on the PlanetScale API is 600 requests per minute. This rate limit is subject to change based on usage during the beta period.

You will see a **429 error response code** if you hit the rate limit.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Reassign objects owned by one role to another role
Source: https://planetscale.com/docs/api/reference/reassign_role_objects

post /organizations/{organization}/databases/{database}/branches/{branch}/roles/{id}/reassign

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `delete_production_branch_password`, `delete_branch_password`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `manage_passwords`, `manage_production_branch_passwords` |
| Database | `manage_passwords`, `manage_production_branch_passwords` |
| Branch | `manage_passwords` |



# Remove a member from an organization
Source: https://planetscale.com/docs/api/reference/remove_organization_member

delete /organizations/{organization}/members/{id}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `write_organization`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `write_organization` |



# Renew a password
Source: https://planetscale.com/docs/api/reference/renew_password

post /organizations/{organization}/databases/{database}/branches/{branch}/passwords/{id}/renew

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `connect_production_branch`, `connect_branch`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `manage_passwords`, `manage_production_branch_passwords` |
| Database | `manage_passwords`, `manage_production_branch_passwords` |
| Branch | `manage_passwords` |



# Renew role expiration
Source: https://planetscale.com/docs/api/reference/renew_role

post /organizations/{organization}/databases/{database}/branches/{branch}/roles/{id}/renew

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `create_production_branch_password`, `create_branch_password`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `manage_passwords`, `manage_production_branch_passwords` |
| Database | `manage_passwords`, `manage_production_branch_passwords` |
| Branch | `manage_passwords` |



# Reset default credentials
Source: https://planetscale.com/docs/api/reference/reset_default_role

post /organizations/{organization}/databases/{database}/branches/{branch}/roles/reset-default

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `delete_production_branch_password`, `delete_branch_password`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `manage_passwords`, `manage_production_branch_passwords` |
| Database | `manage_passwords`, `manage_production_branch_passwords` |
| Branch | `manage_passwords` |



# Reset a role's password
Source: https://planetscale.com/docs/api/reference/reset_role

post /organizations/{organization}/databases/{database}/branches/{branch}/roles/{id}/reset

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `delete_production_branch_password`, `delete_branch_password`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `manage_passwords`, `manage_production_branch_passwords` |
| Database | `manage_passwords`, `manage_production_branch_passwords` |
| Branch | `manage_passwords` |



# Review a deploy request
Source: https://planetscale.com/docs/api/reference/review_deploy_request

post /organizations/{organization}/databases/{database}/deploy-requests/{number}/reviews
Review a deploy request by either approving or commenting on the deploy request
### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `approve_deploy_request`, `review_deploy_request`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `approve_deploy_requests` |
| Database | `approve_deploy_requests` |



# Sample OAuth application
Source: https://planetscale.com/docs/api/reference/sample-oauth-application

This sample OAuth application shows how to implement OAuth authentication with PlanetScale in a Next.js application.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/4b81ea4-green_lilman2x.png?fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=5cc2ad95133d1f57cde0278a66eb4a09" alt="PlanetPets mascot illustration" data-og-width="294" width="294" data-og-height="367" height="367" data-path="docs/images/reference/4b81ea4-green_lilman2x.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/4b81ea4-green_lilman2x.png?w=280&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=b6dcc17a519477db639724caadb6d927 280w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/4b81ea4-green_lilman2x.png?w=560&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=f94845fd94cf26810b35abc7df775f34 560w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/4b81ea4-green_lilman2x.png?w=840&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=bf97d218132f4e641b809a74772401f0 840w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/4b81ea4-green_lilman2x.png?w=1100&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=5392acbf9e1fb9165cfb268d9376a1ca 1100w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/4b81ea4-green_lilman2x.png?w=1650&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=911725a3c0b3bbcd2a3ec462bbf821e2 1650w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/4b81ea4-green_lilman2x.png?w=2500&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=829fd966d2b2cc41071e870dc337c50c 2500w" />
</Frame>

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/04dbe99-title2x.png?fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=2f95c74d56df7ebac31d38d280c45192" alt="PlanetPets logo" data-og-width="552" width="552" data-og-height="119" height="119" data-path="docs/images/reference/04dbe99-title2x.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/04dbe99-title2x.png?w=280&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=dd252fe491045b1094ef31e3439417d5 280w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/04dbe99-title2x.png?w=560&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=305175667db0d13b5b717599aeb1293a 560w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/04dbe99-title2x.png?w=840&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=eed8e6fe11e5349a0c892a74b463b3c8 840w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/04dbe99-title2x.png?w=1100&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=adfcc26d9bf47dc07c6a6f0fb83589ba 1100w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/04dbe99-title2x.png?w=1650&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=d5b390f9e5a6ec839cf919ba0fe4e523 1650w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/04dbe99-title2x.png?w=2500&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=0038b06959ee879b573533f013c8dd0c 2500w" />
</Frame>

PlanetPets is a simple app that users sign in to using their GitHub account. Afterward, users are prompted to give PlanetPets access to their PlanetScale account. The user's organizations are then presented as "gardens" where their databases are "trees." Within PlanetPets, users can water their "trees" to grow new branches, creating a new branch in their database with a randomly generated name.

If you want to test out the app, go to [planetpets.planetscale.vercel.app](https://planetpets.planetscale.vercel.app/).

If you want to read the code or set it up yourself, see the [PlanetPets GitHub repo](https://github.com/planetscale/planetpets).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Service tokens
Source: https://planetscale.com/docs/api/reference/service-tokens

Using service tokens with the PlanetScale API.

## Overview

This document will show you how to create a service token for the API in PlanetScale. All PlanetScale API endpoints require one. You can read more about using service tokens outside of the API in the [PlanetScale service token documentation](/docs/api/reference/service-tokens).

## Authorization header

To make requests to the API, add the service token values in the `Authorization` header in your HTTP API request using the following format:

<CodeGroup>
  ```text Text theme={null}
  Authorization: <SERVICE_TOKEN_ID>:<SERVICE_TOKEN>
  ```
</CodeGroup>

Here is a cURL example:

<CodeGroup>
  ```curl cURL theme={null}
  curl --request GET 'https://api.planetscale.com/v1/organizations' \
  --header 'Authorization: <SERVICE_TOKEN_ID>:<SERVICE_TOKEN>'
  ```
</CodeGroup>

## Creating a service token

To create a service token using the dashboard, log into your organization and click **Settings > Service tokens > New service token**.

Give the token a descriptive name (this is used for your reference only) and click **Create service token**.

### Service token ID

Copy this value to use as `SERVICE_TOKEN_ID`.

The ID is also visible on the service token page after you continue to token permissions.

### Service token

The token is generated immediately after the service token is created.

Copy this value to use as `SERVICE_TOKEN`.

<Frame caption="Modal showing service token ID and token secret">
  <img src="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/ef1a137-new-service-token.png?fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=4b456a7e112e18a02736a90f58483550" alt="Modal showing service token ID and token secret" data-og-width="972" width="972" data-og-height="716" height="716" data-path="docs/images/reference/ef1a137-new-service-token.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/ef1a137-new-service-token.png?w=280&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=4666dabba7991a52d18974fdbe25ea69 280w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/ef1a137-new-service-token.png?w=560&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=dc4722fd61a17d2c4a6e2660b97d5cf4 560w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/ef1a137-new-service-token.png?w=840&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=db59effe9f02014abd2e6197c70aa70d 840w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/ef1a137-new-service-token.png?w=1100&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=d1cf9382ee0a2de682b83c1f5324bb57 1100w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/ef1a137-new-service-token.png?w=1650&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=af9bcd45aef430db49f663a7735efd72 1650w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/ef1a137-new-service-token.png?w=2500&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=e4d6b05613260517d680b415c34b48a9 2500w" />
</Frame>

## Access permissions

You can access your specific service token page from your organization's **Settings > Service token** page.

Service tokens are configured with granular permissions for both organizations and databases access. On the page for your specific service token you can add one or many of the following permissions.

<Frame caption="Showing the UI for organization and database access with buttons to add what access permissions.">
  <img src="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/858081d-service-token-accesses.png?fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=6db7a4f4d768b0dde79c97ecb7ce96c0" alt="Showing the UI for organization and database access with buttons to add what access permissions" data-og-width="2080" width="2080" data-og-height="1528" height="1528" data-path="docs/images/reference/858081d-service-token-accesses.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/858081d-service-token-accesses.png?w=280&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=1735ad0f78eef3800dccdad6dbc40332 280w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/858081d-service-token-accesses.png?w=560&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=8d340df7870dc6470acd8d9ef8bd3d33 560w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/858081d-service-token-accesses.png?w=840&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=c58c4842cc025074895eca31b463eb4e 840w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/858081d-service-token-accesses.png?w=1100&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=2dbe5a2bef084f0a118ca617f2ce6cd0 1100w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/858081d-service-token-accesses.png?w=1650&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=63e187f68d3fe12c81af0da5d03a1d70 1650w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/858081d-service-token-accesses.png?w=2500&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=0f4054c2b5eaf611962bef647b303305 2500w" />
</Frame>

Please note that you may only add service token accesses that you are also authorized to do. For example, as an organization member, you can't create a service token with `create_databases` access.

### Organization access permissions

A service token can have granular permissions across an organization with one or multiple of the following access permissions.

Check the box next to each permission option you need to grant. Once you are done, click **Save permissions**.

| Access permissions        | Description                                                                 |
| :------------------------ | :-------------------------------------------------------------------------- |
| read\_organization        | Get information about an organization                                       |
| read\_invoices            | Get invoices for an organization                                            |
| read\_audit\_logs         | Get audit logs for an organization                                          |
| read\_databases           | Get information about an organization's databases                           |
| create\_databases         | Create organization databases                                               |
| delete\_databases         | Delete organization databases                                               |
| read\_oauth\_applications | Get information about an organization's OAuth applications and their tokens |
| read\_oauth\_tokens       | Get OAuth tokens for an OAuth application                                   |
| write\_oauth\_tokens      | Create and refresh OAuth grants and tokens for an OAuth application         |
| delete\_oauth\_tokens     | Delete OAuth tokens for an OAuth application                                |
| read\_service\_tokens     | Get information about service tokens                                        |
| write\_service\_tokens    | Create and update service tokens                                            |
| delete\_service\_tokens   | Delete service tokens                                                       |
| write\_teams              | Create and update teams                                                     |
| read\_metrics\_endpoints  | Get information about branch metrics endpoints                              |

### Database access permissions

A service token can have granular permissions across a database with one or multiple of the following access permissions.

Select the database you want to grant access for and check the box next to each permission option you need to grant. Once you are done, click **Save permissions**.

| Access permissions                   | Description                                         |
| :----------------------------------- | :-------------------------------------------------- |
| read\_database                       | Get information about a database                    |
| write\_database                      | Update information about a database                 |
| delete\_database                     | Delete a database                                   |
| create\_branch                       | Create a database branch                            |
| read\_branch                         | Read a database branch                              |
| delete\_branch                       | Delete a database branch                            |
| delete\_production\_branch           | Delete a production database branch                 |
| connect\_branch                      | Connect to a database branch                        |
| connect\_production\_branch          | Connect to a production database branch             |
| delete\_branch\_password             | Delete a password for a non-production branch       |
| delete\_production\_branch\_password | Delete a password for a production branch           |
| create\_deploy\_request              | Create a deploy request                             |
| read\_deploy\_request                | Read a deploy request                               |
| approve\_deploy\_request             | Approve a deploy request                            |
| create\_comment                      | Create a deploy request comment                     |
| read\_comment                        | Read a deploy request comment                       |
| read\_backups                        | List backups                                        |
| write\_backups                       | Create and update backups                           |
| delete\_backups                      | Delete development branch backups                   |
| delete\_production\_branch\_backups  | Delete production branch backups                    |
| restore\_backup                      | Restore a development branch backup                 |
| restore\_production\_branch\_backup  | Restore a production branch backup                  |
| write\_branch\_vschema               | Update the VSchema for a database branch            |
| write\_production\_branch\_vschema   | Update the VSchema for a production database branch |

<Tip>
  **Database creation with a service token**

  Service tokens are automatically granted full permissions to any database that they create.
</Tip>

## Service tokens and deploy requests approvals

When a database requires administrator approval for deploy requests (located in your database's Settings page), a service token cannot approve a deploy request created by the same service token. Also, users can't approve a deploy request created by a service token that they created.

## Example

### Creating a PlanetScale branch with service tokens

Creating a database branch is one of the many API endpoints documented in our [PlanetScale API docs](https://api.planetscale.com/reference/post_organizations_organization_databases_database_branches).

The following steps are required to make a successful API request:

<Steps>
  <Step>
    Create a service token as described at the top of this page. Make sure to copy and paste the service token ID and service token somewhere safe.
  </Step>

  <Step>
    You need to make sure your service token has the correct access permissions. Each endpoint documentation describes the service token access permissions it needs in the `Authorization` section. For example, in the create a branch endpoint you will need to grant the service token the `create_branch` access for your database:

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/675ebf8-H0I9wF2.png?fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=cba99b19a56badc7d3786a7115fa264a" alt="Service token access permissions for create_branch" data-og-width="1312" width="1312" data-og-height="688" height="688" data-path="docs/images/reference/675ebf8-H0I9wF2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/675ebf8-H0I9wF2.png?w=280&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=61b5c5f8d494db15a0c96536847c0498 280w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/675ebf8-H0I9wF2.png?w=560&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=3f504a12c5a60f3800a7030ee23dbe7a 560w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/675ebf8-H0I9wF2.png?w=840&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=231e6409e134e8625113bf3432a9f521 840w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/675ebf8-H0I9wF2.png?w=1100&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=6cd1be05525afdd1d034b1882ae14cf8 1100w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/675ebf8-H0I9wF2.png?w=1650&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=d7285d394d19509da8bcdecbf34d3d72 1650w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/reference/675ebf8-H0I9wF2.png?w=2500&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=934c314f2408a5b9971937d9e146ff72 2500w" />
    </Frame>
  </Step>

  <Step>
    After your permissions are set, you can now use the service token in a cURL request:

    <CodeGroup>
      ```curl cURL theme={null}
      curl --request POST \
           --url https://api.planetscale.com/v1/organizations/ORGANIZATION_NAME/databases/DATABASE_NAME/branches \
           --header 'Authorization: SERVICE_TOKEN_ID:SERVICE_TOKEN' \
           --header 'accept: application/json' \
           --header 'content-type: application/json'
           --data '{"name": "BRANCH_NAME","parent_branch": "PARENT_BRANCH_NAME"}'
      ```
    </CodeGroup>

    Make sure to fill in the `ORGANIZATION_NAME`, `DATABASE_NAME`, `BRANCH_NAME`, `PARENT_BRANCH_NAME`, `SERVICE_TOKEN_ID`, and `SERVICE_TOKEN` variables.
  </Step>

  <Step>
    You will get a response with the following data, as described in the specific API endpoints documentation:

    <CodeGroup>
      ```json JSON expandable theme={null}
      {
        "id": "xsvewamrebwz",
        "type": "Branch",
        "name": "new-feature",
        "created_at": "2023-01-10T23:31:01.573Z",
        "updated_at": "2023-01-10T23:31:01.573Z",
        "restore_checklist_completed_at": null,
        "access_host_url": "xsvewamrebwz.us-east-1.psdb.cloud",
        "schema_last_updated_at": "2023-01-10T23:31:01.573Z",
        "mysql_address": "us-east.connect.psdb.cloud",
        "mysql_provider_address": "aws.connect.psdb.cloud",
        "initial_restore_id": null,
        "mysql_edge_address": "aws.connect.psdb.cloud",
        "ready": false,
        "production": false,
        "sharded": false,
        "shard_count": 0,
        "actor": {
          "id": "g2dr4sbhz6ag",
          "type": "User",
          "display_name": "Taylor Barnett",
          "avatar_url": "https://www.gravatar.com/avatar/2a44999e8816311f19eea3d7516d9204?d=https%3A%2F%2Fapp.planetscale.com%2Fgravatar-fallback.png&s=64"
        },
        "restored_from_branch": null,
        "html_url": "https://app.planetscale.com/taylorhackyplace/example-test/new-feature",
        "url": "https://api.planetscale.com/v1/organizations/taylorhackyplace/databases/example-test/branches/new-feature",
        "region": {
          "id": "kc0e1ij8juzp",
          "type": "Region",
          "provider": "AWS",
          "enabled": true,
          "public_ip_addresses": [
            "23.23.187.137",
            "52.6.141.108",
            "52.70.2.89",
            "50.17.188.76",
            "52.2.251.189",
            "52.72.234.74",
            "35.174.68.24",
            "52.5.253.172",
            "54.156.81.4",
            "34.200.24.255",
            "35.174.79.154",
            "44.199.177.24"
          ],
          "display_name": "AWS us-east-1",
          "location": "Northern Virginia",
          "slug": "us-east"
        },
        "multiple_admins_required_for_demotion": false,
        "parent_branch": "main"
      }
      ```
    </CodeGroup>
  </Step>
</Steps>

### Potential errors

If you get a `{"code":"not_found","message":"Not Found"}` response, it is likely you either did not change the variables in the example cURL request or you did not set the access permissions correctly for the service token.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Skip revert period
Source: https://planetscale.com/docs/api/reference/skip_revert_period

post /organizations/{organization}/databases/{database}/deploy-requests/{number}/skip-revert
Skips the revert period for a deploy request
### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_deploy_request`, `create_deploy_request`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `deploy_deploy_requests` |
| Database | `deploy_deploy_requests` |



# Test a webhook
Source: https://planetscale.com/docs/api/reference/test_webhook

post /organizations/{organization}/databases/{database}/webhooks/{id}/test
Sends a test event to the webhook
### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `write_database`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `write_databases` |
| Database | `write_database` |



# Update auto-apply for deploy request
Source: https://planetscale.com/docs/api/reference/update_auto_apply

put /organizations/{organization}/databases/{database}/deploy-requests/{number}/auto-apply
Enables or disabled the auto-apply setting for a deploy request
### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_deploy_request`, `create_deploy_request`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `deploy_deploy_requests` |
| Database | `deploy_deploy_requests` |



# Update auto-delete branch for deploy request
Source: https://planetscale.com/docs/api/reference/update_auto_delete_branch

put /organizations/{organization}/databases/{database}/deploy-requests/{number}/auto-delete-branch
Enables or disabled the auto-delete branch setting for a deploy request
### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_deploy_request`, `create_deploy_request`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `deploy_deploy_requests` |
| Database | `deploy_deploy_requests` |



# Update a backup
Source: https://planetscale.com/docs/api/reference/update_backup

patch /organizations/{organization}/databases/{database}/branches/{branch}/backups/{id}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `write_backups`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `write_backups` |
| Database | `write_backups` |
| Branch | `write_backups` |



# Upsert a bouncer resize request
Source: https://planetscale.com/docs/api/reference/update_bouncer_resize_request

patch /organizations/{organization}/databases/{database}/branches/{branch}/bouncers/{bouncer}/resizes

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `write_database`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `write_databases` |
| Database | `write_database` |



# Upsert a change request
Source: https://planetscale.com/docs/api/reference/update_branch_change_request

patch /organizations/{organization}/databases/{database}/branches/{branch}/changes

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `write_database`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `write_databases` |
| Database | `write_database` |



# Change a branch cluster configuration
Source: https://planetscale.com/docs/api/reference/update_branch_cluster_config

patch /organizations/{organization}/databases/{database}/branches/{branch}/cluster

### Authorization
A service token   must have at least one of the following access   in order to use this API endpoint:

**Service Token Accesses**
 `write_database`





# Update an IP restriction entry
Source: https://planetscale.com/docs/api/reference/update_database_postgres_cidr

put /organizations/{organization}/databases/{database}/cidrs/{id}

### Authorization
A   OAuth token must have at least one of the following   scopes in order to use this API endpoint:

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `write_databases` |
| Database | `write_database` |



# Update database settings
Source: https://planetscale.com/docs/api/reference/update_database_settings

patch /organizations/{organization}/databases/{database}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `write_database`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `write_databases` |
| Database | `write_database` |



# Update database throttler configurations
Source: https://planetscale.com/docs/api/reference/update_database_throttler

patch /organizations/{organization}/databases/{database}/throttler

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_deploy_request`, `create_deploy_request`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `deploy_deploy_requests` |
| Database | `deploy_deploy_requests` |



# Update deploy request throttler configurations
Source: https://planetscale.com/docs/api/reference/update_deploy_request_throttler

patch /organizations/{organization}/databases/{database}/deploy-requests/{number}/throttler

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `read_deploy_request`, `create_deploy_request`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `deploy_deploy_requests` |
| Database | `deploy_deploy_requests` |



# Configure keyspace settings
Source: https://planetscale.com/docs/api/reference/update_keyspace

patch /organizations/{organization}/databases/{database}/branches/{branch}/keyspaces/{keyspace}

### Authorization
A service token   must have at least one of the following access   in order to use this API endpoint:

**Service Token Accesses**
 `create_branch`





# Update the VSchema for the keyspace
Source: https://planetscale.com/docs/api/reference/update_keyspace_vschema

patch /organizations/{organization}/databases/{database}/branches/{branch}/keyspaces/{keyspace}/vschema

### Authorization
A service token   must have at least one of the following access   in order to use this API endpoint:

**Service Token Accesses**
 `write_production_branch_vschema`, `write_branch_vschema`





# Update an organization
Source: https://planetscale.com/docs/api/reference/update_organization

patch /organizations/{organization}

### Authorization
A   OAuth token must have at least one of the following   scopes in order to use this API endpoint:

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `write_organization` |



# Update organization member role
Source: https://planetscale.com/docs/api/reference/update_organization_membership

patch /organizations/{organization}/members/{id}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `write_organization`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `write_organization` |



# Update an organization team
Source: https://planetscale.com/docs/api/reference/update_organization_team

patch /organizations/{organization}/teams/{team}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `write_teams`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `write_organization` |



# Update a password
Source: https://planetscale.com/docs/api/reference/update_password

patch /organizations/{organization}/databases/{database}/branches/{branch}/passwords/{id}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `connect_production_branch`, `connect_branch`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `manage_passwords`, `manage_production_branch_passwords` |
| Database | `manage_passwords`, `manage_production_branch_passwords` |
| Branch | `manage_passwords` |



# Update role name
Source: https://planetscale.com/docs/api/reference/update_role

patch /organizations/{organization}/databases/{database}/branches/{branch}/roles/{id}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `create_production_branch_password`, `create_branch_password`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `manage_passwords`, `manage_production_branch_passwords` |
| Database | `manage_passwords`, `manage_production_branch_passwords` |
| Branch | `manage_passwords` |



# Update a webhook
Source: https://planetscale.com/docs/api/reference/update_webhook

patch /organizations/{organization}/databases/{database}/webhooks/{id}

### Authorization
A service token or OAuth token must have at least one of the following access or scopes in order to use this API endpoint:

**Service Token Accesses**
 `write_database`

**OAuth Scopes**

 | Resource | Scopes |
| :------- | :---------- |
| Organization | `write_databases` |
| Database | `write_database` |



# Verify workflow data
Source: https://planetscale.com/docs/api/reference/verify_workflow

patch /organizations/{organization}/databases/{database}/workflows/{number}/verify-data





# Cancel a workflow
Source: https://planetscale.com/docs/api/reference/workflow_cancel

delete /organizations/{organization}/databases/{database}/workflows/{number}





# Complete a workflow
Source: https://planetscale.com/docs/api/reference/workflow_complete

patch /organizations/{organization}/databases/{database}/workflows/{number}/complete





# Cutover traffic
Source: https://planetscale.com/docs/api/reference/workflow_cutover

patch /organizations/{organization}/databases/{database}/workflows/{number}/cutover





# Retry a failed workflow
Source: https://planetscale.com/docs/api/reference/workflow_retry

patch /organizations/{organization}/databases/{database}/workflows/{number}/retry





# Reverse traffic cutover
Source: https://planetscale.com/docs/api/reference/workflow_reverse_cutover

patch /organizations/{organization}/databases/{database}/workflows/{number}/reverse-cutover





# Reverse traffic
Source: https://planetscale.com/docs/api/reference/workflow_reverse_traffic

patch /organizations/{organization}/databases/{database}/workflows/{number}/reverse-traffic





# Switch primary traffic
Source: https://planetscale.com/docs/api/reference/workflow_switch_primaries

patch /organizations/{organization}/databases/{database}/workflows/{number}/switch-primaries





# Switch replica traffic
Source: https://planetscale.com/docs/api/reference/workflow_switch_replicas

patch /organizations/{organization}/databases/{database}/workflows/{number}/switch-replicas





# Service tokens
Source: https://planetscale.com/docs/api/service-tokens

PlanetScale provides the ability to create service tokens for your PlanetScale organization via the CLI or directly in the UI.

## Overview

Service tokens provide an alternate authentication method to be used with the PlanetScale CLI and API. They are typically used in automated scenarios where `pscale auth login` cannot be used. Service tokens are also required for any calls to the API, as well as minting OAuth tokens for API use.

## Create service tokens using the PlanetScale dashboard

To create a service token using the dashboard, log into your organization, go to the [**"Settings"** > **"Service tokens"**](https://app.planetscale.com/~/settings/service-tokens) page, and click the **"New service token"** button.

Give the token a name (this is used for your reference only) and click **"Create service token"**.

The modal will update, displaying your service token where the Name field was. Copy the ID and token values as you'll need them moving forward. Click **"Edit token permissions"** to proceed.

<Tip>
  Be sure to copy the service token after you create it. There's no way to retrieve the token value once you leave this page.
</Tip>

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/modal-with-service-token-2.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=da8d0101c3e31f2e8dc9e0873464b242" alt="Service token detail page" data-og-width="1220" width="1220" data-og-height="972" height="972" data-path="docs/images/assets/docs/concepts/service-tokens/modal-with-service-token-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/modal-with-service-token-2.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=d307c6662a0d099300655c0d4fba6119 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/modal-with-service-token-2.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=5a3b5239d6579760ecbf917749228634 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/modal-with-service-token-2.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=c2bf5bed573d2cf628b15153366f2e4f 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/modal-with-service-token-2.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=d1620b1a55536a30a5b3d8869f11cffa 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/modal-with-service-token-2.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=65aa70f815baebc479dcaad3d750b75c 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/modal-with-service-token-2.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=85b6024c0ce9d0f57d3f6b4824cf23ae 2500w" />
</Frame>

## Assign service token permissions

Service tokens are configured with granular permissions, both for the organization that owns them as well as on a per database level. Before you can use a service token, these permissions must be added.

### Add organization permissions

Organization permissions are required when performing operations that are specific to the organization and not for an individual database. To enable a service token for performing these operations, locate the **Organization access** section and click **"Add organization permissions"**.

In the **Organization access permissions** modal, check the box next to each of the permission scopes that you want to assign to the token. Click **"Save permissions"** once finished.

For a full list of organization access permissions, see the [API documentation for service tokens](/docs/api/reference/service-tokens#organization-access-permissions).

### Add database permissions

In order to perform operations specific to a database, permissions can be assigned per-database. To do this, locate the section titled **Database access** and click **"Add database access"** to open the **Database access permissions** modal.

Select the database you want to grant access to and check the box next to each permission option you need to grant. Once you are done, click **"Save permissions"**.

For a full list of database access permissions, see the [API documentation for service tokens](/docs/api/reference/service-tokens#database-access-permissions).

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/db-access-permissions-2.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=8d11a26d102b1b857eaa29427594e7fa" alt="The Database access permissions modal." data-og-width="1173" width="1173" data-og-height="1236" height="1236" data-path="docs/images/assets/docs/concepts/service-tokens/db-access-permissions-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/db-access-permissions-2.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=080f35bbafec1334fad669c3f77721a7 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/db-access-permissions-2.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=6f914704dcf4e35efafca277f62d5a3d 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/db-access-permissions-2.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=5fce17d59a468dfabe4f431d5b74f0ec 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/db-access-permissions-2.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=c0f05ca24f1a071725da080d096f2943 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/db-access-permissions-2.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=7fb9723619bdff443c9dbc2e86e1b7a9 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/db-access-permissions-2.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=f96495c65cda038f8d9326509acbe6e7 2500w" />
</Frame>

### Add permissions for all databases

Organization admins can grant database permissions to all current and future databases. This is available in the **Permissions for all databases** section.
Any permission added to all databases will not be able to be disabled on an individual basis.

## Service tokens and deploy requests approvals

When a database requires administrator approval for deploy requests (located in your database's **Settings** page), a service token cannot approve a deploy request created by the same service token. Also, users can't approve a deploy request created by a service token that they created.

## Use a service token with the PlanetScale CLI

To use service tokens with the PlanetScale CLI, set the following environment variables in your terminal:

```bash  theme={null}
export PLANETSCALE_SERVICE_TOKEN=<YOUR_SERVICE_TOKEN>
export PLANETSCALE_SERVICE_TOKEN_ID=<YOUR_SERVICE_TOKEN_ID>
```

When you execute commands using the PlanetScale CLI, it will automatically parse those values and use them to access the service. However, you’ll also need to pass in your organization name using the `--org` flag like so:

```bash  theme={null}
pscale branch create <DB_NAME> <BRANCH_NAME> --org <ORG_NAME>
```

If you don’t want to set environment variables, you may also pass in the Service Token and Service Token ID by using the [`--service-token` and `--service-token-id` flags](/docs/cli/service-token) respectively:

```bash  theme={null}
pscale branch create <DB_NAME> <BRANCH_NAME> --org <ORG_NAME> --service-token <SERVICE_TOKEN> --service-token-id <SERVICE_TOKEN_ID>
```

## Use a service token with the PlanetScale API

In order to execute a request to the PlanetScale API, you'll need a service token to execute requests directly or for minting OAuth tokens. Both the ID and token are required in the `Authorization` header without a scheme. Below is an example of how to use a service token to list details about the organizations the token can access:

```bash  theme={null}
curl --request GET \
     --url 'https://api.planetscale.com/v1/organizations' \
     --header 'Authorization: <SERVICE_TOKEN_ID>:<SERVICE_TOKEN>'
```

Refer to the [API docs](/docs/api/reference/getting-started-with-planetscale-api) for more details on how to use the API.

## Modify service token permissions

If you want to modify the permissions granted to a service token, start by opening the service token from the settings pane. Select the three dots next to the organization or database name permissions you want to modify and click **"Edit permissions"**.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/edit-org-perms-2.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=9ba825a47cb35891a186806c9d5cbd3e" alt="The location of the Edit permissions option for organization permissions." data-og-width="1353" width="1353" data-og-height="1012" height="1012" data-path="docs/images/assets/docs/concepts/service-tokens/edit-org-perms-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/edit-org-perms-2.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=c047e281d07d18af97d50216217484c2 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/edit-org-perms-2.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=50b01ecf30eb3aeac46074619ea13b10 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/edit-org-perms-2.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=e2d2f4ee4a206eff6c0ac0976ae4dfc2 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/edit-org-perms-2.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=dcdf0f7a27746655a0a034037a426e66 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/edit-org-perms-2.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=bd099020352d78798ff411329e3160e7 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/edit-org-perms-2.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=43c9817b950458e3ff03915c1ec50332 2500w" />
</Frame>

This will open a modal that allows you to modify the permissions the service token has to access that organization.

## Delete a service token

You can delete a service token at any time from the service token detail page. Simply click the **"Delete service token"** button.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/delete-service-token-2.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=bf4de5be09851bc2dfee8b464c18de6a" alt="Delete service token." data-og-width="1361" width="1361" data-og-height="337" height="337" data-path="docs/images/assets/docs/concepts/service-tokens/delete-service-token-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/delete-service-token-2.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=2ec3db41858b06d6c2f28c502ed86247 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/delete-service-token-2.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=00087d4968eecd5f1d379809edda3bb7 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/delete-service-token-2.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=b000c1a21326ca33ac3af6390d347ad1 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/delete-service-token-2.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=3cf45a3ef6c25f59146dfe96ebc1f9cf 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/delete-service-token-2.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=11faf6e93e8992e6bdf072826acaf59b 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/delete-service-token-2.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=9f37a9e22b1144b3319c8933f786d11d 2500w" />
</Frame>

## Manage service tokens using the PlanetScale CLI

Service tokens can also be created and managed directly from the [PlanetScale CLI](/docs/cli/service-token).

### Create a new service token

Use the following command to create a service token:

```bash  theme={null}
pscale service-token create
```

This command will return a service token ID and value for your use.

### Add database access permissions

You can add database access permissions to your service token for each database in your organization.

To add database access permissions, use the command:

```bash  theme={null}
pscale service-token add-access <SERVICE_TOKEN_ID> <ACCESS_PERMISSION> --database <DB_NAME>
```

For example, to give a service token the ability to create, read, and delete branches on a specific database, use the following command:

```bash  theme={null}
pscale service-token add-access <SERVICE_TOKEN_ID> read_branch delete_branch create_branch --database <DB_NAME>
```

A complete list of service token access permissions can be found in the [PlanetScale API documentation](/docs/api/reference/service-tokens#access-permissions).

### Remove database access permissions

You can also remove database access permissions for a service token.

Use the following command to remove one or more permissions:

```bash  theme={null}
pscale service-token delete-access <SERVICE_TOKEN_ID> <ACCESS_PERMISSION> --database <DB_NAME>
```

### Delete a service token

To delete a service token, run the following command:

```bash  theme={null}
pscale service-token delete <SERVICE_TOKEN_ID>
```

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Webhook events reference
Source: https://planetscale.com/docs/api/webhook-events

Webhooks in PlanetScale allow you to trigger an HTTP POST callback to a configured URL when specific events happen within your PlanetScale organization.

## Overview

Webhooks can be used to build integrations, such as notifications, and automate workflows. See the [webhooks documentation](/docs/api/webhooks) for more information.

## Webhook headers

All webhooks from PlanetScale will have an `X-PlanetScale-Signature` header. This header is a SHA-256 HMAC hex digest of the request body, using your webhook secret as the key. You can use this header to verify that the webhook payload was sent by PlanetScale. See the documentation on [validating a webhook signature](/docs/api/webhooks#validating-a-webhook-signature) for more information.

## Webhook request body parameters

| Parameter   | Type    | Description                                                                                                              |
| :---------- | :------ | :----------------------------------------------------------------------------------------------------------------------- |
| `timestamp` | integer | Unix epoch time.                                                                                                         |
| `event`     | string  | Name of the webhook event, see [webhook events](#webhook-events) for more info about each event.                         |
| `resource`  | object  | Information about the event, uses the same response body as the API responses for the same resource. See examples below. |

## Webhook events

| Webhook event                                                     | `event` parameter                | Trigger                                                                        |
| :---------------------------------------------------------------- | :------------------------------- | :----------------------------------------------------------------------------- |
| [Branch anomaly](#branch-anomaly)                                 | `branch.anomaly`                 | The branch has a new anomaly in insights.                                      |
| [Branch primary promoted](#branch-primary-promoted)               | `branch.primary_promoted`        | A new primary has been promoted for the branch.                                |
| [Branch ready](#branch-ready)                                     | `branch.ready`                   | The branch is created and ready to connect.                                    |
| [Branch sleeping](#branch-sleeping)                               | `branch.sleeping`                | The branch is now sleeping.                                                    |
| [Branch start maintenance](#branch-start-maintenance)             | `branch.start_maintenance`       | A production branch is about to start maintenance.                             |
| [Cluster storage](#cluster-storage)                               | `cluster.storage`                | A Postgres database has reached a storage threshold (60%, 75%, 85%, 90%, 95%). |
| [Database access request](#database-access-request)               | `database.access_request`        | PlanetScale staff opens an access request.                                     |
| [Deploy request opened](#deploy-request-opened)                   | `deploy_request.opened`          | The deploy request has been opened.                                            |
| [Deploy request queued](#deploy-request-queued)                   | `deploy_request.queued`          | The deploy request has been added to the deploy queue.                         |
| [Deploy request in progress](#deploy-request-in-progress)         | `deploy_request.in_progress`     | The deploy request has started running.                                        |
| [Deploy request pending cutover](#deploy-request-pending-cutover) | `deploy_request.pending_cutover` | The deploy request is ready to cutover and waiting on the user.                |
| [Deploy request schema applied](#deploy-request-schema-applied)   | `deploy_request.schema_applied`  | The deploy request has finished applying the schema.                           |
| [Deploy request errored](#deploy-request-errored)                 | `deploy_request.errored`         | The deploy request has stopped due to an error.                                |
| [Deploy request reverted](#deploy-request-reverted)               | `deploy_request.reverted`        | The deploy request has been reverted.                                          |
| [Deploy request closed](#deploy-request-closed)                   | `deploy_request.closed`          | The deploy request has been closed.                                            |
| [Keyspace storage](#keyspace-storage)                             | `keyspace.storage`               | A keyspace has reached a storage threshold (60%, 75%, 85%, 90%, 95%).          |
| [Webhook test](#webhook-test)                                     | `webhook.test`                   | A webhook test is triggered.                                                   |

<Note>
  If there is an event you want to use that is not included in this list, please [contact us](https://planetscale.com/contact) and let us know what event you want to trigger a webhook on.
</Note>

### Branch anomaly

The branch has a new anomaly event in PlanetScale Insights.

The `branch.anomaly` event uses the same response body as a `200` response from the [Get a branch](/docs/api/reference/get_branch) API endpoint. The link includes a detailed description of each field in the API reference.

**Example:**

```json expandable theme={null}
{
  "timestamp": 1698252879,
  "event": "branch.anomaly",
  "organization": "myorg",
  "database": "example_database",
  "resource": {
    "id": "ecrmjy2f4a5o",
    "type": "Branch",
    "name": "dev",
    "created_at": "2023-10-25T16:54:12.879Z",
    "updated_at": "2023-10-25T16:54:39.820Z",
    "restore_checklist_completed_at": null,
    "access_host_url": "ecrmjy2f4a5o.us-east-4.psdb.cloud",
    "schema_last_updated_at": "2023-10-25T16:54:12.879Z",
    "mysql_address": "us-east.connect.psdb.cloud",
    "mysql_provider_address": "aws.connect.psdb.cloud",
    "initial_restore_id": null,
    "schema_ready": true,
    "state": "ready",
    "cluster_name": "PS_DEV",
    "mysql_edge_address": "aws.connect.psdb.cloud",
    "ready": true,
    "production": false,
    "safe_migrations": false,
    "sharded": false,
    "shard_count": 0,
    "stale_schema": false,
    "actor": {
      "id": "g2dr4sbhz6ag",
      "type": "User",
      "display_name": "PlanetScale Bot",
      "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
    },
    "restored_from_branch": null,
    "private_connectivity": false,
    "private_edge_connectivity": false,
    "html_url": "https://app.planetscale.com/demo-db/example_database/dev",
    "url": "https://api.planetscale.com/v1/organizations/demo-db/databases/example_database/branches/dev",
    "region": {
      "id": "kc0e1ij8juzp",
      "type": "Region",
      "provider": "AWS",
      "enabled": true,
      "public_ip_addresses": [
        "23.23.187.137",
        "52.6.141.108",
        "52.70.2.89",
        "50.17.188.76",
        "52.2.251.189",
        "52.72.234.74",
        "35.174.68.24",
        "52.5.253.172",
        "54.156.81.4",
        "34.200.24.255",
        "35.174.79.154",
        "44.199.177.24",
        "35.173.174.19",
        "44.212.228.57",
        "44.216.88.45"
      ],
      "display_name": "AWS us-east-1",
      "location": "N. Virginia",
      "slug": "us-east",
      "current_default": true
    },
    "parent_branch": "main"
  }
}
```

### Branch primary promoted

A new primary has been promoted for the Postgres database.

The `branch.primary_promoted` event uses the same response body as a `200` response from the [Get a branch](/docs/api/reference/get_branch) API endpoint. The link includes a detailed description of each field in the API reference.

**Example:**

```json expandable theme={null}
{
  "timestamp": 1698252879,
  "event": "branch.primary_promoted",
  "organization": "myorg",
  "database": "example_database",
  "resource": {
    "id": "q9x7ylb3hcmx",
    "type": "Branch",
    "name": "main",
    "cluster_architecture": "x86_64",
    "cluster_name": "PS_20_AWS_X86",
    "cluster_display_name": "PS-20",
    "cluster_iops": null,
    "restore_checklist_completed_at": null,
    "schema_last_updated_at": "2025-09-17T20:49:14.983Z",
    "state": "ready",
    "created_at": "2025-07-15T12:38:22.664Z",
    "updated_at": "2025-09-24T21:00:35.717Z",
    "actor": {
      "id": "u2dr4sbhz6ag",
      "type": "User",
      "display_name": "Mike Coutermarsh",
      "avatar_url": "https://www.gravatar.com/avatar/236f8e1435745283319e2bcfeed75072?d=https%3A%2F%2Fapp.planetscale.com%2Fgravatar-fallback.png&s=64"
    },
    "restored_from_branch": null,
    "frozen": true,
    "safe_migrations": false,
    "default": true,
    "enabling_vectors": false,
    "metal": false,
    "production": true,
    "ready": true,
    "stale_schema": false,
    "parent_branch": null,
    "private_edge_connectivity": false,
    "replicas": 2,
    "read_only_reason": null,
    "read_only_at": null,
    "minimum_storage_bytes": 21474836480,
    "maximum_storage_bytes": 3324304687104,
    "storage_autoscaling": true,
    "storage_shrinking": true,
    "storage_type": "gp3",
    "storage_iops": 4000,
    "storage_throughput_mibs": 300,
    "has_replicas": true,
    "has_read_only_replicas": false,
    "has_roles": true,
    "region": {
      "id": "ri0pbcmdkjsh",
      "type": "Region",
      "provider": "AWS",
      "enabled": true,
      "public_ip_addresses": [
        "18.117.23.127",
        "3.131.243.164",
        "3.132.168.252",
        "3.131.252.213",
        "3.132.182.173",
        "3.15.49.114",
        "3.209.149.66",
        "3.215.97.46",
        "34.193.111.15"
      ],
      "display_name": "AWS us-east-2",
      "location": "Ohio",
      "slug": "aws-us-east-2",
      "current_default": false
    },
    "kind": "postgresql"
  }
}
```

### Branch ready

The branch is created and ready to connect.

The `branch.ready` event uses the same response body as a `200` response from the [Get a branch](/docs/api/reference/get_branch) API endpoint. The link includes a detailed description of each field in the API reference.

**Example:**

```json expandable theme={null}
{
  "timestamp": 1698252879,
  "event": "branch.ready",
  "organization": "myorg",
  "database": "example_database",
  "resource": {
    "id": "ecrmjy2f4a5o",
    "type": "Branch",
    "name": "dev",
    "created_at": "2023-10-25T16:54:12.879Z",
    "updated_at": "2023-10-25T16:54:39.820Z",
    "restore_checklist_completed_at": null,
    "access_host_url": "ecrmjy2f4a5o.us-east-4.psdb.cloud",
    "schema_last_updated_at": "2023-10-25T16:54:12.879Z",
    "mysql_address": "us-east.connect.psdb.cloud",
    "mysql_provider_address": "aws.connect.psdb.cloud",
    "initial_restore_id": null,
    "schema_ready": true,
    "state": "ready",
    "cluster_name": "PS_DEV",
    "mysql_edge_address": "aws.connect.psdb.cloud",
    "ready": true,
    "production": false,
    "safe_migrations": false,
    "sharded": false,
    "shard_count": 0,
    "stale_schema": false,
    "actor": {
      "id": "g2dr4sbhz6ag",
      "type": "User",
      "display_name": "PlanetScale Bot",
      "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
    },
    "restored_from_branch": null,
    "private_connectivity": false,
    "private_edge_connectivity": false,
    "html_url": "https://app.planetscale.com/demo-db/example_database/dev",
    "url": "https://api.planetscale.com/v1/organizations/demo-db/databases/example_database/branches/dev",
    "region": {
      "id": "kc0e1ij8juzp",
      "type": "Region",
      "provider": "AWS",
      "enabled": true,
      "public_ip_addresses": [
        "23.23.187.137",
        "52.6.141.108",
        "52.70.2.89",
        "50.17.188.76",
        "52.2.251.189",
        "52.72.234.74",
        "35.174.68.24",
        "52.5.253.172",
        "54.156.81.4",
        "34.200.24.255",
        "35.174.79.154",
        "44.199.177.24",
        "35.173.174.19",
        "44.212.228.57",
        "44.216.88.45"
      ],
      "display_name": "AWS us-east-1",
      "location": "N. Virginia",
      "slug": "us-east",
      "current_default": true
    },
    "parent_branch": "main"
  }
}
```

### Branch sleeping

The branch is now sleeping.

The `branch.sleeping` event uses the same response body as a `200` response from the [Get a branch](/docs/api/reference/get_branch) API endpoint. The link includes a detailed description of each field in the API reference.

**Example:**

```json expandable theme={null}
{
  "timestamp": 1697739653,
  "event": "branch.sleeping",
  "organization": "myorg",
  "database": "example_database",
  "resource": {
    "id": "bffzv8jfk9gc",
    "type": "Branch",
    "name": "dev",
    "created_at": "2023-10-14T18:17:42.998Z",
    "updated_at": "2023-10-19T18:17:43.105Z",
    "restore_checklist_completed_at": null,
    "access_host_url": "bffzv8jfk9gc.us-east-4.psdb.cloud",
    "schema_last_updated_at": "2023-10-19T18:07:57.623Z",
    "mysql_address": "us-east.connect.psdb.cloud",
    "mysql_provider_address": "aws.connect.psdb.cloud",
    "initial_restore_id": null,
    "schema_ready": true,
    "state": "ready",
    "cluster_name": "PS_DEV",
    "mysql_edge_address": "aws.connect.psdb.cloud",
    "ready": true,
    "production": false,
    "safe_migrations": false,
    "sharded": false,
    "shard_count": 0,
    "stale_schema": false,
    "actor": {
      "id": "g2dr4sbhz6ag",
      "type": "User",
      "display_name": "PlanetScale Bot",
      "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
    },
    "restored_from_branch": null,
    "private_connectivity": false,
    "private_edge_connectivity": false,
    "html_url": "https://app.planetscale.com/demo-db/example_database/dev",
    "url": "https://api.planetscale.com/v1/organizations/demo-db/databases/example_database/branches/dev",
    "region": {
      "id": "kc0e1ij8juzp",
      "type": "Region",
      "provider": "AWS",
      "enabled": true,
      "public_ip_addresses": [
        "23.23.187.137",
        "52.6.141.108",
        "52.70.2.89",
        "50.17.188.76",
        "52.2.251.189",
        "52.72.234.74",
        "35.174.68.24",
        "52.5.253.172",
        "54.156.81.4",
        "34.200.24.255",
        "35.174.79.154",
        "44.199.177.24",
        "35.173.174.19",
        "44.212.228.57",
        "44.216.88.45"
      ],
      "display_name": "AWS us-east-1",
      "location": "N. Virginia",
      "slug": "us-east",
      "current_default": true
    },
    "parent_branch": "main"
  }
}
```

### Branch start maintenance

A production branch is about to start maintenance. This webhook is available for both Vitess and Postgres databases, with different behaviors:

<Tabs>
  <Tab title="Vitess">
    Vitess maintenance webhooks are available for enterprise customers with maintenance windows configured. During maintenance, Vitess and security upgrades are rolled out. Any queued cluster configuration changes are applied.
  </Tab>

  <Tab title="Postgres">
    Postgres maintenance webhooks are available for all customers. During maintenance, security patches and upgrades are rolled out. Your database cluster might restart, causing connections to be temporarily disconnected. See the [Postgres maintenance windows documentation](/docs/postgres/cluster-configuration/maintenance-windows) for more information.
  </Tab>
</Tabs>

By subscribing to this webhook, you will receive a notification right before maintenance for your production branch is about to start.

The `branch.start_maintenance` event uses the same response body as a `200` response from the [Get a branch](/docs/api/reference/get_branch) API endpoint.

**Example:**

```json expandable theme={null}
{
  "timestamp": 1698252879,
  "event": "branch.start_maintenance",
  "organization": "myorg",
  "database": "example_database",
  "resource": {
    "id": "q9x7ylb3hcmx",
    "type": "Branch",
    "name": "main",
    "cluster_architecture": "x86_64",
    "cluster_name": "PS_20_AWS_X86",
    "cluster_display_name": "PS-20",
    "cluster_iops": null,
    "restore_checklist_completed_at": null,
    "schema_last_updated_at": "2025-09-17T20:49:14.983Z",
    "state": "ready",
    "created_at": "2025-07-15T12:38:22.664Z",
    "updated_at": "2025-09-24T21:00:35.717Z",
    "actor": {
      "id": "u2dr4sbhz6ag",
      "type": "User",
      "display_name": "Mike Coutermarsh",
      "avatar_url": "https://www.gravatar.com/avatar/236f8e1435745283319e2bcfeed75072?d=https%3A%2F%2Fapp.planetscale.com%2Fgravatar-fallback.png&s=64"
    },
    "restored_from_branch": null,
    "frozen": true,
    "safe_migrations": false,
    "default": true,
    "enabling_vectors": false,
    "metal": false,
    "production": true,
    "ready": true,
    "stale_schema": false,
    "parent_branch": null,
    "private_edge_connectivity": false,
    "replicas": 2,
    "read_only_reason": null,
    "read_only_at": null,
    "minimum_storage_bytes": 21474836480,
    "maximum_storage_bytes": 3324304687104,
    "storage_autoscaling": true,
    "storage_shrinking": true,
    "storage_type": "gp3",
    "storage_iops": 4000,
    "storage_throughput_mibs": 300,
    "has_replicas": true,
    "has_read_only_replicas": false,
    "has_roles": true,
    "region": {
      "id": "ri0pbcmdkjsh",
      "type": "Region",
      "provider": "AWS",
      "enabled": true,
      "public_ip_addresses": [
        "18.117.23.127",
        "3.131.243.164",
        "3.132.168.252",
        "3.131.252.213",
        "3.132.182.173",
        "3.15.49.114",
        "3.209.149.66",
        "3.215.97.46",
        "34.193.111.15"
      ],
      "display_name": "AWS us-east-2",
      "location": "Ohio",
      "slug": "aws-us-east-2",
      "current_default": false
    },
    "kind": "postgresql"
  }
}
```

### Cluster storage

A storage threshold (60%, 75%, 85%, 90%, 95%) has been crossed for a Postgres database.

```json  theme={null}
{
  "timestamp": 1745526125,
  "event": "cluster.storage",
  "organization": "myorg",
  "database": "example_database",
  "resource": {
    "branch_name": "main",
    "storage_percentage": 44,
    "shards": [
      {
        "bytes_used": 208470605824,
        "bytes_total_capacity": 472424366080,
        "bytes_available": 263953760256,
        "storage_percentage": 44.1
      }
    ]
  }
}
```

### Database access request

PlanetScale staff opens an access request to a database.

The `database.access_request` event is triggered when PlanetScale staff initiates an access request to your organization for support purposes.

**Example:**

```json expandable theme={null}
{
  "timestamp": 1745617110,
  "event": "database.access_request",
  "organization": "planetscale",
  "database": "planetscale",
  "resource": {
    "type": "PlanetScaleAdminAccessRequest",
    "id": "23wl4nynt06x",
    "actor": {
      "id": "abc123",
      "type": "User",
      "display_name": "Mike Coutermarsh",
      "avatar_url": "https://www.gravatar.com/avatar/236f8e1435745283319e2bcfeed75072?d=https%3A%2F%2Fapp.planetscale.com%2Fgravatar-fallback.png&s=64"
    },
    "revoked_by": null,
    "responder": null,
    "access_target": {
      "id": "abc123",
      "type": "Organization",
      "name": "planetscale",
      "created_at": "2021-02-17T20:03:56.393Z",
      "deleted_at": null,
      "updated_at": "2023-06-02T14:13:01.679Z"
    },
    "organization": {
      "id": "abc123",
      "type": "Organization",
      "name": "planetscale",
      "created_at": "2021-02-17T20:03:56.393Z",
      "deleted_at": null,
      "updated_at": "2023-06-02T14:13:01.679Z"
    },
    "state": "pending",
    "access_reason": "This is a test.",
    "denial_reason": null,
    "responded_at": null,
    "created_at": "2025-04-25T21:38:30.088Z",
    "cancelled_at": null,
    "expires_at": null,
    "duration": 24,
    "duration_units": "hour"
  }
}
```

### Deploy request opened

The deploy request has been opened.

The `deploy_request.opened` event uses the same response body as a `200` response from the [Get a deploy request](/docs/api/reference/get_deploy_request) API endpoint. The link includes a detailed description of each field in the API reference.

**Example:**

```json expandable theme={null}
{
  "timestamp": 1698252899,
  "event": "deploy_request.opened",
  "organization": "myorg",
  "database": "example_database",
  "resource": {
    "id": "4xsz0ql82y4n",
    "type": "DeployRequest",
    "actor": {
      "id": "g2dr4sbhz6ag",
      "type": "User",
      "display_name": "PlanetScale Bot",
      "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
    },
    "closed_by": null,
    "branch": "dev",
    "branch_deleted": false,
    "branch_deleted_by": null,
    "branch_deleted_at": null,
    "into_branch": "main",
    "into_branch_sharded": false,
    "into_branch_shard_count": 0,
    "approved": false,
    "state": "open",
    "deployment_state": "pending",
    "num_comments": 0,
    "deployment": {
      "id": "uvkd7injje2f",
      "type": "Deployment",
      "into_branch": "main",
      "deploy_request_number": 5,
      "actor": {
        "id": "boo",
        "type": "User",
        "display_name": "Ghost",
        "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
      },
      "cutover_actor": null,
      "cancelled_actor": null,
      "schema_last_updated_at": "2023-10-25T16:54:59.728Z",
      "preceding_deployments": [],
      "deploy_operations": [],
      "deploy_operation_summaries": [],
      "deployable": false,
      "cutover_expiring": false,
      "lint_errors": [],
      "deployment_revert_request": null,
      "auto_cutover": true,
      "created_at": "2023-10-25T16:54:59.863Z",
      "cutover_at": null,
      "deploy_check_errors": null,
      "finished_at": null,
      "queued_at": null,
      "ready_to_cutover_at": null,
      "started_at": null,
      "state": "pending",
      "submitted_at": null,
      "updated_at": "2023-10-25T16:54:59.863Z"
    },
    "html_url": "https://app.planetscale.com/demo-db/example_database/deploy-requests/5",
    "number": 5,
    "notes": "",
    "html_body": "",
    "created_at": "2023-10-25T16:54:59.797Z",
    "updated_at": "2023-10-25T16:54:59.797Z",
    "closed_at": null,
    "deployed_at": null
  }
}
```

### Deploy request queued

The deploy request has been added to the deploy queue.

The `deploy_request.queued` event uses the same response body as a `200` response from the [Get a deploy request](/docs/api/reference/get_deploy_request) API endpoint. The link includes a detailed description of each field in the API reference.

**Example:**

```json expandable theme={null}
{
  "timestamp": 1698252953,
  "event": "deploy_request.queued",
  "organization": "myorg",
  "database": "example_database",
  "resource": {
    "id": "4xsz0ql82y4n",
    "type": "DeployRequest",
    "actor": {
      "id": "g2dr4sbhz6ag",
      "type": "User",
      "display_name": "PlanetScale Bot",
      "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
    },
    "closed_by": null,
    "branch": "dev",
    "branch_deleted": false,
    "branch_deleted_by": null,
    "branch_deleted_at": null,
    "into_branch": "main",
    "into_branch_sharded": false,
    "into_branch_shard_count": 0,
    "approved": false,
    "state": "open",
    "deployment_state": "queued",
    "num_comments": 0,
    "deployment": {
      "id": "uvkd7injje2f",
      "type": "Deployment",
      "into_branch": "main",
      "deploy_request_number": 5,
      "actor": {
        "id": "g2dr4sbhz6ag",
        "type": "User",
        "display_name": "PlanetScale Bot",
        "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
      },
      "cutover_actor": null,
      "cancelled_actor": null,
      "schema_last_updated_at": "2023-10-25T16:55:00.288Z",
      "preceding_deployments": [],
      "deploy_operations": [
        {
          "id": "wo1m619ufrpc",
          "type": "DeployOperation",
          "state": "pending",
          "keyspace_name": "example_database",
          "table_name": "Persons",
          "operation_name": "ALTER",
          "eta_seconds": null,
          "progress_percentage": null,
          "deploy_error_docs_url": null,
          "ddl_statement": "ALTER TABLE `Persons` DROP COLUMN `Address`",
          "syntax_highlighted_ddl": "<div class=\"line line-1\"><span class=\"k\">ALTER</span> <span class=\"k\">TABLE</span> <span class=\"nv\">`Persons`</span> </div><div class=\"line line-2\">  <span class=\"k\">DROP</span> <span class=\"k\">COLUMN</span> <span class=\"nv\">`Address`</span></div>",
          "created_at": "2023-10-25T16:55:26.562Z",
          "updated_at": "2023-10-25T16:55:26.562Z",
          "can_drop_data": true,
          "table_recently_used": false,
          "table_recently_used_at": null,
          "deploy_errors": null
        }
      ],
      "deploy_operation_summaries": [],
      "deployable": true,
      "cutover_expiring": false,
      "lint_errors": [],
      "deployment_revert_request": null,
      "auto_cutover": false,
      "created_at": "2023-10-25T16:54:59.863Z",
      "cutover_at": null,
      "deploy_check_errors": null,
      "finished_at": null,
      "queued_at": "2023-10-25T16:55:53.543Z",
      "ready_to_cutover_at": null,
      "started_at": null,
      "state": "queued",
      "submitted_at": null,
      "updated_at": "2023-10-25T16:55:53.552Z"
    },
    "html_url": "https://app.planetscale.com/demo-db/example_database/deploy-requests/5",
    "number": 5,
    "notes": "",
    "html_body": "",
    "created_at": "2023-10-25T16:54:59.797Z",
    "updated_at": "2023-10-25T16:55:53.556Z",
    "closed_at": null,
    "deployed_at": null
  }
}
```

### Deploy request in progress

The deploy request has started running.

The `deploy_request_in_progress` event uses the same response body as a `200` response from the [Get a deploy request](/docs/api/reference/get_deploy_request) API endpoint. The link includes a detailed description of each field in the API reference.

**Example:**

```json expandable theme={null}
{
  "timestamp": 1698252961,
  "event": "deploy_request.in_progress",
  "organization": "myorg",
  "database": "example_database",
  "resource": {
    "id": "4xsz0ql82y4n",
    "type": "DeployRequest",
    "actor": {
      "id": "g2dr4sbhz6ag",
      "type": "User",
      "display_name": "PlanetScale Bot",
      "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
    },
    "closed_by": null,
    "branch": "dev",
    "branch_deleted": false,
    "branch_deleted_by": null,
    "branch_deleted_at": null,
    "into_branch": "main",
    "into_branch_sharded": false,
    "into_branch_shard_count": 0,
    "approved": false,
    "state": "open",
    "deployment_state": "in_progress",
    "num_comments": 0,
    "deployment": {
      "id": "uvkd7injje2f",
      "type": "Deployment",
      "into_branch": "main",
      "deploy_request_number": 5,
      "actor": {
        "id": "g2dr4sbhz6ag",
        "type": "User",
        "display_name": "PlanetScale Bot",
        "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
      },
      "cutover_actor": null,
      "cancelled_actor": null,
      "schema_last_updated_at": "2023-10-25T16:55:00.288Z",
      "preceding_deployments": [],
      "deploy_operations": [
        {
          "id": "wo1m619ufrpc",
          "type": "DeployOperation",
          "state": "pending",
          "keyspace_name": "example_database",
          "table_name": "Persons",
          "operation_name": "ALTER",
          "eta_seconds": null,
          "progress_percentage": null,
          "deploy_error_docs_url": null,
          "ddl_statement": "ALTER TABLE `Persons` DROP COLUMN `Address`",
          "syntax_highlighted_ddl": "<div class=\"line line-1\"><span class=\"k\">ALTER</span> <span class=\"k\">TABLE</span> <span class=\"nv\">`Persons`</span> </div><div class=\"line line-2\">  <span class=\"k\">DROP</span> <span class=\"k\">COLUMN</span> <span class=\"nv\">`Address`</span></div>",
          "created_at": "2023-10-25T16:55:26.562Z",
          "updated_at": "2023-10-25T16:55:26.562Z",
          "can_drop_data": true,
          "table_recently_used": false,
          "table_recently_used_at": null,
          "deploy_errors": null
        }
      ],
      "deploy_operation_summaries": [],
      "deployable": true,
      "cutover_expiring": false,
      "lint_errors": [],
      "deployment_revert_request": null,
      "auto_cutover": false,
      "created_at": "2023-10-25T16:54:59.863Z",
      "cutover_at": null,
      "deploy_check_errors": null,
      "finished_at": null,
      "queued_at": "2023-10-25T16:55:53.543Z",
      "ready_to_cutover_at": null,
      "started_at": "2023-10-25T16:56:01.035Z",
      "state": "in_progress",
      "submitted_at": "2023-10-25T16:55:53.702Z",
      "updated_at": "2023-10-25T16:56:01.057Z"
    },
    "html_url": "https://app.planetscale.com/demo-db/example_database/deploy-requests/5",
    "number": 5,
    "notes": "",
    "html_body": "",
    "created_at": "2023-10-25T16:54:59.797Z",
    "updated_at": "2023-10-25T16:56:01.063Z",
    "closed_at": null,
    "deployed_at": null
  }
}
```

### Deploy request errored

The deploy request has stopped due to an error.

The `deploy_request.errored` event uses the same response body as a `200` response from the [Get a deploy request](/docs/api/reference/get_deploy_request) API endpoint. The link includes a detailed description of each field in the API reference.

**Example:**

```json expandable theme={null}
{
  "timestamp": 1697736651,
  "event": "deploy_request.errored",
  "organization": "myorg",
  "database": "example_database",
  "resource": {
    "id": "jwdnj3q31jd6",
    "type": "DeployRequest",
    "actor": {
      "id": "g2dr4sbhz6ag",
      "type": "User",
      "display_name": "PlanetScale Bot",
      "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
    },
    "closed_by": null,
    "branch": "dev",
    "branch_deleted": false,
    "branch_deleted_by": null,
    "branch_deleted_at": null,
    "into_branch": "main",
    "into_branch_sharded": false,
    "into_branch_shard_count": 0,
    "approved": false,
    "state": "open",
    "deployment_state": "error",
    "num_comments": 0,
    "deployment": {
      "id": "xqvfpq4yllwc",
      "type": "Deployment",
      "into_branch": "main",
      "deploy_request_number": 1,
      "actor": {
        "id": "boo",
        "type": "User",
        "display_name": "Ghost",
        "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
      },
      "cutover_actor": null,
      "cancelled_actor": null,
      "schema_last_updated_at": "2023-10-19T17:29:49.430Z",
      "preceding_deployments": [],
      "deploy_operations": [],
      "deploy_operation_summaries": [],
      "deployable": false,
      "cutover_expiring": false,
      "lint_errors": [
        {
          "type": "DeploymentLintError",
          "lint_error": "NO_UNIQUE_KEY",
          "subject_type": "table_error",
          "statement": "",
          "keyspace_name": "example_database",
          "table_name": "Persons",
          "error_description": "table \"Persons\" has no unique key: all tables must have at least one unique, not-null key without using text / blob columns or partial indexes.",
          "column_name": "",
          "foreign_key_column_names": [],
          "auto_increment_column_names": [],
          "charset_name": "",
          "engine_name": "",
          "vindex_name": null,
          "json_path": null,
          "schema_identifier": "SOURCE",
          "conflict_columns": null,
          "conflict_indexes": null,
          "conflict_constraints": null,
          "conflict_partitions": null,
          "conflict_error": "CONFLICT_UNSPECIFIED",
          "conflict_ddl_verb": "DDL_VERB_UNSPECIFIED",
          "check_constraint_name": "",
          "enum_value": "",
          "partitioning_type": "",
          "partition_name": "",
          "view_name": "",
          "boost_query": null
        }
      ],
      "deployment_revert_request": null,
      "auto_cutover": true,
      "created_at": "2023-10-19T17:29:49.032Z",
      "cutover_at": null,
      "deploy_check_errors": "",
      "finished_at": null,
      "queued_at": null,
      "ready_to_cutover_at": null,
      "started_at": null,
      "state": "error",
      "submitted_at": null,
      "updated_at": "2023-10-19T17:29:54.166Z"
    },
    "html_url": "https://app.planetscale.com/demo-db/example_database/deploy-requests/1",
    "number": 1,
    "notes": "",
    "html_body": "",
    "created_at": "2023-10-19T17:29:48.993Z",
    "updated_at": "2023-10-19T17:29:54.197Z",
    "closed_at": null,
    "deployed_at": null
  }
}
```

### Deploy request pending cutover

The deploy request is ready to apply the schema and is waiting on the user to confirm.

The `deploy_request.pending_cutover` event uses the same response body as a `200` response from the [Get a deploy request](/docs/api/reference/get_deploy_request) API endpoint. The link includes a detailed description of each field in the API reference.

**Example:**

```json expandable theme={null}
{
  "timestamp": 1698252961,
  "event": "deploy_request.pending_cutover",
  "organization": "myorg",
  "database": "example_database",
  "resource": {
    "id": "4xsz0ql82y4n",
    "type": "DeployRequest",
    "actor": {
      "id": "g2dr4sbhz6ag",
      "type": "User",
      "display_name": "PlanetScale Bot",
      "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
    },
    "closed_by": null,
    "branch": "dev",
    "branch_deleted": false,
    "branch_deleted_by": null,
    "branch_deleted_at": null,
    "into_branch": "main",
    "into_branch_sharded": false,
    "into_branch_shard_count": 0,
    "approved": false,
    "state": "open",
    "deployment_state": "pending_cutover",
    "num_comments": 0,
    "deployment": {
      "id": "uvkd7injje2f",
      "type": "Deployment",
      "into_branch": "main",
      "deploy_request_number": 5,
      "actor": {
        "id": "g2dr4sbhz6ag",
        "type": "User",
        "display_name": "PlanetScale Bot",
        "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
      },
      "cutover_actor": null,
      "cancelled_actor": null,
      "schema_last_updated_at": "2023-10-25T16:55:00.288Z",
      "preceding_deployments": [],
      "deploy_operations": [
        {
          "id": "wo1m619ufrpc",
          "type": "DeployOperation",
          "state": "in_progress",
          "keyspace_name": "example_database",
          "table_name": "Persons",
          "operation_name": "ALTER",
          "eta_seconds": null,
          "progress_percentage": 100,
          "deploy_error_docs_url": null,
          "ddl_statement": "ALTER TABLE `Persons` DROP COLUMN `Address`",
          "syntax_highlighted_ddl": "<div class=\"line line-1\"><span class=\"k\">ALTER</span> <span class=\"k\">TABLE</span> <span class=\"nv\">`Persons`</span> </div><div class=\"line line-2\">  <span class=\"k\">DROP</span> <span class=\"k\">COLUMN</span> <span class=\"nv\">`Address`</span></div>",
          "created_at": "2023-10-25T16:55:26.562Z",
          "updated_at": "2023-10-25T16:55:26.562Z",
          "can_drop_data": true,
          "table_recently_used": false,
          "table_recently_used_at": null,
          "deploy_errors": null
        }
      ],
      "deploy_operation_summaries": [],
      "deployable": true,
      "cutover_expiring": false,
      "lint_errors": [],
      "deployment_revert_request": {
        "id": "11ljwej314iy",
        "type": "DeploymentRevert",
        "actor": {
          "id": "boo",
          "type": "User",
          "display_name": "Ghost",
          "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
        },
        "cancelled_at": null,
        "finished_at": null,
        "waiting_period_end_at": null,
        "deploy_revert_operations": []
      },
      "auto_cutover": false,
      "created_at": "2023-10-25T16:54:59.863Z",
      "cutover_at": null,
      "deploy_check_errors": null,
      "finished_at": null,
      "queued_at": "2023-10-25T16:55:53.543Z",
      "ready_to_cutover_at": null,
      "started_at": "2023-10-25T16:56:01.035Z",
      "state": "in_progress",
      "submitted_at": "2023-10-25T16:55:53.702Z",
      "updated_at": "2023-10-25T16:56:01.057Z"
    },
    "html_url": "https://app.planetscale.com/demo-db/example_database/deploy-requests/5",
    "number": 5,
    "notes": "",
    "html_body": "",
    "created_at": "2023-10-25T16:54:59.797Z",
    "updated_at": "2023-10-25T16:56:01.063Z",
    "closed_at": null,
    "deployed_at": null
  }
}
```

### Deploy request schema applied

The deploy request has finished applying the schema.

The `deploy_request.schema_applied` event uses the same response body as a `200` response from the [Get a deploy request](/docs/api/reference/get_deploy_request) API endpoint. The link includes a detailed description of each field in the API reference.

```json expandable theme={null}
{
  "timestamp": 1698252989,
  "event": "deploy_request.schema_applied",
  "organization": "myorg",
  "database": "example_database",
  "resource": {
    "id": "4xsz0ql82y4n",
    "type": "DeployRequest",
    "actor": {
      "id": "g2dr4sbhz6ag",
      "type": "User",
      "display_name": "PlanetScale Bot",
      "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
    },
    "closed_by": null,
    "branch": "dev",
    "branch_deleted": false,
    "branch_deleted_by": null,
    "branch_deleted_at": null,
    "into_branch": "main",
    "into_branch_sharded": false,
    "into_branch_shard_count": 0,
    "approved": false,
    "state": "open",
    "deployment_state": "complete_pending_revert",
    "num_comments": 0,
    "deployment": {
      "id": "uvkd7injje2f",
      "type": "Deployment",
      "into_branch": "main",
      "deploy_request_number": 5,
      "actor": {
        "id": "g2dr4sbhz6ag",
        "type": "User",
        "display_name": "PlanetScale Bot",
        "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
      },
      "cutover_actor": {
        "id": "g2dr4sbhz6ag",
        "type": "User",
        "display_name": "PlanetScale Bot",
        "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
      },
      "cancelled_actor": {
        "id": "boo",
        "type": "User",
        "display_name": "Ghost",
        "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
      },
      "schema_last_updated_at": "2023-10-25T16:55:00.288Z",
      "preceding_deployments": [],
      "deploy_operations": [
        {
          "id": "krosesxjzl1p",
          "type": "DeployOperation",
          "state": "complete",
          "keyspace_name": "example_database",
          "table_name": "Persons",
          "operation_name": "ALTER",
          "eta_seconds": 0,
          "progress_percentage": 100,
          "deploy_error_docs_url": null,
          "ddl_statement": "ALTER TABLE `Persons` DROP COLUMN `Address`",
          "syntax_highlighted_ddl": "<div class=\"line line-1\"><span class=\"k\">ALTER</span> <span class=\"k\">TABLE</span> <span class=\"nv\">`Persons`</span> </div><div class=\"line line-2\">  <span class=\"k\">DROP</span> <span class=\"k\">COLUMN</span> <span class=\"nv\">`Address`</span></div>",
          "created_at": "2023-10-25T16:56:01.399Z",
          "updated_at": "2023-10-25T16:56:28.745Z",
          "can_drop_data": true,
          "table_recently_used": false,
          "table_recently_used_at": null,
          "deploy_errors": ""
        }
      ],
      "deploy_operation_summaries": [],
      "deployable": true,
      "cutover_expiring": false,
      "lint_errors": [],
      "deployment_revert_request": {
        "id": "1cio8bfvx0pp",
        "type": "DeploymentRevert",
        "actor": {
          "id": "boo",
          "type": "User",
          "display_name": "Ghost",
          "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
        },
        "cancelled_at": null,
        "finished_at": null,
        "waiting_period_end_at": "2023-10-25T17:26:29.153Z",
        "deploy_revert_operations": [
          {
            "id": "p3r4o6t4wr7x",
            "type": "DeployRevertOperation",
            "ddl_statement": "ALTER TABLE `Persons` DROP COLUMN `Address`",
            "operation_name": "ALTER",
            "state": "submit",
            "table_name": "Persons",
            "created_at": "2023-10-25T16:56:28.833Z",
            "updated_at": "2023-10-25T16:56:29.067Z",
            "revert_errors": null
          }
        ]
      },
      "auto_cutover": false,
      "created_at": "2023-10-25T16:54:59.863Z",
      "cutover_at": "2023-10-25T16:56:22.312Z",
      "deploy_check_errors": null,
      "finished_at": "2023-10-25T16:56:29.121Z",
      "queued_at": "2023-10-25T16:55:53.543Z",
      "ready_to_cutover_at": "2023-10-25T16:56:01.426Z",
      "started_at": "2023-10-25T16:56:01.035Z",
      "state": "complete_pending_revert",
      "submitted_at": "2023-10-25T16:55:53.702Z",
      "updated_at": "2023-10-25T16:56:29.121Z"
    },
    "html_url": "https://app.planetscale.com/demo-db/example_database/deploy-requests/5",
    "number": 5,
    "notes": "",
    "html_body": "",
    "created_at": "2023-10-25T16:54:59.797Z",
    "updated_at": "2023-10-25T16:56:29.128Z",
    "closed_at": null,
    "deployed_at": "2023-10-25T16:56:22.312Z"
  }
}
```

### Deploy request reverted

The deploy request has been reverted.

The `deploy_request.reverted` event uses the same response body as a `200` response from the [Get a deploy request](/docs/api/reference/get_deploy_request) API endpoint. The link includes a detailed description of each field in the API reference.

```json expandable theme={null}
{
  "timestamp": 1698253029,
  "event": "deploy_request.reverted",
  "organization": "myorg",
  "database": "example_database",
  "resource": {
    "id": "4xsz0ql82y4n",
    "type": "DeployRequest",
    "actor": {
      "id": "g2dr4sbhz6ag",
      "type": "User",
      "display_name": "PlanetScale Bot",
      "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
    },
    "closed_by": null,
    "branch": "dev",
    "branch_deleted": false,
    "branch_deleted_by": null,
    "branch_deleted_at": null,
    "into_branch": "main",
    "into_branch_sharded": false,
    "into_branch_shard_count": 0,
    "approved": false,
    "state": "open",
    "deployment_state": "in_progress_revert",
    "num_comments": 0,
    "deployment": {
      "id": "uvkd7injje2f",
      "type": "Deployment",
      "into_branch": "main",
      "deploy_request_number": 5,
      "actor": {
        "id": "g2dr4sbhz6ag",
        "type": "User",
        "display_name": "PlanetScale Bot",
        "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
      },
      "cutover_actor": {
        "id": "g2dr4sbhz6ag",
        "type": "User",
        "display_name": "PlanetScale Bot",
        "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
      },
      "cancelled_actor": {
        "id": "boo",
        "type": "User",
        "display_name": "Ghost",
        "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
      },
      "schema_last_updated_at": "2023-10-25T16:55:00.288Z",
      "preceding_deployments": [],
      "deploy_operations": [
        {
          "id": "krosesxjzl1p",
          "type": "DeployOperation",
          "state": "complete",
          "keyspace_name": "example_database",
          "table_name": "Persons",
          "operation_name": "ALTER",
          "eta_seconds": 0,
          "progress_percentage": 100,
          "deploy_error_docs_url": null,
          "ddl_statement": "ALTER TABLE `Persons` DROP COLUMN `Address`",
          "syntax_highlighted_ddl": "<div class=\"line line-1\"><span class=\"k\">ALTER</span> <span class=\"k\">TABLE</span> <span class=\"nv\">`Persons`</span> </div><div class=\"line line-2\">  <span class=\"k\">DROP</span> <span class=\"k\">COLUMN</span> <span class=\"nv\">`Address`</span></div>",
          "created_at": "2023-10-25T16:56:01.399Z",
          "updated_at": "2023-10-25T16:56:28.745Z",
          "can_drop_data": true,
          "table_recently_used": false,
          "table_recently_used_at": null,
          "deploy_errors": ""
        }
      ],
      "deploy_operation_summaries": [],
      "deployable": true,
      "cutover_expiring": false,
      "lint_errors": [],
      "deployment_revert_request": {
        "id": "1cio8bfvx0pp",
        "type": "DeploymentRevert",
        "actor": {
          "id": "g2dr4sbhz6ag",
          "type": "User",
          "display_name": "PlanetScale Bot",
          "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
        },
        "cancelled_at": null,
        "finished_at": "2023-10-25T16:57:09.472Z",
        "waiting_period_end_at": "2023-10-25T17:26:29.153Z",
        "deploy_revert_operations": [
          {
            "id": "p3r4o6t4wr7x",
            "type": "DeployRevertOperation",
            "ddl_statement": "ALTER TABLE `Persons` DROP COLUMN `Address`",
            "operation_name": "ALTER",
            "state": "complete",
            "table_name": "Persons",
            "created_at": "2023-10-25T16:56:28.833Z",
            "updated_at": "2023-10-25T16:57:09.437Z",
            "revert_errors": ""
          }
        ]
      },
      "auto_cutover": false,
      "created_at": "2023-10-25T16:54:59.863Z",
      "cutover_at": "2023-10-25T16:56:22.312Z",
      "deploy_check_errors": null,
      "finished_at": "2023-10-25T16:56:29.121Z",
      "queued_at": "2023-10-25T16:55:53.543Z",
      "ready_to_cutover_at": "2023-10-25T16:56:01.426Z",
      "started_at": "2023-10-25T16:56:01.035Z",
      "state": "in_progress_revert",
      "submitted_at": "2023-10-25T16:55:53.702Z",
      "updated_at": "2023-10-25T16:57:02.319Z"
    },
    "html_url": "https://app.planetscale.com/demo-db/example_database/deploy-requests/5",
    "number": 5,
    "notes": "",
    "html_body": "",
    "created_at": "2023-10-25T16:54:59.797Z",
    "updated_at": "2023-10-25T16:57:02.327Z",
    "closed_at": null,
    "deployed_at": "2023-10-25T16:56:22.312Z"
  }
}
```

### Deploy request closed

The deploy request has been closed.

The `deploy_request.closed` event uses the same response body as a `200` response from the [Get a deploy request](/docs/api/reference/get_deploy_request) API endpoint. The link includes a detailed description of each field in the API reference.

```json expandable theme={null}
{
  "timestamp": 1698253030,
  "event": "deploy_request.closed",
  "organization": "myorg",
  "database": "example_database",
  "resource": {
    "id": "4xsz0ql82y4n",
    "type": "DeployRequest",
    "actor": {
      "id": "g2dr4sbhz6ag",
      "type": "User",
      "display_name": "PlanetScale Bot",
      "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
    },
    "closed_by": {
      "id": "g2dr4sbhz6ag",
      "type": "User",
      "display_name": "PlanetScale Bot",
      "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
    },
    "branch": "dev",
    "branch_deleted": false,
    "branch_deleted_by": null,
    "branch_deleted_at": null,
    "into_branch": "main",
    "into_branch_sharded": false,
    "into_branch_shard_count": 0,
    "approved": false,
    "state": "closed",
    "deployment_state": "complete_revert",
    "num_comments": 0,
    "deployment": {
      "id": "uvkd7injje2f",
      "type": "Deployment",
      "into_branch": "main",
      "deploy_request_number": 5,
      "actor": {
        "id": "g2dr4sbhz6ag",
        "type": "User",
        "display_name": "PlanetScale Bot",
        "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
      },
      "cutover_actor": {
        "id": "g2dr4sbhz6ag",
        "type": "User",
        "display_name": "PlanetScale Bot",
        "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
      },
      "cancelled_actor": {
        "id": "boo",
        "type": "User",
        "display_name": "Ghost",
        "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
      },
      "schema_last_updated_at": "2023-10-25T16:55:00.288Z",
      "preceding_deployments": [],
      "deploy_operations": [
        {
          "id": "krosesxjzl1p",
          "type": "DeployOperation",
          "state": "complete",
          "keyspace_name": "example_database",
          "table_name": "Persons",
          "operation_name": "ALTER",
          "eta_seconds": 0,
          "progress_percentage": 100,
          "deploy_error_docs_url": null,
          "ddl_statement": "ALTER TABLE `Persons` DROP COLUMN `Address`",
          "syntax_highlighted_ddl": "<div class=\"line line-1\"><span class=\"k\">ALTER</span> <span class=\"k\">TABLE</span> <span class=\"nv\">`Persons`</span> </div><div class=\"line line-2\">  <span class=\"k\">DROP</span> <span class=\"k\">COLUMN</span> <span class=\"nv\">`Address`</span></div>",
          "created_at": "2023-10-25T16:56:01.399Z",
          "updated_at": "2023-10-25T16:56:28.745Z",
          "can_drop_data": true,
          "table_recently_used": false,
          "table_recently_used_at": null,
          "deploy_errors": ""
        }
      ],
      "deploy_operation_summaries": [],
      "deployable": true,
      "cutover_expiring": false,
      "lint_errors": [],
      "deployment_revert_request": {
        "id": "1cio8bfvx0pp",
        "type": "DeploymentRevert",
        "actor": {
          "id": "g2dr4sbhz6ag",
          "type": "User",
          "display_name": "PlanetScale Bot",
          "avatar_url": "https://app.planetscale.com/gravatar-fallback.png"
        },
        "cancelled_at": null,
        "finished_at": "2023-10-25T16:57:09.472Z",
        "waiting_period_end_at": "2023-10-25T17:26:29.153Z",
        "deploy_revert_operations": [
          {
            "id": "p3r4o6t4wr7x",
            "type": "DeployRevertOperation",
            "ddl_statement": "ALTER TABLE `Persons` DROP COLUMN `Address`",
            "operation_name": "ALTER",
            "state": "complete",
            "table_name": "Persons",
            "created_at": "2023-10-25T16:56:28.833Z",
            "updated_at": "2023-10-25T16:57:09.437Z",
            "revert_errors": ""
          }
        ]
      },
      "auto_cutover": false,
      "created_at": "2023-10-25T16:54:59.863Z",
      "cutover_at": "2023-10-25T16:56:22.312Z",
      "deploy_check_errors": null,
      "finished_at": "2023-10-25T16:56:29.121Z",
      "queued_at": "2023-10-25T16:55:53.543Z",
      "ready_to_cutover_at": "2023-10-25T16:56:01.426Z",
      "started_at": "2023-10-25T16:56:01.035Z",
      "state": "complete_revert",
      "submitted_at": "2023-10-25T16:55:53.702Z",
      "updated_at": "2023-10-25T16:57:09.836Z"
    },
    "html_url": "https://app.planetscale.com/demo-db/example_database/deploy-requests/5",
    "number": 5,
    "notes": "",
    "html_body": "",
    "created_at": "2023-10-25T16:54:59.797Z",
    "updated_at": "2023-10-25T16:57:10.046Z",
    "closed_at": "2023-10-25T16:57:09.995Z",
    "deployed_at": "2023-10-25T16:56:22.312Z"
  }
}
```

### Keyspace storage

A storage threshold (60%, 75%, 85%, 90%, 95%) has been crossed for a keyspace.

<Note>
  These webhooks are only sent for PlanetScale Metal keyspaces.
</Note>

```json  theme={null}
{
  "timestamp": 1745526125,
  "event": "keyspace.storage",
  "organization": "myorg",
  "database": "example_database",
  "resource": {
    "branch_name": "main",
    "keyspace_name": "example_database",
    "storage_percentage": 44,
    "shards": [
      {
        "shard": "x-x",
        "bytes_used": 208470605824,
        "bytes_total_capacity": 472424366080,
        "bytes_available": 263953760256,
        "storage_percentage": 44.1
      }
    ]
  }
}
```

### Webhook test

A webhook test is triggered.

You can only send one webhook test every 20 seconds.

**Example:**

```json expandable theme={null}
{
  "timestamp": 1697738828,
  "event": "webhook.test",
  "organization": "myorg",
  "database": "example_database",
  "resource": {
    "id": "34rbgzmvgb9m",
    "type": "Database",
    "region": {
      "id": "kc0e1ij8juzp",
      "type": "Region",
      "provider": "AWS",
      "enabled": true,
      "public_ip_addresses": [
        "23.23.187.137",
        "52.6.141.108",
        "52.70.2.89",
        "50.17.188.76",
        "52.2.251.189",
        "52.72.234.74",
        "35.174.68.24",
        "52.5.253.172",
        "54.156.81.4",
        "34.200.24.255",
        "35.174.79.154",
        "44.199.177.24",
        "35.173.174.19",
        "44.212.228.57",
        "44.216.88.45"
      ],
      "display_name": "AWS us-east-1",
      "location": "N. Virginia",
      "slug": "us-east",
      "current_default": true
    },
    "name": "example_database",
    "notes": "",
    "state": "ready",
    "created_at": "2023-10-18T22:16:54.071Z",
    "updated_at": "2023-10-19T17:25:40.577Z",
    "ready": true,
    "sharded": false,
    "html_url": "https://app.planetscale.com/demo-db/example_database",
    "url": "https://api.planetscale.com/v1/organizations/demo-db/databases/example_database",
    "branches_url": "https://api.planetscale.com/v1/organizations/demo-db/databases/example_database/branches",
    "plan": "scaler_pro"
  }
}
```

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Webhooks
Source: https://planetscale.com/docs/api/webhooks



## Webhooks in PlanetScale

With a webhook in PlanetScale, you can trigger an HTTP POST callback to a configured URL when a specific event occurs within your PlanetScale organization. The callback payload will contain useful data related to the event. With this data, webhooks can be used to build custom integrations, notifications, and automate other workflows.

Webhooks in PlanetScale for Vitess are not like MySQL triggers and cannot be triggered on database table events like `INSERT`, `UPDATE`, or `DELETE`. Instead, think of events for your database cluster's life cycle such as:

| Webhook event         | Trigger                                                               |
| :-------------------- | :-------------------------------------------------------------------- |
| Branch ready          | The branch is created and ready to connect.                           |
| Deploy request opened | The deploy request has been opened.                                   |
| Deploy request queued | The deploy request has been added to the deploy queue.                |
| Deploy request closed | The deploy request has been closed.                                   |
| Keyspace storage      | A keyspace has reached a storage threshold (60%, 75%, 85%, 90%, 95%). |

For more information about the events you can trigger a webhook with in PlanetScale, including example payloads, and supporte database engines, see the [webhook event reference documentation](/docs/api/webhook-events).

## Common webhook use cases

There are various scenarios where webhooks can be useful, some of them include:

* Creating notifications in Slack, Microsoft Teams, GitHub, and other tools
* Integrating with CI/CD processes for the automation of schema changes
* Updating external issue trackers like Jira

## Managing webhooks

You can manage webhooks in PlanetScale using three different methods:

* **Dashboard** — Create and manage webhooks through the web interface (instructions below)
* **CLI** — Use the [`pscale webhook`](/docs/cli/webhook) command to manage webhooks from your terminal
* **API** — Use the [webhook API endpoints](/docs/api/reference/list_webhooks) for managing webhooks in your applications

## Setting up a webhook using the dashboard

You must be a [database administrator](/docs/security/access-control#database-administrator) to create a webhook for a database.

<Steps>
  <Step>
    From the PlanetScale organization dashboard, select your database
  </Step>

  <Step>
    Click on **Settings** from the menu on the left
  </Step>

  <Step>
    Navigate to the **Webhooks** page from the sub-menu on the left under **Settings**
  </Step>

  <Step>
    Click **"Add webhook"**.
  </Step>

  <Step>
    Add an unique URL for the webhook to send events to. The URL must:

    * Use HTTPS. You can use sites like [https://webhook.site](https://webhook.site/) or tools like [ngrok](https://ngrok.com/docs/getting-started/) to create a HTTPS URL to test webhooks locally.
    * Be reachable from the internet, no local hosts.
    * Be able to handle incoming HTTP callbacks.
  </Step>

  <Step>
    Select the events you want to trigger the webhook. See the [webhook events reference](/docs/api/webhook-events) for more information on the events and example response bodies.
  </Step>

  <Step>
    Click **"Save webhook"**.
  </Step>

  <Step>
    Select the **...** ellipsis and click **"Test webhook"** to send a test event to the webhook. You should receive a POST request with the event type `webhook.test`.
  </Step>

  <Step>
    On the **"Webhooks"** page, you can see the status of the last sent webhook under each webhook you set up.
  </Step>
</Steps>

## Handling deliveries

There are a few things to remember when receiving a webhook from PlanetScale:

* You must receive events with an HTTPS server.
* Your server must [quickly respond](#responding-to-webhooks-quickly) with a `2xx` successful status code to indicate that the webhook was received successfully.
* PlanetScale will not follow any redirects from the server.
* PlanetScale's webhooks will originate from one of the following IP addresses:
  ```
  3.209.149.66
  3.215.97.46
  34.193.111.15
  ```
* We recommend [validating all webhook signatures](#validating-a-webhook-signature) to ensure the webhook came from PlanetScale, not from a third party, and was not tampered with.

### Responding to webhooks quickly

To protect your service from being overloaded by webhook deliveries, we recommend responding to the webhook request immediately and handling the processing of the webhook asynchronously. PlanetScale will wait a maximum of 2 seconds for a response from your server before marking the webhook as failed.

### Limits

* Each database can have up to **5 webhooks**. If you need more webhooks per database, please [contact us](https://planetscale.com/contact).
* You can only send one webhook test **every 20 seconds**.
* Webhooks that repeatedly fail will be disabled.

### Validating a webhook signature

To ensure that your server only processes webhook deliveries sent by PlanetScale, we recommend validating the webhook signature before processing the delivery further.

All webhooks from PlanetScale will have an `X-PlanetScale-Signature` header. This header is a SHA-256 HMAC hex digest of the request body, using your webhook secret as the key. You can use this header to verify that the webhook payload was sent by PlanetScale.

To do this, you need to:

1. Retrieve your webhook secret from PlanetScale. Go to your database's settings page > **"Webhooks"** page. Click the **...** ellipsis next to the webhook you want to validate and click **"Show secret"**.
2. Copy and securely store your webhook secret on your server. Never hard code the secret into your application or check it into source control. Follow the best practices for your deployment provider and framework for storing secrets.
3. Validate incoming webhook payloads against the secret to verify that the payload was sent by PlanetScale. You should calculate a hash using the JSON payload and the webhook secret. Then, compare the calculated hash to the `X-PlanetScale-Signature` header. If the two hashes match, the webhook payload is valid.

### Example webhook signature validation

The following are examples of validating a webhook signature that uses a SHA-256 HMAC hex digest of the request body.

#### JavaScript

```javascript  theme={null}
const crypto = require('crypto')

const WEBHOOK_SECRET = process.env.WEBHOOK_SECRET

const verify_signature = (req) => {
  const signature = crypto.createHmac('sha256', WEBHOOK_SECRET).update(JSON.stringify(req.body)).digest('hex')
  const trusted = Buffer.from(signature, 'ascii')
  const header = req.headers.get('x-planetscale-signature')

  if (header === undefined) {
    return false
  }

  const untrusted = Buffer.from(header)
  return crypto.timingSafeEqual(trusted, untrusted)
}
```

You can then call `verifySignature` in any JavaScript environment when you receive a webhook.

<Warning>
  You must create the digest using the *exact* body string sent in the POST request.
  If you are using [Express](https://expressjs.com/), you need to ensure that you grab the raw body string.
  You'll want to use `bodyParser.raw` instead of `express.json` for getting the POST request body:

  ```javascript  theme={null}
  const app = express()
  app.use(bodyParser.raw({ inflate: true, type: 'application/json' }))
  ```
</Warning>

Then you can access the body with `req.body` as shown above.

#### TypeScript

```typescript  theme={null}
import crypto from 'crypto'

const WEBHOOK_SECRET: string = process.env.WEBHOOK_SECRET

const verify_signature = (req: Request): boolean => {
  const signature = crypto.createHmac('sha256', WEBHOOK_SECRET).update(JSON.stringify(req.body)).digest('hex')
  const trusted = Buffer.from(signature, 'ascii')
  const header = req.headers.get('x-planetscale-signature')

  if (header === undefined) {
    return false
  }

  const untrusted = Buffer.from(header)
  return crypto.timingSafeEqual(trusted, untrusted)
}
```

You can then call `verify_signature` when you receive a webhook.

```typescript  theme={null}
const handleWebhook = (req: Request, res: Response) => {
  if (!verify_signature(req)) {
    res.status(401).send('Unauthorized')
    return
  }
  // The rest of your logic here
}
```

#### Python

This example shows how to validate the webhook in a Flask app.

```python expandable theme={null}
from flask import Flask, request
from hashlib import sha256
import hmac

app = Flask(__name__)

# Use the PLANETSCALE_WEBHOOK_SECRET environment variable to set the secret token
SECRET_TOKEN = os.environ.get('PLANETSCALE_WEBHOOK_SECRET', 'default_secret_token')

@app.route('/webhook', methods=['POST'])
def webhook():
    payload_body = request.data
    signature_header = request.headers.get('x-planetscale-signature')

    try:
        verify_signature(payload_body, SECRET_TOKEN, signature_header)
        # The signature is valid, you can process the payload here
        return "Signature is valid."
    except Exception as e:
        # Handle the verification failure here
        return str(e), 403

def verify_signature(payload_body, secret_token, signature_header):
    if not signature_header:
        raise Exception("x-planetscale-signature header is missing!")

    hash_object = hmac.new(secret_token.encode('utf-8'), msg=payload_body, digestmod=sha256)
    expected_signature = hash_object.hexdigest()

    if not hmac.compare_digest(expected_signature, signature_header):
        raise Exception("Request signatures didn't match!")

if __name__ == '__main__':
    app.run(debug=True)
```

#### Ruby on Rails

```ruby  theme={null}
SECRET = Rails.application.credentials.planetscale.fetch(:webhook_secret)

# Private method in your controller
def verify_signature
  body = request.body.read
  signature = request.headers["X-PlanetScale-Signature"]

  calculated_signature = OpenSSL::HMAC.hexdigest(OpenSSL::Digest.new("sha256"), SECRET, body)

  unless ActiveSupport::SecurityUtils.secure_compare(signature, calculated_signature)
    render(json: { message: "Unauthorized" }, status: :unauthorized)
  end
end
```

You can then call `verify_signature` on each request to validate the webhook.

```ruby  theme={null}
before_action :verify_signature, only: [:create]
```

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Billing
Source: https://planetscale.com/docs/billing

PlanetScale applies billing plans at the **database level**. You can create several databases under one Organization.

## Overview

Usage charges are scoped to each database, which you can find all together in the [billing section of your Organization](/docs/billing#organization-usage-and-billing-page). Each plan is further broken down in the [PlanetScale plans documentation](/docs/planetscale-plans).

<Note>
  We use **[gibibytes, otherwise known as binary gigabytes](https://simple.wikipedia.org/wiki/Gibibyte)**, to calculate storage and usage limits. For reference, 1 binary gigabyte is equivalent to 2^30 bytes.
</Note>

## Organization usage and billing page

Each organization has its own billing page available for administrators, from which you can:

* View your current and previous usage per database
* Enter/update your credit card information
* Activate coupons
* Add/update your business address, Vat ID, and other information shown on invoices
* Download current and previous invoices

**To find your billing page:**

<Steps>
  <Step>
    Go to your [PlanetScale dashboard](https://app.planetscale.com)
  </Step>

  <Step>
    Select the organization whose billing page you want to view
  </Step>

  <Step>
    Click on "Settings" in the top nav
  </Step>

  <Step>
    Click on "Billing" in the side nav
  </Step>
</Steps>

### PlanetScale invoice details

Invoices provide line items for both usage and discounts received.
Depending on the configuration of your database and the features you use, you may see line items for:

* Your primary database branch
* Development branch usage
* Storage usage per GB (for non-Metal databases)
* Read-only region nodes and storage (for Vitess databases)
* Prorated discounts, if the branch existed for a smaller time period than the billing period

Storage is prorated by a percentage equal to the existence of a branch's hours/billing period in hours.

### Download an invoice

To download an invoice, go to [your billing page](#organization-usage-and-billing-page) (`Organization > Settings > Billing`).

You'll see a table of current and previous monthly invoices. You can download an invoice by month by clicking the "**Download**" button. This will send you to a Stripe invoice page, where you'll have the option to download the complete invoice in PDF format, see invoice details, or download your receipt.

To see more details about your billing from the PlanetScale dashboard, click the "**View details**" button on the Billing page next to the month you want to view. This will show you an overview of the charges for all of the databases in your organization.

## Payment methods

All plans require that your organization has a valid payment method on file. You can use a debit or credit card, but we do not accept pre-paid cards.

It's possible to use the same card for multiple organizations, but only after the card has been used to successfully pay for a database in the first organization.

## Spend management

You have the option to set spend alerts from your organization's billing page. Once on the billing page, click the checkbox on the right next to "Enable spend alerts", type in the max budget, and click "Save". Organization administrators will receive an email when the organization hits 75% and 100% of its maximum monthly spend.

## Using coupons

You can redeem a coupon in the PlanetScale dashboard. To redeem a coupon, you must first enter your credit card information. Once you have a credit card on file, go to your Organization Settings page, click "Billing", click "Redeem a coupon" on the right, enter your coupon, and click "Redeem coupon".

<Note>
  You may incur additional costs if your usage continues beyond the period, dollar amount, or any other metrics specified in the coupon terms. Additional costs will be charged to your card on file. If you have any questions about the terms of the coupon, please reach out to [our Support team](#need-help).
</Note>

### How do coupons affect invoices?

You will see your coupon reflected in your monthly invoice. Go to "Settings" > "Billing" and select the invoice for the month(s) where your coupon was active. You will see a note at the top of the invoice similar to this:

`Amount reflects your $xx.xx discount with code YOURCOUPONCODE`

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/billing/coupons-in-invoices.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=15c64da4c648d4960016de355c9b54aa" alt="PlanetScale dashboard - Example coupon factored into invoice" data-og-width="1702" width="1702" data-og-height="935" height="935" data-path="docs/images/assets/docs/concepts/billing/coupons-in-invoices.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/billing/coupons-in-invoices.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=a95fb680d0d0aa25e904b247ee3a4549 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/billing/coupons-in-invoices.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=fcc15e5e41ee3136b12a60796ef38971 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/billing/coupons-in-invoices.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=c4aaf0355a8e221402712cf778679de4 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/billing/coupons-in-invoices.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=7ba3b6c8d242f40d2165edbe6a5aa90a 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/billing/coupons-in-invoices.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=ce764e6a7d2516f2e578935f5b48db32 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/billing/coupons-in-invoices.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=1bfc08ab80e7899660653b4edc1104b5 2500w" />
</Frame>

## Canceling your plan

Deleting a database will end its plan and prorate the plan fee on your current invoice. You can delete a database from its settings page.

## Why do I see a pre-authorization charge on my card?

If you added a new billing method to your account or created a new database afterwards, you may see a temporary hold on your credit card. This is a pre-authorization that we use to verify that your card is valid, but you will not be charged the amount. This pre-authorization is automatically cancelled after verification, but it may take a few days for your bank to update your account statement to show this cancellation.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale CLI
Source: https://planetscale.com/docs/cli



To interact with PlanetScale and manage your databases, you can use the `pscale` CLI to do the following:

* Create, delete and list your databases and branches
* Open a secure MySQL shell instance
* Manage your deploy requests
* ...and more!

<Note>
  `pscale` can use the MySQL command-line client to quickly open an interactive shell for a database branch. Optional instructions for installing the MySQL client can be found for each platform below.
</Note>

## Getting Started

Make sure to first [set up your PlanetScale developer environment](/docs/cli/planetscale-environment-setup). Once you've installed the `pscale` CLI, you can interact with PlanetScale and manage your databases straight from the command line.

## Available Commands

Use `pscale [command] [command]` to start up the `pscale` CLI in your terminal.

| **Command**                                  | **Subcommands/Options**                                                                                                                                                                                                                      | **Flags**                                                                                                                                                                        | **Product**      | **Description**                                                                                                                                                                        |
| :------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [`api`](/docs/cli/api)                       |                                                                                                                                                                                                                                              | `--help`, `--org string`, `--database string`, `--branch string`, `--field key=value`, `--header stringArray`, `--input string`, `--method string`, `--query key=value`          | Vitess, Postgres | Performs authenticated calls against the PlanetScale API and prints the response to stdout.                                                                                            |
| [`audit-log`](/docs/cli/audit-log)           | `list`                                                                                                                                                                                                                                       | `--help`, `--org string`                                                                                                                                                         | Vitess, Postgres | List all [audit logs](/docs/security/audit-log#review-your-organization-audit-log)                                                                                                     |
| [`auth`](/docs/cli/auth)                     | `login`, `logout`                                                                                                                                                                                                                            | `--help`                                                                                                                                                                         | Vitess, Postgres | Authenticate via console                                                                                                                                                               |
| [`backup`](/docs/cli/backup)                 | `create`, `delete`, `list`, `restore`, `show`                                                                                                                                                                                                | `--help`, `--org string`                                                                                                                                                         | Vitess, Postgres | Manage [branch backups](/docs/vitess/backups)                                                                                                                                          |
| [`branch`](/docs/cli/branch)                 | `create`, `delete`, `diff`, `keyspaces`, `list`, `promote`, `refresh-schema`, `schema`, `show`, `switch`, `vschema`                                                                                                                          | `--help`, `--org string`                                                                                                                                                         | Vitess, Postgres | Manage [branches](/docs/vitess/schema-changes/branching)                                                                                                                               |
| [`completion`](/docs/cli/completion)         | `bash`, `zsh`, `fish`, `powershell`                                                                                                                                                                                                          | `--help`                                                                                                                                                                         | Vitess, Postgres | Generate completion script for specified shell                                                                                                                                         |
| [`connect`](/docs/cli/connect)               | `<database_name>` `<branch_name>`                                                                                                                                                                                                            | `--execute string`, `--execute-env-url string`, `--execute-protocol string`, `--help`, `--host string`, `--org string`, `--port string`, `--remote-addr string`, `--role string` | Vitess           | Create a [secure connection](/docs/vitess/tutorials/connect-any-application#option-2-connect-using-the-planetscale-proxy) to the given database and branch                             |
| [`database`](/docs/cli/database)             | `create`, `delete`, `dump`, `list`, `restore-dump`, `show`                                                                                                                                                                                   | `--help`                                                                                                                                                                         | Vitess, Postgres | Manage databases                                                                                                                                                                       |
| [`deploy-request`](/docs/cli/deploy-request) | `apply`, `cancel`, `close`, `create`, `deploy`, `diff`, `edit`, `list`, `revert`, `review`, `show`, `skip-revert`                                                                                                                            | `--help`                                                                                                                                                                         | Vitess           | Manage [deploy requests](/docs/vitess/schema-changes/branching#1-create-a-deploy-request) including [gated deployments](/docs/vitess/schema-changes/deploy-requests#gated-deployments) |
| `help`                                       | `audit-log`, `auth`, `backup`, `branch`, `completion`, `connect`, `data-import`, `database`, `deploy-request`, `help`, `mcp`, `org`, `password`, `ping`, `region`, `role`, `service-token`, `shell`, `signup`, `size`, `webhook`, `workflow` | `--help`                                                                                                                                                                         | Vitess, Postgres | View help for any command                                                                                                                                                              |
| [`org`](/docs/cli/org)                       | `list`, `show`, `switch`                                                                                                                                                                                                                     | `--help`                                                                                                                                                                         | Vitess, Postgres | Manage and switch [organizations](/docs/security/access-control)                                                                                                                       |
| [`mcp`](/docs/cli/mcp)                       | `install`, `server`                                                                                                                                                                                                                          | `--target string`, `--help`                                                                                                                                                      | Vitess, Postgres | Install and start an [MCP server](/docs/vitess/connecting/mcp)                                                                                                                         |
| [`password`](/docs/cli/password)             | `create`, `delete`, `list`                                                                                                                                                                                                                   | `--help`, `--org string`                                                                                                                                                         | Vitess           | Manage [branch credentials](/docs/vitess/connecting/connection-strings)                                                                                                                |
| [`ping`](/docs/cli/ping)                     |                                                                                                                                                                                                                                              | `--help`, `--count, -n int`, `--concurrency int`, `--provider, -p string` `--timeout duration`                                                                                   | Vitess, Postgres | Check [latency](/docs/vitess/connecting/network-latency) between your machine and PlanetScale's public regions                                                                         |
| [`region`](/docs/cli/region)                 | `list`                                                                                                                                                                                                                                       | `--org string`                                                                                                                                                                   | Vitess, Postgres | View available [regions](/docs/vitess/regions)                                                                                                                                         |
| [`role`](/docs/cli/role)                     | `create`, `delete`, `get`, `list`, `reassign`, `renew`, `reset`, `reset-default`, `update`                                                                                                                                                   | `--help`, `--org string`, `--inherited-roles string`, `--ttl duration`, `--force`, `--successor string`, `--name string`, `--web`                                                | Postgres         | Manage [Postgres roles](/docs/postgres/connecting/roles)                                                                                                                               |
| [`service-token`](/docs/cli/service-token)   | `add-access`, `create`, `delete`, `delete-access`, `list`, `show-access`                                                                                                                                                                     | `--help`, `--org string`                                                                                                                                                         | Vitess, Postgres | Manage access of [service tokens](/docs/api/reference/service-tokens)                                                                                                                  |
| [`size`](/docs/planetscale-plans)            | `cluster list`                                                                                                                                                                                                                               | `--help`, `--org string`, `--region string`, `--metal`                                                                                                                           | Vitess, Postgres | View available [cluster sizes](/docs/planetscale-plans)                                                                                                                                |
| [`shell`](/docs/cli/shell)                   | `<database_name>` `<branch_name>`                                                                                                                                                                                                            | `--help`, `--local-addr string`, `--org string`, `--remote-addr string`                                                                                                          | Vitess, Postgres | Open a MySQL shell instance to the specified database and branch                                                                                                                       |
| [`signup`](/docs/cli/signup)                 |                                                                                                                                                                                                                                              | `--help`                                                                                                                                                                         | Vitess, Postgres | Sign up for a new PlanetScale account                                                                                                                                                  |
| [`webhook`](/docs/cli/webhook)               | `create`, `delete`, `list`, `show`, `test`, `update`                                                                                                                                                                                         | `--help`, `--org string`, `--events string`, `--url string`, `--enabled`                                                                                                         | Vitess, Postgres | Manage [webhooks](/docs/api/webhooks) for databases                                                                                                                                    |
| [`workflow`](/docs/cli/workflow)             | `cancel`, `complete`, `create`, `cutover`, `list`, `retry`, `reverse-cutover`, `reverse-traffic`, `show`, `switch-traffic`, `verify-data`                                                                                                    | `--help`, `--org string`                                                                                                                                                         | Vitess           | Manage the workflows for PlanetScale databases                                                                                                                                         |

## Flags

You may use the following flags with the PlanetScale CLI commands.

| **Flag**                    | **Description**                                                                                                |
| :-------------------------- | :------------------------------------------------------------------------------------------------------------- |
| `--api-token string`        | The API token to use for authenticating against the PlanetScale API                                            |
| `--api-url string`          | The base URL for the PlanetScale API. (default "[https://api.planetscale.com/](https://api.planetscale.com/)") |
| `--config string`           | Config file *(default: \$HOME/.config/planetscale/pscale.yml)*                                                 |
| `--debug`                   | Enable debug mode                                                                                              |
| `-f, --format string`       | Show output in specific format. Possible values: *\[human, json, csv] (default: "human")*                      |
| `-h, --help`                | Get more information about a command                                                                           |
| `--no-color`                | Disable color output                                                                                           |
| `--service-token string`    | Service Token for authenticating                                                                               |
| `--service-token-id string` | The Service Token ID for authenticating                                                                        |
| `--version`                 | Show pscale version                                                                                            |

## Service tokens permissions

A complete list of access permissions available for use with service tokens can be found in the [PlanetScale API documentation](/docs/api/reference/service-tokens#access-permissions).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale CLI commands: audit log
Source: https://planetscale.com/docs/cli/audit-log



## Getting Started

Make sure to first [set up your PlanetScale developer environment](/docs/cli/planetscale-environment-setup). Once you've installed the `pscale` CLI, you can interact with PlanetScale and manage your databases straight from the command line.

## The `audit log` command

Lists all [audit logs](/docs/security/audit-log) in an organization. The user running the command must have [Organization-level permissions](/docs/security/access-control), specifically `list_organization_audit_logs`.

**Usage:**

```bash  theme={null}
pscale audit-log <SUB-COMMAND> <FLAG>
```

### Available sub-commands

| **Sub-command** | **Description**                        | **Product**      |
| :-------------- | :------------------------------------- | :--------------- |
| `list`          | List all audit logs in an organization | Postgres, Vitess |

### Available flags

| **Flag**                    | **Description**                                         |
| :-------------------------- | :------------------------------------------------------ |
| `-h`, `--help`              | View help for `audit-log` command                       |
| `--action`                  | Filter based on action type                             |
| `--limit` int               | The number of events to return. Min: 1, Max: 100        |
| `--starting-after` string   | The ID of the audit log to start after (for pagination) |
| `--org <ORGANIZATION_NAME>` | The organization for the current user                   |

### Global flags

| **Command**                     | **Description**                                                                      |
| :------------------------------ | :----------------------------------------------------------------------------------- |
| `--api-token <TOKEN>`           | The API token to use for authenticating against the PlanetScale API.                 |
| `--api-url <URL>`               | The base URL for the PlanetScale API. Default is `https://api.planetscale.com/`.     |
| `--config <CONFIG_FILE>`        | Config file. Default is `$HOME/.config/planetscale/pscale.yml`.                      |
| `--debug`                       | Enable debug mode.                                                                   |
| `-f`, `--format <FORMAT>`       | Show output in a specific format. Possible values: `human` (default), `json`, `csv`. |
| `--no-color`                    | Disable color output.                                                                |
| `--service-token <TOKEN>`       | The service token for authenticating.                                                |
| `--service-token-id <TOKEN_ID>` | The service token ID for authenticating.                                             |

## Examples

### The `list` sub-command with `--org` flag

**Command:**

```bash  theme={null}
pscale audit-log list --org <ORGANIZATION_NAME>
```

**Output:**

```bash  theme={null}
 ID (25)      ACTOR (25)  ACTION                   EVENT                     REMOTE IP      LOCATION         CREATED AT
------------- ----------- ------------------------ ------------------------ --------------- ---------------- ------------
xxxxxxxxxx  Name        Open_web_console main    branch.open_web_console  xxx.xxx.xxx.x   Los Angeles, CA  1 day ago
```

### Pagination

Use the ID from the last result and pass it as the `--starting-after` to retrieve the next page of results.

```bash  theme={null}
pscale audit-log list --limit 5 --starting-after <ID>
```

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale CLI commands: auth
Source: https://planetscale.com/docs/cli/auth



## Getting Started

Make sure to first [set up your PlanetScale developer environment](/docs/cli/planetscale-environment-setup). Once you've installed the `pscale` CLI, you can interact with PlanetScale and manage your databases straight from the command line.

## The `auth` command

This command allows you to login, logout, and refresh your authentication. Auth tokens generated by `pscale` are valid for one month.

**Usage:**

```bash  theme={null}
pscale auth <SUB-COMMAND> <FLAG>
```

### Available sub-commands

| **Sub-Command** | **Description**                       | **Product**      |
| :-------------- | :------------------------------------ | :--------------- |
| `login`         | Authenticate with the PlanetScale API | Postgres, Vitess |
| `logout`        | Log out of the PlanetScale API        | Postgres, Vitess |
| `check`         | Check if you are authenticated        | Postgres, Vitess |

### Available flags

| **Flag**       | **Description**              |
| :------------- | :--------------------------- |
| `-h`, `--help` | View help for `auth` command |

### Global flags

| **Command**                     | **Description**                                                                      |
| :------------------------------ | :----------------------------------------------------------------------------------- |
| `--api-token <TOKEN>`           | The API token to use for authenticating against the PlanetScale API.                 |
| `--api-url <URL>`               | The base URL for the PlanetScale API. Default is `https://api.planetscale.com/`.     |
| `--config <CONFIG_FILE>`        | Config file. Default is `$HOME/.config/planetscale/pscale.yml`.                      |
| `--debug`                       | Enable debug mode.                                                                   |
| `-f`, `--format <FORMAT>`       | Show output in a specific format. Possible values: `human` (default), `json`, `csv`. |
| `--no-color`                    | Disable color output.                                                                |
| `--service-token <TOKEN>`       | The service token for authenticating.                                                |
| `--service-token-id <TOKEN_ID>` | The service token ID for authenticating.                                             |

## Examples

### The `login` sub-command

**Command:**

```bash  theme={null}
pscale auth login
```

**Output:**

A new browser tab will open and ask you to sign in via browser if you're not already signed in. Next, you'll be asked to confirm the Device confirmation code displayed in your terminal:

```bash  theme={null}
Confirmation Code: XXXXXXX
```

If they match, click "Confirm code", and you'll be signed in to the CLI.

### The `logout` sub-command

**Command:**

```bash  theme={null}
pscale auth logout
```

**Output:**

```bash  theme={null}
Press Enter to log out of the PlanetScale API.
```

### The `check` sub-command

**Command:**

```bash  theme={null}
pscale auth check
```

**Output:**

```bash  theme={null}
You are authenticated.
```

If you are not authenticated, exit code 1 will be returned.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale CLI commands: backup
Source: https://planetscale.com/docs/cli/backup



## Getting Started

Make sure to first [set up your PlanetScale developer environment](/docs/cli/planetscale-environment-setup). Once you've installed the `pscale` CLI, you can interact with PlanetScale and manage your databases straight from the command line.

## The `backup` command

This command allows you to create, list, show, and delete [branch backups](/docs/vitess/backups).

**Usage:**

```bash  theme={null}
pscale backup <SUB-COMMAND> <FLAG>
```

### Available sub-commands

| **Sub-command**                                     | **Description**                    | **Product**      |
| :-------------------------------------------------- | :--------------------------------- | :--------------- |
| `create <DATABASE_NAME> <BRANCH_NAME>`              | Backup a branch's data and schema  | Postgres, Vitess |
| `delete <DATABASE_NAME> <BRANCH_NAME> <BACKUP_ID>`  | Delete a branch backup             | Postgres, Vitess |
| `list <DATABASE_NAME> <BRANCH_NAME>`                | List all backups of a branch       | Postgres, Vitess |
| `restore <DATABASE_NAME> <BRANCH_NAME> <BACKUP_ID>` | Restore a backup to a new branch   | Postgres, Vitess |
| `show <DATABASE_NAME> <BRANCH_NAME> <BACKUP_ID>`    | Show a specific backup of a branch | Postgres, Vitess |

### Available flags

| **Flag**                    | **Description**                       |
| :-------------------------- | :------------------------------------ |
| `-h`, `--help`              | View help for `backup` command        |
| `--org <ORGANIZATION_NAME>` | The organization for the current user |

### Global flags

| **Command**                     | **Description**                                                                      |
| :------------------------------ | :----------------------------------------------------------------------------------- |
| `--api-token <TOKEN>`           | The API token to use for authenticating against the PlanetScale API.                 |
| `--api-url <URL>`               | The base URL for the PlanetScale API. Default is `https://api.planetscale.com/`.     |
| `--config <CONFIG_FILE>`        | Config file. Default is `$HOME/.config/planetscale/pscale.yml`.                      |
| `--debug`                       | Enable debug mode.                                                                   |
| `-f`, `--format <FORMAT>`       | Show output in a specific format. Possible values: `human` (default), `json`, `csv`. |
| `--no-color`                    | Disable color output.                                                                |
| `--service-token <TOKEN>`       | The service token for authenticating.                                                |
| `--service-token-id <TOKEN_ID>` | The service token ID for authenticating.                                             |

## Examples

### The `list` sub-command with `--org` flag

**Command:**

```bash  theme={null}
pscale backup list <DATABASE_NAME> <BRANCH_NAME> --org <ORGANIZATION_NAME>
```

**Output:**

```bash  theme={null}
ID             NAME                  STATE     SIZE    CREATED AT    UPDATED AT    STARTED AT    EXPIRES AT          COMPLETED AT
-------------- --------------------- --------- ------- ------------- ------------- ------------- ------------------- --------------
xxxxxxxx   2022.02.11 16:01:03   success   24.1M   3 hours ago   3 hours ago   3 hours ago   1 day from now      3 hours ago
xxxxxxxx   2022.02.10 16:01:03   success   23.2M   1 day ago     1 day ago     1 day ago     20 hours from now   1 day ago
```

### The `show` sub-command

**Command:**

```bash  theme={null}
pscale backup list <DATABASE_NAME> <BRANCH_NAME> <BACKUP_ID>
```

You can find the `<BACKUP_ID>` by running the `pscale backup list <DATABASE_NAME> <BRANCH_NAME>` command.

**Output:**

```bash  theme={null}
ID             NAME                  STATE     SIZE    CREATED AT    UPDATED AT    STARTED AT    EXPIRES AT          COMPLETED AT
-------------- --------------------- --------- ------- ------------- ------------- ------------- ------------------- --------------
xxxxxxxx   2022.02.11 16:01:03   success   24.1M   3 hours ago   3 hours ago   3 hours ago   1 day from now      3 hours ago
```

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale CLI commands: branch
Source: https://planetscale.com/docs/cli/branch



## Getting Started

Make sure to first [set up your PlanetScale developer environment](/docs/cli/planetscale-environment-setup). Once you've installed the `pscale` CLI, you can interact with PlanetScale and manage your databases straight from the command line.

## The `branch` command

This command allows you to create, delete, diff, and manage [branches](/docs/vitess/schema-changes/branching).

**Usage:**

```bash  theme={null}
pscale branch <SUB-COMMAND> <FLAG>
```

### Available sub-commands

| **Sub-command**                                         | **Sub-command flags**                                                                                    | **Description**                                                  | **Product**      |
| :------------------------------------------------------ | :------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------- | :--------------- |
| `create <DATABASE_NAME> <BRANCH_NAME>`                  | `--from <SOURCE_BRANCH>`, `--region <BRANCH_REGION>`, `--restore <BACKUP_NAME>`, `--seed-data`, `--wait` | Create a new branch on the specified database                    | Postgres, Vitess |
| `delete <DATABASE_NAME> <BRANCH_NAME>`                  | `--force`                                                                                                | Delete the specified branch from the a database                  | Postgres, Vitess |
| `diff <DATABASE_NAME> <BRANCH_NAME>`                    | `--web`                                                                                                  | Show the diff of the specified branch against the parent branch. | Vitess           |
| `list <DATABASE_NAME>`                                  | `--web`                                                                                                  | List all branches of a database                                  | Postgres, Vitess |
| `promote <DATABASE_NAME> <BRANCH_NAME>`                 |                                                                                                          | Promote a database branch to production                          | Vitess           |
| `refresh-schema <DATABASE_NAME> <BRANCH_NAME>`          |                                                                                                          | Refresh the schema for a database branch                         | Vitess           |
| `safe-migrations enable <DATABASE_NAME> <BRANCH_NAME>`  |                                                                                                          | Enables safe migrations for a database branch                    | Vitess           |
| `safe-migrations disable <DATABASE_NAME> <BRANCH_NAME>` |                                                                                                          | Disables safe migrations for a database branch                   | Vitess           |
| `schema <DATABASE_NAME> <BRANCH_NAME>`                  | `--web`                                                                                                  | Show the schema of a branch                                      | Vitess           |
| `show <DATABASE_NAME> <BRANCH_NAME>`                    | `--web`                                                                                                  | Show a specific backup of a branch                               | Postgres, Vitess |
| `switch <BRANCH_NAME> --database <DATABASE_NAME>`       | `--database <DATABASE_NAME>`\*, `--create`, `parent-branch <BRANCH_NAME>`                                | Switch to the specified branch                                   | Postgres, Vitess |

> \* *Flag is required*

#### Sub-command flag descriptions

Some of the sub-commands have additional flags unique to the sub-command. This section covers what each of those does. See the above table for which context.

| **Sub-command flag**            | **Description**                                                                                                   | **Applicable sub-commands**                |
| :------------------------------ | :---------------------------------------------------------------------------------------------------------------- | :----------------------------------------- |
| `--from <SOURCE_BRANCH>`        | Parent branch that you want to create a new branch off of                                                         | `create`                                   |
| `--region <BRANCH_REGION>`      | Region where database should be created                                                                           | `create`                                   |
| `--restore <BACKUP_NAME>`       | Create a new branch from a specified backup                                                                       | `create`                                   |
| `--seed-data`                   | Create a new branch and seed data using the [Data Branching® feature](/docs/vitess/schema-changes/data-branching) | `create`                                   |
| `--web`                         | Perform the action in your web browser                                                                            | `create`, `diff`, `list`, `schema`, `show` |
| `--wait`                        | Wait until the branch is ready                                                                                    | `create`                                   |
| `--major-version`               | The major version of the branch (Postgres only). Currently supports `17` or `18`.                                 | `create`                                   |
| `--database <DATABASE_NAME>`    | Specify the database name                                                                                         | `switch`                                   |
| `--create`                      | Create a new branch if it does not exist                                                                          | `switch`                                   |
| `--parent-branch <BRANCH_NAME>` | If a new branch is being created, use this to specify a parent branch. Default is `main`.                         | `switch`                                   |

<Note>
  The `--region` flag can not be used with `--restore` when creating a branch. Branch backups will be restored to their original region.
</Note>

### Available flags

| **Flag**                    | **Description**                       |
| :-------------------------- | :------------------------------------ |
| `-h`, `--help`              | View help for auth command            |
| `--org <ORGANIZATION_NAME>` | The organization for the current user |

### Global flags

| **Command**                     | **Description**                                                                      |
| :------------------------------ | :----------------------------------------------------------------------------------- |
| `--api-token <TOKEN>`           | The API token to use for authenticating against the PlanetScale API.                 |
| `--api-url <URL>`               | The base URL for the PlanetScale API. Default is `https://api.planetscale.com/`.     |
| `--config <CONFIG_FILE>`        | Config file. Default is `$HOME/.config/planetscale/pscale.yml`.                      |
| `--debug`                       | Enable debug mode.                                                                   |
| `-f`, `--format <FORMAT>`       | Show output in a specific format. Possible values: `human` (default), `json`, `csv`. |
| `--no-color`                    | Disable color output.                                                                |
| `--service-token <TOKEN>`       | The service token for authenticating.                                                |
| `--service-token-id <TOKEN_ID>` | The service token ID for authenticating.                                             |

## Examples

### The `list` sub-command with `--web` flag

**Command:**

```bash  theme={null}
pscale branch list <DATABASE_NAME> --web
```

**Output:**

Opens the Branches page, `<https://app.planetscale.com/org/database/branches>`, in browser.

### The `diff` sub-command

**Command:**

```bash  theme={null}
pscale branch diff <DATABASE_NAME> <BRANCH_NAME>
```

**Output:**

```sql  theme={null}
-- users --
+CREATE TABLE `users` (
+  `id` bigint unsigned NOT NULL AUTO_INCREMENT,
+  `name` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL,
+  PRIMARY KEY (`id`)
+) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;
```

This will return the diff against the parent branch.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale CLI commands: completion
Source: https://planetscale.com/docs/cli/completion



## Getting Started

Make sure to first [set up your PlanetScale developer environment](/docs/cli/planetscale-environment-setup). Once you've installed the `pscale` CLI, you can interact with PlanetScale and manage your databases straight from the command line.

## The `completion` command

This command allows you to generate a completion script for the specified shell.

**Usage:**

```bash  theme={null}
pscale completion <SUB-COMMAND> <FLAG>
```

### Available sub-commands

| **Sub-command** | **Description**                             | **Product** |
| :-------------- | :------------------------------------------ | :---------- |
| `bash`          | Generated completion for `bash` shell       | All         |
| `zsh`           | Generated completion for `zsh` shell        | All         |
| `fish`          | Generated completion for `fish` shell       | All         |
| `powershell`    | Generated completion for `powershell` shell | All         |

### Available flags

| **Flag**       | **Description**                    |
| :------------- | :--------------------------------- |
| `-h`, `--help` | View help for `completion` command |

### Global flags

| **Command**                     | **Description**                                                                      |
| :------------------------------ | :----------------------------------------------------------------------------------- |
| `--api-token <TOKEN>`           | The API token to use for authenticating against the PlanetScale API.                 |
| `--api-url <URL>`               | The base URL for the PlanetScale API. Default is `https://api.planetscale.com/`.     |
| `--config <CONFIG_FILE>`        | Config file. Default is `$HOME/.config/planetscale/pscale.yml`.                      |
| `--debug`                       | Enable debug mode.                                                                   |
| `-f`, `--format <FORMAT>`       | Show output in a specific format. Possible values: `human` (default), `json`, `csv`. |
| `--no-color`                    | Disable color output.                                                                |
| `--service-token <TOKEN>`       | The service token for authenticating.                                                |
| `--service-token-id <TOKEN_ID>` | The service token ID for authenticating.                                             |

## How to load completions

### Bash

```bash  theme={null}
source <(pscale completion bash)

# To load completions for each session, execute once:
# Linux:
pscale completion bash > /etc/bash_completion.d/pscale
# macOS:
pscale completion bash > /usr/local/etc/bash_completion.d/pscale
```

### Zsh

```bash  theme={null}
# If shell completion is not already enabled in your environment,
# you will need to enable it.  You can execute the following once:

echo "autoload -U compinit; compinit" >> ~/.zshrc

# To load completions for each session, execute once:
pscale completion zsh > "${fpath[1]}/_yourprogram"

# You will need to start a new shell for this setup to take effect.
```

### fish

```bash  theme={null}
pscale completion fish | source

# To load completions for each session, execute once:
pscale completion fish > ~/.config/fish/completions/pscale.fish
```

### PowerShell

```bash  theme={null}
pscale completion powershell | Out-String | Invoke-Expression

# To load completions for every new session, run:
pscale completion powershell > pscale.ps1
# and source this file from your PowerShell profile.
```

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale CLI commands: connect
Source: https://planetscale.com/docs/cli/connect



## Getting Started

Make sure to first [set up your PlanetScale developer environment](/docs/cli/planetscale-environment-setup). Once you've installed the `pscale` CLI, you can interact with PlanetScale and manage your databases straight from the command line.

## The `connect` command

This command creates a secure connection to a database branch for a local client.

**Usage:**

```bash  theme={null}
pscale connect <DATABASE_NAME> <BRANCH_NAME> <FLAG>
```

If there is only one branch available on the database, you can leave off `<BRANCH_NAME>`.

### Available flags

| **Flag**                           | **Description**                                                                                                                  |
| :--------------------------------- | :------------------------------------------------------------------------------------------------------------------------------- |
| `--execute <COMMAND>`              | Run the specified command after successfully connecting to the database.                                                         |
| `--execute-env-url <ENV_VAR_NAME>` | Environment variable name that contains the exposed Database URL. Default is `DATABASE_URL`.                                     |
| `--execute-protocol <PROTOCOL>`    | Protocol for the exposed URL (by default `DATABASE_URL`) value in execute. Default is `mysql2`.                                  |
| `-h`, `--help`                     | Help with the `connect` command.                                                                                                 |
| `--host <HOST>`                    | Local host to bind and listen for connections. Default is `127.0.0.1`.                                                           |
| `--org <ORG_NAME>`                 | The organization of the database you want to connect to.                                                                         |
| `--port <PORT>`                    | Local port to bind and listen for connections. Default is `3306`.                                                                |
| `--remote-addr <ADDRESS>`          | PlanetScale Database remote network address. By default, the remote address is automatically populated from the PlanetScale API. |
| `--role <ROLE>`                    | Define the access level [with a role](/docs/vitess/security/password-roles)                                                      |

Available roles for the `--role` flag are:

* `reader`
* `writer`
* `readwriter`
* `admin`

### Global flags

| **Command**                     | **Description**                                                                      |
| :------------------------------ | :----------------------------------------------------------------------------------- |
| `--api-token <TOKEN>`           | The API token to use for authenticating against the PlanetScale API.                 |
| `--api-url <URL>`               | The base URL for the PlanetScale API. Default is `https://api.planetscale.com/`.     |
| `--config <CONFIG_FILE>`        | Config file. Default is `$HOME/.config/planetscale/pscale.yml`.                      |
| `--debug`                       | Enable debug mode.                                                                   |
| `-f`, `--format <FORMAT>`       | Show output in a specific format. Possible values: `human` (default), `json`, `csv`. |
| `--no-color`                    | Disable color output.                                                                |
| `--service-token <TOKEN>`       | The service token for authenticating.                                                |
| `--service-token-id <TOKEN_ID>` | The service token ID for authenticating.                                             |

## Examples

### The `connect` command with `--execute` flag

**Command:**

```bash  theme={null}
pscale connect <DATABASE_NAME> <BRANCH_NAME> --execute 'node app.js'
```

**Output:**

This command connects to the specified PlanetScale branch and runs the `node app.js` command. Since no `--execute-env-url` flag was passed, it uses the default `DATABASE_URL` environment variable. You can find a full example of this in our [Node quickstart](/docs/vitess/tutorials/connect-nodejs-app).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale CLI commands: data-imports
Source: https://planetscale.com/docs/cli/data-imports



## Getting Started

Make sure to first [set up your PlanetScale developer environment](/docs/cli/planetscale-environment-setup). Once you've installed the `pscale` CLI, you can interact with PlanetScale and manage your databases straight from the command line.

## The `service-token` command

This command allows you to manage your [Vitess database import](/docs/vitess/imports/database-imports) from the PlanetScale CLI. This is not currently available for Postgres databases.

**Usage:**

```bash  theme={null}
pscale data-imports <SUB-COMMAND> <FLAG>
```

### Available sub-commands

| **Sub-command**            | **Sub-command flags**                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | **Description**                                                                  | **Product** |
| :------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------- | :---------- |
| `cancel <DATABASE_NAME>`   | `--name`\*, `--force`                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Cancel data import request                                                       | All         |
| `detach-external-database` | `--name`\*, `--force`                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Detach external database that is used as a source for PlanetScale database       | All         |
| `get <DATABASE_NAME>`      | `--name`\*                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | Get the current state of a data import request into a PlanetScale database       | All         |
| `lint`                     | `--database <EXTERNAL_DATABASE_NAME>`\*, `--host <EXTERNAL_HOST_NAME>`\*, `--password <EXTERNAL_DATABASE_PASS>`\*, `--ssl-mode <SSL_MODE>`\*, `--username <EXTERNAL_DATABASE_USER>`\*, `--port <PORT>`, `--ssl-certificate-authority <SSL_CA>`, `--ssl-client-certificate <SSL_CERT>`, `--ssl-client-key <SSL_CLIENT_KEY>`, `--ssl-server-name <SSL_SERVER_NAME>`                                                                                                                         | Lint external database for compatibility with PlanetScale                        | All         |
| `make-primary`             | `--name`\*, `--force`                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Mark PlanetScale's database as the Primary, and the external database as Replica | All         |
| `make-replica`             | `--name`\*, `--force`                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | Mark PlanetScale's database as the Replica, and the external database as Primary | All         |
| `start`                    | `--database <EXTERNAL_DATABASE_NAME>`\*, `--host <EXTERNAL_HOST_NAME_>`\*, `--port <EXTERNAL_PORT_NUMBER>`, `--name <PLANETSCALE_DATABASE_NAME>`\*, `--username <EXTERNAL_DATABASE_USERNAME>`\*, `--password <EXTERNAL_DATABASE_PASSWORD>`\*, `--region <PLANETSCALE_DATABASE_REGION>`, `--ssl-certificate-authority <SSL_CA>`, `--ssl-client-certificate <SSL_CERT>`, `--ssl-client-key <SSL_CLIENT_KEY>`, `--ssl-mode <SSL_MODE>`\*, `--ssl-server-name <SSL_SERVER_NAME>`, `--dry-run` | Start importing data from an external database                                   | All         |

> \* *Flag is required*

#### Sub-command flag descriptions

Some of the sub-commands have additional flags unique to the sub-command. This section covers what each of those does. See the above table for which context.

| **Sub-command flag**                      | **Description**                                                                                            | **Applicable sub-commands**                                                          |
| :---------------------------------------- | :--------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------- |
| `--database <PLANETSCALE_DATABASE_NAME>`  | Name of the PlanetScale database for which you wish to cancel the import                                   | `cancel`, `lint`                                                                     |
| `--name <PLANETSCALE_DATABASE_NAME>`      | Name of the PlanetScale database you are importing into                                                    | `cancel`, `get`, `detach-external-database`, `make-primary`, `make-replica`, `start` |
| `--force`                                 | Make PlanetScale database replica or primary without confirmation                                          | `cancel`, `detach-external-database`, `make-primary`, `make-replica`                 |
| `--database <EXTERNAL_DATABASE_NAME>`     | Name of the external database                                                                              | `lint`, `start`                                                                      |
| `--dry-run`                               | Only run compatibility check; do not start the import (true by default)                                    | `start`                                                                              |
| `--host <EXTERNAL_HOST_NAME_>`            | Host name of the external database                                                                         | `start`, `lint`                                                                      |
| `--port <EXTERNAL_PORT_NUMBER>`           | Port number to connect to external database (default 3306)                                                 | `start`, `lint`                                                                      |
| `--username <EXTERNAL_DATABASE_USERNAME>` | Username of the external database you are importing from                                                   | `start`, `lint`                                                                      |
| `--password <EXTERNAL_DATABASE_PASSWORD>` | External database password                                                                                 | `start`, `lint`                                                                      |
| `--region <PLANETSCALE_DATABASE_REGION>`  | Region of the PlanetScale database you're importing into                                                   | `start`                                                                              |
| `--ssl-certificate-authority <SSL_CA>`    | The full CA certificate chain                                                                              | `start`, `lint`                                                                      |
| `--ssl-client-certificate <SSL_CERT>`     | Client Certificate to authenticate PlanetScale with your database server                                   | `start`, `lint`                                                                      |
| `--ssl-client-key <SSL_CLIENT_KEY>`       | Private key for the client certificate                                                                     | `start`, `lint`                                                                      |
| `--ssl-mode <SSL_MODE>`                   | SSL verification mode, allowed values: `disabled`, `preferred`, `required`, `verify_ca`, `verify_identity` | `start`, `lint`                                                                      |
| `--ssl-server-name <SSL_SERVER_NAME>`     | SSL server name override                                                                                   | `start`, `lint`                                                                      |

### Available flags

| **Flag**                    | **Description**                       |
| :-------------------------- | :------------------------------------ |
| `-h`, `--help`              | View help for `service-token` command |
| `--org <ORGANIZATION_NAME>` | The organization for the current user |

### Global flags

| **Command**                     | **Description**                                                                      |
| :------------------------------ | :----------------------------------------------------------------------------------- |
| `--api-token <TOKEN>`           | The API token to use for authenticating against the PlanetScale API.                 |
| `--api-url <URL>`               | The base URL for the PlanetScale API. Default is `https://api.planetscale.com/`.     |
| `--config <CONFIG_FILE>`        | Config file. Default is `$HOME/.config/planetscale/pscale.yml`.                      |
| `--debug`                       | Enable debug mode.                                                                   |
| `-f`, `--format <FORMAT>`       | Show output in a specific format. Possible values: `human` (default), `json`, `csv`. |
| `--no-color`                    | Disable color output.                                                                |
| `--org`                         | The organization for the current user.                                               |
| `--service-token <TOKEN>`       | The service token for authenticating.                                                |
| `--service-token-id <TOKEN_ID>` | The service token ID for authenticating.                                             |

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale CLI commands: database
Source: https://planetscale.com/docs/cli/database



## Getting Started

Make sure to first [set up your PlanetScale developer environment](/docs/cli/planetscale-environment-setup). Once you've installed the `pscale` CLI, you can interact with PlanetScale and manage your databases straight from the command line.

## The `database` command

This command allows you to create, read, delete, dump, and restore databases.

**Usage:**

```bash  theme={null}
pscale database <SUB-COMMAND> <FLAG>
```

### Available sub-commands

| **Sub-command**                              | **Sub-command flags**                                                                                                                                                                                                                                                                | **Description**                                            | **Product**      |
| :------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------- | :--------------- |
| `create <DATABASE_NAME>`                     | `--region <REGION_NAME>`, `--plan <PLAN>`, `--cluster_size <CLUSTER_SIZE>`                                                                                                                                                                                                           | Create a database with the specified name                  | Postgres, Vitess |
| `delete <DATABASE_NAME>`                     | `--force`                                                                                                                                                                                                                                                                            | Delete the specified database                              | Postgres, Vitess |
| `dump <DATABASE_NAME> <BRANCH_NAME>`         | `--local-addr <ADDRESS>`, `--output <DIRECTORY_NAME>`, `--tables <TABLES_LIST>`, `--threads <NUMBER_OF_THREADS> (defaults to 16)`                                                                                                                                                    | Backup and dump the specified database                     | Vitess           |
| `list <DATABASE_NAME>`                       |                                                                                                                                                                                                                                                                                      | List all databases in the current org                      | Postgres, Vitess |
| `restore-dump <DATABASE_NAME> <BRANCH_NAME>` | `--dir <DIRECTORY_NAME>`\*, `--local-addr <ADDRESS>`, `--overwrite-tables`, `--threads <NUMBER_OF_THREADS> (defaults to 1)`, `--allow-different-destination`, `--show-details`, `--schema-only`, `--data-only`, `--starting-table <STARTING_TABLE>`, `--ending-table <ENDING_TABLE>` | Restore the specified database from a local dump directory | Postgres, Vitess |
| `show <DATABASE_NAME>`                       | `--web`                                                                                                                                                                                                                                                                              | Retrieve information about a database                      | Postgres, Vitess |

> \* *Flag is required*

#### Sub-command flag descriptions

Some of the sub-commands have additional flags unique to the sub-command. This section covers what each of those does. See the above table for which context.

| **Sub-command flag**            | **Description**                                                                                                                       | **Applicable sub-commands** |
| :------------------------------ | :------------------------------------------------------------------------------------------------------------------------------------ | :-------------------------- |
| `--region`                      | Specify the [region](/docs/vitess/regions) of the new database. Default is `us-east`.                                                 | `create`                    |
| `--plan`                        | Specify the plan for the databas. Currently, `scaler_pro` is the only option and the default.                                         | `create`                    |
| `--cluster_size`                | For Scaler Pro databases, you may specify the cluster size. Default is `PS_10`                                                        | `create`                    |
| `--major-version`               | The major version of the database (Postgres only). Currently supports `17` or `18`.                                                   | `create`                    |
| `--force`                       | Delete a database without confirmation.                                                                                               | `delete`                    |
| `--local-addr <ADDRESS>`        | Local address to bind and listen for connections. By default the proxy binds to 127.0.0.1 with a random port.                         | `dump`, `restore-dump`      |
| `--threads`                     | Number of concurrent threads to use                                                                                                   | `dump`, `restore-dump`      |
| `--output <DIRECTORY_NAME>`     | Output directory of the dump. By default the dump is saved to a folder in the current directory.                                      | `dump`                      |
| `--tables <TABLES_LIST>`        | Comma separated string of tables to dump. By default, all tables are dumped.                                                          | `dump`                      |
| `--wheres string`               | Comma separated string of WHERE clauses to filter the tables to dump.                                                                 | `dump`                      |
| `--replica`                     | Dump from a replica (if available; will fail if not).                                                                                 | `dump`                      |
| `--rdonly`                      | Dump from a rdonly tablet (if available; will fail if not).                                                                           | `dump`                      |
| `--keyspace <KEYSPACE_NAME>`    | Optionally target a specific keyspace to be dumped. Useful for sharded databases.                                                     | `dump`                      |
| `--shard <SHARD_NAME>`          | Optional shard to target, must be used with keyspace                                                                                  | `dump`                      |
| `--output-format <FORMAT>`      | Output format for the dump. Options: `sql` (default), `json`, `csv`.                                                                  | `dump`                      |
| `--dir <DIRECTORY_NAME>`        | Directory containing the files to be used for the restore.                                                                            | `restore-dump`              |
| `--overwrite-tables`            | If true, will attempt to DROP TABLE before restoring.                                                                                 | `restore-dump`              |
| `--allow-different-destination` | If true, will allow you to restore the files to a database with a different name without needing to rename the existing dump's files. | `restore-dump`              |
| `--show-details`                | If true, will add extra output during the restore process.                                                                            | `restore-dump`              |
| `--schema-only`                 | If true, will only restore the schema files during the restore process.                                                               | `restore-dump`              |
| `--data-only`                   | If true, will only restore the data files during the restore process.                                                                 | `restore-dump`              |
| `--starting-table <TABLE_NAME>` | Table to start from for the restore (useful for restarting from a certain point)                                                      | `restore-dump`              |
| `--ending-table <TABLE_NAME>`   | Table to end at for the restore (useful for stopping restore at a certain point)                                                      | `restore-dump`              |
| `--web`                         | Perform the action in your web browser                                                                                                | `show`                      |

### Available flags

| **Flag**                    | **Description**                                              |
| :-------------------------- | :----------------------------------------------------------- |
| `-h`, `--help`              | Get help with the `database` command                         |
| `--org <ORGANIZATION_NAME>` | Specify the organization for the database you're acting upon |

### Global flags

| **Command**                     | **Description**                                                                      |
| :------------------------------ | :----------------------------------------------------------------------------------- |
| `--api-token <TOKEN>`           | The API token to use for authenticating against the PlanetScale API.                 |
| `--api-url <URL>`               | The base URL for the PlanetScale API. Default is `https://api.planetscale.com/`.     |
| `--config <CONFIG_FILE>`        | Config file. Default is `$HOME/.config/planetscale/pscale.yml`.                      |
| `--debug`                       | Enable debug mode.                                                                   |
| `-f`, `--format <FORMAT>`       | Show output in a specific format. Possible values: `human` (default), `json`, `csv`. |
| `--no-color`                    | Disable color output.                                                                |
| `--service-token <TOKEN>`       | The service token for authenticating.                                                |
| `--service-token-id <TOKEN_ID>` | The service token ID for authenticating.                                             |

<Note>
  The `--format` flag does not apply to the database dump files created by the `dump` subcommand. However, you can control the output format using the `--output-format` flag, which supports SQL (default), JSON, and CSV formats. When using SQL format, dumps are compatible with [mydumper](https://github.com/mydumper/mydumper).
</Note>

## Examples

### Create a new `scaler_pro` database

**Command:**

```bash  theme={null}
pscale database create new-database --region <REGION_NAME> --plan scaler_pro --cluster_size PS_80
```

**Output:**

Database `new-database` was successfully created.

### Create a dump of an existing branch:

This command is only available for Vitess databases. For Postgres databases, use the [pg\_dump](https://www.postgresql.org/docs/current/app-pgdump.html) command instead.

**Command:**

```bash  theme={null}
pscale database dump <CURRENT_DATABASE_NAME> <BRANCH_NAME> --output="<DIRECTORY_FOR_BACKUP>" --org="<ORIGINAL_ORGANIZATION>"
```

**Output:**

A local export of your database will be generated within the current directory by default but since we are providing an `--output` location above that will be used instead.

### Export data in different formats:

You can specify the output format when dumping your Vitess database using the `--output-format` flag:

**Export as JSON:**

```bash  theme={null}
pscale database dump <DATABASE_NAME> <BRANCH_NAME> --output-format=json
```

**Export as CSV:**

```bash  theme={null}
pscale database dump <DATABASE_NAME> <BRANCH_NAME> --output-format=csv
```

**Export as SQL (default):**

```bash  theme={null}
pscale database dump <DATABASE_NAME> <BRANCH_NAME> --output-format=sql
```

### Restore a backup to an existing branch:

**Command:**

```bash  theme={null}
pscale database restore-dump <DESTINATION_DATABASE_NAME> <BRANCH_NAME> --dir="<DIRECTORY_FOR_BACKUP>" --org="<DESTINATION_ORGANIZATION>"
```

**Output:**

You should receive output indicating the restore is progressing until it completes successfully.

The command above will allow you to restore an existing backup to another branch located either within the same organization/database as the original, or within a completely different organization/database.

As of `pscale` v0.218.0 or newer the `--allow-different-destination` flag is now available. If this flag is provided it will make the steps below about renaming the files unnecessary.

If you opt to import into a database with a different name you will have to make sure you rename the files from your backup beforehand.

For example, the files will be named something like this:

```bash  theme={null}
<CURRENT_DATABASE_NAME>.<TABLE_NAME>-schema.sql
```

And you will want to rename all of the files in the dump folder to have the new database name if it is not the same as the existing one:

```bash  theme={null}
<DESTINATION_DATABASE_NAME>.<TABLE_NAME>-schema.sql
```

If importing into a branch that already contains table definitions that you want to overwrite, you may also be required to pass in the optional `--overwrite-tables` flag.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale CLI commands: deploy-request
Source: https://planetscale.com/docs/cli/deploy-request



## Getting Started

Make sure to first [set up your PlanetScale developer environment](/docs/cli/planetscale-environment-setup). Once you've installed the `pscale` CLI, you can interact with PlanetScale and manage your databases straight from the command line.

## The `deploy-request` command

This command allows you to create, review, diff, and manage deploy requests for your Vitess clusters. This command is not currently available for Postgres database clusters.

**Usage:**

```bash  theme={null}
pscale deploy-request <SUB-COMMAND> <FLAG>
```

<Note>
  Your database must have a production branch with [safe migrations](/docs/vitess/schema-changes/safe-migrations) enabled before you can create a deploy request.
</Note>

### Available sub-commands

| Sub-command                                       | Sub-command flags                                                                       | Description                                              | **Product** |
| :------------------------------------------------ | :-------------------------------------------------------------------------------------- | :------------------------------------------------------- | :---------- |
| `apply <DATABASE_NAME> <DR_NUMBER>`               |                                                                                         | Trigger a deploy request to swap over to the new schema. | All         |
| `cancel <DATABASE_NAME> <DR_NUMBER>`              |                                                                                         | Cancel a deploy request.                                 | All         |
| `close <DATABASE_NAME> <DR_NUMBER>`               |                                                                                         | Close the specified deploy request.                      | All         |
| `create <DATABASE_NAME> <BRANCH_NAME>`            | `--into <BRANCH_NAME>`, `--notes <NOTE>`, `--enable-auto-apply`, `--disable-auto-apply` | Create a new deploy request.                             | All         |
| `deploy <DATABASE_NAME> <DR_NUMBER\|BRANCH_NAME>` | `--instant`                                                                             | Deploy the specified deploy request.                     | All         |
| `diff <DATABASE_NAME> <DR_NUMBER>`                | `--web`                                                                                 | Show the diff of the specified deploy request.           | All         |
| `edit <DATABASE_NAME> <DR_NUMBER>`                | `--enable-auto-apply`, `--disable-auto-apply`                                           | Edit a deploy request.                                   | All         |
| `list <DATABASE_NAME>`                            | `--web`                                                                                 | List all deploy requests for a database.                 | All         |
| `revert <DATABASE_NAME> <DR_NUMBER>`              |                                                                                         | Revert a deployed deploy request.                        | All         |
| `review <DATABASE_NAME> <DR_NUMBER>`              | `--web`, `--approve`, `--comment <COMMENT>`                                             | Approve or comment on a deploy request.                  | All         |
| `show <DATABASE_NAME> <DR_NUMBER\|BRANCH_NAME>`   | `--web`                                                                                 | Show the specified deploy request.                       | All         |
| `skip-revert <DATABASE_NAME> <DR_NUMBER>`         |                                                                                         | Skip and close a pending deploy request revert.          | All         |

> \* *Flag is required*

The value `<DR_NUMBER>` represents the deploy request number (not to be confused with `id`). To see a deploy request number, run `pscale deploy-request list <DATABASE_NAME>`.

You can also find the number in the PlanetScale dashboard in the URL of the specified deploy request: `https://app.planetscale.com/<ORGANIZATION>/<DATABASE>/deploy-requests/<DR_NUMBER>`.

#### Sub-command flag descriptions

Some of the sub-commands have additional flags unique to the sub-command. This section covers what each of those does. See the above table for which context.

| Sub-command flag       | Description                                                                                                                         | Applicable sub-commands |
| :--------------------- | :---------------------------------------------------------------------------------------------------------------------------------- | :---------------------- |
| `--into <BRANCH_NAME>` | Specify that the new deploy request deploy to a specified branch. Default is `main`.                                                | `create`                |
| `--notes <NOTE>`       | A note describing the deploy request. Acts as the first comment.                                                                    | `create`                |
| `--enable-auto-apply`  | Enable auto-apply for this deploy request. When enabled, the deploy request will swap over to the new schema once ready.            | `create`, `edit`        |
| `--disable-auto-apply` | Disable auto-apply for this deploy request. If neither flag is provided, the setting is inherited from the previous deploy request. | `create`, `edit`        |
| `--web`                | Perform the action in your web browser                                                                                              | `diff`, `list`, `show`  |
| `--approve`            | Approve a deploy request                                                                                                            | `review`                |
| `--comment <COMMENT>`  | Leave a comment on a deploy request                                                                                                 | `review`                |
| `--instant`            | Deploy a deploy request using MySQL's built-in ALGORITHM=INSTANT option. Deployment will be faster, but cannot be reverted.         | `deploy`                |

### Available flags

| Flag                        | Description                                                        |
| :-------------------------- | :----------------------------------------------------------------- |
| `-h`, `--help`              | Get help with the `deploy-request` command                         |
| `--org <ORGANIZATION_NAME>` | Specify the organization for the deploy request you're acting upon |

### Global flags

| Command                         | Description                                                                          |
| :------------------------------ | :----------------------------------------------------------------------------------- |
| `--api-token <TOKEN>`           | The API token to use for authenticating against the PlanetScale API.                 |
| `--api-url <URL>`               | The base URL for the PlanetScale API. Default is `https://api.planetscale.com/`.     |
| `--config <CONFIG_FILE>`        | Config file. Default is `$HOME/.config/planetscale/pscale.yml`.                      |
| `--debug`                       | Enable debug mode.                                                                   |
| `-f`, `--format <FORMAT>`       | Show output in a specific format. Possible values: `human` (default), `json`, `csv`. |
| `--no-color`                    | Disable color output.                                                                |
| `--service-token <TOKEN>`       | The service token for authenticating.                                                |
| `--service-token-id <TOKEN_ID>` | The service token ID for authenticating.                                             |

## Examples

### The `deploy-request` command with `review` subcommand and `--comment` flag

**Command:**

```bash  theme={null}
pscale deploy-request review <DATABASE_NAME> 1 --comment 'Lets wait on this.'
```

**Output:**

A comment is added to the deploy request `<DATABASE_NAME>`/1.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale CLI commands: keyspace
Source: https://planetscale.com/docs/cli/keyspace



## Getting started

Make sure to first [set up your PlanetScale developer environment](/docs/cli/planetscale-environment-setup). Once you've installed the `pscale` CLI, you can interact with PlanetScale and manage your databases straight from the command line.

## The `keyspace` command

This command allows you to view your keyspaces and view or update your Vitess VSchemas. This command is not available for Postgres databases.

**Usage:**

```bash  theme={null}
pscale keyspace <SUB-COMMAND> <FLAG>
```

### Available sub-commands

| **Sub-command**                                                 | **Sub-command flags**                                                                                                                                                                                       | **Product** | **Description**                                                          |
| :-------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------- | :----------------------------------------------------------------------- |
| `create <DATABASE_NAME> <BRANCH_NAME> <KEYSPACE_NAME>`          | `--cluster-size <SIZE>`, `--additional-replicas <NUMBER>`, `--shards <NUMBER>`                                                                                                                              | Vitess      | Create a new keyspace within a database branch.                          |
| `list <DATABASE_NAME> <BRANCH_NAME>`                            |                                                                                                                                                                                                             | Vitess      | List all keyspaces within a database branch.                             |
| `show <DATABASE_NAME> <BRANCH_NAME> <KEYSPACE_NAME>`            |                                                                                                                                                                                                             | Vitess      | Show a specific keyspace within a database branch.                       |
| `resize <DATABASE_NAME> <BRANCH_NAME> <KEYSPACE_NAME>`          | `--cluster-size <SIZE>`, `--additional-replicas <NUMBER>`                                                                                                                                                   | Vitess      | Resize a keyspace.                                                       |
| `resize cancel <DATABASE_NAME> <BRANCH_NAME> <KEYSPACE_NAME>`   |                                                                                                                                                                                                             | Vitess      | Cancel an ongoing keyspace resize.                                       |
| `resize status <DATABASE_NAME> <BRANCH_NAME> <KEYSPACE_NAME>`   |                                                                                                                                                                                                             | Vitess      | Show the status of the keyspace's last resize.                           |
| `settings <DATABASE_NAME> <BRANCH_NAME> <KEYSPACE_NAME>`        |                                                                                                                                                                                                             | Vitess      | Show the settings for a keyspace.                                        |
| `update-settings <DATABASE_NAME> <BRANCH_NAME> <KEYSPACE_NAME>` | `-i`, `--interactive`, `--replication-durability-constraints-strategy <STRATEGY>`, `--vreplication-batch-replication-events`, `--vreplication-enable-noblob-binlog-mode`, `--vreplication-optimize-inserts` | Vitess      | Update the settings for a keyspace.                                      |
| `vschema show <DATABASE_NAME> <BRANCH_NAME> <KEYSPACE_NAME>`    |                                                                                                                                                                                                             | Vitess      | Show the VSchema for a sharded keyspace. Empty on non-sharded keyspaces. |
| `vschema update <DATABASE_NAME> <BRANCH_NAME> <KEYSPACE_NAME>`  | `--vschema <FILE>`\*                                                                                                                                                                                        | Vitess      | Update a VSchema of a keyspace.                                          |
| `rollout-status <DATABASE_NAME> <BRANCH_NAME> <KEYSPACE_NAME>`  |                                                                                                                                                                                                             | Vitess      | Check the status of a keyspace resize request.                           |

> \* *Flag is required*

#### Sub-command flag descriptions

| **Sub-command flag**                                       | **Description**                                                                                                       | **Applicable sub-commands** |
| :--------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------- | :-------------------------- |
| `--additional-replicas <NUMBER>`                           | `<NUMBER>` is the number of replicas to add to the keyspace. By default, production branches include 2 replicas.      | `create`, `resize`          |
| `--cluster-size <SIZE>`                                    | `<SIZE>` is the size of the database cluster.                                                                         | `create`, `resize`          |
| `-i, --interactive`                                        | Run the command in interactive mode.                                                                                  | `update-settings`           |
| `--replication-durability-constraints-strategy <STRATEGY>` | Replication strategy to use. Options: maximum, dynamic, minimum (default "maximum").                                  | `update-settings`           |
| `--shards <NUMBER>`                                        | Number of shards in the keyspace (default 1).                                                                         | `create`                    |
| `--vreplication-batch-replication-events`                  | When enabled, sends fewer queries to MySQL to improve performance.                                                    | `update-settings`           |
| `--vreplication-enable-noblob-binlog-mode`                 | When enabled, omits changed BLOB and TEXT columns from replication events, which reduces binlog sizes. (default true) | `update-settings`           |
| `--vreplication-optimize-inserts`                          | When enabled, skips sending INSERT events for rows that have yet to be replicated. (default true)                     | `update-settings`           |
| `--vschema <FILE>`                                         | `<FILE>` is the path to the updated VSchema file.                                                                     | `vschema update`            |

### Available flags

| **Flag**                    | **Description**                       |
| :-------------------------- | :------------------------------------ |
| `-h`, `--help`              | View help for auth command            |
| `--org <ORGANIZATION_NAME>` | The organization for the current user |

### Global flags

| **Command**                     | **Description**                                                                      |
| :------------------------------ | :----------------------------------------------------------------------------------- |
| `--api-token <TOKEN>`           | The API token to use for authenticating against the PlanetScale API.                 |
| `--api-url <URL>`               | The base URL for the PlanetScale API. Default is `https://api.planetscale.com/`.     |
| `--config <CONFIG_FILE>`        | Config file. Default is `$HOME/.config/planetscale/pscale.yml`.                      |
| `--debug`                       | Enable debug mode.                                                                   |
| `-f`, `--format <FORMAT>`       | Show output in a specific format. Possible values: `human` (default), `json`, `csv`. |
| `--no-color`                    | Disable color output.                                                                |
| `--service-token <TOKEN>`       | The service token for authenticating.                                                |
| `--service-token-id <TOKEN_ID>` | The service token ID for authenticating.                                             |

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale CLI commands: mcp
Source: https://planetscale.com/docs/cli/mcp



## Getting Started

Make sure to first [set up your PlanetScale developer environment](/docs/cli/planetscale-environment-setup). Once you've installed the `pscale` CLI, you can interact with PlanetScale and manage your databases straight from the command line.

## The `mcp` command

This command installs and enables support for a PlanetScale MCP server.

**Usage:**

```bash  theme={null}
pscale mcp <SUB-COMMAND> <FLAG>
```

### Available sub-commands

| **Sub-command** | **Product**      | **Description**        |
| :-------------- | :--------------- | :--------------------- |
| `install`       | Postgres, Vitess | Install the MCP server |
| `server`        | Postgres, Vitess | Start the MCP server   |

### Available flags

| **Flag**       | **Description**                                                                                  |
| :------------- | :----------------------------------------------------------------------------------------------- |
| `--target`     | The target installation for the MCP server. `claude` and `cursor` are the only supported values. |
| `-h`, `--help` | View help for `mcp` command                                                                      |

### Global flags

| **Command**                     | **Description**                                                                      |
| :------------------------------ | :----------------------------------------------------------------------------------- |
| `--api-token <TOKEN>`           | The API token to use for authenticating against the PlanetScale API.                 |
| `--api-url <URL>`               | The base URL for the PlanetScale API. Default is `https://api.planetscale.com/`.     |
| `--config <CONFIG_FILE>`        | Config file. Default is `$HOME/.config/planetscale/pscale.yml`.                      |
| `--debug`                       | Enable debug mode.                                                                   |
| `-f`, `--format <FORMAT>`       | Show output in a specific format. Possible values: `human` (default), `json`, `csv`. |
| `--no-color`                    | Disable color output.                                                                |
| `--service-token <TOKEN>`       | The service token for authenticating.                                                |
| `--service-token-id <TOKEN_ID>` | The service token ID for authenticating.                                             |

## Examples

### The `mcp` command with `list` sub-command

**Command:**

```bash  theme={null}
pscale mcp install --target cursor
```

**Output:**

```bash  theme={null}
MCP server successfully configured for cursor at /Users/your-name/.cursor/mcp.json
```

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale CLI commands: org
Source: https://planetscale.com/docs/cli/org



## Getting Started

Make sure to first [set up your PlanetScale developer environment](/docs/cli/planetscale-environment-setup). Once you've installed the `pscale` CLI, you can interact with PlanetScale and manage your databases straight from the command line.

## The `org` command

This command allows you to list, show, and switch [organizations](/docs/security/access-control#organization-member).

**Usage:**

```bash  theme={null}
pscale org <SUB-COMMAND> <FLAG>
```

### Available sub-commands

| **Sub-command**              | **Sub-command flags**  | **Product**      | **Description**                                         |
| :--------------------------- | :--------------------- | :--------------- | :------------------------------------------------------ |
| `list`                       |                        | Postgres, Vitess | List all currently active organizations with timestamps |
| `show`                       |                        | Postgres, Vitess | Display the currently active organization               |
| `switch <ORGANIZATION_NAME>` | `--save-config <PATH>` | Postgres, Vitess | Switch the currently active organization                |

#### Sub-command flag descriptions

Some of the sub-commands have additional flags unique to the sub-command. This section covers what each of those does. See the above table for which context.

| **Sub-command flag**   | **Description**                                                                                                             | **Applicable sub-commands** |
| :--------------------- | :-------------------------------------------------------------------------------------------------------------------------- | :-------------------------- |
| `--save-config <PATH>` | Path to store the organization. By default, the configuration is automatically deduced based on where `pscale` is executed. | `switch`                    |

### Available flags

| **Flag**       | **Description**             |
| -------------- | --------------------------- |
| `-h`, `--help` | View help for `org` command |

### Global flags

| **Command**                     | **Description**                                                                                                                                     |
| :------------------------------ | :-------------------------------------------------------------------------------------------------------------------------------------------------- |
| `--api-token <TOKEN>`           | The API token to use for authenticating against the PlanetScale API.                                                                                |
| `--api-url <URL>`               | The base URL for the PlanetScale API. Default is `https://api.planetscale.com/`.                                                                    |
| `--config <CONFIG_FILE>`        | Config file. Default is `$HOME/.config/planetscale/pscale.yml`. Local override inside a Git repository is `$CWD/.pscale.yml` in the project's root. |
| `--debug`                       | Enable debug mode.                                                                                                                                  |
| `-f`, `--format <FORMAT>`       | Show output in a specific format. Possible values: `human` (default), `json`, `csv`.                                                                |
| `--no-color`                    | Disable color output.                                                                                                                               |
| `--service-token <TOKEN>`       | The service token for authenticating.                                                                                                               |
| `--service-token-id <TOKEN_ID>` | The service token ID for authenticating.                                                                                                            |

## Examples

### The `org` command with `switch` sub-command

**Command:**

```bash  theme={null}
pscale org switch <ORGANIZATION_NAME>
```

**Output:**

Successfully switched to organization `<ORGANIZATION_NAME>` (using file: `/Users/name/.config/planetscale/pscale.yml`)

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale CLI commands: password
Source: https://planetscale.com/docs/cli/password



## Getting Started

Make sure to first [set up your PlanetScale developer environment](/docs/cli/planetscale-environment-setup). Once you've installed the `pscale` CLI, you can interact with PlanetScale and manage your databases straight from the command line.

## The `password` command

This command allows you to create, list, and delete branch passwords/credentials.

**Usage:**

```bash  theme={null}
pscale password <SUB-COMMAND> <FLAG>
```

### Available sub-commands

| **Sub-command**                                        | **Sub-command flags** | **Product** | **Description**                                  |
| :----------------------------------------------------- | :-------------------- | :---------- | :----------------------------------------------- |
| `create <DATABASE_NAME> <BRANCH_NAME> <PASSWORD_NAME>` | `--ttl`, `--role`     | Vitess      | Create new credentials to access a branch's data |
| `delete <DATABASE_NAME> <BRANCH_NAME> <PASSWORD_ID>`   | `--force`             | Vitess      | Delete the specified branch credentials          |
| `list <DATABASE_NAME> <BRANCH_NAME>`                   | `--web`               | Vitess      | List all credentials of a database               |

The value `<PASSWORD_ID>` represents the ID number of the set of credentials. To find all available credentials and their IDs, run `pscale list <DATABASE_NAME> <BRANCH_NAME>`.

#### Sub-command flag descriptions

Some of the sub-commands have additional flags unique to the sub-command. This section covers what each of those does. See the above table for which context.

| **Sub-command flag** | **Description**                                                                                                  | **Applicable sub-commands** |
| :------------------- | :--------------------------------------------------------------------------------------------------------------- | :-------------------------- |
| `--ttl`              | TTL defines the time to live for the password in seconds. By default, it is 0, which means it will never expire. | `create`                    |
| `--role <ROLE>`      | Add a [role to a password](/docs/vitess/security/password-roles)                                                 | `create`                    |
| `--force`            | Delete a password without confirmation.                                                                          | `delete`                    |
| `--web`              | Perform the action in your web browser                                                                           | `list`                      |

Available roles for the `--role` flag are:

* `reader`
* `writer`
* `readwriter`
* `admin`

### Available flags

| **Flag**                    | **Description**                       |
| :-------------------------- | :------------------------------------ |
| `-h`, `--help`              | View help for `password` command      |
| `--org <ORGANIZATION_NAME>` | The organization for the current user |

### Global flags

| **Command**                     | **Description**                                                                      |
| :------------------------------ | :----------------------------------------------------------------------------------- |
| `--api-token <TOKEN>`           | The API token to use for authenticating against the PlanetScale API.                 |
| `--api-url <URL>`               | The base URL for the PlanetScale API. Default is `https://api.planetscale.com/`.     |
| `--config <CONFIG_FILE>`        | Config file. Default is `$HOME/.config/planetscale/pscale.yml`.                      |
| `--debug`                       | Enable debug mode.                                                                   |
| `-f`, `--format <FORMAT>`       | Show output in a specific format. Possible values: `human` (default), `json`, `csv`. |
| `--no-color`                    | Disable color output.                                                                |
| `--service-token <TOKEN>`       | The service token for authenticating.                                                |
| `--service-token-id <TOKEN_ID>` | The service token ID for authenticating.                                             |

## Examples

### The `password` command with `delete` sub-command

**Command:**

```bash  theme={null}
pscale password delete <DATABASE_NAME> <BRANCH_NAME> <PASSWORD_ID>
```

**Output:**

```bash  theme={null}
? Please type <DATABASE_NAME>/<BRANCH_NAME>/<PASSWORD_ID> to confirm:
Password <PASSWORD_ID> was successfully deleted from <BRANCH_NAME>.
```

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale CLI commands: ping
Source: https://planetscale.com/docs/cli/ping



## Getting Started

Make sure to first [set up your PlanetScale developer environment](/docs/cli/planetscale-environment-setup). Once you've installed the `pscale` CLI, you can interact with PlanetScale and manage your databases straight from the command line.

## The `ping` command

This command allows you to see the [latency](/docs/vitess/connecting/network-latency) between your machine and PlanetScale's public regions.

**Usage:**

```bash  theme={null}
pscale ping
```

### Available flags

| **Flag**           | **Description**                                                                               |
| :----------------- | :-------------------------------------------------------------------------------------------- |
| `-h`, `--help`     | View help for `ping` command                                                                  |
| `-n`, `--count`    | Number of total pings. (default 10)                                                           |
| `-p`, `--provider` | Only ping endpoints for the specified infrastructure provider. Possible values: `aws`, `gcp`. |
| `--concurrency`    | Number of concurrent pings. (default 8)                                                       |
| `--timeout`        | Timeout for a ping to succeed. (default 5s)                                                   |

### Global flags

| **Command**                     | **Description**                                                                      |
| :------------------------------ | :----------------------------------------------------------------------------------- |
| `--api-token <TOKEN>`           | The API token to use for authenticating against the PlanetScale API.                 |
| `--api-url <URL>`               | The base URL for the PlanetScale API. Default is `https://api.planetscale.com/`.     |
| `--config <CONFIG_FILE>`        | Config file. Default is `$HOME/.config/planetscale/pscale.yml`.                      |
| `--debug`                       | Enable debug mode.                                                                   |
| `-f`, `--format <FORMAT>`       | Show output in a specific format. Possible values: `human` (default), `json`, `csv`. |
| `--no-color`                    | Disable color output.                                                                |
| `--service-token <TOKEN>`       | The service token for authenticating.                                                |
| `--service-token-id <TOKEN_ID>` | The service token ID for authenticating.                                             |

## Examples

**Command:**

```bash  theme={null}
pscale ping
```

**Output:**

```shell  theme={null}
  NAME (17)                     LATENCY   ENDPOINT                                         TYPE
 ----------------------------- --------- ------------------------------------------------ -----------
  AWS us-west-2                 34.6ms    aws.connect.psdb.cloud                           optimized
  AWS us-west-2                 34.8ms    us-west.connect.psdb.cloud                       direct
  GCP us-central1               57.5ms    gcp.connect.psdb.cloud                           optimized
  GCP us-central1               57.9ms    gcp-us-central1.connect.psdb.cloud               direct
  AWS us-east-2                 60.5ms    aws-us-east-2.connect.psdb.cloud                 direct
  GCP us-east4                  69.2ms    gcp-us-east4.connect.psdb.cloud                  direct
  AWS us-east-1                 70.2ms    us-east.connect.psdb.cloud                       direct
  GCP northamerica-northeast1   80.9ms    gcp-northamerica-northeast1.connect.psdb.cloud   direct
  AWS ap-northeast-1            116.6ms   ap-northeast.connect.psdb.cloud                  direct
  GCP asia-northeast3           149.5ms   gcp-asia-northeast3.connect.psdb.cloud           direct
  AWS ap-southeast-2            150.9ms   aws-ap-southeast-2.connect.psdb.cloud            direct
  AWS eu-central-1              154.4ms   eu-central.connect.psdb.cloud                    direct
  AWS eu-west-1                 157.2ms   eu-west.connect.psdb.cloud                       direct
  AWS sa-east-1                 179.5ms   aws-sa-east-1.connect.psdb.cloud                 direct
  AWS ap-southeast-1            189.4ms   ap-southeast.connect.psdb.cloud                  direct
  AWS ap-south-1                243.1ms   ap-south.connect.psdb.cloud                      direct
  AWS eu-west-2                 670.5ms   aws-eu-west-2.connect.psdb.cloud                 direct
```

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale environment set up
Source: https://planetscale.com/docs/cli/planetscale-environment-setup



## Table of Contents

<CardGroup>
  <Card title="macOS installation" href="#macos-instructions" icon="square-1" horizontal />

  <Card title="Linux installation" href="#linux-instructions" icon="square-2" horizontal />

  <Card title="Windows installation" href="#windows-instructions" icon="square-3" horizontal />

  <Card title="Manual setup (any OS)" href="#manual-setup-any-os" icon="square-4" horizontal />

  <Card title="Using the PlanetScale CLI" href="#using-the-planetscale-cli" icon="square-5" horizontal />
</CardGroup>

## Setup overview

### macOS instructions

To install the PlanetScale CLI on macOS, we recommend using Homebrew.

How to install or verify Homebrew is on your computer:

<Steps>
  <Step>
    Open Terminal.
  </Step>

  <Step>
    Check if you have Homebrew installed by running the following command:

    ```bash  theme={null}
    brew -v
    ```
  </Step>

  <Step>
    If you don't see "Homebrew" and a version number, then download and [install Homebrew](https://brew.sh/).
  </Step>

  <Step>
    Once you've installed Homebrew, repeat *Step 2* to verify.
  </Step>
</Steps>

**Installing via Homebrew**

* Run the following command:

```bash  theme={null}
brew install planetscale/tap/pscale
```

* To install the MySQL command-line client:

```bash  theme={null}
brew install mysql-client
```

* To install the PostgreSQL command-line client:

```bash  theme={null}
brew install postgresql@17
```

To upgrade to the latest version:

```bash  theme={null}
brew upgrade pscale
```

### Linux instructions

`pscale` is available as downloadable binaries from the [PlanetScale releases](https://github.com/planetscale/cli/releases/latest) page.

Download the .deb or .rpm from the [releases](https://github.com/planetscale/cli/releases/latest) page and install with `sudo dpkg -i` and `sudo rpm -i` respectively.

The MySQL and PostgreSQL command-line clients can be installed via your package manager.

**MySQL client:**

* Debian-based distributions:

```bash  theme={null}
sudo apt-get install mysql-client
```

* RPM-based distributions:

```bash  theme={null}
sudo yum install community-mysql
```

**PostgreSQL client:**

* Debian-based distributions:

```bash  theme={null}
sudo apt-get install postgresql-client
```

* RPM-based distributions:

```bash  theme={null}
sudo yum install postgresql
```

### Windows instructions

On Windows, `pscale` is available via [scoop](https://scoop.sh/), and as a downloadable binary from the [PlanetScale releases](https://github.com/planetscale/cli/releases/latest) page:

```bash  theme={null}
scoop bucket add pscale https://github.com/planetscale/scoop-bucket.git
scoop install pscale mysql postgresql
```

To upgrade to the latest version:

```bash  theme={null}
scoop update pscale
```

**Installation via binary**

Download the latest [Windows release](https://github.com/planetscale/cli/releases/latest) and unzip the `pscale.exe` file into the folder of your choice. Then, run it from PowerShell or whatever terminal you regularly use.

**MySQL client setup:**

The MySQL command-line client is available in the [Windows MySQL Installer](https://dev.mysql.com/doc/refman/8.0/en/windows-installation.html). To launch `pscale shell` you will need to have the `mysql.exe` executable's directory in your shell's PATH.

In PowerShell, add that directory to your current shell's PATH:

```powershell  theme={null}
$env:path += ";C:\Program Files\MySQL\MySQL Server 8.0\bin"
```

**PostgreSQL client setup:**

The PostgreSQL command-line client is available from the [PostgreSQL downloads page](https://www.postgresql.org/download/windows/). After installation, you'll need to add the PostgreSQL bin directory to your PATH to use `psql`:

```powershell  theme={null}
$env:path += ";C:\Program Files\PostgreSQL\17\bin"
```

## Manual setup (any OS)

If you prefer to manually install the `pscale` binary for your operating system, the following two methods may be used.

### Download the binary

Download the pre-compiled binaries from the [PlanetScale releases](https://github.com/planetscale/cli/releases/latest) page and download the binary for your operating system to the desired location. The binary may be run using the terminal of your choice from that location.

### Install using `bin`

[bin](https://github.com/marcosnils/bin) is a cross-platform tool to manage binary files. You can install the `pscale` CLI using `bin` with the following command:

```bash  theme={null}
bin install https://github.com/planetscale/cli
```

### Install the MySQL or PostgreSQL Client

In either case, the MySQL or PostgreSQL client will need to be installed separately as well.

**MySQL client:** Refer to the [official MySQL documentation](https://dev.mysql.com/doc/refman/8.0/en/installing.html) and select the operating system you are working with.

**PostgreSQL client:** Refer to the [official PostgreSQL documentation](https://www.postgresql.org/download/) and select the operating system you are working with.

## Using the PlanetScale CLI

See all available commands by running:

```bash  theme={null}
pscale --help
```

Verify that you're using the latest version:

```bash  theme={null}
pscale version
```

You're all set! Check out our [CLI reference page](/docs/cli) to explore all that's possible with the PlanetScale CLI.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale CLI commands: region
Source: https://planetscale.com/docs/cli/region



## Getting Started

Make sure to first [set up your PlanetScale developer environment](/docs/cli/planetscale-environment-setup). Once you've installed the `pscale` CLI, you can interact with PlanetScale and manage your databases straight from the command line.

## The `region` command

This command lists all available [PlanetScale regions](/docs/vitess/regions).

**Usage:**

```bash  theme={null}
pscale region <SUB-COMMAND> <FLAG>
```

### Available sub-commands

| **Sub-command** | **Product**      | **Description**  |
| :-------------- | :--------------- | :--------------- |
| `list`          | Postgres, Vitess | List all regions |

### Available flags

| **Flag**       | **Description**                |
| :------------- | :----------------------------- |
| `-h`, `--help` | View help for `region` command |

### Global flags

| **Command**                     | **Description**                                                                      |
| :------------------------------ | :----------------------------------------------------------------------------------- |
| `--api-token <TOKEN>`           | The API token to use for authenticating against the PlanetScale API.                 |
| `--api-url <URL>`               | The base URL for the PlanetScale API. Default is `https://api.planetscale.com/`.     |
| `--config <CONFIG_FILE>`        | Config file. Default is `$HOME/.config/planetscale/pscale.yml`.                      |
| `--debug`                       | Enable debug mode.                                                                   |
| `-f`, `--format <FORMAT>`       | Show output in a specific format. Possible values: `human` (default), `json`, `csv`. |
| `--no-color`                    | Disable color output.                                                                |
| `--service-token <TOKEN>`       | The service token for authenticating.                                                |
| `--service-token-id <TOKEN_ID>` | The service token ID for authenticating.                                             |

## Examples

### The `region` command with `list` sub-command

**Command:**

```bash  theme={null}
pscale region list
```

**Output:**

```bash  theme={null}
  NAME                                             SLUG                         ENABLED
 ------------------------------------------------- ---------------------------- ---------
  AWS us-east-2 (Ohio)                             aws-us-east-2                 Yes
  AWS us-east-1 (N. Virginia)                      us-east                       Yes
  AWS us-west-2 (Oregon)                           us-west                       Yes
  AWS eu-west-1 (Dublin)                           eu-west                       Yes
  AWS ap-south-1 (Mumbai)                          ap-south                      Yes
  AWS ap-southeast-1 (Singapore)                   ap-southeast                  Yes
  AWS ap-northeast-1 (Tokyo)                       ap-northeast                  Yes
  AWS eu-central-1 (Frankfurt)                     eu-central                    Yes
  AWS ap-southeast-2 (Sydney)                      aws-ap-southeast-2            Yes
  AWS sa-east-1 (Sao Paulo)                        aws-sa-east-1                 Yes
  GCP us-central1 (Council Bluffs, Iowa)           gcp-us-central1               Yes
  AWS eu-west-2 (London)                           aws-eu-west-2                 Yes
  GCP us-east4 (Ashburn, Virginia)                 gcp-us-east4                  Yes
  GCP northamerica-northeast1 (Montréal, Québec)   gcp-northamerica-northeast1   Yes
  GCP asia-northeast3 (Seoul, South Korea)         gcp-asia-northeast3           Yes
  GCP europe-west1 (St Ghislain, Belgium)          gcp-europe-west1              Yes
  AWS ca-central-1 (Montreal)                      aws-ca-central-1              Yes
```

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale CLI commands: role
Source: https://planetscale.com/docs/cli/role



## Getting Started

Make sure to first [set up your PlanetScale developer environment](/docs/cli/planetscale-environment-setup). Once you've installed the `pscale` CLI, you can interact with PlanetScale and manage your databases straight from the command line.

## The `role` command

Manage database roles for a Postgres database branch. This command is only supported for Postgres databases.

**Usage:**

```bash  theme={null}
pscale role [command]
```

### Available sub-commands

| **Sub-Command** | **Product** | **Description**                                       |
| :-------------- | :---------- | :---------------------------------------------------- |
| `create`        | Postgres    | Create a new role for a Postgres database branch      |
| `delete`        | Postgres    | Delete a role                                         |
| `get`           | Postgres    | Retrieve information about a specific role            |
| `list`          | Postgres    | List all roles for a Postgres database branch         |
| `reassign`      | Postgres    | Reassign objects owned by a role to another role      |
| `renew`         | Postgres    | Renew a role's expiration                             |
| `reset`         | Postgres    | Reset a role's password                               |
| `reset-default` | Postgres    | Reset the credentials for the default `postgres` role |
| `update`        | Postgres    | Update a role's name                                  |

### Available flags

| **Flag**       | **Description**                       |
| :------------- | :------------------------------------ |
| `-h`, `--help` | View help for `role` command          |
| `--org string` | The organization for the current user |

### Global flags

| **Command**                     | **Description**                                                                      |
| :------------------------------ | :----------------------------------------------------------------------------------- |
| `--api-token <TOKEN>`           | The API token to use for authenticating against the PlanetScale API.                 |
| `--api-url <URL>`               | The base URL for the PlanetScale API. Default is `https://api.planetscale.com/`.     |
| `--config <CONFIG_FILE>`        | Config file. Default is `$HOME/.config/planetscale/pscale.yml`.                      |
| `--debug`                       | Enable debug mode.                                                                   |
| `-f`, `--format <FORMAT>`       | Show output in a specific format. Possible values: `human` (default), `json`, `csv`. |
| `--no-color`                    | Disable color output.                                                                |
| `--service-token <TOKEN>`       | The service token for authenticating.                                                |
| `--service-token-id <TOKEN_ID>` | The service token ID for authenticating.                                             |

## Examples

### The `create` sub-command

Create a new role for a Postgres database branch:

**Usage:**

```bash  theme={null}
pscale role create <database> <branch> <name> [flags]
```

**Available flags:**

* `--inherited-roles string` - Comma-separated list of role names to inherit privileges from. Common values are 'pg\_read\_all\_data' for read access, 'pg\_write\_all\_data' for write access, and 'postgres' for admin access.
* `--ttl duration` - TTL defines the time to live for the role. Durations such as "30m", "24h", or bare integers such as "3600" (seconds) are accepted. The default TTL is 0s, which means the role will never expire.

**Example:**

```bash  theme={null}
pscale role create my-database main api-user --inherited-roles pg_read_all_data --ttl 24h
```

### The `delete` sub-command

Delete a role:

**Usage:**

```bash  theme={null}
pscale role delete <database> <branch> <role-id> [flags]
```

**Available flags:**

* `--force` - Delete a role without confirmation
* `--successor string` - Role to transfer ownership to before deletion. Usually 'postgres'.

**Aliases:** `delete`, `rm`

**Example:**

```bash  theme={null}
pscale role delete my-database main role-123 --successor postgres
```

### The `get` sub-command

Retrieve information about a specific role:

**Usage:**

```bash  theme={null}
pscale role get <database> <branch> <role-id> [flags]
```

**Example:**

```bash  theme={null}
pscale role get my-database main role-123
```

### The `list` sub-command

List all roles for a Postgres database branch:

**Usage:**

```bash  theme={null}
pscale role list <database> <branch> [flags]
```

**Available flags:**

* `-w`, `--web` - List roles in your web browser.

**Aliases:** `list`, `ls`

**Example:**

```bash  theme={null}
pscale role list my-database main
```

### The `reassign` sub-command

Reassign objects owned by one role to any other role:

<Warning>
  Be careful with this command. Reassigning objects like databases, tables, or schemas will change who is able to write to them, alter them, or delete them.
</Warning>

**Usage:**

```bash  theme={null}
pscale role reassign <database> <branch> <role-id> --successor <role-id>
```

**Available flags:**

* `--force` - Force reset without confirmation

**Example:**

```bash  theme={null}
pscale role reassign my-database main role-123 --successor postgres
```

### The `renew` sub-command

Renew a role's expiration:

**Usage:**

```bash  theme={null}
pscale role renew <database> <branch> <role-id> [flags]
```

**Example:**

```bash  theme={null}
pscale role renew my-database main role-123
```

### The `reset` sub-command

Reset the credentials for any API-created role:

<Warning>
  Be careful with this command. If you are currently using the affected role's credentials for connecting to your database, running this command will reset the password, and new connections using the old password will not work.
</Warning>

**Usage:**

```bash  theme={null}
pscale role reset <database> <branch> <role-id> [flags]
```

**Available flags:**

* `--force` - Force reset without confirmation

**Example:**

```bash  theme={null}
pscale role reset my-database main role-123
```

### The `reset-default` sub-command

Reset the credentials for the default `postgres` role:

<Warning>
  Be careful with this command. If you are currently using the default `postgres` role credentials for connecting to your database, running this command will reset the password, and new connections using the old password will not work.
</Warning>

**Usage:**

```bash  theme={null}
pscale role reset-default <database> <branch> [flags]
```

**Available flags:**

* `--force` - Force reset without confirmation

**Example:**

```bash  theme={null}
pscale role reset-default my-database main
```

### The `update` sub-command

Update a role's name:

**Usage:**

```bash  theme={null}
pscale role update <database> <branch> <role-id> [flags]
```

**Available flags:**

* `--name string` - New name for the role

**Example:**

```bash  theme={null}
pscale role update my-database main role-123 --name new-role-name
```

## Related documentation

<CardGroup>
  <Card title="Managing Postgres roles" href="/docs/postgres/connecting/roles" icon="angles-right" horizontal />

  <Card title="Postgres roles API documentation" href="/docs/api/reference/list_roles" icon="angles-right" horizontal />
</CardGroup>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale CLI commands: service-token
Source: https://planetscale.com/docs/cli/service-token



## Getting Started

Make sure to first [set up your PlanetScale developer environment](/docs/cli/planetscale-environment-setup). Once you've installed the `pscale` CLI, you can interact with PlanetScale and manage your databases straight from the command line.

## The `service-token` command

This command allows you to create, list, and manage access for service tokens.

**Usage:**

```bash  theme={null}
pscale service-token <SUB-COMMAND> <FLAG>
```

### Available sub-commands

| **Sub-command**                                      | **Sub-command flags**        | **Product**      | **Description**                                                               |
| :--------------------------------------------------- | :--------------------------- | :--------------- | :---------------------------------------------------------------------------- |
| `add-access <TOKEN_ID> <PERMISSION> <PERMISSION>`    | `--database <DATABASE_NAME>` | Postgres, Vitess | Add individual permissions to the specified service token in the organization |
| `create`                                             | `--name <NAME>`              | Postgres, Vitess | Create a service token for the organization                                   |
| `delete <TOKEN_ID>`                                  |                              | Postgres, Vitess | Delete an entire service token in an organization                             |
| `delete-access <TOKEN_ID> <PERMISSION> <PERMISSION>` | `--database <DATABASE_NAME>` | Postgres, Vitess | Delete individual permissions granted to a service token in the organization  |
| `list`                                               |                              | Postgres, Vitess | List the IDs and names of an organization's service tokens                    |
| `show-access`                                        |                              | Postgres, Vitess | Fetch a service token and its accesses                                        |

#### Sub-command flag descriptions

Some of the sub-commands have additional flags unique to the sub-command. This section covers what each of those does. See the above table for which context.

| **Sub-command flag**         | **Description**                     | **Applicable sub-commands**   |
| :--------------------------- | :---------------------------------- | :---------------------------- |
| `--database <DATABASE_NAME>` | The database this project is using. | `add-access`, `delete-access` |
| `--name <NAME>`              | The name for the service token.     | `create`                      |

### Available flags

| **Flag**                    | **Description**                       |
| :-------------------------- | :------------------------------------ |
| `-h`, `--help`              | View help for `service-token` command |
| `--org <ORGANIZATION_NAME>` | The organization for the current user |

### Global flags

| **Command**                     | **Description**                                                                      |
| :------------------------------ | :----------------------------------------------------------------------------------- |
| `--api-token <TOKEN>`           | The API token to use for authenticating against the PlanetScale API.                 |
| `--api-url <URL>`               | The base URL for the PlanetScale API. Default is `https://api.planetscale.com/`.     |
| `--config <CONFIG_FILE>`        | Config file. Default is `$HOME/.config/planetscale/pscale.yml`.                      |
| `--debug`                       | Enable debug mode.                                                                   |
| `-f`, `--format <FORMAT>`       | Show output in a specific format. Possible values: `human` (default), `json`, `csv`. |
| `--no-color`                    | Disable color output.                                                                |
| `--service-token <TOKEN>`       | The service token for authenticating.                                                |
| `--service-token-id <TOKEN_ID>` | The service token ID for authenticating.                                             |

## Examples

### Adding service token access for a database

**Command:**

```bash  theme={null}
pscale service-token add-access <TOKEN_ID> read_branch delete_branch create_branch --database <DATABASE_NAME>
```

This will add the following permissions to the specified access token: `read_branch`, `delete_branch`, `create_branch`. The output will show the updated permissions on all databases.

You can find a list of all permissions in the [PlanetScale API documentation](/docs/api/reference/service-tokens#access-permissions).

**Output:**

| RESOURCE NAME | RESOURCE TYPE | ACCESSES                                     |
| :------------ | :------------ | :------------------------------------------- |
| dbname        | Database      | read\_branch, delete\_branch, create\_branch |
| dbname2       | Database      | create\_branch                               |
| my-org        | Organization  | create\_databases, delete\_databases         |

### Adding service token access for an organization

**Command:**

```bash  theme={null}
pscale service-token add-access <TOKEN_ID> read_organization
```

This will grant the service token the `read_organization` access on the organization.

**Output:**

| RESOURCE NAME | RESOURCE TYPE | ACCESSES           |
| :------------ | :------------ | :----------------- |
| my-org        | Organization  | read\_organization |

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Service tokens
Source: https://planetscale.com/docs/cli/service-tokens

PlanetScale provides the ability to create service tokens for your PlanetScale organization via the CLI or directly in the UI.

## Overview

Service tokens provide an alternate authentication method to be used with the PlanetScale CLI and API. They are typically used in automated scenarios where `pscale auth login` cannot be used. Service tokens are also required for any calls to the API, as well as minting OAuth tokens for API use.

## Create service tokens using the PlanetScale dashboard

To create a service token using the dashboard, log into your organization, go to the [**"Settings"** > **"Service tokens"**](https://app.planetscale.com/~/settings/service-tokens) page, and click the **"New service token"** button.

Give the token a name (this is used for your reference only) and click **"Create service token"**.

The modal will update, displaying your service token where the Name field was. Copy the ID and token values as you'll need them moving forward. Click **"Edit token permissions"** to proceed.

<Tip>
  Be sure to copy the service token after you create it. There's no way to retrieve the token value once you leave this page.
</Tip>

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/modal-with-service-token-2.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=da8d0101c3e31f2e8dc9e0873464b242" alt="Service token detail page" data-og-width="1220" width="1220" data-og-height="972" height="972" data-path="docs/images/assets/docs/concepts/service-tokens/modal-with-service-token-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/modal-with-service-token-2.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=d307c6662a0d099300655c0d4fba6119 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/modal-with-service-token-2.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=5a3b5239d6579760ecbf917749228634 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/modal-with-service-token-2.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=c2bf5bed573d2cf628b15153366f2e4f 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/modal-with-service-token-2.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=d1620b1a55536a30a5b3d8869f11cffa 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/modal-with-service-token-2.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=65aa70f815baebc479dcaad3d750b75c 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/modal-with-service-token-2.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=85b6024c0ce9d0f57d3f6b4824cf23ae 2500w" />
</Frame>

## Assign service token permissions

Service tokens are configured with granular permissions, both for the organization that owns them as well as on a per database level. Before you can use a service token, these permissions must be added.

### Add organization permissions

Organization permissions are required when performing operations that are specific to the organization and not for an individual database. To enable a service token for performing these operations, locate the **Organization access** section and click **"Add organization permissions"**.

In the **Organization access permissions** modal, check the box next to each of the permission scopes that you want to assign to the token. Click **"Save permissions"** once finished.

For a full list of organization access permissions, see the [API documentation for service tokens](https://planetscale.com/docs/api/reference/service-tokens#organization-access-permissions).

### Add database permissions

In order to perform operations specific to a database, permissions can be assigned per-database. To do this, locate the section titled **Database access** and click **"Add database access"** to open the **Database access permissions** modal.

Select the database you want to grant access to and check the box next to each permission option you need to grant. Once you are done, click **"Save permissions"**.

For a full list of database access permissions, see the [API documentation for service tokens](https://planetscale.com/docs/api/reference/service-tokens#database-access-permissions).

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/db-access-permissions-2.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=8d11a26d102b1b857eaa29427594e7fa" alt="The Database access permissions modal." data-og-width="1173" width="1173" data-og-height="1236" height="1236" data-path="docs/images/assets/docs/concepts/service-tokens/db-access-permissions-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/db-access-permissions-2.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=080f35bbafec1334fad669c3f77721a7 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/db-access-permissions-2.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=6f914704dcf4e35efafca277f62d5a3d 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/db-access-permissions-2.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=5fce17d59a468dfabe4f431d5b74f0ec 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/db-access-permissions-2.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=c0f05ca24f1a071725da080d096f2943 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/db-access-permissions-2.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=7fb9723619bdff443c9dbc2e86e1b7a9 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/db-access-permissions-2.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=f96495c65cda038f8d9326509acbe6e7 2500w" />
</Frame>

### Add permissions for all databases

Organization admins can grant database permissions to all current and future databases. This is available in the **Permissions for all databases** section.
Any permission added to all databases will not be able to be disabled on an individual basis.

## Service tokens and deploy requests approvals

When a database requires administrator approval for deploy requests (located in your database's **Settings** page), a service token cannot approve a deploy request created by the same service token. Also, users can't approve a deploy request created by a service token that they created.

## Use a service token with the PlanetScale CLI

To use service tokens with the PlanetScale CLI, set the following environment variables in your terminal:

```bash  theme={null}
export PLANETSCALE_SERVICE_TOKEN=<YOUR_SERVICE_TOKEN>
export PLANETSCALE_SERVICE_TOKEN_ID=<YOUR_SERVICE_TOKEN_ID>
```

When you execute commands using the PlanetScale CLI, it will automatically parse those values and use them to access the service. However, you’ll also need to pass in your organization name using the `--org` flag like so:

```bash  theme={null}
pscale branch create <DB_NAME> <BRANCH_NAME> --org <ORG_NAME>
```

If you don’t want to set environment variables, you may also pass in the Service Token and Service Token ID by using the [`--service-token` and `--service-token-id` flags](/docs/cli/service-token) respectively:

```bash  theme={null}
pscale branch create <DB_NAME> <BRANCH_NAME> --org <ORG_NAME> --service-token <SERVICE_TOKEN> --service-token-id <SERVICE_TOKEN_ID>
```

## Use a service token with the PlanetScale API

In order to execute a request to the PlanetScale API, you'll need a service token to execute requests directly or for minting OAuth tokens. Both the ID and token are required in the `Authorization` header without a scheme. Below is an example of how to use a service token to list details about the organizations the token can access:

```bash  theme={null}
curl --request GET \
     --url 'https://api.planetscale.com/v1/organizations' \
     --header 'Authorization: <SERVICE_TOKEN_ID>:<SERVICE_TOKEN>'
```

Refer to the [API docs](https://planetscale.com/docs/api/reference/getting-started-with-planetscale-api) for more details on how to use the API.

## Modify service token permissions

If you want to modify the permissions granted to a service token, start by opening the service token from the settings pane. Select the three dots next to the organization or database name permissions you want to modify and click **"Edit permissions"**.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/edit-org-perms-2.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=9ba825a47cb35891a186806c9d5cbd3e" alt="The location of the Edit permissions option for organization permissions." data-og-width="1353" width="1353" data-og-height="1012" height="1012" data-path="docs/images/assets/docs/concepts/service-tokens/edit-org-perms-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/edit-org-perms-2.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=c047e281d07d18af97d50216217484c2 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/edit-org-perms-2.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=50b01ecf30eb3aeac46074619ea13b10 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/edit-org-perms-2.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=e2d2f4ee4a206eff6c0ac0976ae4dfc2 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/edit-org-perms-2.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=dcdf0f7a27746655a0a034037a426e66 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/edit-org-perms-2.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=bd099020352d78798ff411329e3160e7 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/edit-org-perms-2.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=43c9817b950458e3ff03915c1ec50332 2500w" />
</Frame>

This will open a modal that allows you to modify the permissions the service token has to access that organization.

## Delete a service token

You can delete a service token at any time from the service token detail page. Simply click the **"Delete service token"** button.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/delete-service-token-2.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=bf4de5be09851bc2dfee8b464c18de6a" alt="Delete service token." data-og-width="1361" width="1361" data-og-height="337" height="337" data-path="docs/images/assets/docs/concepts/service-tokens/delete-service-token-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/delete-service-token-2.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=2ec3db41858b06d6c2f28c502ed86247 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/delete-service-token-2.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=00087d4968eecd5f1d379809edda3bb7 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/delete-service-token-2.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=b000c1a21326ca33ac3af6390d347ad1 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/delete-service-token-2.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=3cf45a3ef6c25f59146dfe96ebc1f9cf 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/delete-service-token-2.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=11faf6e93e8992e6bdf072826acaf59b 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/service-tokens/delete-service-token-2.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=9f37a9e22b1144b3319c8933f786d11d 2500w" />
</Frame>

## Manage service tokens using the PlanetScale CLI

Service tokens can also be created and managed directly from the [PlanetScale CLI](/docs/cli/service-token).

### Create a new service token

Use the following command to create a service token:

```bash  theme={null}
pscale service-token create
```

You can optionally specify a name for the service token using the `--name` flag:

```bash  theme={null}
pscale service-token create --name "My Token"
```

This command will return a service token ID and value for your use.

### Add database access permissions

You can add database access permissions to your service token for each database in your organization.

To add database access permissions, use the command:

```bash  theme={null}
pscale service-token add-access <SERVICE_TOKEN_ID> <ACCESS_PERMISSION> --database <DB_NAME>
```

For example, to give a service token the ability to create, read, and delete branches on a specific database, use the following command:

```bash  theme={null}
pscale service-token add-access <SERVICE_TOKEN_ID> read_branch delete_branch create_branch --database <DB_NAME>
```

A complete list of service token access permissions can be found in the [PlanetScale API documentation](https://planetscale.com/docs/api/reference/service-tokens#access-permissions).

### Remove database access permissions

You can also remove database access permissions for a service token.

Use the following command to remove one or more permissions:

```bash  theme={null}
pscale service-token delete-access <SERVICE_TOKEN_ID> <ACCESS_PERMISSION> --database <DB_NAME>
```

### Delete a service token

To delete a service token, run the following command:

```bash  theme={null}
pscale service-token delete <SERVICE_TOKEN_ID>
```

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale CLI commands: shell
Source: https://planetscale.com/docs/cli/shell



## Getting Started

Make sure to first [set up your PlanetScale developer environment](/docs/cli/planetscale-environment-setup). Once you've installed the `pscale` CLI, you can interact with PlanetScale and manage your databases straight from the command line.

## The `shell` command

This command opens a secure shell instance to your database so that you can manipulate it from the command line.

For MySQL databases, it uses the MySQL command-line client (`mysql`). For Postgres databases, it uses the Postgres command-line client (`psql`). The appropriate client [must be installed](/docs/cli/planetscale-environment-setup) prior to use.

<Note>
  If your Managed cluster has public connectivity disabled, you must connect over PrivateLink. Set up the VPC endpoint using the service name we provide, make sure your DNS resolves your PlanetScale hostnames to that PrivateLink endpoint, and then run `pscale shell mydatabase mybranch`.

  If you use a VPN such as Tailscale, do not override `psdb.cloud` DNS in your VPN configuration. Split‑DNS misconfiguration can point hostnames at the wrong place and break shell connectivity.
</Note>

**Usage:**

```bash  theme={null}
pscale shell <DATABASE_NAME> <BRANCH_NAME> <FLAG>
```

By default, if no branch names are given and there is only one branch, it automatically opens a shell to that branch. If there are multiple branches for the given database, you'll be prompted to choose one.

### Available flags

| **Flag**                    | **Description**                                                                                                                                                 |
| :-------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `-h`, `--help`              | Help for shell command                                                                                                                                          |
| `--local-addr <ADDRESS>`    | Local address to bind and listen for connections. By default the proxy binds to `127.0.0.1` with a random port.                                                 |
| `--org <ORGANIZATION_NAME>` | The organization for the current user                                                                                                                           |
| `--remote-addr <ADDRESS>`   | PlanetScale Database remote network address. By default the remote address is populated automatically from the PlanetScale API                                  |
| `--replica`                 | When enabled, the password will route all reads to the branch's primary replicas and all read-only regions                                                      |
| `--role <ROLE>`             | Role defines the access level, allowed values are: reader, writer, readwriter, admin. Defaults to 'reader' for replica passwords, otherwise defaults to 'admin' |

Available roles for the `--role` flag are:

* `reader` - Read-only access
* `writer` - Write-only access
* `readwriter` - Read and write access
* `admin` - Full administrative access

For replica connections (`--replica` flag), the default role is `reader`. For regular connections, the default role is `admin`.

### Global flags

| **Command**                     | **Description**                                                                     |
| :------------------------------ | :---------------------------------------------------------------------------------- |
| `--api-token <TOKEN>`           | The API token to use for authenticating against the PlanetScale API                 |
| `--api-url <URL>`               | The base URL for the PlanetScale API (default `https://api.planetscale.com/`)       |
| `--config <CONFIG_FILE>`        | Config file (default is `$HOME/.config/planetscale/pscale.yml`)                     |
| `--debug`                       | Enable debug mode                                                                   |
| `-f`, `--format <FORMAT>`       | Show output in a specific format. Possible values: `human` (default), `json`, `csv` |
| `--no-color`                    | Disable color output                                                                |
| `--service-token <TOKEN>`       | Service Token for authenticating                                                    |
| `--service-token-id <TOKEN_ID>` | The Service Token ID for authenticating                                             |

## Examples

### Basic shell usage

**Open a shell to a database (auto-selects branch if only one exists):**

```bash  theme={null}
pscale shell mydatabase
```

**Open a shell to a specific branch:**

```bash  theme={null}
pscale shell mydatabase mybranch
```

Once the shell is opened, you can run SQL as expected.

**Example MySQL session:**

```bash  theme={null}
DATABASE_NAME/BRANCH_NAME >
DATABASE_NAME/BRANCH_NAME > show tables;
+---------------+
| Tables_in_db |
+---------------+
| users       |
+---------------+
DATABASE_NAME/BRANCH_NAME > exit;
```

**Example Postgres session:**

```bash  theme={null}
psql-17 (17.5 (Homebrew))
SSL connection (protocol: TLSv1.3, cipher: TLS_AES_128_GCM_SHA256, compression: off, ALPN: postgresql)
Type "help" for help.

pg/|⚠ main ⚠|> \dt
                   List of relations
 Schema |    Name     | Type  |          Owner
--------+-------------+-------+-------------------------
 public | users | table | pscale_api_2e2o0t28kd0v
(1 row)

pg/|⚠ main ⚠|> \q
```

Type `exit` (MySQL) or `\q` (Postgres) to exit the shell.

### Using replica connections

**Connect to a read-only replica:**

```bash  theme={null}
pscale shell mydatabase mybranch --replica
```

Replica connections route all reads to the branch's primary replicas, and defaults to `reader` role.

### Using specific roles

**Connect with a specific role:**

```bash  theme={null}
pscale shell mydatabase mybranch --role reader
```

### Import an existing .sql file using the `shell` command

**Command:**

The following example assumes you have already ran the `pscale shell` command and you have the shell open to run a MySQL command.

To import an existing `.sql` file you may have available you would want to use the MySQL `source` command and provide it the path to your file:

```bash  theme={null}
DATABASE_NAME/BRANCH_NAME > source <YOUR_DUMP_FILE>.sql;
```

**Output:**

Your file should be imported as expected.

<Note>
  When importing `.sql` dump files there are a few caveats to be aware of as sometimes the `.sql` file may have everything wrapped in a `START TRANSACTION;` / `COMMIT;` transaction which will result in the import timing out if it takes more than 20 seconds to complete due to our 20 second transaction timeout limit so you will want to make sure those are removed prior to beginning an import of the file.

  Additionally, if your current schema requires our [Vitess foreign key constraints](/docs/vitess/foreign-key-constraints) support you may need to ensure it has been enabled within your database Settings area first before proceeding with your import.
</Note>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale CLI commands: signup
Source: https://planetscale.com/docs/cli/signup



## Getting Started

Make sure to first [set up your PlanetScale developer environment](/docs/cli/planetscale-environment-setup). Once you've installed the `pscale` CLI, you can interact with PlanetScale and manage your databases straight from the command line.

## The `signup` command

This command allows you to sign up for a PlanetScale account straight from the command line. You'll be prompted to register with an email and password.

**Usage:**

```bash  theme={null}
pscale signup <FLAG>
```

### Available flags

| **Flag**       | **Description**                |
| :------------- | :----------------------------- |
| `-h`, `--help` | View help for `signup` command |

### Global flags

| **Command**                     | **Description**                                                                      |
| :------------------------------ | :----------------------------------------------------------------------------------- |
| `--api-token <TOKEN>`           | The API token to use for authenticating against the PlanetScale API.                 |
| `--api-url <URL>`               | The base URL for the PlanetScale API. Default is `https://api.planetscale.com/`.     |
| `--config <CONFIG_FILE>`        | Config file. Default is `$HOME/.config/planetscale/pscale.yml`.                      |
| `--debug`                       | Enable debug mode.                                                                   |
| `-f`, `--format <FORMAT>`       | Show output in a specific format. Possible values: `human` (default), `json`, `csv`. |
| `--no-color`                    | Disable color output.                                                                |
| `--service-token <TOKEN>`       | The service token for authenticating.                                                |
| `--service-token-id <TOKEN_ID>` | The service token ID for authenticating.                                             |

## Examples

### The `signup` command

**Command:**

```bash  theme={null}
pscale signup
```

**Output:**

```bash  theme={null}
You are registering a new PlanetScale account.
? What is your e-mail? me@example.com
? Please type your password ******************
? Please confirm your password ******************
You've successfully signed up for PlanetScale!
Please check your email for a confirmation link and then get started with `pscale auth login`.
```

The password must be a minimum of 10 characters and include: 1 uppercase, digit, or special character.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale CLI commands: size
Source: https://planetscale.com/docs/cli/size



Lists the sizes for various components within PlanetScale.

## Usage

```bash  theme={null}
pscale size [command]
```

## Available commands

| **Command** | **Product**      | **Description**                          |
| :---------- | :--------------- | :--------------------------------------- |
| `cluster`   | Postgres, Vitess | List the sizes for PlanetScale databases |

## Flags

| **Flag**       | **Description**                       |
| :------------- | :------------------------------------ |
| `-h`, `--help` | Help for size                         |
| `--org string` | The organization for the current user |

### Global flags

| **Command**                 | **Description**                                                                                                    |
| :-------------------------- | :----------------------------------------------------------------------------------------------------------------- |
| `--api-token string`        | The API token to use for authenticating against the PlanetScale API                                                |
| `--api-url string`          | The base URL for the PlanetScale API. (default "\<[https://api.planetscale.com/>](https://api.planetscale.com/>)") |
| `--config string`           | Config file (default is \$HOME/.config/planetscale/pscale.yml)                                                     |
| `--debug`                   | Enable debug mode                                                                                                  |
| `-f`, `--format string`     | Show output in a specific format. Possible values: \[human, json, csv] (default "human")                           |
| `--no-color`                | Disable color output                                                                                               |
| `--service-token string`    | The service token for authenticating                                                                               |
| `--service-token-id string` | The service token ID for authenticating                                                                            |

## The `cluster` sub-command

List the sizes for PlanetScale databases.

**Usage:**

```bash  theme={null}
pscale size cluster [command]
```

**Aliases:** `cluster`, `clusters`

### Available sub-commands

| **Command** | **Product** | **Description**                                              |
| :---------- | :---------- | :----------------------------------------------------------- |
| `list`      | All         | List the sizes that are available for a PlanetScale database |

## The `list` sub-command

List the sizes that are available for a PlanetScale database. Use `--engine` to specify the database engine type.

**Usage:**

```bash  theme={null}
pscale size cluster list [flags]
```

**Aliases:** `list`, `ls`

### Available flags

| **Flag**          | **Description**                                                                                        |
| :---------------- | :----------------------------------------------------------------------------------------------------- |
| `--engine string` | The database engine to show cluster sizes for. Supported values: mysql, postgresql. Defaults to mysql. |
| `-h`, `--help`    | Help for list                                                                                          |
| `--metal`         | View cluster sizes and rates for clusters with metal storage                                           |
| `--region string` | View cluster sizes and rates for a specific region                                                     |

## Examples

### List all available cluster sizes (defaults to Vitess)

```bash  theme={null}
pscale size cluster list
```

### List Vitess cluster sizes explicitly

```bash  theme={null}
pscale size cluster list --engine mysql
```

### List PostgreSQL cluster sizes

```bash  theme={null}
pscale size cluster list --engine postgresql
```

### List cluster sizes for a specific organization

```bash  theme={null}
pscale size cluster list --org <ORG_NAME>
```

### List cluster sizes for a specific region

```bash  theme={null}
pscale size cluster list --region <REGION_NAME>
```

### List PostgreSQL cluster sizes for a specific region

```bash  theme={null}
pscale size cluster list --engine postgresql --region us-east
```

### List Metal cluster sizes (Vitess only)

```bash  theme={null}
pscale size cluster list --metal
```

For more information about PlanetScale cluster sizes and pricing, see:

* [PlanetScale Postgres pricing](https://planetscale.com/docs/postgres/pricing)
* [PlanetScale Vitess plans documentation](https://planetscale.com/docs/planetscale-plans)

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale CLI commands: webhook
Source: https://planetscale.com/docs/cli/webhook



## Getting Started

Make sure to first [set up your PlanetScale developer environment](/docs/cli/planetscale-environment-setup). Once you've installed the `pscale` CLI, you can interact with PlanetScale and manage your databases straight from the command line.

## The `webhook` command

This command allows you to create, list, update, test, and delete [webhooks](/docs/api/webhooks) for your database.

**Usage:**

```bash  theme={null}
pscale webhook <SUB-COMMAND> <FLAG>
```

### Available sub-commands

| **Sub-command**                       | **Description**                     | **Product**      |
| :------------------------------------ | :---------------------------------- | :--------------- |
| `create <DATABASE_NAME>`              | Create a new webhook for a database | Vitess, Postgres |
| `delete <DATABASE_NAME> <WEBHOOK_ID>` | Delete a webhook from a database    | Vitess, Postgres |
| `list <DATABASE_NAME>`                | List all webhooks for a database    | Vitess, Postgres |
| `show <DATABASE_NAME> <WEBHOOK_ID>`   | Show details for a specific webhook | Vitess, Postgres |
| `test <DATABASE_NAME> <WEBHOOK_ID>`   | Send a test event to a webhook      | Vitess, Postgres |
| `update <DATABASE_NAME> <WEBHOOK_ID>` | Update an existing webhook          | Vitess, Postgres |

#### Sub-command flags

| **Sub-command flag** | **Description**                                                                                                                                            | **Applicable sub-commands** |
| :------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------- | :-------------------------- |
| `--events <EVENTS>`  | Comma-separated list of events to trigger the webhook. See [webhook events](/docs/api/webhook-events) for available events.                                | `create`, `update`          |
| `--url <URL>`        | The HTTPS URL where webhook events will be sent                                                                                                            | `create`, `update`          |
| `--enabled`          | Enable or disable the webhook. Use `--enabled` or `--enabled=true` to enable, `--enabled=false` to disable. Webhooks are disabled by default when created. | `update`                    |

### Available flags

| **Flag**                    | **Description**                       |
| :-------------------------- | :------------------------------------ |
| `-h`, `--help`              | View help for `webhook` command       |
| `--org <ORGANIZATION_NAME>` | The organization for the current user |

### Global flags

| **Command**                     | **Description**                                                                      |
| :------------------------------ | :----------------------------------------------------------------------------------- |
| `--api-token <TOKEN>`           | The API token to use for authenticating against the PlanetScale API.                 |
| `--api-url <URL>`               | The base URL for the PlanetScale API. Default is `https://api.planetscale.com/`.     |
| `--config <CONFIG_FILE>`        | Config file. Default is `$HOME/.config/planetscale/pscale.yml`.                      |
| `--debug`                       | Enable debug mode.                                                                   |
| `-f`, `--format <FORMAT>`       | Show output in a specific format. Possible values: `human` (default), `json`, `csv`. |
| `--no-color`                    | Disable color output.                                                                |
| `--service-token <TOKEN>`       | The service token for authenticating.                                                |
| `--service-token-id <TOKEN_ID>` | The service token ID for authenticating.                                             |

## Examples

### List webhooks for a database

**Command:**

```bash  theme={null}
pscale webhook list <DATABASE_NAME> --org <ORGANIZATION_NAME>
```

This lists all webhooks configured for the specified database.

**Output:**

```bash  theme={null}
No webhooks exist in database <DATABASE_NAME>.
```

Or if webhooks exist:

```bash  theme={null}
  ID             URL                                   EVENTS                          ENABLED   CREATED AT   UPDATED AT
 -------------- ------------------------------------- ------------------------------- --------- ------------ ------------
  abc123xyz      https://example.com/webhook           branch.ready, branch.sleeping   Yes       2 days ago   1 day ago
```

### Create a webhook

**Command:**

```bash  theme={null}
pscale webhook create <DATABASE_NAME> --org <ORGANIZATION_NAME> \
  --events "branch.ready,branch.sleeping" \
  --url https://example.com/webhook
```

This creates a new webhook for the specified database with the selected events. The webhook will be disabled by default until you enable it with the `update` command.

**Output:**

```bash  theme={null}
  ID             URL                                   SECRET                                                             EVENTS                          ENABLED   CREATED AT   UPDATED AT
 -------------- ------------------------------------- ------------------------------------------------------------------ ------------------------------- --------- ------------ ------------
  abc123xyz      https://example.com/webhook           8e46bd50ca092655b1efdfca329f0d79eb976714030a8bfa031397eb0d1cb433   branch.ready, branch.sleeping   No        now          now
```

<Note>
  When you create a webhook, a secret is generated and displayed **only once** in the output. Store this secret securely as you'll need it to [validate webhook signatures](/docs/api/webhooks#validating-a-webhook-signature). You can also view the secret later from the database settings page in the dashboard.
</Note>

### Show webhook details

**Command:**

```bash  theme={null}
pscale webhook show <DATABASE_NAME> <WEBHOOK_ID> --org <ORGANIZATION_NAME>
```

This displays detailed information about a specific webhook, including its ID, URL, secret, events, enabled status, and timestamps.

**Output:**

```bash  theme={null}
  ID             URL                                   SECRET                                                             EVENTS                          ENABLED   CREATED AT       UPDATED AT
 -------------- ------------------------------------- ------------------------------------------------------------------ ------------------------------- --------- ---------------- ----------------
  abc123xyz      https://example.com/webhook           b4c29e6ae54a6456496cec7dcbfad7ace6e973a694802de2978b4d6e001fca6e   branch.ready, branch.sleeping   No        25 seconds ago   25 seconds ago
```

<Note>
  The `show` command displays the webhook secret, which is useful if you need to retrieve it after creation. Store this secret securely as you'll need it to [validate webhook signatures](/docs/api/webhooks#validating-a-webhook-signature).
</Note>

### Update a webhook

**Command:**

```bash  theme={null}
pscale webhook update <DATABASE_NAME> <WEBHOOK_ID> --org <ORGANIZATION_NAME> --enabled
```

This enables an existing webhook. To disable a webhook, use `--enabled=false`. You can also use this command to update other webhook settings like events and URL.

**Output:**

```bash  theme={null}
  ID             URL                                   EVENTS                          ENABLED   CREATED AT     UPDATED AT
 -------------- ------------------------------------- ------------------------------- --------- -------------- ------------
  abc123xyz      https://example.com/webhook           branch.ready, branch.sleeping   Yes       58 seconds ago now
```

### Test a webhook

**Command:**

```bash  theme={null}
pscale webhook test <DATABASE_NAME> <WEBHOOK_ID> --org <ORGANIZATION_NAME>
```

This sends a test event to the webhook URL to verify it's configured correctly. You can only send one test event every 20 seconds per webhook.

**Output:**

```bash  theme={null}
Test event was successfully sent to webhook <WEBHOOK_ID>.
```

### Delete a webhook

**Command:**

```bash  theme={null}
pscale webhook delete <DATABASE_NAME> <WEBHOOK_ID> --org <ORGANIZATION_NAME>
```

This permanently deletes the webhook from the database.

**Output:**

```bash  theme={null}
Webhook <WEBHOOK_ID> was successfully deleted from <DATABASE_NAME>.
```

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale CLI commands: workflow
Source: https://planetscale.com/docs/cli/workflow



## Getting Started

Make sure to first [set up your PlanetScale developer environment](/docs/cli/planetscale-environment-setup). Once you've installed the `pscale` CLI, you can interact with PlanetScale and manage your databases straight from the command line.

## The `workflow` command

This command allows you to create and manage [workflows](/docs/vitess/scaling/workflows) for Vitess databases, including creating workflows, listing them, and performing actions like traffic switching and cutover. This command is not supported for Postgres databases.

**Usage:**

```bash  theme={null}
pscale workflow <SUB-COMMAND> <FLAG>
```

### Available sub-commands

| **Sub-command**                                     | **Sub-command flags**                                                                                                                                                                                                                   | **Product** | **Description**                                                         |
| :-------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------- | :---------------------------------------------------------------------- |
| `cancel <DATABASE_NAME> <WORKFLOW_NUMBER>`          | `--force`                                                                                                                                                                                                                               | Vitess      | Cancel a workflow that is in progress                                   |
| `complete <DATABASE_NAME> <WORKFLOW_NUMBER>`        |                                                                                                                                                                                                                                         | Vitess      | Mark a workflow as complete                                             |
| `create <DATABASE_NAME> <BRANCH_NAME>`              | `--defer-secondary-keys`, `--global-keyspace <KEYSPACE_NAME>`, `-i`, `--interactive`, `--name <WORKFLOW_NAME>`, `--on-ddl <ACTION>`, `--source-keyspace <KEYSPACE_NAME>`, `--tables <TABLE_NAMES>`, `--target-keyspace <KEYSPACE_NAME>` | Vitess      | Create a new workflow within a branch                                   |
| `cutover <DATABASE_NAME> <WORKFLOW_NUMBER>`         | `--force`                                                                                                                                                                                                                               | Vitess      | Completes the workflow, cutting over all traffic to the target keyspace |
| `list <DATABASE_NAME>`                              |                                                                                                                                                                                                                                         | Vitess      | List all of the workflows for a PlanetScale database                    |
| `retry <DATABASE_NAME> <WORKFLOW_NUMBER>`           |                                                                                                                                                                                                                                         | Vitess      | Retry a workflow that has been stopped or failed                        |
| `reverse-cutover <DATABASE_NAME> <WORKFLOW_NUMBER>` |                                                                                                                                                                                                                                         | Vitess      | Reverse the cutover of a workflow back to the source keyspace           |
| `reverse-traffic <DATABASE_NAME> <WORKFLOW_NUMBER>` |                                                                                                                                                                                                                                         | Vitess      | Route queries back to the source keyspace for a specific workflow       |
| `show <DATABASE_NAME> <WORKFLOW_NUMBER>`            |                                                                                                                                                                                                                                         | Vitess      | Show a specific workflow for a PlanetScale database                     |
| `switch-traffic <DATABASE_NAME> <WORKFLOW_NUMBER>`  | `--force`, `--replicas-only`                                                                                                                                                                                                            | Vitess      | Route queries to the target keyspace for a specific workflow            |
| `verify-data <DATABASE_NAME> <WORKFLOW_NUMBER>`     |                                                                                                                                                                                                                                         | Vitess      | Verify data consistency for a specific workflow                         |

#### Sub-command flag descriptions

Some of the sub-commands have additional flags unique to the sub-command. This section covers what each of those does. See the above table for which context.

| **Sub-command flag**                | **Type** | **Description**                                                                                                                          | **Applicable sub-commands**           |
| :---------------------------------- | :------- | :--------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------ |
| `--defer-secondary-keys`            | boolean  | Don't create secondary indexes for tables until they've been copied (default true)                                                       | `create`                              |
| `--force`                           | boolean  | Force the operation without prompting for confirmation                                                                                   | `cancel`, `cutover`, `switch-traffic` |
| `--global-keyspace <KEYSPACE_NAME>` | string   | Choose an unsharded keyspace where sequence tables will be created for any workflow table that contains AUTO\_INCREMENT                  | `create`                              |
| `-i, --interactive`                 | boolean  | Create the workflow in interactive mode                                                                                                  | `create`                              |
| `--name <WORKFLOW_NAME>`            | string   | Name of the workflow                                                                                                                     | `create`                              |
| `--on-ddl <ACTION>`                 | string   | Action to take when a DDL statement is encountered during a running workflow. Options: EXEC, EXEC\_IGNORE, STOP, IGNORE (default "STOP") | `create`                              |
| `--replicas-only`                   | boolean  | Route read queries from the replica and read-only tablets to the target keyspace                                                         | `switch-traffic`                      |
| `--source-keyspace <KEYSPACE_NAME>` | string   | Keyspace where the tables will be copied from                                                                                            | `create`                              |
| `--tables <TABLE_NAMES>`            | strings  | Tables to migrate to the target keyspace                                                                                                 | `create`                              |
| `--target-keyspace <KEYSPACE_NAME>` | string   | Keyspace where the tables will be copied to                                                                                              | `create`                              |

### Available flags

| **Flag**                    | **Description**                       |
| :-------------------------- | :------------------------------------ |
| `-h`, `--help`              | View help for workflow command        |
| `--org <ORGANIZATION_NAME>` | The organization for the current user |

### Global flags

| **Command**                     | **Description**                                                                      |
| :------------------------------ | :----------------------------------------------------------------------------------- |
| `--api-token <TOKEN>`           | The API token to use for authenticating against the PlanetScale API.                 |
| `--api-url <URL>`               | The base URL for the PlanetScale API. Default is `https://api.planetscale.com/`.     |
| `--config <CONFIG_FILE>`        | Config file. Default is `$HOME/.config/planetscale/pscale.yml`.                      |
| `--debug`                       | Enable debug mode.                                                                   |
| `-f`, `--format <FORMAT>`       | Show output in a specific format. Possible values: `human` (default), `json`, `csv`. |
| `--no-color`                    | Disable color output.                                                                |
| `--service-token <TOKEN>`       | The service token for authenticating.                                                |
| `--service-token-id <TOKEN_ID>` | The service token ID for authenticating.                                             |

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# How PlanetScale uses AI
Source: https://planetscale.com/docs/how-we-use-ai

Which PlanetScale features use generative AI, and how we ensure customer data stays secure and private.

## Overview

PlanetScale uses generative AI to power two [Insights](/docs/postgres/monitoring/query-insights) features. This overview explains those features and how PlanetScale ensures safety and privacy of customer data.

## Which features use generative AI

Two features of [Insights](/docs/postgres/monitoring/query-insights) use generative AI:

* Postgres [index recommendations](/docs/postgres/monitoring/schema-recommendations) use a large language model to interpret query patterns and identify potential indexes that could improve performance of those patterns. The prompt sent to the LLM includes one or more query patterns and one or more table schemas. Query patterns and table schemas include the names of tables and columns, but not any actual row data. This process runs automatically on each database each night.
* Query summaries for both [Postgres](/docs/postgres/monitoring/query-insights#query-deep-dive) and [Vitess](/docs/vitess/monitoring/query-insights#query-deep-dive) use a large language model to convert a query pattern to a one-sentence description. The prompt sent to the LLM contains a query pattern. This feature is invoked only when a user clicks on the "Summarize query" button on an Insights query-details page.

## Privacy and security

All AI use is covered by PlanetScale's existing [terms of service](https://planetscale.com/legal/agreement) and [subprocessor list](https://planetscale.com/legal/subprocessors). Our LLM provider, [Google Gemini](https://cloud.google.com/gemini/docs/discover/data-governance), encrypts all communication in transit, does not use prompts for training, and retains prompts only long enough to screen for abuse.

## Opting out

We understand that not every customer wants to use AI. You can disable the LLM-based features for all the databases in your account or organization on the organization settings page.


# PlanetScale documentation
Source: https://planetscale.com/docs/index

PlanetScale is a relational database platform that brings you scale, performance, and reliability — without sacrificing developer experience.

We offer both [Vitess](/docs/vitess) and [PostgreSQL](/docs/postgres) clusters, powered by locally-attached NVMe drives that deliver unlimited IOPS and ultra-low latency.

<Columns cols={2}>
  <Card title="Vitess documentation" icon="database" href="/docs/vitess">
    PlanetScale for Vitess offers fully-managed Vitess clusters with unlimited scalability. Features include sharding, branching, deploy requests, query insights, and more.

    <a href="/docs/vitess">View the Vitess docs</a>
  </Card>

  <Card title="Postgres documentation" icon="database" href="/docs/postgres">
    PlanetScale Postgres is a fully-managed PostgreSQL-compatible database. Features include high availability, query insights, branching, and more.

    <a href="/docs/postgres">View the Postgres docs</a>
  </Card>
</Columns>

All clusters are deployed with one primary and a minimum of 2 replicas across 3 availability zones for high availability. Automated failovers, managed version upgrades, and a team of database experts holding the pager means you can spend your time shipping instead of worrying about your database.

For large-scale applications that are hitting the limits of vertical scale, PlanetScale's [Vitess](https://planetscale.com/vitess) is a great option. You get the power of horizontal sharding, built-in connection pooling, and non-blocking schema changes so you'll never have to worry about outgrowing your database solution.

<Frame><img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/planetscale-dashboard-darkmode-BDgbDVqL.avif?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=f54e3d8d9812e3bdc0cbc6121a5ba024" alt="PlanetScale dashboard" data-og-width="3026" width="3026" data-og-height="1910" height="1910" data-path="docs/images/planetscale-dashboard-darkmode-BDgbDVqL.avif" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/planetscale-dashboard-darkmode-BDgbDVqL.avif?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=7ef1d37c62d60cf6b7e2a28f267d53ff 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/planetscale-dashboard-darkmode-BDgbDVqL.avif?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=965fa37f57ffca2d4f27602261f13b61 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/planetscale-dashboard-darkmode-BDgbDVqL.avif?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=cbd6807dbb7adca6ba768e343d3c8ab5 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/planetscale-dashboard-darkmode-BDgbDVqL.avif?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=517429c9d6635b30b12af78aad9db11b 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/planetscale-dashboard-darkmode-BDgbDVqL.avif?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=ad1a9ebf9c6ea6a35e943610c0de6e38 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/planetscale-dashboard-darkmode-BDgbDVqL.avif?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=58af09960f7c96388acbc8e33d779dba 2500w" /></Frame>

## PlanetScale plans

<Columns cols={3}>
  <Card title="PlanetScale plans" icon="credit-card" href="/docs/planetscale-plans">
    What's the difference between Scaler Pro and Enterprise?

    <br />

    <a href="/docs/planetscale-plans">Learn more</a>
  </Card>

  <Card title="Deployment options" icon="server" href="/docs/plans/deployment-options">
    PlanetScale offers multi-tenant and single-tenant deployment options.

    <a href="/docs/plans/deployment-options">Learn more</a>
  </Card>

  <Card title="Bring your own cloud" icon="cloud" href="/docs/vitess/managed">
    Learn about our Enterprise offering — PlanetScale Managed.

    <br />

    <a href="/docs/vitess/managed">Learn more</a>
  </Card>
</Columns>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale Metal
Source: https://planetscale.com/docs/metal

PlanetScale Metal databases are the same PlanetScale databases you know and love, powered by blazing-fast, locally-attached NVMe SSD drives instead of network-attached storage.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal.png?fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=6fa63babac9db310367e4ececce42342" className="block dark:hidden" alt="Metal SSD" data-og-width="1944" width="1944" data-og-height="622" height="622" data-path="docs/metal/metal.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal.png?w=280&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=23b7923d9634dc6ec215881a525a489f 280w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal.png?w=560&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=66b25d4361d98825ccaab8cb3de820d5 560w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal.png?w=840&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=88c6c30ebbab7a7c620eb6de4a00e04f 840w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal.png?w=1100&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=abc5751a04c65ae70b66ac46953a9d53 1100w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal.png?w=1650&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=97c778134c92e179200133981a1d4688 1650w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal.png?w=2500&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=1167f19fa5ac256ccf1d9aac70cfd4c4 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal-darkmode.png?fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=19467d0c351d6d43217647b6a80d5dc3" className="hidden dark:block" alt="Metal SSD" data-og-width="1942" width="1942" data-og-height="622" height="622" data-path="docs/metal/metal-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal-darkmode.png?w=280&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=a3a8ccebf57d21f52be8b50a29c6253a 280w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal-darkmode.png?w=560&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=063ff77e9f103f51ca3e99fcd22a6cea 560w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal-darkmode.png?w=840&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=0da7b850f8b2e6653be2d408751a91d7 840w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal-darkmode.png?w=1100&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=89fcf7f2087b6a457dc15e8a9b72bd7a 1100w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal-darkmode.png?w=1650&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=b74b787dfd44297af0616a3f23451340 1650w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal-darkmode.png?w=2500&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=2981b8cfe043f9beb2e2396b042c9f52 2500w" />
</Frame>

This translates to significant latency reduction, more consistent IO performance, and unlimited I/O Operations Per Second (IOPS).
Metal is an excellent choice for high-IOPS and other performance-critical workloads.
With Metal, your database now has the ability to use modern NVMe SSD technology to its full potential.

**Note**: Metal nodes for Postgres now [start at \$50/month](https://planetscale.com/blog/50-dollar-planetscale-metal-is-ga-for-postgres).

## Using a Metal database

PlanetScale databases come in two main flavors: **Metal** and **network-attached storage**.
When you create a database on PlanetScale, you can choose between these two options:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/nas-and-metal.png?fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=9826423b1d67d10f3d5fce0f9f14670e" className="block dark:hidden" alt="Choose between network attached storage and Metal" data-og-width="2674" width="2674" data-og-height="1428" height="1428" data-path="docs/metal/nas-and-metal.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/nas-and-metal.png?w=280&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=5a420ad6b359c2d601955c0f4d68b93b 280w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/nas-and-metal.png?w=560&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=f54e9eae31f1a1131d6c2ee16552e5d4 560w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/nas-and-metal.png?w=840&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=1d62414617f67cdf791ca18e0c81a539 840w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/nas-and-metal.png?w=1100&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=6e58774fc39eb3dd31fa94ac7c152366 1100w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/nas-and-metal.png?w=1650&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=4206456b5c08db803902de8a9cb69123 1650w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/nas-and-metal.png?w=2500&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=f54d5c9b6429bcbc54067ef50c3b4a6b 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/nas-and-metal-darkmode.png?fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=b1206c3cc112a3a66a552d17e812a05d" className="hidden dark:block" alt="Choose between network attached storage and Metal" data-og-width="2674" width="2674" data-og-height="1428" height="1428" data-path="docs/metal/nas-and-metal-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/nas-and-metal-darkmode.png?w=280&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=5db530e5342b965174bcbc76ec3f27ca 280w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/nas-and-metal-darkmode.png?w=560&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=51d44ddeda2105f9a45b38844b806b40 560w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/nas-and-metal-darkmode.png?w=840&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=e1289d6270bd558c14d0915ec4281aba 840w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/nas-and-metal-darkmode.png?w=1100&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=a939ea1001839aac12595098fcd96520 1100w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/nas-and-metal-darkmode.png?w=1650&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=ef0d0c8b707179690e9ff825de06b1eb 1650w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/nas-and-metal-darkmode.png?w=2500&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=44a42d35d3b2cf7e8eee812ac706c07f 2500w" />
</Frame>

<Warning>
  The storage for Metal databases does not autoscale.
  It is important to keep a close eye on the storage capacity of Metal databases, and upgrade well before running out of space.
</Warning>

When you create a Metal database, you must choose a drive size up front.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal-drive-size.png?fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=700f7bcb7ad62ff4cd8d9f3d24723867" className="block dark:hidden" alt="Select storage drive size for a Metal database" data-og-width="3788" width="3788" data-og-height="1944" height="1944" data-path="docs/metal/metal-drive-size.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal-drive-size.png?w=280&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=6327c6a1713e978dc728bcb7f0fbeff0 280w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal-drive-size.png?w=560&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=dec1dc601b224d275e8ef1b8eb346d80 560w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal-drive-size.png?w=840&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=313bc6ae9ca833a3a8567587b5b4b407 840w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal-drive-size.png?w=1100&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=414a67e200f9663b083bac34128a01a9 1100w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal-drive-size.png?w=1650&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=0217a3b280b0f12b256a76757bb07a67 1650w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal-drive-size.png?w=2500&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=a43313432785d24726b65a3cd2de10e4 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal-drive-size-darkmode.png?fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=77e0875ce2cc316845ddd77e3f54ffd5" className="hidden dark:block" alt="Select storage drive size for a Metal database" data-og-width="3790" width="3790" data-og-height="1944" height="1944" data-path="docs/metal/metal-drive-size-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal-drive-size-darkmode.png?w=280&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=8b67ace76e2a4ea20f55a449b46c9e7d 280w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal-drive-size-darkmode.png?w=560&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=90e9d680aa707a94e76ac4b95e01d34f 560w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal-drive-size-darkmode.png?w=840&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=7353dc7e0028200ccc82da15cf53d679 840w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal-drive-size-darkmode.png?w=1100&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=30d5e052771db5486cd7de506db2d9ca 1100w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal-drive-size-darkmode.png?w=1650&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=b20abb1ba68d7178878b1a71f04c9245 1650w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal-drive-size-darkmode.png?w=2500&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=5770bb74c4275b8e67a7fe7a2e4e6ad7 2500w" />
</Frame>

You should select a drive size that best suits your current data size, while also taking into account growth trends.
When the time comes that you need more storage, we make it easy to upgrade to larger NVMe drives with just a few clicks of a button.
You can learn more about creating and resizing Metal databases in our [creation and upgrade documentation](/docs/metal/create-a-metal-database).

## Monitoring your storage

Fixed-sized drives also means that you must closely monitor how much storage your database is using.
You can do so by looking at the storage information on the PlanetScale dashboard:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/metal/storage.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=e2a6efe1aea907f77635a1e51059ef1c" className="block dark:hidden" alt="Storage indicator in PlanetScale" data-og-width="1450" width="1450" data-og-height="1290" height="1290" data-path="docs/metal/storage.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/metal/storage.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=9888722ce397cc216335755e099f857b 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/metal/storage.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=aabba37339379838b3fcfdcce72e8771 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/metal/storage.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=b8fd34859c301153cc7c1ab0abf9cbbf 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/metal/storage.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=66c4ee065707439a6767378f84c98dec 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/metal/storage.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=7b9abb7a8ff0158e2eae04da1871decc 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/metal/storage.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=a493eea4fc6feabe7e7869406198c870 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/metal/storage-darkmode.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=986fd7f52435aadf1ccd83299b934fb0" className="hidden dark:block" alt="Storage indicator in PlanetScale" data-og-width="1450" width="1450" data-og-height="1290" height="1290" data-path="docs/metal/storage-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/metal/storage-darkmode.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=3e50eaf82fc86bdf6b989dc8ab7fd7ec 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/metal/storage-darkmode.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=8293d74c8e663c23306fb1462b135ae4 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/metal/storage-darkmode.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=7c289f42543fa78211720be4cebbfe4d 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/metal/storage-darkmode.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=c3b920c4077c2a1684d9b49a4d286264 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/metal/storage-darkmode.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=d76e415348ad5cb56a8278b81d6b0b74 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/metal/storage-darkmode.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=9ce18ecc3d8d74035d728a1eb14fc637 2500w" />
</Frame>

We will send you email notices when your database storage reaches the following thresholds: 60%, 75%, 85%, 90%, 95%.
We will also email you when we estimate that your storage will run out in 1 week and 24 hours, based on recent usage trends.

Reaching or getting close to a drive's max capacity is dangerous and can lead to failures.
It's important to closely monitor your database's disk usage in the dashboard and check your regular storage email notifications.
We have an additional safeguard in place to protect your data: When we detect that your Metal disk has 6GiB or less of available space, we automatically switch it to read-only mode until resized.
Ideally, you should resize to a larger drive long before reaching this point.

## Workload suitability

Using Metal is a clear win for many workloads.
Many of our current Metal customers have been able to either (A) save money, (B) increase performance, or (C) do both at the same time by switching to Metal.
The per-GB cost of metal storage is more affordable than network-attached storage with high IOPS capacity, and the improved IO performance allows you to use smaller compute instances in some cases.

There are some scenarios where a network-attached storage database may be a better choice.
Here, we provide some general suggestions to help you choose the ideal type of database for your needs.
If you'd like a more personalized assessment, please [reach out to support](https://planetscale.com/contact?initial=support) with the specifics of your workload.

Generally, these types of database workloads are ideal for Metal:

* If your workload has significant I/O demands, Metal is an ideal choice.
  A network-attached storage database has limitations on how quickly it can read and write data due to the additional network hops.
  Metal databases allow you to unlock the full potential of modern NVMe technology, providing ultra-high throughput.
* If you have experience running up against the limits of AWS EBS IOPS or have a large `gp3` or `io2` `EBS` volume bill.
  Metal provides unlimited IOPS and will likely yield performance improvement, cost savings, or both.
  There is no need to pay extra for access to the I/O throughput of the local drive.
* If low-latency database performance is critical to your business needs.
* If you are concerned about long-tail p99+ performance.

However, there are some scenarios where choosing network-attached storage may still be preferable:

* Very small databases that have both low compute and low storage requirements may be better suited for network-attached storage.
* Databases where the majority of active rows fit in RAM.
  In this case, the I/O demand on the storage is probably low, and you won't see as much of a performance boost by using Metal.
* If you frequently resize your database, Metal may not be the best option.
  Metal instance resizes generally take longer than a network-attached storage resize as they require copying data between drives.

## Metal Performance

We've mentioned several times that Metal can provide you better performance.
What does that look like in practice?
Let's look at two examples.

### PlanetScale Insights

Below is a screenshot showing the p50 and p95 response times for the database that was powering PlanetScale Insights as of Q4 2024.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal-insights-p50-p95.png?fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=6246089292754f9a5e44c2ac3b61e4d9" alt="The effect of Metal on the Insights database" data-og-width="2796" width="2796" data-og-height="1906" height="1906" data-path="docs/metal/metal-insights-p50-p95.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal-insights-p50-p95.png?w=280&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=c3d5d6435ce5d8fc3a935fe292ecf9cf 280w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal-insights-p50-p95.png?w=560&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=fbc4015b39e06e8c8eb3963e3784fd27 560w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal-insights-p50-p95.png?w=840&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=d574f2e7aa654b9d63e035103eb68c03 840w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal-insights-p50-p95.png?w=1100&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=28b4da04a3a595c5bb46e78106dc905d 1100w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal-insights-p50-p95.png?w=1650&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=a05a186b44dfb7ca52e4a8002c7e40c5 1650w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/metal-insights-p50-p95.png?w=2500&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=327ac10020bbcbe188b566f4d240a40d 2500w" />
</Frame>

You can pretty clearly see when the database was switched over from network-attached storage instance types to Metal.
The p50 response times were cut in half, and the p95 had approximately a 7x improvement.

The workload was and continues to be very I/O bound.
The Insights database ingests a large amount of time-series data, and is frequently queried to pull the data that we use to generate graphs in Insights.
Metal provides a huge improvement for this type of workload.

### Large, sharded database

One of our existing customers runs several large, sharded databases.
We migrated these databases to Metal during the internal release of our product.
Below is a screenshot of the p99 latencies of a set of shards that we migrated from network-attached storage database to Metal instances.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/customer-p99.png?fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=b014586040a54c142d7238427e2b3491" alt="The effect of Metal on a large sharded database" data-og-width="2956" width="2956" data-og-height="1390" height="1390" data-path="docs/metal/customer-p99.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/customer-p99.png?w=280&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=36c2befa042b582ee8b8450d284b1074 280w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/customer-p99.png?w=560&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=ee4dd70246f20a939962a631f4322300 560w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/customer-p99.png?w=840&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=0233c515f1660a173cdbbb27fd7c4855 840w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/customer-p99.png?w=1100&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=9e193ee07495d7e38fb47eac48cf5361 1100w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/customer-p99.png?w=1650&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=e75fb46197f08544b855b2b298b94d44 1650w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/customer-p99.png?w=2500&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=ed3ac36dc16991043e735c4db96836b7 2500w" />
</Frame>

Though their p99 response times were already very good, Metal was able to further cut it in half.

### Cost and performance

We migrated yet another large customer during our internal release.
After switching to Metal, they saw a significant improvement in API call latency for one of their critical APIs.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/customer-api-improvement.png?fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=36e58b7cae7b9d7a1c60a001c28d573e" alt="API latency improvement" data-og-width="2854" width="2854" data-og-height="1228" height="1228" data-path="docs/metal/customer-api-improvement.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/customer-api-improvement.png?w=280&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=02bd5bd0b6bd0c3f75c9d53c74015b02 280w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/customer-api-improvement.png?w=560&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=245b2c675bcf5d549a9667c03223964a 560w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/customer-api-improvement.png?w=840&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=b5c88348425602bece3e3c264ebf9050 840w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/customer-api-improvement.png?w=1100&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=51001ae24e50a89b53439755648d805d 1100w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/customer-api-improvement.png?w=1650&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=98e6380eb3862ef675fc72fcae87081d 1650w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/customer-api-improvement.png?w=2500&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=488299959319f1266e2efe2c11824bcd 2500w" />
</Frame>

We can clearly see that starting on Dec 20, the long tail of latency was reduced significantly.
This is due to the lower latency and improved consistency of local NVMe disk performance.

This same customer also saw some significant cost savings with their move to Metal.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/customer-pricing-drop.png?fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=ac730044638647cbceccbfe28200a2c9" alt="Cost savings with Metal" data-og-width="2552" width="2552" data-og-height="1728" height="1728" data-path="docs/metal/customer-pricing-drop.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/customer-pricing-drop.png?w=280&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=4b892ff4731ed98440ec7e3d4dba0b45 280w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/customer-pricing-drop.png?w=560&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=bbfbf37f7dcfad0b197ab819b487668c 560w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/customer-pricing-drop.png?w=840&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=537fecc860d51d6de35e280dafebce58 840w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/customer-pricing-drop.png?w=1100&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=87cc3be659719be54e23803d7a90ad7d 1100w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/customer-pricing-drop.png?w=1650&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=17fce7faf8e35dfb64fb39c7e51d36bd 1650w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/metal/customer-pricing-drop.png?w=2500&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=afc6a46439ad4bee177a0b3419aa82a3 2500w" />
</Frame>

The performance of the database improved and the AWS costs to run dropped from over $100 per day to ~$30 per day.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Create a Metal database
Source: https://planetscale.com/docs/metal/create-a-metal-database



export const YouTubeEmbed = ({id, title}) => {
  return <Frame>
      <iframe src={`https://www.youtube-nocookie.com/embed/${id}?rel=0`} title={title} className="aspect-video w-full" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" />
    </Frame>;
};

[PlanetScale Metal](/docs/metal) databases can be created in a similar way to other PlanetScale databases.
However, there are a few important things to keep in mind when creating a new Metal database or upgrading an existing database to Metal, which will be covered here.

<YouTubeEmbed id="6BBrgJcpTBY" title="Create a database on PlanetScale" />

## Create a new Metal database

After logging in to `app.planetscale.com`, click "New database" -> "Create new database."
Next, enter the name of your new database and select the "PlanetScale Metal" option.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-new-db.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=fa12b24902ed3f1498d6412e9842537a" className="block dark:hidden" alt="New Metal database" data-og-width="2674" width="2674" data-og-height="1428" height="1428" data-path="docs/images/metal/metal-new-db.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-new-db.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=f085e0f1822de711fa18dfd0b5e63b37 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-new-db.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=7bce192a92f4edf19cda8fdf379ef3ea 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-new-db.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=85a28006f795f87371933ac8f695d083 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-new-db.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=90c9d75920f33977f5a18663c74e70ad 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-new-db.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=4bfc69ac97dd2ba8627b7a2b127e4a51 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-new-db.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=b9e3290bc48e215fa887029c474a78ec 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-new-db-darkmode.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=2401b83cd85c1db8ea2d260381d8faef" className="hidden dark:block" alt="New Metal database" data-og-width="2674" width="2674" data-og-height="1428" height="1428" data-path="docs/images/metal/metal-new-db-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-new-db-darkmode.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=bf433acb069636505a4d17e49e3fef6f 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-new-db-darkmode.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=759ea6c6a2292fc02a510c85c23984f0 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-new-db-darkmode.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=7a79d02fdd5595f322993250b5437215 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-new-db-darkmode.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=c4692b881fefe1eff6e0818f6d0f8e03 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-new-db-darkmode.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=2931d9dc36a8540b1c35eb14106c3296 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-new-db-darkmode.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=3af4f71bebdc0915faacec6881fc29e8 2500w" />
</Frame>

This brings up a set of options to choose from for the size of your Metal database.
Start by choosing the vCPU and RAM combination that best suits your needs, then use the dropdown to select the drive size for the instance.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-new-db-choose-size.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=52cb4305bc9732cbc8ebfd7cb1393c88" className="block dark:hidden" alt="Select Metal database size" data-og-width="2698" width="2698" data-og-height="1904" height="1904" data-path="docs/images/metal/metal-new-db-choose-size.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-new-db-choose-size.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=e7b8152ca7c2e927b29ac76641dd9f1f 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-new-db-choose-size.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=07d933a45dce08e78975d53ff44d604a 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-new-db-choose-size.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=b412e7038fbfa9963c998622b1383e54 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-new-db-choose-size.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=6ddbe7c768f0b97f2f2274b5678c33f6 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-new-db-choose-size.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=c7ad008fea809f4dddb46c9172d76c13 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-new-db-choose-size.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=67d9a25d7eaf881ec244341ab332f536 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-new-db-choose-size-darkmode.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=4fa454a4c8f8c5a39ebf55b5b95e2016" className="hidden dark:block" alt="Select Metal database size" data-og-width="2698" width="2698" data-og-height="1904" height="1904" data-path="docs/images/metal/metal-new-db-choose-size-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-new-db-choose-size-darkmode.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=c5a64de7a623df5058240fa90538b53e 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-new-db-choose-size-darkmode.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=ac7b9710bb06937b1e20c8d1e6ae8728 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-new-db-choose-size-darkmode.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=394095b6a5d31e13bda11de833892641 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-new-db-choose-size-darkmode.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=ed1790f5077a0893df725729bbb5018f 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-new-db-choose-size-darkmode.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=7f812f1d5e9426af90a0f5c704e0592c 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-new-db-choose-size-darkmode.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=db02d95e84d7f5c59c39a55bf3896654 2500w" />
</Frame>

As opposed to [network-attached storage](/docs/plans/planetscale-skus#network-attached-storage) databases, [Metal](/docs/plans/planetscale-skus#metal) databases do not autoscale their storage size.
Therefore, it's important to make a good size choice from the start.
If you are starting a new project from scratch on a Metal database and you do not expect massive initial growth, it is likely best to choose the smallest drive possible.
If you intend to migrate an existing database into this in the near future, ensure that your drive will fit all of the data while also allowing room for further growth.

When ready, click "Create database."
After database initialization completes, you can begin using the database.

## Upgrading an existing database to Metal

You can also upgrade an existing keyspace in your database to Metal.
This is a no-downtime operation.
To do this, select your database, and then click on the "**Clusters**" in the navigation pane on the left side of the dashboard.
From here, you should choose the keyspace that you want to upgrade.
Click on the cluster size drop-down and scroll down to the Metal instance types.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-upgrade.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=d5bf61d8eb3a669c432283c6f7ce3653" className="block dark:hidden" alt="Upgrade keyspace to Metal" data-og-width="3047" width="3047" data-og-height="1800" height="1800" data-path="docs/images/metal/metal-upgrade.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-upgrade.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=7752d8a2a126b1a230b14e747205b6dc 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-upgrade.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=9ac482be65d151f9d2b66e1e79908293 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-upgrade.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=b89c2e29e71f682f7eb2fae7151d0cc5 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-upgrade.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=b0e44dbb585dd983a2594486bed2ddd6 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-upgrade.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=bd7781955482fd94191d04e85940948d 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-upgrade.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=79ed77f76a75d9786d563d70825e095b 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-upgrade-darkmode.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=e091491269ec7517706fe2ed1484f868" className="hidden dark:block" alt="Upgrade keyspace to Metal" data-og-width="3046" width="3046" data-og-height="1797" height="1797" data-path="docs/images/metal/metal-upgrade-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-upgrade-darkmode.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=3f62bb74ba6224f1393a5304070f98e4 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-upgrade-darkmode.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=1410001bc184f148da2f7ae68bae1562 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-upgrade-darkmode.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=1237168c24e33b94aca231f2f8ee866b 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-upgrade-darkmode.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=06cf6482f24298c6945fb0440450e915 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-upgrade-darkmode.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=809787c6b37d5bef4dd86715c9919aa2 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/metal-upgrade-darkmode.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=fa6d76d31d89c3486251199be8124aaf 2500w" />
</Frame>

Select the desired compute and storage combination, and then click "Save changes."

Keep in mind that this is not an immediate operation.
If you have a large database, it may take a while for the upgrade to complete since behind the scenes, your entire database needs to be migrated to the new NVMe drives.
Ensure that you upgrade well before reaching max drive capacity.
We recommend upgrading at no later than 75% in most cases, and even earlier than that if you are growing quickly.

## Monitoring Metal storage

<Warning>
  The storage for Metal databases does not autoscale.
  It is important to keep a close eye on the storage capacity of Metal databases, and upgrade well before running out of space.
</Warning>

There are several ways to monitor this.

You can view storage information on the right side of the main PlanetScale dashboard for Metal databases.

After the upgrade is complete, we recommend going to the Insights page and view your query latency diagrams.
If you transitioned from a network-attached storage keyspace to a Metal one of the same size, you should see a reduction in query latency.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/storage.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=d55514fd14f9dd42caab4c578d621d6f" className="block dark:hidden" alt="Metal storage info" data-og-width="1450" width="1450" data-og-height="1290" height="1290" data-path="docs/images/metal/storage.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/storage.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=fc3f3336869931538dc74b9340bfb276 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/storage.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=1948b04f8ff7b7a15d344e3fa70a8232 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/storage.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=f5fbda3db9f639922246ca6c0be8c11f 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/storage.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=74999b7db20436b4b27ad64c4813453a 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/storage.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=1670ae210f2974966a209dbcc3a61da6 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/storage.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=fd13087ee5563e31f00d9b5b9e1f953f 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/storage-darkmode.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=225f437db1b103b0af5d725fad955a9b" className="hidden dark:block" alt="Metal storage info" data-og-width="1450" width="1450" data-og-height="1290" height="1290" data-path="docs/images/metal/storage-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/storage-darkmode.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=2b82638773136e2b233f58f580b9063b 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/storage-darkmode.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=22150b9eedb91d1fd179a723d4fb5368 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/storage-darkmode.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=32f0668a07490725ca2ed6b64b280bba 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/storage-darkmode.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=89f536d173fabe20c36240ceda36b22f 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/storage-darkmode.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=51e50ef2c3e6834ebcb9a12a8d96f0bb 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/storage-darkmode.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=d98bdd763cce8678d1c5b0a6d2dacfe6 2500w" />
</Frame>

You should make a habit of regularly logging in and checking the health of your database, keep an eye on this number.
If PlanetScale detects that you have only 6GiB or less of available storage, it will cause your database to reject writes, preferring to keep the database available rather than cause a total system failure due to running out of storage.
This is a safety measure put in place to protect your data.
You should upgrade to a larger instance long before reaching this point.
You can upgrade to a larger Metal instance / drive using the same set of steps described above.

Additionally, operations such as deploy requests may not run if you do not have enough storage.
The exception to this is if you are performing an [instant deployment](/docs/vitess/schema-changes/deploy-requests#instant-deployments).

Deploying online schema changes with VReplication requires that we [make a copy of the affected tables](/docs/vitess/schema-changes/how-online-schema-change-tools-work#initializing-the-ghost-table-schema).
If you are nearing max capacity or making a change on a very large table, you risk not having enough storage to begin the online schema change.
We will let you know that there is not enough space to create a deploy request in these cases.

It is critical to upgrade your instance storage size well before you are nearing max capacity.
We will sent you email notices when your database storage reaches the following thresholds: 60%, 75%, 85%, 90%, 95%.
We will also email you when we estimate that your storage will run out in 1 week and 24 hours, based on recent usage trends.

The exact point at which you should upgrade depends on your data growth rate, drive size, and other factors.
We recommend upgrading no later than at 75% capacity, and even before that in some cases of fast growth.
Upgrading to a larger drive takes time, as it requires copying your database to new drives, so it's important to upgrade well before hitting max capacity.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# How to check performance after upgrading to Metal
Source: https://planetscale.com/docs/metal/metal-performance

PlanetScale [Metal](/docs/metal) databases offer performance benefits for many types of workloads.

## Upgrading to Metal

When upgrading your existing PlanetScale database to Metal, its useful to know where to look to see how the upgrade has improved performance.
Here, we cover the two main places you can inspect: Insights and the database metrics panel.

In this page, we will show the results of upgrading from a `PS-640` to an `M-640`.
These both have 8 vCPUs and 64GB of RAM, but use a different underlying storage system that allows for the `M-640` to have improved performance.
Let's look at the effects of this in PlanetScale.

## Insights

After upgrading to Metal, Insights is a great place to look to see how performance has changed.
When you visit the Insights page, there are several tabs you can use to view graphs for different statistics.
The first and default one is a query latency chart.

### Query latency

After zooming in to the time period when we upgraded to Metal, here is what we see:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-p50-p95-p99.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=5c0926c621b244ce54a1589e5457b7c5" className="block dark:hidden" alt="Insights query latency" data-og-width="3356" width="3356" data-og-height="2286" height="2286" data-path="docs/images/metal/insights-p50-p95-p99.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-p50-p95-p99.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=873fb00f0a34d4029873620539cd1621 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-p50-p95-p99.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=8b37e611c2a267e9783bf220954f2c72 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-p50-p95-p99.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=9c24b565e3083a754910eb89cd3568e4 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-p50-p95-p99.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=ef38ddc4b0b6cc387b3aae46342b20be 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-p50-p95-p99.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=c6168ae24036bfff71b4ef2988e3648a 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-p50-p95-p99.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=76aad6891c7470983f17d07b02a80843 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-p50-p95-p99-darkmode.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=24be79a237990f084d92fe4bf3c1bf82" className="hidden dark:block" alt="Insights query latency" data-og-width="3356" width="3356" data-og-height="2286" height="2286" data-path="docs/images/metal/insights-p50-p95-p99-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-p50-p95-p99-darkmode.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=53291c1d46bb57ff95f9e96e551bbfbe 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-p50-p95-p99-darkmode.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=8ca956d6eb375f2297fc75d505eb5686 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-p50-p95-p99-darkmode.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=b2954a9748b4dbf153ddda8b36332b80 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-p50-p95-p99-darkmode.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=b846a0c2772413f0ebbed219fba8b789 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-p50-p95-p99-darkmode.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=3579d7c4ee78fec24088bbb6b297dcd0 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-p50-p95-p99-darkmode.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=9adeaa2a58edd77aa3d1d7c58d7693bd 2500w" />
</Frame>

Insights shows indicators on cluster resize and VTGate resize events.
We can see in the graph above the purple vertical line indicates when we upgraded to Metal (plus or minus a few minutes).
This clearly correlates with an order-of-magnitude or more improvement in p95, and p99 query latencies.
You can also click the "p99.9" icon to enable this additional metric, and see yet another significant improvement.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-p50-p95-p99-p99.9.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=3c104c335ece3fa2160dd352dd57fb44" className="block dark:hidden" alt="Insights query latency" data-og-width="3354" width="3354" data-og-height="2286" height="2286" data-path="docs/images/metal/insights-p50-p95-p99-p99.9.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-p50-p95-p99-p99.9.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=33b218b0ebc0fde64db10e866a238ea7 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-p50-p95-p99-p99.9.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=1284913e1971da6d4ac47588af75f9ff 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-p50-p95-p99-p99.9.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=829eb6bf55c4e668c55f37191d417712 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-p50-p95-p99-p99.9.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=4e1aa99675a561492e9fea4bc31b3835 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-p50-p95-p99-p99.9.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=709773de9ac6eca3cf52d282d1c61901 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-p50-p95-p99-p99.9.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=b046f54807ba204fac5fbf9d01a55dd3 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-p50-p95-p99-p99.9-darkmode.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=87bb2af42bff5022ff3672d801d9dffd" className="hidden dark:block" alt="Insights query latency" data-og-width="3356" width="3356" data-og-height="2286" height="2286" data-path="docs/images/metal/insights-p50-p95-p99-p99.9-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-p50-p95-p99-p99.9-darkmode.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=e382a812e108376ffe0f2b41e1821088 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-p50-p95-p99-p99.9-darkmode.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=a5d81f318d76f5bc8d1c59275470e129 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-p50-p95-p99-p99.9-darkmode.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=271637c4dad22aa51e15c6d3b508846a 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-p50-p95-p99-p99.9-darkmode.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=d070f1a4b60337e0685f5382538c44e3 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-p50-p95-p99-p99.9-darkmode.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=f16ee9d92d523e1413f28ef41c930520 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-p50-p95-p99-p99.9-darkmode.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=31fd6306431c3bc815e5003fff91b738 2500w" />
</Frame>

### Anomalies

The anomalies graph shows the % of queries that are considered anomalous, or slow-running.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-anomalies.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=cbf4f51fde22c00a47acea28edf77d9d" className="block dark:hidden" alt="Insights anomalies" data-og-width="3356" width="3356" data-og-height="2286" height="2286" data-path="docs/images/metal/insights-anomalies.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-anomalies.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=bf6a63369104d86d3162d22c0888a456 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-anomalies.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=b50805e9f9c68c1b932d5035dd65df20 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-anomalies.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=62f485af10568d236bf712b7f4de939f 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-anomalies.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=83f9bb03489776c53e0fee581d32ef3b 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-anomalies.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=65ebd07b3176cd5899fa1c36a321c439 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-anomalies.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=c6379f92341aeb5280ef15f88a770ccb 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-anomalies-darkmode.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=3dab38d383d64b18ea10183419a29ade" className="hidden dark:block" alt="Insights anomalies" data-og-width="3356" width="3356" data-og-height="2286" height="2286" data-path="docs/images/metal/insights-anomalies-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-anomalies-darkmode.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=dd4b97f921c7c1b8fd27d58292cfbbee 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-anomalies-darkmode.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=231da393af64981d29e31062beeaa245 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-anomalies-darkmode.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=6f31039560ae24dbbafff096f1f3063c 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-anomalies-darkmode.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=736174d4e8cf12d31dedd63d6ed6dbcb 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-anomalies-darkmode.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=8c82e6727f1bf5a98e0c3751031200c4 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-anomalies-darkmode.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=b0de8819b3bb7c3c686dbd4f566e9673 2500w" />
</Frame>

We already had a very small number (less than 0.3%) but even so we see an improvement.

### Queries

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-qps.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=19b002dcdef8801f0366109d6944ef3b" className="block dark:hidden" alt="Insights queries" data-og-width="3356" width="3356" data-og-height="2286" height="2286" data-path="docs/images/metal/insights-qps.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-qps.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=abe1759126df29351a05b7a779ba139c 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-qps.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=a7ec93288befd8167aa4dae57095a948 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-qps.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=9cb0e8c0cc0d95104f9c982e34178913 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-qps.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=cabdd763f00dce299663f67c9a13a369 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-qps.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=981cc2f9bf571030c1a0ef1912244ced 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-qps.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=b380801d68ecaf3680f39998cc1a1e8c 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-qps-darkmode.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=820e6dc713cd8684c673c212ec3804ba" className="hidden dark:block" alt="Insights queries" data-og-width="3356" width="3356" data-og-height="2286" height="2286" data-path="docs/images/metal/insights-qps-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-qps-darkmode.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=75b8053141c73871d893c48c45020034 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-qps-darkmode.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=61eb4fe49f986bc9915f5d1d4c3e183e 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-qps-darkmode.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=e43582047f76c37af8d533b37104f588 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-qps-darkmode.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=9cefb44a72ace43ce8f0a216638e127d 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-qps-darkmode.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=36a3c6f14e4e9f9c30eb72f684141808 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-qps-darkmode.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=a3c8bc1bdb4f8a51b8dbc7bd3a73dcc3 2500w" />
</Frame>

We also see a drastic improvement in average QPS.
Prior to switching over we were averaging around 2k queries per second, and after the average is closer to 14k QPS.

At the moment PlanetScale switched your traffic from the non-Metal database to the new Metal one, queries are buffered for a short span of time (seconds) to allow Vitess to do the failover to the new Metal primary and replicas.
After the failover completed, the buffered queries are quickly pushed to the database to be fulfilled.
This is a zero-downtime operation, but you may see a short spike in QPS or latency when the failover occurs.

In this example, we are using synthetic TPCC load.
In a real production workload you likely would not see such a large QPS jump, as that is dependant on demand from connected application servers.
This similarly applies for row reads and writes below.
However, what you may find is that an instance size with the same compute specs is able to handle more QPS load before needing to upgrade.
The lower IO latency and unlimited IOPS allow you to squeeze more work out of a given node in your database cluster.

### Rows read

This is the rows read graph:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-rows-read.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=7ec6c2b0f698fd564651130a841eef5e" className="block dark:hidden" alt="Insights rows read" data-og-width="3356" width="3356" data-og-height="2286" height="2286" data-path="docs/images/metal/insights-rows-read.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-rows-read.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=cc8c18a72d001906591c8c1b29a7bb25 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-rows-read.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=4f02214adf45e37f30e84d15de730ef6 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-rows-read.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=279eddab8e1740ae306fee12afb92479 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-rows-read.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=ebab6037a86297f7e0ebcb7964f21353 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-rows-read.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=2557053692c05b667e8af3a1bf37f8d4 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-rows-read.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=cb1a3ac03c197f7d0f7fb49b95f75028 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-rows-read-darkmode.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=820e189ed1b5b5bb7e1c42f1824ee1cc" className="hidden dark:block" alt="Insights rows read" data-og-width="3356" width="3356" data-og-height="2286" height="2286" data-path="docs/images/metal/insights-rows-read-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-rows-read-darkmode.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=92f77cb95c2d7ad6f6bf8b33f288b5f8 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-rows-read-darkmode.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=c8210f7ced8c0bbbf8947a0674a8892d 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-rows-read-darkmode.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=4c96910125a7be100cc232794a29347c 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-rows-read-darkmode.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=e62b853ba5c423bb1bc4ebe335f169cd 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-rows-read-darkmode.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=861927b7ad92c41b407672c4f6fea483 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-rows-read-darkmode.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=0beb8918e82f9a928922fa2487a5b53d 2500w" />
</Frame>

We can definitely see that there was an increase from the pre-Metal rows read.
However, we also see a jump for a short burst before the switch over to Metal.

This increase appears due to the way upgrading to Metal works.
When you upgrade a typical Scaler Pro database to Metal, the (greatly simplified) steps that happen in the background are:

* (A) PlanetScale spins up three new NVMe-backed instances for you (one primary and two replicas)
* (B) Once these are up and running with all the necessary MySQL and Vitess components, we begin copying the data from your existing database onto the three NVMe drives, starting with restoring from the most recent backup.
* (C) Once the backup is restored, we catch the backup up to the current state of your other database.
  This can be as quick as a few minutes or seconds if there isn't much new data, but can take some time, as we see here, if it has a lot of data to catch up.
* (D) Once the state of the old and new databases are identical, the cut over is made and the Metal instances begin serving your database traffic.
* (E) The old instances are torn down.

Depending on your database and growth rate, you may see a similar spike due to step (C).
This could be as short as a minute, or as slow as over an hour, depending on the characteristics of your database workload.

### Rows written

We get a clear jump in rows written after the upgrade:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-rows-written.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=8a50a94417b6f78e95b653add8605341" className="block dark:hidden" alt="Insights rows written" data-og-width="3356" width="3356" data-og-height="2286" height="2286" data-path="docs/images/metal/insights-rows-written.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-rows-written.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=0917e0bd02b6a1e23b4f4a0a52ef89f9 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-rows-written.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=fb32d5a4240b321dbeee50ca88e1dcb7 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-rows-written.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=08d39a01b5ae26d6f54b3d3972a56ab4 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-rows-written.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=4db2c812bf56bd24d407e31c40ac3116 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-rows-written.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=21456ad0da9ef14ca96b77e11524a326 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-rows-written.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=23f424896b941b49113e5aeb77fe8249 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-rows-written-darkmode.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=992542f22a53ab5ba61a0e9ec09e2aa9" className="hidden dark:block" alt="Insights rows written" data-og-width="3356" width="3356" data-og-height="2286" height="2286" data-path="docs/images/metal/insights-rows-written-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-rows-written-darkmode.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=0f1ecd7fec8bf702c395bf320772cd08 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-rows-written-darkmode.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=12cdf44af3190a24908bd7899745b234 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-rows-written-darkmode.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=5c45c457b17bedac4432093fb4838998 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-rows-written-darkmode.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=5fed9d2375f21d915f0b3fcc1a6958a3 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-rows-written-darkmode.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=dc3e83ce82934d4e3782c2f5e9a550a2 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/insights-rows-written-darkmode.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=94fc1293f83ae296753aae78888acf68 2500w" />
</Frame>

Unlimited IOPS allows for increased write throughput.
We can see that before the switch to Metal, we were only utilizing 20-25% of our CPU capabilities, and much of this was due to being I/O bottlenecked for this particular workload.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/metal/cpu-ram.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=8fd610ae64cfce5be383697781a077de" className="block dark:hidden" alt="Insights rows written" data-og-width="1182" width="1182" data-og-height="414" height="414" data-path="docs/images/metal/cpu-ram.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/metal/cpu-ram.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=411ddc584966056894ad7b385e4f82a7 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/metal/cpu-ram.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=e2ac6d9574b19c29a12d26bce58435bf 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/metal/cpu-ram.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=dd21691d1d902a1b848f3ea4f524da1f 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/metal/cpu-ram.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=9f0e8a10649b9ad6da1e241c2345ab00 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/metal/cpu-ram.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=269c92cf3a3b97296c2ffcee42b23c45 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/metal/cpu-ram.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=c9557f62a10ef2c92a2cdc01cbcea59e 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/metal/cpu-ram-darkmode.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=8422c1acdd8a8398a6bdc8e277de264a" className="hidden dark:block" alt="Insights rows written" data-og-width="1172" width="1172" data-og-height="416" height="416" data-path="docs/images/metal/cpu-ram-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/metal/cpu-ram-darkmode.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=14bcbc0f56463ab99a31a6eceec8eaa9 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/metal/cpu-ram-darkmode.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=66e02965f12e3a13825ed4ab07b7414e 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/metal/cpu-ram-darkmode.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=59d0cce695ccad47f144b3e96c64186b 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/metal/cpu-ram-darkmode.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=68cde1ac32af6a7b8db901e7870a4fcb 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/metal/cpu-ram-darkmode.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=896104e4de9858296280f081a8004f6f 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/metal/cpu-ram-darkmode.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=71d3ad66d3c3e7ee43e8b45eeb809dd7 2500w" />
</Frame>

Once we switched to Metal and had faster IO operations and unlimited IOPS, we were suddenly able to increase our CPU utilization to 80%+.

## Primary metrics

Another good place to look after upgrading to Metal is the metrics for your primary database node.
From the database's Dashboard page, click on your primary from the architecture diagram.
A panel should display on the right side of your screen with metrics from this instance.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/primary-metrics.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=f386833d8ba582d2d7d9ffb4604a6895" className="block dark:hidden" alt="Database primary metrics" data-og-width="1334" width="1334" data-og-height="2072" height="2072" data-path="docs/images/metal/primary-metrics.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/primary-metrics.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=7aa325e40ca944da0be3b14aaa234d29 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/primary-metrics.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=937e38ec2bd39fdd169bd5292fdd5d7b 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/primary-metrics.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=9e3d75ac0efe5b9ca15c7e762cef8c7d 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/primary-metrics.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=513e5e89718b9bb08941f4c914425c61 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/primary-metrics.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=d2c3610ba123a10fd874ad2cd8719798 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/primary-metrics.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=068b3f08c8cfed33e059a56d9bc936d8 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/primary-metrics-darkmode.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=2876d5e773d36bb24ea1aba76fed8f1d" className="hidden dark:block" alt="Database primary metrics" data-og-width="1334" width="1334" data-og-height="2072" height="2072" data-path="docs/images/metal/primary-metrics-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/primary-metrics-darkmode.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=db72fbf8a787992642c345c99ddab6be 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/primary-metrics-darkmode.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=6afe907e0ba4f08dce8b4e741ec0dfbd 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/primary-metrics-darkmode.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=f0710611b3d11a5ebac648c0f444c608 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/primary-metrics-darkmode.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=038b312f1e7ce27ae9f66de4de254c63 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/primary-metrics-darkmode.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=ac0a3b2fc5169b3713db7e5cb1677c4a 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metal/primary-metrics-darkmode.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=fa01712611970a5d1b4799e2a1972fe3 2500w" />
</Frame>

If the change was recently made, you can change the graph to display metrics for only the past hour using the drop down.
This gives you good insight into how the upgrade affected your IOPS capabilities, as well as CPU and RAM.
Note specifically the large jump in IOPS after the switch to Metal.

## Why is Metal so much faster?

PlanetScale databases come in two main flavors: **Metal** and **network-attached storage**.

[**Network-attached storage**](/docs/plans/planetscale-skus#network-attached-storage) (Amazon Elastic Block Storage or Google Persistent Disk) databases store all data on storage volumes that are attached to your database's compute resources over the network.
For databases in AWS we use Elastic Block Storage (EBS) and in GCP we use Persistent Disk.
These network-attached storage solutions are convenient for several reasons.
For one, it is easy to resize such storage volumes.
PlanetScale leverages this to auto-scale your storage as the size of your database grows or shrinks, allowing you to pay a per-GB storage price.

This also means that you can pair a tiny compute instance with a large amount of storage.
This works well for large data sets that are not frequently queried.
The opposite is also true — you can pair a small amount of storage with a large compute instance for workloads that are heavily CPU-bound.

One disadvantage of using **network-attached storage** storage is I/O latency.
Reads from and writes to disk need to make network round-trips to be fulfilled.
The intra-AZ network speeds in AWS and GCP data centers are generally very good, but still slower than accessing a locally-attached solid-state drive.
There is also the issue of IOPS.
The popular `gp3` EBS volume class provides 3000 IOPS included, but using more than this requires paying for additional IO bandwidth, leading to more expensive databases.

These disadvantages do not just apply to PlanetScale.
Many of the popular cloud-hosted database solutions, including those offered by Amazon and Google, use network-attached storage to simplify storage scalability.
This convenience and scalability comes at a performance cost.

**Metal** databases store all data on locally-attached NVMe SSDs.
Using direct-attached storage provides a clear solution to the performance issues described above.
The removal of network round-trips for I/O operations means low-latency IO, and we sidestep the issue of needing to pay for increased IOPS completely.
Your database now has the ability to use modern NVMe SSD technology to it's full potential.

## Data durability

One of the most important aspects of a database is storing data durably, even in the face of unexpected outages or hardware failure.

By default, all databases on PlanetScale have their data replicated three times: once on the primary database node, and then again on each of the two replicas.
More replicas can be added if desired.

When using a network-attached storage database, each of the three database instances stores a copy of the data on three distinct network-attached volumes.
When using network-attached storage, the compute instances and storage volumes are decoupled.
If one of your database compute nodes gets taken down due to an underlying hardware failure, the data is still preserved on the EBS or PD volume and can be quickly re-attached to a new node.
PlanetScale handles the detection, re-attachment, and recovery from such failures automatically.

A Metal cluster also has high durability, but with different characteristics.
Each of the three database instances (1 primary, 2 replicas) stores a copy of the data on the NVMe drives attached to the instance.
However, in this case, if one of the three compute nodes goes down, the data also goes with it.
This is why replication is so critical.
In this scenario, we still have two other copies of the data, and PlanetScale will automatically detect and replace the node that failed, bringing the total back up to three.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale plans
Source: https://planetscale.com/docs/planetscale-plans



## Overview

PlanetScale is built to accommodate all companies at all stages. Whether you need a hassle-free managed database for your side project or you’re running millions of queries per second at the scale of YouTube, we have a solution for you.

Our plans are split into two general offerings: [Base (self-serve)](#base) and [Enterprise](#planetscale-enterprise-plan).

<Columns cols={2}>
  <Card title="Base" icon="database" href="#base">
    PlanetScale for Vitess offers fully-managed Vitess clusters with unlimited scalability. Features include sharding, branching, deploy requests, query insights, and more.

    <a href="/docs/vitess">View the Vitess docs</a>
  </Card>

  <Card title="Enterprise" icon="rocket" href="#planetscale-enterprise-plan">
    PlanetScale Postgres is a fully-managed PostgreSQL-compatible database. Features include high availability, query insights, branching, and more.

    <a href="/docs/postgres">View the Postgres docs</a>
  </Card>
</Columns>

## Base

Our base plan is completely self-serviceable. Just sign up for a PlanetScale account to get started.

PlanetScale databases come in two flavors: **network-attached storage** and **Metal**.
[Network-attached storage](/docs/plans/planetscale-skus#network-attached-storage) (Amazon Elastic Block Storage or Google Persistent Disk) databases come with autoscaling storage and have varying levels of compute power.
[Metal databases](/docs/plans/planetscale-skus#metal) are backed by locally-attached NVMe drives for storage, unlocking incredible performance and cost-efficiencies. Because the drives are locally-attached, you need to choose both your compute and storage resources when you create your database.

<Note>
  Cluster size options are capped to `PS-160` and `M-320` SKUs until you have a successfully paid an invoice of at least \$100.
  If you need larger sizes immediately, please [contact us](https://planetscale.com/contact) to unlock all sizes.
</Note>

On top of processing and memory, all **Base** cluster sizes share the following:

|                                                                   | **Vitess**                                                                                                    | **Postgres**                                                                                                                                              |
| :---------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------ | :-------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Storage/month (network-attached storage)**                      | 10 GB included; \$0.50 per instance per additional 1 GB\*                                                     | 10 GB included; \$0.50 per instance per additional 1 GB\*                                                                                                 |
| **Storage/month (Metal)**                                         | Depends on selected NVMe drive size                                                                           | Depends on selected NVMe drive size                                                                                                                       |
| **Row reads/month**                                               | *Unmetered*                                                                                                   | *Unmetered*                                                                                                                                               |
| **Row writes/month**                                              | *Unmetered*                                                                                                   | *Unmetered*                                                                                                                                               |
| **Available cluster sizes**                                       | 22                                                                                                            | 22                                                                                                                                                        |
| **Availability zones**                                            | 3                                                                                                             | 3                                                                                                                                                         |
| **Production branches**                                           | 1 included\*\*                                                                                                | 1 included                                                                                                                                                |
| **Development branches**                                          | \~1,440 hours included (2× hours of current month)                                                            | Billed as a new cluster                                                                                                                                   |
| **Concurrent Connections**                                        | *Unmetered*                                                                                                   | *Unmetered*                                                                                                                                               |
| **Single node**                                                   | Not available                                                                                                 | Available starting at \$5/mo                                                                                                                              |
| **Query Insights retention**                                      | 7 days                                                                                                        | 7 days                                                                                                                                                    |
| **Horizontal sharding**                                           | Included                                                                                                      | [Coming soon](https://planetscale.com/blog/announcing-neki)                                                                                               |
| **Vertical sharding**                                             | Included                                                                                                      | Included                                                                                                                                                  |
| [**Deployment options**](/docs/plans/deployment-options)          | Multi-tenant                                                                                                  | Multi-tenant                                                                                                                                              |
| **Read-only regions**                                             | Available as an add-on                                                                                        | Coming soon                                                                                                                                               |
| **Web console**                                                   | Included                                                                                                      | Not available                                                                                                                                             |
| **PlanetScale CLI**                                               | Included                                                                                                      | Included                                                                                                                                                  |
| **Connection pooling**                                            | [VTGates](/docs/vitess/scaling/vtgates) based on cluster size                                                 | [PgBouncer](/docs/postgres/connecting/pgbouncer)                                                                                                          |
| **SSO**                                                           | Available as an add-on\*\*\*                                                                                  | Available as an add-on\*\*\*                                                                                                                              |
| **Audit log retention**                                           | 6 months                                                                                                      | 6 months                                                                                                                                                  |
| **Private connections**                                           | [AWS](/docs/vitess/connecting/private-connections) and [GCP](/docs/vitess/connecting/private-connections-gcp) | [AWS](/docs/postgres/connecting/private-connections/aws-privatelink) and [GCP](/docs/postgres/connecting/private-connections/gcp-private-service-connect) |
| **BAAs**                                                          | Included                                                                                                      | Included                                                                                                                                                  |
| **Automatic backups**                                             | Every 12 hours                                                                                                | Every 12 hours                                                                                                                                            |
| **Support**                                                       | Standard, upgrade available\*\*\*                                                                             | Standard, upgrade available\*\*\*                                                                                                                         |
| [**Data Branching®**](/docs/vitess/schema-changes/data-branching) | Included                                                                                                      | Not available                                                                                                                                             |

\* For HA network-attached storage databases, production branch storage is billed at $1.50/GB (1 primary + 2 replicas) and development branch storage is billed at $0.50/GB (1 primary).

\*\* Additional production branches are billed at the cost of your selected cluster size per month.

\*\*\* [SSO](/docs/security/sso) and [Business support](/docs/support#business) options are available on the base plan for an additional fee.

### Additional production branches

Each HA production branch in the base plan provisions a separate, production database cluster in our infrastructure. Upon adding an additional production branch, you will be prompted to select the cluster size for the new branch.

Each cluster size is priced based on the selected region. You can find the full list of pricing in our [Vitess cluster pricing documentation](/docs/plans/cluster-sizing).

If you have a `main` network-attached storage production branch using the **PS-40** cluster size and two additional production branches using the **PS-20** cluster size, the total cost for the database would be **\$217.00** per month.

| **Production branch cluster** | **Cost per unit** | **Quantity** | **Total per month** |
| :---------------------------- | :---------------- | :----------- | :------------------ |
| PS-40                         | \$99.00           | 1            | \$99.00             |
| PS-20                         | \$59.00           | 2            | \$118.00            |
| **Grand total**               |                   |              | **\$217.00**        |

Also note that pricing is prorated.
If you create a new database in the middle of a billing cycle, you will only be charged for the appropriate fraction of the month.
This also applies to changes to an existing database, such as upsizing.
For example, if you have a database that started the month as a `PS-10` and at the halfway point in the month you upgrade to a `PS-20`, you would be charged $39/2 + $59/2 = \$49 (assuming no additional other charges for storage, etc). The billing for the new sizes begins as soon as you begin the cluster resize.

### Development branches

Billing for development branches differs depending on whether you're using PlanetScale Postgres or Vitess.

### Vitess development branches

Development branches, `PS-DEV` are billed for the time that they are running, prorated to the nearest second. Databases include `hours_in_current_month * 2` of development branch time per month (1,440 hours for a 30 day month) at no additional cost. Any time used over the included is billed at a rate of \~$0.014 per hour (`$10 / hours\_in\_current\_month\`). You may see how many development branch hours have been used at any time by visiting your [organization billing page](https://app.planetscale.com/~/settings/billing/). Data is updated hourly.

### Postgres development branches

Postgres development branches, `PS-DEV`, are billed for the time that they are running, prorated to the nearest second. Each development branch is \$5 per month. Development branches are not intended for HA production traffic, as they do not come with any replicas or have maintenance windows.

<Note>
  You can set spend email alerts from your billing page. See the [Spend management documentation](/docs/billing#spend-management) for more information.
</Note>

If a database is created in the middle of a billing cycle, the included development branch hours are prorated. For example, if you create your database with 15 days remaining in the current month, the database will have `15 days * 2` (720 hours) included for that billing period.

### Fractional vCPU allocation

Some tiers of the base plan indicate a fractional vCPU allocation. These branches run on our multi-tenant platform and this indicates the minimum number of cycles dedicated to your workload. The vast majority of the time, there are spare compute cycles available on the underlying machine instances hosting your branch, and those are available to be used by your workload as needed for no additional charge.

If you find the performance of a given query to be substantially inconsistent over the course of a given day, you may want to upgrade to a higher tier for more consistent performance.

## PlanetScale Enterprise plan

PlanetScale's Enterprise Plan is great for large-scale businesses who require the enterprise-level SLAs, want expert assistance through enterprise support, and would prefer to run in your own AWS or GCP account.

We offer many different deployment options, all of which come with the same set of standard features. The table below covers those shared features, as well as the different options that vary depending on your chosen deployment.

|                                                        | PlanetScale Enterprise                  | **[PlanetScale Managed](/docs/plans/managed)**                                                                                                                                                                                    |
| :----------------------------------------------------- | :-------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Horizontal sharding                                    | <Icon icon="check" color="blue" />      | <Icon icon="check" color="blue" />                                                                                                                                                                                                |
| Customizable feature limits                            | <Icon icon="check" color="blue" />      | <Icon icon="check" color="blue" />                                                                                                                                                                                                |
| [Support](/docs/support)                               | Business — Enterprise upgrade available | Enterprise                                                                                                                                                                                                                        |
| PCI compliant                                          | <Icon icon="xmark" color="red" />       | <Icon icon="check" />                                                                                                                                                                                                             |
| Dedicated AWS/GCP account                              | Optional                                | <Icon icon="check" color="blue" />                                                                                                                                                                                                |
| Bring your own cloud (an AWS or GCP account *you* own) | <Icon icon="xmark" color="red" />       | <Icon icon="check" color="blue" />                                                                                                                                                                                                |
| Billing                                                | Directly from PlanetScale               | Partial payment through PlanetScale and infrastructure costs through AWS/GCP. Take advantage of your own discounts. Optionally, purchase through [AWS Marketplace](https://aws.amazon.com/marketplace/pp/prodview-luy3krhkpjne4). |

## How do I know if I need an Enterprise plan?

If you’re not sure whether your use case requires an Enterprise plan, we’re more than happy to chat with you to figure it out together. You can [fill out this form](https://planetscale.com/contact), and we’ll be in touch.

In general, if you need any of the following, Enterprise may be the best solution for you:

* Enhanced support — our expert team becomes an extension of your own. Additional options for technical account management, Slack-based support, and phone escalation.
* You need additional support to horizontally shard and migrate your database(s) to PlanetScale
* You need your database deployed in a single-tenant environment
* You need to keep your data in **your own** AWS or GCP account
* You need a PCI DSS certified service provider
* Mission-critical [response times](/docs/support#initial-response-times) including continuous support coverage
* Any other customizations — Our Enterprise plans offer a lot of flexibility, so if you have a requirement that’s not listed here, it’s best to [reach out](https://planetscale.com/contact) and we can see how we can help

## Plan add-ons

### Single Sign-on (SSO)

You can [add SSO](/docs/security/sso) for your organization on the Scaler Pro plan for an additional fee of \$199/month.

### User-scheduled backups

On the Scaler Pro plan, we run automated backups every 12 hours. Disk space for default backups is not counted against your plan's storage limit.

You can also [schedule additional backups yourself](/docs/vitess/backups#create-manual-backups) as needed. For these additional user-scheduled backups, storage is billed at **\$0.023 per GB** per month. Backups include system tables as well as your data and start at around 140MB.

## Discounts

We offer two types of discounts:

* **Consumption commitment** — Save 2.5% if you agree to an annual consumption commitment.
* **Upfront payment** — Save an extra 10% if you pay your annual bill upfront.

You have the option to combine these two discounts for a total of 12.5% off your bill if you commit to an annual consumption commitment dollar amount **and** pay for the year upfront.

<Note>
  To be eligible for either of these discounts, you must commit to a minimum of \$5,000 per year.
</Note>

If you're interested in either of these discounts, please fill out our [contact form](https://planetscale.com/contact) and let us know which discount you're interested in, along with the following information:

* Annual spend commitment amount (in dollars)
* PlanetScale organization name (if it exists)
* Company name
* Company address
* Business point of contact name, email, phone
* Billing name, phone, email, address

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Cluster sizing
Source: https://planetscale.com/docs/plans/cluster-sizing



export const YouTubeEmbed = ({id, title}) => {
  return <Frame>
      <iframe src={`https://www.youtube-nocookie.com/embed/${id}?rel=0`} title={title} className="aspect-video w-full" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" />
    </Frame>;
};

You can easily upsize and downsize your database cluster from within the PlanetScale dashboard. This documentation covers some information about selecting a cluster size upon database creation as well as how to upsize and downsize.

<Note>
  If you are on a consumption commitment plan, please be aware that any changes in cluster size will be reflected against your monthly or annual consumption commitment amount. Changes to the originally selected cluster size may cause you to utilize this amount either more quickly or slowly. If you have further questions, please reach out to your account manager or our [Support](https://planetscale.com/contact) team.
</Note>

<YouTubeEmbed id="y94VExata3A" title="Resize your database" />

## Selecting a cluster size

Selecting the correct cluster size for your database can have a dramatic impact on how it performs and how much it costs.

A good rule of thumb is when you notice CPU usage is consistently at or close to 100% for an extended period of time, you may benefit from [upsizing your cluster](/docs/plans/planetscale-skus#upsizing-and-downsizing-clusters). Conversely, if your CPU usage is consistently below 50%, you may be able to downsize. You can monitor your CPU usage by clicking on your database, clicking "Primary" in your architecture diagram, and referencing the chart under "Metrics and performance".

<Warning>
  For Metal instances, you have to consider both the compute and the storage, as storage does not autoscale. For more information about adjusting a Metal instance, see [Upgrading an existing database to Metal](/docs/metal/create-a-metal-database#upgrading-an-existing-database-to-metal).
</Warning>

There are also special cases where you may want to temporarily upsize out of caution if you're anticipating a large spike in traffic, such as during a launch or event. In these cases, you can easily [upsize](/docs/plans/#upsizing-and-downsizing-clusters) ahead of your event, and then downsize after.

If you are switching between [network-attached storage](/docs/plans/planetscale-skus#network-attached-storage) (Amazon Elastic Block Storage or Google Persistent Disk) and [Metal](/docs/plans/planetscale-skus#metal), or changing the size of your Metal instance, be aware this switch takes additional time.

### Comparing PlanetScale to other database providers

If you are migrating from an existing cloud provider with resource-based pricing, be sure to compare your currently selected instance with our available cluster sizes.

Keep in mind, each database comes with a production branch with two replicas. Vitess databases include 1,440 hours worth of development branches. The development branches essentially equate to two extra "always on" databases. In many cases, you can deprecate your dev/staging databases that you pay extra for with other providers in favor of the development branches. In the end, this usually results in significant cost savings.

Databases in PlanetScale also come with additional beneficial infrastructure that is not easily configured or available in other hosted database solutions. For more information on what is provisioned with each database, read our [Vitess Architecture](/docs/vitess/architecture) and [Postgres Architecture](/docs/vitess/architecture) docs.

If you are unsure which plan or cluster size is right for your application, [contact us](https://planetscale.com/contact) to get further assistance.

Our self-serve plans are flexible enough to handle the majority of customers. However, there are several use cases where you may need a more custom plan. This is where our Enterprise offerings shine.

## Sharding with Vitess

You can create sharded Vitess keyspaces on any plan by adding a new sharded keyspace using the [cluster configuration page](/docs/vitess/cluster-configuration) and running an [unsharded to sharded workflow](/docs/vitess/sharding/sharding-quickstart) in your dashboard.

If you would like additional support from our expert team, our [Enterprise plan](/docs/planetscale-plans#planetscale-enterprise-plan) may be a good fit. [Get in touch](https://planetscale.com/contact) for a quick assessment.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Deployment options
Source: https://planetscale.com/docs/plans/deployment-options

PlanetScale offers two deployment options to accommodate your application and business needs: multi-tenancy and single-tenancy.

## Overview

**Multi-tenancy** — In a multi-tenant environment, your database is hosted on infrastructure shared with other users/customers.

**Single-tenancy** — In a single-tenant environment, your database is hosted on dedicated infrastructure that is not shared with any other users/customers.

Your deployment options depend on the [PlanetScale plan](/docs/planetscale-plans) you choose. This documentation covers some differences between single-tenancy and multi-tenancy so you can evaluate which plan may be suitable for you. For an in-depth look at the different plans we offer, see the [PlanetScale plans documentation](/docs/planetscale-plans).

## Multi-tenancy deployment on PlanetScale

Multi-tenancy is the default deployment option. When you sign up for a PlanetScale account with a Scaler Pro plan, your databases will be created in our multi-tenancy deployment offering. Our Enterprise plan also offers a multi-tenancy deployment option.

### Multi-tenancy highlights

* Your database runs on PlanetScale's [secure cloud infrastructure](/docs/security)
* Lowest cost plans
* No configuration requirements on your end
* BAAs available for HIPAA compliance
* [Private connection support](/docs/vitess/connecting/private-connections) via AWS PrivateLink
* [Private connection support](/docs/vitess/connecting/private-connections-gcp) via GCP Private Service Connect

## Single-tenancy deployment on PlanetScale

Single-tenant deployment options are available with [PlanetScale Enterprise](/docs/planetscale-plans#planetscale-enterprise-plan). Companies that require their PlanetScale databases to be hosted in a single-tenant environment have two options: Enterprise — Single-tenant and Managed.

<Note>
  In both options, your database is deployed in a single-tenant environment. The main difference between Enterprise — Single-tenant and Managed is who owns the underlying account. With [Managed](/docs/vitess/managed), your database is deployed in your own AWS/GCP account. With Enterprise — Single-tenant, PlanetScale owns and manages the account.

  If you're interested in learning more, please [reach out](https://planetscale.com/contact), and we can figure out the best solution for your use case.
</Note>

### Single-tenancy highlights

* Your resources run on a separate isolated AWS or GCP account, apart from other customers
* Your databases can be deployed to any cloud provider region you choose that offers three Availability Zones, even [regions](/docs/vitess/regions) that PlanetScale does not offer on our self-serve plans
* You can continue to use the PlanetScale UI and CLI in the same way that you would our general self-serve offerings, but the infrastructure runs in an isolated environment
* BAAs available for HIPAA compliance
* Support for private database connectivity via:
  * **AWS** — [AWS PrivateLink](https://aws.amazon.com/privatelink/) (recommended) or [VPC Peering](https://docs.aws.amazon.com/vpc/latest/peering/what-is-vpc-peering.html)
  * **GCP** — [Private Service Connect](https://cloud.google.com/vpc/docs/private-access-options)

If you have any questions about which deployment option may be best for you, [reach out to us](https://planetscale.com/contact) and we can help you figure it out.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale Managed overview
Source: https://planetscale.com/docs/plans/managed



## What is PlanetScale Managed?

PlanetScale Managed is a single-tenant deployment of PlanetScale within your Amazon Web Services (AWS) or Google Cloud Platform (GCP) account. In this configuration, you can use the same API, CLI, and web interface that PlanetScale offers, with the benefit of running entirely in your own AWS or GCP account.

We have packaged the best parts of PlanetScale into a container and can deploy and operate them in your own account, bringing you the best of SaaS with the added benefit of a deployment free of noisy neighbors, enhanced support, and additional security guarantees.

With PlanetScale Managed, it is more than just an on-premises deployment of your database; you are getting the PlanetScale expert team operating your database alongside your team for a *truly* fully managed database solution. The PlanetScale team is on-call for your databases.

## How does PlanetScale Managed work?

PlanetScale Managed is a packaged [data plane](https://en.wikipedia.org/wiki/Data_plane), built on [Vitess and Kubernetes](https://planetscale.com/blog/scaling-hundreds-of-thousands-of-database-clusters-on-kubernetes), that's deployed to an AWS Organizations member account or GCP project that you own and we operate. Your database lives entirely inside a dedicated member account or project within your cloud organization. PlanetScale will not have access to other member accounts or projects nor your organization-level settings within the cloud service provider. At the same time, you still get to interact with your databases through the web application, pscale CLI, or the PlanetScale API, as you usually would with our hosted product. This includes developer experience features such as non-blocking schema changes, safe migrations, database branching, query insights, and more.

If you are an existing PlanetScale user, moving to PlanetScale Managed requires no changes to your existing developer workflows.

The database is deployed in a single-tenant environment and isolated in an AWS Organizations member account or GCP project from the rest of your organization's infrastructure. By default, all connections are encrypted, but public. You have the option to use private database connectivity through [AWS PrivateLink](/docs/vitess/managed/aws/privatelink) or [GCP Private Service Connect](/docs/vitess/managed/gcp/private-service-connect), which are only available on single-tenancy deployment options, including PlanetScale Managed.

Read more on how PlanetScale Managed works inside either cloud provider:

<Columns cols={2}>
  <Card title="PlanetScale Managed on Amazon Web Services" icon="aws" horizontal href="/docs/vitess/managed/aws" />

  <Card title="PlanetScale Managed on Google Cloud Platform" icon="google" horizontal href="/docs/vitess/managed/gcp" />
</Columns>

## Benefits of PlanetScale Managed

Single-tenancy is one of many benefits when it comes to PlanetScale Managed. Still, with this PlanetScale Enterprise service, you also get:

* Assistance with [horizontal sharding](/docs/vitess/sharding)
* Option to sign BAAs for [HIPAA compliance](https://planetscale.com/blog/planetscale-and-hipaa)
* Deployment to additional regions
* [PCI compliance](https://planetscale.com/blog/planetscale-managed-is-now-pci-compliant) (AWS only)
* Additional [support options](/docs/support#enterprise)
* Available on [AWS Marketplace](https://aws.amazon.com/marketplace/pp/prodview-luy3krhkpjne4). Your PlanetScale purchase through the AWS Marketplace and the resources you use on PlanetScale will qualify against your EDP commitment.

## How do I get PlanetScale Managed?

If you are interested in seeing if PlanetScale Managed fits your needs, [contact us](https://planetscale.com/contact), and we can chat more about your requirements and see if PlanetScale Managed is a good fit for you.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Cluster sizes
Source: https://planetscale.com/docs/plans/planetscale-skus

Here you'll find a complete list of all the database sizes available to you on PlanetScale.

If your needs exceed what is available here, we can easily spin up additional sizes for you. Just [reach out](https://planetscale.com/contact) and let us know. We also support [sharding](https://planetscale.com/sharding) on Vitess clusters.

## Network-attached storage

**Network-attached storage** databases come with autoscaling storage and have varying levels of compute power.

|             | **Processor** | **Memory** |
| :---------- | :------------ | :--------- |
| **PS-10**   | 1/8 vCPU      | 1 GB RAM   |
| **PS-20**   | 1/4 vCPU      | 2 GB RAM   |
| **PS-40**   | 1/2 vCPU      | 4 GB RAM   |
| **PS-80**   | 1 vCPU        | 8 GB RAM   |
| **PS-160**  | 2 vCPUs       | 16 GB RAM  |
| **PS-320**  | 4 vCPUs       | 32 GB RAM  |
| **PS-400**  | 8 vCPUs       | 32 GB RAM  |
| **PS-640**  | 8 vCPUs       | 64 GB RAM  |
| **PS-700**  | 16 vCPUs      | 32 GB RAM  |
| **PS-900**  | 16 vCPUs      | 64 GB RAM  |
| **PS-1280** | 16 vCPUs      | 128 GB RAM |
| **PS-1400** | 32 vCPUs      | 64 GB RAM  |
| **PS-1800** | 32 vCPUs      | 128 GB RAM |
| **PS-2100** | 48 vCPUs      | 96 GB RAM  |
| **PS-2560** | 32 vCPUs      | 256 GB RAM |
| **PS-2700** | 48 vCPUs      | 128 GB RAM |
| **PS-2800** | 64 vCPUs      | 128 GB RAM |

## Metal

[Metal databases](/docs/metal) are backed by locally-attached NVMe drives for storage, unlocking incredible performance and cost-efficiencies. Because the drives are locally-attached, you need to choose both your compute and storage resources when you create your database.
The storage options vary by cloud provided, so we break out the options into AWS and GCP sections.

### Metal options on AWS

|            | **Processor** | **Memory** | **NVMe Storage options** | **Notes**     |
| :--------- | :------------ | :--------- | :----------------------- | ------------- |
| **M-10**   | 1/8 vCPU      | 1 GB RAM   | configurable             | Postgres only |
| **M-20**   | 1/4 vCPU      | 2 GB RAM   | configurable             | Postgres only |
| **M-40**   | 1/2 vCPU      | 4 GB RAM   | configurable             | Postgres only |
| **M-80**   | 1 vCPU        | 8 GB RAM   | configurable             | Postgres only |
| **M-160**  | 2 vCPUs       | 16 GB RAM  | configurable             |               |
| **M-320**  | 4 vCPUs       | 32 GB RAM  | 229 GB                   |               |
| **M-320**  | 4 vCPUs       | 32 GB RAM  | 929 GB                   |               |
| **M-320**  | 4 vCPUs       | 32 GB RAM  | 2,490 GB                 |               |
| **M-640**  | 8 vCPUs       | 64 GB RAM  | 466 GB                   |               |
| **M-640**  | 8 vCPUs       | 64 GB RAM  | 1,866 GB                 |               |
| **M-640**  | 8 vCPUs       | 64 GB RAM  | 4,992 GB                 |               |
| **M-1280** | 16 vCPUs      | 128 GB RAM | 942 GB                   |               |
| **M-1280** | 16 vCPUs      | 128 GB RAM | 3,739 GB                 |               |
| **M-2560** | 32 vCPUs      | 256 GB RAM | 1,891 GB                 |               |
| **M-2560** | 32 vCPUs      | 256 GB RAM | 7,492 GB                 |               |

### Metal options on GCP

|            | **Processor** | **Memory** | **NVMe Storage options** |
| :--------- | :------------ | :--------- | :----------------------- |
| **M-160**  | 2 vCPUs       | 16 GB RAM  | 367 GB                   |
| **M-160**  | 2 vCPUs       | 16 GB RAM  | 742 GB                   |
| **M-160**  | 2 vCPUs       | 16 GB RAM  | 1,492 GB                 |
| **M-160**  | 2 vCPUs       | 16 GB RAM  | 2,992 GB                 |
| **M-160**  | 2 vCPUs       | 16 GB RAM  | 5,992 GB                 |
| **M-160**  | 2 vCPUs       | 16 GB RAM  | 8,992 GB                 |
| **M-320**  | 4 vCPUs       | 32 GB RAM  | 367 GB                   |
| **M-320**  | 4 vCPUs       | 32 GB RAM  | 742 GB                   |
| **M-320**  | 4 vCPUs       | 32 GB RAM  | 1,492 GB                 |
| **M-320**  | 4 vCPUs       | 32 GB RAM  | 2,992 GB                 |
| **M-320**  | 4 vCPUs       | 32 GB RAM  | 5,992 GB                 |
| **M-320**  | 4 vCPUs       | 32 GB RAM  | 8,992 GB                 |
| **M-640**  | 8 vCPUs       | 64 GB RAM  | 367 GB                   |
| **M-640**  | 8 vCPUs       | 64 GB RAM  | 742 GB                   |
| **M-640**  | 8 vCPUs       | 64 GB RAM  | 1,492 GB                 |
| **M-640**  | 8 vCPUs       | 64 GB RAM  | 2,992 GB                 |
| **M-640**  | 8 vCPUs       | 64 GB RAM  | 5,992 GB                 |
| **M-640**  | 8 vCPUs       | 64 GB RAM  | 8,992 GB                 |
| **M-1280** | 16 vCPUs      | 128 GB RAM | 742 GB                   |
| **M-1280** | 16 vCPUs      | 128 GB RAM | 1,492 GB                 |
| **M-1280** | 16 vCPUs      | 128 GB RAM | 2,992 GB                 |
| **M-1280** | 16 vCPUs      | 128 GB RAM | 5,992 GB                 |
| **M-1280** | 16 vCPUs      | 128 GB RAM | 8,992 GB                 |
| **M-2560** | 32 vCPUs      | 256 GB RAM | 1,492 GB                 |
| **M-2560** | 32 vCPUs      | 256 GB RAM | 2,992 GB                 |
| **M-2560** | 32 vCPUs      | 256 GB RAM | 5,992 GB                 |
| **M-2560** | 32 vCPUs      | 256 GB RAM | 8,992 GB                 |

## Selecting a cluster size

Selecting the correct cluster size for your database can have a dramatic impact on how it performs and how much it costs.

A good rule of thumb is when you notice CPU usage is consistently at or close to 100% for an extended period of time, you may benefit from [upsizing your cluster](#upsizing-and-downsizing-clusters). Conversely, if your CPU usage is consistently below 50%, you may be able to downsize. You can monitor your CPU usage by clicking on your database, clicking "Primary" in your architecture diagram, and referencing the chart under "Metrics and performance".

There are also special cases where you may want to temporarily upsize out of caution if you're anticipating a large spike in traffic, such as during a launch or event. In these cases, you can easily [upsize](#upsizing-and-downsizing-scaler-pro-clusters) ahead of your event, and then downsize after. Changing cluster sizes is a seamless operation that requires no downtime.

If you are migrating from an existing cloud provider with resource-based pricing, be sure to compare your currently selected instance with our available cluster sizes.

Keep in mind, each database comes with a production branch with two replicas, as well as 1,440 hours worth of development branches (for Vitess databases). The development branches essentially equate to two extra "always on" databases. In many cases, you can deprecate your dev/staging databases that you pay extra for with other providers in favor of the development branches. In the end, this usually results in significant cost savings.

Databases in PlanetScale also come with additional beneficial infrastructure that is not easily configured or available in other hosted database solutions. For more information on what is provisioned with each database, read our [Vitess architecture](/docs/vitess/architecture) and [Postgres architecture](/docs/postgres/postgres-architecture) docs.

If you are unsure which plan or cluster size is right for your application, [contact us](https://planetscale.com/contact) to get further assistance.

Our self-serve plans are flexible enough to handle the majority of customers. However, there are several use cases where you may need a more custom plan. This is where our Enterprise offerings shine.

## Upsizing and downsizing clusters

As your application scales, upgrading or downgrading your cluster is a seamless operation that does not involve any downtime.

To change cluster sizes, go to your PlanetScale dashboard, click on your database, click the gear icon that specifies your current cluster size, select the new cluster size, and click "Update".

The time it takes to change sizes depends on the size and region of your database. Larger databases may take 20 minutes to upsize/downsize. However, this is all done online, so you will not experience any downtime. Keep in mind, once you update your cluster size, you cannot change sizes again until the first size change completes.

When you choose to change cluster size, we upgrade each of your replicas one by one: delete the tablet container, create a new tablet container of the new size, attach the persistent volume, start it up, and connect it to the primary. Once that's complete, we fail the primary over to one of those new replicas, and do the same thing to the old primary.

## Transaction pools (Vitess)

Each Vitess database has a predefined limit on the number of simultaneous transactions it supports.
This is also known as the *transaction pool*.
These limits are put in place as a protection mechanism for your database.
The limits for each size are shown in the table below:

|                            | **Transaction Pool\*** |
| :------------------------- | :--------------------- |
| **PS-10** and **M-10**     | 70                     |
| **PS-20** and **M-20**     | 75                     |
| **PS-40** and **M-40**     | 75                     |
| **PS-80** and **M-80**     | 110                    |
| **PS-160** and **M-160**   | 158                    |
| **PS-320** and **M-320**   | 211                    |
| **PS-400**                 | 211                    |
| **PS-640** and **M-640**   | 281                    |
| **PS-700**                 | 211                    |
| **PS-900**                 | 281                    |
| **PS-1280** and **M-1280** | 375                    |
| **PS-1400**                | 281                    |
| **PS-1800**                | 375                    |
| **PS-2100**                | 328                    |
| **PS-2560** and **M-2560** | 500                    |
| **PS-2700**                | 438                    |
| **PS-2800**                | 375                    |

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Vantage integration
Source: https://planetscale.com/docs/plans/vantage

With [Vantage](https://www.vantage.sh/), you can set up [PlanetScale cost management](https://vantage.sh/integrations/planetscale) to report on PlanetScale costs alongside other infrastructure providers, such as AWS or GCP.

After integrating, you can create [cost reports](https://docs.vantage.sh/cost_reports) to break down costs per database and branch.

Beyond reporting, set up budget alerts, forecast usage, and view active database costs in Vantage. Vantage connects to your PlanetScale organizations using an OAuth flow.

## Prerequisites

* The [Organization Admin role](/docs/security/access-control) in PlanetScale
* A [Vantage account](https://console.vantage.sh/signup)

<Note>
  Database cost reporting in Vantage is not available for [PlanetScale Managed](/docs/vitess/managed) customers via the integration.
</Note>

## Configure the Vantage integration

<Steps>
  <Step>
    From the Vantage console, navigate to the [Integrations page](https://console.vantage.sh/settings/integrations).
  </Step>

  <Step>
    Select **PlanetScale**, then click **Connect PlanetScale Account**.
  </Step>

  <Step>
    The PlanetScale login screen is displayed. Log in to your PlanetScale account and select the organizations you want to connect with.
  </Step>

  <Step>
    Click **Authorize access**.
  </Step>

  <Step>
    On the [PlanetScale Settings](https://console.vantage.sh/settings/planetscale/) page in Vantage, you should see the status of your connection change to **Importing**.
  </Step>
</Steps>

Costs will be ingested and processed in Vantage once you add the integration. It typically takes less than 15 minutes to ingest PlanetScale costs. The costs will be available on your **All Resources** Cost Report in Vantage as soon as they are processed.

<Note>
  PlanetScale data refreshes daily in Vantage.
</Note>

## View PlanetScale costs in Vantage

In Vantage, you can create cost reports to drill down into your costs. Vantage displays PlanetScale costs by Organization, Service, Category, and Resource.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/vantage/vantage-console.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=46b4fe1abdb755f67231f53402352c7f" alt="Image of a PlanetScale Cost Report in Vantage showing costs per database" data-og-width="2792" width="2792" data-og-height="1878" height="1878" data-path="docs/images/assets/docs/integrations/vantage/vantage-console.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/vantage/vantage-console.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=b001625dbe59b47c07774ad1284fd129 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/vantage/vantage-console.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=5aa483c616a9a0625a89e14637137ddd 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/vantage/vantage-console.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=2faa0509aaf6a18ed782d10e8ca41b01 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/vantage/vantage-console.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=fb76560995d65e83cbfac4e1cf5ec213 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/vantage/vantage-console.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=106ba6087edd5e910c04fbccbef41ff7 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/vantage/vantage-console.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=9e57eb2651cd3fe225c3740607b52171 2500w" />
</Frame>

In the graphic above, PlanetScale costs are grouped by database for the month. For complete cost reporting dimensions and more information, see the [PlanetScale documentation](https://docs.vantage.sh/connecting_planetscale) for Vantage.

## Billing

The Vantage integration is available on all our [plans](/docs/planetscale-plans).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale Postgres
Source: https://planetscale.com/docs/postgres



PlanetScale Postgres is a fully-managed PostgreSQL-compatible database that brings you scale, performance, and reliability — without sacrificing developer experience.

With PlanetScale Postgres, you get the power of branching, query insights, automated backups, replicas, and many more powerful database features without the operational complexity.

<CardGroup>
  <Card title="Postgres quickstart guide" href="/docs/postgres/tutorials/planetscale-postgres-quickstart" icon="rocket-launch">
    Deploy a Postgres database and learn the basics of using PlanetScale.
  </Card>

  <Card title="Architecture" href="/docs/postgres/postgres-architecture" icon="database">
    Learn about PlanetScale Postgres architecture.
  </Card>

  <Card title="Database operations" href="/docs/postgres/operations-philosophy" icon="wrench">
    The operational philosophy behind PlanetScale Postgres databases.
  </Card>

  <Card title="Import a database" href="/docs/postgres/imports/postgres-imports" icon="file-import">
    Import your existing Postgres database to PlanetScale.
  </Card>
</CardGroup>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Postgres vs Vitess
Source: https://planetscale.com/docs/postgres-vs-vitess

Compare PlanetScale's Postgres and Vitess products to choose the best fit for your application

PlanetScale offers two powerful database products: **Postgres** (our PostgreSQL engine) and **Vitess** (our MySQL-compatible engine). Both provide PlanetScale's signature branching workflow and enterprise-grade scaling capabilities.

## Feature comparison

| Feature                          | Postgres                                                             | Vitess                                                                   |
| -------------------------------- | -------------------------------------------------------------------- | ------------------------------------------------------------------------ |
| **Branching**                    | ✅ Schema and data branches                                           | ✅ Schema and data branches                                               |
| **Deploy requests**              | ❌ Not available                                                      | ✅ Online schema changes                                                  |
| **Horizontal sharding**          | ❌ ([Neki](https://planetscale.com/blog/announcing-neki) coming soon) | ✅ Explicit sharding                                                      |
| **Read replicas**                | ✅                                                                    | ✅                                                                        |
| **Read-only regions**            | ❌                                                                    | ✅                                                                        |
| **Serverless driver**            | ❌                                                                    | ✅                                                                        |
| **Connection pooling**           | ✅ Built-in (PgBouncer)                                               | ✅ Built-in                                                               |
| **Query Insights**               | ✅                                                                    | ✅                                                                        |
| **Automatic and custom backups** | ✅                                                                    | ✅                                                                        |
| **PITR**                         | ✅                                                                    | ❌                                                                        |
| **Multi-region**                 | ✅                                                                    | ✅                                                                        |
| **SQL compatibility**            | Fully PostgreSQL compatible                                          | Some [MySQL compatibility](/docs/vitess/mysql-compatibility) limitations |
| **Max cluster size**             | Single cluster                                                       | Unlimited shards                                                         |

## Which product should you choose?

For most teams, **choose based on your existing database experience**:

* **Choose Postgres** if you're currently using PostgreSQL or prefer its feature set
* **Choose Vitess** if you're currently using MySQL or have a large-scale cluster that requires [near-term horizontal sharding](https://planetscale.com/blog/how-to-scale-your-database-and-when-to-shard-mysql)

## Scale considerations

If you're operating at **massive scale** (multi-TB datasets, tens to hundreds of thousands of QPS), Vitess is currently the best option. We've successfully migrated several large PostgreSQL workloads to Vitess for customers who needed unlimited horizontal scaling.

However, if you can manage on a single PostgreSQL cluster for the foreseeable future, **staying on Postgres** might be the right choice. We're developing [Neki](https://planetscale.com/blog/announcing-neki), our sharded PostgreSQL solution, which will bring Vitess-style horizontal sharding to PostgreSQL.

We offer [Metal](https://planetscale.com/metal) clusters with over 100 TB of storage that handle throughput and latency much better than traditional EBS-backed instances. Check out our [benchmarks](https://planetscale.com/blog/benchmarking-postgres) to see how Metal stacks up against other providers.


# Back up and restore
Source: https://planetscale.com/docs/postgres/backups

PlanetScale Postgres databases include comprehensive backup and restore capabilities to protect your data. The backup system provides both automated scheduled backups and manual on-demand backups, as well as [point-in-time recovery (PITR)](/docs/postgres/backups/point-in-time-recovery) support through Write-Ahead Log (WAL) archiving.

## Automatic scheduled backups

### Default backup schedule

All PlanetScale Postgres databases include automatic scheduled backups that run every 12 hours for production and development branches. Default backups are created automatically and don't require any setup. There are no additional charges for these backups. WAL logs are retained for the same period as your oldest backup.

Both backups and WAL files are stored in highly durable, available, and secure storage off of your cluster, in the same cloud provider and region as the cluster.

## Additional backups

<Note>
  Custom backup schedules and manually created on-demand backups can incur additional charges. See [Backup Pricing](#backup-pricing) for more information.
</Note>

### Custom backup schedules

You can create additional custom backup schedules for more frequent backups or specific timing requirements. You must be an organization administrator or database administrator to create or modify a custom backup schedule.

<Steps>
  <Step>
    From the PlanetScale organization dashboard, select the desired database
  </Step>

  <Step>
    Navigate to the **Backups** page from the menu on the left
  </Step>

  <Step>
    Click **Add new schedule**
  </Step>

  <Step>
    Configure your schedule:

    * **Run every**: Configure the frequency of the backup schedule based on hours, days, weeks, months
    * **Time**: Specify when the backup should start. Depending on frequency pick either the offset of the hour, the time of the day, the day of the week and time of that day, or which day of the week of the month, and the hour of that day
    * **Retention**: How long to keep backups (hours, days, weeks, months, or years)
    * **Name**: The name for your custom backup schedule. If left blank, an auto-generated name will be created.
  </Step>

  <Step>
    Click **Save schedule**
  </Step>
</Steps>

### Manual backups

You can create a one-off on-demand backup of your database branch:

<Steps>
  <Step>
    From the PlanetScale organization dashboard, select the desired database
  </Step>

  <Step>
    Navigate to the **Backups** page from the menu on the left
  </Step>

  <Step>
    Click **Create new backup**
  </Step>

  <Step>
    Select a branch
  </Step>

  <Step>
    Enter a name
  </Step>

  <Step>
    Select if this backup is an [Emergency backup](#emergency-backups)
  </Step>

  <Step>
    Specify how long you want to keep the backup (hours, days, weeks, months, or years)
  </Step>

  <Step>
    Click **Create backup**
  </Step>
</Steps>

### Emergency backups

Emergency backups are available when you need an immediate backup regardless of database state or ongoing operations. Unlike regular backups, emergency backups do not rely on previous backups or previous WALs.

They also may use additional primary database resources during creation. They are only recommended when regular backups are failing due to WAL corruption or other issues.

<Note>
  Emergency backups may impact database performance and should only be used in critical situations where immediate backup is required.
</Note>

## Viewing backups

### Backup List

All backups can be viewed on your Backups page.

<Steps>
  <Step>
    From the PlanetScale organization dashboard, select the desired database
  </Step>

  <Step>
    Navigate to the **Backups** page from the menu on the left
  </Step>

  <Step>
    View all backups with details:

    * Backup name
    * Branch name
    * Size
    * Backed up (a timestamp will be displayed if backup has completed)
  </Step>
</Steps>

You can click into any backup to see the branch, expiration date, restore to a new branch, and to toggle preventing backup deletion.

## Restoring from Backups

### Creating a Restored Branch

Restore any backup to a new database branch:

<Steps>
  <Step>
    From the PlanetScale organization dashboard, select the desired database
  </Step>

  <Step>
    Navigate to the **Backups** page from the menu on the left
  </Step>

  <Step>
    Select the backup you wish to restore from
  </Step>

  <Step>
    Click **Restore to new branch**
  </Step>

  <Step>
    Configure the restored branch:

    * **Branch name**: Name for the new branch
    * **Cluster size**: Choose the desired size of the new cluster
  </Step>

  <Step>
    Click **Restore backup**
  </Step>
</Steps>

A new branch will be created based on this backup and will become visible under the **Branches** page.

### Point-in-time recovery

PlanetScale Postgres supports precise [point-in-time recovery (PITR)](/docs/postgres/backups/point-in-time-recovery) using Write-Ahead Log (WAL) files. This allows you to restore to any specific moment within your backup retention window, not just to backup snapshots.

**Key benefits:**

* Restore to any timestamp between your oldest backup and 5 minutes prior to the current time
* More precise than traditional backup-only restoration
* Essential for recovering from data corruption or accidental changes

**Restore time considerations:** The time required depends on your cluster size, data volume, and how far back you're restoring. Restoring to recent points (within default 12-hour backup intervals) is typically quick, while restoring to older points may require replaying multiple days of WAL files.

For detailed examples and step-by-step instructions, see the [Point-in-time recovery documentation](/docs/postgres/backups/point-in-time-recovery).

### Restored Branch Behavior

* Restored branches are independent database branches
* They include all data as of the restore point
* Database extensions are **not** restored - you'll need to reinstall any extensions
* Restored branches can be promoted to production or used for testing
* Backup expiration is automatically extended to ensure the restored branch can create new backups

## Backup retention

The default, included backup schedule retains backups for 2 days. You can set a custom retention period for custom backup schedules and manual one-off backups.

Additionally, you can choose to protect any individual backup from deletion. Click on the backup you'd like to protect, and click the "**Prevent backup deletion**" toggle. Keep in mind that custom backups and included backups that are retained beyond the default retention policy will incur additional charges.

## Backup pricing

PlanetScale Postgres includes backup storage with every database branch. Each branch receives backup storage equal to **twice the allocated disk size**. For example, a database with 50 GB of storage includes 100 GB of backup storage.

To learn more about how backup storage is measured and pricing for additional storage, see [pricing](https://planetscale.com/docs/postgres/pricing).

### Monitor your usage

Track your backup storage usage and costs on the **Backups** page in your database dashboard. The page shows a breakdown of storage usage and any overage charges per branch, helping you optimize retention policies if needed.

If you need additional assistance, [contact](https://planetscale.com/contact?initial=support) the PlanetScale support team.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Point-in-time recovery
Source: https://planetscale.com/docs/postgres/backups/point-in-time-recovery

Point-in-time recovery (PITR) allows you to restore your PlanetScale Postgres database to any specific moment within your backup retention window. Unlike traditional backups that restore to specific backup snapshots, PITR gives you precise control over when to restore your data.

## How PITR Works

PlanetScale Postgres uses [PostgreSQL's Write-Ahead Logging (WAL)](https://www.postgresql.org/docs/current/wal-intro.html) system combined with periodic backups to enable PITR.

PostgreSQL's WAL system records all changes to the database before they're applied to the actual data files. Each transaction is first written to the WAL log, ensuring durability even if the system crashes. These WAL files are continuously archived, creating a complete record of every change made to your database. During PITR, PostgreSQL starts with a base backup and then "replays" the relevant WAL files to reconstruct the exact database state at your specified point in time.

In short, there are 3 key things to know about:

* **Backups**: Snapshots of your database taken at regular intervals
* **WAL Archiving**: Continuous archiving of all database changes as they occur
* **Recovery Process**: Restores from a base backup and replays WAL files to reach the exact point in time requested

## Performing a point-in-time recovery

PITR is available for any point in time within your retention window — by default from 2 days ago up to 5 minutes before the current time (the 5-minute buffer ensures WAL files are fully processed and archived).

If you have created manual on-demand backups, have a custom backup schedule, or have enabled the **Prevent backup deletion** toggle on any backup, then PITR is available from the oldest backup available up to 5 minutes before the current time.

<Note>
  Additional backups beyond the default incur additional charges. See [Backups](/docs/postgres/pricing#backups) for more information.
</Note>

### Create the PITR backup branch

<Steps>
  <Step>
    From the PlanetScale organization dashboard, select the desired database
  </Step>

  <Step>
    Navigate to the **Backups** page from the menu on the left
  </Step>

  <Step>
    On the right side of the page, you'll see a box labeled "**Point-in-time recovery**"
  </Step>

  <Step>
    Click the Branch dropdown to select the branch you'd like to restore from
  </Step>

  <Step>
    Select your target date and time within the available window
  </Step>

  <Step>
    Click **Restore backup**

    <Steps>
      <Step>
        You'll see a summary of the selected branch, time you are restoring to, and size
      </Step>

      <Step>
        Name your new branch
      </Step>

      <Step>
        Select a cluster size
      </Step>

      <Step>
        Click "Restore backup"
      </Step>
    </Steps>
  </Step>
</Steps>

This will create a new branch with the schema and data from the selected point in time. See [Branching](/docs/postgres/branching) for next steps on how to connect and potentially promote your new branch.

## Restore time considerations

Restore time depends on three key factors:

* **Cluster size**: Data can only be restored as fast as your cluster can handle. Consider up-scaling the disk configuration (for network-attached storage) or upgrading your cluster size (for PlanetScale Metal)
* **Data volume**: The amount of data being restored affects initial backup restoration time
* **WAL replay duration**: The time between your chosen restore point and the nearest available backup affects WAL replay time

PostgreSQL must start with the closest available backup taken *before* your target restore point and replay all WAL files from that backup until the target time.

### Example 1: Restore to a point 8 hours ago

Your database uses the default backup schedule: backups every 12 hours, retained for 2 days. You want to restore to a point 8 hours ago.

```text  theme={null}
Available Backups: [-------- 2 days retention --------]
                   |-------|-------|-------|-------|
                   48hrs   36hrs   24hrs   12hrs   NOW
                   (exp)   ✓       ✓       ✓
Restore Process:   |===============|~~~~~~~|
                   Base Backup     WAL Replay
                   (12hrs ago)     (4 hours)
                                   ↑
                                Target: 8hrs ago
Result: Quick restore (typically minutes to under an hour)
```

### Example 2: Restore to a point 16 days ago (with a custom backup schedule)

In addition to the default backup schedule, your database has a custom schedule: weekly backups every Sunday, retained for 31 days. You want to restore to a point 16 days ago (mid-week).

```text  theme={null}
Available Backups: [-------------- 31 days retention --------------]
                   |----------|----------|----------|----------|
                   28d (Sun)  21d (Sun)  14d (Sun)  7d (Sun)   NOW
                   ✓          ✓          ✓          ✓
Restore Process:   |===============|~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~|
                   Base Backup     WAL Replay
                   (21 days ago)   (5 days)
                                   ↑
                                Target: 16 days ago
Result: Longer restore time (hours, depending on database activity volume)
```

The total restore time increases with the gap between your target restore point and the nearest prior backup. Databases with high transaction volume will have more WAL files to replay, extending the process.

## When to Use PITR

Point-in-time recovery is ideal for:

* **Data corruption recovery**: Restore to just before corruption occurred
* **Accidental deletions**: Recover data that was mistakenly removed
* **Failed migrations**: Roll back to before a problematic schema change
* **Compliance requirements**: Access historical data states for auditing

## Troubleshooting

### "Recovery point not available"

* The requested time may be outside your retention window
* Check that the timestamp is at least 5 minutes ago

### "WAL files missing"

* WAL archiving may have been interrupted
* Network connectivity issues during archival
* Storage system failures
* Consider using the nearest available backup point instead

If you need additional assistance, [contact](https://planetscale.com/contact?initial=support) the PlanetScale support team.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Branching
Source: https://planetscale.com/docs/postgres/branching

Branches on PlanetScale Postgres are isolated database deployments that provide you with separate environments for development and testing, as well as restoring from backups.

When your PostgreSQL database is first initialized, a single production branch is created called `main` which acts as the default branch. You can then create development branches that you can use for development before shipping schema changes to production.

Branches are completely isolated databases. Changes made in one branch, whether to the schema or the data, do not affect any other branches for a given database. There is no data replication between branches, so writing data into one will not replicate to another.

<Note>
  New branches incur additional charges. The cost depends on the selected resources for the branch. The monthly price for the new branch will be shown during branch creation. See [pricing](/docs/postgres/pricing) for more information.
</Note>

## Development and production branches

PlanetScale Postgres provides two types of database branches:

* **Development branches** — Development branches run on `PS-DEV` instances that have limited performance capabilities, different egress rates, and no replicas. This is great for experimentation, testing new schemas, and limited exploration of your data.

* **Production branches** — Production branches are intended for production traffic and include optional replicas for high availability. Production branches can be created on non-HA ([single node](/docs/postgres/cluster-configuration/single-node)) or HA (primary + 2 replicas) nodes.

## Create a branch

<Note>
  We are still in the process of building out our full branching functionality Postgres. You can currently create a new empty branch with no schema and no data or create a branch from a backup, which includes schema and data.
</Note>

There are two ways to create a new Postgres branch: from the Branches page (no schema or data included) or by restoring from a backup (schema and data included).

Each branch is its own isolated database and uses its own storage separate from production. You will be charged for the storage consumed by all production and development branches.

We do not recommend using production data for development environments.

### From the Branches page

This method does not include schema or data.

Branches created via this method will always initialize as a single node `PS-DEV` database, which does not have any replicas and begins at \$5/month, depending on the region. After initialization, you have the option to upsize the branch or add replicas from the "Clusters" page in the dashboard.

<Steps>
  <Step>Click **Branches**</Step>
  <Step>Click **New branch**</Step>
  <Step>Give the branch a name</Step>

  <Step>
    Choose the base branch. This currently does not copy the schema.
  </Step>

  <Step>Select a region</Step>
  <Step>Click **Create branch**</Step>
</Steps>

### From a backup

This method includes both the schema and the data for the selected backup.

The cluster size, and therefore cost, is inherited from your main parent branch.

<Steps>
  <Step>
    From the PlanetScale organization dashboard, select the desired database
  </Step>

  <Step>
    Navigate to the **Backups** page from the menu on the left
  </Step>

  <Step>
    Select the backup you wish to restore from
  </Step>

  <Step>
    Click **Restore to new branch**
  </Step>

  <Step>
    Configure the restored branch:

    * **Branch name**: Name for the new branch
    * **Cluster size**: Choose the desired size of the new cluster
  </Step>

  <Step>
    Click **Restore backup**
  </Step>
</Steps>

A new branch will be created based on this backup and will become visible under the **Branches** page.

## View all branches

To view all branches for your PostgreSQL database:

<Steps>
  <Step>From the PlanetScale organization dashboard, select the desired database</Step>
  <Step>Click on the **Branches** drop down</Step>
  <Step>You'll see a list of all branches with their type (development/production)</Step>
  <Step>You can select a branch by its name to see its **Dashboard**</Step>
</Steps>

Similarly you can navigate directly to the Branches page to see all branches:

<Steps>
  <Step>From the PlanetScale organization dashboard, select the desired database</Step>
  <Step>Navigate to the **Branches** page from the menu on the left</Step>
</Steps>

## Connect to a branch

Each branch has its own connection details. To connect to a specific branch:

<Steps>
  <Step>Navigate to the desired branch following either of the paths shown above</Step>
  <Step>Click the **Connect** button</Step>
  <Step>If you haven't already, follow the instructions to **Create the default role**</Step>
  <Step>Choose your preferred connection method ([direct or PgBouncer](/docs/postgres/connecting))</Step>
  <Step>Copy the connection string or credentials</Step>
  <Step>You should create new roles for specific use-cases and tailor their permissions appropriately for them</Step>
  <Step>In your application, use the specific role(s) and the preferred connection method.</Step>
</Steps>

For detailed connection instructions, see the [PostgreSQL connection documentation](/docs/postgres/connecting).

## Rename a branch

To rename a branch:

<Steps>
  <Step>From the PlanetScale organization dashboard, select the desired database</Step>
  <Step>Navigate to the **Branches** page from the menu on the left</Step>
  <Step>Click on the three dots ("...") next to the branch you want to rename</Step>
  <Step>Select **Rename branch**</Step>
  <Step>Enter the new branch name</Step>
  <Step>Click **Save changes**</Step>
</Steps>

Renaming a branch does not affect that branch's credentials. You do not need to regenerate credentials if you rename a branch.

## Set as default branch

The default branch serves as the source branch when creating new development branches. To change the default branch:

### From the Branches page

<Steps>
  <Step>From the PlanetScale organization dashboard, select the desired database</Step>
  <Step>Go to the **Branches** tab in your database dashboard</Step>
  <Step>Click on the three dots ("...") next to the branch you want to set as default</Step>
  <Step>Click **Set as default branch**</Step>
</Steps>

### From the Settings page

<Steps>
  <Step>From the PlanetScale organization dashboard, select the desired database</Step>
  <Step>Navigate to the **Settings** page from the menu on the left</Step>
  <Step>Select the branch from the **Default branch** dropdown you want to be the default branch</Step>
  <Step>Scroll down and click **Save database settings**</Step>
</Steps>

If you change the default branch and intend for it to power your production application, you may need to update your application credentials to reference the new default branch.

## Delete a branch

You can delete a branch that you no longer need:

<Steps>
  <Step>From the PlanetScale organization dashboard, select the desired database</Step>
  <Step>Go to the **Branches** tab in your database dashboard</Step>
  <Step>Click on the three dots ("...") next to the branch you want to delete</Step>
  <Step>Select **Delete**</Step>
  <Step>Confirm the deletion</Step>
</Steps>

**Important notes:**

* You cannot delete the default branch. You must first set another branch as the default branch.
* You cannot set development branches as the default branch.
* Only [Organization Administrators and Database Administrators](/docs/security/access-control) have permission to delete production branches. Database Members can delete non-production branches.
* Development branches are billed only for the time they are used, so it's beneficial to delete them when no longer needed.

## Schema changes

Since PlanetScale Postgres branches don't use [deploy requests like in Vitess](/docs/vitess/schema-changes/deploy-requests), you make schema changes directly on each branch:

<Steps>
  <Step>Connect to your development branch</Step>
  <Step>Make your schema changes using standard PostgreSQL DDL commands</Step>
  <Step>Test your changes in the development environment</Step>
  <Step>When ready, manually apply the same changes to your production branch</Step>
</Steps>

There's currently no automated way to merge schema changes between PlanetScale Postgres branches. You must manually copy your changes from development to production branches.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Cluster configuration
Source: https://planetscale.com/docs/postgres/cluster-configuration



The Clusters page in your PlanetScale dashboard allows you to monitor your cluster utilization and configure cluster settings for each branch in your database. You can:

* Adjust the cluster size and instance type ([Metal](/docs/metal) vs network-attached storage)
* Configure the number of replicas
* Monitor cluster utilization with real-time graphs
* Configure storage settings including:
  * Disk size configuration
  * Autoscaling settings and thresholds
  * Storage limits
  * IOPS configuration
  * Bandwidth settings
* Modify PostgreSQL [parameters](/docs/postgres/cluster-configuration/parameters)
* Manage PostgreSQL [extensions](/docs/postgres/extensions)
* View and track configuration changes

These settings may only be changed by a [database administrator or organization administrator](/docs/security/access-control).

## Adjusting cluster size

To adjust your cluster size:

<Steps>
  <Step>From the PlanetScale organization dashboard, select the desired database</Step>
  <Step>Navigate to the **Clusters** page from the menu on the left</Step>
  <Step>In the Cluster size section, click on the cluster size dropdown</Step>

  <Step>
    Select the new cluster size, (Metal or network-attached storage). If you select Metal, make sure you select the
    correct storage size.
  </Step>

  <Step>Click Queue instance changes > Apply changes to apply the new cluster size</Step>
</Steps>

<Note>
  You cannot change the [CPU architecture](/docs/postgres/cluster-configuration/cpu-architectures) (AMD, Intel, ARM) of an existing cluster. To use a different architecture, you'll need to create a new branch with the desired cluster type.
</Note>

Cluster resizing may take several minutes to complete and you cannot make additional configuration changes until the resize is finished.

Consider the case where you need to upgrade from an `M-160` database cluster to an `M-320`, doubling the compute resources of each node.
After applying the upgrade, three new `M-320` nodes are created.
These are caught up with the primary through a combination of a backup restore and data replication.

<img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-resize-1.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=ced1ff95e9ce762110b0f652d8886005" alt="Cluster Resize 1" data-og-width="2162" width="2162" data-og-height="1187" height="1187" data-path="docs/postgres/cluster-resize-1.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-resize-1.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=81a7bb92955f32a0264c85d2dd743a38 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-resize-1.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=7f0484d209c5923bac082e55cc59b088 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-resize-1.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=3183eac038676fae4d991b7b9aa64b39 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-resize-1.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=8d538f93b6d47b1db0511809836e4f70 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-resize-1.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=1e69ccd44aefa11bc057d2a5734f0da5 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-resize-1.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=bbba3fc94e8cf31e0eb9357734636e78 2500w" />

Once these new `M-320` replicas are sufficiently caught up, the operator transitions primaryship to the one of the new `M-320` nodes.

After this, the old `M-160` replicas are decommissioned, using the new ones for all replica traffic.
During each node replacement, the connections to the decommissioned node will be terminated.
Your clients will need to establish new connections with the new nodes.

<img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-resize-2.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=71a04aee7f55f0d5ff393df7cdd2e598" alt="Cluster Resize 2" data-og-width="2181" width="2181" data-og-height="932" height="932" data-path="docs/postgres/cluster-resize-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-resize-2.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=a43236270b1ded7a957c5e762b582496 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-resize-2.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=67306337aa11cea1b461d92d972c84b7 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-resize-2.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=5a9b2d4fa925f42cad377f95edeea916 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-resize-2.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=3ebbb5dca2cfe6d41da78715f624b313 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-resize-2.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=063c25fb9baf5210b9a1796957eed852 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-resize-2.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=b2c73a85655df524bbcf62a131334032 2500w" />

During the primary cutover all database connections will be terminated.
Normally a primary promotion proceeds in a fast and orderly manner in less than 5 seconds.
In cases where the operator is not able to quickly and cleanly shutdown the primary due to unresponsive user queries or transactions, the the operator will failover to a replica after a timeout of 30 seconds.

For all the steps leading up to the node replacement, your existing M-160 database cluster remains fully functional.
Due to the way this works, it's important for your application to have connection retry logic.

## Managing replicas

[Replicas](/docs/postgres/scaling/replicas) provide read scalability and high availability for your PostgreSQL database. Each production branch (excluding single node) comes with 2 replicas by default.

<Note>
  If you add additional replicas beyond the default 2, you will incur additional charges. Billing for additional replicas begins once the replica change has completed. See [pricing](https://planetscale.com/docs/postgres/pricing) to understand the additional costs.
</Note>

To adjust the number of replicas:

<Steps>
  <Step>From the PlanetScale organization dashboard, select the desired database</Step>
  <Step>Navigate to the **Clusters** page from the menu on the left</Step>
  <Step>In the **Cluster size** section, use the dropdown **Replicas** to select your desired number of replicas</Step>
  <Step>Click **Queue instance changes**</Step>
  <Step>Click **Apply changes** to immediately begin the process to update the number of replicas</Step>
</Steps>

## Monitoring cluster utilization

The cluster utilization graph provides real-time insights into your database's CPU and memory utilization. You can change the graph view to the past hour, 6 hours, 12 hours, day, or week. The graph also displays indicators when cluster settings were changed.

<Note>
  If you are consistently close to 100% CPU, we recommend upsizing your cluster. Likewise, if you are consistently below 50% CPU, you can likely safely downgrade.
</Note>

## Parameters and settings

You can customize your PostgreSQL cluster's behavior in the **Parameters** tab on the Clusters page. This allows you to configure the following settings:

* PgBouncer
* Resource usage
* Write-ahead logs (WAL)
* Query tuning
* Connections and authentication
* Failover

For more information about each setting, refer to the [Parameters documentation](/docs/postgres/cluster-configuration/parameters).

## Configuring storage

You can configure your cluster's storage settings in the "**Storage**" tab on the Clusters page. Storage configuration includes:

* Setting the minimum disk size for your cluster
* Enabling autoscaling to automatically increase storage when approaching limits
* Configuring storage limits to control maximum storage allocation
* Monitoring current storage usage

For detailed information about storage configuration options, refer to the [Storage configuration documentation](/docs/postgres/cluster-configuration/cluster-storage).

## Managing extensions

PostgreSQL [extensions](/docs/postgres/extensions) provide additional functionality for your database. The "**Extensions**" tab on the Clusters page allows you to view and enable/disable available extensions.

### Extension management

* View currently installed extensions
* See available extensions for installation
* Enable or disable extensions
* Configure extension parameters

## Monitoring configuration changes

The Clusters page provides visibility into all changes made to your cluster settings.

* Navigate to the Changes tab for the selected branch on the Clusters page
* View a chronological list of all configuration changes
* See details including:
  * What was changed (cluster size, replicas, parameters)
  * Previous and new values
  * Status of the change
  * Who initiated the change
  * When the change was started and completed

<Note>
  When updating a cluster's size, some [parameters](/docs/postgres/cluster-configuration/parameters) will automatically be adjusted. Each cluster size is associated with default parameter settings, changing the cluster size will also update those defaults. The exception to this is if you manually override a default parameter setting. In that case, a cluster size adjustment will not automatically change that setting.
</Note>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Cluster storage configuration
Source: https://planetscale.com/docs/postgres/cluster-configuration/cluster-storage

You can configure storage settings for network-attached storage PlanetScale Postgres clusters in the "**Storage**" tab on the Clusters page for your database.

<Note>
  For PlanetScale Postgres clusters launched on PlanetScale Metal instances, storage is scaled by directly scaling the cluster instance size. Storage autoscaling is not available for Metal clusters. To learn more see the documentation for [PlanetScale Metal](/docs/metal)
</Note>

## Configuring storage settings

You must be a database or organization administrator to modify these settings. Adjusting these settings may incur additional charges. To learn more about pricing for storage, see [pricing](/docs/postgres/pricing).

<Steps>
  <Step>From the PlanetScale organization dashboard, select the desired database</Step>
  <Step>Navigate to the **Clusters** page from the menu on the left</Step>
  <Step>Choose the branch whose storage settings you'd like to configure in the "**Branch**" dropdown</Step>
  <Step>Select the **Storage** tab</Step>
  <Step>Configure your disk size and autoscaling settings</Step>
  <Step>Set your storage limit as needed</Step>
  <Step>Click "**Queue storage changes**"</Step>
  <Step>Once you're ready to apply the changes, click "**Apply changes**"</Step>
</Steps>

## Minimum disk size configuration

Configure the minimum disk size for your database cluster. This setting determines the initial storage capacity allocated to your database. The disk size is specified in GB and serves as the baseline storage allocation for your cluster.

<Note>
  The maximum disk size for network-attached storage is 16384 GB (16 TiB).
</Note>

## Enable autoscaling

<Note>
  PlanetScale storage autoscaling is only for network-attached storage database clusters. For [PlanetScale Metal](/docs/metal) based clusters you will need to increase the cluster instance size.
</Note>

Disk autoscaling is enabled by default upon database creation. Disk autoscaling automatically increases storage when your database approaches a disk size utilization threshold, preventing storage-related outages without manual intervention. You can also enable automatic disk shriking.

Both of these options can be configured by going to "Clusters" > "Storage" > and clicking the "Enable autoscaling" checkbox.

For more information, see the [Disk autoscaling documentation](/docs/postgres/cluster-configuration/disk-autoscaling).

## Storage limit

The storage limit sets the maximum amount of storage that can be allocated to your database cluster through autoscaling. This acts as a ceiling to prevent unlimited storage growth and helps control costs.

When autoscaling is enabled, your storage can grow from the minimum disk size up to the storage limit you specify. The storage limit should be set higher than your initial disk size to allow for growth while providing a reasonable upper bound for your storage costs.

<Note>
  The maximum disk size for network-attached storage is 16384 GB (16 TiB).
</Note>

## IOPS

Configure the maximum input/output operations per second for your database. This will be limited by your database cluster size and disk size.

### Storage volume type and IOPS

| Storage type | Default IOPS | Maximum IOPS                         |
| ------------ | ------------ | ------------------------------------ |
| AWS gp3      | 3000         | 16,000 (at 32GB or larger disk size) |

## Bandwidth

The maximum amount of data that can be read or written to your database in a single second. This will be limited by your database cluster size and configured IOPS.

### Storage volume type and bandwidth

| Storage type | Default bandwidth | Maximum bandwidth                     |
| ------------ | ----------------- | ------------------------------------- |
| AWS gp3      | 125 MiB/s         | 1,000 MiB/s (at 4,000 IOPS or higher) |

### Storage throughput limits

For databases created on AWS-based clusters the **maximum configurable throughput** your cluster can support is based on CPU architecture and cluster size.

| CPU Architecture  | Cluster Size                                                                 | Maximum Throughput (in MiB/s) |
| ----------------- | ---------------------------------------------------------------------------- | ----------------------------- |
| AWS x86-64        | PS-DEV, PS-10, PS-20, PS-40, PS-80, PS-160, PS-320, PS-640, PS-1280, PS-2560 | 1000                          |
| AWS aarch64/ARM64 | PS-DEV, PS-10, PS-20, PS-40, PS-80, PS-160, PS-320, PS-640, PS-1280          | 593                           |
| AWS aarch64/ARM64 | PS-2560                                                                      | 1000                          |

## Monthly storage cost

Displays the estimated monthly cost for your current storage configuration. If you adjust your storage configuration the number shown represents the new monthly estimate for the configured values. Billing for storage changes begins once the storage change has completed.

## Tracking changes to storage settings

You can click on the "Changes" tab on the Clusters page to view a log of any changes made to your storage settings. The log will include the settings affected, the original and updated values, status, user that made the changes, start time, and end time.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# CPU Architectures
Source: https://planetscale.com/docs/postgres/cluster-configuration/cpu-architectures

When deploying PostgreSQL databases, choosing the right CPU architecture is crucial for optimizing performance, cost, and efficiency. PlanetScale Postgres supports both x86-64 and ARM64 (aarch64) architectures, with ARM64 instances powered by AWS Graviton processors.

## Architecture Overview

### x86-64 (Intel/AMD)

The x86-64 architecture has been the dominant server architecture for decades. These processors offer:

* Mature ecosystem with extensive software optimization
* Wide compatibility with existing applications and tools
* High single-threaded performance
* Established performance benchmarks and tuning practices

### ARM64 (AWS Graviton)

ARM64 represents the next generation of server processors, with [AWS Graviton chips](https://aws.amazon.com/ec2/graviton/) specifically designed for cloud workloads:

* Custom silicon optimized for cloud applications
* Superior price-performance ratio
* Lower power consumption
* Built on modern 64-bit ARM architecture

## Performance Comparison

### CPU Performance

* **Single-threaded**: x86-64 processors typically offer higher single-threaded performance
* **Multi-threaded**: ARM64 Graviton processors excel in multi-threaded workloads common in database operations
* **Memory bandwidth**: Graviton processors provide higher memory bandwidth, beneficial for data-intensive PostgreSQL operations

### PostgreSQL-Specific Performance

* **OLTP workloads**: ARM64 shows 10-20% better performance per dollar for typical transaction processing
* **Analytics workloads**: Both architectures perform similarly for complex analytical queries
* **Concurrent connections**: ARM64 handles high-concurrency scenarios more efficiently
* **Background processes**: ARM64's multi-core design benefits PostgreSQL's background maintenance tasks

## Cost Considerations

### Infrastructure Costs

* **ARM64 instances**: Typically 20-40% lower cost than equivalent x86-64 instances
* **Performance per dollar**: ARM64 generally provides better price-performance ratios
* **Energy efficiency**: ARM64 processors consume less power, reducing operational costs

### Total Cost of Ownership

* **Development overhead**: x86-64 may have lower initial setup costs due to existing tooling
* **Long-term savings**: ARM64 offers significant cost savings for sustained workloads
* **Scaling costs**: ARM64 becomes more cost-effective as your database scales

## Compatibility and Ecosystem

### Software Compatibility

* **PostgreSQL**: Full native support on both architectures ([PostgreSQL supported platforms](https://www.postgresql.org/docs/current/supported-platforms.html))
* **Extensions**: Most popular PostgreSQL extensions support ARM64 ([PostgreSQL ARM64 package repository](https://www.postgresql.org/about/news/arm64-on-aptpostgresqlorg-2033/))
* **Tools**: Database administration tools work seamlessly on both platforms
* **Drivers**: All major PostgreSQL drivers support ARM64

### Migration Considerations

* **Existing applications**: Applications using standard PostgreSQL drivers require no changes
* **Binary extensions**: Custom compiled extensions may need recompilation for ARM64
* **Performance tooling**: Some x86-specific optimization tools may not be available on ARM64

## Decision Matrix

| Consideration                   | x86-64            | ARM64 (Graviton)                           |
| :------------------------------ | :---------------- | :----------------------------------------- |
| **Price-performance ratio**     | Standard          | Superior (20-40% cost savings)             |
| **Single-threaded performance** | Higher            | Good                                       |
| **Multi-threaded performance**  | Good              | Superior                                   |
| **PostgreSQL extensions**       | Universal support | Most supported, some require recompilation |
| **Energy efficiency**           | Standard          | Superior                                   |

**Choose x86-64 when:**

* Legacy applications or custom extensions require x86-64
* Maximum single-threaded performance is critical
* Strict compliance requirements mandate x86-64
* Existing monitoring infrastructure is x86-64 specific

**Choose ARM64 (Graviton) when:**

* Cost optimization is a priority
* Running concurrent, multi-threaded workloads
* Starting new projects without legacy constraints
* Environmental impact reduction is important

## Getting Started

Both architectures are available when creating new PostgreSQL databases in PlanetScale Postgres. You can specify your preferred architecture during the database creation process. For most new deployments, ARM64 provides the best combination of performance, cost-effectiveness, and future-proofing.

Consider running performance tests with your specific workload on both architectures to make the most informed decision for your use case.

<Note>
  Once you've selected your desired CPU architecture, you can not modify a launched database cluster to be mixed CPU architecture nor modify your cluster to change CPU architecture. For help in swapping your CPU architecture, please [reach out to support](https://planetscale.com/contact?initial=support).
</Note>

## CPU Architecture availability

Depending on your target region for your deployment, there may or may not be certain cluster size configurations that are available from the underlying provider. PlanetScale aims to have the most complete availability of resources for you to use and as is such may enable or disable certain configurations over time based on availability.

<Note>
  The most accurate source of this information is on the [Create a new database](https://app.planetscale.com/new) page and then selecting the desired region.

  For customers of managed deployments, please [reach out to support](https://planetscale.com/contact?initial=support) for assistance in confirming availability for your deployment.
</Note>

## Additional Resources

<Columns cols={2}>
  <Card title="AWS Graviton Performance Studies" icon="angles-right" horizontal href="https://aws.amazon.com/ec2/graviton" />

  <Card title="PostgreSQL Performance Tuning" icon="angles-right" horizontal href="https://wiki.postgresql.org/wiki/Performance_Optimization" />

  <Card title="PostgreSQL ARM64 Support Documentation" icon="angles-right" horizontal href="https://www.postgresql.org/docs/current/supported-platforms.html" />

  <Card title="AWS RDS PostgreSQL ARM64 Migration Guide" icon="angles-right" horizontal href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide" />
</Columns>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Disk autoscaling
Source: https://planetscale.com/docs/postgres/cluster-configuration/disk-autoscaling

Disk autoscaling automatically increases storage when your database approaches a disk size utilization threshold, preventing storage-related outages without manual intervention. You can also enable automatic disk shrinking to shrink your disk allocation.

<Note>
  PlanetScale storage autoscaling is only for network-attached storage database clusters. For [PlanetScale
  Metal](/docs/metal) clusters, you need to increase the cluster instance size to increase storage space.
</Note>

Cloud providers like AWS and GCP limit how frequently network-attached disks can be resized.
In both cases, there is a multi-hour cooldown period between resizing operations. These volumes also typically do not support shrinking. PlanetScale disk autoscaling handles the automatic increasing and decreasing of disk size beyond these AWS and GCP limitations.

**Disk autoscaling is enabled by default upon database creation.**

We provide three scaling modes designed to optimize cost and performance while maintaining data availability:

* **In-place growth mode** — This is the default scaling mode that expands storage capacity by resizing existing volumes directly, without requiring failovers or connection disruption. This method leverages AWS EBS's native resize capability.
* **Surge growth mode** — Surge growth creates new volumes with larger capacity and orchestrates failover to the new storage, circumventing [AWS EBS resize limitations](https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_ModifyVolume.html).
* **Shrink mode** — In shrink mode, the autoscaler reduces storage capacity for underutilized volumes. This can help to optimize costs after a surge event.

Autoscaling will grow the cluster's storage without requiring you to make any configuration changes. The new additional space will become available as soon as the scaling action has completed.

<Note>
  There are pricing implications when you enable disk autoscaling. You will be billed for the *allocated* disk size, not
  the actual total storage. Make sure you enable shrink mode to automatically adjust down to optimize costs.
</Note>

## Autoscaling thresholds and behaviors

When enabled, disk surge and shrink autoscaling will kick in when your disk utilization reaches the following thresholds:

### Surge mode thresholds

Automatic disk growth activates when disk utilization reaches these thresholds:

* **70%** for disks smaller than 4 TiB
* **25%** for disks larger than or equal to 4 TiB

### Shrink mode thresholds

Automatic shrinking activates when disk utilization falls below these thresholds:

* **12.5%** for disks smaller than 1 TiB
* **15%** for disks between 1 TiB and 2 TiB
* **25%** for disks larger than 2 TiB

For example, if you have a 200 GiB disk allocation and are only using 20 GiB, we mark the disk for shrinking because it's below the existing 12.5% threshold. We wait until the 3 day cooldown period has passed, and then we shrink it.

### Key disk autoscaling behavior:

* Cluster storage can only scale once in a multi-hour period
* Cluster storage scales proportionally based on current size
  * Smaller disks receive larger percentage increases, while larger disks receive smaller percentage increases
* All disks grow by a minimum of 50% when autoscaling occurs
* No shrink is triggered if a disk was recently autoscaled. We wait for up to 3 days until we start observing shrink conditions.
* If you need to scale cluster storage by more than 200% within 24 hours, manually scale disk size ahead of time
* Autoscaling will not scale past your configured [**Storage limits**](#storage-limits)

## Surge growth mode

Our surge growth creates new volumes with larger capacity and orchestrates failover to the new storage, circumventing AWS EBS resize limitations.

When our disk auto-scaler is able to spread out disk scale-up sufficiently, no downtime is required to scale the disks. When data growth is rapid, the auto-scaler may need to complete a **surge resize** to support the writes.
In this case, PlanetScale creates brand new, larger network-attached storage volumes to replace the old ones. Surge growth causes a brief failover event that severs existing database connections. Applications must handle connection recovery.

If the surge autoscaler is able to complete the resize before your disk fills, downtime will be minimal for growing the disk (seconds).
If your disk fills before the new disks are ready, you will experience a longer period of downtime.

We make every effort to keep your network-attached storage disk from filling, but it's important for the database administrators to pay close attention to storage and take manual intervention when necessary.

## Shrink mode

In shrink mode, the disk autoscaler reduces storage capacity for underutilized volumes through surge operations, as AWS EBS does not support in-place volume shrinking. This mode optimizes costs by rightsizing storage while also preventing oscillation between grow and shrink operations.

When the shrink condition is detected, we have a 3 day cooldown period before we start the actual shrinking action for automatic shrinking.

Shrink operations cause a brief failover event that severs existing database connections. Applications must handle connection recovery.

## Enable or disable disk autoscaling

Disk autoscaling is enabled by default upon database creation. Both of these options can be configured by going to "Clusters" > "Storage" > and clicking the "Enable autoscaling" checkbox. You also have the option to **only** automatically scale up.

## Storage limits

The storage limit sets the maximum amount of storage that can be allocated to your database cluster through autoscaling. This acts as a ceiling to prevent unlimited storage growth and helps control costs.

When autoscaling is enabled, your storage can grow from the minimum disk size up to the storage limit you specify. The storage limit should be set higher than your initial disk size to allow for growth while providing a reasonable upper bound for your storage costs.

<Note>
  The maximum disk size for network-attached storage is 16384 GB (16 TiB).
</Note>


# Maintenance windows
Source: https://planetscale.com/docs/postgres/cluster-configuration/maintenance-windows

Maintenance windows have been temporarily disabled for PlanetScale Postgres databases.

PlanetScale Postgres databases previously supported weekly maintenance windows, during which Postgres image upgrades would occur. These required connections to be terminated and re-established on upgraded nodes.

To ensure better connection stability for customers, we've moved to optional updates and have temporarily disabled maintenance windows for Postgres databases.

You can now [manually update your cluster](/docs/postgres/cluster-configuration/updates) to access new extensions and other software updates.


# Cluster configuration parameters
Source: https://planetscale.com/docs/postgres/cluster-configuration/parameters

You can configure your PlanetScale Postgres cluster settings in the "**Parameters**" tab on the Clusters page for your database.

You can configure your PlanetScale Postgres cluster settings in the "**Parameters**" tab on the Clusters page for your database.

The defaults for each parameter depend on the configuration of your cluster. The defaults have been chosen to optimize performance, resource usage, and connection handling for each cluster size. However, you are able to fine-tune each of these settings as needed.

## Configuring parameter values

You must be a database or organization administrator to modify these settings.

1. From the PlanetScale organization dashboard, select the desired database
2. Navigate to the **Clusters** page from the menu on the left
3. Choose the branch whose parameters you'd like to configure in the "**Branch**" dropdown
4. Select the **Parameters** tab
5. Search for a specific parameter or scroll through the page to see all configurable parameters
6. Update the value for the parameter(s) you wish to adjust
7. Click "**Queue parameter changes**"
8. Once you're ready to apply the changes, click "**Apply changes**"

## Tracking changes to parameters

You can click on the "Changes" tab on the Clusters page to view a log of any changes made to your parameter settings. The log will include the settings affected, the original and updated values, status, user that made the changes, start time, and end time.

<Note>
  When updating a cluster's size, some [parameters](/docs/postgres/cluster-configuration/parameters) will automatically
  be adjusted. Each cluster size is associated with default parameter settings, changing the cluster size will also
  update those defaults. The exception to this is if you manually override a default parameter setting. In that case, a
  cluster size adjustment will not automatically change that setting.
</Note>

## Parameter change types

PostgreSQL parameter changes fall into two categories based on how they are applied:

* **Reloadable changes**: The parameter can be updated without restarting PostgreSQL, resulting in zero downtime
* **Restart-required changes**: PostgreSQL requires a cluster restart for the parameter to take effect

Parameters that require a restart are marked with a ✅ in the "Restart Required" column in the [parameter reference table](#default-parameter-values) below.

### Restart behavior for production clusters

When you apply parameters that require a restart, PlanetScale performs a rolling restart process to minimize downtime:

1. Configuration changes are first applied to replica instances and they are restarted
2. Once replicas are ready, a switchover promotes one replica to become the new primary
3. The configuration is applied to the former primary (now a replica) and it is restarted

<img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-configuration/config-change-restart.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=45d150b0d1b17b527ce56bf4b675f903" alt="Config change with restart" data-og-width="3501" width="3501" data-og-height="1169" height="1169" data-path="docs/postgres/cluster-configuration/config-change-restart.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-configuration/config-change-restart.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=830ae7067c370466ca4271deba042608 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-configuration/config-change-restart.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=6af727f44c8af722e3f504bd7efe11f7 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-configuration/config-change-restart.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=dfb37f96d52a91a37d7bf40a73d2ab3c 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-configuration/config-change-restart.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=db47ef6bbefcd554bdb45e27998c859d 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-configuration/config-change-restart.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=579b4012953bc76a77c5cc5e240ab534 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-configuration/config-change-restart.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=2bd8a73b10e3ddfe6919524f166e9a04 2500w" />

This rolling restart process minimizes downtime, but there remains a brief several-second window of unavailability during the primary switchover. All direct database connections will be terminated during this process, so your application should implement connection retry logic. With the exception of the `Number of processes` parameter for PgBouncer, PgBouncer connections persist through all parameter changes and do not require reconnection.

Some parameters are [required by PostgreSQL](https://www.postgresql.org/docs/current/hot-standby.html#HOT-STANDBY-ADMIN) to be applied to the primary before replicas, which may result in a slightly longer unavailability period.

## Default parameter values

The following table shows the default values for parameters that are displayed by default to customers. You can find additional parameters in the search field.

| Parameter                             | Restart Required            | Description                                                                                                                 |
| ------------------------------------- | --------------------------- | --------------------------------------------------------------------------------------------------------------------------- |
| **PgBouncer**                         |                             |                                                                                                                             |
| Number of processes                   | ✅ (restarts PGBouncer only) | Sets the number of PgBouncer processes that will run on each node in this branch's cluster                                  |
| default\_pool\_size                   |                             | Sets the number of server connections to allow per user/database pair                                                       |
| max\_client\_conn                     |                             | Sets the maximum number of client connections allowed                                                                       |
| max\_db\_client\_connections          |                             | Sets the maximum number of client connections allowed per database (regardless of user). 0 is unlimited                     |
| max\_db\_connections                  |                             | Sets the maximum number of server connections allowed per database (regardless of user). 0 is unlimited                     |
| max\_prepared\_statements             |                             | Sets the maximum number of client-prepared statements available across server connections                                   |
| max\_user\_connections                |                             | Sets the maximum number of server connections allowed per user (regardless of database). 0 is unlimited                     |
| server\_lifetime                      |                             | Sets how long an unused server connection stays open                                                                        |
| server\_idle\_timeout                 |                             | Sets how long an idle server connection stays open                                                                          |
| **Resource usage**                    |                             |                                                                                                                             |
| effective\_io\_concurrency            |                             | Sets the number of simultaneous requests that can be handled efficiently by the disk subsystem                              |
| effective\_cache\_size                |                             | Sets the planner's assumption about the total size of the data caches                                                       |
| huge\_pages                           | ✅                           | Controls whether huge pages are requested for the main shared memory area                                                   |
| maintenance\_io\_concurrency          |                             | Sets the number of simultaneous requests that can be handled efficiently by the disk subsystem for maintenance operations   |
| maintenance\_work\_mem                |                             | Sets the maximum memory to be used for maintenance operations                                                               |
| max\_parallel\_maintenance\_workers   |                             | Sets the maximum number of parallel processes per maintenance operation                                                     |
| max\_parallel\_workers                |                             | Sets the maximum number of parallel workers that can be active at one time                                                  |
| max\_parallel\_workers\_per\_gather   |                             | Sets the maximum number of parallel processes per executor node                                                             |
| max\_worker\_processes                | ✅                           | Sets the maximum number of background processes that the cluster can support                                                |
| shared\_buffers                       | ✅                           | Sets the amount of memory the database server uses for shared memory buffers                                                |
| work\_mem                             |                             | Sets the amount of memory the database will use for internal operations like sorting and hashing                            |
| **Write-ahead log**                   |                             |                                                                                                                             |
| max\_slot\_wal\_keep\_size            |                             | Sets the maximum WAL size that can be reserved by replication slots                                                         |
| max\_wal\_size                        |                             | Sets the WAL size that triggers a checkpoint                                                                                |
| min\_wal\_size                        |                             | Sets the minimum size to shrink the WAL to                                                                                  |
| wal\_buffers                          | ✅                           | Sets the number of disk-page buffers in shared memory for WAL                                                               |
| wal\_level                            | ✅                           | Sets the level of information written to the WAL                                                                            |
| **Query tuning**                      |                             |                                                                                                                             |
| deadlock\_timeout                     |                             | Sets the maximum time to wait on a lock before checking for deadlocks                                                       |
| default\_statistics\_target           |                             | Sets the default statistics target for table columns without a column-specific target set                                   |
| random\_page\_cost                    |                             | Sets the planner's estimate of the cost of a nonsequentially fetched disk page                                              |
| seq\_page\_cost                       |                             | Sets the planner's estimate of the cost of a sequentially fetched disk page                                                 |
| **Connections and authentication**    |                             |                                                                                                                             |
| max\_connections                      | ✅                           | Sets the maximum number of concurrent connections                                                                           |
| **Replication**                       |                             |                                                                                                                             |
| hot\_standby\_feedback                |                             | Sends feedback to the primary about queries being executed on the standby. Required for logical replication failover        |
| max\_logical\_replication\_workers    | ✅                           | Sets the maximum number of logical replication workers                                                                      |
| max\_replication\_slots               | ✅                           | Sets the maximum number of replication slots that the server can support                                                    |
| max\_sync\_workers\_per\_subscription |                             | Sets the maximum number of synchronization workers per subscription                                                         |
| max\_wal\_senders                     | ✅                           | Sets the maximum number of WAL senders                                                                                      |
| sync\_replication\_slots              |                             | Enables standbys to synchronize logical replication streams from the primary. Required for logical replication failover     |
| **Failover**                          |                             |                                                                                                                             |
| failover\_delay                       |                             | Sets the time to wait before triggering a failover to drain inflight transactions                                           |
| **Statistics**                        |                             |                                                                                                                             |
| track\_io\_timing                     |                             | Enables timing of database I/O calls. This may cause significant overhead                                                   |
| **Logging**                           |                             |                                                                                                                             |
| log\_lock\_waits                      |                             | Logs the duration of lock waits that exceed the deadlock\_timeout                                                           |
| log\_min\_duration\_statement         |                             | Sets the minimum execution time above which all statements will be logged                                                   |
| **Client connection defaults**        |                             |                                                                                                                             |
| shared\_preload\_libraries            | ✅                           | Specifies shared libraries to preload into the server at server start                                                       |
| **Autovacuum**                        |                             |                                                                                                                             |
| autovacuum\_vacuum\_scale\_factor     |                             | Specifies a fraction of the table size to add to autovacuum\_vacuum\_threshold when deciding whether to trigger a VACUUM    |
| autovacuum\_analyze\_scale\_factor    |                             | Specifies a fraction of the table size to add to autovacuum\_analyze\_threshold when deciding whether to trigger an ANALYZE |

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Single node
Source: https://planetscale.com/docs/postgres/cluster-configuration/single-node

Single node Postgres databases come with a single primary and no replicas, and are a great cost-effective option for development or production workloads that do not require high availability.

<Note>
  Single node is only available for network-attached storage databases. You cannot create a single node
  [Metal](/docs/metal) database.
</Note>

Single node databases are available on all network-attached storage cluster sizes and begin at \$5/month. For full single node pricing, see the [pricing](https://planetscale.com/pricing) page.

To create a single node database, just select "**Single node**" and your cluster size during the database creation process.

## Resizing your single node database

You can upsize or downsize your single node Postgres database by going to "**Clusters**", selecting the cluster size from the dropdown, and clicking "**Queue instance changes**", "**Apply changes**". During the resize, we will surge a new replica on the selected size, sync your data, promote the replica to primary, and decommission the old node.

## Switch from single node to HA

To upgrade your single node database to an HA cluster, go to the Clusters page, select the "HA Primary + Multi-replica" option, choose your cluster size from the dropdown (network-attached storage or [Metal](/docs/metal)), click "Queue instance changes", and click "Apply changes".

During this process, 2 additional replicas across 3 Availability Zones are added to your database cluster. If you selected a Metal cluster, your data is copied over to the new instances with locally-attached storage. It follows the process described in the [Upgrading to a Metal database documentation](/docs/metal/create-a-metal-database#upgrading-an-existing-database-to-metal).

## Switch from HA to single node

To switch an HA database with 1 primary and 2+ replicas to single node, go to your Clusters page, select "Single node", choose a cluster size, click "Queue instance changes", and click "Apply changes".

This will remove any existing replicas you have and leave you with a single non-HA primary.


# Cluster updates
Source: https://planetscale.com/docs/postgres/cluster-configuration/updates

PlanetScale occasionally releases software updates for your Postgres cluster.

## How to update your cluster

To check if your cluster has any available updates, go to the **Clusters** page in your PlanetScale dashboard and look for the "Cluster update available" indicator.

<Note>
  If you make any other cluster changes that require a cluster restart, such as enabling an extension, we will roll out the latest updates during that cluster restart.
</Note>

To update your cluster:

<Steps>
  <Step>Click the **"Queue update"** button on your Clusters page. This will queue the updates without immediately starting the update process.</Step>
  <Step>You should see "This cluster has queued changes". You can batch additional cluster changes before applying them if needed.</Step>
  <Step>Once you're ready, click **"Apply changes"**.</Step>
  <Step>In the confirmation modal, review the warning that applying changes will close any open connections. Make sure your application is equipped to handle closed connections, then click **"Apply changes"** to confirm.</Step>
  <Step>You can view the status of your changes under the **Changes** tab.</Step>
</Steps>


# Supported versions
Source: https://planetscale.com/docs/postgres/cluster-configuration/versions

When creating a PlanetScale Postgres cluster, you can choose from any of the Postgres versions we support.

PlanetScale Postgres supports versions 17 and 18 of Postgres.

Specifically, the following versions are currently supported:

* 17.5
* 18.1

New databases will be created using the latest version by default and we recommend sticking to that default, but you can choose an older version if you need to.

## Major version upgrades

PlanetScale doesn't currently offer in-place upgrades between major versions of Postgres. To upgrade from Postgres 17 to 18, create a new PostgreSQL 18 database and perform an online migration from your existing PlanetScale Postgres 17 database using our [import guides](/docs/postgres/imports/postgres-imports).


# Connections overview
Source: https://planetscale.com/docs/postgres/connecting

There are several ways to connect to Postgres databases, each with their advantages and tradeoffs.

## Connecting to your PlanetScale Postgres database

Connecting to your PlanetScale Postgres database involves understanding several key components. This page provides an overview of connection options — for detailed instructions, see the linked documentation below.

### Roles and credentials

PlanetScale provides two types of roles for database access:

* **Default postgres role** — A near-superuser role with extensive permissions, ideal for administrative tasks and initial database setup. This role should not be used for application connections.
* **User-defined roles** — Custom roles with specific permission sets that follow the principle of least privilege. These are recommended for all application connections and allow credential rotation without downtime.

Connection credentials include a hostname, username (formatted as `{role}.{branch_id}`), password (prefixed with `pscale_pw_`), and database name. Learn more about [managing roles and creating credentials](/docs/postgres/connecting/roles).

### Connection strings

PlanetScale databases require SSL/TLS encryption for all connections. Connection strings include parameters for the host, port, username, password, database name, and SSL configuration. The port determines the connection method:

* **Port 5432** — Direct connections to Postgres, bypassing PgBouncer
* **Port 6432** — Connections through PgBouncer for connection pooling

The [connections quickstart](/docs/postgres/connecting/quickstart) provides detailed connection string examples and explains when to use each connection method.

### Private connectivity

For enhanced security and reduced latency, PlanetScale supports private connectivity that keeps traffic within cloud provider networks:

* **AWS PrivateLink** — Establishes private connections from your AWS VPC to PlanetScale databases without exposing traffic to the public internet. See the [AWS PrivateLink documentation](/docs/postgres/connecting/private-connections/aws-privatelink).
* **GCP Private Service Connect** — Provides private connectivity from your Google Cloud VPC to PlanetScale databases. See the [GCP Private Service Connect documentation](/docs/postgres/connecting/private-connections/gcp-private-service-connect).

### Neon Serverless Driver

For serverless and edge environments, PlanetScale supports connections via the [Neon serverless driver](/docs/postgres/connecting/neon-serverless-driver). This driver is optimized for platforms like Vercel Functions, AWS Lambda, and edge runtimes like Cloudflare Workers. Both HTTP and WebSocket modes are supported, HTTP mode for simple one-shot queries, and WebSocket mode for transactions and session-based features.

## Understanding Postgres connections

Postgres uses a connection-per-process architecture. Each connection made to a Postgres server [spawns a new process](https://planetscale.com/blog/processes-and-threads), which consumes system resources including memory and CPU. For this reason, it's important to manage the number of direct connections to keep the system performant.

Connection pooling is the primary solution to this challenge. In the Postgres ecosystem, [PgBouncer](https://www.pgbouncer.org/) is the most widely-used connection pooler. PgBouncer instances sit between clients and the Postgres server, maintaining a small pool of connections to Postgres while accepting a much larger number of client connections. PgBouncer routes client requests through these pooled connections efficiently.

## Connection options

PlanetScale provides several ways to connect to your Postgres database:

1. **Direct primary connections** - Connect directly to your Postgres primary server on port `5432`. This provides the lowest latency and full Postgres session capabilities. Use this for administrative tasks, long-running operations, and data imports.

2. **Direct replica connections** - Connect directly to read-only replicas on port `5432` by appending `|replica` to your username. Use this for read-only queries that can tolerate replication lag.

3. **Local PgBouncer (primary only)** - All Postgres databases include a local PgBouncer running on the same host as the primary. Connect via port `6432`. This is recommended for all application connections to the primary.

4. **Dedicated replica PgBouncer** - Create dedicated PgBouncer instances that pool connections to your replicas. These run on separate nodes and are useful for read-heavy workloads. Connect via port `6432` with the PgBouncer name appended to your username.

5. **Dedicated primary PgBouncer** - Create dedicated PgBouncer instances that pool connections to your primary database. These run on separate nodes and provide improved high availability, with connections persisting through cluster resizes, upgrades, and most failover scenarios. Connect via port `6432` with the PgBouncer name appended to your username.

The following sections describe each option in detail to help you choose the right connection method for your use case.

## Direct primary connections

Direct connections provide the lowest-latency access to your Postgres primary instance.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/qlTxKzpSogl0IGN-/docs/images/assets/docs/postgres/connecting/diagram-direct-connect.png?fit=max&auto=format&n=qlTxKzpSogl0IGN-&q=85&s=2a840a2f0a8dbbfe08873fd00218c8d4" alt="Direct connections" style={{ maxHeight: '250px', width: 'auto' }} data-og-width="1436" width="1436" data-og-height="720" height="720" data-path="docs/images/assets/docs/postgres/connecting/diagram-direct-connect.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/qlTxKzpSogl0IGN-/docs/images/assets/docs/postgres/connecting/diagram-direct-connect.png?w=280&fit=max&auto=format&n=qlTxKzpSogl0IGN-&q=85&s=6dd2b702b047c27dacbbfd2d99bce6f1 280w, https://mintcdn.com/planetscale-cad1a68a/qlTxKzpSogl0IGN-/docs/images/assets/docs/postgres/connecting/diagram-direct-connect.png?w=560&fit=max&auto=format&n=qlTxKzpSogl0IGN-&q=85&s=80a242d1e349f0e90ee37a2d0821e11b 560w, https://mintcdn.com/planetscale-cad1a68a/qlTxKzpSogl0IGN-/docs/images/assets/docs/postgres/connecting/diagram-direct-connect.png?w=840&fit=max&auto=format&n=qlTxKzpSogl0IGN-&q=85&s=4adf3f093c0bb4f6bc512891b054ff25 840w, https://mintcdn.com/planetscale-cad1a68a/qlTxKzpSogl0IGN-/docs/images/assets/docs/postgres/connecting/diagram-direct-connect.png?w=1100&fit=max&auto=format&n=qlTxKzpSogl0IGN-&q=85&s=b79c78392a2a69b5d40b90a4dfb1daa0 1100w, https://mintcdn.com/planetscale-cad1a68a/qlTxKzpSogl0IGN-/docs/images/assets/docs/postgres/connecting/diagram-direct-connect.png?w=1650&fit=max&auto=format&n=qlTxKzpSogl0IGN-&q=85&s=b75c3302150748c6ab8207e2b6b2b967 1650w, https://mintcdn.com/planetscale-cad1a68a/qlTxKzpSogl0IGN-/docs/images/assets/docs/postgres/connecting/diagram-direct-connect.png?w=2500&fit=max&auto=format&n=qlTxKzpSogl0IGN-&q=85&s=8177d33c936d06052fd88a0b2dd73d2c 2500w" />
</Frame>

However, these connections are considered *heavy-weight* since each one consumes significant resources. Direct connections are recommended only for specific scenarios:

1. Administrative tasks, like creating new databases/schemas, manual DDL commands, and installing extensions.
2. Long-running operations like `VACUUM`s and large analytical queries that are executed infrequently.
3. Importing data during a migration or other bulk-loading operations.
4. When you need features like `SET`, pub/sub, and other features not provided by PgBouncer pooled connections.

Because having too many direct connections degrades performance, PlanetScale sets `max_connections` to a conservative default value that varies depending on cluster size. To find this value, navigate to the "Clusters" page and select the "Parameters" tab.

<img src="https://mintcdn.com/planetscale-cad1a68a/-ubEebx1La8vHdTj/docs/images/assets/docs/postgres/connecting/cluster-configuration-parameters-darkmode.png?fit=max&auto=format&n=-ubEebx1La8vHdTj&q=85&s=801d2653e9765b82900038d0edb026c1" alt="Navigate to the Cluster Parameters page" data-og-width="3006" width="3006" data-og-height="1296" height="1296" data-path="docs/images/assets/docs/postgres/connecting/cluster-configuration-parameters-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/-ubEebx1La8vHdTj/docs/images/assets/docs/postgres/connecting/cluster-configuration-parameters-darkmode.png?w=280&fit=max&auto=format&n=-ubEebx1La8vHdTj&q=85&s=587b35c7f9093dc942071a74a7c4318b 280w, https://mintcdn.com/planetscale-cad1a68a/-ubEebx1La8vHdTj/docs/images/assets/docs/postgres/connecting/cluster-configuration-parameters-darkmode.png?w=560&fit=max&auto=format&n=-ubEebx1La8vHdTj&q=85&s=2ff3a7914d60fdafc5316ff1fe1c7979 560w, https://mintcdn.com/planetscale-cad1a68a/-ubEebx1La8vHdTj/docs/images/assets/docs/postgres/connecting/cluster-configuration-parameters-darkmode.png?w=840&fit=max&auto=format&n=-ubEebx1La8vHdTj&q=85&s=89566c8bdc5fdb0d2415d7a704f0ee3b 840w, https://mintcdn.com/planetscale-cad1a68a/-ubEebx1La8vHdTj/docs/images/assets/docs/postgres/connecting/cluster-configuration-parameters-darkmode.png?w=1100&fit=max&auto=format&n=-ubEebx1La8vHdTj&q=85&s=8421ca38f515b4cffeba734a98ca3294 1100w, https://mintcdn.com/planetscale-cad1a68a/-ubEebx1La8vHdTj/docs/images/assets/docs/postgres/connecting/cluster-configuration-parameters-darkmode.png?w=1650&fit=max&auto=format&n=-ubEebx1La8vHdTj&q=85&s=40015cd621076a730ae41ef365f2045a 1650w, https://mintcdn.com/planetscale-cad1a68a/-ubEebx1La8vHdTj/docs/images/assets/docs/postgres/connecting/cluster-configuration-parameters-darkmode.png?w=2500&fit=max&auto=format&n=-ubEebx1La8vHdTj&q=85&s=89370dd542ae0ec4767916edc3ec3cda 2500w" />

Search for `max_connections` to view the current configured value. This can be increased if necessary, though doing so requires careful consideration as increasing direct connections can negatively impact performance.

When the `max_connections` limit is reached, error messages like the following will appear:

```
FATAL: sorry, too many clients already
```

Or variations such as:

```
FATAL: remaining connection slots are reserved for non-replication superuser connections
```

For application connections outside of the specific use cases listed above, PgBouncer should be used instead.

## Direct replica connections

The main purpose for the default [Replicas](/docs/postgres/scaling/replicas) in a cluster is to maintain [high-availability](/docs/postgres/operations-philosophy), but they can also be used to handle read traffic. Since replicas are read-only, they are only capable of serving `SELECT` queries. All write traffic (`INSERT`, `UPDATE`, etc) must be sent to the primary.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/-ubEebx1La8vHdTj/docs/images/assets/docs/postgres/connecting/diagram-replica-direct-connect.png?fit=max&auto=format&n=-ubEebx1La8vHdTj&q=85&s=2b0958ff6f84878f6eb093924a4c939e" alt="Direct replica connections" style={{ maxHeight: '250px', width: 'auto' }} data-og-width="1936" width="1936" data-og-height="792" height="792" data-path="docs/images/assets/docs/postgres/connecting/diagram-replica-direct-connect.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/-ubEebx1La8vHdTj/docs/images/assets/docs/postgres/connecting/diagram-replica-direct-connect.png?w=280&fit=max&auto=format&n=-ubEebx1La8vHdTj&q=85&s=e7ee06700c9b07b64fa79d4d4757a72f 280w, https://mintcdn.com/planetscale-cad1a68a/-ubEebx1La8vHdTj/docs/images/assets/docs/postgres/connecting/diagram-replica-direct-connect.png?w=560&fit=max&auto=format&n=-ubEebx1La8vHdTj&q=85&s=1f743b850238e0792c719a82deed2954 560w, https://mintcdn.com/planetscale-cad1a68a/-ubEebx1La8vHdTj/docs/images/assets/docs/postgres/connecting/diagram-replica-direct-connect.png?w=840&fit=max&auto=format&n=-ubEebx1La8vHdTj&q=85&s=22ca449fae8cd0b53fda8008882df0c5 840w, https://mintcdn.com/planetscale-cad1a68a/-ubEebx1La8vHdTj/docs/images/assets/docs/postgres/connecting/diagram-replica-direct-connect.png?w=1100&fit=max&auto=format&n=-ubEebx1La8vHdTj&q=85&s=f916e538b8d42743a581078f58eb77f7 1100w, https://mintcdn.com/planetscale-cad1a68a/-ubEebx1La8vHdTj/docs/images/assets/docs/postgres/connecting/diagram-replica-direct-connect.png?w=1650&fit=max&auto=format&n=-ubEebx1La8vHdTj&q=85&s=12f78a1e567e1269acd46722b915d38a 1650w, https://mintcdn.com/planetscale-cad1a68a/-ubEebx1La8vHdTj/docs/images/assets/docs/postgres/connecting/diagram-replica-direct-connect.png?w=2500&fit=max&auto=format&n=-ubEebx1La8vHdTj&q=85&s=1bcea6276dc2373953a3833958cd9602 2500w" />
</Frame>

Replicas always experience some level of replication lag — the delay between data arriving at the primary and being replicated to a replica. Frequently, replication lag is measured in milliseconds, but it can grow to multiple seconds, especially when the server is experiencing high write traffic or network issues.

Because of these factors, queries should only be sent to replicas if they meet the following criteria: (A) they are read-only and (B) they can tolerate being slightly out-of-sync with the data on the primary. For reads that cannot tolerate this lag, send them to the primary.

To connect to a replica, append `|replica` to your credential username and use port `5432`. For example:

```bash  theme={null}
psql 'host=xxxxxxxxxx-useast1-1.horizon.psdb.cloud \
      port=5432 \
      user=postgres.xxxxxxxxxx|replica \
      password=pscale_pw_xxxxxxxxxxxxxxxxxx \
      dbname=my_database \
      sslnegotiation=direct \
      sslmode=verify-full \
      sslrootcert=system'
```

Learn more about replicas and when to use them in the [database replicas documentation](/docs/postgres/scaling/replicas).

## PgBouncer connections

PgBouncer provides connection pooling for your Postgres database, allowing applications to scale beyond the constraints of direct connections. Connections from application servers should be made via PgBouncer whenever possible. PlanetScale provides three types of PgBouncer instances:

### Local PgBouncer

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/qlTxKzpSogl0IGN-/docs/images/assets/docs/postgres/connecting/diagram-local-pgbouncer.png?fit=max&auto=format&n=qlTxKzpSogl0IGN-&q=85&s=9b9065fc891bd115df330a3e81ce3a89" alt="Local PgBouncer connections" style={{ maxHeight: '250px', width: 'auto' }} data-og-width="1992" width="1992" data-og-height="880" height="880" data-path="docs/images/assets/docs/postgres/connecting/diagram-local-pgbouncer.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/qlTxKzpSogl0IGN-/docs/images/assets/docs/postgres/connecting/diagram-local-pgbouncer.png?w=280&fit=max&auto=format&n=qlTxKzpSogl0IGN-&q=85&s=31db0ac4dc51e5b8c51f70b9bffebf6c 280w, https://mintcdn.com/planetscale-cad1a68a/qlTxKzpSogl0IGN-/docs/images/assets/docs/postgres/connecting/diagram-local-pgbouncer.png?w=560&fit=max&auto=format&n=qlTxKzpSogl0IGN-&q=85&s=2fb9e10eb2dec1afdaf9bd629586c07e 560w, https://mintcdn.com/planetscale-cad1a68a/qlTxKzpSogl0IGN-/docs/images/assets/docs/postgres/connecting/diagram-local-pgbouncer.png?w=840&fit=max&auto=format&n=qlTxKzpSogl0IGN-&q=85&s=50db56be8f8a93ac95c391c67888cae1 840w, https://mintcdn.com/planetscale-cad1a68a/qlTxKzpSogl0IGN-/docs/images/assets/docs/postgres/connecting/diagram-local-pgbouncer.png?w=1100&fit=max&auto=format&n=qlTxKzpSogl0IGN-&q=85&s=c8bcbf9763a6452e6bf1369cb6fe958d 1100w, https://mintcdn.com/planetscale-cad1a68a/qlTxKzpSogl0IGN-/docs/images/assets/docs/postgres/connecting/diagram-local-pgbouncer.png?w=1650&fit=max&auto=format&n=qlTxKzpSogl0IGN-&q=85&s=c3e7e4a499d051ec34013abc046342fd 1650w, https://mintcdn.com/planetscale-cad1a68a/qlTxKzpSogl0IGN-/docs/images/assets/docs/postgres/connecting/diagram-local-pgbouncer.png?w=2500&fit=max&auto=format&n=qlTxKzpSogl0IGN-&q=85&s=ff83356fd4a4d750702eeb0bea02c9b3 2500w" />
</Frame>

All PlanetScale Postgres databases include a local PgBouncer instance running on the same host node as the Postgres primary. This is recommended for all application connections to the primary. To connect via the local PgBouncer, use the same credentials as a direct connection but change the port from `5432` to `6432`.

<Note>
  The local PgBouncer only routes connections to the primary. To pool connections to replicas, use a [dedicated replica PgBouncer](#dedicated-replica-pgbouncers).
</Note>

### Dedicated replica PgBouncer

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/qlTxKzpSogl0IGN-/docs/images/assets/docs/postgres/connecting/diagram-dedicated-replica-pgbouncer.png?fit=max&auto=format&n=qlTxKzpSogl0IGN-&q=85&s=88b54024b5a322863603a313a4c638be" alt="Dedicated replica PgBouncer connections" style={{ maxHeight: '250px', width: 'auto' }} data-og-width="2656" width="2656" data-og-height="792" height="792" data-path="docs/images/assets/docs/postgres/connecting/diagram-dedicated-replica-pgbouncer.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/qlTxKzpSogl0IGN-/docs/images/assets/docs/postgres/connecting/diagram-dedicated-replica-pgbouncer.png?w=280&fit=max&auto=format&n=qlTxKzpSogl0IGN-&q=85&s=78ccf92159aff36d2a4a2dcf41a2394e 280w, https://mintcdn.com/planetscale-cad1a68a/qlTxKzpSogl0IGN-/docs/images/assets/docs/postgres/connecting/diagram-dedicated-replica-pgbouncer.png?w=560&fit=max&auto=format&n=qlTxKzpSogl0IGN-&q=85&s=b33f3bc676fc132157718499c9227714 560w, https://mintcdn.com/planetscale-cad1a68a/qlTxKzpSogl0IGN-/docs/images/assets/docs/postgres/connecting/diagram-dedicated-replica-pgbouncer.png?w=840&fit=max&auto=format&n=qlTxKzpSogl0IGN-&q=85&s=ec61ba42b30aa02d1f34996ab72a8392 840w, https://mintcdn.com/planetscale-cad1a68a/qlTxKzpSogl0IGN-/docs/images/assets/docs/postgres/connecting/diagram-dedicated-replica-pgbouncer.png?w=1100&fit=max&auto=format&n=qlTxKzpSogl0IGN-&q=85&s=43f6f6e7b04af1ce2731c4a9c786ecd7 1100w, https://mintcdn.com/planetscale-cad1a68a/qlTxKzpSogl0IGN-/docs/images/assets/docs/postgres/connecting/diagram-dedicated-replica-pgbouncer.png?w=1650&fit=max&auto=format&n=qlTxKzpSogl0IGN-&q=85&s=b1f879cb3f994eb9319f80e7e1985e76 1650w, https://mintcdn.com/planetscale-cad1a68a/qlTxKzpSogl0IGN-/docs/images/assets/docs/postgres/connecting/diagram-dedicated-replica-pgbouncer.png?w=2500&fit=max&auto=format&n=qlTxKzpSogl0IGN-&q=85&s=ed626514b403d3fa46506295c9812018 2500w" />
</Frame>

[Dedicated replica PgBouncers](/docs/postgres/connecting/pgbouncer#dedicated-replica-pgbouncers) run on nodes separate from the Postgres instances and pool connections to your replicas. These are useful for read-heavy workloads that send significant read traffic to replicas.

### Dedicated primary PgBouncers

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/qlTxKzpSogl0IGN-/docs/images/assets/docs/postgres/connecting/diagram-dedicated-primary-pgbouncer.png?fit=max&auto=format&n=qlTxKzpSogl0IGN-&q=85&s=93af653242cfb44619b8596fc0836f20" alt="Dedicated primary PgBouncer connections" style={{ maxHeight: '250px', width: 'auto' }} data-og-width="2196" width="2196" data-og-height="880" height="880" data-path="docs/images/assets/docs/postgres/connecting/diagram-dedicated-primary-pgbouncer.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/qlTxKzpSogl0IGN-/docs/images/assets/docs/postgres/connecting/diagram-dedicated-primary-pgbouncer.png?w=280&fit=max&auto=format&n=qlTxKzpSogl0IGN-&q=85&s=5168a497d5221a954e0bee88309c4af3 280w, https://mintcdn.com/planetscale-cad1a68a/qlTxKzpSogl0IGN-/docs/images/assets/docs/postgres/connecting/diagram-dedicated-primary-pgbouncer.png?w=560&fit=max&auto=format&n=qlTxKzpSogl0IGN-&q=85&s=624fcb98078b2ec2cb0473938fc5c58d 560w, https://mintcdn.com/planetscale-cad1a68a/qlTxKzpSogl0IGN-/docs/images/assets/docs/postgres/connecting/diagram-dedicated-primary-pgbouncer.png?w=840&fit=max&auto=format&n=qlTxKzpSogl0IGN-&q=85&s=d7fe587d470147685011ca0ba0e4eec0 840w, https://mintcdn.com/planetscale-cad1a68a/qlTxKzpSogl0IGN-/docs/images/assets/docs/postgres/connecting/diagram-dedicated-primary-pgbouncer.png?w=1100&fit=max&auto=format&n=qlTxKzpSogl0IGN-&q=85&s=302054516a9b896ef9a6889884980a7f 1100w, https://mintcdn.com/planetscale-cad1a68a/qlTxKzpSogl0IGN-/docs/images/assets/docs/postgres/connecting/diagram-dedicated-primary-pgbouncer.png?w=1650&fit=max&auto=format&n=qlTxKzpSogl0IGN-&q=85&s=0dad20bce3db2f3c4fa648a6a66321a1 1650w, https://mintcdn.com/planetscale-cad1a68a/qlTxKzpSogl0IGN-/docs/images/assets/docs/postgres/connecting/diagram-dedicated-primary-pgbouncer.png?w=2500&fit=max&auto=format&n=qlTxKzpSogl0IGN-&q=85&s=ca399d7e3e61e9c055c2acbda263499e 2500w" />
</Frame>

[Dedicated primary PgBouncers](/docs/postgres/connecting/pgbouncer#dedicated-primary-pgbouncers) provide connection pooling for your primary database on nodes separate from the Postgres servers. Connections through dedicated PgBouncers persist through cluster resizes, upgrades, and most failover scenarios, providing improved high availability.

## Connecting to dedicated PgBouncers

Connect to replica or primary PgBouncers via port `6432` and append the name of the PgBouncer to your username. For example, if your PgBouncer is named `read-bouncer`, the connection username should be `postgres.xxxxxxxxxx|read-bouncer`.

```bash  theme={null}
psql 'host=xxxxxxxxxx-useast1-1.horizon.psdb.cloud \
      port=6432 \
      user=postgres.xxxxxxxxxx|read-bouncer \
      password=pscale_pw_xxxxxxxxxxxxxxxxxx \
      dbname=my_database \
      sslnegotiation=direct \
      sslmode=verify-full \
      sslrootcert=system'
```

Learn more about [creating, configuring, and connecting to PgBouncers](/docs/postgres/connecting/pgbouncer).


# PostgreSQL Proxy - Client Error Reference
Source: https://planetscale.com/docs/postgres/connecting/client-error-reference

This document provides a comprehensive reference of all error messages that the Exosphere PostgreSQL proxy may send to clients. These errors follow the PostgreSQL wire protocol format and include standard SQLSTATE error codes.

## Error format

All errors sent by Exosphere follow the PostgreSQL ErrorResponse message format:

| Field    | Description                                |
| :------- | :----------------------------------------- |
| Severity | ERROR, FATAL, PANIC, WARNING, NOTICE, etc. |
| Code     | 5-character SQLSTATE code (e.g., "28P01")  |
| Message  | Human-readable error description           |
| Hint     | Optional suggestion for resolution         |

## Error severity levels

| Severity    | Description                             | Client Action                 |
| ----------- | --------------------------------------- | ----------------------------- |
| **FATAL**   | Connection-terminating error            | Must reconnect                |
| **ERROR**   | Request failed but connection remains   | Can retry or continue         |
| **WARNING** | Potential issue but operation continues | Take note, prepare for action |
| **NOTICE**  | Informational message                   | For awareness only            |

## Error categories

### Authentication and authorization errors

#### SSL/TLS required

* **Severity:** FATAL
* **Code:** 28000 (invalid\_authorization\_specification)
* **Message:** "SSL connection is required"
* **Hint:** "Use sslmode=require or connect with SSL enabled"
* **When:** Client attempts unencrypted connection when TLS is mandatory
* **Resolution:** Configure client to use SSL/TLS (e.g., `sslmode=verify-full` in connection string)

#### Invalid user format

* **Severity:** FATAL
* **Code:** 28000 (invalid\_authorization\_specification)
* **Message:** "invalid user format: username must include branch (e.g., user.branch)"
* **When:** Username doesn't follow required format for branch routing
* **Resolution:** Include branch identifier in username (format: `username.branchname`)

#### Authentication failure

* **Severity:** FATAL
* **Code:** 28P01 (invalid\_password)
* **Message:** Various authentication-specific messages
* **When:** Password validation fails
* **Resolution:** Verify credentials are correct

### Connection and network errors

#### Startup message errors

* **Severity:** FATAL
* **Code:** 08006 (connection\_failure)
* **Messages:**
  * "failed to read startup message"
  * "startup message too short: %d bytes"
  * "incomplete startup message: expected %d bytes, got %d"
  * "failed to parse startup header"
* **When:** Initial connection handshake fails
* **Resolution:** Likely a client library bug - check for driver updates

#### Backend connection failures

* **Severity:** FATAL
* **Code:** 08006 (connection\_failure)
* **Messages:**
  * "failed to connect to upstream"
  * "failed to send startup message"
  * "failed to setup backend"
  * "connection retry timeout - branch %s unavailable after %s"
* **When:** Proxy cannot establish connection to backend database
* **Resolution:** Retry with exponential backoff - this is often transient

### Routing and branch resolution errors

#### Branch not found

* **Severity:** FATAL
* **Code:** 28000 (invalid\_authorization\_specification)
* **Message:** "branch %s does not exist"
* **When:** Specified branch identifier is not recognized
* **Resolution:** Verify branch name is correct

#### Member not found

* **Severity:** FATAL
* **Code:** 28000 (invalid\_authorization\_specification)
* **Message:** "member %s not found in branch %s"
* **When:** Specific database member requested doesn't exist
* **Resolution:** Check member name and branch configuration

#### No primary available

* **Severity:** FATAL
* **Code:** 08006 (connection\_failure)
* **Message:** "no primary available for branch %s"
* **When:** Primary database instance is unavailable
* **Resolution:** Database outage - retry with exponential backoff

#### No replica available

* **Severity:** FATAL
* **Code:** 08006 (connection\_failure)
* **Message:** "no replica available for branch %s"
* **When:** No read replicas are available for the branch
* **Resolution:** All replicas are down - retry with exponential backoff

#### No running members

* **Severity:** FATAL
* **Code:** 08006 (connection\_failure)
* **Message:** "no running members available for branch %s"
* **When:** All database instances in branch are down
* **Resolution:** Total database outage - retry, but investigate the cause as this indicates all instances are down

#### Pooler restriction

* **Severity:** FATAL
* **Code:** 28000 (invalid\_authorization\_specification)
* **Message:** "pooler only supports primary destinations for branch %s"
* **When:** Attempting to use pooler with non-primary target
* **Resolution:** Connect to replicas via port 5432 instead of 6432 (pgbouncer port doesn't support replicas)

## SQLSTATE error codes

Exosphere uses standard PostgreSQL SQLSTATE codes for compatibility:

| Code      | Class                 | Description                           | Common Scenarios                    |
| :-------- | :-------------------- | :------------------------------------ | :---------------------------------- |
| **08006** | Connection Exception  | connection\_failure                   | Network issues, backend unavailable |
| **22001** | Data Exception        | string\_data\_right\_truncation       | Value exceeds field length          |
| **23505** | Integrity Constraint  | unique\_violation                     | Duplicate key violation             |
| **28000** | Invalid Authorization | invalid\_authorization\_specification | Auth configuration issues           |
| **28P01** | Invalid Authorization | invalid\_password                     | Authentication failure              |

## Client library considerations

### Connection retry logic

* Errors with code `08006` are typically transient and safe to retry
* Errors with code `28000` or `28P01` indicate configuration issues - don't retry without changes

### Error handling best practices

1. Always check the SQLSTATE code, not just the message text
2. Implement exponential backoff for connection failures (08006)
3. Handle shutdown notices gracefully by proactively reconnecting
4. Log full error details including severity and code for debugging

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# IP restrictions
Source: https://planetscale.com/docs/postgres/connecting/ip-restrictions

Manage which IP addresses can establish connections to your database.

You can manage IP address restrictions for database connections in the "**IP restrictions**" tab under Settings for your database.
IP restrictions control which networks can connect to your database, providing an additional layer of security beyond authentication.

IP restrictions restrict database connections to the specified IP ranges.
Rules apply to all roles and schemas unless specified otherwise.

## Creating an IP restriction rule

You must be a database or organization administrator to create or modify an IP restriction rule.

<Steps>
  <Step>
    From the PlanetScale organization dashboard, select the desired database
  </Step>

  <Step>
    Navigate to **Settings** from the menu on the left
  </Step>

  <Step>
    Select the **IP restrictions** tab
  </Step>

  <Step>
    Click "**New rule**"
  </Step>

  <Step>
    Configure the rule settings:

    * **Role** (optional): Leave empty to apply to all PostgreSQL roles, or specify a particular role
    * **Schema** (optional): Leave empty to apply to all PostgreSQL schemas, or specify a particular schema
    * **IP ranges** (required): Enter a comma-separated list of IP addresses or CIDR ranges (e.g., `1.2.3.4/32, 10.0.0.0/8`)
  </Step>

  <Step>
    Click "**Create rule**"
  </Step>
</Steps>

## How IP restriction rules work

IP restrictions restrict database connections to the specified IP ranges. The behavior depends on how you configure each rule:

* **Apply to all roles and schemas**: Leave both the **Role** and **Schema** fields empty
* **Apply to specific role**: Specify a role name in the **Role** field to restrict connections for that role across all schemas
* **Apply to specific schema**: Specify a schema name in the **Schema** field to restrict connections to that schema from all roles
* **Apply to specific role and schema**: Specify both to create a rule that applies only when that role connects to that schema

Multiple rules can be created to build complex access policies for your database cluster.

## IP range format

The **IP ranges** field accepts:

* Individual IP addresses in CIDR notation (e.g., `1.2.3.4/32`)
* IP ranges in CIDR notation (e.g., `10.0.0.0/8`, `192.168.1.0/24`)
* Multiple entries separated by commas (e.g., `1.2.3.4/32, 10.0.0.0/8`)

## Editing an IP restriction rule

<Steps>
  <Step>
    Navigate to **Settings** → **IP restrictions**
  </Step>

  <Step>
    Click the menu icon (**...**) on the right side of the rule you want to edit
  </Step>

  <Step>
    Select **Edit** from the menu
  </Step>

  <Step>
    Modify the rule settings as needed:

    * **Role**: Change the role or leave empty to apply to all PostgreSQL roles
    * **Schema**: Change the schema or leave empty to apply to all PostgreSQL schemas
    * **IP ranges**: Update the comma-separated list of IP addresses or CIDR ranges
  </Step>

  <Step>
    Click "**Update rule**" to save your changes
  </Step>
</Steps>

## Deleting an IP restriction rule

<Steps>
  <Step>
    Navigate to **Settings** → **IP restrictions**
  </Step>

  <Step>
    Click the menu icon (**...**) on the right side of the rule you want to delete
  </Step>

  <Step>
    Select **Delete** from the menu
  </Step>

  <Step>
    Review the confirmation dialog showing the rule details (role, schema, and IP ranges)
  </Step>

  <Step>
    Click "**Delete rule**" to confirm deletion, or "**Cancel**" to keep the rule
  </Step>
</Steps>

<Warning>
  Deleting an IP restrictions rule is irreversible. After deletion, connections from those IP ranges will no longer be restricted, potentially allowing broader access to your database.
</Warning>

<Note>
  Rule creation and modification only applies to connections established after the change.
  It does not impact or disconnect existing connections, even if they break the newly-established rules.
</Note>

## Best practices

When configuring IP restrictions rules:

* Start with the most restrictive rules that meet your requirements
* Use CIDR notation to define ranges efficiently (e.g., `/24` for a subnet rather than listing individual IPs)
* Document the purpose of each rule by using descriptive role names or organizing rules by application
* Regularly audit your IP restrictions rules to remove access that is no longer needed
* Consider creating separate roles for different applications or environments to enable fine-grained access control

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Using the Neon serverless driver
Source: https://planetscale.com/docs/postgres/connecting/neon-serverless-driver



PlanetScale supports connections via the [Neon serverless driver](https://neon.com/docs/serverless/serverless-driver).
This is a good option for connecting to your PlanetScale database in serverless environments like [Vercel Functions](https://vercel.com/docs/functions) or [AWS Lambda](https://aws.amazon.com/pm/lambda/).
You can find detailed documentation on [Neon's website](https://neon.com/docs/serverless/serverless-driver) or on the [GitHub repository](https://github.com/neondatabase/serverless).

## HTTP vs WebSocket modes

The Neon serverless driver supports two connection modes:

* **HTTP mode** — Uses the `neon` function to execute queries over HTTP. Faster for single queries and non-interactive transactions. No connection state is maintained between requests.
* **WebSocket mode** — Uses the `Pool` object to establish a WebSocket connection. Required for session support, interactive transactions, or `node-postgres` compatibility.

PlanetScale supports both modes. Choose based on your use case:

| Use case                                        | Recommended mode |
| ----------------------------------------------- | ---------------- |
| Single queries                                  | HTTP             |
| Non-interactive transactions (batch of queries) | HTTP             |
| Interactive transactions                        | WebSocket        |
| Session-based features                          | WebSocket        |
| `node-postgres` compatibility                   | WebSocket        |

## Setting up credentials

Both modes use the same credentials setup.

<Steps>
  <Step>
    Install the driver via npm:

    ```bash  theme={null}
    npm install @neondatabase/serverless
    ```
  </Step>

  <Step>
    You'll need to create a [Postgres role](/docs/postgres/connecting/roles) to use with the driver.
    Once you have these credentials, place them in environment variables:

    ```bash  theme={null}
    DATABASE_HOST=XXXX.pg.psdb.cloud
    DATABASE_PORT=5432
    DATABASE_NAME=XXXX
    DATABASE_USERNAME=XXXX
    DATABASE_PASSWORD=pscale_pw_XXXX
    ```

    These can all be added to a unified Postgres connection URL for use by the driver:

    ```bash  theme={null}
    DATABASE_URL="postgresql://$DATABASE_USERNAME:$DATABASE_PASSWORD@$DATABASE_HOST:$DATABASE_PORT/$DATABASE_NAME"
    ```
  </Step>
</Steps>

## Using HTTP mode

HTTP mode is the simplest way to execute queries. You must configure the `fetchEndpoint` to use PlanetScale's SQL endpoint.

```ts  theme={null}
import { neon, neonConfig } from "@neondatabase/serverless";

// This MUST be set for PlanetScale Postgres connections
neonConfig.fetchEndpoint = (host) => `https://${host}/sql`;

const sql = neon(process.env.DATABASE_URL!);

const posts = await sql`SELECT * FROM posts WHERE id = ${postId}`;
```

The `neon` function returns a tagged template literal that automatically handles parameterized queries, protecting against SQL injection. See [Neon's HTTP mode documentation](https://neon.com/docs/serverless/serverless-driver#use-the-driver-over-http) for additional configuration options.

### Non-interactive transactions

HTTP mode supports non-interactive transactions where you send a batch of queries to be executed together. Use the `transaction` function:

```ts  theme={null}
import { neon, neonConfig } from "@neondatabase/serverless";

// This MUST be set for PlanetScale Postgres connections
neonConfig.fetchEndpoint = (host) => `https://${host}/sql`;

const sql = neon(process.env.DATABASE_URL!);

const [posts, tags] = await sql.transaction([
  sql`SELECT * FROM posts ORDER BY posted_at DESC LIMIT ${limit}`,
  sql`SELECT * FROM tags`,
]);
```

## Using WebSocket mode

WebSocket mode provides a full `Pool` interface compatible with the `pg` library. See [Neon's WebSocket documentation](https://neon.com/docs/serverless/serverless-driver#use-the-driver-over-websockets) for the full API reference. This mode requires additional configuration for PlanetScale connections.

<Steps>
  <Step>
    When connecting, you must set the following configuration options:

    ```ts  theme={null}
    neonConfig.pipelineConnect = false;
    neonConfig.wsProxy = (host, port) => `${host}/v2?address=${host}:${port}`;
    ```
  </Step>

  <Step>
    Here's a complete example:

    ```ts  theme={null}
    import ws from "ws";
    import { Pool, neonConfig } from "@neondatabase/serverless";

    neonConfig.webSocketConstructor = ws;
    // These MUST be set for PlanetScale Postgres connections
    neonConfig.pipelineConnect = false;
    neonConfig.wsProxy = (host, port) => `${host}/v2?address=${host}:${port}`;

    const pool = new Pool({ connectionString: process.env.DATABASE_URL });

    const posts = await pool.query("SELECT * FROM posts WHERE id = $1", [postId]);

    pool.end();
    ```
  </Step>
</Steps>

<Note>
  In browser or edge environments that have a native `WebSocket` global, you don't need to import `ws` or set `neonConfig.webSocketConstructor`.
</Note>

## Security

PlanetScale requires `SCRAM-SHA-256` for all authentication to Postgres servers.
We maintain this strict requirement for security purposes.

For WebSocket connections, you must set `neonConfig.pipelineConnect = false;`.
This adds a bit of additional latency, but is necessary to connect using `SCRAM-SHA-256`.
When this is `"password"` (the default value) it requires using cleartext password authentication, reducing connection security.

HTTP mode connections handle authentication automatically and don't require this configuration.


# PgBouncer
Source: https://planetscale.com/docs/postgres/connecting/pgbouncer

PgBouncer provides connection pooling for your Postgres database.

## When to use PgBouncer

PgBouncer is generally recommended for OLTP workloads. All application connections should be routed through PgBouncer whenever possible. Learn more about the pros and cons of the different connection methods on the [connections overview page](/docs/postgres/connecting).

PlanetScale provides several options for using PgBouncer, including local PgBouncers and dedicated replica PgBouncers.

PgBouncer connections operate in transaction mode, which means pooled server connections are assigned to client connections on a per-transaction level. This provides excellent performance for OLTP workloads but limits certain PostgreSQL features that require persistent connections. Learn more at the [PgBouncer documentation](https://www.pgbouncer.org/features.html).

## When to NOT use PgBouncer

For use cases that require long-running operations, direct connections on port `5432` are recommended. For example:

* Schema changes and DDL
* OLAP, analytics, reporting, or batch processing
* Session-specific features: Custom session variables, temporary tables
* ETL processes and data streaming
* Long-running transactions or queries that span multiple transactions
* Creating a local backup with `pg_dump`

## Local PgBouncer

Every PlanetScale Postgres database includes an instance of PgBouncer running on the same node as the primary Postgres database (local PgBouncer). To connect to PgBouncer, use the same credentials as a direct connection, but use port `6432` instead of the Postgres default of `5432`. For example:

```bash  theme={null}
psql 'host=xxxxxxxxxx-useast1-1.horizon.psdb.cloud port=6432 user=postgres.xxxxxxxxxx password=pscale_pw_xxxxxxxxxxxxxxxxxx dbname=my_database sslnegotiation=direct sslmode=verify-full sslrootcert=system'
```

<Note>
  The local PgBouncer does not support routing queries to replicas. All connections through the local PgBouncer are automatically routed to the primary database, regardless of the username specification. Use a [dedicated replica PgBouncer](#dedicated-replica-pgbouncers) for replica access.
</Note>

## Dedicated replica PgBouncers

Dedicated replica PgBouncers can be created to run on nodes separate from the Postgres servers. This is useful for applications that send significant read traffic to replicas and need connection pooling. This offers similar high-availability benefits as the local PgBouncer but is used for read-only replica traffic.

### Creating a dedicated replica PgBouncer

You must be a database or organization administrator to create PgBouncers.

1. From the PlanetScale organization dashboard, select the desired database
2. Navigate to the **Clusters** page from the menu on the left
3. Choose the branch where you want to add a PgBouncer in the "**Branch**" dropdown
4. Select the **PgBouncers** tab
5. Scroll down to the "**Dedicated replica PgBouncers**" section
6. Click the "**Add a replica PgBouncer**" button

<img src="https://mintcdn.com/planetscale-cad1a68a/NiPqJ55qlHgki0wO/docs/postgres/connecting/dedicated-replica-pgbouncer-darkmode.png?fit=max&auto=format&n=NiPqJ55qlHgki0wO&q=85&s=8af1b617b0ea838d3614194b69205402" alt="Dedicated replica PgBouncer" data-og-width="2568" width="2568" data-og-height="998" height="998" data-path="docs/postgres/connecting/dedicated-replica-pgbouncer-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/NiPqJ55qlHgki0wO/docs/postgres/connecting/dedicated-replica-pgbouncer-darkmode.png?w=280&fit=max&auto=format&n=NiPqJ55qlHgki0wO&q=85&s=6ab08db6f888eb2851c9579f76920c7a 280w, https://mintcdn.com/planetscale-cad1a68a/NiPqJ55qlHgki0wO/docs/postgres/connecting/dedicated-replica-pgbouncer-darkmode.png?w=560&fit=max&auto=format&n=NiPqJ55qlHgki0wO&q=85&s=b59a0ae7704fcfb2de5dbc6834d0471c 560w, https://mintcdn.com/planetscale-cad1a68a/NiPqJ55qlHgki0wO/docs/postgres/connecting/dedicated-replica-pgbouncer-darkmode.png?w=840&fit=max&auto=format&n=NiPqJ55qlHgki0wO&q=85&s=cfaa535fe291f6267bf717973f417fbe 840w, https://mintcdn.com/planetscale-cad1a68a/NiPqJ55qlHgki0wO/docs/postgres/connecting/dedicated-replica-pgbouncer-darkmode.png?w=1100&fit=max&auto=format&n=NiPqJ55qlHgki0wO&q=85&s=ee262a05e511a675187d70aa79267020 1100w, https://mintcdn.com/planetscale-cad1a68a/NiPqJ55qlHgki0wO/docs/postgres/connecting/dedicated-replica-pgbouncer-darkmode.png?w=1650&fit=max&auto=format&n=NiPqJ55qlHgki0wO&q=85&s=7ceaee633642871d9270714abc0c65fc 1650w, https://mintcdn.com/planetscale-cad1a68a/NiPqJ55qlHgki0wO/docs/postgres/connecting/dedicated-replica-pgbouncer-darkmode.png?w=2500&fit=max&auto=format&n=NiPqJ55qlHgki0wO&q=85&s=17a06a6c718665759e0ffc2e1afb3e08 2500w" />

7. In the pop-up dialog, give the new PgBouncer a descriptive name. Note that names can not be modified after creation.
8. Select a size based on your connection pooling needs (see [PgBouncer pricing](/docs/postgres/pricing#pgbouncer-pricing) for available sizes)

<img src="https://mintcdn.com/planetscale-cad1a68a/7zfT4hLZ7cFflCIU/docs/postgres/connecting/create-pgbouncer-darkmode.png?fit=max&auto=format&n=7zfT4hLZ7cFflCIU&q=85&s=73733be79c6ab0884c52163e66820cc0" alt="Create a PgBouncer" data-og-width="1066" width="1066" data-og-height="1022" height="1022" data-path="docs/postgres/connecting/create-pgbouncer-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/7zfT4hLZ7cFflCIU/docs/postgres/connecting/create-pgbouncer-darkmode.png?w=280&fit=max&auto=format&n=7zfT4hLZ7cFflCIU&q=85&s=67b5f2fa3eb07e0d47042977914f0713 280w, https://mintcdn.com/planetscale-cad1a68a/7zfT4hLZ7cFflCIU/docs/postgres/connecting/create-pgbouncer-darkmode.png?w=560&fit=max&auto=format&n=7zfT4hLZ7cFflCIU&q=85&s=16c6136c5f124814f906fcb97d0292d9 560w, https://mintcdn.com/planetscale-cad1a68a/7zfT4hLZ7cFflCIU/docs/postgres/connecting/create-pgbouncer-darkmode.png?w=840&fit=max&auto=format&n=7zfT4hLZ7cFflCIU&q=85&s=7e26384ef51956bfc898a841e4d06f9d 840w, https://mintcdn.com/planetscale-cad1a68a/7zfT4hLZ7cFflCIU/docs/postgres/connecting/create-pgbouncer-darkmode.png?w=1100&fit=max&auto=format&n=7zfT4hLZ7cFflCIU&q=85&s=dc4fd340f5524e7bebf32132a0f3cf32 1100w, https://mintcdn.com/planetscale-cad1a68a/7zfT4hLZ7cFflCIU/docs/postgres/connecting/create-pgbouncer-darkmode.png?w=1650&fit=max&auto=format&n=7zfT4hLZ7cFflCIU&q=85&s=e44ab20081ce65d596a1dd018af645d9 1650w, https://mintcdn.com/planetscale-cad1a68a/7zfT4hLZ7cFflCIU/docs/postgres/connecting/create-pgbouncer-darkmode.png?w=2500&fit=max&auto=format&n=7zfT4hLZ7cFflCIU&q=85&s=09aa9a313a3e481a67a0fd34cde81986 2500w" />

9. Click "**Create PgBouncer**"
10. Wait a few minutes for the creation to complete

A new entry for the PgBouncer will appear in the Dedicated replica PgBouncers section once provisioning is complete.

Multiple replica PgBouncers can be created if needed. This is useful for adding additional PgBouncer capacity or for having distinct bouncers for different client applications to manage connection pooling with more precision.

#### Availability zone affinity

Dedicated replica PgBouncers can be configured to prefer routing to the Postgres replica servers inside their own availability zone. Applications deployed across several zones can benefit from lower replica query latency in this configuration. However, if your application is deployed to a single zone, this mode may direct most queries to one replica server while replicas in other zones receive little traffic. Allowing the bouncer to load balance across availability zones, without preferring its own zone, will spread the query volume across the replica servers for single-zone applications.

Select the **Prefer routing to replicas in the same availability zone** checkbox to enable affinity.

### Connecting to dedicated replica PgBouncers

Connect to dedicated replica PgBouncers by appending `|pgbouncer-name` to the username of any [role you have created](/docs/postgres/connecting/roles). For example, if your username is `user1.abcdefghi` and the dedicated replica PgBouncer is named `read-bouncer`, the connection username should be `user1.abcdefghi|read-bouncer`.

The hostname and password remain the same. Use port `6432` for dedicated PgBouncer connections:

```bash  theme={null}
psql 'host=xxxxxxxxxx-useast1-1.horizon.psdb.cloud \
      port=6432 \
      user=postgres.xxxxxxxxxx|read-bouncer \
      password=pscale_pw_xxxxxxxxxxxxxxxxxx \
      dbname=my_database \
      sslnegotiation=direct \
      sslmode=verify-full \
      sslrootcert=system'
```

## Dedicated primary PgBouncers

A dedicated primary PgBouncer provides connection pooling for your primary database, running on nodes separate from the Postgres servers. Connections through dedicated PgBouncers persist through cluster resizes, upgrades, and most failover scenarios, providing improved high availability. Primary bouncers are configured in the same way as replica bouncers.

### Connecting to dedicated primary PgBouncers

Connect to dedicated primary PgBouncers by appending `|pgbouncer-name` to the username of any [role you have created](/docs/postgres/connecting/roles). For example, if your username is `user1.abcdefghi` and the dedicated primary PgBouncer is named `write-pool`, the connection username should be `user1.abcdefghi|write-pool`.

The hostname and password remain the same. Use port `6432` for dedicated PgBouncer connections:

```bash  theme={null}
psql 'host=xxxxxxxxxx-useast1-1.horizon.psdb.cloud \
      port=6432 \
      user=postgres.xxxxxxxxxx|write-pool \
      password=pscale_pw_xxxxxxxxxxxxxxxxxx \
      dbname=my_database \
      sslnegotiation=direct \
      sslmode=verify-full \
      sslrootcert=system'
```

## Configuring PgBouncers

Each PgBouncer on the "PgBouncers" tab can be individually configured with a section like this under each PgBouncer:

<img src="https://mintcdn.com/planetscale-cad1a68a/NiPqJ55qlHgki0wO/docs/postgres/connecting/pgbouncer-settings-darkmode.png?fit=max&auto=format&n=NiPqJ55qlHgki0wO&q=85&s=5174f46872d030aca728d92e2f0ebe98" alt="Configure a PgBouncer" data-og-width="2566" width="2566" data-og-height="878" height="878" data-path="docs/postgres/connecting/pgbouncer-settings-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/NiPqJ55qlHgki0wO/docs/postgres/connecting/pgbouncer-settings-darkmode.png?w=280&fit=max&auto=format&n=NiPqJ55qlHgki0wO&q=85&s=dc0bb545591e95143e770cce608d2e5a 280w, https://mintcdn.com/planetscale-cad1a68a/NiPqJ55qlHgki0wO/docs/postgres/connecting/pgbouncer-settings-darkmode.png?w=560&fit=max&auto=format&n=NiPqJ55qlHgki0wO&q=85&s=4363884a98e626d1bec2ad04edd279ef 560w, https://mintcdn.com/planetscale-cad1a68a/NiPqJ55qlHgki0wO/docs/postgres/connecting/pgbouncer-settings-darkmode.png?w=840&fit=max&auto=format&n=NiPqJ55qlHgki0wO&q=85&s=8eb07efa43da3a82fc5d485d5cc9f74a 840w, https://mintcdn.com/planetscale-cad1a68a/NiPqJ55qlHgki0wO/docs/postgres/connecting/pgbouncer-settings-darkmode.png?w=1100&fit=max&auto=format&n=NiPqJ55qlHgki0wO&q=85&s=80b6fde3e247d9409b6f267ed06915f5 1100w, https://mintcdn.com/planetscale-cad1a68a/NiPqJ55qlHgki0wO/docs/postgres/connecting/pgbouncer-settings-darkmode.png?w=1650&fit=max&auto=format&n=NiPqJ55qlHgki0wO&q=85&s=2c792bfcdbb11ccc53e5286da2d7e52e 1650w, https://mintcdn.com/planetscale-cad1a68a/NiPqJ55qlHgki0wO/docs/postgres/connecting/pgbouncer-settings-darkmode.png?w=2500&fit=max&auto=format&n=NiPqJ55qlHgki0wO&q=85&s=83b89ed0ebb11bd74cdf96e99bc0e07c 2500w" />

The basic settings are at the top, with advanced settings available as an option. Adjusting advanced settings is not recommended unless there is a good understanding of how PgBouncer works.

### Configurable parameters

The following parameters can be configured for both the local and dedicated replica PgBouncers.

#### Basic settings

| Parameter             | Description                                                                                                                                     |
| --------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- |
| default\_pool\_size   | How many server connections to allow per user/database pair. Default: `20`                                                                      |
| min\_pool\_size       | Add more server connections to pool if below this number. Improves behavior when load returns after inactivity. Default: `0`                    |
| max\_client\_conn     | Maximum number of client connections allowed. Default: `100`                                                                                    |
| server\_lifetime      | The pooler will close unused server connections that have been connected longer than this. 0 means use once then close. Default: `3600` seconds |
| server\_idle\_timeout | Close server connections idle longer than this many seconds. 0 disables this timeout. Default: `600` seconds                                    |

#### Advanced settings

Advanced parameters should only be adjusted with a thorough understanding of PgBouncer internals.

| Parameter                                 | Description                                                                                                                                            |
| ----------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Connection Limits**                     |                                                                                                                                                        |
| max\_prepared\_statements                 | When non-zero, PgBouncer tracks protocol-level named prepared statements in transaction and statement pooling mode. Default: `200`                     |
| max\_db\_connections                      | Do not allow more than this many server connections per database (regardless of user). 0 is unlimited. Default: `0`                                    |
| max\_db\_client\_connections              | Do not allow more than this many client connections per database (regardless of user). 0 is unlimited. Default: `0`                                    |
| max\_user\_connections                    | Do not allow more than this many server connections per user (regardless of database). 0 is unlimited. Default: `0`                                    |
| max\_user\_client\_connections            | Do not allow more than this many client connections per user (regardless of database). 0 is unlimited. Default: `0`                                    |
| reserve\_pool\_size                       | How many additional connections to allow to a pool. 0 disables. Default: `0`                                                                           |
| reserve\_pool\_timeout                    | If a client has not been serviced in this time, use additional connections from the reserve pool. 0 disables. Default: `5` seconds                     |
| **Timeouts**                              |                                                                                                                                                        |
| query\_timeout                            | Cancel queries running longer than this. Use with smaller server-side statement\_timeout for network problems. Default: `0` seconds                    |
| query\_wait\_timeout                      | Maximum time queries wait for execution. Client disconnected if query not assigned to server in time. 0 disables. Default: `120` seconds               |
| client\_idle\_timeout                     | Close client connections idle longer than this. Should be larger than client-side lifetime settings. Default: `0` seconds                              |
| client\_login\_timeout                    | Disconnect clients that don't log in within this time. Prevents dead connections stalling SUSPEND and restart. Default: `60` seconds                   |
| idle\_transaction\_timeout                | If a client has been in "idle in transaction" state longer, it will be disconnected. Default: `0` seconds                                              |
| cancel\_wait\_timeout                     | Maximum time cancel requests wait for execution. Client disconnected if not assigned to server in time. 0 disables. Default: `10` seconds              |
| autodb\_idle\_timeout                     | How long database pools stay cached after last use. After timeout, unused pools are freed and stats reset. Default: `3600` seconds                     |
| suspend\_timeout                          | How long to wait for buffer flush during SUSPEND or reboot (-R). Connection dropped if flush fails. Default: `10` seconds                              |
| **Server Health**                         |                                                                                                                                                        |
| server\_check\_query                      | Simple query to check if server connection is alive. Empty string disables sanity checking. Default: `select 1`                                        |
| server\_check\_delay                      | How long to keep released connections available for immediate re-use without running server\_check\_query. 0 always runs check. Default: `30` seconds  |
| **Logging**                               |                                                                                                                                                        |
| log\_connections                          | Log successful logins. Default: `1` (enabled)                                                                                                          |
| log\_disconnections                       | Log disconnections with reasons. Default: `1` (enabled)                                                                                                |
| log\_pooler\_errors                       | Log error messages the pooler sends to clients. Default: `1` (enabled)                                                                                 |
| **Parameter Handling**                    |                                                                                                                                                        |
| ignore\_startup\_parameters               | Allow additional startup parameters that PgBouncer normally rejects. Specify here so PgBouncer knows admin handles them. Default: `extra_float_digits` |
| track\_extra\_parameters                  | Additional parameters to track per client beyond the defaults. Maintained in client cache and restored when client active. Default: `IntervalStyle`    |
| **Low-Level Performance**                 |                                                                                                                                                        |
| pkt\_buf                                  | Internal buffer size for packets. Affects TCP packet size and memory usage. No need to set large for libpq packets. Default: `4096` bytes              |
| sbuf\_loopcnt                             | How many times to process data on one connection before proceeding. Prevents big result sets stalling PgBouncer. 0 = no limit. Default: `5`            |
| disable\_pqexec                           | Disable Simple Query protocol (PQexec). Improves security by preventing some SQL injection attacks. 0 = enabled, 1 = disabled. Default: `0`            |
| **Infrastructure (Local PgBouncer only)** |                                                                                                                                                        |
| Number of processes                       | Sets the number of PgBouncer processes that will run on each node in this branch's cluster. Default: `1`                                               |

Learn more about PgBouncer configuration [on their official website](https://www.pgbouncer.org/features.html).

## How PgBouncer works

Connection reuse is the key mechanism that makes PgBouncer effective. When a client completes a transaction, PgBouncer returns the server connection to the pool rather than closing it. The next client transaction can immediately reuse that existing connection without incurring the overhead of spawning a new Postgres process. This allows a single pooled connection to serve hundreds or thousands of client connections over its lifetime, enabling applications to scale far beyond the constraints of direct connections.

### Pooling modes

PgBouncer supports three pooling modes that determine how connections are assigned:

* **Session Pooling**: Each client connection is given a dedicated connection from the PgBouncer pool for its entire duration. This mode does not provide connection multiplexing benefits.
* **Statement pooling**: Assigns client connections to pooled server connections on a per-query basis. This mode does not allow multi-statement transactions, which is unsuitable for most use cases.
* **Transaction Pooling**: Assigns client connections to pooled server connections on a per-transaction level and allows multi-statement transactions. This is the most suitable mode for the vast majority of workloads and is used by all PlanetScale PgBouncer instances.

### Limitations of transaction pooling

PgBouncer's transaction pooling mode provides excellent performance for OLTP workloads but limits certain PostgreSQL features that require persistent connections:

* Prepared statements that persist across transactions (protocol-level prepared statements work with `max_prepared_statements` configured)
* Temporary tables
* `LISTEN`/`NOTIFY`
* Session-level advisory locks
* `SET` commands that persist beyond a transaction

For operations requiring these features, use a direct connection instead (see the [connections overview](/docs/postgres/connecting#direct-primary-connections)).

### Benefits during maintenance operations

Using PgBouncer provides improved availability during configuration changes. When modifying Postgres Parameters, some changes require the server to be restarted. When these restarts happen, any direct connections to Postgres will be terminated. However, when using PgBouncer, client connections are maintained and PgBouncer handles reconnecting to Postgres after it restarts. The [operations philosophy documentation](/docs/postgres/operations-philosophy) covers more details on how connections are managed during various database lifecycle operations.

### Scaling PgBouncer

PgBouncer itself is a lightweight process, but high connection volumes or high query throughput can eventually exhaust its capacity. PlanetScale offers multiple PgBouncer sizes to handle different workload demands. Each size provides increased CPU and memory resources, allowing PgBouncer to handle more concurrent client connections and higher query throughput without becoming a bottleneck. See [PgBouncer pricing](/docs/postgres/pricing#pgbouncer-pricing) for available sizes.

### PgBouncer error messages

PgBouncer has custom error messages that may be encountered in addition to standard Postgres errors. The [PgBouncer config documentation](https://www.pgbouncer.org/config.html) describes these errors and can be a helpful resource for troubleshooting connection issues.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Connect privately with AWS PrivateLink
Source: https://planetscale.com/docs/postgres/connecting/private-connections/aws-privatelink

When you use AWS PrivateLink, your network traffic between your VPC and PlanetScale stays within the AWS network, without traversing the public internet.

[AWS PrivateLink](https://aws.amazon.com/privatelink/) is a highly available, scalable technology that enables you to privately connect your VPC to supported AWS services, VPC endpoint services, and AWS Marketplace partner services.

### When to use AWS PrivateLink

By default, PlanetScale Postgres databases use secure connections over the public internet with industry-standard TLS encryption. This approach is secure and meets the needs of most customers. However, you may want to consider AWS PrivateLink if:

* **Compliance requirements**: Your organization has stronger regulatory or compliance mandates that require database connections to avoid the public internet entirely
* **Enhanced security posture**: You want an additional layer of network isolation for sensitive data workloads
* **Network architecture**: Your existing AWS infrastructure is designed around private connectivity patterns
* **Reduced network latency**: AWS PrivateLink can help reduce latency by avoiding the extra network hop through a [NAT gateway](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html) that's typically required for outbound internet connections from private subnets. While this latency difference is often minimal (typically single-digit milliseconds), it may be noticeable if you're migrating from a database that was previously hosted directly within your VPC

AWS PrivateLink provides these security and compliance benefits by ensuring your database traffic never leaves the AWS backbone network.

<Note>
  Normal PlanetScale Postgres connectivity (as described in our [standard connection documentation](/docs/postgres/connecting)) uses secure TLS encryption over the public internet and is appropriate for most use cases. AWS PrivateLink is primarily beneficial for compliance and enhanced security requirements.
</Note>

### PrivateLink pricing

PlanetScale does not charge any additional fees for AWS PrivateLink connectivity. However, AWS charges standard PrivateLink pricing for VPC endpoints, which includes:

| Charge Type                     | Rate              | Description                                                |
| ------------------------------- | ----------------- | ---------------------------------------------------------- |
| **VPC endpoint hourly charges** | \~\$0.01 per hour | Per VPC endpoint (varies by region)                        |
| **Data processing charges**     | \~\$0.01 per GB   | Data processed through the VPC endpoint (varies by region) |

For current pricing in your region, see the [AWS PrivateLink pricing page](https://aws.amazon.com/privatelink/pricing/).

## Prerequisites

* A PlanetScale Postgres database in an AWS region
* An AWS VPC in the same region where you want to establish the private connection
* Appropriate AWS IAM permissions to create VPC endpoints (see [AWS VPC endpoint permissions documentation](https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-iam.html))
* Appropriate AWS IAM permissions to create and modify Security Groups (see [AWS IAM permissions for security groups documentation](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-policies-for-amazon-ec2.html#iam-policies-security-groups))

## Establishing a VPC endpoint

1. **Retrieve the Private Service Name**:
   1. From the PlanetScale organization dashboard, select the desired database
   2. Navigate to **Settings** from the menu on the left
   3. Select **Roles**
   4. Click on a role with permissions to the relevant `Branch`
   5. Copy the `Private Host` and `Private Service Name` from the role details

<img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/aws-private-host-names.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=7f25404edbdd159f28a8b599fc246156" alt="Private connection strings" data-og-width="1842" width="1842" data-og-height="402" height="402" data-path="docs/postgres/connecting/private-connections/aws-private-host-names.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/aws-private-host-names.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=d84ddbd81aac0518f29d647fd0a67a0f 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/aws-private-host-names.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=af6d05832fced533c3a3540ae923c93b 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/aws-private-host-names.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=c231d2702d4b7d5f958464e1cbe83ee5 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/aws-private-host-names.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=70bcb583dc8f822f54fcaeb83638c584 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/aws-private-host-names.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=ab409eae91f99cea5f59b80b97a9964d 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/aws-private-host-names.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=4e7102d24093a9833c3fe5466e3ee3cd 2500w" />

Save these two attributes for your records and the rest of the configuration.

<Note>
  Both the `Private Host` and `Private Service Name` values are the same for all roles for a given PlanetScale database `Branch`. Once enabled, any role can use the PrivateLink endpoint. You do not need to configure this per PlanetScale `Role`.
</Note>

1. **Create a Security Group for the Endpoint**:
   You will need an AWS Security Group configured to allow inbound traffic for the required ports. You can configure access using either the security group ID of your application hosts, your VPC's CIDR configuration, or specific subnet CIDR configurations. Ensure your [security groups](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-groups.html) allow:

   * **Inbound PostgreSQL (port 5432)**: For direct database connections
   * **Inbound PSBouncer (port 6432)**: For pooled connections via PSBouncer

   An example using the AWS CLI:

   ```bash  theme={null}
   # Create the security group (capture its ID)
   SG_ID=$(aws ec2 create-security-group \
   --group-name PScalePrivateLinkEndpointSG \
   --description "Security group for PlanetScale PrivateLink endpoint" \
   --vpc-id <your-vpc-id> \
   --query 'GroupId' --output text)

   # Option A (preferred): allow only from a client SG (replace sg-CLIENT)
   aws ec2 authorize-security-group-ingress \
      --group-id "$SG_ID" \
      --ip-permissions '[
        {"IpProtocol":"tcp","FromPort":5432,"ToPort":5432,"UserIdGroupPairs":[{"GroupId":"sg-CLIENT"}]},
        {"IpProtocol":"tcp","FromPort":6432,"ToPort":6432,"UserIdGroupPairs":[{"GroupId":"sg-CLIENT"}]}
      ]'

   # Option B: allow from entire VPC CIDR (replace with your actual CIDR)
   #aws ec2 authorize-security-group-ingress \
   #--group-id "$SG_ID" \
   #--ip-permissions '[
   #   {"IpProtocol":"tcp","FromPort":5432,"ToPort":5432,"IpRanges":[{"CidrIp":"10.0.0.0/16"}]},
   #   {"IpProtocol":"tcp","FromPort":6432,"ToPort":6432,"IpRanges":[{"CidrIp":"10.0.0.0/16"}]}
   #]'
   ```

   Replace `<your-vpc-id>` with your actual VPC ID. You can find your VPC ID and its CIDR block using:

   ```bash  theme={null}
   aws ec2 describe-vpcs --query 'Vpcs[*].[VpcId,CidrBlock]' --output table
   ```

2. **Navigate to VPC Endpoints**: In your AWS Console:

   1. Confirm you are in the proper `<aws-region>` from the dropdown on the top right
   2. In the search field at the top left enter "Endpoints".
   3. Click the link listed as a **VPC Feature**.

   <img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/endpoint-search.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=eaef632858754672ef532ff2a2d49c6b" alt="Endpoint search" data-og-width="2598" width="2598" data-og-height="872" height="872" data-path="docs/postgres/connecting/private-connections/endpoint-search.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/endpoint-search.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=841f7ed92b0bdf4888a4f4e91295d808 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/endpoint-search.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=114328369251e66300695dbd59fbc8b1 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/endpoint-search.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=b34c80eec9b07e2bf6e61a1327dff774 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/endpoint-search.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=17d71bdcf19f8120c8095347ec9ab185 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/endpoint-search.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=cdea36621a833694256616ccb955a02d 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/endpoint-search.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=591af4eb9b1108c4ecf96265c5f35b85 2500w" />

3. **Create a new endpoint**: Click "**Create Endpoint**".

   <img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/create-new-endpoint.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=bbc1824a503ec0a5f6b27977f32835da" alt="Create a new endpoint" data-og-width="2598" width="2598" data-og-height="874" height="874" data-path="docs/postgres/connecting/private-connections/create-new-endpoint.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/create-new-endpoint.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=7d78a0a6ca4f78baab86663e780ca60e 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/create-new-endpoint.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=0cec45339e7d262577889e0c7a7e16a4 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/create-new-endpoint.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=b38a902b0db5ad77dcca3e7264d7a21f 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/create-new-endpoint.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=b634d07779946ada67a148209aa8c790 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/create-new-endpoint.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=2d8801f44effe53255f0a004d5163736 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/create-new-endpoint.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=4e09c67ca1fd0ad152d7012c515d21a2 2500w" />

4. **Select endpoint type**: Choose "Endpoint services that use NLBs and GWLBs".

   <img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/type-of-endpoint.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=526a744609e2df3a49c21b6e1adc0269" alt="Menu to select endpoint type" data-og-width="2588" width="2588" data-og-height="1280" height="1280" data-path="docs/postgres/connecting/private-connections/type-of-endpoint.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/type-of-endpoint.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=d5c5c21c3e775e3f93af1e73cd36c49b 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/type-of-endpoint.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=06e89fc8e1a9ec47629e5d78117f60d7 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/type-of-endpoint.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=15847eee2fd208a4404853e1bf8d7e9d 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/type-of-endpoint.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=d775b4cf91b2dd1b405172eac27de484 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/type-of-endpoint.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=dd9a23e857a5b50c135a3f5f124428dd 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/type-of-endpoint.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=158d23e1223a03a5af1b7cdc96b808bb 2500w" />

5. **Enter service name**: Enter in the "Service name" text box the `Private Service Name` retrieved from the PlanetScale dashboard. Click "**Verify service**" to confirm the service exists.

   <img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/verified-endpoint.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=c94da8e818f0ed9181764b0eb97401d3" alt="Endpoint service name and verification" data-og-width="1994" width="1994" data-og-height="548" height="548" data-path="docs/postgres/connecting/private-connections/verified-endpoint.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/verified-endpoint.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=d9cae4bf401c4372fb6d916233ccf47a 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/verified-endpoint.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=e2da51bcfb1d2faeccb06eff4f1693f7 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/verified-endpoint.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=d3e1b9ebf1ac7d4784612527fd001208 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/verified-endpoint.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=34ef6146e0f309be45f282cf7b16cb91 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/verified-endpoint.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=43b430f22f780acc77d324a68a72f87f 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/verified-endpoint.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=8045d22f0d07d13202157897e4497eac 2500w" />

6. **Configure VPCs**: Choose the VPC that should have access to the PlanetScale service endpoint.

7. **Enable DNS names**: Click the "Additional settings" dropdown arrow to reveal DNS configuration options, and select the "**Enable DNS name**" checkbox.

8. **Configure Subnets**: Choose the subnets that should have endpoint interfaces for the PlanetScale service endpoint. It is recommended that you select at least 2. You should select subnets that your application servers have access to.

<img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/network-subnets-config.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=8be0ebbfcc7c356d3a1dbe41fcdc50f1" alt="Subnets" data-og-width="1978" width="1978" data-og-height="1632" height="1632" data-path="docs/postgres/connecting/private-connections/network-subnets-config.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/network-subnets-config.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=8b4956809072b7129c2892b352cbeb40 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/network-subnets-config.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=a0b9bfaebf8613f1380b49272768888e 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/network-subnets-config.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=f69afdcdef91629204d27596c209b7b3 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/network-subnets-config.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=1341d8303a94f4942416448683727a25 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/network-subnets-config.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=b04e522a752414cdfb2ed424f2879d65 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/network-subnets-config.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=49889780e7ef328a1c87baaae07d74ba 2500w" />

9. **Configure security groups**: Choose the appropriate security group to control which resources can send traffic to the PlanetScale service endpoint. Use the one created earlier if you created one for this purpose.

<img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/security-groups.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=b8de8e5e51d5b772a8b6f32b7a828a6d" alt="Security Groups" data-og-width="2710" width="2710" data-og-height="514" height="514" data-path="docs/postgres/connecting/private-connections/security-groups.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/security-groups.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=0f427f4b9325d146c5484c040d1f7f8c 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/security-groups.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=19a7332357bba7cbde1a9430e6e6fcad 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/security-groups.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=ea83ac732762da72aa6c0f6442885080 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/security-groups.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=c6e3764e42629d11ee2ae8094cdd07c0 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/security-groups.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=482ffdd2b4c1bc06784126de65ef6d86 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/security-groups.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=00cb783f608a6884a0e8a04c80780d92 2500w" />

10. **Create the endpoint**: Click "**Create endpoint**" and wait for the VPC endpoint status to show "Available" (this may take several minutes).

<img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/available-endpoint.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=82cb9829d128862ed2ead700439adad7" alt="Available Endpoint" data-og-width="1602" width="1602" data-og-height="1098" height="1098" data-path="docs/postgres/connecting/private-connections/available-endpoint.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/available-endpoint.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=dbb942840764c8a6d89f2fc1b1c06e59 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/available-endpoint.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=d7a1766ab4fce84d2df49cfc75432f7e 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/available-endpoint.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=798f424f2fa386cbaeec169a8396b73b 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/available-endpoint.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=61986496a68eefc47fab4161433ad007 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/available-endpoint.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=746cef0e9d7e0f3528a01748210b141c 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/available-endpoint.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=194992e9506403863fa8950487c313a6 2500w" />

## Verifying your VPC endpoint connectivity

1. **Confirm endpoint status**: In the AWS Console, verify that your endpoint's status shows "Available".

2. **Test DNS resolution**: From an EC2 instance in your configured VPC, run a DNS lookup to confirm resolution to your VPC's IP range. Use the `Private Host` you recorded earlier from the PlanetScale dashboard:

   ```bash  theme={null}
   dig +short <YOUR ENDPOINT>.private-pg.psdb.cloud
   10.0.2.120
   10.0.1.118
   ```

3. **Test your new connection**:

   Once you have confirmed DNS resolution, test the private endpoint:

   ```bash  theme={null}
   psql 'host=<YOUR ENDPOINT>.private-pg.psdb.cloud port=5432 user=postgres.XYZ234 password=pscale_pw_REDACTED dbname=postgres sslnegotiation=direct sslmode=verify-full sslrootcert=system'
   ```

## Update your connection strings

Once your VPC endpoint is established and verified, you're ready to update your application's connection strings to use the private endpoint address instead of the standard public endpoint.

## Security group considerations

Ensure your [security groups](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-security-groups.html) allow:

* **Outbound PostgreSQL (port 5432)**: For direct database connections
* **Outbound PSBouncer (port 6432)**: For pooled connections via PSBouncer
* **Inbound - any application-specific ports**: Based on your connection requirements

For more details about connection types and when to use each port, see our [connection documentation](/docs/postgres/connecting) and [PSBouncer guide](/docs/postgres/connecting/pgbouncer).

## Network ACL considerations

VPC [Network ACLs (NACLs)](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html) operate at the subnet level and provide an additional layer of security beyond security groups. For AWS PrivateLink connections to PlanetScale, ensure your NACLs allow:

* **Outbound PostgreSQL (ports 5432, 6432)**: For database connections
* **Ephemeral ports (1024-65535)**: For return traffic from AWS PrivateLink endpoints

Most default NACL configurations allow all outbound traffic and are compatible with PrivateLink. If using custom restrictive NACLs, add explicit allow rules for the above ports.

## Troubleshooting

If you're experiencing connectivity issues:

1. **Verify endpoint status**: Ensure your VPC endpoint shows "Available" status
2. **Check security groups**: Confirm your security groups allow the required ports
3. **Check NACLs**: Confirm that your VPC's NACLs are configured to allow the correct network traffic
4. **Test DNS resolution**: Verify DNS is resolving to private IP addresses in your VPC CIDR range
5. **Use AWS Reachability Analyzer**: The [Reachability Analyzer](https://docs.aws.amazon.com/vpc/latest/reachability/what-is-reachability-analyzer.html) allows you to inspect the path between two resources (such as a client and your PlanetScale Postgres endpoint) and provides guidance on why connectivity might be failing
6. **Contact support**: If issues persist, contact PlanetScale support with your endpoint configuration details

## Next steps

* [Learn about PostgreSQL roles and permissions](/docs/postgres/connecting/roles)
* [Configure connection pooling with PSBouncer](/docs/postgres/connecting/pgbouncer)
* [Monitor your connections and performance](/docs/postgres/monitoring/query-insights)

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Connect privately with GCP Private Service Connect
Source: https://planetscale.com/docs/postgres/connecting/private-connections/gcp-private-service-connect

When you use GCP Private Service Connect, your network traffic between your VPC and PlanetScale stays within the Google Cloud network, without traversing the public internet.

[GCP Private Service Connect](https://cloud.google.com/vpc/docs/private-service-connect) is a highly available, scalable technology that enables you to privately connect your VPC to supported GCP services, endpoint services, and partner services.

### When to use GCP Private Service Connect

By default, PlanetScale Postgres databases use secure connections over the public internet with industry-standard TLS encryption. This approach is secure and meets the needs of most customers. However, you may want to consider GCP Private Service Connect if:

* **Compliance requirements**: Your organization has regulatory or compliance mandates that require database connections to avoid the public internet entirely
* **Enhanced security posture**: You want an additional layer of network isolation for sensitive data workloads
* **Network architecture**: Your existing GCP infrastructure is designed around private connectivity patterns
* **Reduced network latency**: GCP Private Service Connect can help reduce latency by keeping traffic within Google's network backbone

GCP Private Service Connect provides these security and compliance benefits by ensuring your database traffic never leaves the Google Cloud network.

<Note>
  Normal PlanetScale Postgres connectivity (as described in our [standard connection documentation](/docs/postgres/connecting)) uses secure TLS encryption over the public internet and is appropriate for most use cases. GCP Private Service Connect is primarily beneficial for compliance and enhanced security requirements.
</Note>

### Private Service Connect pricing

PlanetScale does not charge any additional fees for GCP Private Service Connect connectivity. However, Google Cloud charges standard Private Service Connect pricing for endpoints, which includes:

* **Private Service Connect endpoint charges**: Based on your endpoint configuration and usage
* **Network egress charges**: Standard GCP egress pricing may apply for data transfer

For current pricing in your region, see the [GCP Private Service Connect pricing page](https://cloud.google.com/vpc/pricing#private-service-connect).

## Prerequisites

* A PlanetScale Postgres database in a GCP region
* A GCP VPC in the same region where you want to establish the private connection
* Appropriate GCP IAM permissions to create Private Service Connect endpoints
* **Required APIs enabled**: Cloud DNS API and Service Directory API must be enabled in your project for automatic DNS zone creation (see [GCP documentation](https://cloud.google.com/vpc/docs/dns-vpc-hosted-services#auto-dns-consumer))

## Establishing a Private Service Connect endpoint

1. **Retrieve the Private Service Name**:

   1. From the PlanetScale organization dashboard, select the desired database
   2. Navigate to **Settings** from the menu on the left
   3. Select **Roles**
   4. Click on a role with permissions to the relevant `Branch`
   5. Copy the `Private Host` and `Private Service Name` from the role details

   <img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-private-host-names.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=e9f6dc8d5c8e1200605ecbc591654ada" alt="Private connection strings" data-og-width="1814" width="1814" data-og-height="356" height="356" data-path="docs/postgres/connecting/private-connections/psc-private-host-names.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-private-host-names.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=2e2c447eb05997378a3db533db3f619d 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-private-host-names.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=c1f03cd844054a61b07bde1a1ecc2ad4 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-private-host-names.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=605af81afa51b1b288d2d719893ef59d 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-private-host-names.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=dcb6414f97afbcf4e57a6f1dd86d5b6c 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-private-host-names.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=c0409e80527977ce87ca6ddb08dd0e2c 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-private-host-names.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=881652af414bec7ed4d9fbd82e1ad014 2500w" />

   Save these two attributes for your records and the rest of the configuration.

   <Note>
     Both the `Private Host` and `Private Service Name` values are the same for all roles for a given PlanetScale database `Branch`. Once enabled, any role can use the Private Service Connect endpoint. You do not need to configure this per PlanetScale `Role`.
   </Note>

2. **Enable required APIs**: Ensure Cloud DNS and Service Directory APIs are enabled for automatic DNS zone creation:

   Via GCP Console:

   1. Confirm you are in the proper `<gcp-region>` from the project selector
   2. From the top search bar, search for `CloudDNS` and select it from the results
   3. Click to enable the API
   4. From the top search bar, search for `Service Directory` and select it from the results
   5. Click to enable the API

3. **Navigate to Private Service Connect**: In the GCP Console:

   1. Confirm you are in the proper `<gcp-region>` from the project selector
   2. From the top search bar, search for `Private Service Connect` and select it from the results

   <img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-search.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=04c0d189aa91668dbd413f4a9e7154fb" alt="Private Service Connect console" data-og-width="1416" width="1416" data-og-height="648" height="648" data-path="docs/postgres/connecting/private-connections/psc-search.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-search.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=85b6de8ace83743eabbdaa3c6c1c5a13 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-search.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=48827fe097d04a2276067ca0c1bd05c9 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-search.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=9228523aeafb7184172b6611e7f9209e 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-search.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=95389e0daca62f787e5a955725154402 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-search.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=75c0011d3c2650462ffd88032162fafe 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-search.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=dbc51f7a82468445e695796bf84e3c43 2500w" />

4. **Create a new endpoint**: Click "**+ Connect endpoint**".

5. **Configure endpoint details**:

   * **Target**: Select "Published Service"
   * **Target Service**: Enter the `Private Service Name` recorded from the PlanetScale Dashboard
   * **Endpoint name**: Choose a descriptive name (e.g., "planetscale-main" uses the `Branch` name in it. This name will be part of the connection host string you use going forward)
   * **Network**: Select your VPC network
   * **Subnet**: Choose a subnet that your application servers can access
   * **Create an IP Address**: Reserve a static IP address for the endpoint
   * **Enable Global Access**: Recommended - allows applications in other regions to reach the endpoint
   * **Create a namespace**: Recommended - Set a namespace in `Service Directory` to enable creation of an entry for this endpoint

   <img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-endpoint-configuration.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=8a0e0a7a61ae82a5a409cf5846919bed" alt="Endpoint configuration details" data-og-width="1180" width="1180" data-og-height="1914" height="1914" data-path="docs/postgres/connecting/private-connections/psc-endpoint-configuration.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-endpoint-configuration.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=60a26f6bb0e3edcf94a0342c370a7de7 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-endpoint-configuration.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=1c3ae72db3a3ad08e6e101497c271686 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-endpoint-configuration.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=e4447d283c8007df9565460c8af1936d 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-endpoint-configuration.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=b986474bcb1d3b9cedf552005ee3a9c5 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-endpoint-configuration.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=d0b12b08d7e2ef87daf0944d3bcdebd7 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-endpoint-configuration.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=d860a6fc91fc35697d53bd3c2b597e8f 2500w" />

6. **Create the endpoint**: Click "**Add Endpoint**" and wait for the endpoint status to show "Accepted" (this may take several minutes).

## Verifying your Private Service Connect endpoint connectivity

1. **Confirm endpoint status**: In the GCP Console, verify that your endpoint's status shows "Accepted".

   <img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-active-endpoint.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=599f0bde1b2d4e4b4ca0b215f361f801" alt="Active Endpoint" data-og-width="2164" width="2164" data-og-height="1454" height="1454" data-path="docs/postgres/connecting/private-connections/psc-active-endpoint.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-active-endpoint.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=70ab88598763c06a473739d249af2248 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-active-endpoint.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=57919ed18bd637b22c1252e0833aaa96 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-active-endpoint.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=bb3813bb4de27b0e9ab9e0e6d2d7ef02 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-active-endpoint.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=8d8ae727d98c8ce7bd4e7485a2ecd0f7 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-active-endpoint.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=7372e67d613b79d8bbd3e68fd7210311 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-active-endpoint.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=31bf63e6ac9a3f47d6f6c82a65fcae13 2500w" />

2. **Test DNS resolution**: If Cloud DNS is enabled, GCP automatically creates a private Cloud DNS zone for your endpoint. The DNS zone will match the `Private Host` (recorded earlier from the PlanetScale Dashboard).

   You can verify by navigating to the `Cloud DNS` page from the left Nav:

   <img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-clouddns-record.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=82e2f0554056e66348c166ca1933d83c" alt="Cloud DNS resource" data-og-width="2326" width="2326" data-og-height="874" height="874" data-path="docs/postgres/connecting/private-connections/psc-clouddns-record.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-clouddns-record.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=0914ba0c1f43ecb33d334742f256b3d8 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-clouddns-record.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=ab1937c061a9c6451550db4eef5ffecc 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-clouddns-record.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=e23e24f44d3bfbb3b896635e58892691 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-clouddns-record.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=39c3277a0f79f40efd0caca4a1ae4a22 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-clouddns-record.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=4f5e4e1c3e73b51c900435f2fead02a5 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/connecting/private-connections/psc-clouddns-record.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=39262d84f6eb2ba1ad53181f4d065978 2500w" />

   To address the actual host endpoint you will use, you need to combine the `Endpoint name` you defined with the DNS zone name here (same as the `Private Host` recorded earlier).

   From this example:

   * `Endpoint name` = planetscale-main
   * `Private Host` = gcp-us-central1-1.private-pg.psdb.cloud

   Which results in:

   ```
   planetscale-main.gcp-us-central1-1.private-pg.psdb.cloud
   ```

   To confirm DNS is working properly in your VPC, run a DNS lookup from a VM instance in your configured VPC:

   ```bash  theme={null}
   dig +short planetscale-main.gcp-us-central1-1.private-pg.psdb.cloud
   10.128.0.17
   ```

3. **Test your PostgreSQL connection**:

   Once you have confirmed DNS resolution, test the private endpoint:

   ```bash  theme={null}
   psql 'host=planetscale-main.gcp-us-central1-1.private-pg.psdb.cloud port=5432 user=postgres.XYZ234 password=pscale_pw_REDACTED dbname=postgres sslmode=require'
   ```

## Update your connection strings

Once your Private Service Connect endpoint is established and verified, update your application's connection strings to use the private endpoint address. Note that you need both the `Endpoint name` you configured and the `Private Host` from the PlanetScale dashboard to form the full hostname for your application to use.

* **Original**: `gcp-us-central1-1.pg.psdb.cloud`
* **Private**: `planetscale-main.gcp-us-central1-1.private-pg.psdb.cloud`

Replace the hostname in your connection strings while keeping all other parameters (user, password, database name, etc.) the same.

## VPC considerations

Your VPC configuration should allow:

* **Private Google Access**: Enable this if your compute instances don't have external IP addresses
* **Subnet routing**: Ensure proper routing between your application subnets and the PSC endpoint subnet
* **Network tags**: Use network tags to organize and control access to your PSC endpoint

## Troubleshooting

If you're experiencing connectivity issues:

1. **Verify endpoint status**: Ensure your Private Service Connect endpoint shows "Accepted" status
2. **Test DNS resolution**: Verify DNS is resolving to the private IP address in your VPC
3. **Check VPC routing**: Ensure there are no route conflicts or missing routes
4. **Verify network connectivity**: Use tools like `telnet` or `nc` to test port connectivity
5. **Contact support**: If issues persist, contact PlanetScale support with your endpoint configuration details

## Next steps

* [Learn about PostgreSQL roles and permissions](/docs/postgres/connecting/roles)
* [Configure connection pooling with PgBouncer](/docs/postgres/connecting/pgbouncer)
* [Monitor your connections and performance](/docs/postgres/monitoring/query-insights)

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Connections quickstart
Source: https://planetscale.com/docs/postgres/connecting/quickstart

PlanetScale Postgres works with any standard PostgreSQL driver or library that supports SSL connections.

We recommend reading the [Connections overview documentation](/docs/postgres/connecting) before going through this quickstart. It contains important information such as when and how to use PgBouncer over direct connections.

## Creating a password

Every PostgreSQL database comes with a default `postgres` role that you can generate upon creating your database. This role has the most privileged access to your database and can be used to [create additional roles](./roles) with fewer permissions. See the [`postgres` role section](#default-postgres-role) for more information.

<Steps>
  <Step>
    From the PlanetScale organization dashboard, select the desired database
  </Step>

  <Step>
    Select the desired branch from the dropdown
  </Step>

  <Step>
    Click "**Connect**"
  </Step>

  <Step>
    Click "**Create default role**"
  </Step>

  <Step>
    This generates the unique username and password pair for your default user that you can use to access the designated branch of your database.
  </Step>
</Steps>

<Tip>
  Make sure you copy the credentials, as we will not display them once you leave the page. If you need to access your default credentials after leaving the page, you have to reset the default password.
</Tip>

You'll be provided with the following:

```bash  theme={null}
DATABASE_HOST=**********-<REGION>.horizon.psdb.cloud
DATABASE_NAME=<DATABASE_NAME>
DATABASE_USERNAME=postgres.<BRANCH_ID>
DATABASE_PASSWORD=pscale_pw_**************************
```

Here is an example of the connection string that connects directly (without [PgBouncer](/docs/postgres/connecting/pgbouncer)) you can use to connect to the Postgres CLI:

```bash  theme={null}
psql 'host=xxxxxxxxxx-useast1-1.horizon.psdb.cloud \
      port=5432 \
      user=postgres.xxxxxxxxxx \
      password=pscale_pw_xxxxxxxxxxxxxxxxxx \
      dbname=my_database \
      sslnegotiation=direct \
      sslmode=verify-full \
      sslrootcert=system'
```

### Connection parameters

| Parameter          | Description                                                                                                                                     |
| :----------------- | :---------------------------------------------------------------------------------------------------------------------------------------------- |
| **host**           | Your database hostname in the format `{id+region}.horizon.psdb.cloud`                                                                           |
| **port**           | Connection port (`5432` for direct, `6432` for PgBouncer)                                                                                       |
| **user**           | Your role username in the format `{role}.{branch_id}`                                                                                           |
| **password**       | Your role password (begins with `pscale_pw_`)                                                                                                   |
| **dbname**         | Your database name                                                                                                                              |
| **sslmode**        | Set to `verify-full` for secure connections (required)                                                                                          |
| **sslrootcert**    | Set to `system`. See the [Secure connections documentation](/docs/postgres/connecting/quickstart#secure-connections) if that produces an error. |
| **sslnegotiation** | Set to `direct` for improved performance (optional)                                                                                             |

## Secure connections

All PlanetScale Postgres connections require SSL/TLS encryption. The following parameters are used to enforce this:

| Parameter               | Required | Description                                                                                                                                                                                                                                                                                                                              |
| :---------------------- | :------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `sslmode=verify-full`   | Required | Verifies both encryption and server identity                                                                                                                                                                                                                                                                                             |
| `sslrootcert=system`    | Required | Uses [system certificate](https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNECT-SSLROOTCERT) store by default. If this method doesn't work, you can enter the path to your root certificate instead. The exact [locations of root certificates](/docs/postgres/connecting) differ by SSL implementation and platform. |
| `sslnegotiation=direct` | Optional | Enables direct SSL negotiation for better performance.                                                                                                                                                                                                                                                                                   |

## Additional roles

You should not connect to your database from application servers with the default role, as you will need to take some downtime if you ever have to rotate your password.

Instead, you can [create additional roles](/docs/postgres/connecting/roles) with fine-grained permission settings for this purpose. Check out our [roles](/docs/postgres/connecting/roles) documentation for more information.

## Resetting the default password

If you need to reset the password for the default `postgres` role:

<Steps>
  <Step>
    From the PlanetScale organization dashboard, select the desired database
  </Step>

  <Step>
    Navigate to the **Roles** page in **Settings**
  </Step>

  <Step>
    Find the role labeled **default** for the branch in question
  </Step>

  <Step>
    Open the three dot menu for the role and select **Reset credentials**
  </Step>

  <Step>
    Update the credentials any place that they are in use
  </Step>
</Steps>

<Warning>
  Resetting the default password will disconnect any existing connections using the previous credentials.
</Warning>

## Default `postgres` role

The default `postgres` role is similar to the Postgres `superuser`, but with fewer permissions. It is defined by the following statement:

```sql  theme={null}
CREATE ROLE $POSTGRES_USERNAME
  NOSUPERUSER CREATEDB CREATEROLE INHERIT LOGIN REPLICATION BYPASSRLS PASSWORD '$PASSWORD';
```

It also inherits the following permissions:

```sql  theme={null}
GRANT pg_read_all_data,
  pg_write_all_data,
  pg_read_all_settings,
  pg_read_all_stats,
  pg_stat_scan_tables,
  pg_monitor,
  pg_signal_backend,
  pg_checkpoint,
  pg_maintain,
  pg_use_reserved_connections,
  pg_create_subscription
TO $ALMOST_SUPERUSER_ROLENAME WITH ADMIN OPTION;
```

You should not connect to your database from your applications using the default role, as you will need to take some downtime if you ever have to rotate your password. Instead, you should [create additional roles](./roles) as needed.

## Connection types: Direct vs PgBouncer

PlanetScale offers two connection methods for PostgreSQL databases: direct (port `5432`) and via PgBouncer (port `6432`).

<Note>
  **PlanetScale's use of PgBouncer (port `6432`) does not support replica routing.** All connections through PgBouncer are automatically routed to the primary database, regardless of the username specification. Use direct connections (port `5432`) for replica access.
</Note>

### Direct connection (Port 5432)

Using port 5432 bypasses PgBouncer and connects directly to Postgres. This is the recommended way to connect for any operations that require long-running queries or persistent connections.

PgBouncer operates in transaction pooling mode, where connections are returned to the pool after each transaction completes. This means that session-level features and long-running operations are interrupted between transactions.

Direct connections are recommended for:

* Schema changes and DDL
* OLAP, analytics, reporting, or batch processing
* Session-specific features: Custom session variables, temporary tables
* ETL processes and data streaming
* Long-running transactions or queries that span multiple transactions

Additionally, if you're connecting to a replica, you must connect directly. PgBouncer is not supported for replica connections.

### PgBouncer connection (Port 6432)

[PgBouncer](/docs/postgres/connecting/pgbouncer) enables high availability for a Postgres database by efficiently pooling connections and buffering queries during failovers. PgBouncer is generally recommended for OLTP workloads. For example, we'd recommend routing your application connections through PgBouncer. You can connect through PgBouncer by updating your connection string to use port `6432`.

PgBouncer connections operate in transaction mode, which means each connection is only held for the duration of a single transaction. This provides excellent performance for OLTP workloads but limits certain PostgreSQL features that require persistent connections. For use cases that require long-running operations, we recommend a direct connection on port `5432`.

## Routing to replicas

PlanetScale Postgres supports routing connections to replicas for improved read performance and load distribution. To connect to a replica, append `|replica` to your credential username. For example:

```bash  theme={null}
# Connect to replica
user=postgres.xxxxxxxxx|replica # where postgres.xxxxxxxxx is your username
```

You can append `|replica` to any role you create on your Postgres database.

<Note>
  **PlanetScale's use of PgBouncer (port `6432`) does not support replica routing.** All connections through PgBouncer are automatically routed to the primary database, regardless of the username specification. Use direct connections (port `5432`) for replica access.
</Note>

## Authentication

PlanetScale Postgres uses SCRAM-SHA-256 authentication, which provides enhanced security over traditional password authentication methods.

### Username format

All usernames follow the format `{role}.{branch_id}`:

| Component         | Description                                    | Examples                                         |
| :---------------- | :--------------------------------------------- | :----------------------------------------------- |
| **role**          | The role name                                  | `postgres`, `app_user`                           |
| **branch\_id**    | The unique identifier for your database branch | `cnlmx96ec5kw`                                   |
| **Full username** | Complete format: `{role}.{branch_id}`          | `postgres.cnlmx96ec5kw`, `app_user.cnlmx96ec5kw` |

The branch ID in the username tells PlanetScale's routing layer (Exosphere) which specific database branch to connect to.

### Password format

All PlanetScale Postgres passwords begin with `pscale_pw_` followed by a unique string:

```bash  theme={null}
pscale_pw_XXXXXXXXXXXXXXXXXXXX
```

## Strong security model

PlanetScale roles are created for use with a single database branch. This strong security model allows you to generate roles that are tied to a branch, and cannot access data/schema from another branch.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Managing Roles for your Postgres database
Source: https://planetscale.com/docs/postgres/connecting/roles

When you connect to your database, you have the option to connect with the default role or a user-generated role. This document covers the differences between each option and how to create new roles.

You should not connect to the database from your application servers using the default role. If you ever need to rotate your default role credentials and you use the default role to connect to your application, you will have to take some downtime while rotating the credentials.

Instead, we recommend creating [user-defined roles](#user-defined-roles) for this purpose. We'll first cover the default role below, and then explain how to generate user-defined roles.

## Default role

The default `postgres` role is similar to the Postgres `superuser`, but with fewer permissions. It is defined by the following statement:

```sql  theme={null}
CREATE ROLE $POSTGRES_USERNAME
  NOSUPERUSER CREATEDB CREATEROLE INHERIT LOGIN REPLICATION BYPASSRLS PASSWORD '$PASSWORD';
```

It also inherits the following permissions:

```sql  theme={null}
GRANT pg_read_all_data,
  pg_write_all_data,
  pg_read_all_settings,
  pg_read_all_stats,
  pg_stat_scan_tables,
  pg_monitor,
  pg_signal_backend,
  pg_checkpoint,
  pg_maintain,
  pg_use_reserved_connections,
  pg_create_subscription
TO $ALMOST_SUPERUSER_ROLENAME WITH ADMIN OPTION;
```

## User-defined roles

When creating custom roles for your application, you can select from a variety of permissions to grant specific capabilities. User-defined roles allow you to implement the principle of least privilege by granting only the permissions necessary for each use case.

For examples of common user-defined roles, see [User-defined role examples and use cases](#user-defined-role-examples-and-use-cases).

### Available permissions

Below is a list of available permissions you can set on user-defined roles.

**Data access permissions**

* **pg\_read\_all\_data** — Read data from all tables, views, and sequences. This permission allows `SELECT` queries across all database objects.

* **pg\_write\_all\_data** — Write data to all tables, views, and sequences. This permission allows `INSERT`, `UPDATE`, `DELETE`, and `TRUNCATE` operations. Note that write operations typically require `pg_read_all_data` as well to read the data being modified.

**Configuration and monitoring permissions**

* **pg\_read\_all\_settings** — Read all configuration variables. This allows viewing database configuration parameters.

* **pg\_read\_all\_stats** — Read all `pg_stat_*` views. This provides access to database statistics and performance metrics.

* **pg\_stat\_scan\_tables** — Execute monitoring functions that may take `ACCESS SHARE` locks on tables. This is useful for running database monitoring and analysis operations.

* **pg\_monitor** — Read and execute monitoring views and functions. This is a convenience role that combines several monitoring-related permissions.

**Administrative permissions**

* **pg\_signal\_backend** — Signal another backend to cancel a query or terminate its session. This is useful for managing long-running queries and terminating problematic connections.

* **pg\_checkpoint** — Execute the `CHECKPOINT` command. Checkpoints ensure that all data is written to disk and are important for database recovery.

* **pg\_maintain** — Execute maintenance operations including `VACUUM`, `ANALYZE`, `CLUSTER`, `REFRESH MATERIALIZED VIEW`, `REINDEX`, and `LOCK TABLE`. These operations are essential for database performance and maintenance.

* **pg\_use\_reserved\_connections** — Use connection slots reserved via `reserved_connections`. This allows connecting to the database even when all regular connection slots are in use.

* **pg\_create\_subscription** — Allow users with `CREATE` permission on the database to issue `CREATE SUBSCRIPTION`. This is used for logical replication scenarios.

**Superuser-equivalent permission**

* **postgres** — The default near-superuser role with extensive permissions. This role can create, modify, and drop databases, users, roles, tables, schemas, and all other objects. Use this permission carefully, as it grants broad administrative capabilities.

## Creating new user-defined roles

There are several ways to create a new role:

* Using the "Connect" button in your dashboard
* Using "Roles" section in your database settings
* Using the [`CREATE ROLE`](https://www.postgresql.org/docs/current/sql-createrole.html) command as the default role (which has elevated privileges).
* Using the Postgres [Roles API](/docs/api/reference/list_roles)
* Using the PlanetScale CLI [pscale role commands](/docs/cli/role)

### Creating roles in the dashboard

To create a new role in the dashboard, you can either click the "Connect" button on the database overview page, or navigate to "Settings" > "Roles" and click "New role".

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/roles/image3.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=b3432ca882cfad33aae8c66151915aef" alt="Configure the new role" data-og-width="1980" width="1980" data-og-height="2356" height="2356" data-path="docs/images/assets/docs/postgres/roles/image3.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/roles/image3.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=2b66a320d0fefa7ba182f112e84375b6 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/roles/image3.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=2c088beb0e7fbe62b008568c13696b0a 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/roles/image3.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=1edb57cf63f9fbb99f3b7817b416d9c7 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/roles/image3.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=13b06d2207b178c41cda07019ee9f836 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/roles/image3.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=299d2c7509f89dee878fc38315dc0931 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/roles/image3.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=55f5c6e273f65b87aab34f9e5003cd39 2500w" />
</Frame>

### Creating roles via the CLI

You can also manage roles directly from the command line using the PlanetScale `pscale role` CLI. This provides a convenient way to create, list, and manage roles as part of your development workflow or automation scripts.

Make sure you have the [PlanetScale CLI](/docs/cli) installed

#### Available commands

**Create a new role:**

```
pscale role create <database> <branch> <name> [flags]
```

Example:

```
pscale role create my-database main api-user --inherited-roles pg_read_all_data --ttl 24h
```

**List all roles for a branch:**

```
pscale role list <database> <branch>
```

**Get details for a specific role:**

```
pscale role get <database> <branch> <role-id>
```

**Delete a role:**

```
pscale role delete <database> <branch> <role-id> [--successor <other-role>]
```

**Update a role's name:**

```
pscale role update <database> <branch> <role-id> --name <new-name>
```

**Renew a role's expiration:**

```
pscale role renew <database> <branch> <role-id>
```

**Reset the default postgres role credentials:**

```
pscale role reset-default <database> <branch>
```

**Reset a role's password:**

```
pscale role reset --org <org> <database> <branch> <role-id>
```

This command resets the password for a role after prompting for confirmation. It returns the role object with the new password.

**Reassign objects owned by a role:**

```
pscale role reassign --org <org> <database> <branch> <donor> --successor <recipient>
```

This command assigns all objects owned by the donor role to the recipient role.

Roles created via the CLI will appear in your database settings and can be managed through the dashboard as well.

### Creating roles via `CREATE ROLE`

When you create a role via the Postgres [`CREATE ROLE`](https://www.postgresql.org/docs/current/sql-createrole.html) command, these will not display on your database settings. It is up to you to manage these via the `psql` CLI.

PlanetScale's routing layer uses the `user` to identify which database or branch we are sending queries to. For example, the user `matt.nk35mx55qq` routes to the PlanetScale database with branch id `nk35mx55qq`. When you create a new role, you do not need to specify the branch id on the user. You can simply set the user to `matt`.

However, when you connect, you must append the branch id to the user so we know which branch to route to.

<Steps>
  <Step>
    From the PlanetScale organization dashboard, select the desired database
  </Step>

  <Step>
    Select the desired branch from the dropdown
  </Step>

  <Step>
    Click "**Connect**"
  </Step>

  <Step>
    Copy the branch id
  </Step>

  <Step>
    Append it to your user with `.branch_id`
  </Step>
</Steps>

## Viewing, deleting, and renaming roles

On the roles page, you will see all roles created via the dashboard, API, and the `pscale role` CLI. However, we will not display roles created manually via `CREATE ROLE` commands.

You can rename a role by clicking the "..." button for the role on the Roles page at "Settings" > "Roles".

### Deleting a role

To delete a role, click the "..." for the role on the database Roles page at "Settings" > "Roles".

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/roles/image5.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=cea125353fc5aad2e54f0976c89a7950" alt="Rename or delete a role" data-og-width="3128" width="3128" data-og-height="1470" height="1470" data-path="docs/images/assets/docs/postgres/roles/image5.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/roles/image5.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=fe2967a92dd2563429ec5ad5a93d8027 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/roles/image5.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=1d4d1e7184db2798c0463e8814aa565e 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/roles/image5.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=4aa1f9c79c4fa6fe6c90a829a52e3908 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/roles/image5.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=522386ad984a974e88adfe1f8c5ab9d1 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/roles/image5.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=9b9c1b030ce50453a280cfa45d62f501 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/roles/image5.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=fa0b64810f8503196426b3d098f764fb 2500w" />
</Frame>

Deleting a role requires an extra step if the role has created any objects like tables or schemas, if the role has been granted any additional permissions, or if the role has created any other roles. If you try to delete a role that is still referenced, you may see this error: `Role is still referenced and cannot be dropped.`.

Such roles must designate a successor role, to which allowed objects are reassigned. Additional granted permissions are dropped as part of the transfer process. The usual successor role is `postgres`, which you can indicate in the "Delete role" modal.

You can reassign owned objects when deleting a role directly in the dashboard. When you click "Delete role", check the "**Reassign owned objects**" box on the modal.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/qADAk5S-E3Yha3-1/docs/images/assets/docs/postgres/roles/image6.png?fit=max&auto=format&n=qADAk5S-E3Yha3-1&q=85&s=984a78f6a81984aafeffd4b7dc842e88" alt="Specify a successor for a role" data-og-width="1980" width="1980" data-og-height="1620" height="1620" data-path="docs/images/assets/docs/postgres/roles/image6.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/qADAk5S-E3Yha3-1/docs/images/assets/docs/postgres/roles/image6.png?w=280&fit=max&auto=format&n=qADAk5S-E3Yha3-1&q=85&s=ee31179a7639c2692da071e9339cb3c5 280w, https://mintcdn.com/planetscale-cad1a68a/qADAk5S-E3Yha3-1/docs/images/assets/docs/postgres/roles/image6.png?w=560&fit=max&auto=format&n=qADAk5S-E3Yha3-1&q=85&s=85afb016a063fb7181eae42c3a77a8c9 560w, https://mintcdn.com/planetscale-cad1a68a/qADAk5S-E3Yha3-1/docs/images/assets/docs/postgres/roles/image6.png?w=840&fit=max&auto=format&n=qADAk5S-E3Yha3-1&q=85&s=54accbd0d5109c9c340ed04a4f32004a 840w, https://mintcdn.com/planetscale-cad1a68a/qADAk5S-E3Yha3-1/docs/images/assets/docs/postgres/roles/image6.png?w=1100&fit=max&auto=format&n=qADAk5S-E3Yha3-1&q=85&s=1abc5431b57affad58994e98fd66791a 1100w, https://mintcdn.com/planetscale-cad1a68a/qADAk5S-E3Yha3-1/docs/images/assets/docs/postgres/roles/image6.png?w=1650&fit=max&auto=format&n=qADAk5S-E3Yha3-1&q=85&s=c2a3690a5ad22b2e239febe3a9941ee1 1650w, https://mintcdn.com/planetscale-cad1a68a/qADAk5S-E3Yha3-1/docs/images/assets/docs/postgres/roles/image6.png?w=2500&fit=max&auto=format&n=qADAk5S-E3Yha3-1&q=85&s=f9c21a7dee6acbb3680800261a2ac609 2500w" />
</Frame>

You can choose successors other than `postgres`, but only by using the API or the `pscale role` CLI.  Deleting a role that owns objects, has additional permissions, or has created other roles will fail if no successor is specified.

You can [delete a role using the `pscale role` CLI](/docs/cli/role#the-delete-sub-command) with:

```bash  theme={null}
pscale role delete --org <org> <database> main <role-id> --successor postgres
```

### Reassigning objects owned by a role

If you need to transfer ownership of database objects from one role to another without deleting the role, you can reassign the objects from the dashboard or the CLI.

**In the dashboard**, click the "..." button for the role on the Roles page at "Settings" > "Roles", then select "**Reassign objects**". This will transfer all objects owned by the role to the `postgres` role. Reassigning objects to a recipient other than `postgres` is only possible through the API and the CLI.

**Using the CLI**, you can reassign objects with:

```bash  theme={null}
pscale role reassign --org <org> <database> <branch> <donor> --successor <recipient>
```

This command will prompt for confirmation before transferring ownership of all objects from the donor role to the recipient role.

### Resetting role credentials

If you need to reset a role's password, you can do so from the dashboard or the CLI.

**In the dashboard**, click the "..." button for the role on the Roles page at "Settings" > "Roles", then select "**Reset credentials**". This will generate a new password for the role.

**Using the CLI**, you can reset a role's password with:

```bash  theme={null}
pscale role reset --org <org> <database> <branch> <role-id>
```

This command will prompt for confirmation before resetting the password and return the role object with the new password.

## User-defined role examples and use cases

Understanding when to use user-defined roles versus the default role is essential for maintaining secure and maintainable database access patterns. This section provides practical examples of role configurations for common scenarios.

### When to use user-defined roles vs. the default role

**Use user-defined roles when:**

* **Connecting from application servers**: Application connections should never use the default role. This allows you to rotate the default role credentials without application downtime.
* **Principle of least privilege**: Different parts of your application or different services may need different levels of access. Create specific roles for each use case.
* **Managing team access**: Different team members may need different permissions (e.g., developers vs. data analysts vs. DBAs).
* **Integrating third-party tools**: External tools and services should have their own roles with limited permissions appropriate to their function.

**Use the default role when:**

* **Performing administrative tasks**: Creating schemas, managing database structure, or performing major database migrations.
* **Initial database setup**: Setting up the initial database structure and creating the first set of user-defined roles.

### Example role configurations

The following are some example permission configurations that you may use for user-defined roles. Your use cases may vary, but these are generic examples.

**Application read-write role**

For a typical web application that needs to read and write data, you may consider these permissions:

* `pg_read_all_data`
* `pg_write_all_data`

**Read-only analytics role**

For analytics tools or reporting dashboards that only need to query data:

* `pg_read_all_data`
* `pg_read_all_settings`
* `pg_read_all_stats`

**Monitoring and observability role**

For monitoring tools like Datadog, New Relic, or custom monitoring solutions:

* `pg_monitor`
* `pg_read_all_stats`
* `pg_read_all_settings`
* `pg_stat_scan_tables`

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale Postgres database dashboard
Source: https://planetscale.com/docs/postgres/dashboard

When you navigate to a database in your PlanetScale organization, you'll see a comprehensive view of your cluster health, performance metrics, and management options. You can filter this view by [branch](/docs/postgres/branching) by selecting from the branch dropdown at the top.

From the dashboard you can review:

* Your cluster's topology diagram
* Real-time performance metrics
* Summary and statistics
* Connection management
* Branch-specific views and controls

## Cluster topology

The cluster topology diagram provides a visual representation of your PostgreSQL database infrastructure, including:

* **Primary node**: The main database instance that handles all write operations
* **Replica nodes**: Read-only copies of your primary database for improved read performance and high availability

Each node includes information about the region, instance type, real-time resource utilization (CPU and memory percentage), and cluster size.

If you had additional replicas beyond the 2 default, you'll see them in this diagram.

## Database summary

The database summary section on the right-hand side displays key statistics about your PostgreSQL environment, including:

* **PostgreSQL version**: Shows the current PostgreSQL version (e.g., "17.4")
* **Tables**: Total number of tables across all schemas
* **Branches**: Count of database branches in your environment
* **CPU utilization**: Percentage of CPU currently used
* **Next backup**: Shows when the next scheduled backup will occur (e.g., "in 8 hours")
* **Total storage**: Amount of storage currently used

Production branches are clearly marked with visual indicators and badges to distinguish them from development branches. The summary also shows the current state and health of each branch, making it easy to assess your database environment at a glance.

## Performance metrics

The performance metrics section includes a dropdown to select different metrics and a time-series graph showing data over the selected time period. Available metrics include:

* Query latency (shown as p95, p99, p50, and p99.9 percentiles)
* Queries per second
* Rows read
* Rows written
* Query errors

You can select each metric from the dropdown to update the graph. There's also a "View all query insights" link to access more detailed query performance data.

### Slowest queries

The "Slowest queries during the last 24 hours" section at the bottom of the dashboard shows a detailed table with:

* **Query**: The actual SQL query text
* **Count**: Number of times the query was executed
* **p50 latency (ms)**: The median query execution time in milliseconds

This helps you identify performance bottlenecks and queries that may need optimization.

## Connecting to your database

The "**Connect**" button allows you to generate or reset your default credentials for your Postgres database. For more information, see the [Connecting documentation](/docs/postgres/connecting).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Development Environments
Source: https://planetscale.com/docs/postgres/development-environments

When using development environments, it's important to have a way to work with your database and test schema changes in isolation from production. We have several recommendations for how to accomplish this when using PlanetScale Postgres.

<Note>
  Schema and code changes must be deployed in the correct sequence. When adding a new column to a table along with a new application feature, you likely want to execute the schema change before deploying the corresponding code changes. In other cases, such as removing a feature and with it a table in your database, you would make your code changes first and then drop the table second.

  We recommend reviewing [this blog post](https://planetscale.com/blog/safely-making-database-schema-changes) for more information about deploying database schema changes alongside application changes.
</Note>

## Use local Postgres

Since PlanetScale uses standard PostgreSQL, one option is to use locally-running Postgres servers for all of your development work.

In this case, you would create a new local database for each branch a developer is working on. After creating the database, run your database migration scripts to populate it with your project's schema. Optionally, you could have a script to populate your tables with dummy data to improve local testing. When you are ready to commit your code changes, execute the necessary schema changes on your production branch, being [careful about ordering](https://planetscale.com/blog/safely-making-database-schema-changes).

While this is a convenient option in some cases, local databases can make it difficult for multiple developers to collaborate on shared branches, as schema changes must be manually synchronized across each developer's local environment.

## Use PlanetScale branches

PlanetScale Postgres supports [branching](/docs/postgres/branching). This allows you to spin up lightweight Postgres databases separate from production that can be used for development and testing. Creating a new branch produces a completely empty Postgres database. No schema or data is copied to the branch, unless restoring from a backup (more on this later).

Branches can be used similarly to local Postgres instances. The advantage is, since they are hosted by PlanetScale, they can be easily shared amongst development teams.

If using branches for your development environments, you would [create new database branch](/docs/postgres/branching#create-a-branch) for each code branch a developer/team is working on. You would then run your schema migrations and any data loading scripts you need to initialize the branch. When you are ready to commit your code changes, execute the necessary schema changes on your production branch, and then [delete the branch](/docs/postgres/branching#delete-a-branch).

## Use PlanetScale branches + backups

PlanetScale gives you the ability to create new branches that are [initialized from a recent backup](/docs/postgres/branching#from-a-backup). This can be used instead of empty branches to create development environments that are pre-populated with schema and data.

There are some downsides to this approach, however. This approach copies production data to development branches, which may pose security risks if your database contains sensitive customer information.

Another consideration is cost. Each branch is its own, isolated database and uses its own storage separate from production. You will be charged for the storage consumed by both your production branches and your development branches.

## Use branching + schema (coming soon)

We are working on schema-only branching. With this feature, new branches will be created with identical schema to your production database, but with no data. This will allow developers to skip the step of pushing your schema to each created branch. From here, developers can either develop and test with an empty database, or run custom scripts to populate the schema with dummy data.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Extensions
Source: https://planetscale.com/docs/postgres/extensions

PostgreSQL extensions are add-on modules that extend the functionality of your database beyond the core features. They provide additional data types, functions, operators, indexes, and other capabilities without requiring modifications to the PostgreSQL source code.

<Note>
  You can submit and vote for the extensions you want us to support next at [ps-extensions.io](https://ps-extensions.io/).
</Note>

Extensions work by packaging related database objects (functions, data types, operators, etc.) into a single installable unit. When you install an extension, PostgreSQL loads the necessary code and creates the required database objects in your database schema. This modular approach allows you to add only the functionality you need while keeping your database lightweight and maintainable.

## Types of Extensions

PostgreSQL extensions generally fall into two main categories:

### Native Extensions

These are extensions that ship with PostgreSQL itself and are maintained by the PostgreSQL core team. Examples include:

* **Data type extensions**: `citext`, `hstore`, `uuid-ossp` for specialized data types
* **Index extensions**: `btree_gin`, `btree_gist` for advanced indexing capabilities
* **Text search extensions**: `unaccent`, `pg_trgm` for full-text search enhancements
* **Utility extensions**: `pgcrypto` for cryptographic functions, `tablefunc` for table manipulation

### Community Extensions

These are developed and maintained by the community or third-party organizations. Popular examples include:

* **Vector search**: `pgvector` for AI/ML vector operations
* **Query optimization**: `pg_hint_plan` for query plan hints
* **Partitioning**: `pg_partman` for automated table partitioning

<Warning>
  Extensions that modify shared memory settings, background worker processes, or core database functionality typically require a restart. You can only enable these extensions through the [Dashboard](#configuring-extensions-in-the-dashboard).

  Most simple extensions like data types or functions can be installed without a restart.
</Warning>

## Managing Extensions

<Note>
  Some extensions may require that you use a role with superuser privileges to enable them on your PlanetScale Postgres database. See [supported extensions](#supported-extensions) for more information.
</Note>

### Installing an Extension

To install an extension in your database, use the `CREATE EXTENSION` command:

```sql  theme={null}
CREATE EXTENSION IF NOT EXISTS pgcrypto;
```

### Removing an Extension

To remove an extension from your database, use the `DROP EXTENSION` command:

```sql  theme={null}
DROP EXTENSION IF EXISTS pgcrypto;
```

### Updating an Extension

To update an extension to a newer version, use the `ALTER EXTENSION` command:

```sql  theme={null}
ALTER EXTENSION pgcrypto UPDATE;
```

### Viewing Installed Extensions

To see all extensions currently installed in your database, use this query:

```sql  theme={null}
SELECT name, default_version, installed_version, comment
FROM pg_available_extensions
WHERE installed_version IS NOT NULL
ORDER BY name;
```

To view all available extensions (both installed and available for installation):

```sql  theme={null}
SELECT name, default_version, comment
FROM pg_available_extensions
ORDER BY name;
```

### Installing Extensions in Specific Schemas

By default, extensions are installed in the `public` schema. To install an extension in a specific schema:

```sql  theme={null}
CREATE EXTENSION IF NOT EXISTS pgcrypto SCHEMA my_schema;
```

To move an existing extension to a different schema:

```sql  theme={null}
ALTER EXTENSION pgcrypto SET SCHEMA new_schema;
```

<Note>
  Not all extensions support being installed in non-public schemas. Most utility and data type extensions work fine, but some system-level extensions must remain in the public schema.
</Note>

## Configuring Extensions in the Dashboard

<img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/extensions/configure-extensions.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=302182a33fecf2d1249c09bf789417f5" alt="Configuring Extensions" data-og-width="2182" width="2182" data-og-height="1266" height="1266" data-path="docs/postgres/extensions/configure-extensions.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/extensions/configure-extensions.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=af9ce7f2b12a9e8ed1fae56f893b833a 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/extensions/configure-extensions.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=cfa0e454fb27894977c890280f89aefd 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/extensions/configure-extensions.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=f52fef0fa6a024579e06a83deab87322 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/extensions/configure-extensions.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=202621209cea2adb9f66d1dc5bc0162d 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/extensions/configure-extensions.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=cfbc152a65a3f2b142f3fa30fd09a646 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/extensions/configure-extensions.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=0ae4b1a13c7ca56fd00852e7650a0a6e 2500w" />

Some extensions require explicit configuration through the PlanetScale dashboard and may consume additional resources on your database instances. Configuring these extensions may require a database restart.
The extensions that require a restart are marked with a ✅ in the "Restart required" column. For `Production` clusters, these reboots are applied in a rolling cadence through the cluster's instances.

For detailed configuration instructions and parameter descriptions for each extension, please refer to their individual documentation pages via the 📝 links in the "Additional Notes" column.

## Troubleshooting

### Common Extension Issues

#### Extension not found

```sql  theme={null}
ERROR: extension "extension_name" is not available
```

This means the extension is not installed on the server or not supported. Check the [supported extensions list](#supported-extensions) below.

#### Permission denied

```sql  theme={null}
ERROR: permission denied to create extension "extension_name"
```

Extensions typically require database owner or superuser privileges. Contact support if you encounter permission issues.

#### Extension already exists

```sql  theme={null}
ERROR: extension "extension_name" already exists
```

Use `CREATE EXTENSION IF NOT EXISTS extension_name;` to avoid this error, or check installed extensions first.

#### Dependency conflicts

```sql  theme={null}
ERROR: required extension "dependency_name" is not installed
```

Some extensions depend on others. Install the required dependency first, then retry installing your desired extension.

## Supported Extensions

### Table Column Legend

<Note>
  Newer Postgres versions ship with newer versions of extensions. When upgrading to a newer Postgres version, make sure to test your application for compatibility with those extension versions.\*
</Note>

* **Extension**: Extension name with link to official documentation
* **Description**: Brief description of extension functionality
* **Version**: Version number available in PlanetScale (or *TBD* if placeholder)
* **Superuser required**: ⭐ = Extension requires superuser privileges to use its functionality
* **Restart required**: ✅ = Must be enabled through PlanetScale dashboard first because the extension requires shared libraries and a restart (blank = install directly with `CREATE EXTENSION`)
* **Additional Notes**: 📝 = Link to detailed configuration documentation

### Supported Native PostgreSQL Extensions

#### Postgres 18.1

| Extension                                                                                                | Description                                                                                           | Version | Superuser required | Restart required | Additional Notes                                   |
| -------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------- | ------- | ------------------ | ---------------- | -------------------------------------------------- |
| [autoinc](https://www.postgresql.org/docs/current/contrib-spi.html#CONTRIB-SPI-AUTOINC)                  | Functions for autoincrementing fields                                                                 | 1.0     | ⭐                  |                  |                                                    |
| [bloom](https://www.postgresql.org/docs/current/bloom.html)                                              | Bloom filter index access method                                                                      | 1.0     | ⭐                  |                  |                                                    |
| [btree\_gin](https://www.postgresql.org/docs/current/btree-gin.html)                                     | Provides GIN operator classes that implement B-tree equivalent behavior for various data types        | 1.3     |                    |                  |                                                    |
| [btree\_gist](https://www.postgresql.org/docs/current/btree-gist.html)                                   | Provides GiST index operator classes that implement B-tree equivalent behavior                        | 1.8     |                    |                  |                                                    |
| [citext](https://www.postgresql.org/docs/current/citext.html)                                            | Provides case-insensitive character string type                                                       | 1.8     |                    |                  |                                                    |
| [cube](https://www.postgresql.org/docs/current/cube.html)                                                | Data type for representing multidimensional cubes                                                     | 1.5     |                    |                  |                                                    |
| [dict\_int](https://www.postgresql.org/docs/current/dict-int.html)                                       | Text search dictionary template for integers                                                          | 1.0     |                    |                  |                                                    |
| [dict\_xsyn](https://www.postgresql.org/docs/current/dict-xsyn.html)                                     | Text search dictionary for extended synonym processing                                                | 1.0     | ⭐                  |                  |                                                    |
| [earthdistance](https://www.postgresql.org/docs/current/earthdistance.html)                              | Calculate great circle distances on the surface of the Earth                                          | 1.2     | ⭐                  |                  |                                                    |
| [fuzzystrmatch](https://www.postgresql.org/docs/current/fuzzystrmatch.html)                              | Functions for determining similarities and distance between strings                                   | 1.2     |                    |                  |                                                    |
| [hstore](https://www.postgresql.org/docs/current/hstore.html)                                            | Data type for storing sets of key/value pairs within a single PostgreSQL value                        | 1.8     |                    |                  |                                                    |
| [insert\_username](https://www.postgresql.org/docs/current/contrib-spi.html#CONTRIB-SPI-INSERT-USERNAME) | Functions for tracking who changed a table                                                            | 1.0     | ⭐                  |                  |                                                    |
| [intagg](https://www.postgresql.org/docs/current/intagg.html)                                            | Integer aggregator and enumerator (deprecated)                                                        | 1.1     | ⭐                  |                  |                                                    |
| [intarray](https://www.postgresql.org/docs/current/intarray.html)                                        | Functions and operators for manipulating null-free arrays of integers                                 | 1.5     |                    |                  |                                                    |
| [isn](https://www.postgresql.org/docs/current/isn.html)                                                  | Data types for international product numbering standards                                              | 1.3     |                    |                  |                                                    |
| [lo](https://www.postgresql.org/docs/current/lo.html)                                                    | Support for managing Large Objects (also called BLOBs)                                                | 1.2     |                    |                  |                                                    |
| [ltree](https://www.postgresql.org/docs/current/ltree.html)                                              | Data type for representing labels of data stored in hierarchical tree-like structures                 | 1.3     |                    |                  |                                                    |
| [moddatetime](https://www.postgresql.org/docs/current/contrib-spi.html#CONTRIB-SPI-MODDATETIME)          | Functions for tracking last modification time                                                         | 1.0     | ⭐                  |                  |                                                    |
| [pg\_stat\_statements](https://www.postgresql.org/docs/current/pgstatstatements.html)                    | Provides a means for tracking planning and execution statistics of all SQL statements                 | 1.12    | ⭐                  | ✅                | [📝](/docs/postgres/extensions/pg_stat_statements) |
| [pg\_trgm](https://www.postgresql.org/docs/current/pgtrgm.html)                                          | Functions and operators for determining the similarity of alphanumeric text based on trigram matching | 1.6     |                    |                  |                                                    |
| [pgcrypto](https://www.postgresql.org/docs/current/pgcrypto.html)                                        | Cryptographic functions for PostgreSQL                                                                | 1.4     |                    |                  |                                                    |
| [pgrowlocks](https://www.postgresql.org/docs/current/pgrowlocks.html)                                    | Show row-level locking information                                                                    | 1.2     | ⭐                  |                  |                                                    |
| [pgstattuple](https://www.postgresql.org/docs/current/pgstattuple.html)                                  | Obtain tuple-level statistics                                                                         | 1.5     | ⭐                  |                  |                                                    |
| [plpgsql](https://www.postgresql.org/docs/current/plpgsql.html)                                          | Loadable procedural language for the PostgreSQL database system                                       | 1.0     |                    |                  |                                                    |
| [postgres\_fdw](https://www.postgresql.org/docs/current/postgres-fdw.html)                               | Foreign data wrapper for remote PostgreSQL servers                                                    | 1.2     | ⭐                  |                  |                                                    |
| [seg](https://www.postgresql.org/docs/current/seg.html)                                                  | Data type for representing line segments or floating-point intervals                                  | 1.4     |                    |                  |                                                    |
| [tablefunc](https://www.postgresql.org/docs/current/tablefunc.html)                                      | Functions that return tables (multiple rows)                                                          | 1.0     |                    |                  |                                                    |
| [tcn](https://www.postgresql.org/docs/current/tcn.html)                                                  | Provides a trigger function that notifies listeners of changes to any table                           | 1.0     |                    |                  |                                                    |
| [tsm\_system\_rows](https://www.postgresql.org/docs/current/tsm-system-rows.html)                        | Table sampling method SYSTEM\_ROWS which can be used in TABLESAMPLE clause                            | 1.0     |                    |                  |                                                    |
| [tsm\_system\_time](https://www.postgresql.org/docs/current/tsm-system-time.html)                        | Table sampling method SYSTEM\_TIME which can be used in TABLESAMPLE clause                            | 1.0     |                    |                  |                                                    |
| [unaccent](https://www.postgresql.org/docs/current/unaccent.html)                                        | Text search dictionary for removing accents (diacritic signs) from lexemes                            | 1.1     |                    |                  |                                                    |
| [uuid-ossp](https://www.postgresql.org/docs/current/uuid-ossp.html)                                      | Functions to generate universally unique identifiers (UUIDs)                                          | 1.1     |                    |                  |                                                    |

#### Postgres 17.5

| Extension                                                                                                | Description                                                                                           | Version | Superuser required | Restart required | Additional Notes                                   |
| -------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------- | ------- | ------------------ | ---------------- | -------------------------------------------------- |
| [autoinc](https://www.postgresql.org/docs/current/contrib-spi.html#CONTRIB-SPI-AUTOINC)                  | Functions for autoincrementing fields                                                                 | 1.0     | ⭐                  |                  |                                                    |
| [bloom](https://www.postgresql.org/docs/current/bloom.html)                                              | Bloom filter index access method                                                                      | 1.0     | ⭐                  |                  |                                                    |
| [btree\_gin](https://www.postgresql.org/docs/current/btree-gin.html)                                     | Provides GIN operator classes that implement B-tree equivalent behavior for various data types        | 1.3     |                    |                  |                                                    |
| [btree\_gist](https://www.postgresql.org/docs/current/btree-gist.html)                                   | Provides GiST index operator classes that implement B-tree equivalent behavior                        | 1.7     |                    |                  |                                                    |
| [citext](https://www.postgresql.org/docs/current/citext.html)                                            | Provides case-insensitive character string type                                                       | 1.6     |                    |                  |                                                    |
| [cube](https://www.postgresql.org/docs/current/cube.html)                                                | Data type for representing multidimensional cubes                                                     | 1.5     |                    |                  |                                                    |
| [dict\_int](https://www.postgresql.org/docs/current/dict-int.html)                                       | Text search dictionary template for integers                                                          | 1.0     |                    |                  |                                                    |
| [dict\_xsyn](https://www.postgresql.org/docs/current/dict-xsyn.html)                                     | Text search dictionary for extended synonym processing                                                | 1.0     | ⭐                  |                  |                                                    |
| [earthdistance](https://www.postgresql.org/docs/current/earthdistance.html)                              | Calculate great circle distances on the surface of the Earth                                          | 1.2     | ⭐                  |                  |                                                    |
| [fuzzystrmatch](https://www.postgresql.org/docs/current/fuzzystrmatch.html)                              | Functions for determining similarities and distance between strings                                   | 1.2     |                    |                  |                                                    |
| [hstore](https://www.postgresql.org/docs/current/hstore.html)                                            | Data type for storing sets of key/value pairs within a single PostgreSQL value                        | 1.8     |                    |                  |                                                    |
| [insert\_username](https://www.postgresql.org/docs/current/contrib-spi.html#CONTRIB-SPI-INSERT-USERNAME) | Functions for tracking who changed a table                                                            | 1.0     | ⭐                  |                  |                                                    |
| [intagg](https://www.postgresql.org/docs/current/intagg.html)                                            | Integer aggregator and enumerator (deprecated)                                                        | 1.1     | ⭐                  |                  |                                                    |
| [intarray](https://www.postgresql.org/docs/current/intarray.html)                                        | Functions and operators for manipulating null-free arrays of integers                                 | 1.5     |                    |                  |                                                    |
| [isn](https://www.postgresql.org/docs/current/isn.html)                                                  | Data types for international product numbering standards                                              | 1.2     |                    |                  |                                                    |
| [lo](https://www.postgresql.org/docs/current/lo.html)                                                    | Support for managing Large Objects (also called BLOBs)                                                | 1.1     |                    |                  |                                                    |
| [ltree](https://www.postgresql.org/docs/current/ltree.html)                                              | Data type for representing labels of data stored in hierarchical tree-like structures                 | 1.3     |                    |                  |                                                    |
| [moddatetime](https://www.postgresql.org/docs/current/contrib-spi.html#CONTRIB-SPI-MODDATETIME)          | Functions for tracking last modification time                                                         | 1.0     | ⭐                  |                  |                                                    |
| [pg\_stat\_statements](https://www.postgresql.org/docs/current/pgstatstatements.html)                    | Provides a means for tracking planning and execution statistics of all SQL statements                 | 1.11    | ⭐                  | ✅                | [📝](/docs/postgres/extensions/pg_stat_statements) |
| [pg\_trgm](https://www.postgresql.org/docs/current/pgtrgm.html)                                          | Functions and operators for determining the similarity of alphanumeric text based on trigram matching | 1.6     |                    |                  |                                                    |
| [pgcrypto](https://www.postgresql.org/docs/current/pgcrypto.html)                                        | Cryptographic functions for PostgreSQL                                                                | 1.3     |                    |                  |                                                    |
| [pgrowlocks](https://www.postgresql.org/docs/current/pgrowlocks.html)                                    | Show row-level locking information                                                                    | 1.2     | ⭐                  |                  |                                                    |
| [pgstattuple](https://www.postgresql.org/docs/current/pgstattuple.html)                                  | Obtain tuple-level statistics                                                                         | 1.5     | ⭐                  |                  |                                                    |
| [plpgsql](https://www.postgresql.org/docs/current/plpgsql.html)                                          | Loadable procedural language for the PostgreSQL database system                                       | 1.0     |                    |                  |                                                    |
| [postgres\_fdw](https://www.postgresql.org/docs/current/postgres-fdw.html)                               | Foreign data wrapper for remote PostgreSQL servers                                                    | 1.1     | ⭐                  |                  |                                                    |
| [seg](https://www.postgresql.org/docs/current/seg.html)                                                  | Data type for representing line segments or floating-point intervals                                  | 1.4     |                    |                  |                                                    |
| [tablefunc](https://www.postgresql.org/docs/current/tablefunc.html)                                      | Functions that return tables (multiple rows)                                                          | 1.0     |                    |                  |                                                    |
| [tcn](https://www.postgresql.org/docs/current/tcn.html)                                                  | Provides a trigger function that notifies listeners of changes to any table                           | 1.0     |                    |                  |                                                    |
| [tsm\_system\_rows](https://www.postgresql.org/docs/current/tsm-system-rows.html)                        | Table sampling method SYSTEM\_ROWS which can be used in TABLESAMPLE clause                            | 1.0     |                    |                  |                                                    |
| [tsm\_system\_time](https://www.postgresql.org/docs/current/tsm-system-time.html)                        | Table sampling method SYSTEM\_TIME which can be used in TABLESAMPLE clause                            | 1.0     |                    |                  |                                                    |
| [unaccent](https://www.postgresql.org/docs/current/unaccent.html)                                        | Text search dictionary for removing accents (diacritic signs) from lexemes                            | 1.1     |                    |                  |                                                    |
| [uuid-ossp](https://www.postgresql.org/docs/current/uuid-ossp.html)                                      | Functions to generate universally unique identifiers (UUIDs)                                          | 1.1     |                    |                  |                                                    |

### Supported Community Extensions

#### Postgres 18.1

| Extension                                                                                               | Description                                                                                                                                                                                              | Version | Superuser required | Restart required | Additional Notes                               |
| ------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------------ | ---------------- | ---------------------------------------------- |
| [address\_standardizer](https://postgis.net/docs/manual-3.5/Extras.html#Address_Standardizer)           | Used to parse an address into constituent elements. Generally used to support geocoding address normalization step. (Part of PostGIS)                                                                    | 3.6.1   | ⭐                  |                  |                                                |
| [address\_standardizer\_data\_us](https://postgis.net/docs/manual-3.5/Extras.html#Address_Standardizer) | Address Standardizer US dataset example. (Part of PostGIS)                                                                                                                                               | 3.6.1   | ⭐                  |                  |                                                |
| [hypopg](https://github.com/HypoPG/hypopg) (installed by default)                                       | Hypothetical indexes - allows testing the impact of indexes without actually creating them                                                                                                               | 1.4.2   |                    |                  |                                                |
| [pg\_cron](https://github.com/citusdata/pg_cron)                                                        | Simple cron-based job scheduler for PostgreSQL that allows you to run SQL commands on a schedule                                                                                                         | 1.6.7   | ⭐                  | ✅                | [📝](/docs/postgres/extensions/pg_cron)        |
| [pg\_duckdb](https://github.com/duckdb/pg_duckdb)                                                       | Embeds DuckDB's analytical database engine directly into PostgreSQL for high-performance analytical queries                                                                                              | 1.1.0   | ⭐                  | ✅                | [📝](/docs/postgres/extensions/pg_duckdb)      |
| [pg\_hint\_plan](https://github.com/ossc-db/pg_hint_plan)                                               | Provides query plan hints to control the PostgreSQL planner                                                                                                                                              | 1.8.0   | ⭐                  | ✅                | [📝](/docs/postgres/extensions/pg_hint_plan)   |
| [pg\_partman](https://github.com/pgpartman/pg_partman)                                                  | Extension to create and manage both time-based and serial-based table partition sets                                                                                                                     | 5.3.1   |                    |                  |                                                |
| [pg\_partman\_bgw](https://github.com/pgpartman/pg_partman)                                             | Background worker to automatically run partition maintenance for pg\_partman                                                                                                                             | 5.3.1   |                    | ✅                | [📝](/docs/postgres/extensions/pg_partman_bgw) |
| [pg\_squeeze](https://github.com/cybertec-postgresql/pg_squeeze/)                                       | Automatically cleans up unused space in tables (table bloat)                                                                                                                                             | 1.9.1   | ⭐                  | ✅                | [📝](/docs/postgres/extensions/pg_squeeze)     |
| [pgvector](https://github.com/pgvector/pgvector)                                                        | Open-source vector similarity search for PostgreSQL, designed for AI/ML applications                                                                                                                     | 0.8.1   | ⭐                  |                  |                                                |
| [postgis](https://postgis.net/)                                                                         | PostGIS extends the capabilities of the PostgreSQL relational database by adding support for storing, indexing, and querying geospatial data                                                             | 3.6.1   | ⭐                  |                  |                                                |
| [postgis\_sfcgal](https://postgis.net/docs/manual-3.5/reference_sfcgal.html)                            | SFCGAL is a C++ wrapper library around CGAL that provides advanced 2D and 3D spatial functions                                                                                                           | 3.6.1   | ⭐                  |                  |                                                |
| [postgis\_tiger\_geocoder](https://postgis.net/docs/manual-3.5/Extras.html#Tiger_Geocoder)              | A plpgsql based geocoder written to work with the TIGER (Topologically Integrated Geographic Encoding and Referencing system) / Line and Master Address database export released by the US Census Bureau | 3.6.1   |                    |                  |                                                |
| [postgis\_topology](https://postgis.net/docs/manual-3.5/Topology.html)                                  | The PostGIS Topology types and functions are used to manage topological objects such as faces, edges and nodes                                                                                           | 3.6.1   | ⭐                  |                  |                                                |
| [TimescaleDB](https://github.com/timescale/timescaledb)                                                 | A time-series database for high-performance real-time analytics packaged as a Postgres extension ([Apache 2 Edition](https://docs.tigerdata.com/about/latest/timescaledb-editions/))                     | 2.23.1  |                    | ✅                | [📝](/docs/postgres/extensions/timescaledb)    |

#### Postgres 17.5

| Extension                                                                                               | Description                                                                                                                                                                                              | Version | Superuser required | Restart required | Additional Notes                               |
| ------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------- | ------------------ | ---------------- | ---------------------------------------------- |
| [address\_standardizer](https://postgis.net/docs/manual-3.5/Extras.html#Address_Standardizer)           | Used to parse an address into constituent elements. Generally used to support geocoding address normalization step. (Part of PostGIS)                                                                    | 3.5.3   | ⭐                  |                  |                                                |
| [address\_standardizer\_data\_us](https://postgis.net/docs/manual-3.5/Extras.html#Address_Standardizer) | Address Standardizer US dataset example. (Part of PostGIS)                                                                                                                                               | 3.5.3   | ⭐                  |                  |                                                |
| [hypopg](https://github.com/HypoPG/hypopg) (installed by default)                                       | Hypothetical indexes - allows testing the impact of indexes without actually creating them                                                                                                               | 1.4.2   |                    |                  |                                                |
| [pg\_cron](https://github.com/citusdata/pg_cron)                                                        | Simple cron-based job scheduler for PostgreSQL that allows you to run SQL commands on a schedule                                                                                                         | 1.6.5   | ⭐                  | ✅                | [📝](/docs/postgres/extensions/pg_cron)        |
| [pg\_duckdb](https://github.com/duckdb/pg_duckdb)                                                       | Embeds DuckDB's analytical database engine directly into PostgreSQL for high-performance analytical queries                                                                                              | 1.0.0   | ⭐                  | ✅                | [📝](/docs/postgres/extensions/pg_duckdb)      |
| [pg\_hint\_plan](https://github.com/ossc-db/pg_hint_plan)                                               | Provides query plan hints to control the PostgreSQL planner                                                                                                                                              | 1.7.0   | ⭐                  | ✅                | [📝](/docs/postgres/extensions/pg_hint_plan)   |
| [pg\_partman](https://github.com/pgpartman/pg_partman)                                                  | Extension to create and manage both time-based and serial-based table partition sets                                                                                                                     | 5.2.4   |                    |                  |                                                |
| [pg\_partman\_bgw](https://github.com/pgpartman/pg_partman)                                             | Background worker to automatically run partition maintenance for pg\_partman                                                                                                                             | 5.2.4   |                    | ✅                | [📝](/docs/postgres/extensions/pg_partman_bgw) |
| [pg\_squeeze](https://github.com/cybertec-postgresql/pg_squeeze/)                                       | Automatically cleans up unused space in tables (table bloat)                                                                                                                                             | 1.9.0   | ⭐                  | ✅                | [📝](/docs/postgres/extensions/pg_squeeze)     |
| [pgvector](https://github.com/pgvector/pgvector)                                                        | Open-source vector similarity search for PostgreSQL, designed for AI/ML applications                                                                                                                     | 0.8.0   | ⭐                  |                  |                                                |
| [postgis](https://postgis.net/)                                                                         | PostGIS extends the capabilities of the PostgreSQL relational database by adding support for storing, indexing, and querying geospatial data                                                             | 3.5.3   | ⭐                  |                  |                                                |
| [postgis\_sfcgal](https://postgis.net/docs/manual-3.5/reference_sfcgal.html)                            | SFCGAL is a C++ wrapper library around CGAL that provides advanced 2D and 3D spatial functions                                                                                                           | 3.5.3   | ⭐                  |                  |                                                |
| [postgis\_tiger\_geocoder](https://postgis.net/docs/manual-3.5/Extras.html#Tiger_Geocoder)              | A plpgsql based geocoder written to work with the TIGER (Topologically Integrated Geographic Encoding and Referencing system) / Line and Master Address database export released by the US Census Bureau | 3.5.3   |                    |                  |                                                |
| [postgis\_topology](https://postgis.net/docs/manual-3.5/Topology.html)                                  | The PostGIS Topology types and functions are used to manage topological objects such as faces, edges and nodes                                                                                           | 3.5.3   | ⭐                  |                  |                                                |
| [TimescaleDB](https://github.com/timescale/timescaledb)                                                 | A time-series database for high-performance real-time analytics packaged as a Postgres extension ([Apache 2 Edition](https://docs.tigerdata.com/about/latest/timescaledb-editions/))                     | 2.21.3  |                    | ✅                | [📝](/docs/postgres/extensions/timescaledb)    |

### PlanetScale Extensions

These extensions are installed by PlanetScale and cannot be disabled.

| Extension         | Description                                                           | Version | Additional Notes                           |
| ----------------- | --------------------------------------------------------------------- | ------- | ------------------------------------------ |
| pgextwlist        | PostgreSQL Extension Allowlist                                        | N/A     |                                            |
| pginsights        | PlanetScale Insights plugin                                           | N/A     | [📝](/docs/postgres/extensions/pginsights) |
| pg\_pscale\_utils | Utility extension to handle superuser actions in PlanetScale Postgres | N/A     |                                            |

## Need Additional Extensions?

If you require an extension that is not currently supported, please [reach out to support](https://planetscale.com/contact?initial=support). We regularly evaluate and add new extensions based on customer needs and security considerations.


# Extensions: pg_cron
Source: https://planetscale.com/docs/postgres/extensions/pg_cron

pg_cron is a simple cron-based job scheduler for PostgreSQL. It allows you to run SQL commands on a schedule, similar to how cron jobs work in Unix-like systems.

## Dashboard Configuration

This extension requires activation via the PlanetScale Dashboard before it can be used. It must be enabled through shared libraries and requires a database restart.

To enable pg\_cron:

<Steps>
  <Step>From the PlanetScale organization dashboard, select the desired database</Step>
  <Step>Navigate to the **Clusters** page from the menu on the left</Step>
  <Step>Choose the branch whose extensions you'd like to configure in the "**Branch**" dropdown</Step>
  <Step>Select the **Extensions** tab</Step>
  <Step>Enable pg\_cron and configure its parameters</Step>
  <Step>Click **Queue extension changes** to apply the configuration</Step>
  <Step>Once you're ready to apply the changes, click "**Apply changes**"</Step>
</Steps>

## Parameters

### cron.database\_name

* **Type**: String
* **Default**: `postgres`
* **Description**: Name of the database where pg\_cron is installed (via CREATE EXTENSION).

### cron.launch\_active\_jobs

* **Type**: Boolean
* **Default**: `true`
* **Description**: Switch to enable/disable running cron jobs - applies to all jobs.

### cron.log\_min\_messages

* **Type**: Select
* **Options**: error, warning, notice, info, log, debug
* **Default**: `warning`
* **Description**: Lowest severity messages to log from the launcher background worker.

### cron.log\_run

* **Type**: Boolean
* **Default**: `true`
* **Description**: Log all cron runs in the cron.job\_run\_details table.

### cron.log\_statement

* **Type**: Boolean
* **Default**: `true`
* **Description**: Log all cron statements before running them.

### cron.max\_running\_jobs

* **Type**: Integer
* **Default**: `1`
* **Minimum**: `1`
* **Description**: Maximum number of jobs that can run at once. Must be less than or equal to cluster-level max\_worker\_processes.

## Usage

After enabling the extension through the dashboard, you can install it in your database:

```sql  theme={null}
CREATE EXTENSION IF NOT EXISTS pg_cron;
```

Then you can schedule jobs using the `cron.schedule` function:

```sql  theme={null}
-- Schedule a job to run every minute
SELECT cron.schedule('job-name', '* * * * *', 'SELECT 1;');
```

## External Documentation

For more detailed information about pg\_cron usage and functionality, see the [official pg\_cron documentation](https://github.com/citusdata/pg_cron).


# Extensions: pg_duckdb
Source: https://planetscale.com/docs/postgres/extensions/pg_duckdb

pg_duckdb is a Postgres extension that embeds DuckDB, a high-performance analytical database, directly into Postgres.

<Warning>
  We don't recommend running `pg_duckdb` directly on your PlanetScale Postgres database as it can consume significant resources during analytical queries. If you want to use DuckDB for analytics, we recommend using [MotherDuck](https://motherduck.com/) to host your analytical workloads separately.
</Warning>

## Dashboard Configuration

This extension requires activation via the PlanetScale dashboard before it can be used. It must be enabled through shared libraries and requires a database restart.

To enable pg\_duckdb:

<Steps>
  <Step>From the PlanetScale organization dashboard, select the desired database</Step>
  <Step>Navigate to the **Clusters** page from the menu on the left</Step>
  <Step>Choose the branch whose extensions you'd like to configure in the "**Branch**" dropdown</Step>
  <Step>Select the **Extensions** tab</Step>
  <Step>Enable `pg_duckdb` and configure its parameters</Step>
  <Step>Click **Queue extension changes** to apply the configuration</Step>
  <Step>Once you're ready to apply the changes, click "**Apply changes**"</Step>
</Steps>

## Parameters

### duckdb.postgres\_role

* **Type**: String
* **Default**: `pscale_superuser`
* **Description**: Specifies the Postgres role that is allowed to use DuckDB execution and manage secrets.

### duckdb.memory\_limit

* **Type**: Integer
* **Default**: 0
* **Description**: Maximum memory DuckDB can use per connection in megabytes. Setting to 0 activates DuckDB's default (80% of available RAM).

## Usage

After enabling the extension through the dashboard, you can install it in your database:

```sql  theme={null}
CREATE EXTENSION IF NOT EXISTS pg_duckdb;
```

Once installed, you can use DuckDB's analytical capabilities directly from PostgreSQL. For example:

```sql  theme={null}
-- Query PostgreSQL tables using DuckDB's analytical engine
SELECT * FROM duckdb.read_parquet('s3://bucket/data.parquet');
```

## External Documentation

For more detailed information about `pg_duckdb` usage and functionality, see the [official `pg_duckdb` documentation](https://github.com/duckdb/pg_duckdb).


# Extensions: pg_hint_plan
Source: https://planetscale.com/docs/postgres/extensions/pg_hint_plan

The pg_hint_plan extension allows you to influence the query planner's decisions by providing hints. It can be used to optimize query performance by guiding the planner towards more efficient execution plans.

## Dashboard Configuration

The pg\_hint\_plan extension requires activation via the PlanetScale Dashboard before it can be used. It must be enabled through shared libraries and requires a database restart.

To enable pg\_hint\_plan:

<Steps>
  <Step>From the PlanetScale organization dashboard, select the desired database</Step>
  <Step>Navigate to the **Clusters** page from the menu on the left</Step>
  <Step>Choose the branch whose extensions you'd like to configure in the "**Branch**" dropdown</Step>
  <Step>Select the **Extensions** tab</Step>
  <Step>Enable pg\_hint\_plan and configure its parameters</Step>
  <Step>Click **Queue extension changes** to apply the configuration</Step>
  <Step>Once you're ready to apply the changes, click "**Apply changes**"</Step>
</Steps>

## Parameters

### pg\_hint\_plan.enable\_hint

* **Type**: Boolean
* **Default**: `true`
* **Description**: Enable/disable hint functionality.

### pg\_hint\_plan.enable\_hint\_table

* **Type**: Boolean
* **Default**: `false`
* **Description**: Enable hint table functionality.

### pg\_hint\_plan.parse\_messages

* **Type**: Select
* **Options**: error, warning, notice, info, log, debug
* **Default**: `info`
* **Description**: Control parsing message output.

### pg\_hint\_plan.debug\_print

* **Type**: Select
* **Options**: off, on, detailed, verbose
* **Default**: `off`
* **Description**: Enable debug output.

### pg\_hint\_plan.message\_level

* **Type**: Select
* **Options**: error, warning, notice, info, log, debug
* **Default**: `info`
* **Description**: Set message verbosity level.

## Usage

<Note>
  Unlike most PostgreSQL extensions, `pg_hint_plan` does not require `CREATE EXTENSION` to function. Once enabled
  through the dashboard, it's automatically loaded and available for use. You only need to run `CREATE EXTENSION` if you
  plan to use the hint table functionality (when `pg_hint_plan.enable_hint_table` is enabled).
</Note>

After enabling the extension through the dashboard, you can optionally install it in your database for hint table functionality:

```sql  theme={null}
-- Only required if using hint tables
CREATE EXTENSION IF NOT EXISTS pg_hint_plan;
```

Then you can use hints in your queries:

```sql  theme={null}
/*+
    SeqScan(t1)
    IndexScan(t2 t2_pkey)
*/
SELECT * FROM table1 t1 JOIN table2 t2 ON t1.id = t2.id;
```

## External Documentation

For more detailed information about pg\_hint\_plan usage and hint syntax, see the [official pg\_hint\_plan documentation](https://github.com/ossc-db/pg_hint_plan).


# Extensions: pg_partman_bgw
Source: https://planetscale.com/docs/postgres/extensions/pg_partman_bgw

The pg_partman_bgw extension is a background worker for managing partitioned tables in PostgreSQL. It automates the creation and maintenance of partitioned tables, making it easier to manage large datasets.

## Dashboard Configuration

This extension requires activation via the PlanetScale Dashboard before it can be used. It must be enabled through shared libraries and requires a database restart.

To enable pg\_partman\_bgw:

<Steps>
  <Step>From the PlanetScale organization dashboard, select the desired database</Step>
  <Step>Navigate to the **Clusters** page from the menu on the left</Step>
  <Step>Choose the branch whose extensions you'd like to configure in the "**Branch**" dropdown</Step>
  <Step>Select the **Extensions** tab</Step>
  <Step>Enable pg\_partman\_bgw and configure its parameters</Step>
  <Step>Click **Queue extension changes** to apply the configuration</Step>
  <Step>Once you're ready to apply the changes, click "**Apply changes**"</Step>
</Steps>

## Parameters

### pg\_partman\_bgw\.interval

* **Type**: Integer
* **Default**: `3600`
* **Description**: Interval (in seconds) between partition maintenance runs.

### pg\_partman\_bgw\.dbname

* **Type**: String
* **Default**: `postgres`
* **Description**: Database name where the background worker should connect.

### pg\_partman\_bgw\.role

* **Type**: String
* **Default**: `postgres`
* **Description**: Role that the background worker should use when connecting.

### pg\_partman\_bgw\.analyze

* **Type**: Boolean
* **Default**: `false`
* **Description**: Whether to run ANALYZE on newly created partitions.

### pg\_partman\_bgw\.jobmon

* **Type**: Boolean
* **Default**: `true`
* **Description**: Enable job monitoring for partition maintenance activities.

## Usage

After enabling the extension through the dashboard, the background worker will automatically run partition maintenance based on your pg\_partman configuration. You'll also need to install the pg\_partman extension:

```sql  theme={null}
CREATE EXTENSION IF NOT EXISTS pg_partman;
```

The background worker works in conjunction with pg\_partman to automatically maintain your partitioned tables.

## External Documentation

For more detailed information about pg\_partman and pg\_partman\_bgw usage, see the [official pg\_partman documentation](https://github.com/pgpartman/pg_partman).


# Extensions: pg_squeeze
Source: https://planetscale.com/docs/postgres/extensions/pg_squeeze

pg_squeeze automatically cleans up unused space in tables (table bloat). pg_squeeze helps maintain optimal table performance by removing dead space that accumulates over time from UPDATE and DELETE operations.

## Dashboard Configuration

This extension requires activation via the PlanetScale Dashboard before it can be used. It must be enabled through shared libraries and requires a database restart.

To enable pg\_squeeze:

<Steps>
  <Step>From the PlanetScale organization dashboard, select the desired database</Step>
  <Step>Navigate to the **Clusters** page from the menu on the left</Step>
  <Step>Choose the branch whose extensions you'd like to configure in the "**Branch**" dropdown</Step>
  <Step>Select the **Extensions** tab</Step>
  <Step>Enable pg\_squeeze and configure its parameters</Step>
  <Step>Click **Queue extension changes** to apply the configuration</Step>
  <Step>Once you're ready to apply the changes, click "**Apply changes**"</Step>
</Steps>

## Parameters

### squeeze.max\_xlock\_time

* **Type**: Integer (milliseconds)
* **Default**: `0`
* **Minimum**: `0`
* **Description**: The maximum time the processed table may be locked exclusively.

### squeeze.worker\_autostart

* **Type**: String
* **Default**: `postgres`
* **Description**: Space-separated list of databases to start background workers for automatically.

### squeeze.workers\_per\_database

* **Type**: Integer
* **Default**: `1`
* **Minimum**: `1`
* **Description**: Maximum number of worker processes launched per database. Must be less than or equal to cluster-level max\_worker\_processes.

## Usage

After enabling the extension through the dashboard, you can install it in your database:

```sql  theme={null}
CREATE EXTENSION IF NOT EXISTS pg_squeeze;
```

Then you can schedule tables for squeezing:

```sql  theme={null}
-- Schedule a table to be squeezed
SELECT squeeze.squeeze_table('public', 'your_table_name');
```

## External Documentation

For more detailed information about pg\_squeeze usage and configuration, see the [official pg\_squeeze documentation](https://github.com/cybertec-postgresql/pg_squeeze/).


# Extensions: pg_stat_statements
Source: https://planetscale.com/docs/postgres/extensions/pg_stat_statements

pg_state_statements is a PostgreSQL extension that tracks execution statistics of all SQL statements executed by a server. It provides insights into query performance, allowing you to identify slow queries and optimize them

## Dashboard Configuration

This extension requires activation via the PlanetScale Dashboard before it can be used. It must be enabled through shared libraries and requires a database restart.

To enable pg\_stat\_statements:

<Steps>
  <Step>From the PlanetScale organization dashboard, select the desired database</Step>
  <Step>Navigate to the **Clusters** page from the menu on the left</Step>
  <Step>Choose the branch whose extensions you'd like to configure in the "**Branch**" dropdown</Step>
  <Step>Select the **Extensions** tab</Step>
  <Step>Enable pg\_stat\_statements and configure its parameters</Step>
  <Step>Click **Queue extension changes** to apply the configuration</Step>
  <Step>Once you're ready to apply the changes, click "**Apply changes**"</Step>
</Steps>

## Parameters

### pg\_stat\_statements.max

* **Type**: Integer
* **Default**: `5000`
* **Description**: Maximum number of statements tracked by the extension.

### pg\_stat\_statements.track

* **Type**: Select
* **Options**: top, all
* **Default**: `top`
* **Description**: Which statements to track. 'top' tracks top-level statements only, 'all' tracks all statements including nested ones.

### pg\_stat\_statements.track\_utility

* **Type**: Boolean
* **Default**: `true`
* **Description**: Whether to track utility statements (such as PREPARE, EXPLAIN, etc.).

### pg\_stat\_statements.track\_planning

* **Type**: Boolean
* **Default**: `false`
* **Description**: Whether to track planning statistics for statements.

### pg\_stat\_statements.save

* **Type**: Boolean
* **Default**: `true`
* **Description**: Whether to save statement statistics across server restarts.

## Usage

After enabling the extension through the dashboard, you can install it in your database:

```sql  theme={null}
CREATE EXTENSION IF NOT EXISTS pg_stat_statements;
```

Then you can query the statistics view:

```sql  theme={null}
-- View the most time-consuming queries
SELECT
    query,
    calls,
    total_exec_time,
    mean_exec_time,
    rows
FROM pg_stat_statements
ORDER BY total_exec_time DESC
LIMIT 10;
```

## External Documentation

For more detailed information about pg\_stat\_statements usage and the available statistics, see the [official PostgreSQL documentation](https://www.postgresql.org/docs/current/pgstatstatements.html).


# Extensions: pginsights
Source: https://planetscale.com/docs/postgres/extensions/pginsights

The pginsights extension is a PlanetScale Insights plugin that provides query performance monitoring and analysis capabilities. This extension is always enabled for PlanetScale databases and integrates with the Query Insights feature.

## Dashboard Configuration

This extension is always enabled for PlanetScale databases and requires activation via the PlanetScale Dashboard. While the extension itself is always active, some of its parameters can be configured through the dashboard.

To configure pginsights parameters:

<Steps>
  <Step>From the PlanetScale organization dashboard, select the desired database</Step>
  <Step>Navigate to the **Clusters** page from the menu on the left</Step>
  <Step>Choose the branch whose extensions you'd like to configure in the "**Branch**" dropdown</Step>
  <Step>Select the **Extensions** tab</Step>
  <Step>Enable pginsights and configure its parameters</Step>
  <Step>Click **Queue extension changes** to apply the configuration</Step>
  <Step>Once you're ready to apply the changes, click "**Apply changes**"</Step>
</Steps>

## Parameters

### pginsights.raw\_queries

* **Type**: Boolean
* **Default**: `false`
* **Description**: Send full query text for slow, large, or failed queries (may include sensitive data).
* **Documentation**: [Raw Query Collection](/docs/postgres/monitoring/query-insights#raw-query-collection)

### pginsights.normalize\_schema\_names

* **Type**: Boolean
* **Default**: `false`
* **Description**: Merge queries patterns that differ only by schema name.
* **Documentation**: [Schema Name Normalization](/docs/postgres/monitoring/query-insights#schema-name-normalization)

## Usage

The pginsights extension is automatically installed and enabled for all PlanetScale databases. You don't need to manually create the extension - it's always active and collecting query insights.

The extension integrates with PlanetScale's Query Insights dashboard to provide:

* Query performance metrics
* Slow query identification
* Query pattern analysis
* Resource usage tracking

## External Documentation

For more information about Query Insights and how to use the collected data, see the [Query Insights documentation](/docs/postgres/monitoring/query-insights).


# Extensions: TimescaleDB
Source: https://planetscale.com/docs/postgres/extensions/timescaledb

TimescaleDB is a time-series database for high-performance real-time analytics packaged as a Postgres extension.

<Note>
  Only features from the [Apache 2 Edition](https://docs.tigerdata.com/about/latest/timescaledb-editions/) of TimescaleDB are supported.
</Note>

## Dashboard Configuration

This extension requires activation via the PlanetScale Dashboard before it can be used. It must be enabled through shared libraries and requires a database restart.

To enable TimescaleDB:

<Steps>
  <Step>From the PlanetScale organization dashboard, select the desired database</Step>
  <Step>Navigate to the **Clusters** page from the menu on the left</Step>
  <Step>Choose the branch whose extensions you'd like to configure in the "**Branch**" dropdown</Step>
  <Step>Select the **Extensions** tab</Step>
  <Step>Enable timescaledb and configure its parameters</Step>
  <Step>Click **Queue extension changes** to apply the configuration</Step>
  <Step>Once you're ready to apply the changes, click "**Apply changes**"</Step>
</Steps>

## Usage

After enabling the extension through the dashboard, you'll be able to install it in your Postgres cluster:

```sql  theme={null}
CREATE EXTENSION IF NOT EXISTS timescaledb;
```

## External Documentation

For more detailed information about TimescaleDB usage, see the [official documentation](https://github.com/timescale/timescaledb).

## Parameters

### timescaledb.enable\_chunk\_append

* **Type**: Boolean
* **Default**: `true`
* **Description**: Enable chunk append node

### timescaledb.enable\_chunk\_skipping

* **Type**: Boolean
* **Default**: `false`
* **Description**: Enable chunk skipping functionality

### timescaledb.enable\_constraint\_aware\_append

* **Type**: Boolean
* **Default**: `true`
* **Description**: Enable constraint-aware append scans

### timescaledb.enable\_constraint\_exclusion

* **Type**: Boolean
* **Default**: `true`
* **Description**: Enable constraint exclusion

### timescaledb.enable\_custom\_hashagg

* **Type**: Boolean
* **Default**: `false`
* **Description**: Enable custom hash aggregation

### timescaledb.enable\_deprecation\_warnings

* **Type**: Boolean
* **Default**: `true`
* **Description**: Enable warnings when using deprecated functionality

### timescaledb.enable\_event\_triggers

* **Type**: Boolean
* **Default**: `false`
* **Description**: Enable event triggers for chunks creation

### timescaledb.enable\_foreign\_key\_propagation

* **Type**: Boolean
* **Default**: `true`
* **Description**: Enable foreign key propagation

### timescaledb.enable\_job\_execution\_logging

* **Type**: Boolean
* **Default**: `false`
* **Description**: Enable job execution logging

### timescaledb.enable\_now\_constify

* **Type**: Boolean
* **Default**: `true`
* **Description**: Enable now() constify

### timescaledb.enable\_optimizations

* **Type**: Boolean
* **Default**: `true`
* **Description**: Enable TimescaleDB query optimizations

### timescaledb.enable\_ordered\_append

* **Type**: Boolean
* **Default**: `true`
* **Description**: Enable ordered append scans

### timescaledb.enable\_parallel\_chunk\_append

* **Type**: Boolean
* **Default**: `true`
* **Description**: Enable parallel chunk append node

### timescaledb.enable\_qual\_propagation

* **Type**: Boolean
* **Default**: `true`
* **Description**: Enable qualifier propagation

### timescaledb.enable\_runtime\_exclusion

* **Type**: Boolean
* **Default**: `true`
* **Description**: Enable runtime chunk exclusion

### timescaledb.enable\_tiered\_reads

* **Type**: Boolean
* **Default**: `true`
* **Description**: Enable tiered data reads

### timescaledb.enable\_tss\_callbacks

* **Type**: Boolean
* **Default**: `true`
* **Description**: Enable ts\_stat\_statements callbacks

### timescaledb.max\_cached\_chunks\_per\_hypertable

* **Type**: Integer
* **Default**: `1024`
* **Min**: `0`
* **Max**: `65536`
* **Description**: Maximum cached chunks

### timescaledb.max\_open\_chunks\_per\_insert

* **Type**: Integer
* **Default**: `1024`
* **Min**: `0`
* **Max**: `32767`
* **Description**: Maximum open chunks per insert

### timescaledb.restoring

* **Type**: Boolean
* **Default**: `false`
* **Description**: Enable restoring mode for TimescaleDB


# Migrate from Aurora to PlanetScale
Source: https://planetscale.com/docs/postgres/imports/aurora

Use this guide to migrate an existing Aurora (Postgres) database to PlanetScale Postgres.

This guide will cover a no-downtime approach to migrating using Postgres logical replication. If you are willing to tolerate downtime during a maintenance window, you may also use [`pg_dump` and restore](/docs/postgres/imports/postgres-migrate-dumprestore). The `pg_dump`/restore approach is simpler, but is only for applications where downtime is acceptable.

This guide assumes that public internet access is enabled on your Aurora database, as it will be needed to connect and replicate to the new PlanetScale host. If you cannot enable this due to security policies, consider using [AWS DMS](/docs/postgres/imports/aurora-dms) for your migration, or [contact support](https://planetscale.com/contact?initial=support) for more specific guidance.

These instructions work for all versions of Postgres that support logical replication (version 10+). If you have an older version you want to bring to PlanetScale, [contact us](https://planetscale.com/contact?initial=support) for guidance.

Before beginning a migration, you should check our [extensions documentation](/docs/postgres/extensions) to ensure that all of the extensions you rely on will work on PlanetScale.

As an alternative to this guide, you can also try our [Postgres migration scripts](https://github.com/planetscale/migration-scripts/tree/main/postgres-direct). These allow you to automate some of the manual steps that we describe in this guide.

## 1. Prepare your PlanetScale database

Go to `app.planetscale.com` and create a new database. A few things to check when configuring your database:

* Ensure you select the correct cloud region. You typically want to use the same region that you deploy your other application infrastructure to.
* This guide assumes you are migrating from a Postgres Aurora database, so also choose the Postgres option in PlanetScale.
* Choose the best storage option for your needs. For applications needing high-performance and low-latency I/O, use [PlanetScale Metal](/docs/metal). For applications that need more flexible storage options or smaller compute instances, choose "Elastic Block Storage" or "Persistent Disk."

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=0e7ca13a6351b7e5364d24416b83a816" alt="Create a new PlanetScale Postgres database" data-og-width="2726" width="2726" data-og-height="2148" height="2148" data-path="docs/images/assets/docs/postgres/neon/image.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=91f0fbefca3fdb5c798b8ba29f397550 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=93c1f48350fe2f5f6066b33f9157a84e 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=68b2c55f471d0b2d64f3d3e50843d6b0 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=f2d33fa644794d0806af901658c63430 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=4fe66bc0957449acd0a0bb6775a34082 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=6959ae35b1808fc9b994a7d7766f270a 2500w" />
</Frame>

Once the database is created and ready, navigate to your dashboard and click the "Connect" button.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image2.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=e4784f2b40a6f88e2a601f6659be6712" alt="Connect to a PlanetScale Postgres database" data-og-width="3950" width="3950" data-og-height="1522" height="1522" data-path="docs/images/assets/docs/postgres/neon/image2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image2.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=35904d388380db65a9cf22ead105fba4 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image2.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=3593d6da1c20252552cb54c1526ed027 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image2.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=7b115652225005446dbd9f8641607f91 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image2.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=e647fc714478c4d2f67e6e4054ad4c07 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image2.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=b7cf52021b91f7922bd209ef5da0d3ad 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image2.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=74588936353bdc994269d4f2fa5085c9 2500w" />
</Frame>

From here, follow the instructions to create a new default role. This role will act as your admin role, with the highest level of privileges.

Though you may use this one for your migration, we recommend you use a separate role with lesser privileges for your migration and general database connections.

To create a new role, navigate to the [Role management page](/docs/postgres/connecting/roles) in your database settings. Click "New role" and give the role a memorable name. By default, `pg_read_all_data` and `pg_write_all_data` are enabled. In addition to these, enable `pg_create_subscription` and `postgres`, and then create the role.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image3.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=9345f5bdc50a2f4ff4377b29113593f9" alt="New Postgres role privileges" data-og-width="1882" width="1882" data-og-height="2296" height="2296" data-path="docs/images/assets/docs/postgres/neon/image3.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image3.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=381e4a65ddfaad1e630f3c5c60ec9883 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image3.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=a35fe462b430de0a0f9ebaa37f7a58cd 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image3.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=236cc405e176abd706451721f2725862 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image3.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=de2ce65079050ea0df953c50d589a526 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image3.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=cdeca3bf12d0bd60e21773df8f3b4123 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image3.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=2fc3ccd3aff59df61ef18ffa048296fc 2500w" />
</Frame>

Copy the password and all other connection credentials into environment variables for later use:

```bash  theme={null}
PLANETSCALE_USERNAME=pscale_api_XXXXXXXXXX.XXXXXXXXXX
PLANETSCALE_PASSWORD=pscale_pw_XXXXXXXXXXXXXXXXXXXXXXX
PLANETSCALE_HOST=XXXX.pg.psdb.cloud
PLANETSCALE_DBNAME=postgres
```

We also recommend that you increase `max_worker_processes` for the duration of the migration, in order to speed up data copying. Go to the "Parameters" tab of the "Clusters" page:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image4.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=b5d48fae3e6cad4c1ae316014c403374" alt="Configure parameters" data-og-width="3318" width="3318" data-og-height="1964" height="1964" data-path="docs/images/assets/docs/postgres/neon/image4.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image4.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=4b5874a8e96fb196931e2f4158be0e03 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image4.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=c276e4374fd4ed9bbf670b0c9d64b454 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image4.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=faee7333278a4ecebd20c0114791c8a0 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image4.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=2b65c663586797fafee2a1019d4bf45c 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image4.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=1c762d7dbbd20410b4182ad10ab09f51 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image4.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=0a58144ac8b35386a8ac3609b2ec2554 2500w" />
</Frame>

On this page, increase this value from the default of `4` to `10` or more:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image5.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=14b57255afb2d6ffdc04306e40df04c5" alt="Configure max worker processes" data-og-width="1784" width="1784" data-og-height="1152" height="1152" data-path="docs/images/assets/docs/postgres/neon/image5.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image5.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=df8d165ce6a0c1ba9dd16af3946a181a 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image5.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=7a7571b3e235c962e88c8e1c45a2dae2 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image5.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=4f6fe35d2a93b930533eeff594421ea1 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image5.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=913e8986a50aa44776c427f4ba7dc60f 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image5.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=bf346d34e516c3ceca3b51758a6bed47 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image5.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=87090013cab70e6a63ac4721274c8ad0 2500w" />
</Frame>

You can decrease these values after the migration is complete.

## 2. Configure disk size on PlanetScale

If you are importing into a database backed by network-attached storage, you must configure your disk in advance to ensure your database will fit.
Though we support disk autoscaling for these, AWS and GCP limit how frequently disks can be resized.

If you don't ensure your disk is large enough for the import in advance, it will not be able to resize fast enough for a large data import.

To configure this, navigate to "Clusters" and then the "Storage" tab:

<img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/storage-configuration-min-size.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=a5c9b5d45b3e70904dcd63ebae86ed51" alt="Storage configuration min size" data-og-width="3076" width="3076" data-og-height="2336" height="2336" data-path="docs/postgres/imports/storage-configuration-min-size.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/storage-configuration-min-size.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=e7ce34378d7f13060b6f55798d84ca81 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/storage-configuration-min-size.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=752e51dbf17cd95a054e8bfcffc88310 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/storage-configuration-min-size.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=afcc0a16e5cc562f507a58f397e48648 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/storage-configuration-min-size.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=430433f27649b68a6a612010926fd5ed 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/storage-configuration-min-size.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=f7a31fb98516dea9d3808e0e1649a1d8 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/storage-configuration-min-size.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=e0447135018372dbfde71a8647a6a9a2 2500w" />

On this page, adjust the "Minimum disk size."
You should set this value to at least 150% of the size of the database you are migrating.
For example, if the database you are importing is 330 GB, you should set your minimum disk size to at least 500 GB.

The 50% overhead is to account for:

1. Data growth during the import process and
2. Table and index bloat that can occur during the import process.
   This can be later mitigated with careful [VACUUMing](https://www.postgresql.org/docs/current/sql-vacuum.html) or using an extension like [pg\_squeeze](https://planetscale.com/docs/postgres/extensions/pg_squeeze), but is difficult to avoid during the migration itself.

When ready, queue and apply the changes.
You can check the "Changes" tab to see the status of the resize:

<img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/confirm-disk-size-change.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=ade2793422ccfcf0bd7e6c5ee2511a98" alt="Confirm disk size change" data-og-width="2626" width="2626" data-og-height="1264" height="1264" data-path="docs/postgres/imports/confirm-disk-size-change.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/confirm-disk-size-change.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=b193023eae6603ca54d2d21d4b8cf3ce 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/confirm-disk-size-change.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=36f59f9028dc0674d68594d7efbfd912 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/confirm-disk-size-change.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=92d471e9e4afa9de0dc7ff1e9adcf2e3 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/confirm-disk-size-change.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=f237cdd75c3a3c243b7b6b4c77fdba61 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/confirm-disk-size-change.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=99e53e615c6ca48b396d0b0ada146cf7 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/confirm-disk-size-change.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=226ed370d1291db934cd6fbda032d633 2500w" />

Wait for it to indicate completion.

If you are importing to a Metal database, you must choose a disk size when first creating your database.
You should launch your cluster with a disk size at least 50% larger than the storage used by your current source database (150% of the existing total).

As an example, if you need to import a 330 GB database onto a PlanetScale `M-160` there are three storage sizes available:

<img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/metal-disk-size.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=3c7924b6c3c1c712eb2772b9c16adb41" alt="Metal disk size" data-og-width="2074" width="2074" data-og-height="1812" height="1812" data-path="docs/postgres/imports/metal-disk-size.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/metal-disk-size.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=d2db5c8f73a3d3e450aa7410d517966a 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/metal-disk-size.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=b0c63716f22d784102107f89435f90c5 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/metal-disk-size.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=83ccb63b153d07c065f434a026156c32 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/metal-disk-size.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=b70ee20b312291760b4851520c333006 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/metal-disk-size.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=23f72e5606dd17d426b63d1404436a59 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/metal-disk-size.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=9bb971a81d534a7ac24bde77932d9717 2500w" />

You should use the largest, 1.25TB option during the import.
After importing and cleaning up table bloat, you may be able to downsize to the 468 GB option.
Resizing is a no-downtime operation that can be performed on the [Clusters](https://planetscale.com/docs/postgres/cluster-configuration) page.

## 3. Prepare the Aurora database

For PlanetScale to import your database, it needs to be publicly accessible. You can check this in your AWS dashboard.

In the writer instance of your database cluster, go to the “Connectivity & security” tab, and under “Security” you will see if your database is publicly accessible. If it says “No,” you will need to change it to be publicly accessible through the “Modify” button. If this is an issue, you cannot do this, or you have questions, please [contact support](https://planetscale.com/contact?initial=support) to explore your migration options.

You will also need to change some parameters and ensure that logical replication is enabled. If you don't already have a parameter group for your Aurora cluster, create one from the "Parameter groups" page in the AWS console:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image6.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=437cc52c48f561e2967fc71f1dc0ecd6" alt="AWS parameter groups" data-og-width="4122" width="4122" data-og-height="1606" height="1606" data-path="docs/images/assets/docs/postgres/neon/image6.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image6.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=1eb0773552ec8befea1a5743b371f9e3 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image6.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=f78f4dbba4f2ecd7258a2d5971ddfcbc 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image6.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=b8c500ecd7e0bfc1e636040989b9d271 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image6.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=9b9a166776e5e51088f3cf0bea8d40d0 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image6.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=6533437ced6eb4a9dc97fe69e0236f88 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image6.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=3e2c52b2e21ec9ec3d23ba8241d7a917 2500w" />
</Frame>

From here, click the button to create a new group. Choose whichever name and description you want. Set the `Engine type` to `Aurora Postgres` and the `Parameter family group` to the version that matches your Aurora Postgres database. Set the `Type` to `DB Cluster Parameter Group`.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image7.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=f7faa02e15f9cf0c3463d108f027f8b5" alt="Create an AWS parameter group" data-og-width="4014" width="4014" data-og-height="1734" height="1734" data-path="docs/images/assets/docs/postgres/neon/image7.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image7.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=70e3ddc8922fce6267e8e95176579b5a 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image7.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=fea1791d1837981e940b3fab24df8b89 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image7.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=55f89b14b50fd66eb9936b49a557842b 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image7.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=ec31976f08b4942bc302ac0b4a30615e 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image7.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=f1a8ecfa03f13c4157260de1e8018d90 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image7.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=545b2d54257e923653347bfd6ad99f49 2500w" />
</Frame>

If you already have a custom parameter group for your cluster, you can use the existing one instead. The two key parameters you need to update are adding `pglogical` to `shared_preload_libraries` and setting `rds.logical_replication` to `1`:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image8.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=d527ce318372760d89e0f66b396d9eb2" alt="Preload libraries parameter" data-og-width="3172" width="3172" data-og-height="878" height="878" data-path="docs/images/assets/docs/postgres/neon/image8.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image8.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=baaea23e9ec6477681aa6ac342bd8c5c 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image8.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=fc08b9e8f05a0cdebc4d8be7b0725791 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image8.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=3da38b14614ad15ffb52271c1a2fcf35 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image8.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=3ac296460769938b116b688929409ed2 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image8.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=614612a3c54145c8a8374978838a201a 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image8.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=cbda0d97a3d2374881a486fe2f26e56b 2500w" />
</Frame>

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image9.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=10375ea413b0ea3bf8acd6fdc5036d83" alt="Logical replication parameter" data-og-width="3172" width="3172" data-og-height="678" height="678" data-path="docs/images/assets/docs/postgres/neon/image9.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image9.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=50698550a0d50eecc777488f0afdf221 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image9.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=fe69ea763e66da6604ab64ed3f984c6a 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image9.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=af3248b84015b00e8176d94416d463b8 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image9.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=ab56c9840db2a3a18a5dbe78a5b3d9b9 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image9.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=72f573009fcbc5e2f35195a837b22ff8 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image9.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=2843095ae0b7d11ec42345220d056bae 2500w" />
</Frame>

Once these are set, you need to make sure your Aurora database is configured to use them. Navigate to your Aurora database in the AWS console, click the "Modify" button, and then ensure your database is using the parameter group:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image10.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=f5e74b4e1c11a7357f6ef4dad70f35fb" alt="Set parameter group for Aurora" data-og-width="2506" width="2506" data-og-height="846" height="846" data-path="docs/images/assets/docs/postgres/neon/image10.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image10.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=43efe5b6ad03aae3ee97918cfa13b387 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image10.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=0c563c5ec564953f918c072b11ce6b33 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image10.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=4acb213406faaac4699aaa4bfc461b1c 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image10.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=20f0499c0917a18e98853ae750e1b1b4 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image10.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=355f7e4ccfb5fd7df250b42ae294ff3b 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image10.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=d39a87c7e1f50fcf7450b954d0ddb0a2 2500w" />
</Frame>

When you go to save the changes, select the option to either apply immediately or during your next maintenance window. The changes may take time to propagate. You can confirm that the `wal_level` is set to `logical` by running `SHOW wal_level;` on your Aurora database:

```sql  theme={null}
postgres=> SHOW wal_level;
 wal_level
-----------
 logical
```

If you see a result other than `logical`, then it is not configured correctly. If you are having trouble getting the settings to propagate, you can try restarting the Aurora instance, though that will cause a period of downtime.

## 4. Copy schema from Aurora to PlanetScale

Before we begin migrating data, we first must copy the schema from Aurora to PlanetScale. We do this as a distinct set of steps using `pg_dump`.

<Warning>
  You should not make any schema changes during the migration process. You may continue to select, insert, update, and delete data, keeping your application fully online during this process.
</Warning>

For these instructions, you'll need to connect to Aurora with a role that has permissions to create replication publications and read all data. Your default role that was generated by Aurora when you first created your database should suffice here, but you may also use other roles. We will assume that the credentials for this user and other connection info are stored in the following environment variables.

```bash  theme={null}
AURORA_USERNAME=XXXX
AURORA_PASSWORD=XXXX
AURORA_HOST=XXX
AURORA_DBNAME=XXX
```

Run the below command to take a snapshot of the full schema of the `$AURORA_DBNAME` that you want to migrate:

```bash  theme={null}
PGPASSWORD=$AURORA_PASSWORD \
pg_dump -h $AURORA_HOST \
        -p 5432 \
        -U $AURORA_USERNAME \
        -d $AURORA_DBNAME \
        --schema-only \
        --no-owner \
        --no-privileges \
        -f schema.sql
```

This saves the schema into a file named `schema.sql`.

<Note>
  The above command will dump the tables for all schemas in the current database. If you want to migrate only one specific schema, you can add the `--schema=SCHEMA_NAME` option.
</Note>

The schema then needs to be loaded into your new PlanetScale database:

```bash  theme={null}
PGPASSWORD=$PLANETSCALE_PASSWORD \
psql -h $PLANETSCALE_HOST \
     -p 5432 \
     -U $PLANETSCALE_USERNAME \
     -d $PLANETSCALE_DBNAME \
     -f schema.sql
```

In the output of this command, you might see some error messages of the form:

```
psql:schema.sql:LINE: ERROR: DESCRIPTION
```

You should inspect these to see if they are of any concern. You can [reach out to our support](https://planetscale.com/contact) if you need assistance at this step.

## 5. Set up logical replication

We now must create a `PUBLICATION` on Aurora that the PlanetScale database can subscribe to for data copying and replication.

To create a publication for all tables in all schemas of the current database, run the following command on your Aurora database:

```sql  theme={null}
CREATE PUBLICATION replicate_to_planetscale FOR ALL TABLES;
```

You should see this if it created correctly:

```sql  theme={null}
CREATE PUBLICATION
```

<Note>
  To publish changes for only one specific schema, run the following query:

  ```sql  theme={null}
  SELECT 'CREATE PUBLICATION replicate_to_planetscale FOR TABLE ' ||
         string_agg(format('%I.%I', schemaname, tablename), ', ') || ';'
  FROM pg_tables
  WHERE schemaname = 'YOUR_SCHEMA_NAME';
  ```

  This will generate a query that looks like this:

  ```sql  theme={null}
  CREATE PUBLICATION replicate_to_planetscale FOR TABLE
    public.table_1,
    public.table_2,
    ...
    public.table_n;
  ```

  You can then copy/paste this and execute on Aurora. This will create a publication that only publishes changes for the tables in `YOUR_SCHEMA_NAME`
</Note>

After creating the publication on Aurora, we then need to tell PlanetScale to `SUBSCRIBE` to this publication.

```sql  theme={null}
PGPASSWORD=$PLANETSCALE_PASSWORD psql \
  -h $PLANETSCALE_HOST \
  -U $PLANETSCALE_USERNAME \
  -p 5432 $PLANETSCALE_DBNAME \
  -c "
CREATE SUBSCRIPTION replicate_from_aurora
CONNECTION 'host=$AURORA_HOST dbname=$AURORA_DBNAME user=$AURORA_USERNAME password=$AURORA_PASSWORD'
PUBLICATION replicate_to_planetscale WITH (copy_data = true);"
```

Data copying and replication will begin at this point. To check in on the row counts for the tables, you can run a query like this on your source and target databases:

```sql  theme={null}
SELECT table_name, row_count FROM (
  SELECT 'table_name_1' as table_name, COUNT(*) as row_count FROM table_name_1 UNION ALL
  SELECT 'table_name_2', COUNT(*) FROM table_name_2 UNION ALL
  ...
  SELECT 'table_name_N', COUNT(*) FROM table_name_N
) t ORDER BY table_name;
```

When the row counts match (or nearly match) you can begin testing and prepare for your application to cutover to use PlanetScale.

## 6. Handling sequences

Logical replication is great at migrating all of your data over to PlanetScale. However, logical replication does *not* synchronize the `nextval` values for [sequences](https://www.postgresql.org/docs/current/sql-createsequence.html) in your database. Sequences are often used for things like auto incrementing IDs, so it's important to ensure we update this before you switch your traffic to PlanetScale.

You can see all of the sequences and their corresponding `nextval`s on your source Aurora database using this command:

```sql  theme={null}
SELECT schemaname, sequencename, last_value + increment_by AS next_value
FROM pg_sequences;
```

An example output from this command:

```sql  theme={null}
 schemaname |   sequencename   | next_value
------------+------------------+------------
 public     | users_id_seq     |        105
 public     | posts_id_seq     |       1417
 public     | followers_id_seq |       3014
```

What this means is that we have three sequences in our database. In this case, they are all being used for auto-incrementing primary keys. The `nextval` for the `users_id_seq` is 105, the `nextval` for the `posts_id_seq` is 1417, and the `nextval` for the `followers_id_seq` is 3014. If you run the same query on your new PlanetScale database, you'll see something like:

```sql  theme={null}
 schemaname |   sequencename   | next_value
------------+------------------+------------
 public     | users_id_seq     |          0
 public     | posts_id_seq     |          0
 public     | followers_id_seq |          0
```

If you switch traffic over to PlanetScale in this state, you'll likely encounter errors when inserting new rows:

```sql  theme={null}
ERROR:  duplicate key value violates unique constraint "XXXX"
DETAIL:  Key (id)=(ZZZZ) already exists.
```

Before switching over, you need to progress all of these sequences forward so that the `nextval`s produced will be greater than any of the values previously produced on the source Aurora database, avoiding constraint violations. There are several approaches you can take for this. A simple way to solve the problem is to first run this query on your source Aurora database:

```sql  theme={null}
SELECT 'SELECT setval(''' || schemaname || '.' || sequencename || ''', '
       || (last_value + 10000) || ');' AS query
FROM pg_sequences;
```

This will generate a sequence of queries that will advance the `nextval` by 10,000 for each sequence:

```sql  theme={null}
                      query
--------------------------------------------------
 SELECT setval('public.users_id_seq', 10104);
 SELECT setval('public.posts_id_seq', 11416);
 SELECT setval('public.followers_id_seq', 13013);
```

You would then execute these on your target PlanetScale database. You need to ensure you advance each sequence far enough forward so that the sequences in the Aurora database will not reach these `nextval`s before you switch your primary to PlanetScale. For tables that have a high insertion rate, you might need to increase this by a larger value (say, 100,000 or 1,000,000).

## 7. Cutting over to PlanetScale

Before you cutover, it's good to have confidence that the replication is fully caught up between Aurora and PlanetScale. You can do this using Log Sequence Numbers (LSNs). The goal is to see these match up between the source Aurora database and the target PlanetScale database exactly. If they don't, it indicates that the PlanetScale database is not fully caught-up with the changes happening on Aurora.

You can run this on Aurora to see the current LSN:

```sql  theme={null}
postgres=> SELECT pg_current_wal_lsn();
 pg_current_wal_lsn
--------------------
 0/703FE460
```

Then on PlanetScale, you would run the following query to check for a match:

```sql  theme={null}
postgres=> SELECT received_lsn, latest_end_lsn
             FROM pg_stat_subscription
             WHERE subname = 'replicate_from_aurora';
 received_lsn | latest_end_lsn
--------------+----------------
 0/703FE460   | 0/703FE460
```

Once you are comfortable that all your data has successfully copied over and replication is sufficiently caught up, it's time to switch to PlanetScale. In your application code, prepare the cutover by changing the database connection credentials to go to PlanetScale rather than Aurora. Then, you can deploy this new version of your application, which will begin using PlanetScale as your primary database.

After doing this, new rows written to PlanetScale will not be reverse-replicated to Aurora. Thus, it's important to ensure you are fully ready for the cutover at this point.

Once this is complete, PlanetScale is now your primary database! We recommend you keep your old database around for at least a few days, just in case you discover any data or schemas you forgot to copy over to PlanetScale.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Postgres Imports - Amazon DMS with CloudFormation
Source: https://planetscale.com/docs/postgres/imports/aurora-dms

This method uses Infrastructure as Code with Step Functions workflow automation for a completely managed migration experience.

[Amazon Database Migration Service (DMS)](https://aws.amazon.com/dms/) provides a fully automated approach to migrate your PostgreSQL database to PlanetScale Postgres.

## Overview

This automated migration method:

<Steps>
  <Step>
    **Pre-migration schema setup** (essential for production)
  </Step>

  <Step>
    Deploys DMS infrastructure via CloudFormation template
  </Step>

  <Step>
    Configures source and target database endpoints automatically
  </Step>

  <Step>
    Creates Step Functions workflow for automated migration orchestration
  </Step>

  <Step>
    Provides built-in monitoring, notifications, and automated cleanup
  </Step>

  <Step>
    Requires minimal manual intervention - mostly console clicks
  </Step>
</Steps>

<Warning>
  **Critical: AWS DMS Schema Object Limitations**

  AWS DMS **only migrates table data and primary keys**. All other PostgreSQL schema objects must be handled separately:

  * Secondary indexes
  * Sequences and their current values
  * Views, functions, and stored procedures
  * Constraints (foreign keys, unique, check)
  * Triggers and custom data types

  Deploy your complete Aurora schema to PlanetScale BEFORE starting DMS migration to preserve performance and avoid application errors.
</Warning>

<Note>
  This method requires an AWS account and will incur AWS DMS charges. The CloudFormation template includes cost optimization features. Review [AWS DMS pricing](https://aws.amazon.com/dms/pricing/) before proceeding.
</Note>

## General Prerequisites

Before starting the migration:

* Have an active AWS user with CloudFormation, EC2, DMS, SNS, and Step Functions permissions
* Source Aurora database accessible from AWS VPC
* Connection details for your PlanetScale Postgres database from the console
* VPC with at least 2 subnets in different Availability Zones

## Source database prerequisites

The Task that AWS DMS runs will automatically perform these 7 validation checks before starting the migration. Confirm before starting this process that they will succeed.

| Check Name                      | What It Validates                                          | Required Action (if needed)                                                       |
| :------------------------------ | :--------------------------------------------------------- | :-------------------------------------------------------------------------------- |
| Database Version Compatibility  | Verifies your PostgreSQL version is supported by AWS DMS   | Ensure you're running a supported PostgreSQL version (9.4+)                       |
| Target Database Privileges      | Confirms PlanetScale user has sufficient permissions       | No action should be needed - PlanetScale credentials include required permissions |
| Replication Slots Available     | Checks that replication slots are available for CDC        | Verify `max_replication_slots >= 1` in Aurora parameter group                     |
| Source Database Read Privileges | Validates source user can read all tables for migration    | Ensure source user has SELECT privileges on all tables to migrate                 |
| WAL Level Configuration         | Confirms WAL level is set to 'logical' for CDC replication | Set `wal_level = logical` in Aurora parameter group (requires restart)            |
| WAL Sender Timeout              | Ensures WAL sender timeout is at least 10 seconds          | Set `wal_sender_timeout >= 10000` (10 seconds) in parameter group                 |
| Maximum WAL Senders             | Verifies enough WAL sender processes for CDC               | Set `max_wal_senders >= 2` in Aurora parameter group                              |

## Step 1: Pre-Migration Schema Setup

Deploy your complete Aurora schema to PlanetScale BEFORE starting the CloudFormation stack. This ensures optimal performance and prevents application errors.

### Extract and Apply Schema

<Steps>
  <Step>
    Extract your complete schema from Aurora:

    ```bash  theme={null}
    pg_dump -h aurora-cluster-endpoint.amazonaws.com -p 5432 \
            -U username -d database --schema-only \
            --no-owner --no-privileges -f aurora_schema.sql
    ```
  </Step>

  <Step>
    Apply the schema to PlanetScale:

    ```bash  theme={null}
    psql -h your-planetscale-host -p 5432 -U username -d database -f aurora_schema.sql
    ```
  </Step>
</Steps>

<Note>
  **Foreign Key Constraints**

  If the schema application fails due to foreign key constraint issues, you can temporarily remove them from the schema file and apply them after DMS completes the data migration.
</Note>

### Verify Schema Application

Quickly verify your schema was applied successfully:

```sql  theme={null}
-- Check that tables and sequences exist
SELECT
    (SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public') as tables,
    (SELECT COUNT(*) FROM information_schema.sequences WHERE sequence_schema = 'public') as sequences,
    (SELECT COUNT(*) FROM pg_indexes WHERE schemaname = 'public') as indexes;
```

## Step 2: Check DMS IAM Roles

Before deploying, check if DMS service roles already exist in your AWS account:

<Steps>
  <Step>
    Go to [IAM Console](https://console.aws.amazon.com/iam/)
  </Step>

  <Step>
    Click "Roles" in the left sidebar
  </Step>

  <Step>
    Search for these role names:

    * `dms-vpc-role`
    * `dms-cloudwatch-logs-role`
  </Step>
</Steps>

* **If neither role exists**: Set `CreateDMSRoles` parameter to `true` in Step 4
* **If both roles exist**: Set `CreateDMSRoles` parameter to `false` in Step 4
* **If one role exists but not the other**: Consider manually creating the roles per guidance in the [AWS DMS documentation](https://docs.aws.amazon.com/dms/latest/userguide/CHAP_VPC_Endpoints.html#CHAP_VPC_Endpoints.prereq) and set `CreateDMSRoles` parameter to `false` in Step 4

## Step 3: Download CloudFormation Template

Get the CloudFormation template:

<Steps>
  <Step>
    Visit: [https://github.com/planetscale/migration-scripts/tree/main/postgres-planetscale](https://github.com/planetscale/migration-scripts/tree/main/postgres-planetscale)
  </Step>

  <Step>
    Right-click on `aurora-to-ps-dms.yaml` → "Save link as"
  </Step>

  <Step>
    Save the file to your computer
  </Step>
</Steps>

## Step 4: Deploy CloudFormation Stack

<Steps>
  <Step>
    Navigate to [AWS CloudFormation Console](https://console.aws.amazon.com/cloudformation/)
  </Step>

  <Step>
    Click **"Create stack"** → **"With new resources (standard)"**
  </Step>

  <Step>
    Select **"Upload a template file"**
  </Step>

  <Step>
    Click **"Choose file"** and select the downloaded template
  </Step>

  <Step>
    Click **"Next"**
  </Step>
</Steps>

### Configure Stack Parameters

**Stack name**: `postgres2planetscale` or any name you want but note that overly-long names can cause resource creation issues

#### VPC Information

* **VPC ID**: Select your VPC from dropdown
* **Subnet IDs**: Select 2+ subnets in different AZs which are "public" subnets in that they have route tables through an Internet Gateway (IGW) and NACLs that allow outbound routing. See [Subnet types](https://docs.aws.amazon.com/vpc/latest/userguide/configure-subnets.html#subnet-types) in the AWS documentation for more information

#### Source Database (Your Aurora Postgres database)

* **Source Endpoint Host**: Your Aurora hostname (primary write endpoint is best, not a proxy)
* **Source Port**: `5432` (or your custom port, valid range: 1024-65535)
* **Source Database Name**: Your database name (1-63 characters, must start with a letter, alphanumeric and underscore only)
* **Source Username**: Your database username (1-63 characters, must start with a letter, alphanumeric and underscore only)
* **Source Password**: Your database password (4-128 characters, will be hidden in console)

#### Target Database (PlanetScale Postgres)

From your PlanetScale console connection details:

* **Target Endpoint Host**: PlanetScale host (from connection details)
* **Target Port**: `5432` (standard PostgreSQL port)
* **Target Database Name**: PlanetScale database name (1-63 characters, must start with a letter, alphanumeric and underscore only)
* **Target Username**: PlanetScale username (1-63 characters, must start with a letter, alphanumeric and underscore only)
* **Target Password**: PlanetScale password (4-128 characters, will be hidden in console)

#### Additional Features

* **DMS Instance Class**: `dms.c6i.xlarge` (template default, can select from dropdown)
  * Options include: dms.t3.micro, dms.t3.small, dms.t3.medium, dms.t3.large, dms.c6i.large, dms.c6i.xlarge, dms.c6i.2xlarge, dms.c6i.4xlarge
  * Recommended: dms.c6i.large or larger for production workloads
* **Migration Type**: `full-load-and-cdc` (recommended)
  * Options: `full-load`, `cdc`, `full-load-and-cdc`
* **Migration Bucket Name**: Base name for S3 bucket that stores DMS assessment reports
  * Must be 3-35 characters, start/end with lowercase letter or number
  * Can contain lowercase letters, numbers, hyphens, and periods
  * Region and account ID will be automatically appended to create unique bucket name
  * Example: `my-migration-bucket` becomes `my-migration-bucket-us-east-1-123456789012`
* **Enable Automation**: `true` ⭐ **Important: This creates the Step Functions workflow**
* **Create DMS Roles**: `true` or `false` (based on Step 2 findings)
* **Notification Email**: Your email address for migration status updates and alerts

<Note>
  **Schema-First Approach Built-In**

  The CloudFormation template is pre-configured for schema-first migrations with:

  * `TargetTablePrepMode: DO_NOTHING` (automatically set, uses your existing schema)
  * Enhanced performance tuning settings
  * Built-in row-level validation
  * Batch processing optimizations
  * Memory tuning for large datasets

  Complete Step 1 (pre-migration schema setup) before deploying this stack for optimal results.
</Note>

<Steps>
  <Step>
    Click **"Next"** → **"Next"** → Check **"I acknowledge that AWS CloudFormation might create IAM resources"**
  </Step>

  <Step>
    Click **"Submit"**
  </Step>
</Steps>

## Step 5: Wait for Stack Completion

<Steps>
  <Step>
    Stay on the CloudFormation console
  </Step>

  <Step>
    Click on your stack name to view details
  </Step>

  <Step>
    Watch the **"Events"** tab for progress
  </Step>

  <Step>
    Wait for stack status to show **`CREATE_COMPLETE`** (typically 10-15 minutes)
  </Step>
</Steps>

## Step 6: Confirm your email notification subscription

<Steps>
  <Step>
    Check the inbox for the email used above
  </Step>

  <Step>
    You will get an email from `no-reply@sns.amazonaws.com` "DMS Migration Workflow Notifications"
  </Step>

  <Step>
    Click the link for **Confirm Subscription**
  </Step>
</Steps>

Note that after the migration task completes and the stack is deleted, you would receive no further communications from this AWS SNS Topic, but other SNS topics may use the same address.

## Step 7: Get Workflow Input Configuration

<Steps>
  <Step>
    In your completed CloudFormation stack, click the **"Outputs"** tab
  </Step>

  <Step>
    Find the output key **`StepFunctionsPayloadTemplate`**
  </Step>

  <Step>
    **Copy the entire JSON value** (this contains the configuration for your migration) The JSON should look like the following example:

    ```json  theme={null}
    {
        "replicationInstanceArn": "arn:aws:dms:us-east-2:1234567890:rep:YMZ2AH4YAJCRNJKOWBR7EEIRGE",
        "sourceEndpointArn": "arn:aws:dms:us-east-2:1234567890:endpoint:SIVNPTNFJZDCVK4ODTN6ZLONN4",
        "targetEndpointArn": "arn:aws:dms:us-east-2:1234567890:endpoint:MLSCVENBKVBWJKRVJ27EWB32IU",
        "replicationTaskArn": "arn:aws:dms:us-east-2:1234567890:task:QZCBNW565VH2JG2KE5UXX42LS4",
        "sourceEndpointHost": "prod-aurora-cluster.cluster-abc1234567.us-east-2.rds.amazonaws.com",
        "sourcePort": "5432"
    }
    ```
  </Step>
</Steps>

## Step 8: Locate Step Functions Workflow

<Steps>
  <Step>
    While still in the **"Outputs"** tab find the key **`StepFunctionsConsoleURL`**
  </Step>

  <Step>
    Click on the URL link to open the AWS Step Functions console for the workflow created here
  </Step>
</Steps>

The workflow includes these automated steps:

* **Network Connectivity Check**: Tests connections to both source and target databases
* **Security Group Auto-Fix**: Automatically corrects Aurora security group settings if DMS connectivity fails
* **Pre-Migration Validation**: Validates database schemas, table structures, and data types with row-level validation
* **Migration Task Start**: Launches optimized DMS full-load and CDC replication with performance tuning
* **Progress Monitoring**: Continuously monitors migration progress with enhanced error handling and batch processing
* **Built-in Optimization**: Uses tuned task settings for improved throughput and memory management

## Step 9: Start Migration Workflow

<Steps>
  <Step>
    In the Step Functions state machine, click **"Start execution"**
  </Step>

  <Step>
    **Execution name**: Leave as auto-generated
  </Step>

  <Step>
    **Input**: **Paste the JSON** you copied from CloudFormation outputs
  </Step>

  <Step>
    Click **"Start execution"**
  </Step>
</Steps>

The workflow will automatically:

* Test database connections
* Start the DMS migration task
* Monitor progress
* Send notifications
* Handle errors and retries

## Step 10: Monitor Migration Progress

There are several tools you can use to monitor the progress and if necessary troubleshoot potential failures.

### DMS Console

<Steps>
  <Step>
    Go to [DMS Console](https://console.aws.amazon.com/dms/)
  </Step>

  <Step>
    Click **"Tasks"**
  </Step>

  <Step>
    Select the task **Identifier** for this migration
  </Step>

  <Step>
    View your task for detailed table-level progress
  </Step>
</Steps>

### Step Functions Console

<Steps>
  <Step>
    Watch the visual workflow progress in the Step Functions console
  </Step>

  <Step>
    Each step will show green (success), red (failure), or blue (in progress)
  </Step>

  <Step>
    Click on individual steps to see detailed logs
  </Step>
</Steps>

### CloudWatch Dashboard

<Steps>
  <Step>
    Navigate to [CloudWatch Console](https://console.aws.amazon.com/cloudwatch/)
  </Step>

  <Step>
    Click **"Dashboards"** → **"Automatic Dashboards"** → **"DMS-Migration-Dashboard"**
  </Step>

  <Step>
    Monitor key metrics:

    * Full load progress percentage
    * CDC latency
    * Error counts
    * Throughput
  </Step>
</Steps>

### Wait for automated emails

You will receive an email once the migration has reached a 100% full load and CDC replication is ongoing.

If the workflow does fail at any point, you will instead receive an email with where the failure occurred and then you can review the previously mentioned tools for more information.

## Step 11: Post-Migration Sequence Synchronization

After DMS completes, sequences need their values synchronized:

<Warning>
  **Critical: Sequence Synchronization**

  Sequence values must be set ahead of Aurora values to prevent duplicate key errors when applications start using PlanetScale.
</Warning>

### Get Current Sequence Values from Aurora

```sql  theme={null}
-- Run on Aurora database to get all current sequence values
SELECT
    sequence_name,
    last_value,
    'SELECT setval(''' || sequence_name || ''', ' || (last_value + 1000) || ');' as update_command
FROM information_schema.sequences
WHERE sequence_schema = 'public'
ORDER BY sequence_name;
```

### Update Sequences in PlanetScale

```sql  theme={null}
-- For each sequence, run the update command from above
-- Example commands (values set ahead of Aurora):
SELECT setval('users_id_seq', 16234);  -- Aurora value + 1000
SELECT setval('orders_id_seq', 99765);  -- Aurora value + 1000
SELECT setval('products_id_seq', 6432);  -- Aurora value + 1000

-- Verify sequence values are ahead of Aurora
SELECT sequence_name, last_value
FROM information_schema.sequences
WHERE sequence_schema = 'public'
ORDER BY sequence_name;
```

### Apply Remaining Constraints

Now apply foreign key constraints that were deferred:

```sql  theme={null}
-- Apply foreign key constraints
\i constraints.sql

-- Verify constraints were applied successfully
SELECT conname, contype, conrelid::regclass AS table_name
FROM pg_constraint
WHERE connamespace = 'public'::regnamespace
  AND contype = 'f'  -- foreign key constraints
ORDER BY conrelid::regclass::text;
```

## Step 12: Application Cutover

When the Step Functions workflow or DMS task itself indicates migration is ready (Status is "Load completed, replication ongoing"), then you can begin your cutover process.

### Comprehensive Pre-Cutover Validation

<Warning>
  **Complete Validation Required**

  Validate ALL schema objects and data integrity before cutover. Missing objects will cause application failures.
</Warning>

```sql  theme={null}
-- Validate table row counts match Aurora
SELECT
    schemaname,
    tablename,
    n_tup_ins as estimated_rows
FROM pg_stat_user_tables
WHERE schemaname = 'public'
ORDER BY tablename;
```

### Pre-Cutover Checklist (Automated)

AWS DMS ensures:

* <Icon icon="check" color="blue" /> Full load is 100% complete
* <Icon icon="check" color="blue" /> CDC latency is under 5 seconds
* <Icon icon="check" color="blue" /> Data validation passes
* <Icon icon="check" color="blue" /> Both databases are synchronized

### Cutover Process

<Steps>
  <Step>
    **Complete sequence synchronization and constraint application** using steps above
  </Step>

  <Step>
    **Run comprehensive validation** to ensure all objects are functional
  </Step>

  <Step>
    **Put application in maintenance mode**, pause all writes from your application to Aurora
  </Step>

  <Step>
    **Wait for DMS to confirm final sync**
  </Step>

  <Step>
    **Update your application's database connection strings** to use PlanetScale details
  </Step>

  <Step>
    **Restart or redeploy your application**
  </Step>

  <Step>
    **Test critical functionality**, especially features using sequences and indexes
  </Step>
</Steps>

## Automated Cleanup (mostly)

<Warning>
  **Schema Objects and Cleanup**

  The CloudFormation stack cleanup **does not** affect your migrated schema objects in PlanetScale. Your indexes, sequences, and other objects remain intact.
</Warning>

<Steps>
  <Step>
    Go to your CloudFormation stack
  </Step>

  <Step>
    Click **"Delete"**
  </Step>

  <Step>
    Click the **"Delete"** confirmation popup

    <Note>
      The first time you attempt to delete the stack, the full process will fail to delete some of the resources. Minimally the S3 bucket created to store the DMS pre-migration test reports will need to be manually emptied before it can be deleted. If the Step Functions workflow had to modify Aurora security group, then the rule added needs to be deleted as well. Both of these resources could be safely left behind, however the S3 bucket's data will incur ongoing charges.
    </Note>
  </Step>

  <Step>
    In the CloudFormation stack **"Resources"** tab, find the resources where deletion failed
  </Step>

  <Step>
    Find the Resource named **PreMigrationAssessmentBucket**
  </Step>

  <Step>
    Click to open the link under the **Physical ID** heading for this resource
    This will take you to the Amazon S3 console for this bucket. The assessment-folder is a versioned object in S3, which means that directly deleting it here does not actually remove it, but instead places a deleted marker on the version.
  </Step>

  <Step>
    To fully empty the bucket you will need to navigate to the main console page. Click **"Buckets"** from the top nav, or **"General purpose buckets"** from the left nav
  </Step>

  <Step>
    Select the bucket used, it will start with the name of the CloudFormation stack
  </Step>

  <Step>
    Click **"Empty"**
  </Step>

  <Step>
    You can now re-attempt the stack deletion from the 1st step here
  </Step>
</Steps>

## Troubleshooting

### Stack Creation Issues

**Permission Errors:**

* Ensure your AWS user has CloudFormation, DMS, Step Functions, and IAM permissions
* Check that you acknowledged IAM resource creation during stack creation

**Network Issues:**

* Verify your VPC allows internet access for DMS to reach PlanetScale
* Check security groups allow port 5432 access
* Ensure subnets are in different Availability Zones

### Step Functions Workflow Issues

**Workflow Creation Fails:**

* Verify you copied the complete JSON from CloudFormation outputs
* Check that the Step Functions execution role exists

**Migration Task Fails:**

* Check Step Functions execution details for specific error messages
* Verify database connection details are correct
* Ensure source database has logical replication enabled

### Connection Problems

**Source Database:**

* Verify hostname, port, username, and password
* Check that source database allows connections from DMS subnet
* Ensure database has logical replication enabled (`rds.logical_replication = 1` for RDS)

**Target Database (PlanetScale):**

* Double-check connection details from PlanetScale console
* Verify PlanetScale database is active and accessible
* Test connectivity from AWS region to PlanetScale

### Schema-Related Issues

**"sequence does not exist" errors after cutover:**

```sql  theme={null}
-- Check if sequences exist
SELECT * FROM information_schema.sequences WHERE sequence_name = 'your_sequence';

-- Recreate missing sequence
CREATE SEQUENCE your_sequence START WITH 1;
SELECT setval('your_sequence', (SELECT MAX(id) FROM your_table));
```

**Application slowness after migration:**

* Missing indexes are the most common cause
* Run `EXPLAIN ANALYZE` on slow queries to identify missing indexes
* Apply indexes from your schema extraction

**Foreign key constraint violations:**

```sql  theme={null}
-- Find constraint violations before applying constraints
SELECT COUNT(*) FROM child_table c
WHERE NOT EXISTS (SELECT 1 FROM parent_table p WHERE p.id = c.parent_id);
```

**Function/view dependency errors:**

* Apply objects in correct order: sequences → indexes → views → functions → constraints
* Check for Aurora-specific functions that may need modification for PlanetScale

**Permission errors during schema application:**

* Ensure PlanetScale user has CREATE privileges
* Check if objects already exist and need to be dropped first

## Step Functions Workflow Benefits

Using the automated Step Functions workflow provides:

* **Visual Progress Tracking**: See each migration phase in real-time
* **Automatic Error Handling**: Built-in retry logic and error notifications
* **Audit Trail**: Complete log of migration steps and timings

## Advanced Options

### CloudFormation Template Optimizations

The updated CloudFormation template includes these performance enhancements:

**Task Configuration Improvements:**

* `BatchApplyEnabled: true` - Improves target database write performance
* `ValidationMode: ROW_LEVEL` - Built-in data validation with 10K failure tolerance
* Memory tuning: 2GB total memory limit with optimized batch sizing
* Enhanced CDC processing with 5-second commit timeout
* Statement caching for improved query performance

**Monitoring Enhancements:**

* Comprehensive logging for all DMS components
* CloudWatch integration for real-time metrics
* Automated failure handling and notifications

**Schema-First Integration:**

* `TargetTablePrepMode: DO_NOTHING` preserves your pre-deployed schema
* `FullLoadIgnoreConflicts: true` handles edge cases gracefully
* Optimized for existing table structures and indexes

### Custom Migration Settings

Modify template parameters for:

* Different DMS instance sizes
* Custom migration types (full-load only, CDC only)
* Extended monitoring periods
* Custom notification settings

### Multiple Database Migration

Deploy multiple stacks with different names to migrate multiple databases in parallel.

## Migration Considerations

Before migration, review:

<Columns cols={2}>
  <Card title="PostgreSQL version compatibility" icon="database" horizontal href="/docs/vitess/imports/postgres#postgresql-version-compatibility" />

  <Card title="Extension support limitations" icon="battery-exclamation" horizontal href="/docs/vitess/imports/postgres#extension-support" />

  <Card title="Third-party enhancement restrictions" icon="circle-xmark" horizontal href="/docs/vitess/imports/postgres#third-party-enhancements-and-tools" />
</Columns>

**Important:** Allow additional time for post-migration schema object setup. Aurora databases with many indexes or complex constraints may require several hours for complete schema migration.

## Support and Resources

<Columns cols={2}>
  <Card title="Template Issues" icon="github" horizontal href="https://github.com/planetscale/dms-migration-templates" />

  <Card title="AWS Documentation" icon="aws" horizontal href="https://docs.aws.amazon.com/cloudformation/" />

  <Card title="PlanetScale Support" icon="circle-question" horizontal href="https://planetscale.com/contact?initial=support" />
</Columns>

For simpler migrations, consider [pg\_dump/restore](/docs/postgres/imports/postgres-migrate-dumprestore), or [logical replication](/docs/postgres/imports/postgres-migrate-walstream) methods.

**Post-Migration Success Checklist:**

* ✅ All schema objects migrated and validated
* ✅ Sequence values synchronized with Aurora
* ✅ Application performance matches pre-migration levels
* ✅ All critical application features tested
* ✅ Constraints and foreign keys working correctly
* ✅ No application errors in logs for 24+ hours
* ✅ Query performance baseline established

**Migration Timeline Expectations with Optimized Template:**

* Schema setup: 30 minutes - 2 hours (depending on complexity)
* DMS full load: Improved by \~25-40% due to batch processing optimizations
  * Small databases (under 10GB): 30 minutes - 2 hours
  * Medium databases (10-100GB): 2-6 hours
  * Large databases (100GB+): 4-12 hours
* Sequence synchronization: 5-15 minutes
* Validation and cutover: 30-60 minutes
* Total downtime for cutover: 5-30 minutes

**Performance Improvements:**

* Batch apply processing reduces target database load
* Enhanced memory management improves large table handling
* Row-level validation catches issues early without stopping migration

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Import public IP addresses
Source: https://planetscale.com/docs/postgres/imports/import-ips

When importing a database, you may have IP restrictions in place on your source database and may need to grant a set of IP addresses access to your Postgres database so that PlanetScale can make the connection.

If you have IP restrictions on your source database, you'll need to allowlist the PlanetScale IP addresses so we can connect during the import process. The IPs you need depend on the region of your PlanetScale database.

## Retrieve IP addresses via API

You can get the list of IP addresses for your database's region by visiting the following URL in your browser while logged into PlanetScale:

```
https://api.planetscale.com/v1/organizations/<ORG>/databases/<DATABASE>
```

Replace `<ORG>` with your organization name and `<DATABASE>` with your database name.

The response includes a `region` object with a `public_ip_addresses` array containing all the IPs you need to allowlist:

```json  theme={null}
{
  "id": "rr0qdj0nin8q",
  "type": "Database",
  "region": {
    "id": "ri0pbcmdkjsh",
    "type": "Region",
    "provider": "AWS",
    "enabled": true,
    "public_ip_addresses": [
      "18.117.23.127",
      "3.131.243.164",
      "3.132.168.252",
      "3.131.252.213",
      "3.132.182.173",
      "3.15.49.114",
      "3.209.149.66",
      "3.215.97.46",
      "34.193.111.15"
    ],
    "display_name": "AWS us-east-2",
    "location": "Ohio",
    "slug": "aws-us-east-2"
  }
}
```

Add each IP address from the `public_ip_addresses` array to your source database's allowlist before starting the import.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Migrate from Neon to PlanetScale
Source: https://planetscale.com/docs/postgres/imports/neon

Use this guide to migrate an existing Neon database to PlanetScale Postgres.

This guide will cover a no-downtime approach to migrating using Postgres logical replication. If you are willing to tolerate downtime during a maintenance window, you may also use [`pg_dump` and restore](/docs/postgres/imports/postgres-migrate-dumprestore). The `pg_dump`/restore approach is simpler, but is only for applications where downtime is acceptable.

These instructions work for all versions of Postgres that support logical replication (version 10+). If you have an older version you want to bring to PlanetScale, [contact us](https://planetscale.com/contact?initial=support) for guidance.

Before beginning a migration, you should check our [extensions documentation](/docs/postgres/extensions) to ensure that all of the extensions you rely on will work on PlanetScale.

As an alternative to this guide, you can also try our [Postgres migration scripts](https://github.com/planetscale/migration-scripts/tree/main/postgres-direct). These allow you to automate some of the manual steps that we describe in this guide.

## 1. Prepare your PlanetScale database

Go to `app.planetscale.com` and create a new database. A few things to check when configuring your database:

* Ensure you select the correct cloud region. You typically want to use the same region that you deploy your other application infrastructure to.
* This guide assumes you are migrating from a Neon database, so also choose the Postgres option in PlanetScale.
* Choose the best storage option for your needs. For applications needing high-performance and low-latency I/O, use [PlanetScale Metal](/docs/metal). For applications that need more flexible storage options or smaller compute instances, choose "Elastic Block Storage" or "Persistent Disk."

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=0e7ca13a6351b7e5364d24416b83a816" alt="Create a new PlanetScale Postgres database" data-og-width="2726" width="2726" data-og-height="2148" height="2148" data-path="docs/images/assets/docs/postgres/neon/image.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=91f0fbefca3fdb5c798b8ba29f397550 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=93c1f48350fe2f5f6066b33f9157a84e 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=68b2c55f471d0b2d64f3d3e50843d6b0 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=f2d33fa644794d0806af901658c63430 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=4fe66bc0957449acd0a0bb6775a34082 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=6959ae35b1808fc9b994a7d7766f270a 2500w" />
</Frame>

Once the database is created and ready, navigate to your dashboard and click the "Connect" button.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image2.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=e4784f2b40a6f88e2a601f6659be6712" alt="Connect to a PlanetScale Postgres database" data-og-width="3950" width="3950" data-og-height="1522" height="1522" data-path="docs/images/assets/docs/postgres/neon/image2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image2.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=35904d388380db65a9cf22ead105fba4 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image2.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=3593d6da1c20252552cb54c1526ed027 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image2.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=7b115652225005446dbd9f8641607f91 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image2.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=e647fc714478c4d2f67e6e4054ad4c07 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image2.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=b7cf52021b91f7922bd209ef5da0d3ad 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image2.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=74588936353bdc994269d4f2fa5085c9 2500w" />
</Frame>

From here, follow the instructions to create a new default role. This role will act as your admin role, with the highest level of privileges.

Though you may use this one for your migration, we recommend you use a separate role with lesser privileges for your migration and general database connections.

To create a new role, navigate to the [Role management page](/docs/postgres/connecting/roles) in your database settings. Click "New role" and give the role a memorable name. By default, `pg_read_all_data` and `pg_write_all_data` are enabled. In addition to these, enable `pg_create_subscription` and `postgres`, and then create the role.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image3.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=9345f5bdc50a2f4ff4377b29113593f9" alt="New Postgres role privileges" data-og-width="1882" width="1882" data-og-height="2296" height="2296" data-path="docs/images/assets/docs/postgres/neon/image3.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image3.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=381e4a65ddfaad1e630f3c5c60ec9883 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image3.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=a35fe462b430de0a0f9ebaa37f7a58cd 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image3.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=236cc405e176abd706451721f2725862 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image3.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=de2ce65079050ea0df953c50d589a526 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image3.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=cdeca3bf12d0bd60e21773df8f3b4123 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image3.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=2fc3ccd3aff59df61ef18ffa048296fc 2500w" />
</Frame>

Copy the password and all other connection credentials into environment variables for later use:

```bash  theme={null}
PLANETSCALE_USERNAME=pscale_api_XXXXXXXXXX.XXXXXXXXXX
PLANETSCALE_PASSWORD=pscale_pw_XXXXXXXXXXXXXXXXXXXXXXX
PLANETSCALE_HOST=XXXX.pg.psdb.cloud
PLANETSCALE_DBNAME=postgres
```

We also recommend that you increase `max_worker_processes` for the duration of the migration, in order to speed up data copying. Go to the "Parameters" tab of the "Clusters" page:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image4.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=b5d48fae3e6cad4c1ae316014c403374" alt="Configure parameters" data-og-width="3318" width="3318" data-og-height="1964" height="1964" data-path="docs/images/assets/docs/postgres/neon/image4.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image4.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=4b5874a8e96fb196931e2f4158be0e03 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image4.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=c276e4374fd4ed9bbf670b0c9d64b454 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image4.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=faee7333278a4ecebd20c0114791c8a0 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image4.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=2b65c663586797fafee2a1019d4bf45c 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image4.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=1c762d7dbbd20410b4182ad10ab09f51 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image4.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=0a58144ac8b35386a8ac3609b2ec2554 2500w" />
</Frame>

On this page, increase this value from the default of `4` to `10` or more:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image5.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=14b57255afb2d6ffdc04306e40df04c5" alt="Configure max worker processes" data-og-width="1784" width="1784" data-og-height="1152" height="1152" data-path="docs/images/assets/docs/postgres/neon/image5.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image5.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=df8d165ce6a0c1ba9dd16af3946a181a 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image5.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=7a7571b3e235c962e88c8e1c45a2dae2 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image5.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=4f6fe35d2a93b930533eeff594421ea1 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image5.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=913e8986a50aa44776c427f4ba7dc60f 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image5.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=bf346d34e516c3ceca3b51758a6bed47 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/neon/image5.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=87090013cab70e6a63ac4721274c8ad0 2500w" />
</Frame>

You can decrease these values after the migration is complete.

## 2. Configure disk size on PlanetScale

If you are importing into a database backed by network-attached storage, you must configure your disk in advance to ensure your database will fit.
Though we support disk autoscaling for these, AWS and GCP limit how frequently disks can be resized.
If you don't ensure your disk is large enough for the import in advance, it will not be able to resize fast enough for a large data import.

To configure this, navigate to "Clusters" and then the "Storage" tab:

<img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/storage-configuration-min-size.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=a5c9b5d45b3e70904dcd63ebae86ed51" alt="Storage configuration min size" data-og-width="3076" width="3076" data-og-height="2336" height="2336" data-path="docs/postgres/imports/storage-configuration-min-size.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/storage-configuration-min-size.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=e7ce34378d7f13060b6f55798d84ca81 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/storage-configuration-min-size.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=752e51dbf17cd95a054e8bfcffc88310 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/storage-configuration-min-size.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=afcc0a16e5cc562f507a58f397e48648 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/storage-configuration-min-size.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=430433f27649b68a6a612010926fd5ed 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/storage-configuration-min-size.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=f7a31fb98516dea9d3808e0e1649a1d8 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/storage-configuration-min-size.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=e0447135018372dbfde71a8647a6a9a2 2500w" />

On this page, adjust the "Minimum disk size."
You should set this value to at least 150% of the size of the database you are migrating.
For example, if the database you are importing is 330 GB, you should set your minimum disk size to at least 500 GB.

The 50% overhead is to account for:

1. Data growth during the import process and
2. Table and index bloat that can occur during the import process.
   This can be later mitigated with careful [VACUUMing](https://www.postgresql.org/docs/current/sql-vacuum.html) or using an extension like [pg\_squeeze](https://planetscale.com/docs/postgres/extensions/pg_squeeze), but is difficult to avoid during the migration itself.

When ready, queue and apply the changes.
You can check the "Changes" tab to see the status of the resize:

<img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/confirm-disk-size-change.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=ade2793422ccfcf0bd7e6c5ee2511a98" alt="Confirm disk size change" data-og-width="2626" width="2626" data-og-height="1264" height="1264" data-path="docs/postgres/imports/confirm-disk-size-change.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/confirm-disk-size-change.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=b193023eae6603ca54d2d21d4b8cf3ce 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/confirm-disk-size-change.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=36f59f9028dc0674d68594d7efbfd912 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/confirm-disk-size-change.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=92d471e9e4afa9de0dc7ff1e9adcf2e3 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/confirm-disk-size-change.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=f237cdd75c3a3c243b7b6b4c77fdba61 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/confirm-disk-size-change.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=99e53e615c6ca48b396d0b0ada146cf7 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/confirm-disk-size-change.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=226ed370d1291db934cd6fbda032d633 2500w" />

Wait for it to indicate completion.

If you are importing to a Metal database, you must choose a disk size when first creating your database.
You should launch your cluster with a disk size at least 50% larger than the storage used by your current source database (150% of the existing total).

As an example, if you need to import a 330 GB database onto a PlanetScale `M-160` there are three storage sizes available:

<img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/metal-disk-size.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=3c7924b6c3c1c712eb2772b9c16adb41" alt="Metal disk size" data-og-width="2074" width="2074" data-og-height="1812" height="1812" data-path="docs/postgres/imports/metal-disk-size.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/metal-disk-size.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=d2db5c8f73a3d3e450aa7410d517966a 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/metal-disk-size.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=b0c63716f22d784102107f89435f90c5 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/metal-disk-size.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=83ccb63b153d07c065f434a026156c32 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/metal-disk-size.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=b70ee20b312291760b4851520c333006 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/metal-disk-size.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=23f72e5606dd17d426b63d1404436a59 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/metal-disk-size.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=9bb971a81d534a7ac24bde77932d9717 2500w" />

You should use the largest, 1.25TB option during the import.
After importing and cleaning up table bloat, you may be able to downsize to the 468 GB option.
Resizing is a no-downtime operation that can be performed on the [clusters](https://planetscale.com/docs/postgres/cluster-configuration) page.

## 3. Prepare the Neon database

You will need to enable logical replication to use this import guide. To do so, go to the settings page in your Neon project:

<img src="https://mintcdn.com/planetscale-cad1a68a/-T12oTnFGgHO6jM1/docs/postgres/imports/neon-dashboard-darkmode.png?fit=max&auto=format&n=-T12oTnFGgHO6jM1&q=85&s=3cf5c4d00d83164323817fa5d1d019f9" alt="Neon dashboard" data-og-width="3662" width="3662" data-og-height="1662" height="1662" data-path="docs/postgres/imports/neon-dashboard-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/-T12oTnFGgHO6jM1/docs/postgres/imports/neon-dashboard-darkmode.png?w=280&fit=max&auto=format&n=-T12oTnFGgHO6jM1&q=85&s=1182fb7f711c404debc1e4528a71ea90 280w, https://mintcdn.com/planetscale-cad1a68a/-T12oTnFGgHO6jM1/docs/postgres/imports/neon-dashboard-darkmode.png?w=560&fit=max&auto=format&n=-T12oTnFGgHO6jM1&q=85&s=d31474ea66c03ac9bd6853faf6c94261 560w, https://mintcdn.com/planetscale-cad1a68a/-T12oTnFGgHO6jM1/docs/postgres/imports/neon-dashboard-darkmode.png?w=840&fit=max&auto=format&n=-T12oTnFGgHO6jM1&q=85&s=52f37bee5706113a97b16c2314a0e3a5 840w, https://mintcdn.com/planetscale-cad1a68a/-T12oTnFGgHO6jM1/docs/postgres/imports/neon-dashboard-darkmode.png?w=1100&fit=max&auto=format&n=-T12oTnFGgHO6jM1&q=85&s=fabe4a644db5f9afb22daf1453765374 1100w, https://mintcdn.com/planetscale-cad1a68a/-T12oTnFGgHO6jM1/docs/postgres/imports/neon-dashboard-darkmode.png?w=1650&fit=max&auto=format&n=-T12oTnFGgHO6jM1&q=85&s=ff433d48241c6af5960344789d8c132b 1650w, https://mintcdn.com/planetscale-cad1a68a/-T12oTnFGgHO6jM1/docs/postgres/imports/neon-dashboard-darkmode.png?w=2500&fit=max&auto=format&n=-T12oTnFGgHO6jM1&q=85&s=fe80946ff355350b096a3d1a19a95a27 2500w" />

Navigate to the "Logical Replication" section and ensure it is enabled.

<Warning>
  Note the warnings in the Neon dashboard. Changing this setting will sever all existing connections and restart all computes.
</Warning>

<img src="https://mintcdn.com/planetscale-cad1a68a/-T12oTnFGgHO6jM1/docs/postgres/imports/neon-logical-replication-darkmode.png?fit=max&auto=format&n=-T12oTnFGgHO6jM1&q=85&s=ed0577857d38927b37cc7bcbd2b4ac82" alt="Neon logical replication" data-og-width="3662" width="3662" data-og-height="1994" height="1994" data-path="docs/postgres/imports/neon-logical-replication-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/-T12oTnFGgHO6jM1/docs/postgres/imports/neon-logical-replication-darkmode.png?w=280&fit=max&auto=format&n=-T12oTnFGgHO6jM1&q=85&s=59779671c1532b2c8915d3a75e1592f9 280w, https://mintcdn.com/planetscale-cad1a68a/-T12oTnFGgHO6jM1/docs/postgres/imports/neon-logical-replication-darkmode.png?w=560&fit=max&auto=format&n=-T12oTnFGgHO6jM1&q=85&s=915ad1cefbd205b3b19538fcd751985a 560w, https://mintcdn.com/planetscale-cad1a68a/-T12oTnFGgHO6jM1/docs/postgres/imports/neon-logical-replication-darkmode.png?w=840&fit=max&auto=format&n=-T12oTnFGgHO6jM1&q=85&s=4584687f11a0fc07f04080dd79a72176 840w, https://mintcdn.com/planetscale-cad1a68a/-T12oTnFGgHO6jM1/docs/postgres/imports/neon-logical-replication-darkmode.png?w=1100&fit=max&auto=format&n=-T12oTnFGgHO6jM1&q=85&s=1869c547ff123a55219755c3f09e1542 1100w, https://mintcdn.com/planetscale-cad1a68a/-T12oTnFGgHO6jM1/docs/postgres/imports/neon-logical-replication-darkmode.png?w=1650&fit=max&auto=format&n=-T12oTnFGgHO6jM1&q=85&s=2715bf1fb80870054c9ed755d15fd7c9 1650w, https://mintcdn.com/planetscale-cad1a68a/-T12oTnFGgHO6jM1/docs/postgres/imports/neon-logical-replication-darkmode.png?w=2500&fit=max&auto=format&n=-T12oTnFGgHO6jM1&q=85&s=e944fff9e6de75cc158a5cfd040f6248 2500w" />

You can confirm that the `wal_level` is set to `logical` by running `SHOW wal_level;` on your Neon database:

```sql  theme={null}
neondb=> SHOW wal_level;
 wal_level
-----------
 logical
```

If you see a result other than `logical`, then it is not configured correctly. Once this is enabled, return to the dashboard.

For these instructions, you'll need to connect to Neon with a role that has permissions to create replication publications and read all data. Your default role that was generated by Neon when you first created your database should suffice here. To get these connection credentials to use for the migration, click on the "Connect" button from your project dashboard:

<img src="https://mintcdn.com/planetscale-cad1a68a/-T12oTnFGgHO6jM1/docs/postgres/imports/neon-dashboard-connect-darkmode.png?fit=max&auto=format&n=-T12oTnFGgHO6jM1&q=85&s=040cd92464f41783d3e64592752a1e4d" alt="Neon dashboard connect" data-og-width="3662" width="3662" data-og-height="1662" height="1662" data-path="docs/postgres/imports/neon-dashboard-connect-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/-T12oTnFGgHO6jM1/docs/postgres/imports/neon-dashboard-connect-darkmode.png?w=280&fit=max&auto=format&n=-T12oTnFGgHO6jM1&q=85&s=fccd40652619be1eef63f14f6b0dd91a 280w, https://mintcdn.com/planetscale-cad1a68a/-T12oTnFGgHO6jM1/docs/postgres/imports/neon-dashboard-connect-darkmode.png?w=560&fit=max&auto=format&n=-T12oTnFGgHO6jM1&q=85&s=d86bbdcfa89b3ecc2bc256916de7f732 560w, https://mintcdn.com/planetscale-cad1a68a/-T12oTnFGgHO6jM1/docs/postgres/imports/neon-dashboard-connect-darkmode.png?w=840&fit=max&auto=format&n=-T12oTnFGgHO6jM1&q=85&s=8101ed93471b75051d4a5e93f2225f5b 840w, https://mintcdn.com/planetscale-cad1a68a/-T12oTnFGgHO6jM1/docs/postgres/imports/neon-dashboard-connect-darkmode.png?w=1100&fit=max&auto=format&n=-T12oTnFGgHO6jM1&q=85&s=16acf0517791f56a7bc8595970dccca5 1100w, https://mintcdn.com/planetscale-cad1a68a/-T12oTnFGgHO6jM1/docs/postgres/imports/neon-dashboard-connect-darkmode.png?w=1650&fit=max&auto=format&n=-T12oTnFGgHO6jM1&q=85&s=5c611ea3107916c276a7b755c69f1b9a 1650w, https://mintcdn.com/planetscale-cad1a68a/-T12oTnFGgHO6jM1/docs/postgres/imports/neon-dashboard-connect-darkmode.png?w=2500&fit=max&auto=format&n=-T12oTnFGgHO6jM1&q=85&s=012c2c9b07b2bc6a9f6ab2da202a6771 2500w" />

In the connection modal that appears, ensure you have "connection pooling" disabled.

<img src="https://mintcdn.com/planetscale-cad1a68a/-T12oTnFGgHO6jM1/docs/postgres/imports/neon-connect-darkmode.png?fit=max&auto=format&n=-T12oTnFGgHO6jM1&q=85&s=f8b4f9e54af797cf6594b1d65adb8501" alt="Neon connect" data-og-width="2254" width="2254" data-og-height="1740" height="1740" data-path="docs/postgres/imports/neon-connect-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/-T12oTnFGgHO6jM1/docs/postgres/imports/neon-connect-darkmode.png?w=280&fit=max&auto=format&n=-T12oTnFGgHO6jM1&q=85&s=ed66ea629ddc1d25166e362ed870314c 280w, https://mintcdn.com/planetscale-cad1a68a/-T12oTnFGgHO6jM1/docs/postgres/imports/neon-connect-darkmode.png?w=560&fit=max&auto=format&n=-T12oTnFGgHO6jM1&q=85&s=a4eed4b90d402864512f7c2551733676 560w, https://mintcdn.com/planetscale-cad1a68a/-T12oTnFGgHO6jM1/docs/postgres/imports/neon-connect-darkmode.png?w=840&fit=max&auto=format&n=-T12oTnFGgHO6jM1&q=85&s=fef28b3f26b97a0359a729de1a0e1598 840w, https://mintcdn.com/planetscale-cad1a68a/-T12oTnFGgHO6jM1/docs/postgres/imports/neon-connect-darkmode.png?w=1100&fit=max&auto=format&n=-T12oTnFGgHO6jM1&q=85&s=19afc09ce3417ad5e65f2908aa8ae2b8 1100w, https://mintcdn.com/planetscale-cad1a68a/-T12oTnFGgHO6jM1/docs/postgres/imports/neon-connect-darkmode.png?w=1650&fit=max&auto=format&n=-T12oTnFGgHO6jM1&q=85&s=79b0e3a2e4fa0073d5d1ecf65e41a77b 1650w, https://mintcdn.com/planetscale-cad1a68a/-T12oTnFGgHO6jM1/docs/postgres/imports/neon-connect-darkmode.png?w=2500&fit=max&auto=format&n=-T12oTnFGgHO6jM1&q=85&s=315ce4e34777a258c95de9c8320af21c 2500w" />

The credentials displayed here are the ones you can use for the migration. For the rest of this guide, we'll assume these are saved into the following environment variables:

```bash  theme={null}
NEON_USERNAME=XXXX
NEON_PASSWORD=XXXX
NEON_HOST=XXX
NEON_DBNAME=XXX
```

## 4. Copy schema from Neon to PlanetScale

Before we begin migrating data, we first must copy the schema from Neon to PlanetScale. We do this as a distinct set of steps using `pg_dump`.

<Warning>
  You should not make any schema changes during the migration process. You may continue to select, insert, update, and delete data, keeping your application fully online during this process.
</Warning>

Run the below command to take a snapshot of the schema for the `$NEON_DBNAME` database that you want to migrate:

```bash  theme={null}
PGPASSWORD=$NEON_PASSWORD \
pg_dump -h $NEON_HOST \
        -p 5432 \
        -U $NEON_USERNAME \
        -d $NEON_DBNAME \
        --schema-only \
        --no-owner \
        --no-privileges \
        -f schema.sql
```

This saves the schema into a file named `schema.sql`.

<Note>
  The above command will dump the tables for all schemas in the current database. If you want to migrate only one specific schema, you can add the `--schema=SCHEMA_NAME` option.
</Note>

The schema then needs to be loaded into your new PlanetScale database:

```bash  theme={null}
PGPASSWORD=$PLANETSCALE_PASSWORD \
psql -h $PLANETSCALE_HOST \
     -p 5432 \
     -U $PLANETSCALE_USERNAME \
     -d $PLANETSCALE_DBNAME \
     -f schema.sql
```

In the output of this command, you might see some error messages of the form:

```
psql:schema.sql:LINE: ERROR: DESCRIPTION
```

You should inspect these to see if they are of any concern. You can [reach out to our support](https://planetscale.com/contact) if you need assistance at this step.

## 5. Set up logical replication

We now must create a `PUBLICATION` on Neon that the PlanetScale database can subscribe to for data copying and replication.

To create a publication for all tables in all schemas of the current database, connect to the Neon database:

```
PGPASSWORD=$NEON_PASSWORD \
PGSSLMODE=require \
PGCHANNELBINDING=require \
psql \
  -h $NEON_HOST \
  -U $NEON_USERNAME \
  -p 5432 \
  $NEON_DBNAME
```

Then run the following command:

```sql  theme={null}
CREATE PUBLICATION replicate_to_planetscale FOR ALL TABLES;
```

You should see this output if it created correctly:

```sql  theme={null}
CREATE PUBLICATION
```

<Note>
  To publish changes for only one specific schema, run the following query:

  ```sql  theme={null}
  SELECT 'CREATE PUBLICATION replicate_to_planetscale FOR TABLE ' ||
         string_agg(format('%I.%I', schemaname, tablename), ', ') || ';'
  FROM pg_tables
  WHERE schemaname = 'YOUR_SCHEMA_NAME';
  ```

  This will generate a query that looks like this:

  ```sql  theme={null}
  CREATE PUBLICATION replicate_to_planetscale FOR TABLE
    public.table_1,
    public.table_2,
    ...
    public.table_n;
  ```

  You can then copy/paste this and execute on Neon. This will create a publication that only publishes changes for the tables in `YOUR_SCHEMA_NAME`
</Note>

After creating the publication on Neon, we need to tell PlanetScale to `SUBSCRIBE` to this publication.

```sql  theme={null}
PGPASSWORD=$PLANETSCALE_PASSWORD psql \
  -h $PLANETSCALE_HOST \
  -U $PLANETSCALE_USERNAME \
  -p 5432 $PLANETSCALE_DBNAME \
  -c "
CREATE SUBSCRIPTION replicate_from_neon
CONNECTION 'host=$NEON_HOST dbname=$NEON_DBNAME user=$NEON_USERNAME password=$NEON_PASSWORD sslmode=require channel_binding=require'
PUBLICATION replicate_to_planetscale WITH (copy_data = true);"
```

Data copying and replication will begin at this point. To check in on the row counts for the tables, you can run a query like this on your source and target databases:

```sql  theme={null}
SELECT table_name, row_count FROM (
  SELECT 'table_name_1' as table_name, COUNT(*) as row_count FROM table_name_1 UNION ALL
  SELECT 'table_name_2', COUNT(*) FROM table_name_2 UNION ALL
  ...
  SELECT 'table_name_N', COUNT(*) FROM table_name_N
) t ORDER BY table_name;
```

When the row counts match (or nearly match) you can begin testing and prepare for your application to cutover to use PlanetScale.

## 6. Handling sequences

Logical replication is great at migrating all of your data over to PlanetScale. However, logical replication does *not* synchronize the `nextval` values for [sequences](https://www.postgresql.org/docs/current/sql-createsequence.html) in your database. Sequences are often used for things like auto incrementing IDs, so it's important to ensure we update this before you switch your traffic to PlanetScale.

You can see all of the sequences and their corresponding `nextval`s on your source Neon database using this command:

```sql  theme={null}
SELECT schemaname, sequencename, last_value + increment_by AS next_value
FROM pg_sequences;
```

An example output from this command:

```sql  theme={null}
 schemaname |   sequencename   | next_value
------------+------------------+------------
 public     | users_id_seq     |        105
 public     | posts_id_seq     |       1417
 public     | followers_id_seq |       3014
```

What this means is that we have three sequences in our database. In this case, they are all being used for auto-incrementing primary keys. The `nextval` for the `users_id_seq` is 105, the `nextval` for the `posts_id_seq` is 1417, and the `nextval` for the `followers_id_seq` is 3014. If you run the same query on your new PlanetScale database, you'll see something like:

```sql  theme={null}
 schemaname |   sequencename   | next_value
------------+------------------+------------
 public     | users_id_seq     |          0
 public     | posts_id_seq     |          0
 public     | followers_id_seq |          0
```

If you switch traffic over to PlanetScale in this state, you'll likely encounter errors when inserting new rows:

```sql  theme={null}
ERROR:  duplicate key value violates unique constraint "XXXX"
DETAIL:  Key (id)=(ZZZZ) already exists.
```

Before switching over, you need to progress all of these sequences forward so that the `nextval`s produced will be greater than any of the values previously produced on the source Neon database, avoiding constraint violations. There are several approaches you can take for this. A simple way to solve the problem is to first run this query on your source Neon database:

```sql  theme={null}
SELECT 'SELECT setval(''' || schemaname || '.' || sequencename || ''', '
       || (last_value + 10000) || ');' AS query
FROM pg_sequences;
```

This will generate a sequence of queries that will advance the `nextval` by 10,000 for each sequence:

```sql  theme={null}
                      query
--------------------------------------------------
 SELECT setval('public.users_id_seq', 10104);
 SELECT setval('public.posts_id_seq', 11416);
 SELECT setval('public.followers_id_seq', 13013);
```

You would then execute these on your target PlanetScale database. You need to ensure you advance each sequence far enough forward so that the sequences in the Neon database will not reach these `nextval`s before you switch your primary to PlanetScale. For tables that have a high insertion rate, you might need to increase this by a larger value (say, 100,000 or 1,000,000).

## 7. Cutting over to PlanetScale

Before you cutover, it's good to have confidence that the replication between Neon and PlanetScale is fully caught up. You can do this using Log Sequence Numbers (LSNs). The goal is to see these match up between the source Neon database and the target PlanetScale database exactly. If they don't, it indicates that the PlanetScale database is not fully caught-up with the changes happening on Neon.

You can run this on Neon to see the current LSN:

```sql  theme={null}
postgres=> SELECT pg_current_wal_lsn();
 pg_current_wal_lsn
--------------------
 0/703FE460
```

Then on PlanetScale, you would run the following query to check for a match:

```sql  theme={null}
postgres=> SELECT received_lsn, latest_end_lsn
             FROM pg_stat_subscription
             WHERE subname = 'replicate_from_neon';
 received_lsn | latest_end_lsn
--------------+----------------
 0/703FE460   | 0/703FE460
```

Once you are comfortable that all your data has successfully copied over and replication is sufficiently caught up, it's time to switch to PlanetScale. In your application code, prepare the cutover by changing the database connection credentials to go to PlanetScale rather than Neon. Then, you can deploy this new version of your application, which will begin using PlanetScale as your primary database.

After doing this, new rows written to PlanetScale will not be reverse-replicated to Neon. Thus, it's important to ensure you are fully ready for the cutover at this point.

Once this is complete, PlanetScale is now your primary database! We recommend you keep your old database around for at least a few days, just in case you discover any data or schemas you forgot to copy over to PlanetScale.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Postgres Imports
Source: https://planetscale.com/docs/postgres/imports/postgres-imports

For customers looking to migrate their Postgres databases to PlanetScale Postgres, you have several options for how to make this the smoothest event for your business.

Use this guide if you are importing from platforms like Aurora Postgres, RDS Postgres, Neon, Supabase, and other Postgres instances.

<Note>
  If you have IP restrictions in place on our source database and need to grant a set of IP addresses access, see our [Import public IP addresses documentation](/docs/postgres/imports/import-ips).
</Note>

## Migration Options Overview

PlanetScale Postgres provides three primary migration approaches to suit different business requirements, database sizes, and downtime tolerances:

<Columns cols={2}>
  <Card title="Migrate using pgdump/restore" icon="recycle" horizontal href="/docs/postgres/imports/postgres-migrate-dumprestore" />

  <Card title="Migrate using logical replication" icon="laptop" horizontal href="/docs/postgres/imports/postgres-migrate-walstream" />

  <Card title="Migrate using Amazon DMS" icon="aws" horizontal href="/docs/postgres/imports/postgres-migrate-dms" />
</Columns>

You can also utilize our [migration scripts](https://github.com/planetscale/migration-scripts/tree/main/postgres-direct) directly if you prefer. These scripts can be used to migrate straight from any Postgres source that supports logical replication into PlanetScale Postgres.

### 1. pg\_dump and Restore

The [pg\_dump](https://www.postgresql.org/docs/current/app-pgdump.html) method is the simplest approach for migrating smaller PostgreSQL databases. This method involves creating a full backup of your source database using PostgreSQL's built-in `pg_dump` utility and then restoring it to your PlanetScale Postgres database.

**How it works:**

* Export your entire database schema and data using [`pg_dump`](https://www.postgresql.org/docs/current/app-pgdump.html)
* Transfer the dump file to PlanetScale Postgres
* Restore the database using [`pg_restore`](https://www.postgresql.org/docs/current/app-pgrestore.html) or [`psql`](https://www.postgresql.org/docs/current/app-psql.html)

This approach is straightforward and doesn't require additional infrastructure, making it ideal for databases that can tolerate some downtime during the migration process.

### 2. WAL Log Replication

[Write-Ahead Logging (WAL)](https://www.postgresql.org/docs/current/wal-intro.html) replication provides a near-zero downtime migration by continuously streaming transaction logs from your source PostgreSQL database to PlanetScale Postgres.

**How it works:**

* Set up [logical replication](https://www.postgresql.org/docs/current/logical-replication.html) between your source database and PlanetScale Postgres
* Stream [WAL logs](https://www.postgresql.org/docs/current/wal-intro.html) in real-time to keep the target database synchronized
* Perform a quick cutover when ready to switch to the new database

This method is ideal for production databases that require minimal downtime and need to maintain data consistency during the migration process.

### 3. Amazon Database Migration Service (DMS)

[Amazon Database Migration Service (DMS)](https://aws.amazon.com/dms/) provides a managed migration service that can handle complex database migrations with built-in monitoring, error handling, and data validation.

**How it works:**

* Configure [DMS replication instance](https://docs.aws.amazon.com/dms/latest/userguide/CHAP_ReplicationInstance.html) and [endpoints](https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Endpoints.html) for source and target databases
* Set up full load and [change data capture (CDC)](https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Task.CDC.html) for ongoing replication
* Monitor the migration process through the [AWS console](https://console.aws.amazon.com/dms/)
* Perform cutover when the target database is fully synchronized

DMS is particularly useful for large, complex databases that require robust error handling, [data transformation](https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TableMapping.SelectionTransformation.Transformations.html), and detailed migration monitoring.

## Migration Method Comparison

| Feature                             | pg\_dump & Restore                                                                 | WAL Log Replication                             | Amazon DMS                                                                                                                                                               |
| :---------------------------------- | :--------------------------------------------------------------------------------- | :---------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Best For**                        | Small to medium databases                                                          | Production databases requiring minimal downtime | Large, complex databases with transformation needs                                                                                                                       |
| **Downtime**                        | High (hours to days)                                                               | Minimal (minutes)                               | Minimal to none                                                                                                                                                          |
| **Setup Complexity**                | Low                                                                                | Medium                                          | High                                                                                                                                                                     |
| **Infrastructure Requirements**     | None (built-in tools)                                                              | Source DB configuration changes                 | AWS DMS resources                                                                                                                                                        |
| **Data Consistency**                | [Point-in-time snapshot](https://www.postgresql.org/docs/current/backup-dump.html) | Real-time sync                                  | Real-time sync with [validation](https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Validating.html)                                                                  |
| **Cost**                            | Free (built-in tools)                                                              | Low (minimal resources)                         | Medium (AWS DMS charges)                                                                                                                                                 |
| **Database Size Limit**             | Limited by storage/time                                                            | No practical limit                              | No practical limit                                                                                                                                                       |
| **Schema Changes During Migration** | Not supported                                                                      | Limited support                                 | Full support                                                                                                                                                             |
| **Data Transformation**             | None                                                                               | Limited                                         | Extensive [transformation rules](https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TableMapping.SelectionTransformation.Transformations.html) |
| **Error Handling**                  | Manual intervention required                                                       | Basic retry mechanisms                          | Automated error handling and recovery                                                                                                                                    |
| **Rollback Options**                | Manual restore from backup                                                         | Stop replication, switch back                   | Stop [DMS task](https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.html), failback procedures                                                                   |

## Migration Considerations

Before migrating your PostgreSQL database to PlanetScale Postgres, there are several important factors to consider to ensure a smooth migration process.

### PostgreSQL Version Compatibility

PlanetScale Postgres supports [PostgreSQL 18](https://www.postgresql.org/docs/18/index.html) and [PostgreSQL 17](https://www.postgresql.org/docs/17/index.html). If your source database is running an older version of PostgreSQL, you should verify compatibility and consider upgrading your source database before migration, or plan for potential compatibility issues during the migration process.

**Version considerations:**

* **PostgreSQL 18**: Fully supported
* **PostgreSQL 17**: Fully supported
* **Earlier versions**: May require additional testing and validation
* **Version-specific features**: Newer features may not be available in older versions

For detailed information about PostgreSQL version differences, refer to the [PostgreSQL 18 release notes](https://www.postgresql.org/docs/18/release.html) and [PostgreSQL 17 release notes](https://www.postgresql.org/docs/17/release.html).

### Upgrading from PostgreSQL 17 to 18 on PlanetScale

We don't currently offer an automated in-place major version upgrade from PostgreSQL 17 to 18.

You can perform an online upgrade by migrating from your existing PlanetScale Postgres 17 database to a new PostgreSQL 18 database using our import guides:

* For near-zero downtime with logical replication: follow the [WAL replication guide](/docs/postgres/imports/postgres-migrate-walstream)
* For simpler/smaller databases: use [pg\_dump/restore](/docs/postgres/imports/postgres-migrate-dumprestore)
* If you prefer a managed migration service: use [Amazon DMS](/docs/postgres/imports/postgres-migrate-dms)

At a high level, the process is:

1. Create a new PostgreSQL 18 database (same region and similar configuration).
2. Use one of the import methods above to sync data from your PostgreSQL 17 database.
3. Validate data and application behavior, then update your application connection string to the new database.
4. Decommission the old PostgreSQL 17 database when you're ready.

### Extension Support

PlanetScale Postgres will have **limited extension support** at launch. Many PostgreSQL databases rely on extensions to provide additional functionality, and not all extensions will be available initially.

**Important notes about extensions:**

* Review your current database's installed extensions using `\dx` in psql or by querying `pg_extension`
* Identify which extensions are critical to your application's functionality
* Plan for alternative approaches if critical extensions are not supported
* Test your application thoroughly in a staging environment before migrating production data

Common extensions that may require attention:

* [PostGIS](https://postgis.net/) for geospatial data
* [pg\_stat\_statements](https://www.postgresql.org/docs/current/pgstatstatements.html) for query statistics
* [UUID extensions](https://www.postgresql.org/docs/current/uuid-ossp.html)
* [Full-text search extensions](https://www.postgresql.org/docs/current/textsearch.html)

### Third-Party Enhancements and Tools

PlanetScale Postgres does **not support third-party enhancements** to PostgreSQL's core capabilities at launch. This includes:

**Currently unsupported:**

* Custom background workers
* Third-party connection poolers (like [PgBouncer](https://www.pgbouncer.org/))
* External procedural languages beyond the standard ones
* Third-party monitoring tools that require database-level access
* Custom shared libraries or plugins

<Info>
  PlanetScale Postgres includes connection pooling by default.
</Info>

**Alternatives to consider:**

* Migrate custom functions to standard PostgreSQL syntax where possible
* Utilize Metrics, Insights, and 3rd party integrations for monitoring (LINKS HERE)

### Pre-Migration Checklist

Before starting your migration:

1. **Database Assessment**
   * Document your current PostgreSQL version
   * List all installed extensions and their usage
   * Identify any third-party tools or enhancements in use
   * Review custom functions and stored procedures

2. **Compatibility Testing**
   * Test your application against your target PostgreSQL version (18 or 17)
   * Validate that critical extensions are supported or have alternatives
   * Identify any custom code that may need modification

3. **Migration Planning**
   * Choose the appropriate migration method based on your requirements
   * Plan for testing in a staging environment
   * Prepare rollback procedures if needed
   * Schedule migration during low-traffic periods if possible

For the most up-to-date information on supported features and extensions, refer to the [PostgreSQL documentation](https://www.postgresql.org/docs/current/) and PlanetScale Postgres release notes.

# You got this!

Follow the migration guide that's right for you:

<Columns cols={2}>
  <Card title="Migrate using pgdump/restore" icon="recycle" horizontal href="/docs/postgres/imports/postgres-migrate-dumprestore" />

  <Card title="Migrate using WAL replication" icon="laptop" horizontal href="/docs/postgres/imports/postgres-migrate-walstream" />

  <Card title="Migrate using Amazon DMS" icon="aws" horizontal href="/docs/postgres/imports/postgres-migrate-dms" />
</Columns>

If you encounter issues while importing from a Postgres database, please [reach out to support](https://planetscale.com/contact?initial=support) for assistance.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Postgres Imports - Amazon DMS
Source: https://planetscale.com/docs/postgres/imports/postgres-migrate-dms



[Amazon Database Migration Service (DMS)](https://aws.amazon.com/dms/) provides a managed migration service that can handle complex database migrations with built-in monitoring, error handling, and data validation. This method is ideal for large, complex databases that require robust migration capabilities.

## Overview

This migration method involves:

<Steps>
  <Step>
    **Pre-migration schema setup** (strongly recommended for production)
  </Step>

  <Step>
    Setting up AWS DMS replication instance and endpoints
  </Step>

  <Step>
    Configuring source and target database connections
  </Step>

  <Step>
    Creating and running migration tasks with full load and CDC
  </Step>

  <Step>
    Monitoring migration progress and performing cutover
  </Step>
</Steps>

<Warning>
  **Critical: AWS DMS Schema Object Limitations**

  AWS DMS **only migrates table data and primary keys**. All other PostgreSQL schema objects must be handled separately:

  * Secondary indexes
  * Sequences and their current values
  * Views, functions, and stored procedures
  * Constraints (foreign keys, unique, check)
  * Triggers and custom data types

  Deploy your complete schema to PlanetScale BEFORE starting DMS migration to preserve performance and avoid application errors.
</Warning>

<Note>
  This method requires an AWS account and will incur AWS DMS charges. Review [AWS DMS pricing](https://aws.amazon.com/dms/pricing/) before proceeding.
</Note>

<Note>
  **For Aurora users**: Consider the [Aurora to PlanetScale CloudFormation & DMS tutorial](/docs/postgres/imports/aurora-dms) for a fully automated approach using CloudFormation templates and Step Functions workflows instead of manual DMS setup.
</Note>

## Prerequisites

Before starting the migration:

* Active AWS account with appropriate DMS permissions
* Source PostgreSQL database accessible from AWS (consider VPC configuration)
* Connection details for your PlanetScale Postgres database from the console
* Ensure the disk on your PlanetScale database has at least 150% of the capacity of your source database.
  If you are migrating to a PlanetScale database backed by network-attached storage, you can [resize](https://planetscale.com/docs/postgres/cluster-configuration/cluster-storage) your disk manually by setting the "Minimum disk size."
  If you are using Metal, you will need to select a size when first creating your database.
  For example, if your source database is 330GB, you should have at least 500GB of storage available on PlanetScale.
* Understanding of your data transformation requirements (if any)
* Network connectivity between AWS and both source and target databases

## Step 1: Pre-Migration Schema Setup

Deploy your complete schema to PlanetScale BEFORE starting DMS migration. This ensures optimal performance and prevents application errors.

### Extract and Apply Schema

<Steps>
  <Step>
    Extract your complete schema from the source PostgreSQL database:

    ```bash  theme={null}
    pg_dump -h your-postgres-host -p 5432 -U username -d database \
            --schema-only --no-owner --no-privileges \
            --exclude-table-data='*' -f schema_objects.sql
    ```
  </Step>

  <Step>
    Apply the schema to PlanetScale:

    ```bash  theme={null}
    psql -h your-planetscale-host -p 5432 -U username -d database -f schema_objects.sql
    ```
  </Step>
</Steps>

<Note>
  **Foreign Key Constraints**

  If the schema application fails due to foreign key constraint issues, you can temporarily remove them from the schema file and apply them after DMS completes the data migration.
</Note>

### Verify Schema Application

Quickly verify your schema was applied successfully:

```sql  theme={null}
-- Check that tables and sequences exist
SELECT
    (SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public') as tables,
    (SELECT COUNT(*) FROM information_schema.sequences WHERE sequence_schema = 'public') as sequences,
    (SELECT COUNT(*) FROM pg_indexes WHERE schemaname = 'public') as indexes;
```

## Step 2: Set Up AWS DMS

### Create DMS Replication Instance

<Steps>
  <Step>
    Navigate to the [AWS DMS Console](https://console.aws.amazon.com/dms/)
  </Step>

  <Step>
    Click "Create replication instance"
  </Step>

  <Step>
    Configure the instance:

    ```yaml  theme={null}
    Name: planetscale-postgres-migration
    Description: Migration to PlanetScale Postgres
    Instance class: dms.t3.medium (adjust based on your needs)
    Engine version: Latest available
    VPC: Select appropriate VPC
    Multi-AZ: No (for cost savings, Yes for production)
    Publicly accessible: Yes (if needed for connectivity)
    ```
  </Step>
</Steps>

### Configure Security Groups

Ensure your replication instance can connect to:

* Source PostgreSQL database (port 5432)
* PlanetScale Postgres (port 5432)
* Internet for PlanetScale connectivity

## Step 3: Create Source Endpoint

### Configure PostgreSQL source endpoint:

<Steps>
  <Step>
    In DMS Console, go to "Endpoints" > "Create endpoint"
  </Step>

  <Step>
    Configure source endpoint:

    ```yaml  theme={null}
    Endpoint type: Source endpoint
    Endpoint identifier: postgres-source
    Source engine: postgres
    Server name: your-postgres-host
    Port: 5432
    Database name: your-database-name
    Username: your-username
    Password: your-password
    ```
  </Step>
</Steps>

### Advanced settings for PostgreSQL:

```yaml  theme={null}
Extra connection attributes:
pluginName=test_decoding;
slotName=dms_slot_planetscale;
captureDDLs=false;
maxFileSize=32768;
```

## Step 4: Create Target Endpoint

### Configure PlanetScale Postgres target endpoint:

<Steps>
  <Step>
    Create target endpoint with PlanetScale connection details:

    ```yaml  theme={null}
    Endpoint type: Target endpoint
    Endpoint identifier: planetscale-target
    Target engine: postgres
    Server name: [from PlanetScale console]
    Port: [from PlanetScale console]
    Database name: [from PlanetScale console]
    Username: [from PlanetScale console]
    Password: [from PlanetScale console]
    ```
  </Step>
</Steps>

### SSL Configuration:

```yaml  theme={null}
SSL mode: require
```

## Step 5: Test Endpoints

<Steps>
  <Step>
    Select your source endpoint and click "Test connection"
  </Step>

  <Step>
    Select your target endpoint and click "Test connection"
  </Step>

  <Step>
    Ensure both tests pass before proceeding
  </Step>
</Steps>

## Step 6: Create Migration Task

### Configure the migration task:

<Steps>
  <Step>
    Go to "Database migration tasks" > "Create task"
  </Step>

  <Step>
    Configure task settings:

    ```yaml  theme={null}
    Task identifier: postgres-to-planetscale
    Replication instance: planetscale-postgres-migration
    Source database endpoint: postgres-source
    Target database endpoint: planetscale-target
    Migration type: Migrate existing data and replicate ongoing changes
    ```
  </Step>
</Steps>

### Task Settings

**Option 1: Schema-first approach** (recommended for production):

```json expandable theme={null}
{
  "TargetMetadata": {
    "TargetSchema": "",
    "SupportLobs": true,
    "FullLobMode": true,
    "LobChunkSize": 32,
    "LimitedSizeLobMode": false,
    "LobMaxSize": 0,
    "InlineLobMaxSize": 32,
    "BatchApplyEnabled": true,
    "TaskRecoveryTableEnabled": false
  },
  "FullLoadSettings": {
    "TargetTablePrepMode": "DO_NOTHING",
    "CreatePkAfterFullLoad": false,
    "StopTaskCachedChangesApplied": false,
    "MaxFullLoadSubTasks": 8,
    "TransactionConsistencyTimeout": 600,
    "CommitRate": 10000,
    "FullLoadIgnoreConflicts": true
  },
  "ValidationSettings": {
    "EnableValidation": true,
    "ValidationMode": "ROW_LEVEL",
    "ThreadCount": 5,
    "FailureMaxCount": 10000,
    "TableFailureMaxCount": 1000
  },
  "ChangeProcessingTuning": {
    "StatementCacheSize": 50,
    "CommitTimeout": 5,
    "BatchApplyPreserveTransaction": true,
    "BatchApplyTimeoutMin": 1,
    "BatchApplyTimeoutMax": 30,
    "MinTransactionSize": 5000,
    "MemoryKeepTime": 60,
    "BatchApplyMemoryLimit": 1000,
    "MemoryLimitTotal": 2048
  },
  "Logging": {
    "EnableLogging": true,
    "LogComponents": [
      {
        "Id": "TRANSFORMATION",
        "Severity": "LOGGER_SEVERITY_DEFAULT"
      },
      {
        "Id": "SOURCE_UNLOAD",
        "Severity": "LOGGER_SEVERITY_DEFAULT"
      },
      {
        "Id": "TARGET_LOAD",
        "Severity": "LOGGER_SEVERITY_DEFAULT"
      },
      {
        "Id": "SOURCE_CAPTURE",
        "Severity": "LOGGER_SEVERITY_DEFAULT"
      },
      {
        "Id": "TARGET_APPLY",
        "Severity": "LOGGER_SEVERITY_DEFAULT"
      }
    ]
  },
  "ControlTablesSettings": {
    "historyTimeslotInMinutes": 5,
    "ControlSchema": "",
    "HistoryTableEnabled": false,
    "SuspendedTablesTableEnabled": false,
    "StatusTableEnabled": false,
    "FullLoadExceptionTableEnabled": false
  }
}
```

**Option 2: Standard approach** (matches CloudFormation template):

```json expandable theme={null}
{
  "TargetMetadata": {
    "SupportLobs": true,
    "FullLobMode": true,
    "LobChunkSize": 32,
    "BatchApplyEnabled": true,
    "TaskRecoveryTableEnabled": false
  },
  "FullLoadSettings": {
    "TargetTablePrepMode": "DROP_AND_CREATE",
    "CreatePkAfterFullLoad": false,
    "MaxFullLoadSubTasks": 8,
    "TransactionConsistencyTimeout": 600,
    "CommitRate": 10000,
    "FullLoadIgnoreConflicts": true
  },
  "ValidationSettings": {
    "EnableValidation": true,
    "ValidationMode": "ROW_LEVEL",
    "ThreadCount": 5,
    "FailureMaxCount": 10000,
    "TableFailureMaxCount": 1000
  },
  "Logging": {
    "EnableLogging": true,
    "LogComponents": [
      {
        "Id": "SOURCE_UNLOAD",
        "Severity": "LOGGER_SEVERITY_DEFAULT"
      },
      {
        "Id": "SOURCE_CAPTURE",
        "Severity": "LOGGER_SEVERITY_DEFAULT"
      },
      {
        "Id": "TARGET_LOAD",
        "Severity": "LOGGER_SEVERITY_DEFAULT"
      },
      {
        "Id": "TARGET_APPLY",
        "Severity": "LOGGER_SEVERITY_DEFAULT"
      },
      {
        "Id": "TASK_MANAGER",
        "Severity": "LOGGER_SEVERITY_DEFAULT"
      }
    ]
  }
}
```

**Configuration Comparison:**

| Setting                | Schema-First | Standard          | Notes                               |
| ---------------------- | ------------ | ----------------- | ----------------------------------- |
| TargetTablePrepMode    | DO\_NOTHING  | DROP\_AND\_CREATE | Schema-first uses existing schema   |
| ChangeProcessingTuning | Included     | Not needed        | Extra optimization for manual setup |
| Logging Components     | 5 components | 5 components      | Both include all DMS components     |
| ValidationSettings     | Same         | Same              | Both use row-level validation       |

**When to use each approach:**

* **Schema-First**: Production systems, complex schemas, performance-critical applications
* **Standard**: Simple migrations, dev/test environments, when schema objects aren't critical during migration

## Step 7: Configure Table Mappings

### Basic table mapping (migrate all tables):

```json  theme={null}
{
  "rules": [
    {
      "rule-type": "selection",
      "rule-id": "1",
      "rule-name": "1",
      "object-locator": {
        "schema-name": "public",
        "table-name": "%"
      },
      "rule-action": "include",
      "filters": []
    }
  ]
}
```

### Advanced table mapping with transformations:

```json expandable theme={null}
{
  "rules": [
    {
      "rule-type": "selection",
      "rule-id": "1",
      "rule-name": "1",
      "object-locator": {
        "schema-name": "public",
        "table-name": "%"
      },
      "rule-action": "include"
    },
    {
      "rule-type": "transformation",
      "rule-id": "2",
      "rule-name": "2",
      "rule-target": "schema",
      "object-locator": {
        "schema-name": "public"
      },
      "rule-action": "rename",
      "value": "public"
    }
  ]
}
```

## Step 8: Start Migration Task

<Steps>
  <Step>
    Review all task configurations
  </Step>

  <Step>
    Click "Create task" to start the migration
  </Step>

  <Step>
    Monitor the task status in the DMS console
  </Step>
</Steps>

## Step 9: Monitor Migration Progress

### Key metrics to monitor:

* **Full load progress**: Percentage of tables loaded
* **CDC lag**: Latency between source and target
* **Error count**: Any migration errors
* **Throughput**: Records per second

### Using CloudWatch:

Set up [CloudWatch alarms](https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Monitoring.html) for:

* High CDC latency
* Migration errors
* Task failures

```bash  theme={null}
# CLI command to check task status
aws dms describe-replication-tasks \
    --filters Name=replication-task-id,Values=your-task-id
```

## Step 10: Verify Data Migration

### Check table counts and data integrity:

```sql  theme={null}
-- Run on both source and target databases
SELECT
    schemaname,
    tablename,
    n_tup_ins as estimated_rows,
    n_tup_upd as updated_rows,
    n_tup_del as deleted_rows
FROM pg_stat_user_tables
ORDER BY schemaname, tablename;
```

### Validate specific data:

```sql  theme={null}
-- Compare checksums for critical tables
SELECT count(*), md5(string_agg(column_name::text, ''))
FROM your_important_table
ORDER BY primary_key;
```

## Step 11: Prepare for Cutover

### Monitor CDC lag:

Ensure CDC latency is minimal (under 5 seconds) before cutover:

```sql  theme={null}
-- Check DMS validation status
SELECT * FROM awsdms_validation_failures_v1;
```

### Test application connectivity:

1. Create a read-only connection to PlanetScale Postgres
2. Test critical application queries with EXPLAIN ANALYZE
3. Verify performance matches expectations (indexes should be working)
4. Test sequence-dependent operations (INSERT operations)

## Step 12: Post-Migration Sequence Synchronization

After DMS completes, sequences need their values synchronized:

<Warning>
  **Critical: Sequence Synchronization**

  Sequence values must be set ahead of source database values to prevent duplicate key errors when applications start using PlanetScale.
</Warning>

### Get Current Sequence Values from Source

```sql  theme={null}
-- Run on source database to get all current sequence values
SELECT
    sequence_name,
    last_value,
    'SELECT setval(''' || sequence_name || ''', ' || (last_value + 1000) || ');' as update_command
FROM information_schema.sequences
WHERE sequence_schema = 'public'
ORDER BY sequence_name;
```

### Update Sequences in PlanetScale

```sql  theme={null}
-- For each sequence, run the update command from above
-- Example commands (values set ahead of source):
SELECT setval('users_id_seq', 16234);  -- Source value + 1000
SELECT setval('orders_id_seq', 99765);  -- Source value + 1000
SELECT setval('products_id_seq', 6432);  -- Source value + 1000

-- Verify sequence values are ahead of source
SELECT sequence_name, last_value
FROM information_schema.sequences
WHERE sequence_schema = 'public'
ORDER BY sequence_name;
```

### Apply Remaining Constraints

Now apply foreign key constraints that were deferred:

```sql  theme={null}
-- Apply foreign key constraints
\i constraints.sql

-- Verify constraints were applied successfully
SELECT conname, contype, conrelid::regclass AS table_name
FROM pg_constraint
WHERE connamespace = 'public'::regnamespace
  AND contype = 'f'  -- foreign key constraints
ORDER BY conrelid::regclass::text;
```

## Step 13: Comprehensive Pre-Cutover Validation

<Warning>
  **Complete Validation Required**

  Validate ALL schema objects and data integrity before cutover. Missing objects will cause application failures.
</Warning>

```sql  theme={null}
-- Validate table row counts match source
SELECT
    schemaname,
    tablename,
    n_tup_ins as estimated_rows
FROM pg_stat_user_tables
WHERE schemaname = 'public'
ORDER BY tablename;
```

## Step 14: Perform Cutover

When ready to switch to PlanetScale Postgres:

<Steps>
  <Step>
    **Stop application writes** to source database
  </Step>

  <Step>
    **Wait for CDC to catch up** (monitor lag in DMS console)
  </Step>

  <Step>
    **Verify final data consistency**
  </Step>

  <Step>
    **Update application connection strings** to point to PlanetScale
  </Step>

  <Step>
    **Start application** with new connections
  </Step>

  <Step>
    **Stop DMS task** once satisfied with cutover
  </Step>
</Steps>

### Stop the migration task:

```bash  theme={null}
aws dms stop-replication-task \
    --replication-task-arn arn:aws:dms:region:account:task:task-id
```

## Step 15: Cleanup

The task configuration above is already optimized for schema-first migrations. Key settings:

* **DO\_NOTHING** prep mode preserves your existing schema
* **Row-level validation** ensures data integrity
* **Batch processing** optimizations improve performance
* **Memory tuning** handles large datasets efficiently

<Note>
  **Automated vs Manual Configuration**

  For Aurora migrations, consider using the [automated CloudFormation approach](/docs/postgres/imports/aurora-dms) which includes these optimized settings and additional automation features.
</Note>

After successful cutover and schema migration:

<Steps>
  <Step>
    **Delete DMS task**
  </Step>

  <Step>
    **Delete replication instance** (to stop charges)
  </Step>

  <Step>
    **Remove source and target endpoints**
  </Step>

  <Step>
    **Clean up security groups** if created specifically for migration
  </Step>
</Steps>

```bash  theme={null}
# Cleanup commands
aws dms delete-replication-task --replication-task-arn your-task-arn
aws dms delete-replication-instance --replication-instance-arn your-instance-arn
aws dms delete-endpoint --endpoint-arn your-source-endpoint-arn
aws dms delete-endpoint --endpoint-arn your-target-endpoint-arn
```

## Troubleshooting

### Common Issues:

**Connectivity problems:**

* Check security groups and network ACLs
* Verify endpoint configurations
* Test network connectivity from replication instance

**Performance issues:**

* Increase replication instance size
* Adjust parallel load settings
* Monitor source database performance

**Data type mapping issues:**

* Review [DMS data type mapping](https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Reference.DataTypes.html)
* Use transformation rules for custom mappings

**Large object (LOB) handling:**

```json  theme={null}
{
  "TargetMetadata": {
    "SupportLobs": true,
    "FullLobMode": true,
    "LobChunkSize": 32768,
    "LimitedSizeLobMode": false
  }
}
```

### Schema-Related Troubleshooting:

**"sequence does not exist" errors:**

```sql  theme={null}
-- Check if sequence exists
SELECT * FROM information_schema.sequences WHERE sequence_name = 'your_sequence';

-- Recreate missing sequence
CREATE SEQUENCE your_sequence START WITH 1;
SELECT setval('your_sequence', (SELECT MAX(id) FROM your_table));
```

**Missing indexes causing performance issues:**

```sql  theme={null}
-- Find missing indexes by comparing to source
-- Run on source database to get index list
SELECT indexname, indexdef FROM pg_indexes WHERE schemaname = 'public';

-- Check query performance
EXPLAIN (ANALYZE, BUFFERS) SELECT * FROM your_table WHERE indexed_column = 'value';
```

**Foreign key constraint violations:**

```sql  theme={null}
-- Check for constraint violations before applying
SELECT COUNT(*) FROM child_table c
WHERE NOT EXISTS (SELECT 1 FROM parent_table p WHERE p.id = c.parent_id);

-- Apply constraints one by one to isolate issues
ALTER TABLE child_table ADD CONSTRAINT fk_name FOREIGN KEY (parent_id) REFERENCES parent_table(id);
```

**Functions/views with dependency errors:**

```sql  theme={null}
-- Check dependencies
SELECT * FROM pg_depend WHERE objid = 'your_function'::regproc;

-- Apply in dependency order: functions before views that use them
```

**Permission errors during schema application:**

* Ensure PlanetScale database user has CREATE permissions
* Check if objects already exist and need DROP statements
* Verify user has permissions on referenced objects

**Sequence values too low causing duplicate key errors:**

```sql  theme={null}
-- Check current sequence value vs max table value
SELECT last_value FROM your_sequence;
SELECT MAX(id) FROM your_table;

-- Update sequence to safe value
SELECT setval('your_sequence', (SELECT MAX(id) FROM your_table));
```

### Performance Optimization:

1. **Parallel loading**: Increase `MaxFullLoadSubTasks`
2. **Batch apply**: Enable for better target performance
3. **Memory allocation**: Increase replication instance size
4. **Network optimization**: Use placement groups for better network performance

## Cost Optimization

* **Instance sizing**: Start with smaller instances and scale up if needed
* **Multi-AZ**: Disable for dev/test migrations
* **Task lifecycle**: Delete resources immediately after successful migration
* **Data transfer**: Consider AWS region placement to minimize transfer costs

## Schema Considerations

Before migration, review:

<Columns cols={2}>
  <Card title="PostgreSQL version compatibility" icon="database" horizontal href="/docs/vitess/imports/postgres#postgresql-version-compatibility" />

  <Card title="Extension support limitations" icon="battery-exclamation" horizontal href="/docs/vitess/imports/postgres#extension-support" />

  <Card title="Third-party enhancement restrictions" icon="circle-xmark" horizontal href="/docs/vitess/imports/postgres#third-party-enhancements-and-tools" />
</Columns>

**Important:** Plan additional time for post-migration schema object setup. Complex databases may require several hours for index recreation and sequence synchronization.

**Performance Impact Note:** Large indexes can take hours to rebuild on populated tables. Consider the schema-first approach to avoid this performance penalty.

## Next Steps

After successful migration and schema setup:

<Steps>
  <Step>
    **Run comprehensive post-cutover validation** using all verification queries above
  </Step>

  <Step>
    **Monitor application logs** for any sequence or constraint errors
  </Step>

  <Step>
    **Performance baseline comparison** - compare query performance to source database
  </Step>

  <Step>
    **Test critical business workflows** end-to-end
  </Step>

  <Step>
    Set up monitoring and alerting for PlanetScale Postgres
  </Step>

  <Step>
    Plan for ongoing maintenance and backup strategies
  </Step>

  <Step>
    Consider implementing additional PlanetScale features
  </Step>
</Steps>

**Success Criteria:**

* ✅ All schema objects validated and functional
* ✅ Sequence values synchronized and tested
* ✅ Query performance matches or exceeds source database
* ✅ No application errors in logs for 24+ hours
* ✅ All foreign key constraints working correctly

For simpler migrations, consider [pg\_dump/restore](./postgres-migrate-dumprestore) or [WAL streaming](./postgres-migrate-walstream) methods.

If you encounter issues during migration, please [reach out to support](https://planetscale.com/contact?initial=support) for assistance.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Postgres Imports - PGDump and Restore
Source: https://planetscale.com/docs/postgres/imports/postgres-migrate-dumprestore



The [pg\_dump](https://www.postgresql.org/docs/current/app-pgdump.html) and restore method is the simplest approach for migrating PostgreSQL databases to PlanetScale Postgres. This method is ideal for smaller databases that can tolerate downtime during the migration process.

## Overview

This migration method involves:

1. Creating a full backup of your source database using `pg_dump`
2. Transferring the dump file to your local environment or staging area
3. Restoring the database to PlanetScale Postgres using `pg_restore` or `psql`

## Prerequisites

Before starting the migration:

* Ensure you have PostgreSQL client tools installed (`pg_dump`, `pg_restore`, `psql`)
* Have read access to your source PostgreSQL database
* Have connection details for your PlanetScale Postgres database from the console
* Ensure the disk on your PlanetScale database has at least 150% of the capacity of your source database.
  If you are migrating to a PlanetScale database backed by network-attached storage, you can [resize](https://planetscale.com/docs/postgres/cluster-configuration/cluster-storage) your disk manually by setting the "Minimum disk size."
  If you are using Metal, you will need to select a size when first creating your database.
  For example, if your source database is 330GB, you should have at least 500GB of storage available on PlanetScale.
* Verify sufficient storage space for the dump file
* Plan for application downtime during the migration

## Step 1: Create Database Dump

### For a complete database dump:

```bash  theme={null}
pg_dump -h source-host \
        -p source-port \
        -U source-username \
        -d source-database \
        --verbose \
        --no-owner \
        --no-privileges \
        -f database_dump.sql
```

### For a custom format dump (recommended for large databases):

```bash  theme={null}
pg_dump -h source-host \
        -p source-port \
        -U source-username \
        -d source-database \
        --verbose \
        --no-owner \
        --no-privileges \
        -Fc \
        -f database_dump.dump
```

### Command options explained:

* `--verbose`: Provides detailed output during the dump process
* `--no-owner`: Excludes ownership information from the dump
* `--no-privileges`: Excludes privilege information from the dump
* `-Fc`: Creates a custom format dump (binary, compressed)
* `-f`: Specifies the output file name

## Step 2: Get PlanetScale Connection Details

From your PlanetScale console:

<Steps>
  <Step>
    Navigate to your PlanetScale Postgres database
  </Step>

  <Step>
    Go to the "Connect" section
  </Step>

  <Step>
    Copy the connection details including:

    * Host
    * Port
    * Database name
    * Username
    * Password
  </Step>
</Steps>

## Step 3: Restore to PlanetScale Postgres

### For SQL format dumps:

```bash  theme={null}
psql -h planetscale-host \
     -p planetscale-port \
     -U planetscale-username \
     -d planetscale-database \
     -f database_dump.sql
```

### For custom format dumps:

```bash  theme={null}
pg_restore -h planetscale-host \
           -p planetscale-port \
           -U planetscale-username \
           -d planetscale-database \
           --verbose \
           --no-owner \
           --no-privileges \
           database_dump.dump
```

### For parallel restoration (faster for large databases):

```bash  theme={null}
pg_restore -h planetscale-host \
           -p planetscale-port \
           -U planetscale-username \
           -d planetscale-database \
           --verbose \
           --no-owner \
           --no-privileges \
           --jobs=4 \
           database_dump.dump
```

## Step 4: Verify Migration

After the restore completes, verify your migration:

### Check table counts:

```sql  theme={null}
SELECT schemaname, tablename, n_tup_ins as row_count
FROM pg_stat_user_tables
ORDER BY schemaname, tablename;
```

### Verify data integrity:

```sql  theme={null}
-- Check a few sample records from key tables
SELECT count(*) FROM your_main_table;
SELECT * FROM your_main_table LIMIT 5;
```

### Check for errors in logs:

Review the output from the pg\_restore command for any errors or warnings.

## Troubleshooting

### Common Issues:

**Permission errors:**

* Ensure your PlanetScale user has appropriate permissions
* Check that connection details are correct

**Extension errors:**

* Some PostgreSQL extensions may not be available in PlanetScale Postgres
* Review the [extension compatibility guide](/docs/vitess/imports/postgres#extension-support)

**Large object errors:**

* If using large objects (BLOBs), add `--blobs` flag to pg\_dump

```bash  theme={null}
pg_dump --blobs -h source-host -U source-username -d source-database -f database_dump.sql
```

**Timeout errors:**

* For large databases, consider breaking the dump into smaller chunks
* Use custom format with parallel restoration

### Performance Tips:

1. **Use custom format**: Binary format with compression is more efficient
2. **Parallel jobs**: Use `--jobs` parameter for faster restoration
3. **Network considerations**: Ensure stable network connection for large transfers
4. **Disk space**: Monitor available disk space during dump creation

## Schema Considerations

Before migration, review:

<Columns cols={2}>
  <Card title="PostgreSQL version compatibility" icon="database" horizontal href="/docs/vitess/imports/postgres#postgresql-version-compatibility" />

  <Card title="Extension support limitations" icon="battery-exclamation" horizontal href="/docs/vitess/imports/postgres#extension-support" />

  <Card title="Third-party enhancement restrictions" icon="circle-xmark" horizontal href="/docs/vitess/imports/postgres#third-party-enhancements-and-tools" />
</Columns>

## Next Steps

After successful migration:

<Steps>
  <Step>
    Update your application connection strings to point to PlanetScale Postgres
  </Step>

  <Step>
    Test your application thoroughly in a staging environment
  </Step>

  <Step>
    Plan your production cutover
  </Step>

  <Step>
    Monitor performance and optimize as needed
  </Step>
</Steps>

For more advanced migration scenarios or larger databases, consider [WAL streaming](./postgres-migrate-walstream) or [Amazon DMS](./postgres-migrate-dms) methods.

If you encounter issues during migration, please [reach out to support](https://planetscale.com/contact?initial=support) for assistance.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Postgres Imports - WAL Streaming
Source: https://planetscale.com/docs/postgres/imports/postgres-migrate-walstream



[Write-Ahead Logging (WAL)](https://www.postgresql.org/docs/current/wal-intro.html) streaming provides a near-zero downtime migration method by continuously replicating changes from your source PostgreSQL database to PlanetScale Postgres using [logical replication](https://www.postgresql.org/docs/current/logical-replication.html).

## Overview

This migration method involves:

<Steps>
  <Step>
    Setting up logical replication from your source database
  </Step>

  <Step>
    Creating an initial data snapshot
  </Step>

  <Step>
    Continuously streaming WAL changes to keep databases synchronized
  </Step>

  <Step>
    Performing a quick cutover when ready
  </Step>
</Steps>

<Warning>
  This method requires administrative access to your source PostgreSQL database to configure replication settings.
</Warning>

## Prerequisites

Before starting the migration:

* PostgreSQL 10+ on the source database (logical replication support)
* Administrative access to source database configuration
* Network connectivity between source and PlanetScale Postgres
* Connection details for your PlanetScale Postgres database from the console
* Ensure the disk on your PlanetScale database has at least 150% of the capacity of your source database.
  If you are migrating to a PlanetScale database backed by network-attached storage, you can [resize](https://planetscale.com/docs/postgres/cluster-configuration/cluster-storage) your disk manually by setting the "Minimum disk size."
  If you are using Metal, you will need to select a size when first creating your database.
  For example, if your source database is 330GB, you should have at least 500GB of storage available on PlanetScale.
* Understanding of your application's write patterns for cutover planning

## Step 1: Configure Source Database

### Enable logical replication on source database:

Edit your PostgreSQL configuration (`postgresql.conf`):

```ini  theme={null}
# Enable logical replication
wal_level = logical

# Set maximum replication slots
max_replication_slots = 10

# Set maximum WAL senders
max_wal_senders = 10

# Enable logical replication workers
max_logical_replication_workers = 10
```

### Configure authentication (`pg_hba.conf`):

Add an entry to allow replication connections:

```ini  theme={null}
# Allow replication connections
host replication replication_user source_ip/32 md5
```

### Restart PostgreSQL service:

```bash  theme={null}
# On systemd systems
sudo systemctl restart postgresql

# On older systems
sudo service postgresql restart
```

## Step 2: Create Replication User

Connect to your source database and create a replication user:

```sql  theme={null}
-- Create replication user
CREATE USER replication_user WITH REPLICATION LOGIN;

-- Grant necessary permissions
GRANT CONNECT ON DATABASE your_database TO replication_user;
GRANT USAGE ON SCHEMA public TO replication_user;
GRANT SELECT ON ALL TABLES IN SCHEMA public TO replication_user;

-- Grant permissions for future tables
ALTER DEFAULT PRIVILEGES IN SCHEMA public
    GRANT SELECT ON TABLES TO replication_user;
```

## Step 3: Create Publication on Source

Create a publication for the tables you want to replicate:

```sql  theme={null}
-- Create publication for all tables
CREATE PUBLICATION planetscale_migration FOR ALL TABLES;

-- Or create publication for specific tables
CREATE PUBLICATION planetscale_migration FOR TABLE
    table1, table2, table3;

-- Verify publication
SELECT * FROM pg_publication;
```

## Step 4: Get PlanetScale Connection Details

From your PlanetScale console:

<Steps>
  <Step>
    Navigate to your PlanetScale Postgres database
  </Step>

  <Step>
    Go to the "Connect" section
  </Step>

  <Step>
    Copy the connection details including:

    * Host
    * Port
    * Database name
    * Username
    * Password
  </Step>
</Steps>

## Step 5: Create Initial Schema on PlanetScale

Export and import the schema structure:

```bash  theme={null}
# Export schema from source
pg_dump -h source-host \
        -p source-port \
        -U source-username \
        -d source-database \
        --schema-only \
        --no-owner \
        --no-privileges \
        -f schema.sql

# Import schema to PlanetScale
psql -h planetscale-host \
     -p planetscale-port \
     -U planetscale-username \
     -d planetscale-database \
     -f schema.sql
```

## Step 6: Create Initial Data Copy

Create an initial data snapshot:

```bash  theme={null}
# Export data from source
pg_dump -h source-host \
        -p source-port \
        -U source-username \
        -d source-database \
        --data-only \
        --no-owner \
        --no-privileges \
        -f data.sql

# Import data to PlanetScale
psql -h planetscale-host \
     -p planetscale-port \
     -U planetscale-username \
     -d planetscale-database \
     -f data.sql
```

## Step 7: Set Up Logical Replication

Connect to PlanetScale Postgres and create subscription:

```sql  theme={null}
-- Create subscription to source database
CREATE SUBSCRIPTION planetscale_subscription
    CONNECTION 'host=source-host port=source-port
                dbname=source-database user=replication_user
                password=replication-password'
    PUBLICATION planetscale_migration;

-- Check subscription status
SELECT * FROM pg_subscription;

-- Monitor replication status
SELECT * FROM pg_stat_subscription;
```

## Step 8: Monitor Replication

### Check replication lag:

```sql  theme={null}
-- On source database
SELECT * FROM pg_replication_slots;

-- On PlanetScale database
SELECT
    subname,
    received_lsn,
    latest_end_lsn,
    latest_end_time
FROM pg_stat_subscription;
```

### Monitor for conflicts:

```sql  theme={null}
-- Check for subscription errors
SELECT * FROM pg_stat_subscription
WHERE last_msg_failure_time IS NOT NULL;
```

## Step 9: Prepare for Cutover

### Verify data consistency:

```sql  theme={null}
-- Compare row counts between source and target
-- Run on both databases
SELECT
    schemaname,
    tablename,
    n_tup_ins as estimated_rows
FROM pg_stat_user_tables
ORDER BY schemaname, tablename;
```

### Check replication lag:

Ensure replication lag is minimal (ideally under 1 second) before cutover.

## Step 10: Perform Cutover

When ready to switch to PlanetScale Postgres:

<Steps>
  <Step>
    **Stop application writes** to the source database
  </Step>

  <Step>
    **Wait for replication to catch up** (monitor lag)
  </Step>

  <Step>
    **Update application connection strings** to point to PlanetScale
  </Step>

  <Step>
    **Start application** with new connection
  </Step>

  <Step>
    **Monitor** for any issues
  </Step>
</Steps>

### Verify cutover success:

```sql  theme={null}
-- Check that latest data is present
SELECT count(*), max(updated_at) FROM your_main_table;
```

## Step 11: Cleanup (After Successful Cutover)

### Drop subscription on PlanetScale:

```sql  theme={null}
DROP SUBSCRIPTION planetscale_subscription;
```

### Drop publication on source:

```sql  theme={null}
DROP PUBLICATION planetscale_migration;
```

## Troubleshooting

### Common Issues:

**Replication slot conflicts:**

```sql  theme={null}
-- Check existing slots
SELECT * FROM pg_replication_slots;

-- Drop unused slots
SELECT pg_drop_replication_slot('slot_name');
```

**Permission errors:**

* Verify replication user has correct permissions
* Check pg\_hba.conf configuration
* Ensure network connectivity

**Large transaction delays:**

* Monitor for long-running transactions on source
* Consider breaking large operations into smaller batches

**Subscription conflicts:**

```sql  theme={null}
-- Check subscription worker status
SELECT * FROM pg_stat_subscription;

-- Restart subscription if needed
ALTER SUBSCRIPTION planetscale_subscription DISABLE;
ALTER SUBSCRIPTION planetscale_subscription ENABLE;
```

## Performance Considerations

1. **Network bandwidth**: Ensure sufficient bandwidth for initial sync and ongoing replication
2. **Disk I/O**: Monitor disk usage on both source and target during replication
3. **Replication lag**: Keep lag minimal by optimizing source database performance
4. **Conflict resolution**: Understand how PostgreSQL handles replication conflicts

## Schema Considerations

Before migration, review:

<Columns cols={2}>
  <Card title="PostgreSQL version compatibility" icon="database" horizontal href="/docs/vitess/imports/postgres#postgresql-version-compatibility" />

  <Card title="Extension support limitations" icon="battery-exclamation" horizontal href="/docs/vitess/imports/postgres#extension-support" />

  <Card title="Third-party enhancement restrictions" icon="circle-xmark" horizontal href="/docs/vitess/imports/postgres#third-party-enhancements-and-tools" />
</Columns>

## Next Steps

After successful migration:

<Steps>
  <Step>
    Monitor replication performance and lag
  </Step>

  <Step>
    Test application functionality thoroughly
  </Step>

  <Step>
    Set up monitoring and alerting for the new database
  </Step>

  <Step>
    Plan for ongoing maintenance and optimization
  </Step>
</Steps>

For simpler migrations or if you don't have administrative access to your source database, consider the [pg\_dump/restore method](./postgres-migrate-dumprestore). For more complex scenarios, explore [Amazon DMS](./postgres-migrate-dms).

If you encounter issues during migration, please [reach out to support](https://planetscale.com/contact?initial=support) for assistance.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Migrate from Supabase to PlanetScale
Source: https://planetscale.com/docs/postgres/imports/supabase



Use this guide to migrate an existing Supabase database to PlanetScale Postgres.

This guide will cover a no-downtime approach to migrating using Postgres logical replication. If you are willing to tolerate downtime during a maintenance window, you may also use [`pg_dump` and restore](/docs/postgres/imports/postgres-migrate-dumprestore). The `pg_dump`/restore approach is simpler, but is only for applications where downtime is acceptable.

These instructions work for all versions of Postgres that support logical replication (version 10+). If you have an older version you want to bring to PlanetScale, [contact us](https://planetscale.com/contact?initial=support) for guidance.

Before beginning a migration, you should check our [extensions documentation](/docs/postgres/extensions) to ensure that all of the extensions you rely on will work on PlanetScale.

As an alternative to this guide, you can also try our [Postgres migration scripts](https://github.com/planetscale/migration-scripts/tree/main/postgres-direct). These allow you to automate some of the manual steps that we describe in this guide.

## 1. Prepare your PlanetScale database

Go to `app.planetscale.com` and create a new database. A few things to check when configuring your database:

* Ensure you select the correct cloud region. You typically want to use the same region that you deploy your other application infrastructure to.
* Since Supabase uses Postgres, you'll also want to create a Postgres database in PlanetScale.
* Choose the best storage option for your needs. For applications needing high-performance and low-latency I/O, use [PlanetScale Metal](/docs/metal). For applications that need more flexible storage options or smaller compute instances, choose "Elastic Block Storage" or "Persistent Disk."
* Choose between aarch64 and x86-64 architecture. If you don't know which to choose, `aarch64` is a good default choice.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=aaca50d3753073aaac499b3abe680c83" alt="Create a new PlanetScale Postgres database" data-og-width="2726" width="2726" data-og-height="2148" height="2148" data-path="docs/images/assets/docs/postgres/imports/image.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=f7b060cbe656ddd34f337d2870306616 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=2f2759f8703396f8d82d0006cca222dc 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=ea54c4d42a7e6b5626b949b028a0397e 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=f6cada130f06c28a51eae34168733a6c 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=d55b83a571d773396a91cf76dbaa4193 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=172c7f01989b23bb3a4b34b7e3efb7bc 2500w" />
</Frame>

Once the database is created and ready, navigate to your dashboard and click the "Connect" button.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/migration-dashboard-connect.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=270f751b79b7c6411747336f1808f84f" alt="Connect to a PlanetScale Postgres database" data-og-width="3688" width="3688" data-og-height="1522" height="1522" data-path="docs/images/assets/docs/postgres/imports/migration-dashboard-connect.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/migration-dashboard-connect.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=847e17b3d16516f98c95e4dedcd608c5 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/migration-dashboard-connect.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=88ff3e44f389ceea4444e1684d24fa48 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/migration-dashboard-connect.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=660052ad98066d43bc1734d983e543cf 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/migration-dashboard-connect.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=c091929944f238b2d6e09bc903532470 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/migration-dashboard-connect.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=2337b79a44ae1e3cf24a89f7d43fa6df 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/migration-dashboard-connect.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=cca93110dfeec30a5911ff789961468d 2500w" />
</Frame>

From here, follow the instructions to create a new default role. This role will act as your admin role, with the highest level of privileges.

Though you may use this one for your migration, we recommend you use a separate role with lesser privileges for your migration and general database connections.

To create a new role, navigate to the [Role management page](/docs/postgres/connecting/roles). Click "New role" and give the role a memorable name. By default, `pg_read_all_data` and `pg_write_all_data` are enabled. In addition to these, enable `pg_create_subscription` and `postgres`, and then create the role.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image3.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=7b0a8b6b27a1f28e4e5f16dece3f6087" alt="New Postgres role privileges" data-og-width="1882" width="1882" data-og-height="2296" height="2296" data-path="docs/images/assets/docs/postgres/imports/image3.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image3.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=a50f24908e67709672005dd83e01c9bb 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image3.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=96c638d12b0b809e48db540b4ce9d5e4 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image3.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=31670bdb00129efad3e127ded0d29288 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image3.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=f486f0dbc600324559edde676eb51b57 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image3.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=5f2b82317aa9c7261125928f8f4c13cc 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image3.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=99b0e011ed462a95561b4f6270b6973d 2500w" />
</Frame>

Copy the password and all other connection credentials into environment variables for later use:

```
PLANETSCALE_USERNAME=pscale_api_XXXXXXXXXX.XXXXXXXXXX
PLANETSCALE_PASSWORD=pscale_pw_XXXXXXXXXXXXXXXXXXXXXXX
PLANETSCALE_HOST=XXXX.pg.psdb.cloud
PLANETSCALE_DBNAME=postgres
```

We also recommend that you increase `max_worker_processes` for the duration of the migration in order to speed up data copying. Go to the "Parameters" tab of the "Clusters" page:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image4.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=fd168c176e0298871ee45a7123567280" alt="Configure parameters" data-og-width="3318" width="3318" data-og-height="1964" height="1964" data-path="docs/images/assets/docs/postgres/imports/image4.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image4.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=075701b9e7dbbd64b8550a10b3de1d04 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image4.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=3e166ad64beed2f3441696695d9cba1c 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image4.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=9138a06417f7398ff6e9f14f38604bf6 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image4.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=0d33d7bfda846e537e9c8cc995f1a470 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image4.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=51c49ebc128442dfa410edc0cf143171 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image4.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=25c6b8e572da8583f6aa5b5f02dc1cd6 2500w" />
</Frame>

On this page, increase this value from the default of `4` to `10` or more:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image5.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=7a72f9fbfe69dce955277d64ff350468" alt="Configure max worker processes" data-og-width="1784" width="1784" data-og-height="1152" height="1152" data-path="docs/images/assets/docs/postgres/imports/image5.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image5.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=43cdf8d56e56612385e1cd709a95a02d 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image5.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=afe0c4b5a912dd8d370c5da270d2cfca 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image5.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=a12e2dde97abdc8d2501bb815ebd610c 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image5.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=ad7a0405ed2501abf90c8ac700462dd4 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image5.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=c8005e4724a61c1aed8c1aeea1375d80 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image5.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=8535815774dcde2ae30f346d9d7a507e 2500w" />
</Frame>

You can decrease these values after the migration is complete.

## 2. Configure disk size on PlanetScale

If you are importing into a database backed by network-attached storage, you must configure your disk in advance to ensure your database will fit.
Though we support disk autoscaling for these, AWS and GCP limit how frequently disks can be resized.
If you don't ensure your disk is large enough for the import in advance, it will not be able to resize fast enough for a large data import.

To configure this, navigate to "Clusters" and then the "Storage" tab:

<img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/storage-configuration-min-size.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=a5c9b5d45b3e70904dcd63ebae86ed51" alt="Storage configuration min size" data-og-width="3076" width="3076" data-og-height="2336" height="2336" data-path="docs/postgres/imports/storage-configuration-min-size.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/storage-configuration-min-size.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=e7ce34378d7f13060b6f55798d84ca81 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/storage-configuration-min-size.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=752e51dbf17cd95a054e8bfcffc88310 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/storage-configuration-min-size.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=afcc0a16e5cc562f507a58f397e48648 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/storage-configuration-min-size.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=430433f27649b68a6a612010926fd5ed 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/storage-configuration-min-size.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=f7a31fb98516dea9d3808e0e1649a1d8 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/storage-configuration-min-size.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=e0447135018372dbfde71a8647a6a9a2 2500w" />

On this page, adjust the "Minimum disk size."
You should set this value to at least 150% of the size of the database you are migrating.
For example, if the database you are importing is 330 GB, you should set your minimum disk size to at least 500 GB.

The 50% overhead is to account for:

1. Data growth during the import process and
2. Table and index bloat that can occur during the import process.
   This can be later mitigated with careful [VACUUMing](https://www.postgresql.org/docs/current/sql-vacuum.html) or using an extension like [pg\_squeeze](https://planetscale.com/docs/postgres/extensions/pg_squeeze), but is difficult to avoid during the migration itself.

When ready, queue and apply the changes.
You can check the "Changes" tab to see the status of the resize:

<img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/confirm-disk-size-change.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=ade2793422ccfcf0bd7e6c5ee2511a98" alt="Confirm disk size change" data-og-width="2626" width="2626" data-og-height="1264" height="1264" data-path="docs/postgres/imports/confirm-disk-size-change.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/confirm-disk-size-change.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=b193023eae6603ca54d2d21d4b8cf3ce 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/confirm-disk-size-change.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=36f59f9028dc0674d68594d7efbfd912 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/confirm-disk-size-change.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=92d471e9e4afa9de0dc7ff1e9adcf2e3 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/confirm-disk-size-change.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=f237cdd75c3a3c243b7b6b4c77fdba61 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/confirm-disk-size-change.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=99e53e615c6ca48b396d0b0ada146cf7 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/confirm-disk-size-change.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=226ed370d1291db934cd6fbda032d633 2500w" />

Wait for it to indicate completion.

If you are importing to a Metal database, you must choose a disk size when first creating your database.
You should launch your cluster with a disk size at least 50% larger than the storage used by your current source database (150% of the existing total).

As an example, if you need to import a 330 GB database onto a PlanetScale `M-160` there are three storage sizes available:

<img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/metal-disk-size.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=3c7924b6c3c1c712eb2772b9c16adb41" alt="Metal disk size" data-og-width="2074" width="2074" data-og-height="1812" height="1812" data-path="docs/postgres/imports/metal-disk-size.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/metal-disk-size.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=d2db5c8f73a3d3e450aa7410d517966a 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/metal-disk-size.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=b0c63716f22d784102107f89435f90c5 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/metal-disk-size.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=83ccb63b153d07c065f434a026156c32 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/metal-disk-size.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=b70ee20b312291760b4851520c333006 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/metal-disk-size.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=23f72e5606dd17d426b63d1404436a59 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/imports/metal-disk-size.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=9bb971a81d534a7ac24bde77932d9717 2500w" />

You should use the largest, 1.25TB option during the import.
After importing and cleaning up table bloat, you may be able to downsize to the 468 GB option.
Resizing is a no-downtime operation that can be performed on the [clusters](httsp://planetscale.com/docs/postgres/cluster-configuration) page.

## 3. Enable IPv4 direct connections in Supabase

In Supabase, logical replication to external sources requires direct connections. Direct IPv4 connections are not enabled by default. If you have not enabled them yet, go to your project dashboard in Supabase and click the "Connect" button:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image6.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=75da23b6022df1231329326638764c4b" alt="Supabase dashboard" data-og-width="2760" width="2760" data-og-height="1300" height="1300" data-path="docs/images/assets/docs/postgres/imports/image6.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image6.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=47f3fa9d26f9fe875a3bdeb41b2809b3 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image6.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=ed89369c4af6f9c4066984096aa7a22c 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image6.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=d157d1451fd2f7f34ab4d673df77762a 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image6.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=3b8cac8ea4899d5730a82dcdc397ade4 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image6.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=0ad348e56464b1152315c2ae870eb6ae 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image6.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=906fe153d134a0855d99480b0a426bd1 2500w" />
</Frame>

In the connection modal, click "IPv4 add-on."

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image7.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=39d39e9766f6e3eb80d29ae9c56c8d80" alt="Supabase direct" data-og-width="3448" width="3448" data-og-height="1660" height="1660" data-path="docs/images/assets/docs/postgres/imports/image7.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image7.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=429cd3e706588007e1e8f3467c32141b 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image7.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=8df45561e83d5c076635ab6d00562739 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image7.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=2444e506709af5a1442bb901771fc28a 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image7.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=c049f5487f768090a962865422d83d26 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image7.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=7a9e83b6d8217110ae6f88fe8f30f44c 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image7.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=470003eb32f225c5e407c566a478e3d2 2500w" />
</Frame>

In the menu that appears, enable the IPv4 add-on:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image8.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=357454d8ee4ab345b55f87769d3d392c" alt="Supabase IPV4" data-og-width="2006" width="2006" data-og-height="1660" height="1660" data-path="docs/images/assets/docs/postgres/imports/image8.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image8.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=40daa530dd61e5a85105ed307941bfe3 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image8.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=e5cde381f4f5de743488eea33b3658da 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image8.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=de56fface9fce8c87b0a2c3c63bbcd1f 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image8.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=37d78f2b735b21c3f77c46558d4c65df 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image8.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=7f9a78b1246f277707f44add25e2ab02 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/postgres/imports/image8.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=df151b213161b42295b6e557d0b607b6 2500w" />
</Frame>

Supabase notes that enabling this might incur downtime. Take that into account when planning your migration.

## 4. Copy schema from Supabase to PlanetScale

Before we begin migrating data, we first must copy the schema from Supabase to PlanetScale. We do this as a distinct set of steps using `pg_dump`.

<Warning>
  You should not make any schema changes during the migration process. You may continue to select, insert, update, and delete data, keeping your application fully online during this process.
</Warning>

For these instructions, you'll need to connect to your Supabase role that has permissions to create replication publications and read all data. You also must use a direct IPv4 connection. Your default role that was generated by Supabase when you first created your database should suffice here. We will assume that the credentials for this user and other connection info are stored in the following environment variables.

```
SUPABASE_USERNAME=XXXX
SUPABASE_PASSWORD=XXXX
SUPABASE_HOST=XXX
SUPABASE_DBNAME=XXX
```

Run the below command to take a snapshot of the full schema of the `$SUPABASE_DBNAME` that you want to migrate:

```
PGPASSWORD=$SUPABASE_PASSWORD \
pg_dump -h $SUPABASE_HOST \
        -p 5432 \
        -U $SUPABASE_USERNAME \
        -d $SUPABASE_DBNAME \
        --schema-only \
        --no-owner \
        --no-privileges \
        --schema=public \
        -f schema.sql
```

This saves the schema into a file named `schema.sql`.

<Note>
  The above command will dump the tables only for the `public` schema. If you want to include other schemas in the migration, you can repeat these steps for each, or customize the commands to dump multiple schemas at once.
</Note>

The schema then needs to be loaded into your new PlanetScale database:

```bash  theme={null}
PGPASSWORD=$PLANETSCALE_PASSWORD \
psql -h $PLANETSCALE_HOST \
     -p 5432 \
     -U $PLANETSCALE_USERNAME \
     -d $PLANETSCALE_DBNAME \
     -f schema.sql
```

In the output of this command, you might see some error messages of the form:

```bash  theme={null}
psql:schema.sql:LINE: ERROR: DESCRIPTION
```

You should inspect these to see if they are of any concern. You can [reach out to our support](https://planetscale.com/contact?initial=support) if you need assistance at this step.

## 5. Set up logical replication

We now must create a `PUBLICATION` on Supabase that the PlanetScale database can subscribe to for data copying and replication. This example shows how to create a publication that only publishes changes to tables in the `public` schema of your Postgres database. You can adjust the commands if you want to do so for a different schema, or have multiple schemas to migrate.

First, run this command on your Supabase database to get all of the tables in the `public` schema:

```sql  theme={null}
SELECT 'CREATE PUBLICATION replicate_to_planetscale FOR TABLE ' ||
       string_agg(format('%I.%I', schemaname, tablename), ', ') || ';'
FROM pg_tables
WHERE schemaname = 'public';
```

This will generate a query that looks like this:

```sql  theme={null}
CREATE PUBLICATION replicate_to_planetscale FOR TABLE
  public.table_1,
  public.table_2,
  ...
  public.table_n;
```

Take this command and execute it on your Supabase database. You should see this if it created correctly:

```sql  theme={null}
CREATE PUBLICATION
```

We then need to tell PlanetScale to `SUBSCRIBE` to this publication.

```sql  theme={null}
PGPASSWORD=$PLANETSCALE_PASSWORD psql \
  -h $PLANETSCALE_HOST \
  -U $PLANETSCALE_USERNAME \
  -p 5432 $PLANETSCALE_DBNAME \
  -c "
CREATE SUBSCRIPTION replicate_from_supabase
CONNECTION 'host=$SUPABASE_HOST dbname=$SUPABASE_DBNAME user=$SUPABASE_USERNAME password=$SUPABASE_PASSWORD'
PUBLICATION replicate_to_planetscale WITH (copy_data = true);"
```

Data copying and replication will begin at this point. To check in on the row counts on the tables, you can run a query like this on your source and target databases:

```sql  theme={null}
SELECT table_name, row_count FROM (
  SELECT 'table_name_1' as table_name, COUNT(*) as row_count FROM table_name_1 UNION ALL
  SELECT 'table_name_2', COUNT(*) FROM table_name_2 UNION ALL
  ...
  SELECT 'table_name_N', COUNT(*) FROM table_name_N
) t ORDER BY table_name;
```

When the row counts match (or nearly match) you can begin testing and preparing for your application to cutover to use PlanetScale.

## 6. Handling sequences

Logical replication is great at migrating all of your data over to PlanetScale. However, logical replication does *not* synchronize the `nextval` values for [sequences](https://www.postgresql.org/docs/current/sql-createsequence.html) in your database. Sequences are often used for things like auto incrementing IDs, so it's important to ensure we update this before you switch your traffic to PlanetScale.

You can see all of the sequences and their corresponding `nextval`s on your source Supabase database using this command:

```sql  theme={null}
SELECT schemaname, sequencename, last_value + increment_by
  AS next_value
  FROM pg_sequences;
```

An example output from this command:

```
 schemaname |   sequencename   | next_value
------------+------------------+------------
 public     | users_id_seq     |        105
 public     | posts_id_seq     |       1417
 public     | followers_id_seq |       3014
```

What this means is that we have three sequences in our database. In this case, they are all being used for auto-incrementing primary keys. The `nextval` for the `users_id_seq` is 105, the `nextval` for the `posts_id_seq` is 1417, and the `nextval` for the `followers_id_seq` is 3014. If you run the same query on your new PlanetScale database, you'll see something like:

```
 schemaname |   sequencename   | next_value
------------+------------------+------------
 public     | users_id_seq     |          0
 public     | posts_id_seq     |          0
 public     | followers_id_seq |          0
```

If you switch traffic over to PlanetScale in this state, you'll likely encounter errors when inserting new rows:

```bash  theme={null}
ERROR:  duplicate key value violates unique constraint "XXXX"
DETAIL:  Key (id)=(ZZZZ) already exists.
```

Before switching over, you need to progress all of these sequences forward so that the `nextval`s produced will be greater than any of the values previously produced on the source Supabase database, avoiding constraint violations. There are several approaches you can take for this. A simple way to solve the problem is to first run this query on your source Supabase database:

```sql  theme={null}
SELECT 'SELECT setval(''' || schemaname || '.' || sequencename || ''', '
       || (last_value + 10000) || ');' AS query
FROM pg_sequences;
```

This will generate a sequence of queries that will advance the `nextval` by 10,000 for each sequence:

```sql  theme={null}
                      query
--------------------------------------------------
 SELECT setval('public.users_id_seq', 10104);
 SELECT setval('public.posts_id_seq', 11416);
 SELECT setval('public.followers_id_seq', 13013);
```

You would then execute these on your target PlanetScale database. You need to ensure you advance each sequence far enough forward so that the sequences in the Supabase database will not reach these `nextval`s before you switch your primary to PlanetScale. For tables that have a high insertion rate, you might need to increase this by a larger value (say, 100,000 or 1,000,000).

## 7. Cutting over to PlanetScale

Before you cutover, it's good to have confidence that the replication is fully caught up between Supabase and PlanetScale. You can do this using Log Sequence Numbers (LSNs). The goal is to see these match up between the source Supabase database and the target PlanetScale database exactly. If they don't, it indicates that the PlanetScale database is not fully caught-up with the changes happening on Supabase.

You can run this on Supabase to see the current LSN:

```bash  theme={null}
postgres=> SELECT pg_current_wal_lsn();
 pg_current_wal_lsn
--------------------
 0/703FE460
```

Then on PlanetScale, you would run the following query to confirm the match:

```bash  theme={null}
postgres=> SELECT received_lsn, latest_end_lsn
             FROM pg_stat_subscription
             WHERE subname = 'replicate_from_supabase';
 received_lsn | latest_end_lsn
--------------+----------------
 0/703FE460   | 0/703FE460
```

Once you are comfortable that all your data has successfully copied over and replication is sufficiently caught up, it's time to switch to PlanetScale. In your application code, prepare the cutover by changing the database connection credentials to go to PlanetScale rather than Supabase. Then, you can deploy this new version of your application, which will begin using PlanetScale as your primary database.

After doing this, new rows written to PlanetScale will not be reverse-replicated to Supabase. Thus, it's important to ensure you are fully ready for the cutover at this point.

Once this is complete, PlanetScale is now your primary database!

We recommend you keep the old Supabase database around for a few days, in case you discover any data or schemas you forgot to copy over to PlanetScale. If necessary, you can switch traffic back to the old database. However, keep in mind that any database writes that happened with PlanetScale as the primary will not appear on Supabase. This is why it's good to test the database thoroughly before performing the cutover.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Logical replication and Change Data Capture (CDC)
Source: https://planetscale.com/docs/postgres/integrations/logical-cdc

Change Data Capture (CDC) allows you to track and stream data changes from your PostgreSQL database to external systems in real-time. PlanetScale Postgres supports logical replication, enabling CDC through various tools and integrations.

## What is logical replication?

Logical replication is a PostgreSQL feature that replicates data objects and their changes at the logical level, rather than the physical level. It works by:

1. **WAL Level**: The database must be configured with `wal_level = logical` to capture logical changes
2. **Publication**: Creates a logical replication stream on the source database
3. **Replication Slot**: Maintains position in the WAL stream and ensures data consistency
4. **Subscription/Consumer**: External tools consume the logical replication stream

This setup enables:

* Replication of individual transactions and row changes
* Selective replication of specific tables or databases
* Cross-version replication between different PostgreSQL versions
* CDC integration with external tools and data pipelines

## Configuration requirements

### PlanetScale cluster parameters

To enable logical replication on your PlanetScale Postgres cluster, configure these parameters in the **Clusters > Parameters** tab:

| Parameter                | Required Value | Description                                                                                                                                                                                                                                                                                                                                                      |
| ------------------------ | -------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `wal_level`              | `logical`      | Setting the `wal_level` to `logical` enables logical replication, which captures row-level changes in a format that can be flexibly replayed on target systems.                                                                                                                                                                                                  |
| `max_replication_slots`  | 2 x replicas   | Set `max_replication_slots` to twice the number of replicas or subscribers. Each replica uses one slot, with the extra slots reserved for operations like failover.                                                                                                                                                                                              |
| `max_wal_senders`        | 2 x replicas   | Likewise, set `max_wal_senders` to twice the number of replicas or targets, and not less than `max_replication_slots`.                                                                                                                                                                                                                                           |
| `max_slot_wal_keep_size` | > 4GB          | The value of `max_slot_wal_keep_size` should be tuned to ensure you are keeping WAL files long enough for subscribers to consume them, while being sure that the source disk is not overrun by files. A reasonable starting point is > 4GB, and monitor your replication lag, your database's change rate (inserts, updates, deletes), and available disk space. |

In addition, for production environments, configure the following to ensure that your CDC stream is maintained during any switchover or failover. Without these settings, manual intervention will be required to restore data pipelines after these events.

| Parameter                | Required Value | Description                                                                   |
| ------------------------ | -------------- | ----------------------------------------------------------------------------- |
| `sync_replication_slots` | `on`           | Set to `on` to enable synchronization of replication slots to the subcribers. |
| `hot_standby_feedback`   | `on`           | Set to `on` to prevent query conflicts during replication.                    |

Also, in the **Cluster configuration > Parameters** tab of the dashboard UI, under the Failover section, add a comma-delimited list of the replication slot(s) you will create to preserve during any switchover or failover events.

Be sure to apply the queue of configuration changes before proceeding.

### Verify configuration

After setting these configuration parameters in the dashboard, you can verify them in the CLI. For example, to verify the WAL level:

```sql  theme={null}
SHOW wal_level;
```

The result should show `logical`:

```sql  theme={null}
 wal_level
-----------
 logical
```

### CDC tool configuration

Ensure your CDC tool is configured properly:

* **Airbyte**: Ensure replication slots are created with failover support ([Setup Guide](https://docs.airbyte.com/integrations/sources/postgres))
* **AWS DMS**: Manually create failover-enabled replication slots before configuring DMS ([Setup Guide](https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.PostgreSQL.html))
* **ClickHouse**: See ClickPipes documentation for PlanetScale configuration ([Setup Guide](https://clickhouse.com/docs/integrations/clickpipes/postgres/source/planetscale))
* **Debezium**: Configure connector to use failover-enabled replication slots ([Setup Guide](https://debezium.io/documentation/reference/stable/connectors/postgresql.html))
* **Fivetran**: Create your own replication slot with `failover = true` ([Setup Guide](https://fivetran.com/docs/connectors/databases/postgresql))

<Warning>
  Some CDC tools create replication slots automatically. **You must verify** that any auto-created slots have `failover = true` enabled, or manually create the slots yourself with the proper configuration.
</Warning>

## Create and manage users

For production CDC deployments, login as the default user and create a dedicated replication user with minimal privileges:

```sql  theme={null}
-- Create dedicated CDC user
CREATE USER cdc_user WITH REPLICATION PASSWORD 'strong_password';
```

The `WITH REPLICATION` clause allows the user to connect to the server using the replication protocol, to create and user replication slots, to stream WAL files, and to perform the logical decoding operations. You will configure this user to connect from your subscriber/consumer side.

Because of the edge connection settings, to login as this user, add the branch ID after the username, like this:

```sql  theme={null}
cdc_user.12345678
```

Some target systems, like Fivetran, will need to match on the exact login username for some operations after it has logged in. As a workaround for those cases, also create a user with the name + branch ID, like this:

```sql  theme={null}
-- Create dedicated CDC user
CREATE USER "cdc_user.12345678" WITH REPLICATION PASSWORD 'strong_password';
```

To find the branch ID, look at the Settings > Roles page and observe the branch ID in roles that were created in the UI.

## Create and manage replication streams

### Create a replication slot

Using the dedicated replication role, create logical replication slots with the `failover` option enabled to preserve the slots during any switchover or failover events:

```sql  theme={null}
SELECT pg_create_logical_replication_slot(
  'my_cdc_slot',            -- slot_name
  'pgoutput',               -- plugin
  false,                    -- temporary
  false,                    -- two_phase
  true                      -- failover = true (REQUIRED)
);
```

### Create initial publication

Some CDC tools require you to create publications to specify which tables to replicate. You will need to do this as the owner of the tables or the superuser. This example uses the default PlanetScale superuser.

```sql  theme={null}
CREATE PUBLICATION my_cdc_publication FOR TABLE table1, table2;
```

### Add tables to a publication

Currently, tables must be added to the publication individually or as a comma-delimited list. Remember to update your publication when adding new tables that should be replicated.

```sql  theme={null}
ALTER PUBLICATION my_cdc_publication ADD TABLE table3;
```

### Replica identity configuration

For complete change tracking of both row values before and after changes (as well as to support any tables without a primary key), set the replica identity to FULL:

```sql  theme={null}
-- Enable full replica identity for complete change tracking
ALTER TABLE table1 REPLICA IDENTITY FULL;
ALTER TABLE table2 REPLICA IDENTITY FULL;
ALTER TABLE table3 REPLICA IDENTITY FULL;
```

### Verify publications

Issue the following to see active publications with tables. Do this as the default user.

```sql  theme={null}
SELECT p.pubname,
       c.relname AS tablename
FROM pg_publication p
JOIN pg_publication_rel pr ON p.oid = pr.prpubid
JOIN pg_class c ON pr.prrelid = c.oid;
```

## Monitoring and troubleshooting

### PlanetScale metrics for CDC monitoring

PlanetScale provides built-in metrics that are essential for monitoring your CDC setup. Access these through your **Metrics dashboard** to track replication health and performance:

| Metric Category           | Key Indicators for CDC  | What to Monitor                                                                                   |
| ------------------------- | ----------------------- | ------------------------------------------------------------------------------------------------- |
| **WAL archival rate**     | Success/Failed counts   | Monitor failed WAL archival attempts that could impact CDC streams                                |
| **WAL archive age**       | Seconds behind          | Age of oldest unarchived WAL - should be under 60 seconds for healthy CDC                         |
| **WAL storage**           | Storage usage in MB     | Track WAL disk usage; high usage may indicate CDC consumers falling behind                        |
| **Replication lag**       | Lag in seconds          | Monitor delay between primary and replicas; high lag may indicate CDC consumer performance issues |
| **Transaction rate**      | Transactions per second | Track database workload intensity affecting CDC processing                                        |
| **Memory**                | RSS and Memory mapped   | Monitor memory pressure that could impact logical decoding performance                            |
| **Primary Storage Usage** | MB disk utilization     | Monitor disk utilization to be sure WAL files are being consumed quickly enough                   |

<Note>
  For detailed information about interpreting these metrics, see the [Cluster
  Metrics](/docs/postgres/monitoring/metrics) documentation.
</Note>

### Monitoring replication lag

Check replication slot lag. The replication\_lag column shows how much WAL data the publisher is keeping because the subscriber has not confirmed or processed it yet. This value should be kept well below `max_wal_size`.

```sql  theme={null}
SELECT
    slot_name,
    database,
    active,
    restart_lsn,
    confirmed_flush_lsn,
    pg_size_pretty(pg_wal_lsn_diff(pg_current_wal_lsn(), restart_lsn)) AS replication_lag
FROM pg_replication_slots
WHERE slot_type = 'logical';
```

### WAL retention and disk usage

Monitor WAL retention to prevent disk space issues. This is another way to see similar information, and will include any PlanetScale HA replicas.

```sql  theme={null}
SELECT
    slot_name,
    pg_size_pretty(pg_wal_lsn_diff(pg_current_wal_lsn(), restart_lsn)) AS retained_wal
FROM pg_replication_slots;
```

### Common issues

**Issue**: WAL disk space growing rapidly\
**Cause**: Inactive or slow CDC consumers\
**Solution**: Remove unused slots or troubleshoot slow consumers

**Issue**: Failover breaks CDC stream\
**Cause**: Replication slot not properly synchronized\
**Solution**: Verify failover configuration and slot synchronization status

## Best practices

1. **Always enable failover**: **Never** deploy CDC to production without `failover = true` on replication slots and proper PlanetScale cluster configuration
2. **Verify configuration**: Double-check that both your CDC tool and PlanetScale settings are properly configured before going live
3. **Test failover scenarios**: Test actual failover events in staging environments to ensure your configuration works
4. **Regular monitoring**: Monitor replication lag, WAL retention, and slot synchronization status
5. **Slot cleanup**: Remove unused logical replication slots to prevent WAL accumulation
6. **CDC client resilience**: Ensure CDC clients can handle connection interruptions gracefully

## Security considerations

* Logical replication exposes table data - ensure proper access controls
* Use dedicated database users with minimal required privileges for CDC
* Consider network security when streaming to external systems
* Monitor for unauthorized replication slots

For more information about cluster configuration parameters, see the [Cluster configuration parameters](/docs/postgres/cluster-configuration/parameters) documentation.


# Anomalies
Source: https://planetscale.com/docs/postgres/monitoring/anomalies

Anomalies are defined as periods with a substantially elevated percentage of slow-running queries.

## Overview

PlanetScale Insights continuously analyzes your query performance to establish a baseline for expected performance. When a high enough percentage of queries are running more slowly than the baseline expectation, we call this an anomaly.

## Using the Anomalies graph

The graph shown under the Anomalies tab shows the percentage of queries executing slower than the 97.7th (2-sigma) percentile baseline on the y-axis and the period of time on the x-axis. The "expected" line shows the percent of queries that are statistically expected in a database with uniform query performance over time. Slight deviations from the expected value are normal. Only substantial and sustained deviations from the expected value are considered an anomaly.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/anomalies/database-health-graph.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=580a4e81e18689435ede8b8dc369a5d3" alt="Database health graph showing two anomalies" data-og-width="2342" width="2342" data-og-height="1294" height="1294" data-path="docs/images/assets/docs/concepts/anomalies/database-health-graph.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/anomalies/database-health-graph.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=aa7cd0fc5248ddeeafd7fb674ae46bde 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/anomalies/database-health-graph.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=b3f3c7ab58430026771d9584d4037587 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/anomalies/database-health-graph.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=b7d06c37f44fbb46de877510f1710460 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/anomalies/database-health-graph.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=23e23aa35f109df83f87825f781863d0 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/anomalies/database-health-graph.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=645c9bea0a4d4cd1c407ff196b2bd5f8 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/anomalies/database-health-graph.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=560f66942e734c6bec489a3ce94d8b2b 2500w" />
</Frame>

Any periods where your database was unhealthy will be highlighted with a red icon representing a performance anomaly. Each anomaly on the graph is clickable. Clicking on it will pull up more details about it in the table below the graph, such as: duration, percentage of increase, and when the anomaly occurred. We also overlay any deploy requests that happened during that period over the anomaly graph.

On top of this, we also surface any impact to the following:

* The query that triggered the anomaly
* CPU utilization
* Memory
* IOPS
* Queries per second
* Rows written per second
* Rows read per second
* Errors per second

## Anomalies vs query latency

You may notice a correlation between some areas in the query latency graph and the anomalies graph. Conversely, in some cases, you may see a spike in query latency, but no corresponding anomaly.

Increased query latency *can* be indicative of an anomaly, but not always. Query latency may increase and decrease in ways that don't always indicate an actual problem with your database.

For example, you may run a weekly report that consists of a few slow-running queries. These queries are always slow. Every week, you'll see a spike on your query latency graph during the time that your weekly report is generated, but not on your anomaly violations graph. The queries are running at their *expected* latency, so this is not considered an anomaly.

## What should I do if my database has an anomaly?

The purpose of the Anomalies tab is to show you relevant information so you can determine what caused an anomaly and correct the issue.

Let's look at an example scenario. You deploy a feature in your application that contains a new query. This query is slow, running frequently, and is hogging database resources. This new slow query is running so often that it's slowing down the rest of your database. Because your other queries are now running slower than expected, an anomaly is triggered.

In this case, we will surface the new slow-running query so that you can find ways to optimize it to free up some of the resources it's using. Adding an index will often solve the problem. You can test this by adding the index, creating a deploy request, and deploying it. If it's successful, you'll quickly see the anomaly end.

On the other hand, an anomaly does not necessarily mean you need to take any action. One common example where you may see an anomaly is in the case of large active-running backups. In this case, we will tell you that a backup was running during the time of the anomaly.

<Note>
  Even if it causes an anomaly, we do not recommend you turn off backups to prevent possible data loss.
</Note>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Cluster Logs
Source: https://planetscale.com/docs/postgres/monitoring/logs

The Logs dashboard provides comprehensive logging and debugging capabilities for your PlanetScale Postgres database cluster. This centralized view helps you monitor database activity, troubleshoot issues, and analyze system behavior in real-time.

## Dashboard overview

The Logs dashboard displays real-time and historical log entries from your database cluster across all servers. You can filter and analyze logs by:

* **Server Filter**: Monitor all servers or focus on specific replica instances
* **Log Level**: Filter by severity (DEBUG, INFO, WARNING, ERROR)
* **Time Range**: View logs from the past hour up to custom time ranges
* **Branch**: Select which database branch to monitor
* **Search**: Full-text search across log messages
* **Live update**: Toggle on/off the auto-refresh of data every \~10 seconds

## Log filtering and navigation

### Server selection

The server filter dropdown allows you to choose which database instances to view logs from. You can select the Primary, Replicas, or all servers. By default just the Primary is shown:

| Filter Option | Purpose                            |
| :------------ | :--------------------------------- |
| **Primary**   | Monitor main database server logs  |
| **Replicas**  | Monitor individual replica servers |

### Log level filtering

By default all log levels are displayed. You can filter them based on the following log levels:

| Level       | Color Code | Purpose                                    |
| :---------- | :--------- | :----------------------------------------- |
| **DEBUG**   | Gray       | Detailed system information                |
| **INFO**    | Blue       | General information messages               |
| **WARNING** | Yellow     | Potential issues requiring attention       |
| **ERROR**   | Red        | Critical issues requiring immediate action |

### Time range selection

The time range selector offers several preset options:

* **Past 15 minutes**
* **Past hour**
* **Past 3 hours**
* **Past 6 hours**
* **Past 12 hours**
* **Past day**

**Custom range**: Set specific date and time ranges for targeted analysis

<Note>
  All log timestamps are displayed in the timezone configured in the top right of the console under the drop down 'Timezone display'
</Note>

## Log retention

By default, logs are retained for **7 days**. This retention period ensures you have access to recent log history for troubleshooting and analysis while managing storage efficiently.

## Search and filtering

### Full-text search

Use the search bar to find specific:

* Error messages and stack traces
* Connection identifiers
* Query patterns
* Configuration changes

### LogQL search syntax

PlanetScale's logging capabilities utilize Victoria Logs, which supports powerful LogQL query syntax. Click the `?` icon in the search box to see syntax help. Common examples include:

**Text search:**

* `error` - Find logs containing "error"
* `"received message"` - Exact phrase search

**Field filtering:**

* `planetscale.pod:h2l-pod` - Filter by specific pod
* `planetscale.role:replica` - Filter by database role
* `planetscale.container:pgbouncer` - Filter by container type
* `planetscale.availability_zone:us-east-1a` - Filter by availability zone

**Exclusion filters:**

* `NOT error` - Exclude logs containing "error"
* `NOT "received message"` - Exclude specific phrases

**Logical operators:**

* `"received message" OR planetscale.container:pgbouncer` - Match either condition

For complete LogQL syntax documentation, see: [Victoria Logs LogQL Documentation](https://docs.victoriametrics.com/victorialogs/logsql/)

### Advanced filtering

Combine multiple filters for precise log analysis and advanced log discovery:

* **Server type + Log level**: Filter Primary/Replica servers by ERROR/WARNING levels for targeted troubleshooting
* **Time window + Search terms**: Narrow down to specific time ranges with LogQL queries for incident investigation
* **Log level + Search bar**: Combine severity filtering with text search for comprehensive debugging
* **Server selection + Time range + LogQL**: Use all filtering options together for precise log discovery

## Log message structure

When you click on a log entry, a detailed view appears on the right side showing:

| Field                 | Description                                    | Example                                               |
| :-------------------- | :--------------------------------------------- | :---------------------------------------------------- |
| **Time**              | Exact timestamp of the log event with timezone | `2025-06-23 17:31:27 UTC`                             |
| **Level**             | Log severity level with color coding           | `INFO`, `DEBUG`, `ERROR`, `WARNING`                   |
| **Role**              | Database server role                           | `primary`                                             |
| **Pod**               | Kubernetes pod identifier                      | `h2l-8abod1fll1tn-aws-useast1b-1-1335211508-467a6a97` |
| **Container**         | Container identifier                           | `postgres`                                            |
| **Availability Zone** | Geographic server location                     | `us-east-1a`                                          |
| **Message**           | Full log message content                       | Complete error messages, SQL queries, system events   |

The Logs dashboard serves as your primary debugging and monitoring tool, providing the detailed visibility needed to maintain optimal database performance and quickly resolve issues.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Cluster Metrics
Source: https://planetscale.com/docs/postgres/monitoring/metrics

The Metrics dashboard provides comprehensive monitoring and observability for your PostgreSQL database cluster.

export const YouTubeEmbed = ({id, title}) => {
  return <Frame>
      <iframe src={`https://www.youtube-nocookie.com/embed/${id}?rel=0`} title={title} className="aspect-video w-full" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" />
    </Frame>;
};

This centralized view helps you track performance, identify bottlenecks, and ensure optimal database health.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/monitoring/metrics.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=cf28cd7f5f739340efab9fbf9793f863" alt="Metrics Dashboard" data-og-width="4018" width="4018" data-og-height="2616" height="2616" data-path="docs/postgres/monitoring/metrics.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/monitoring/metrics.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=a82363debe238bd1ee141ccc269ab021 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/monitoring/metrics.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=8c87b5efaca0c26ae981922b43bf3ae2 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/monitoring/metrics.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=2c9372d754e82797b28f2f7c7baf47ce 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/monitoring/metrics.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=bc55a3fccbff51cdd3c7648149ce0aa6 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/monitoring/metrics.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=63e7a3986c0eda2280699cac377f5488 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/monitoring/metrics.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=4cd8453551c06d76ffe032c9f873d255 2500w" />
</Frame>

## Dashboard overview

The Metrics dashboard displays real-time and historical data about your database cluster's performance across multiple dimensions. You can filter metrics by:

* **Server Filter**: Monitor all servers or focus on specific instances
* **Branch**: Select which database branch to monitor
* **Time Range**: View data from the past 15 minutes up to custom time ranges
* **Live update**: Toggle on/off the auto-refresh of data every \~30 seconds

<YouTubeEmbed id="0Rg1dS2vPjs" title="Data drop: Metrics for Postgres" />

## Key metrics categories

### Primary cluster utilization

The primary cluster utilization panel shows your primary database server's resource consumption:

| Metric     | Unit    | Purpose                    | Key Insights                                                                    |
| :--------- | :------ | :------------------------- | :------------------------------------------------------------------------------ |
| **CPU**    | Percent | Real-time CPU utilization  | Monitor for consistent performance and identify when optimization may be needed |
| **Memory** | Percent | Current memory consumption | Track memory usage patterns and plan for scaling when approaching limits        |

### Replica monitoring

Each replica displays individual performance metrics in dedicated panels:

| Metric     | Unit    | Purpose                                | Key Insights                                                               |
| :--------- | :------ | :------------------------------------- | :------------------------------------------------------------------------- |
| **CPU**    | Percent | Individual CPU tracking per replica    | Compare replica performance against primary and identify load distribution |
| **Memory** | Percent | Individual memory tracking per replica | Monitor replica resource consumption and ensure balanced utilization       |

### Primary IOPS

| Metric   | Unit              | Purpose                                          | Key Insights                                                                      |
| :------- | :---------------- | :----------------------------------------------- | :-------------------------------------------------------------------------------- |
| **IOPS** | Operations/second | Tracks database read/write operations per second | Monitor I/O patterns and identify peak usage periods for performance optimization |

### Primary storage usage

Storage metrics vary depending on your cluster's storage type:

**For Network-attached Storage clusters:**

| Metric            | Unit  | Purpose                     | Key Insights                                                                     |
| :---------------- | :---- | :-------------------------- | :------------------------------------------------------------------------------- |
| **Storage Usage** | MB/GB | Current storage consumption | Track storage growth trends for capacity planning and ensure adequate free space |

**For PlanetScale Metal clusters:**

| Metric                 | Unit       | Purpose                  | Key Insights                                                                                      |
| ---------------------- | ---------- | ------------------------ | ------------------------------------------------------------------------------------------------- |
| **Primary disk usage** | Percentage | Current disk utilization | Monitor disk space consumption on Metal instances where storage is directly tied to instance size |

<Note>
  **Storage metric differences**: Network-attached storage clusters show absolute storage usage (MB/GB), while PlanetScale Metal clusters display disk usage as a percentage since storage scaling requires changing the entire instance size.
</Note>

### Memory

| Metric             | Unit | Purpose                                            | Key Insights                                                 |
| :----------------- | :--- | :------------------------------------------------- | :----------------------------------------------------------- |
| **Memory mapped**  | MB   | Memory-mapped files used by PostgreSQL             | Monitor shared memory usage for buffer pools and shared data |
| **RSS**            | MB   | Resident Set Size - physical memory currently used | Track actual memory consumption by PostgreSQL processes      |
| **Active cache**   | MB   | Recently accessed cached data in memory            | Monitor hot data retention and cache effectiveness           |
| **Inactive cache** | MB   | Less recently accessed cached data                 | Track memory available for eviction under pressure           |

### Transaction rate

| Metric                      | Unit | Purpose                         | Key Insights                                               |
| :-------------------------- | :--- | :------------------------------ | :--------------------------------------------------------- |
| **Transactions per second** | TPS  | Database transaction throughput | Monitor database workload intensity and performance trends |

### Postgres connections

| Metric                            | Unit  | Purpose                                    | Key Insights                                                         |
| :-------------------------------- | :---- | :----------------------------------------- | :------------------------------------------------------------------- |
| **Active**                        | Count | Currently executing connections            | Monitor active database workload and concurrent operations           |
| **Disabled**                      | Count | Disabled connections                       | Track connection state management and administrative actions         |
| **Fastpath function call**        | Count | Connections using fastpath protocol        | Monitor specialized connection types for performance optimization    |
| **Idle**                          | Count | Idle but connected sessions                | Track connection pool efficiency and unused capacity                 |
| **Idle in transaction**           | Count | Idle connections holding transactions      | Identify potential long-running transactions that may cause blocking |
| **Idle in transaction (aborted)** | Count | Idle connections with aborted transactions | Monitor failed transaction cleanup and connection state recovery     |

### PgBouncer connections

| Metric                | Unit  | Purpose                     | Key Insights                                                              |
| :-------------------- | :---- | :-------------------------- | :------------------------------------------------------------------------ |
| **Total Connections** | Count | Active database connections | Monitor connection patterns and trends for capacity planning cluster size |

### PgBouncer peer utilization

| Metric     | Unit    | Purpose                        | Key Insights                                                   |
| :--------- | :------ | :----------------------------- | :------------------------------------------------------------- |
| **CPU**    | Percent | PgBouncer process CPU usage    | Monitor connection pooler performance and resource consumption |
| **Memory** | Percent | PgBouncer process memory usage | Track memory usage of the connection pooling layer             |

### PgBouncer server pools

| Metric              | Unit  | Purpose                             | Key Insights                                            |
| :------------------ | :---- | :---------------------------------- | :------------------------------------------------------ |
| **Active**          | Count | Active server connections           | Monitor backend database connections from the pool      |
| **Active Cancel**   | Count | Connections being cancelled         | Track connection cleanup and cancellation events        |
| **Being Cancelled** | Count | Connections in cancellation process | Monitor connection state transitions                    |
| **Idle**            | Count | Idle server connections             | Track connection pool efficiency and unused connections |
| **Login**           | Count | Connections in login state          | Monitor authentication and connection establishment     |
| **Testing**         | Count | Connections being tested            | Track connection health check activities                |
| **Tested**          | Count | Recently tested connections         | Monitor connection validation processes                 |
| **Used**            | Count | Total used connections              | Overall connection utilization from the pool            |

### PgBouncer client pools

| Metric            | Unit  | Purpose                               | Key Insights                                                |
| :---------------- | :---- | :------------------------------------ | :---------------------------------------------------------- |
| **Active**        | Count | Active client connections             | Monitor incoming client connection load                     |
| **Active Cancel** | Count | Client connections being cancelled    | Track client-side connection cleanup                        |
| **Waiting**       | Count | Client connections waiting for server | Identify connection queue buildup and potential bottlenecks |

### WAL archival rate

| Metric      | Unit  | Purpose                         | Key Insights                                                    |
| :---------- | :---- | :------------------------------ | :-------------------------------------------------------------- |
| **Success** | Count | Successfully archived WAL files | Monitor backup and replication health                           |
| **Failed**  | Count | Failed WAL archival attempts    | Track archival failures that could impact recovery capabilities |

### WAL archive age

| Metric      | Unit | Purpose                      | Key Insights                                                     |
| :---------- | :--- | :--------------------------- | :--------------------------------------------------------------- |
| **Seconds** | Time | Age of oldest unarchived WAL | Monitor WAL archival latency and ensure timely backup operations |

### WAL storage

| Metric            | Unit | Purpose                             | Key Insights                                                      |
| :---------------- | :--- | :---------------------------------- | :---------------------------------------------------------------- |
| **Storage Usage** | MB   | Write-ahead log storage consumption | Track WAL disk usage for capacity planning and cleanup monitoring |

### Replication lag

| Metric  | Unit    | Purpose                                | Key Insights                                                                      |
| :------ | :------ | :------------------------------------- | :-------------------------------------------------------------------------------- |
| **Lag** | Seconds | Time delay between primary and replica | Monitor replication health and ensure acceptable lag for read replica consistency |

## Interpreting metrics

### Normal operating ranges

* **CPU**: 0-30% for typical workloads
* **Memory**: 20-80% depending on dataset size
* **IOPS**: Varies by workload type (OLTP vs. analytics)
* **Storage Usage**: Keep below 80% for optimal performance (applies to both absolute storage usage and disk usage percentage)

### Performance indicators

* **Consistent Low CPU/Memory**: Indicates healthy, optimized queries
* **Spiky IOPS**: May indicate batch processing or analytical workloads
* **Low Connection Pool Utilization**: Suggests efficient connection management
* **High Active Cache vs Inactive Cache**: Indicates good data locality and efficient query patterns
* **Low Transaction Rate**: May indicate application bottlenecks or connection pooling issues
* **High Idle in Transaction**: Suggests application issues with transaction management

### Troubleshooting with metrics

* **High CPU**: Check for inefficient queries or missing indexes
* **High Memory**: Monitor for high memory usage from large queries or buffer cache pressure
* **High IOPS**: Analyze query patterns and consider query optimization
* **High Storage/Disk Usage**: Plan for storage scaling or data archiving (for Metal clusters, this requires instance resizing)
* **High RSS vs Memory Mapped**: May indicate memory pressure or suboptimal shared\_buffers configuration
* **Low Transaction Rate**: Investigate connection pooling, application logic, or database locks
* **High Idle in Transaction**: Review application transaction handling and connection management
* **Imbalanced Cache (Active/Inactive)**: Consider adjusting memory settings or query optimization

### WAL monitoring best practices

* **Archive age**: Should typically be under 60 seconds for healthy systems
* **Archival success rate**: Aim for 100% success rate with zero failures
* **WAL storage**: Monitor for steady-state usage with periodic cleanup cycles
* **Replication lag**: High lag may indicate WAL transmission issues

## Best practices

1. **Baseline Establishment**: Understand your normal operating ranges
2. **Alert Thresholds**: Set up monitoring alerts for critical thresholds
3. **Trend Analysis**: Use historical data to predict scaling needs
4. **Performance Correlation**: Cross-reference metrics with application performance

The Metrics dashboard serves as your primary tool for maintaining optimal database performance and ensuring reliable service delivery.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Sending Prometheus Metrics to Datadog for PlanetScale Postgres
Source: https://planetscale.com/docs/postgres/monitoring/prometheus-metrics-datadog-postgres

If you're looking for more metrics than PlanetScale's native Datadog integration provides, this tutorial will show how to configure your [Datadog agent](https://docs.datadoghq.com/agent/) to scrape PlanetScale's [Prometheus infrastructure](/docs/vitess/integrations/prometheus-metrics) automatically, allowing you to collect detailed metrics for all of your PlanetScale Postgres branches.

## Overview

In this tutorial, this guide assumes that you have a Datadog Agent Version 7 running. For more information on what the Datadog Agent is and how to install it, start with the [Datadog Agent documentation](https://docs.datadoghq.com/getting_started/agent/).

For the purposes of this guide, the examples use a Datadog agent running with the recommended installation steps on a Linux system.

## Prerequisites

You'll need a working Datadog agent and access to add a [Custom Agent Check](https://docs.datadoghq.com/developers/custom_checks/) to that instance. This may require `root` or `sudo` access on the machine running the Datadog agent.

You'll also need a [Service token](/docs/api/reference/service-tokens) in your Organization, with the `read_metrics_endpoints` permission granted.

## Adding the Plugin to the Datadog Agent

Go to [https://github.com/planetscale/planetscale-datadog](https://github.com/planetscale/planetscale-datadog), which is the repository that has our custom OpenMetrics Check.

Place the unedited `planetscale.py` in the `checks.d` directory of your Datadog Agent.

* On Linux, that is `/etc/datadog-agent/checks.d/`
* On macOS, that is `/opt/datadog-agent/etc/checks.d/`

Make sure that it belongs to the appropriate user. If you're using the recommended Linux installation steps, it will have created a `dd-agent` user:

```bash  theme={null}
$ pwd
/etc/datadog-agent/checks.d
$ ls -al planetscale.py
-rw-r--r-- 1 dd-agent dd-agent 9261 Apr  2 22:54 planetscale.py
```

This file is owned by the `dd-agent` user and group in the `/etc/datadog-agent/checks.d` directory.

If you're on macOS, it will depend on whether you installed the agent as a 'Single User Agent' or a 'Systemwide Agent'. If you picked Single User, there should be no additional permission changes needed. If you installed it as a Systemwide agent, make sure the user and group you installed the agent with have ownership of the file.

## Configuring the Datadog Agent

Now that you have the plugin installed, you need to configure it. In the `conf.d` directory of the Datadog agent take the `conf.d/planetscale.yaml.example` file and edit it with your organization name and Service Token information. It should look like this:

```yaml expandable theme={null}
instances:
  - planetscale_organization: 'your-org' # Required: Your PlanetScale organization ID
    ps_service_token_id: '${TOKEN_ID}' # Required: Your PlanetScale Service Token ID
    ps_service_token_secret: '${TOKEN}' # Required: Your PlanetScale Service Token Secret. Consider using Datadog secrets management: https://docs.datadoghq.com/agent/guide/secrets-management/

    namespace: 'planetscale_postgres' # Required: Namespace for the metrics
    metrics: # Required: List of metrics to collect. Use mapping for renaming/type overrides.
      - planetscale_postgres_connection_state: postgres_connection_state
      - planetscale_edge_postgres_active_connections: edge_active\_connections

    min_collection_interval: 60
    send_distribution_buckets: true
    collect_counters_with_distributions: true
```

This configures the integration to look for all of the branches in your PlanetScale organization, collect specific Postgres metrics, and put them inside the `planetscale_postgres` namespace.

Save the file as `planetscale.yaml`, making sure to double check permissions:

```bash  theme={null}
$ pwd
/etc/datadog-agent/conf.d
$ ls -al planetscale.yaml
-rw-r--r-- 1 root root 1518 Apr  2 22:57 planetscale.yaml
```

## Restart the Datadog Agent

Now that this is configured and installed, restart the Agent:

```bash  theme={null}
$ sudo systemctl restart datadog-agent
```

## Validating the PlanetScale Plugin

Now that the Datadog Agent is running the PlanetScale plugin, metrics should start flowing into Datadog within a couple of minutes. To validate, you can check the Datadog Agent:

```bash  theme={null}
sudo -u dd-agent -- datadog-agent check planetscale
```

If the plugin is installed successfully, this should output the scrape targets for your branches, as well as metadata about when it was last run and how many metrics were emitted.

## Adding Metrics

In the earlier configuration, only two metrics were added. For a complete list of what PlanetScale Postgres exposes, please take a look at the [Postgres Metrics Reference Documentation](/docs/postgres/monitoring/prometheus-metrics-postgres).

Note that the Datadog agent [normalizes metrics with certain suffixes starting in v7.32.0](https://github.com/DataDog/integrations-core/blob/master/openmetrics/README.md):

> Starting in Datadog Agent v7.32.0, in adherence to the OpenMetrics specification standard, counter names ending in \_total must be specified without the \_total suffix. For example, to collect promhttp\_metric\_handler\_requests\_total, specify the metric name promhttp\_metric\_handler\_requests. This submits to Datadog the metric name appended with .count, promhttp\_metric\_handler\_requests.count.

This means that to scrape a metric such as `planetscale_postgres_database_xact_commit_total`, you would configure the Datadog agent for `planetscale_postgres_database_xact_commit`.

If you want to collect additional metrics, you can add them to the list:

```yaml  theme={null}
    metrics: # Required: List of metrics to collect. Use mapping for renaming/type overrides.
      - planetscale_postgres_connection_state: postgres_connection_state
      - planetscale_edge_postgres_active_connections: edge_active_connections
      - planetscale_postgres_wal_size_bytes: postgres_wal_size
      - planetscale_pgbouncer_current_connections: pgbouncer_connections
```

Then, restart the Datadog Agent:

```bash  theme={null}
$ sudo systemctl restart datadog-agent
```

## What's Next?

Now that you're sending Postgres metrics from PlanetScale to Datadog, take a look at our [full list](/docs/postgres/monitoring/prometheus-metrics-postgres) and start building dashboards to monitor your PostgreSQL database performance, connection health, and resource utilization!

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Prometheus metrics for PlanetScale Postgres
Source: https://planetscale.com/docs/postgres/monitoring/prometheus-metrics-postgres

PlanetScale Postgres exposes a Prometheus-compatible endpoint per-branch that allows you to scrape metrics for your database.

## Overview

See our [Prometheus integration](/docs/vitess/integrations/prometheus) documentation for how to set Prometheus up to automatically discover and scrape metrics for your database branches.

If you're using Datadog, see our [Datadog tutorial](/docs/postgres/monitoring/prometheus-metrics-datadog-postgres) for how to setup your Datadog agent to scrape metrics for your branch.

## Metrics

PlanetScale Postgres emits the following metrics to be scraped.

## Database Metrics

| **Name & Description**                                                                                             | **Type** | **Tags**                                                                                                                                                   |
| :----------------------------------------------------------------------------------------------------------------- | :------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **planetscale\_postgres\_connection\_state**  The count and state of Postgres connections                          | Gauge    | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_connection\_state, planetscale\_role, planetscale\_cell, planetscale\_component |
| **planetscale\_postgres\_wal\_archiver\_succeeded\_count**  The count of successfully archived WALs                | Counter  | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_role, planetscale\_cell, planetscale\_component                                 |
| **planetscale\_postgres\_wal\_archiver\_failed\_count**  The count of unsuccessfully archived WALs                 | Counter  | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_role, planetscale\_cell, planetscale\_component                                 |
| **planetscale\_postgres\_wal\_archiver\_last\_age\_succeeded**  The age of the last successfully archived WAL      | Gauge    | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_role, planetscale\_cell, planetscale\_component                                 |
| **planetscale\_postgres\_wal\_size\_bytes**  The cumulative disk size of WALs waiting to be archived               | Gauge    | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_role, planetscale\_cell, planetscale\_component                                 |
| **planetscale\_postgres\_settings\_max\_connections**  The current value of the max\_connections setting           | Gauge    | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_role, planetscale\_cell, planetscale\_component                                 |
| **planetscale\_postgres\_settings\_max\_wal\_size\_bytes**  The current value of the max\_wal\_size\_bytes setting | Gauge    | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_role, planetscale\_cell, planetscale\_component                                 |
| **planetscale\_postgres\_replica\_lag\_seconds**  Replica lag in fine-grained seconds from Postgres                | Gauge    | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_role, planetscale\_cell, planetscale\_component                                 |
| **planetscale\_postgres\_locks**  Count of current lock modes                                                      | Gauge    | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_lock\_mode, planetscale\_role, planetscale\_cell, planetscale\_component        |
| **planetscale\_postgres\_database\_xact\_commit\_total**  Total committed transactions on Postgres databases       | Counter  | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_database\_name, planetscale\_role, planetscale\_cell, planetscale\_component    |

## Edge Metrics

| **Name & Description**                                                                                                                            | **Type** | **Tags**                                                                                                       |
| :------------------------------------------------------------------------------------------------------------------------------------------------ | :------- | :------------------------------------------------------------------------------------------------------------- |
| **planetscale\_edge\_postgres\_active\_connections**  The number of active Postgres and PgBouncer connections to the branch                       | Gauge    | cluster, planetscale\_database\_branch\_id, planetscale\_port, planetscale\_region                             |
| **planetscale\_edge\_postgres\_connection\_drops\_total**  The total number of Postgres and PgBouncer connections that have been dropped          | Counter  | cluster, planetscale\_database\_branch\_id, planetscale\_port, planetscale\_region                             |
| **planetscale\_edge\_postgres\_connection\_errors\_total**  The total number of Postgres and PgBouncer connections that have resulted in an error | Counter  | cluster, planetscale\_database\_branch\_id, planetscale\_port, planetscale\_region                             |
| **planetscale\_edge\_postgres\_bytes\_sent\_total**  The total number of Postgres and PgBouncer bytes sent                                        | Counter  | cluster, planetscale\_database\_branch\_id, planetscale\_port, planetscale\_connector\_id, planetscale\_region |
| **planetscale\_edge\_postgres\_bytes\_received\_total**  The total number of Postgres and PgBouncer bytes received                                | Counter  | cluster, planetscale\_database\_branch\_id, planetscale\_port, planetscale\_connector\_id, planetscale\_region |

## PgBouncer Metrics

| **Name & Description**                                                                                                               | **Type** | **Tags**                                                                                                                                                                         |
| :----------------------------------------------------------------------------------------------------------------------------------- | :------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **planetscale\_pgbouncer\_total\_peers**  The total count of peered processes PgBouncer is running                                   | Gauge    | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_container, planetscale\_role, planetscale\_cell, planetscale\_component                               |
| **planetscale\_pgbouncer\_cpu\_util\_per\_peer\_percentages**  CPU utilization percentage of PgBouncer peered processes              | Gauge    | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_container, planetscale\_role, planetscale\_cell, planetscale\_component                               |
| **planetscale\_pgbouncer\_current\_connections**  The current count PgBouncer connections to Postgres                                | Gauge    | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_container, planetscale\_role, planetscale\_cell, planetscale\_component                               |
| **planetscale\_pgbouncer\_current\_client\_connections**  The current count client connections to PgBouncer                          | Gauge    | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_container, planetscale\_role, planetscale\_cell, planetscale\_component                               |
| **planetscale\_pgbouncer\_pools\_client**  The count and state of PgBouncer client connections                                       | Gauge    | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_container, planetscale\_pgbouncer\_pool, planetscale\_role, planetscale\_cell, planetscale\_component |
| **planetscale\_pgbouncer\_pools\_client\_maxwait\_seconds**  How long the first client connection has waited to connect to PgBouncer | Gauge    | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_container, planetscale\_role, planetscale\_cell, planetscale\_component                               |
| **planetscale\_pgbouncer\_pools\_server**  The count and state of PgBouncer server connections                                       | Gauge    | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_container, planetscale\_pgbouncer\_pool, planetscale\_role, planetscale\_cell, planetscale\_component |

## Infrastructure Metrics

| **Name & Description**                                                                               | **Type** | **Tags**                                                                                                                                                                         |
| :--------------------------------------------------------------------------------------------------- | :------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **planetscale\_pods\_cpu\_util\_percentages**  CPU utilization percentage of database pods           | Gauge    | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_container, planetscale\_role, planetscale\_cell, planetscale\_component                               |
| **planetscale\_pods\_mem\_util\_percentages**  Memory utilization percentage of database pods        | Gauge    | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_container, planetscale\_role, planetscale\_cell, planetscale\_component                               |
| **planetscale\_pods\_iops\_total**  Total IOPS (Input/Output Operations Per Second) of database pods | Counter  | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_container, planetscale\_role, planetscale\_cell, planetscale\_component                               |
| **planetscale\_volume\_available\_bytes**  Available storage space in bytes on Postgres volumes      | Gauge    | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_role, planetscale\_cell, planetscale\_component                                                       |
| **planetscale\_volume\_capacity\_bytes**  Total storage capacity in bytes on Postgres volumes        | Gauge    | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_role, planetscale\_cell, planetscale\_component                                                       |
| **planetscale\_pods\_mem\_rss\_bytes**  RSS memory usage in bytes of database pods                   | Gauge    | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_container, planetscale\_role, planetscale\_cell, planetscale\_component                               |
| **planetscale\_pods\_mem\_mmap\_bytes**  Memory-mapped file usage in bytes of database pods          | Gauge    | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_container, planetscale\_role, planetscale\_cell, planetscale\_component                               |
| **planetscale\_pods\_mem\_active\_cache\_bytes**  Active cache memory in bytes of database pods      | Gauge    | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_container, planetscale\_role, planetscale\_cell, planetscale\_component                               |
| **planetscale\_pods\_mem\_inactive\_cache\_bytes**  Inactive cache memory in bytes of database pods  | Gauge    | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_container, planetscale\_role, planetscale\_cell, planetscale\_component                               |
| **planetscale\_pods\_container\_restarts\_total**  Total container restart events detected           | Counter  | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_container, planetscale\_role, planetscale\_cell, planetscale\_component, planetscale\_restart\_reason |

## Tag Glossary

* **cluster**: The PlanetScale cluster identifier
* **planetscale\_access**: The access path on bytes sent/received metrics (**public** for traffic over the public internet, or **private** for traffic over AWS PrivateLink or GCP Private Service Connect)
* **planetscale\_database\_branch\_id**: The unique identifier for the database branch
* **planetscale\_pod**: The Kubernetes pod name
* **planetscale\_container**: The container name (postgres, pgbouncer, walg-daemon)
* **planetscale\_role**: The database role (primary, replica)
* **planetscale\_cell**: The PlanetScale cell identifier
* **planetscale\_component**: The PlanetScale component identifier
* **planetscale\_connection\_state**: The state of database connections
* **planetscale\_lock\_mode**: The PostgreSQL lock mode
* **planetscale\_database\_name**: The PostgreSQL database name
* **planetscale\_port**: The connection port number
* **planetscale\_region**: The geographic region
* **planetscale\_pgbouncer\_pool**: The PgBouncer connection pool identifier
* **planetscale\_connector\_id**: The edge connector identifier
* **planetscale\_restart\_reason**: The reason for container restart

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Prometheus integration for PlanetScale Postgres
Source: https://planetscale.com/docs/postgres/monitoring/prometheus-postgres

PlanetScale Postgres exposes Prometheus-compatible metrics endpoints for scraping metrics about your database branches. This, along with our API-driven service discovery, allows you to automatically get in-depth information about all of the Postgres databases in your organization.

In order to collect and store these, you will need to use Prometheus or a Prometheus-compatible metrics engine (such as VictoriaMetrics) that is capable of using the [HTTP SD](https://prometheus.io/docs/prometheus/latest/http_sd/) protocol.

## Prerequisites

This document assumes you'll be configuring a Prometheus 3.x instance via a configuration file running on your local machine.

If you are using managed Prometheus via AWS, GCP or another provider, you will have to deploy Prometheus to scrape and forward metrics via `remote_write`, as these services do not support scraping metrics.

## Getting Started

First, provision a new PlanetScale [Service token](/docs/api/reference/service-tokens) in your Organization settings. Make sure to save the ID and token, as they will not be visible after they've been generated.

When that's created, grant the token `read_metrics_endpoints` permissions and click "Save permissions". Your token should have the necessary permissions to access Postgres metrics endpoints.

## Configuring Prometheus

Now that you have a Service Token, you can add a scrape configuration for your PlanetScale organization. A minimal Prometheus configuration should look like the following:

```yaml  theme={null}
scrape_configs:
  - job_name: "${ORG}-postgres"
    http_sd_configs:
    - url: https://api.planetscale.com/v1/organizations/${ORG}/metrics
      authorization:
        type: "token"
        credentials: "${TOKEN_ID}:${TOKEN}"
      refresh_interval: 10m
```

Fill in your organization name in the `job_name` and `url`, and place the Service Token and ID that you created in the previous step for the credentials.

Save this file to `prometheus.yml` in your working directory.

## Start Prometheus

Run Prometheus pointed at this configuration file:

```bash  theme={null}
$ prometheus --config.file=prometheus.yml
```

By default, Prometheus will listen at `0.0.0.0:9090`, which means you can access it in your browser at [http://127.0.0.1:9090](http://127.0.0.1:9090/).

### Validating Service Discovery

First, make sure that Prometheus is properly querying the PlanetScale API for the right branches. If you go to `http://127.0.0.1:9090/service-discovery` you should see the job that you created earlier, with all of your Postgres branches listed under `Discovered labels`.

You can confirm that this matches what's in your organization by using the PlanetScale CLI:

```bash  theme={null}
$ pscale branch list your-postgres-database --org your-org
```

Now, if you go to your list of targets you should see each Postgres branch as an Endpoint with properly configured scraping targets.

## Querying Prometheus

Now that you're collecting metrics for your Postgres branches, the [reference guide](/docs/postgres/monitoring/prometheus-metrics-postgres) has a list of everything that PlanetScale exports.

Here are some example queries you can run:

### Database Connection State

To see the current connection states across your Postgres instances:

```sql  theme={null}
planetscale_postgres_connection_state{planetscale_database_branch_id="your-branch-id"}
```

### Active Edge Connections

To monitor active connections at the edge:

```sql  theme={null}
planetscale_edge_postgres_active_connections{planetscale_database_branch_id="your-branch-id"}
```

### WAL Size Monitoring

To track WAL size in bytes:

```sql  theme={null}
planetscale_postgres_wal_size_bytes{planetscale_database_branch_id="your-branch-id"}
```

### PgBouncer Connection Pools

To monitor PgBouncer connection pool status:

```sql  theme={null}
planetscale_pgbouncer_current_connections{planetscale_database_branch_id="your-branch-id"}
```

### Resource Utilization

To check CPU utilization across your database pods:

```sql  theme={null}
planetscale_pods_cpu_util_percentages{planetscale_database_branch_id="your-branch-id"}
```

Make sure the graph is set to stacked for multi-series metrics to get the best visualization.

## Next Steps

If you keep this Prometheus instance running, it will collect metrics every 30 seconds, and refresh the list of branches every 10 minutes.

For more information, see:

* [Postgres Metrics reference](/docs/postgres/monitoring/prometheus-metrics-postgres) for a complete list of metrics PlanetScale exposes
* [Sending metrics to Datadog](/docs/postgres/monitoring/prometheus-metrics-datadog-postgres) tutorial for using the Datadog agent to collect these metrics
* [Grafana dashboards](/docs/vitess/tutorials/prometheus-metrics-grafana) for visualizing these metrics (instructions applicable for Postgres metrics along with our [Postgres dashboard template](https://github.com/planetscale/grafana-dashboard/blob/main/postgres.json))

## Troubleshooting

### Service Discovery Issues

If you're not seeing your Postgres branches in the service discovery:

1. Verify your service token has the `read_metrics_endpoints` permission
2. Check that your organization name in the URL matches exactly
3. Ensure your service token credentials are correctly formatted as `${TOKEN_ID}:${TOKEN}`

### Missing Metrics

If specific metrics aren't appearing:

1. Confirm the branch is active and healthy
2. Check the scrape interval - some metrics may take time to appear
3. Verify the metric names against our [metrics reference](/docs/postgres/monitoring/prometheus-metrics-postgres)

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Query Insights
Source: https://planetscale.com/docs/postgres/monitoring/query-insights

PlanetScale Insights gives you a detailed look into **all active queries** running against your database.

export const YouTubeEmbed = ({id, title}) => {
  return <Frame>
      <iframe src={`https://www.youtube-nocookie.com/embed/${id}?rel=0`} title={title} className="aspect-video w-full" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" />
    </Frame>;
};

## Overview

This in-dashboard tool allows you to identify queries that are running too often, too long, returning too much data, producing errors, and more. You can scroll through the performance graph to detect the time of impacted performance, quickly identifying any recent issues.

You can also see a [list of all queries](#queries-overview) performed on your database in the last 24 hours. For further analysis, you can sort these by metrics like amount of rows read, time per query, and more.

With this built-in tool, you can easily diagnose issues with your queries, allowing you to optimize individual queries without much digging. We will also alert you of any active issues your database may be having in the [Anomalies](/docs/postgres/monitoring/anomalies) tab. This feature flags queries that are running significantly slower than expected.

<YouTubeEmbed id="4ymmF-Fedcw" title="Data drop: Query Insights for Postgres" />

## Insights page overview

To view Insights for your database, head to the [PlanetScale dashboard](https://app.planetscale.com), select your database, and click the "**Insights**" tab.

The dropdown on the top right lets you select which branch you want to analyze. You can also choose which servers you want to view insights for: primary or replicas.

<Frame><img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/monitoring/query-insights-overview.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=2e54ec60ade18a3f2221e09e6496a19e" alt="PlanetScale Insights overview page" data-og-width="2892" width="2892" data-og-height="1736" height="1736" data-path="docs/postgres/monitoring/query-insights-overview.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/monitoring/query-insights-overview.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=37532bfb9291a19c8ed5b5b682912193 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/monitoring/query-insights-overview.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=ae56ab37d1558387e9fd6b8550e8e314 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/monitoring/query-insights-overview.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=027741ef2aac0a500abf6dafe9f13168 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/monitoring/query-insights-overview.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=4b699ef96388c6af389df52e1f4ca86e 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/monitoring/query-insights-overview.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=4e4703a4ba2a49bd4f28a16a82033032 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/monitoring/query-insights-overview.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=bc997b20fd142523983164c113f6b0b7 2500w" /></Frame>

You can click the dates listed above the graph to scroll through the past seven days. To further narrow down query analysis, you can select a time range by clicking on the graph and dragging the cursor across. This will zoom in on the selected timeframe.

You also have the option to save a screenshot of the graph by clicking "Save".

### Queries overview table

The table underneath the graph shows all queries performed on your database in the selected timeframe (last 24 hours by default).

For more information about how to read and interpret this data, see the [Queries overview](#queries-overview) section.

### Insights graph tabs

Once you have selected the branch and server you want to analyze, you can begin exploring the insights for them in the following tabs:

<Columns cols={2}>
  <Card title="Query latency" icon="code" horizontal href="#query-latency" />

  <Card title="Anomalies" icon="triangle-exclamation" horizontal href="/docs/postgres/monitoring/anomalies" />

  <Card title="Queries" icon="code" horizontal href="#queries" />

  <Card title="Rows read" icon="code" horizontal href="#rows-read" />

  <Card title="Rows written" icon="code" horizontal href="#rows-written" />

  <Card title="Errors" icon="circle-exclamation" horizontal href="#errors" />
</Columns>

The remaining sections of this doc walk through how to interpret and act on the data in each tab. If you'd like to see a practical example of how to use Insights to debug a performance issue, check out our [Announcing Insights blog post](https://planetscale.com/blog/introducing-planetscale-insights-advanced-query-monitoring) or [this YouTube video](https://www.youtube.com/watch?v=kkjAxSViOAA) walking you through an example.

## Query latency

The default tab depicts your database's query latency in milliseconds over the last 24 hours.

By default, the graph contains two line charts showing `p50` and `p95` latency. This means 50% and 95% of requests, respectively, completed faster than the time listed. You can also click on the `p99` and `p99.9` pills to toggle those on, or click `p50` or `p95` to toggle those off.

## Queries

The Queries tab displays insights about all active running queries in your database. The graph displays total queries per second against the specified time period.

## Rows read

The Rows read tab displays the total number of rows read per second across the selected time period.

## Rows written

The Rows written tab displays the total number of rows written per second across the selected time period.

## Errors

The Errors tab surfaces any errors that have been captured on your database in a 24 hour period.

Underneath the graph, you'll find a list of database error messages that have been captured over the selected period.

You can click on any of the error messages on the Errors tab to open a more detailed view. This view shows you the individual queries that produced the error, when they ran, how long they ran, and any query tags attached to them.

## Queries overview

The table underneath the graph shows queries performed on your database in the selected timeframe (last 24 hours by default).

<Note>
  The queries table does not show following statements types: `BEGIN`, `COMMIT`, `RELEASE`, `ROLLBACK`, `SAVEPOINT`, `SAVEPOINT_ROLLBACK`, `SET`.
</Note>

Queries are listed with literals replaced by ordinal placeholder values (e.g. `$1`). Normalizing queries in this way allows them to be grouped together into patterns, irrespective of the specific parameters used in the underlying query.

You may also see one or more orange icons next to some queries.

* An exclamation point icon indicates that the query is not currently using an index and requires a full table scan.

Hovering over the icon will show a tooltip with information about the meaning of the icon.

The query overview table shows the same data for all graph tabs except for [Anomalies](/docs/vitess/monitoring/anomalies) and [Errors](#errors). For more information about the content for each of those, refer to each Anomalies and Errors sections above.

### Available query statistics

You can customize the metrics that show up on the Queries list by selecting columns in the "View options" dropdown.

* **Query** - The query that was run.
* **Schema** - The default database and schema associated with the connection that issued the query, in the format `database.schema`.
* **Qualified table** — The table(s) referenced in the query, in the format `database.schema.table_name`.
* **Table schema** — The schema(s) associated with the tables referenced in the query, in the format `database.schema`. (This may differ from the connection schema.)
* **Table** — The table(s) being queried or modified.
* **% of runtime** — The percent of the total runtime the query pattern is responsible for (query pattern time divided by the cumulative time of all query patterns on your database).
* **% of CPU** — The percent of the total CPU time the query pattern is responsible for (query pattern CPU time divided by the cumulative CPU time of all query patterns on your database).
* **% of I/O** — The percent of the total I/O time the query pattern is responsible for (query pattern IO time divided by the cumulative IO time of all query patterns on your database). This column is only present if `track_io_timing` parameter is set in your database's cluster configuration.
* **Count** — The number of times this query has run.
* **Total time (s)** — The total time the query has run in seconds.
* **CPU time (s)** — The cumulative CPU time the query has consumed in seconds.
* **I/O time (s)** — The cumulative I/O time the query has consumed in seconds. This column is only present if `track_io_timing` parameter is set in your database's cluster configuration.
* **`p50` latency** — The `p50` latency for the query in milliseconds. This means that 50% of requests completed faster than the time listed.
* **`p99` latency** — The `p99` latency for the query in milliseconds. This means that 99% of requests completed faster than the time listed.
* **Max latency** — The maximum observed latency for the query in milliseconds.
* **Rows returned** — The total number of rows fetched by a `SELECT` statement. This includes all times the query has run in the displayed time frame.
* **Rows read** — The total number of rows read. This includes all times the query has run in the displayed time frame.
* **Rows read/rows returned** — The result of dividing total rows read by rows returned in a query. A high number can indicate that your database is reading unnecessary rows, and the query may be improved by adding an index.
* **Rows affected** — The total number of rows modified by an `INSERT`, `UPDATE`, or `DELETE` statement. This includes all times the query has run in the displayed time frame.
* **Block cache hit ratio** – The percentage of blocks read from the shared buffers cache for this query during its execution, avoiding more costly disk reads.
* **Blocks hit** – The total number of blocks read from the shared buffers cache when executing this query.
* **Blocks read** – The total number of blocks read from disk when executing this query.
* **Blocks dirtied** – The total number of blocks modified (but not necessarily flushed to disk) during query execution.
* **Blocks written** – The total number of blocks written to disk during query execution.
* **Bytes returned** – The total number of bytes returned to clients in query responses.
* **Bytes returned per query** — The total number of bytes returned to clients in query responses divided by the number of queries.
* **Last run** — The last time a query was run.

You can also sort the columns for quick analysis by clicking on the title at the top of each column.

If `Show sparklines` is selected, numeric columns in the queries table show a time series graph of the value within the selected time period.

#### Enabling I/O columns

The **% of I/O** and **I/O time** columns require the `track_io_timing` PostgreSQL config setting to be set to 'on'. This setting can be changed in the "Parameters" tab of the datatbase's cluster configuration. Note that we only begin collection I/O query performance after `track_io_timing` is enabled. Enabling `track_io_timing` may impact query performance.

### Query filtering

The search bar above the table allows you to filter queries as needed. You can filter for query SQL, schema (connection schema, and/or schema of tables referenced by the query), table name, query count, query latency, index name, and if the query was indexed. Click on the `?` next to the search bar for the full list of search syntax.

### Query deep dive

Clicking on a query in the Queries list will open a new page with more information about that query.

You'll first see the full query pattern, which displays the query with data normalized away. This query may run several times with different values, which Insights combines into a single query pattern.

You can display an LLM-generated summary of the query by clicking "Summarize query."

#### Additional query information

Beneath the query pattern is a graph with more information about the query. The set of available metrics/tabs include: Query latency, Queries, Rows read, Rows written, Errors and Indexes. The Indexes graph (which is not shown on the database-level page) shows the percentage of queries that used each of the listed indexes in each time bucket.

Beneath the time series graphs you will see summary statistics for the query pattern. These data are scoped to the same time period shown in the main query pattern graphs. The available metrics have the same definitions as the query statistics listed in the main insights tab.

Queries that use an index include a horizontal bar graph that shows the cumulative usage of each index over the complete time period shown in the main query pattern graphs.

To change the time period reflected in the graphs and summary statistics, click and drag to restrict the time window, or click on one of the day icons above the graph to select a different day.

#### Notable queries

Underneath the graph, you'll see a table with more information about notable instances of the query, which are defined as queries that took longer than 1s, read more than 10,000 rows, or produced an error.

If any of the selected queries have [SQL comment tags](https://google.github.io/sqlcommenter/) attached, you'll see the key-value pairs in the table under `Tags`.

The table also surfaces when the query started, rows returned, rows read, rows affected, the time it took the query to run (in ms), and the user associated with the query.

## Extension configuration

This section describes the configuration parameters available for the `pginsights` extension, which is responsible for sending query telemetry to the PlanetScale Insights pipeline. These settings can be changed in the Extensions tab on your database's Clusters page.

### Raw query collection

* **Setting:** `pginsights.raw_queries`
* **Default:** `false` (disabled)

When enabled, Insights collects the full query text for notable queries, including all literal values. When disabled, only the normalized SQL, with literals removed, will be collected.

Enabling complete query collection can be helpful when performance varies significantly within the same query pattern, and you need to see the full SQL statement without placeholders to troubleshoot the underlying issue.

<Note>
  Enabling this setting may result in sensitive data that appears in queries being sent to PlanetScale, where it will be processed and stored in accordance with our privacy policy.
</Note>

### Schema name normalization

* **Setting:** `pginsights.normalize_schema_names`
* **Default:** `false` (disabled)

When enabled, [schema](https://www.postgresql.org/docs/current/ddl-schemas.html) names appearing in queries are normalized as if they were literal values.

Consider the following example query: `select * from myschema.users where id = 1`.

* With `pginsights.normalize_schema_names` set to false, the query will be reported in insights as `select * from myschema.users where id = $1`
* With `pginsights.normalize_schema_names` set to true, the query will be reported in insights as `select * from $1.users where id = $2`

This setting is useful for databases using a schema-per-tenant design, where each user or tenant's data is stored in an isolated Postgres schema. With this feature enabled, query patterns that are identical except for the namespace will be grouped together, resulting in fewer distinct query patterns and more navigable insights. If you are concerned that performance problems may be isolated to only particular schemas, we recommend enabling the `pginsights.raw_queries` setting so that the full query text (including namespaces) is reported along with slow queries.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Schema recommendations
Source: https://planetscale.com/docs/postgres/monitoring/schema-recommendations

With schema recommendations inside of [PlanetScale Insights](/docs/postgres/monitoring/query-insights), you will automatically receive recommendations to improve database performance, reduce memory and storage, and improve your schema.

Schema recommendations use query-level telemetry data, PostgreSQL system tables, and your database's schema to generate tailored recommendations in the form of DDL statements.

<img src="https://mintcdn.com/planetscale-cad1a68a/V2EuARQmozG4IzT3/docs/postgres/monitoring/schema-recommendations-postgres.png?fit=max&auto=format&n=V2EuARQmozG4IzT3&q=85&s=c13558e1580b69c2855f52d095af1556" alt="Schema recommendations" data-og-width="2650" width="2650" data-og-height="1816" height="1816" data-path="docs/postgres/monitoring/schema-recommendations-postgres.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/V2EuARQmozG4IzT3/docs/postgres/monitoring/schema-recommendations-postgres.png?w=280&fit=max&auto=format&n=V2EuARQmozG4IzT3&q=85&s=e01a75ba7ea3821f1568a36b2b8b5f27 280w, https://mintcdn.com/planetscale-cad1a68a/V2EuARQmozG4IzT3/docs/postgres/monitoring/schema-recommendations-postgres.png?w=560&fit=max&auto=format&n=V2EuARQmozG4IzT3&q=85&s=e6f6a8a61e0dbeed9b80ada2de529737 560w, https://mintcdn.com/planetscale-cad1a68a/V2EuARQmozG4IzT3/docs/postgres/monitoring/schema-recommendations-postgres.png?w=840&fit=max&auto=format&n=V2EuARQmozG4IzT3&q=85&s=0a5e743554db4663ef0003cee9025670 840w, https://mintcdn.com/planetscale-cad1a68a/V2EuARQmozG4IzT3/docs/postgres/monitoring/schema-recommendations-postgres.png?w=1100&fit=max&auto=format&n=V2EuARQmozG4IzT3&q=85&s=62cbee971f2310b4c80269600f083761 1100w, https://mintcdn.com/planetscale-cad1a68a/V2EuARQmozG4IzT3/docs/postgres/monitoring/schema-recommendations-postgres.png?w=1650&fit=max&auto=format&n=V2EuARQmozG4IzT3&q=85&s=f2c84bbdaafe9586737b77c130bcfc3f 1650w, https://mintcdn.com/planetscale-cad1a68a/V2EuARQmozG4IzT3/docs/postgres/monitoring/schema-recommendations-postgres.png?w=2500&fit=max&auto=format&n=V2EuARQmozG4IzT3&q=85&s=defe5ff21f3eef50454026fafac61935 2500w" />

## How to use schema recommendations

To find the schema recommendations for your database, go to the “**Insights**” tab in your PlanetScale database and click “**View recommendations**.”

You will see the current open recommendations that may help improve database performance. Select a recommendation to learn more.

Each recommendation will have the following:

* An explanation of the recommended changes, including some of the benefits of the recommended change (E.g., reduced memory and storage, decreased execution time, prevent ID exhaustion)
* The schema or query that it will affect
* DDL that can be applied to resolve the recommendation

<Note>
  Schema recommendations that depend on your database traffic run **once per day**. Recommendations that depend only on
  database schema are run whenever the the schema of your default branch is modified. Schema recommendations are
  generated only for the database's default branch.
</Note>

<img src="https://mintcdn.com/planetscale-cad1a68a/V2EuARQmozG4IzT3/docs/postgres/monitoring/recommendation.png?fit=max&auto=format&n=V2EuARQmozG4IzT3&q=85&s=52742138ffa8d0f6d7e516199be972c4" alt="Example of a recommendation to reduce bloat" data-og-width="1865" width="1865" data-og-height="1282" height="1282" data-path="docs/postgres/monitoring/recommendation.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/V2EuARQmozG4IzT3/docs/postgres/monitoring/recommendation.png?w=280&fit=max&auto=format&n=V2EuARQmozG4IzT3&q=85&s=31d490797a42dc2ab655557c796cc9c0 280w, https://mintcdn.com/planetscale-cad1a68a/V2EuARQmozG4IzT3/docs/postgres/monitoring/recommendation.png?w=560&fit=max&auto=format&n=V2EuARQmozG4IzT3&q=85&s=dcdff4adf0221380e2539bc8a3fb49e3 560w, https://mintcdn.com/planetscale-cad1a68a/V2EuARQmozG4IzT3/docs/postgres/monitoring/recommendation.png?w=840&fit=max&auto=format&n=V2EuARQmozG4IzT3&q=85&s=be78875ec124e38e8b446b45e3cd4a2d 840w, https://mintcdn.com/planetscale-cad1a68a/V2EuARQmozG4IzT3/docs/postgres/monitoring/recommendation.png?w=1100&fit=max&auto=format&n=V2EuARQmozG4IzT3&q=85&s=59c75fc69fcfaf87cb41f4e859cdc4bc 1100w, https://mintcdn.com/planetscale-cad1a68a/V2EuARQmozG4IzT3/docs/postgres/monitoring/recommendation.png?w=1650&fit=max&auto=format&n=V2EuARQmozG4IzT3&q=85&s=a89636cd21f84d7951e6894733317868 1650w, https://mintcdn.com/planetscale-cad1a68a/V2EuARQmozG4IzT3/docs/postgres/monitoring/recommendation.png?w=2500&fit=max&auto=format&n=V2EuARQmozG4IzT3&q=85&s=65dfe7404442cb54508e181a243eb9c2 2500w" />

### Applying a recommendation

Schema recommendations include a DDL statement that can be applied directly to your database, or integrated into your application or ORM framework. We recommend carefully evaluating the performance impacts or downtime assoicated with running the DDL before applying it to a production environment.

### Closing a recommendation

Recommendations are automatically closed when:

* The changes have been deployed to the default branch
* Other schema changes to the default branch make the recommendation unnecessary

Once a recommendation is closed, PlanetScale will never re-suggest it.

## Supported schema recommendations

The following are the currently supported schema recommendations:

* [Adding indexes for inefficient queries](#adding-indexes-for-inefficient-queries)
* [Removing redundant indexes](#removing-redundant-indexes)
* [Preventing primary key ID exhaustion](#preventing-primary-key-id-exhaustion)
* [Dropping unused tables](#dropping-unused-tables)
* [Dropping unused indexes](#dropping-unused-indexes)
* [Rebuilding bloated tables](#rebuilding-bloated-tables)
* [Rebuilding bloated indexes](#rebuilding-bloated-indexes)

The impact of schema recommendations can vary by recommendation. In the following sections, we will inform you of each recommendation’s potential impacts and explain the recommendation further.

<Note>
  Schema recommendations may not be in line with your desired outcomes. PlanetScale shall not be held liable for any
  actions you take based on these recommendations.
</Note>

### Adding indexes for inefficient queries

Indexes are crucial for relational database performance. With no indexes or suboptimal indexes, PostgreSQL may have to read a large number of rows to satisfy queries that only match a few records. This results in slow queries and poor database performance. PlanetScale Insights automatically scans your database and suggests indexes tailored to your schema and workload.

#### How PlanetScale recommends adding indexes

To find indexes that will improve database performance, Insights scans query telemetry data every 24 hours to find query patterns that use significant resources. Normalized versions of these queries, along with relevant schema information, are provided as input to an LLM prompted to return index suggestions. After validating the returned suggestions for syntactic correctness, we evaluate the estimated performance of each query pattern with and without the suggested index. If any suggested index results in substantial query performance improvement, a new index recommendation is created. New index recommendations show the DDL required to create the index, a list of the queries that will benefit from the index, and an estimate of the performance improvement of each query pattern after creating the index.

#### Caveats

* Indexes can dramatically improve read query performance, but they also increase write costs, memory usage, and the size of tables on disk. Evaluating these additional costs against your application’s improvement in read performance is always a good idea.
* Indexing is a complicated topic and depends on many factors, such as the distribution of values in your database and the particular queries your database receives. If you decide to deploy a suggested index to production, it is a good idea to use Insights to verify that your index has the desired effect on relevant queries.
* Query performance with a suggested index is estimated with the [HypoPG extension](https://github.com/HypoPG/hypopg). This extension is installed by default on all PlanetScale Postgres databases. Removing this extension will disable new index recommendations.
* Index suggestions are generated using AI tools. [Learn more about how PlanetScale uses AI here](https://planetscale.com/docs/how-we-use-ai).
* New index suggestions, and other LLM-based features, can be disabled in the organization settings page.

### Removing redundant indexes

While indexes can drastically improve query performance, having unnecessary indexes slows down writes and consumes additional storage and memory.

#### How PlanetScale recommends removing indexes

Insights scans your schema every time it is changed to find redundant indexes. We suggest removing two types of indexes:

* Exact duplicate indexes - an index that has the same columns in the same order
* Left prefix duplicate indexes - an index that has the same columns in the same order as the prefix of another index

There are differences between the two, so note your exact recommendation and the following caveats.

#### Caveats

Removing redundant indexes is more nuanced than adding an index.

* Exact duplicate indexes are *always* safe to remove.
* Left prefix duplicate indexes are *almost always* safe to remove, but in some cases can lead to a performance regression. Usually, the larger index can be used instead of the left prefix duplicate indexes.

### Preventing primary key ID exhaustion

As new rows are inserted, it’s possible for sequence-driven primary keys to exceed the maximum allowable value for the underlying column type. When the column reaches the maximum value, subsequent inserts into the table will fail, which can cause a severe outage to your application.

#### How PlanetScale detects primary key ID exhaustion

Insights scans all of the sequences in your database, finds the owning columns and checks the most recent value daily to identify where you might be approaching primary key ID exhaustion. If Insights detects that one of the columns is above 60% of the maximum allowable type, it will recommend changing the underlying column to a larger type.

Additionally, Insights scans queries to parse joins and correlated subqueries to find foreign keys and suggests increasing the column size for those columns.

#### Changing primary key types

To make space for additional table growth, the primary key column and any foreign key columns that reference the primary key need to be updated to a larger integer type: `BIGINT`. The most straightforward approach is to issue an `ALTER TABLE ... ALTER TYPE ...` command, and this is the DDL that is shown in the schema recommendation. However, there is a *significant* downside to this approach: the table will be completely locked while the entire table is rewritten which will result in downtime if your application interacts with this table. `ALTER TABLE ... ALTER TYPE ...` is not an online operation and rewriting a large table can take many hours.

<Note>Do not run `ALTER TABLE ... ALTER TYPE ...` if the table is actively used by your application.</Note>

Assuming table unavailability is not an option for your application, the best approach is usually to create a new column and migrate to it in a controlled manner. The process will depend on the constraints of your application and your tolerance for downtime, and needs to be carefully planned and executed. For this reason we suggest always creating primary keys with the `BIGINT` data type unless you are certain that the number of rows in the table will be small.

#### Caveats

* Running `ALTER TABLE ... ALTER TYPE ...` will lead to significant downtime for large tables.
* Changing data types without downtime is complicated and will need to be carefully planned and tested.

### Dropping unused tables

Dropping unused tables can help clean up data that is no longer needed and reduce storage. If the table is large, it can also decrease backup and restore time.

If you are unsure if a table should be retained but decide to drop the table, make sure to [create a manual backup](/docs/postgres/backups#manual-backups) of your database before you deploy the change.

If you determine that the table should be retained, close the recommendation, preventing the suggestion from being remade.

#### How PlanetScale recommends dropping unused tables

Insights scans your query performance data daily to identify if any tables are more than four weeks old and haven’t been queried in the last four weeks.

#### Caveats

* Only you can know if the table’s data is no longer needed. Ensure that the table is never used (even infrequently) and does not contain important data before removing it.
* Once a drop unused table recommendation is opened, it will remain open even if it is subsequently queried. Check your Insights data to verify that the table is still unused before permanently dropping it.

### Dropping unused indexes

Dropping unused indexes can help reduce the cost of inserts and updates, and save memory and storage.

#### How PlanetScale recommends dropping unused indexes

Insights scans your query performance data daily to identify if any indexes are more than four weeks old and haven’t been used by any queries in the last four weeks.

#### Caveats

* Dropping an index, even if hasn't been recently used, can affect the performance of future queries. Ensure that your application doesn't make any queries that depend on the index to run efficiently.
* Once a drop unused index recommendation is opened, it will remain open even if the index is subsequently used. Check your Insights data to verify that the index is still unused before permanently dropping it. Insights queries can be filtered by index usage by specifying `index:index_name` in the Insights search box.

### Rebuilding bloated tables

Table bloat is excess physical storage space created by PostgreSQL’s MVCC system when rows are updated or deleted. Excessive table bloat wastes disk space, negatively impacts query performance and slows down backup and DDL operations. Some amount of table bloat is normal, but high or ever-increasing bloat is a situation that needs to be remediated.

Table bloat can occur when frequent `update`s or `delete`s lead to fragmentation in the physical storage layer over time. When this occurs, the table needs to be rebuilt to optimize the physical layout of tuples on disk. To rebuild a bloated table with minimal disruption to your application, we recommend using the `pg_squeeze` extension, which can be enabled on your database's Clusters page. Once that is enabled a table can be registered for regular processing or a one-time cleanup can be performed. The schema recommendation shows the command necessary to perform a one-time cleanup. See the [pg\_squeeze documentation](https://github.com/cybertec-postgresql/pg_squeeze/) for further information.

#### How PlanetScale detects table bloat

Insights scans your database once a day to estimate table bloat based on system tables. Table bloat recommendations are triggered when the estimated percent of physical space lost due to bloat is over 25% and 100MB for a given table.

#### Caveats

* Tables can become bloated for reasons other than physical storage fragmentation, such as very long running transactions that prevent dead tuple reclamation, or infrequent vacuuming possibly caused by inadequate auto vacuum settings. If either of these are the case, you will need to address those underlying causes to resolve table bloat.
* Once a table bloat recommendation is created, it will remain open even if bloat drops below the triggering thresholds.
* Once a table bloat recommendation is closed, it will never be opened for that table again.
* Running `pg_squeeze` will consume database resources, so be sure that your database has sufficient capacity before rebuilding tables.

### Rebuilding bloated indexes

Like table bloat, index bloat is excess physical storage space created by PostgreSQL’s MVCC system when rows are updated or deleted. Excessive index bloat wastes disk space and negatively impacts query performance. Some amount of index bloat is normal, but high or ever-increasing bloat is a situation that needs to be remediated.

Index bloat can occur when frequent `update`s or `delete`s lead to fragmentation in the physical storage layer over time. When this occurs, the index needs to be rebuilt to optimize the physical layout of tuples on disk. The index bloat recommendation shows the requisite `REINDEX INDEX CONCURRENTLY ...` command to rebuild the index and minimize accumulated bloat.

#### How PlanetScale detects index bloat

Insights scans your database once a day to estimate index bloat based on system tables. Index bloat recommendations are triggered when the estimated percent of physical space lost due to bloat is over 30% and 100MB for a given index.

#### Caveats

* Indexes can become bloated for reasons other than physical storage fragmentation, such as very long running transactions that prevent dead tuple reclamation, or infrequent vacuuming possibly caused by inadequate auto vacuum settings. If either of these are the case, you will need to address those underlying causes to resolve index bloat.
* Once an index bloat recommendation is created, it will remain open even if bloat drops below the triggering thresholds.
* Once an index bloat recommendation is closed, it will never be opened for that index again.
* Running `REINDEX INDEX CONCURRENTLY ...` will consume database resources, so be sure that your database has sufficient capacity before rebuilding indexes.


# PlanetScale Postgres operations philosophy
Source: https://planetscale.com/docs/postgres/operations-philosophy

PlanetScale has a high standard for uptime and availability of Postgres databases.

The foundation of achieving this goal in the cloud is our operator: the software that manages new database creation, version upgrades, resizes, failovers, and other operations in an inherently failure-prone environment.

This document presents our architecture and philosophy on operating PlanetScale Postgres databases.
All customers should read it, as it also covers what can be expected during failovers, querying replicas vs the primary, and the tradeoffs of using direct vs pooled connections.

## Database cluster architecture

The cloud is an inherently failure-prone environment.
Servers can experience degraded performance or become unavailable at any time and for unknown reasons.
Because of this, we must treat all cloud server nodes as ephemeral.

We can embrace failure and build database clusters that are resilient in the cloud environment by understanding our constraints and leveraging the cloud's elasticity within known and practical limits.

Every production database cluster in PlanetScale has a primary and two replicas.
This allows us to embrace server failures through failovers.
Some consider failovers as a problem, but we see that as a fundamental building block for operating reliable databases.
Tolerating the brief disruptions produced by failovers has huge upside: PlanetScale can rapidly deliver changes which continuously increase the quality, reliability, and performance of customer databases.

<img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/primary-replicas.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=88ba5e541613740f1be44bb3477517bc" alt="Primary and Replicas" data-og-width="1864" width="1864" data-og-height="1162" height="1162" data-path="docs/postgres/primary-replicas.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/primary-replicas.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=ba4dfb2200cbd2ce199d4c9a42b01469 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/primary-replicas.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=876a930801319614daca988b4241c692 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/primary-replicas.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=93864ee2cf924ac0ad01f7f5a8ba7e04 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/primary-replicas.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=7aaea9d479e76334dbd98db08988987c 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/primary-replicas.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=322ec551e56bc87317c1b842aeb9343c 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/primary-replicas.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=1404514c04ecb501ca34ca04d23ff156 2500w" />

This is the most proven architecture for highly-available database clusters for several reasons:

1. Replicating data to multiple servers is crucial for data durability, especially in a cloud environment.
   All data is replicated 3 ways at minimum.
2. Each instance is in a distinct availability zone.
   This means we can be resilient to zone-wide incidents.
3. Replicas can receive read queries that can tolerate replication lag.
   This allows offloading work from the primary, reserving it for writes and critical reads.
4. In the event of an incident in the primary's availability zone, we can failover to a healthy replica in a different availability zone.
   Likewise, when performing a planned upgrade on the primary instance, we first promote a healthy replica to become primary before upgrading the old primary (now demoted to a replica)
   Failovers take on the order of seconds, not minutes or hours, getting you back online quickly.

Having a three-node configuration as the starting point for all clusters allows PlanetScale to operate these databases in ways many other providers cannot.
Failovers are used to replace primaries when they go down unexpectedly.
The same, proven cluster failover techniques are used for other changes like upgrading instances, version patches, and reconfiguration.
This means customers benefit from tried-and-true database management techniques to increase availability, and minimize (though not eliminate) downtime during these operations.

## Continual improvement

Most Postgres database platforms shy away from version updates and other incremental improvements due to their disruptive nature.
Many services require minutes or even hours of downtime to handle this.

The PlanetScale philosophy is different.
To give our customers the best possible Postgres experience, we believe that regular version bumps, bugfixes, and quality-of-life improvements are necessary.
We aim to do image upgrades no more than once per week, though we may do so more frequently for urgent updates like security patches.

Because this requires node failovers, these upgrades lead to a short period of database unavailability (seconds) about once per week.

## The ideal Postgres experience

PlanetScale provides a well-rounded, out-of-the-box Postgres experience while also giving users what they need to tune it to their liking.
This shows up in several aspects of the product:

* Each database is given custom default tunings depending on CPU, RAM, and disk characteristics.
  Users can adjust and carefully monitor the effects of their own changes via the Clusters and Insights pages.
* In addition, we tune `effective_io_concurrency` based on the IOPS values specific to your database.
  For example, each Metal node type has unique and very large IOPS capacity, so we customize it on a per-instance-type basis.
  But we also let you experiment with your own tunings.
* Users have lots of freedom to customize their Postgres roles via our UI, giving them fine-grained control over permissions.
* Both direct-to-Postgres and PgBouncer connections are available by default.
  Each has tradeoffs, and we let you choose what makes the most sense for your applications.

The aim is to set you up with a strong foundation, and give users the tools they need to tune for their use-cases.

## Minimizing the impact of configuration changes

PlanetScale takes the database unavailability produced by upgrades extremely seriously.
We monitor this downtime across the fleet, and make continual efforts to reduce its impact.
We apply context-appropriate, rather than a one-size fits all, upgrade strategies to ensure that a given upgrade takes no more time than absolutely necessary.

PlanetScale allows manual configuration of a number of parameters via the [clusters](/docs/postgres/cluster-configuration) page.
Each configuration change falls into one of two categories:

* Reloadable changes: The change can be applied with no need to restart Postgres.
  When applying these, there is zero downtime.
* Restart-required: Postgres requires the server be restarted for the change to take effect.
  When applying these, you can expect a brief period of server unavailability due to the restart.

In the latter case, what we do is more sophisticated than a simple server restart.
Because we have a primary and two replicas, we typically start these configuration changes by applying them to and restarting the replicas.
Once these are ready, we do a switchover from the old primary to one of the replicas with the upgraded configuration.
The config is then applied and the final instance restarted, now demoted to a replica.

<img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/config-change-restart.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=edd337e6e4a88f52579e060b8195b1b0" alt="Config change with restart" data-og-width="3501" width="3501" data-og-height="1169" height="1169" data-path="docs/postgres/config-change-restart.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/config-change-restart.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=f68dddb35c0572fbd51b72e1ecbdf432 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/config-change-restart.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=a3ee8da7b70c1c4f7c0da325eb38ff7f 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/config-change-restart.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=1ec42d1196f152519ddc57aeb1e9ffcd 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/config-change-restart.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=09b2c760904df1ef73c587456d7ab4aa 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/config-change-restart.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=7717ba5533d121f99755464865cdc1ad 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/config-change-restart.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=7cd3410dd7ffd5c840e625fa59e30654 2500w" />

This process allows us to minimize end-user downtime.
There remains an unavoidable several-second window of unavailability.
In addition, all direct connections will be terminated, so your application should have retry logic to accommodate for this.
There are a few configuration options that are [required by Postgres](https://www.postgresql.org/docs/current/hot-standby.html#HOT-STANDBY-ADMIN) to be applied to the primary before replicas.
For these, you will notice a slightly longer unavailability period.

Normally a primary shutdown proceeds in a fast and orderly manner within a second or two, allowing the operator to promote a replica to primary.
In cases where the operator is not able to quickly and cleanly shutdown the primary due to unresponsive user queries or transactions, the operator will failover to a replica after a timeout of 30 seconds.

PgBouncer connections will persist through all config changes.

## Database cluster sizing

PlanetScale does not support autoscaling of CPU and RAM.
Instead, users are put in the driver's seat to choose the hardware the database runs on, and are given tools to closely monitor performance and adjust resources when needed.
There are two main ways to increase the compute capacity of a cluster: Adding replicas and resizing all nodes.

### Adding Replicas

Each production PlanetScale database (excluding single node) has 1 primary and 2 [replicas](/docs/postgres/scaling/replicas).
In this standard configuration, read traffic can be [routed to a replica](/docs/postgres/scaling/replicas#how-to-query-postgres-replicas) rather than relying on the primary for everything.
It's important not to over-stress your replicas, however.
If all three nodes in the cluster experience too much strain, failovers will be more disruptive, since the cluster will temporarily go from 3 nodes down to 2.
Reads from replicas must also be able to tolerate replication lag.

If you only need to increase the read capacity of the cluster, a good option can be to add 1 or several additional replicas.
This is done through the [Clusters](/docs/postgres/cluster-configuration) menu, and has no negative impact on the other nodes when being added.

### Resizing a cluster

Resizing a cluster leads to database connections being terminated.
Therefore, it's important for your application to have connection retry logic.

Consider the case where you need to upgrade from an `M-160` database cluster to an `M-320`, doubling the compute resources of each node.
After applying the upgrade, three new `M-320` nodes are created.
These are caught up with the primary through a combination of a backup restore and data replication.

<img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-resize-1.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=ced1ff95e9ce762110b0f652d8886005" alt="Cluster Resize 1" data-og-width="2162" width="2162" data-og-height="1187" height="1187" data-path="docs/postgres/cluster-resize-1.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-resize-1.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=81a7bb92955f32a0264c85d2dd743a38 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-resize-1.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=7f0484d209c5923bac082e55cc59b088 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-resize-1.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=3183eac038676fae4d991b7b9aa64b39 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-resize-1.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=8d538f93b6d47b1db0511809836e4f70 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-resize-1.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=1e69ccd44aefa11bc057d2a5734f0da5 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-resize-1.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=bbba3fc94e8cf31e0eb9357734636e78 2500w" />

Once these new `M-320` replicas are sufficiently caught up, the operator transitions primaryship to one of the new `M-320` nodes.
After this, the old `M-160` replicas are decommissioned, using the new ones for all replica traffic.
During each node replacement, connections to the decommissioned node will be destroyed.
New connections will need to be made to re-establish with one of the new nodes.

<img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-resize-2.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=71a04aee7f55f0d5ff393df7cdd2e598" alt="Cluster Resize 2" data-og-width="2181" width="2181" data-og-height="932" height="932" data-path="docs/postgres/cluster-resize-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-resize-2.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=a43236270b1ded7a957c5e762b582496 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-resize-2.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=67306337aa11cea1b461d92d972c84b7 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-resize-2.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=5a9b2d4fa925f42cad377f95edeea916 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-resize-2.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=3ebbb5dca2cfe6d41da78715f624b313 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-resize-2.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=063c25fb9baf5210b9a1796957eed852 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/cluster-resize-2.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=b2c73a85655df524bbcf62a131334032 2500w" />

During the upgrade process, all database connections will be terminated.
Normally a primary promotion proceeds in a fast and orderly manner in less than 5 seconds.
In cases where the operator is not able to quickly and cleanly shutdown the primary due to unresponsive user queries or transactions, the operator will failover to a replica after a timeout of 30 seconds.

For all the steps leading up to the node replacement, we keep your existing `M-160` database cluster fully functional.
Only brief periods of unavailability are required to ensure we can smoothly replace each node.

Many ORMs have built-in connection retry logic to help with these scenarios.
It's likely your application already has such capabilities if you are using libraries like Rails ActiveRecord, Laravel Eloquent, and Drizzle.

## Unplanned failures

In a cloud-native environment, server failure is to be expected.
This is unavoidable, so we embrace the behavior and have proven failover systems in place to address issues as quickly as possible.
PlanetScale has executed millions of failovers across our Postgres and Vitess database clusters.
They are a well-exercised path and are proven to be an invaluable mechanism for database operations and handling node failures.

### Replicas

Replica health is important to keep your PlanetScale database cluster operating smoothly.
Replica failures are automatically detected and automatically replaced.
There will be a period of time where your cluster has only the primary and a single replica while the replacement replica is created and data restored to it.
This time varies from minutes to hours depending on database size and your instance type.

<img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/failed-replica.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=7b4f0a09b7f9c3551b90d7ec86c252de" alt="Failed replica" data-og-width="1784" width="1784" data-og-height="1008" height="1008" data-path="docs/postgres/failed-replica.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/failed-replica.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=acd60adbeddba6777c2b25d8105d58aa 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/failed-replica.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=794ec9f4310d6c82e77981afa34c0817 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/failed-replica.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=8c4183b129e2884ef0a19444a4d8392b 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/failed-replica.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=5502c33bddec53ab40abd116fcb8e3a5 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/failed-replica.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=42d47c863e1f7e86912ba1e29003c205 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/failed-replica.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=aae598f33d24b7d4313e2618319a6811 2500w" />

Any connections routed to the failed replica will be terminated and need to be re-established.
When re-established, we will route the connections to the other available replica.

### Primaries

The PlanetScale operator will detect a primary failure within seconds and immediately begin the process of replacing the node.
The new primary is chosen based on which one is the most caught-up from the state of the primary.
All Postgres databases are run with semi-sync replication, meaning that at least one replica should have received all the writes that the primary had seen.
Once chosen, this replica is promoted to primary and a new node is brought up to replace it.

<img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/failed-primary.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=ec708d261905e8739112cfb6697a7bb1" alt="Failed primary" data-og-width="2717" width="2717" data-og-height="1047" height="1047" data-path="docs/postgres/failed-primary.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/failed-primary.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=4c0087c4a95ac714ca3bd719adbd6362 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/failed-primary.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=3657137816e7fac527726d9a37af3f9e 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/failed-primary.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=0371c466074bed3edc67f144d9bb060e 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/failed-primary.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=407bd93ca1a913ba50e9128f684c6ea1 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/failed-primary.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=58de5f612a49577735a4bb2b49c05476 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/failed-primary.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=24e70614efa25aba62a9762edf78d59d 2500w" />

The primary failure, detection, and election process typically takes 10 seconds or less when all other components of the cluster are healthy.
During this time your primary database will be inaccessible and no connections can be made.
The time it takes for the new replica to catch-up to the primary varies from minutes to hours depending on database size and your instance type.

## Image upgrades

We have fast iteration cycles at PlanetScale, always aiming to improve the features and capabilities of our customer's Postgres databases.
Postgres minor version updates, PgBouncer updates, and adding support for new extensions all require us to update the container images for your database.
When this happens, the Kubernetes pods that power your primary and replicas are rolled.

<img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/image-upgrade.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=53dbb79032d5891d28a732b17416dfdf" alt="Image upgrade" data-og-width="3153" width="3153" data-og-height="1113" height="1113" data-path="docs/postgres/image-upgrade.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/image-upgrade.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=b3f69621d9cc47bce50688c14347c217 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/image-upgrade.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=20088e65fb77a67112b64690056c39ed 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/image-upgrade.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=77453aa38637e36ef458947dd2f94cd8 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/image-upgrade.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=a5b64133fff86540ab80689e6ca60ae2 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/image-upgrade.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=bf8de5daf72589f48a92ce3558b9e302 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/image-upgrade.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=4e938ed3f4c2ee10aa2943f1919bbb19 2500w" />

As with other operations described earlier, this leads to terminated connections and a brief period of database inaccessibility.
We aim to do image upgrades no more than once per week, though we may do so more frequently for urgent updates like security patches.

## Disk availability, scaling, and cost

For databases using network-attached storage, autoscaling is enabled by default.
This is to protect availability of your database.
Reaching 100% full on a storage volume leads to database downtime.

<img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/disk-autoscaling.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=b0fbcce0ee3969841689fc92a7f0b401" alt="Disk autoscaling" data-og-width="2000" width="2000" data-og-height="1378" height="1378" data-path="docs/postgres/disk-autoscaling.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/disk-autoscaling.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=c92afb8a1438967ec7cdb9d260d63d68 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/disk-autoscaling.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=abe110ab6754b20494f488ff3bbc644d 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/disk-autoscaling.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=727bc70510773c6de8537f23258fb8f7 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/disk-autoscaling.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=dab4a8cf29f49d912431921cb2c6d93f 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/disk-autoscaling.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=48ecff856ede40a3b20eaab22784f883 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/disk-autoscaling.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=889374187d7577a38ee755b34f946d29 2500w" />

Cloud providers like AWS and GCP limit how frequently network-attached disks can be resized.
In both cases, there is a multi-hour cooldown period between resizing operations.
Also, these volumes typically do not support shrinking.

When our disk auto-scaler is able to spread out disk scale-up sufficiently, no downtime is needed to scale the disks.
This is true in the vast majority of cases.

When data growth is rapid, the auto-scaler may need to complete a **surge resize** to support the writes.
In this case, PlanetScale creates brand new database nodes with larger network-attached storage volumes.
Once ready, the nodes in your cluster get replaced with these new ones to increase capacity.
If the surge autoscaler is able to complete the resize before your disk fills, downtime will be minimal for growing the disk (seconds).
If your disk fills before the new disks are ready, you will experience a longer period of downtime.

We make every effort to keep your network-attached storage disk from filling, but it's important for the database administrators to pay close attention to storage and take manual intervention when necessary.


# PlanetScale Postgres architecture
Source: https://planetscale.com/docs/postgres/postgres-architecture

PlanetScale Postgres is a managed PostgreSQL service built on modern cloud infrastructure.

## Overview

Our architecture provides high availability through availability zone distribution, automated failover, and redundancy across multiple components. PlanetScale Postgres is built around a `shared nothing` architecture in alignment with PlanetScale's [principles of extreme fault tolerance](https://planetscale.com/blog/the-principles-of-extreme-fault-tolerance).

This document explains the core components of our PostgreSQL infrastructure, from regional deployment patterns to cluster configuration options and operational capabilities.

## Regional and availability zone architecture

### Geographic distribution

PlanetScale Postgres deploys database clusters across multiple availability zones within a single region. This design provides:

* **Zone-level fault tolerance**: Database instances are automatically distributed across separate availability zones
* **Network isolation**: Each availability zone operates independently with its own network infrastructure

### Cluster design

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-arch-diagram.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=7b6b4c86cfe2b872a394857c2c993ca9" alt="PlanetScale Postgres cluster topology" data-og-width="2612" width="2612" data-og-height="1654" height="1654" data-path="docs/postgres/postgres-arch-diagram.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-arch-diagram.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=cdc9609e609cf49ace22f2b4b846b009 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-arch-diagram.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=7cf762ad536dddec55ac042a59983c5a 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-arch-diagram.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=0d9c08d45218a9d93cb1689146dcc28e 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-arch-diagram.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=280b440805dde747c16a85dab9b1e629 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-arch-diagram.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=f97af7f19f67144887e6dbd47e999bdd 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-arch-diagram.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=6a59558fafc819270def2c432b10f203 2500w" />
</Frame>

PlanetScale Postgres uses a primary-replica architecture distributed across availability zones to provide high availability without compromising performance. This design ensures that your database can survive infrastructure failures while maintaining fast read and write operations.

### Primary-replica architecture

Each production PostgreSQL cluster consists of:

**Primary instance (1)**:

* Handles all write operations
* Located in one availability zone
* Serves read operations when replicas are unavailable
* Source of truth for data replication

**Replica instances (2)**:

* Handle read-only operations
* Located in separate availability zones from primary
* Continuously synchronized with primary through streaming replication
* Available for promotion to primary during failover events

## Orchestration layer

The orchestration layer manages the lifecycle, health, and operations of PostgreSQL clusters across multiple availability zones. This infrastructure layer handles everything from initial deployment to ongoing maintenance and failover operations.

### Kubernetes-based management

Our PostgreSQL clusters run on Kubernetes infrastructure, providing:

* **Automated deployment**: Consistent cluster provisioning across availability zones
* **Health monitoring**: Continuous monitoring of database instance health
* **Resource management**: Dynamic allocation of compute and storage resources
* **Configuration management**: Centralized management of PostgreSQL parameters and settings

### Custom operator

Our custom Kubernetes operator manages all cluster nodes and handles PostgreSQL operations that would otherwise require deep database knowledge. Based on our extensive experience running databases at massive scale, our operator was built for rock-solid handling of PostgreSQL replication, backup requirements, and the specific steps needed for safe failover operations:

* **Failover coordination**: Automated promotion of replica instances when primary fails
* **Backup scheduling**: Coordinated backup operations across all instances
* **Configuration synchronization**: Ensures consistent settings across primary and replicas
* **Scaling operations**: Manages instance resizing and replica addition/removal

## Data replication

**Streaming replication**:
PostgreSQL's built-in streaming replication keeps replicas synchronized with the primary in near real-time. Changes are continuously streamed to replicas, ensuring data consistency across all instances:

* Continuous data streaming from primary to replicas
* Changes confirmed by at least one replica
* Automatic lag monitoring and alerting

**Connection routing**:

* Port 5432: Direct PostgreSQL connections
* Port 6432: [PgBouncer](/docs/postgres/connecting/pgbouncer) connection pooling for optimized connection management
* SSL/TLS encryption required for all connections

## Instance configurability

PlanetScale Postgres offers flexible instance configuration to match your workload requirements and cost constraints. You can choose different CPU architectures, storage types, and performance characteristics to optimize for your specific use case.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-create-config.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=b5961c4d1abf29e63f3c902a8e9c47e0" alt="Creating a database" data-og-width="3240" width="3240" data-og-height="1914" height="1914" data-path="docs/postgres/postgres-create-config.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-create-config.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=ef9b11bb9e53f3ac5c02b6d1884a8bc2 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-create-config.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=1422a26ed1ea5d30f8beb1934cad2ed6 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-create-config.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=27fedf06bc5dfa9f4ed92c119eb932e0 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-create-config.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=7f60c011f695cfebbc3afac3fc555582 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-create-config.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=cca597a2ca45ec3cc1c358bc8567e280 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-create-config.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=22dc4f7f5e2ea9fb7b86ccb6ecd0f1d7 2500w" />
</Frame>

### CPU architecture options

**ARM64 (AWS Graviton)**:

* Cost-optimized instances with good performance characteristics
* Compatible with all PostgreSQL features and extensions
* Lower power consumption and cost per compute unit

**x86-64 (Intel/AMD)**:

* Traditional architecture with broad compatibility
* Optimized for single-threaded performance requirements
* Full compatibility with existing tooling and applications

Learn more about [CPU architecture selection](/docs/postgres/cluster-configuration/cpu-architectures).

### Storage types

Storage choice significantly impacts database performance and cost. PlanetScale offers two distinct storage options optimized for different workload patterns:

**PlanetScale Metal**:

* Direct-attached NVMe storage for maximum performance
* Lower I/O latency compared to network-attached storage
* Fixed storage capacity based on instance size
* Optimal for I/O-intensive workloads

**Network-attached storage (EBS)**:

* Flexible storage scaling up to 16 TiB
* Configurable IOPS and throughput settings
* Automatic storage scaling based on usage patterns
* Cost-effective for variable storage requirements

Learn more about [storage configuration](/docs/postgres/cluster-configuration/cluster-storage).

### Performance and scaling relationships

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-dashboard-summary-metal.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=345b1f6de02345683e2930b488366963" alt="Database dashboard summary" data-og-width="1478" width="1478" data-og-height="1356" height="1356" data-path="docs/postgres/postgres-dashboard-summary-metal.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-dashboard-summary-metal.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=113ca5c69f03eda03e1b5c5b67d67c0c 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-dashboard-summary-metal.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=c15cd5ef2833c586046f95d6bb5f421e 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-dashboard-summary-metal.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=411e3a832ca7c696b197a652e9966da3 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-dashboard-summary-metal.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=1bdb3a58becea1d98b5a4836c85c18db 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-dashboard-summary-metal.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=b5931e8fcc8777d8d9f13c10d5ab78b7 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-dashboard-summary-metal.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=082752ba89e2c1d112cc4806e2c2ea3c 2500w" />
</Frame>

Understanding how different configuration choices affect performance helps you optimize your database for both cost and performance. These relationships guide capacity planning and scaling decisions:

**Compute scaling**:

* Instance size directly affects CPU, memory, and network capacity
* Larger instances support more concurrent connections
* Memory allocation affects query performance and caching efficiency

**Storage performance**:

* Network-attached storage can autoscale to meet storage needs
* Network-attached storage allows for configurable IOPS and disk bandwidth
* PlanetScale Metal provides increased IOPS and ultra-low latency storage
* Throughput limits vary by instance size and storage configuration

## Operational capabilities

PlanetScale Postgres provides comprehensive operational tools that give you visibility into database performance, health, and behavior. These capabilities help you monitor, troubleshoot, and optimize your database without requiring additional setup or external tools.

### Insights and query analysis

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-insights.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=bf30109675fb87450bcd6ed21c393d53" alt="PlanetScale Insights" data-og-width="2886" width="2886" data-og-height="1818" height="1818" data-path="docs/postgres/postgres-insights.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-insights.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=a2edb1ed14543563a939f9c39bec3dfa 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-insights.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=70eac6f83cf01c5a848423179f627aa3 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-insights.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=78832331964bf584a87fcf1911cae4f0 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-insights.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=16f258115045ade9afdfbf5a49bc58e2 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-insights.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=7c76f76b2e962be0819b3942592e005c 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-insights.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=c5955ab193acd7ce486483649c19fbff 2500w" />
</Frame>

**Query performance insights**:

* Automatic detection of slow-running queries and anomalies
* Query execution plan analysis and recommendations
* Historical query performance tracking
* Identification of resource-intensive operations

### Metrics and monitoring

Comprehensive metrics collection provides real-time visibility into database health and performance trends. All metrics are collected automatically and made available through both the dashboard and programmatic interfaces:

**Built-in metrics collection**:

* CPU, memory, and storage utilization across all instances
* Connection count and pooling efficiency
* Replication lag and synchronization status

Learn more about [monitoring and metrics](/docs/postgres/monitoring/metrics).

### Logs and diagnostics

Centralized logging aggregates all database-related logs in a searchable format with 7-day retention. This unified logging system helps with troubleshooting, security monitoring, and performance analysis:

**Centralized logging**:

* PostgreSQL server logs from all instances
* Query execution logs with configurable detail levels
* Error logs with automatic categorization and alerting
* Connection and authentication logs for security monitoring

**Log analysis tools**:

* Search and filtering capabilities across all log sources
* Export capabilities for external analysis tools

### Cluster configuration options

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-config-changes.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=68b5e2bb085005c963fcee511e91c2ec" alt="Tracking changes" data-og-width="2918" width="2918" data-og-height="2174" height="2174" data-path="docs/postgres/postgres-config-changes.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-config-changes.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=c60901964cc0483780030f0a103e5096 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-config-changes.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=a27d443002284bb2f944fe276e9f6142 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-config-changes.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=bc8c4b60c795e7d96699b3039a63acfa 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-config-changes.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=f30ab7a68574ecd9eea4a22b1043b558 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-config-changes.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=f3980cd9e96a56718c3bcb4fae8ec040 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-config-changes.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=16764f6b7ac0cb4094b4453740e62cd5 2500w" />
</Frame>

Flexible configuration options allow you to customize PostgreSQL behavior, enable additional functionality, and optimize performance for your specific workload requirements:

**Extension management**:

* Curated set of PostgreSQL extensions tested for compatibility
* Both Native and Community extensions available
* Managed through dashboard or CLI

Learn more about [extension configuration](/docs/postgres/extensions).

**Parameter tuning**:

* Pre-configured parameter sets optimized for different workload types
* Customizable PostgreSQL configuration parameters
* Automatic parameter adjustment based on instance size changes
* Configuration change tracking

Learn more about [parameter configuration](/docs/postgres/cluster-configuration/parameters).

**Custom Backups and Point-in-time recovery**:

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-pitr.png?fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=5f20ac9f7c14b2f9d118fa389c18798e" alt="Postgres PITR" data-og-width="1766" width="1766" data-og-height="1202" height="1202" data-path="docs/postgres/postgres-pitr.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-pitr.png?w=280&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=21542d969f50893d2cd6aa269ad740b0 280w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-pitr.png?w=560&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=afee9060f9579020f416edbaa323df3e 560w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-pitr.png?w=840&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=e04dc1293ae594ecc2a9b9216e25bc3a 840w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-pitr.png?w=1100&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=e3cf7ed4df22fd48516cb8c211997b74 1100w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-pitr.png?w=1650&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=da264a289609af221c182500e8324aaf 1650w, https://mintcdn.com/planetscale-cad1a68a/wcvpdoJo_ezgDX4B/docs/postgres/postgres-pitr.png?w=2500&fit=max&auto=format&n=wcvpdoJo_ezgDX4B&q=85&s=5e6c752164cb954efa845c8916343f1f 2500w" />
</Frame>

* Automated backup scheduling with configurable retention periods
* Custom backup timing to minimize impact on production workloads
* Point-in-time recovery capabilities within retention windows

Learn more about [backup and restore operations](/docs/postgres/backups).

## Development workflow integration

PlanetScale Postgres integrates with development workflows through database branching and comprehensive migration support. These features enable safe schema changes and smooth transitions from other PostgreSQL platforms.

### Database branching

**Environment isolation**:

* Create isolated database environments from production backups
* Independent schema and data changes without affecting production
* Cost-optimized single-instance architecture for development branches
* Automatic promotion to full high-availability architecture when needed

### Third-party integrations

PlanetScale Postgres integrates with popular monitoring and development tools through standard protocols and APIs. These integrations allow you to incorporate database metrics into existing workflows and toolchains:

**Monitoring system integrations**:

* [Prometheus endpoints](/docs/postgres/monitoring/prometheus-postgres) for custom metrics collection
* [Datadog integration](/docs/postgres/monitoring/prometheus-metrics-datadog-postgres) for unified monitoring dashboards

**Development tool integrations**:

* Standard PostgreSQL connection protocols for universal tool compatibility
* Support for popular database administration tools
* API access for programmatic management and monitoring

## Related documentation

<CardGroup>
  <Card href="/docs/postgres/cluster-configuration" title="Cluster Configuration" icon="angles-right" horizontal />

  <Card href="/docs/postgres/scaling/replicas" title="Scaling with Replicas" icon="angles-right" horizontal />

  <Card href="/docs/postgres/backups" title="Backup and Recovery" icon="angles-right" horizontal />

  <Card href="/docs/postgres/monitoring/metrics" title="Monitoring and Metrics" icon="angles-right" horizontal />

  <Card href="/docs/postgres/branching" title="Database Branching" icon="angles-right" horizontal />
</CardGroup>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale Postgres pricing
Source: https://planetscale.com/docs/postgres/pricing

PlanetScale Postgres databases are billed based on their configuration and usage.

## Overview

Understanding the relationship between **databases**, **branches**, and **clusters** is key to understanding your billing:

* **Database**: Your overall project (e.g., "my-ecommerce-app")
* **Branch**: Isolated database deployments that provide you with separate environments for development and testing, as well as restoring from backups. [Learn more about branching](/docs/postgres/branching)
* **Cluster**: The underlying compute and storage infrastructure that powers each branch

**Each branch runs on its own dedicated cluster** and is billed separately based on its configuration and usage.

* You are billed for the compute and storage resources (cluster) that power each branch (prorated to the millisecond each month)
* Every database branch you launch has default included amounts of:
  * [Cluster disk storage](/docs/postgres/#cluster-disk-storage)
  * [Backup storage](/docs/postgres/#backups)
  * [Network egress data transfer](/docs/postgres/#network-data-transfer)
  * [Replicas](/docs/postgres/#additional-replicas)
* Usage beyond the included defaults is charged based on the costs for that resource, this includes:
  * Cluster disk storage used beyond the included amount (for network-attached storage)
  * Backup storage used beyond the included amount
  * Network egress beyond the included amount
  * Additional replicas

There are no upfront commitments, and as you adjust your configuration or usage changes, pricing will automatically adjust from that point.

<Note>
  For information on Vitess cluster pricing on the Scaler Pro plan, see [Scaler Pro cluster pricing](/docs/plans/cluster-sizing).
</Note>

## Cluster instance size

Each database branch runs on its own dedicated cluster infrastructure. Clusters are billed based on the selected instance size and prorated to the millisecond. Each instance size includes defined amounts of vCPU cores and memory, as well as storage based on the storage type selected ([network-attached storage](/docs/postgres/cluster-configuration/cluster-storage) or [PlanetScale Metal](/docs/metal)). See [cluster pricing](/docs/postgres/#cluster-pricing) for more information.

You can [increase or decrease your cluster size](/docs/postgres/cluster-configuration) at any time. Pricing is prorated to the millisecond, so if you temporarily increase, you will only be charged for the larger cluster size for the time that it was running. Billing for a modified cluster size begins once the resize is completed. You can also [spin up additional production branches](/docs/postgres/branching) at any time for additional cost. The pricing for these is also prorated.

## Cluster disk storage

<Note>
  We use **[gibibytes, otherwise known as binary gigabytes](https://simple.wikipedia.org/wiki/Gibibyte)**, to calculate storage and usage limits. For reference, 1 binary gigabyte is equivalent to 2^30 bytes.
</Note>

### Storage types explained

PlanetScale offers two storage options with different pricing models:

| [PlanetScale Metal](/docs/metal)                                                                                                                                                                                                                                                | [Network-Attached Storage](/docs/postgres/cluster-configuration/cluster-storage)                                                                                                                                                                                                                                                                                                                                      |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **All-inclusive pricing**                                                                                                                                                                                                                                                       | **Configuration-based pricing**                                                                                                                                                                                                                                                                                                                                                                                       |
| - Storage cost is included in your cluster instance price <br /> - High-performance NVMe SSDs directly attached to your servers <br /> - To get more storage, you upgrade to a larger cluster size <br /> - Best for: High-performance applications that need predictable costs | - First 10 GB included with each cluster <br /> - Additional Storage is billed separately from your cluster instance configuration <br /> - Flexible scaling - add storage without changing cluster size <br /> - Automatic disk scaling with [autoscaling](/docs/postgres/cluster-configuration/cluster-storage#enable-autoscaling) <br /> - Best for: Applications with variable storage needs or cost optimization |

### Network-attached storage billing

For network-attached storage clusters, pricing varies by region and cloud provider:

* **Base storage**: you manually configure this in cluster settings, unless [autoscaling](/docs/postgres/cluster-configuration/cluster-storage#enable-autoscaling) is enabled
* **Additional IOPS**: you manually configure this in cluster settings (AWS only)
* **Additional Throughput**: you manually configure this in cluster settings (AWS only)

<Note>
  **AWS vs GCP Pricing**: AWS network-attached storage cluster types allow separate billing for IOPS and throughput that you manually configure. GCP network-attached storage cluster types include IOPS and throughput that scale automatically with disk size at no additional cost. See [AWS Storage Pricing](/docs/postgres/pricing/#aws-storage-pricing) and [GCP Storage Pricing](/docs/postgres/pricing/#gcp-storage-pricing) below for detailed regional pricing.
</Note>

Billing for modified storage attributes begins once the change for the cluster has completed.

**What are IOPS and Throughput?**

* **IOPS** (Input/Output Operations Per Second): How many read/write operations your database can handle per second. Higher IOPS = faster response for many small database queries.
* **Throughput**: How much data can be transferred per second (MiB/s). Higher throughput = faster for large data operations like bulk imports.

For more information on changing your cluster's storage, see the [Cluster storage configuration](/docs/postgres/cluster-configuration/cluster-storage) documentation.

## Backups

### Included backup storage

PlanetScale automatically includes backup storage with every database branch at no extra cost. Each branch gets backup storage equal to **2x your disk size**.

Examples:

| Disk Size | Backup Storage Included |
| :-------- | :---------------------- |
| 25 GB     | 50 GB                   |
| 50 GB     | 100 GB                  |
| 1000 GB   | 2000 GB                 |

<Note>
  For clusters with [network-attached storage](/docs/postgres/cluster-configuration/cluster-storage) with [autoscaling](/docs/postgres/cluster-configuration/cluster-storage#enable-autoscaling) enabled, **backup** storage changes as your **disk storage** autoscales.
</Note>

When your backup and WAL storage exceeds the included amount, additional storage is billed at **\$0.023 per GB per month**.

**Storage overage examples**:

| Storage Overage | Duration | Cost   |
| :-------------- | :------- | :----- |
| 100 GB          | 30 days  | \$2.30 |
| 100 GB          | 15 days  | \$1.15 |

### How backup billing works

**Storage calculation**: Both database data backups and WAL files are automatically compressed before being sent to backup storage. You're billed only on the compressed size, which is typically much smaller than your active database size.

Backup storage usage billing data is updated hourly.

For more details on backup functionality, see [Back up and restore](/docs/postgres/backups).

**What affects storage costs**:

| Factor                | Impact                                                  |
| :-------------------- | :------------------------------------------------------ |
| **Database activity** | More active databases generate more WAL files           |
| **Backup retention**  | Longer retention periods increase storage usage         |
| **Oldest backup age** | WAL files are kept as long as your oldest backup exists |

## [Network data transfer](/docs/postgres/#network-data-transfer)

Outgoing network traffic (egress) is billed based on the amount of data transferred out of PlanetScale Postgres instances to any other host or destination. You are not charged for data transfer for purposes such as Primary to Replica [replication](/docs/postgres/scaling/replicas) or [backups](/docs/postgres/backups). Each branch has a default amount of included network egress that is aggregated for all cluster instances in the entire branch.

| Branch Type          | Included Network Egress |
| -------------------- | ----------------------- |
| Production           | 100 GB per month        |
| Development (PS-DEV) | 10 GB per month         |

See [AWS Egress Pricing](#aws-egress-pricing) and [GCP Egress Pricing](#gcp-egress-pricing) below for detailed regional pricing.

Incoming network traffic (ingress) is free.

## Additional replicas

Each production cluster includes 2 [replicas](/docs/postgres/scaling/replicas) (excluding [single node](/docs/postgres/cluster-configuration/single-node)) to provide high availability and additional read capacity alongside the primary. Certain read-heavy workloads may demand additional read replicas. If you need additional replicas beyond what is included, you can add them for an additional cost based on the branch's instance size and storage requirements. The cost of an additional replica instance is \~1/3 the cost of the original 3-node cluster.

New replica(s) billing begins once the change has completed.

Examples:

| Base cluster configuration (includes 2 replicas) | Monthly base cluster cost           | Additional monthly replica cost     | Monthly total |
| :----------------------------------------------- | :---------------------------------- | :---------------------------------- | :------------ |
| M-160 (r8gd)                                     | \$589                               | \$197                               | \$786         |
| M-320 (r8gd)                                     | \$1,149                             | \$383                               | \$1,532       |
| M-1280 (i8g)                                     | \$5,319                             | \$1,773                             | \$7,092       |
| PS-10 (aarch64 CPU) with 100 GB storage          | $34 + $33.75 (additional storage)   | $12 + $12.50 (\*additional storage) | \$92.25       |
| PS-160 (aarch64 CPU) with 1000 GB storage        | $286 + $371.25 (additional storage) | $96 + $125 (\*additional storage)   | \$878.25      |

\*Note: Replicas with network-attached storage do not include the default included cluster storage disk amount of 10GB. You pay for the entire configured disk size. Figures shown here are for AWS based instances with additional storage amounts and no additional IOPS/Throughput configured.

Development branches and single node databases do not support replicas. For more information on replicas, see [Replicas](/docs/postgres/scaling/replicas).

## Cluster pricing

Cluster instance pricing is based on the cloud provider's own pricing for the compute resources, and varies per region. See the below links for the current region's pricing.

### Amazon Web Services

<CardGroup>
  <Card title="ap-northeast-1 (Tokyo)" href="https://planetscale.com/pricing?region=ap-northeast" icon="angles-right" horizontal />

  <Card title="ap-south-1 (Mumbai)" href="https://planetscale.com/pricing?region=ap-south" icon="angles-right" horizontal />

  <Card title="ap-southeast-1 (Singapore)" href="https://planetscale.com/pricing?region=ap-southeast" icon="angles-right" horizontal />

  <Card title="ap-southeast-2 (Sydney)" href="https://planetscale.com/pricing?region=aws-ap-southeast-2" icon="angles-right" horizontal />

  <Card title="ca-central-1 (Montreal)" href="https://planetscale.com/pricing?region=aws-ca-central-1" icon="angles-right" horizontal />

  <Card title="eu-central-1 (Frankfurt)" href="https://planetscale.com/pricing?region=eu-central" icon="angles-right" horizontal />

  <Card title="eu-west-1 (Dublin)" href="https://planetscale.com/pricing?region=eu-west" icon="angles-right" horizontal />

  <Card title="eu-west-2 (London)" href="https://planetscale.com/pricing?region=aws-eu-west-2" icon="angles-right" horizontal />

  <Card title="sa-east-1 (Sao Paulo)" href="https://planetscale.com/pricing?region=aws-sa-east-1" icon="angles-right" horizontal />

  <Card title="us-east-1 (N. Virginia)" href="https://planetscale.com/pricing?region=us-east" icon="angles-right" horizontal />

  <Card title="us-east-2 (Ohio)" href="https://planetscale.com/pricing?region=us-east-2" icon="angles-right" horizontal />

  <Card title="us-west-2 (Oregon)" href="https://planetscale.com/pricing?region=us-west" icon="angles-right" horizontal />
</CardGroup>

### Google Cloud

<CardGroup>
  <Card title="asia-northeast3 (Seoul, South Korea)" href="https://planetscale.com/pricing?region=gcp-asia-northeast3" icon="angles-right" horizontal />

  <Card title="northamerica-northeast1 (Montréal, Québec)" href="https://planetscale.com/pricing?region=gcp-northamerica-northeast1" icon="angles-right" horizontal />

  <Card title="us-central1 (Council Bluffs, Iowa)" href="https://planetscale.com/pricing?region=gcp-us-central1" icon="angles-right" horizontal />

  <Card title="us-east4 (Ashburn, Virginia)" href="https://planetscale.com/pricing?region=gcp-us-east4" icon="angles-right" horizontal />
</CardGroup>

## Storage pricing

Network-attached storage pricing varies by region and includes three billable components:

* **Storage**: The cost per GB of data stored per month (can [auto-scale](/docs/postgres/cluster-configuration/cluster-storage#enable-autoscaling) with usage if enabled)
* **Additional IOPS**: Cost when you manually configure more input/output operations per second beyond the included baseline
* **Additional Throughput**: Cost when you manually configure more data transfer throughput (MiB/s) beyond the included baseline

<Note>
  **Most applications only pay for base storage.** IOPS and throughput charges only apply if you manually increase these settings beyond the included baseline in your cluster configuration.
</Note>

The table below shows the pricing for all three components across different AWS regions. Your total storage cost will depend on your configuration of each component.

### AWS storage pricing

| Cloud Provider | Region                     | Storage (per GB/month) | Additional IOPS (per IOPS/month) | Additional Throughput (per MiB/s/month) |
| :------------- | :------------------------- | :--------------------- | :------------------------------- | :-------------------------------------- |
| AWS            | ap-northeast-1 (Tokyo)     | \$0.150                | \$0.011                          | \$0.088                                 |
| AWS            | ap-south-1 (Mumbai)        | \$0.143                | \$0.103                          | \$0.084                                 |
| AWS            | ap-southeast-1 (Singapore) | \$0.150                | \$0.011                          | \$0.088                                 |
| AWS            | ap-southeast-2 (Sydney)    | \$0.150                | \$0.011                          | \$0.088                                 |
| AWS            | ca-central-1 (Montreal)    | \$0.138                | \$0.010                          | \$0.081                                 |
| AWS            | eu-central-1 (Frankfurt)   | \$0.149                | \$0.011                          | \$0.088                                 |
| AWS            | eu-west-1 (Dublin)         | \$0.138                | \$0.010                          | \$0.081                                 |
| AWS            | eu-west-2 (London)         | \$0.145                | \$0.011                          | \$0.084                                 |
| AWS            | sa-east-1 (Sao Paulo)      | \$0.238                | \$0.018                          | \$0.139                                 |
| AWS            | us-east-1 (N. Virginia)    | \$0.125                | \$0.009                          | \$0.073                                 |
| AWS            | us-east-2 (Ohio)           | \$0.125                | \$0.009                          | \$0.073                                 |
| AWS            | us-west-2 (Oregon)         | \$0.125                | \$0.009                          | \$0.073                                 |

### GCP storage pricing

The table below shows the storage pricing for Google Cloud Platform regions. GCP storage pricing includes only base storage costs, with IOPS and throughput that scale with disk size.

| Cloud Provider | Region                                     | Storage (per GB/month) |
| :------------- | :----------------------------------------- | :--------------------- |
| GCP            | asia-northeast3 (Seoul, South Korea)       | \$0.221                |
| GCP            | northamerica-northeast1 (Montréal, Québec) | \$0.187                |
| GCP            | us-central1 (Council Bluffs, Iowa)         | \$0.170                |
| GCP            | us-east4 (Ashburn, Virginia)               | \$0.187                |

## Egress pricing

### AWS egress pricing

The table below shows the egress pricing (per GB beyond included amounts) for AWS regions.

| Cloud Provider | Region                     | Egress (per GB) |
| -------------- | -------------------------- | --------------- |
| AWS            | ap-northeast-1 (Tokyo)     | \$0.101         |
| AWS            | ap-south-1 (Mumbai)        | \$0.096         |
| AWS            | ap-southeast-1 (Singapore) | \$0.096         |
| AWS            | ap-southeast-2 (Sydney)    | \$0.111         |
| AWS            | ca-central-1 (Montreal)    | \$0.060         |
| AWS            | eu-central-1 (Frankfurt)   | \$0.060         |
| AWS            | eu-west-1 (Dublin)         | \$0.060         |
| AWS            | eu-west-2 (London)         | \$0.060         |
| AWS            | sa-east-1 (Sao Paulo)      | \$0.137         |
| AWS            | us-east-1 (N. Virginia)    | \$0.060         |
| AWS            | us-east-2 (Ohio)           | \$0.060         |
| AWS            | us-west-2 (Oregon)         | \$0.060         |

### GCP egress pricing

The table below shows the egress pricing (per GB beyond included amounts) for Google Cloud Platform regions.

| Cloud Provider | Region                                     | Egress (per GB) |
| -------------- | ------------------------------------------ | --------------- |
| GCP            | asia-northeast3 (Seoul, South Korea)       | \$0.117         |
| GCP            | europe-west1 (St Ghislain, Belgium)        | \$0.060         |
| GCP            | northamerica-northeast1 (Montréal, Québec) | \$0.060         |
| GCP            | us-central1 (Council Bluffs, Iowa)         | \$0.060         |
| GCP            | us-east4 (Ashburn, Virginia)               | \$0.060         |

## PgBouncer pricing

Dedicated PgBouncer instances are billed monthly based on their size and region. The local PgBouncer (included on the primary database instance) is free. Learn more about PgBouncer configuration in the [PgBouncer documentation](/docs/postgres/connecting/pgbouncer).

PgBouncer instances are available in six sizes:

| Size    | CPU       | Memory |
| ------- | --------- | ------ |
| PGB-5   | 1/16 vCPU | 128 MB |
| PGB-10  | 1/8 vCPU  | 256 MB |
| PGB-20  | 1/4 vCPU  | 512 MB |
| PGB-40  | 1/2 vCPU  | 1 GB   |
| PGB-80  | 1 vCPU    | 2 GB   |
| PGB-160 | 2 vCPU    | 4 GB   |

### AWS PgBouncer pricing

The table below shows the monthly pricing for dedicated PgBouncer instances across AWS regions.

| Region                     | PGB-5 | PGB-10 | PGB-20 | PGB-40 | PGB-80 | PGB-160 |
| -------------------------- | ----- | ------ | ------ | ------ | ------ | ------- |
| ap-northeast-1 (Tokyo)     | \$21  | \$41   | \$82   | \$164  | \$327  | \$653   |
| ap-south-1 (Mumbai)        | \$12  | \$24   | \$47   | \$93   | \$186  | \$371   |
| ap-southeast-1 (Singapore) | \$21  | \$41   | \$82   | \$164  | \$327  | \$653   |
| ap-southeast-2 (Sydney)    | \$21  | \$41   | \$82   | \$163  | \$325  | \$649   |
| ca-central-1 (Montreal)    | \$19  | \$38   | \$75   | \$150  | \$299  | \$598   |
| eu-central-1 (Frankfurt)   | \$21  | \$41   | \$82   | \$164  | \$327  | \$653   |
| eu-west-1 (Dublin)         | \$20  | \$39   | \$77   | \$153  | \$305  | \$610   |
| eu-west-2 (London)         | \$22  | \$44   | \$88   | \$176  | \$351  | \$701   |
| sa-east-1 (Sao Paulo)      | \$30  | \$59   | \$117  | \$234  | \$467  | \$933   |
| us-east-1 (N. Virginia)    | \$18  | \$35   | \$69   | \$138  | \$276  | \$551   |
| us-east-2 (Ohio)           | \$18  | \$35   | \$69   | \$138  | \$276  | \$551   |
| us-west-2 (Oregon)         | \$18  | \$35   | \$69   | \$138  | \$276  | \$551   |

### GCP PgBouncer pricing

The table below shows the monthly pricing for dedicated PgBouncer instances across Google Cloud Platform regions.

| Region                                     | PGB-5 | PGB-10 | PGB-20 | PGB-40 | PGB-80 | PGB-160 |
| ------------------------------------------ | ----- | ------ | ------ | ------ | ------ | ------- |
| asia-northeast3 (Seoul, South Korea)       | \$22  | \$44   | \$87   | \$174  | \$348  | \$695   |
| europe-west1 (Belgium)                     | \$19  | \$38   | \$76   | \$151  | \$302  | \$603   |
| northamerica-northeast1 (Montréal, Québec) | \$19  | \$38   | \$76   | \$151  | \$302  | \$604   |
| us-central1 (Council Bluffs, Iowa)         | \$18  | \$35   | \$70   | \$139  | \$277  | \$554   |
| us-east4 (Ashburn, Virginia)               | \$20  | \$39   | \$78   | \$155  | \$309  | \$617   |

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Database replicas
Source: https://planetscale.com/docs/postgres/scaling/replicas

A replica is a continuously updated copy of your Postgres database.

## Overview

Replicas serve two main purposes:

* They provide a way to reduce load on your primary instance by allowing you to read from a replica.
* They increase database availability by enabling fast failovers for maintenance or unexpected failure.

<Warning>
  Before utilizing replicas for reducing load on the primary, it's important to understand the trade-offs. For more information, see the [Data consistency and replication lag](#data-consistency-and-replication-lag) section.
</Warning>

## How to query Postgres replicas

Postgres replicas can be used to read data and reduce load on the primary. PlanetScale does not automatically route queries to replicas unless you explicitly use a replica credential or tell your application to do so.

<Note>
  **PlanetScale's use of PgBouncer (port `6432`) does not support replica routing.** All connections through PgBouncer are automatically routed to the primary database, regardless of the username specification. Use direct connections (port `5432`) for replica access.
</Note>

To query a replica:

1. Append `|replica` to the end of your username for the branch you want to target:

```bash  theme={null}
user=postgres.cnlmx96ec5kw|replica
```

1. Make sure the port in your connection string is set to `5432`. To pool connections to replicas, use a [dedicated replica PgBouncer](/docs/postgres/connecting/pgbouncer#dedicated-replica-pgbouncers) instead.

Your connection string should look something like this:

```bash  theme={null}
psql 'host=xxxxxxxxxx-useast1-1.horizon.psdb.cloud \
      port=5432 \
      user=postgres.xxxxxxxxxx|replica \
      password=pscale_pw_xxxxxxxxxxxxxxxxxx \
      dbname=my_database \
      sslnegotiation=direct \
      sslmode=verify-full \
      sslrootcert=system'
```

## High availability

Replicas within PlanetScale are used to enable high availability of your database. This is a part of the reason all production branches (excluding [single node](/docs/postgres/cluster-configuration/single-node)) in PlanetScale are provided 2 replicas. In situations where the underlying hardware or service hosting the primary Postgres node fails, our system will automatically elect a new primary node from the available replicas and reroute traffic to that new primary. This process is known as **reparenting** and typically is all handled within milliseconds or seconds.

If you're using [PgBouncer](/docs/postgres/connecting/pgbouncer), querying the primary during a reparent typically goes unnoticed, other than a bit of additional query latency. This is because PlanetScale's PgBouncer buffers queries under the hood during failovers and directs them to the new primary once the failover is complete.

## Multiple availability zones

In cloud architecture, regions are further broken down into logical groupings of data centers known as availability zones (or AZs for short). For example, the `us-east-1` region on AWS contains multiple AZs available to customers starting with `us-east-1a` through `us-east-1f`. The infrastructure for your PlanetScale Postgres database cluster is distributed across 3 availability zones. In the instance of an AZ failure, your database will automatically failover to an available AZ.

## Data consistency and replication lag

PlanetScale Postgres utilizes a consistency model in which data committed (`INSERT`, `UPDATE`, `DELETE`, etc) on the primary must be durably stored on and confirmed by at least one replica before the primary reports the commit succeeded to client.

<Note>
  This consistency model does mean that it's possible for a commit to be visible to other clients/transactions on the primary or the replicas, before the primary reports commit success back to the client.
</Note>

For development branches there is no data replication.

The delay between when a change is applied to a primary and the same change is applied to a replica is known as `replication lag`. Your database's replication lag is viewable on your database main "Dashboard" page.

It is important to be aware of replication lag whenever querying data from your replicas. For example, if you make an update and then immediately try to query for that updated data via a replica, it may not be available yet due to replication lag.

## When should you use replicas in your application?

Replicas are useful for offloading read-heavy workloads from the primary node. By using replicas, you can distribute the read load across multiple nodes, which can help improve the performance of your application. Some examples of where you might want to query a replica are: scheduled jobs, analytics jobs, search features, or aggregate queries. Replicas can also be used to provide a read-only view of your data to users or applications that do not need to write data, such as when a user is logged out or writing one-off queries for debugging purposes.

## Configuring replicas for your database cluster

By default, production databases (excluding [single node](/docs/postgres/cluster-configuration/single-node)) are created with 2 replicas. You may add additional replicas if you need to scale your read traffic. Adding additional replicase beyond the default does not gaurantee an increase in availability of your database.

<Note>
  You are charged for additional replicas you add beyond the default. Billing for additional replicas begins once the replica change has completed. The additional cost is \~ 1/3 the original price of the base cluster running (initial primary + 2 replicas).
</Note>

To see how to increase and manage the number of additional replicas you have, see [Cluster configuration](/docs/postgres/cluster-configuration).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Settings
Source: https://planetscale.com/docs/postgres/settings

The Settings page allows you to configure general settings and options for your PlanetScale Postgres database.

To access the settings:

<Steps>
  <Step>
    From the PlanetScale organization dashboard, select your database
  </Step>

  <Step>
    Navigate to **Settings** on the left sidebar menu
  </Step>
</Steps>

## General Settings

### Database Name

The database name field displays your current database name. **This name cannot be changed** because it's used to connect to the database. Changing the name would break existing connections and applications.

### Default Branch

The default branch dropdown allows you to select which branch serves as the default for your database. This branch will be used as the source when creating new development branches.

To change the default branch:

<Steps>
  <Step>
    Click the **Default branch** dropdown
  </Step>

  <Step>
    Select the desired branch from the list
  </Step>

  <Step>
    Click **Save database settings** at the bottom of the page
  </Step>
</Steps>

### Restrict Branch Regions

The "Restrict branch regions" option allows you to limit where new branches can be created geographically. When enabled, this setting restricts branch creation to the same region as the default branch only.

To enable region restrictions:

<Steps>
  <Step>
    Check the **Restrict branch regions** checkbox
  </Step>

  <Step>
    Click **Save database settings** to apply the change
  </Step>
</Steps>

This setting is useful for:

* Compliance requirements that mandate data residency
* Cost optimization by keeping resources in a specific region
* Reducing latency by keeping all branches in the same geographic area

### Save Settings

After making any changes to the general settings, click the **Save database settings** button to apply your changes.

## Delete Database

The settings page also provides the option to permanently delete your database. This action is irreversible and will:

* Delete the entire database and all of its branches
* Remove all data and backups permanently
* Disconnect any applications currently connected to the database
* Include usage charges in your final invoice through the deletion date

<Warning>
  Database deletion cannot be undone. Make sure you have proper backups and that no critical applications depend on this database before proceeding.
</Warning>

To delete a database:

<Steps>
  <Step>
    Scroll to the bottom of the Settings page
  </Step>

  <Step>
    Click the red **Delete database** button
  </Step>

  <Step>
    Enter the database name
  </Step>

  <Step>
    Click **Delete database** to confirm
  </Step>
</Steps>

Only Organization Administrators and Database Administrators have permission to delete databases. See the [Access Control documentation](/docs/security/access-control) for more information about user permissions.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Read-only mode
Source: https://planetscale.com/docs/postgres/troubleshooting/read-only-mode

PlanetScale may automatically enable read-only mode on a cluster to maintain stability and availability when certain conditions are detected.

Read-only mode preserves cluster functionality while preventing operations that could lead to instability, allowing you to remediate issues before they become critical.

When read-only mode is enabled, attempts to write to the cluster will see the following error:

```sql  theme={null}
invalid statement because cluster is read-only
```

We will also *pause* all logical replication subscriptions writing data into the cluster.
Once your cluster is out of read-only mode you can unpause the subscribers by running

```
ALTER SUBSCRIPTION sub_name ENABLE;
```

on each subscription.

There's no way for a PlanetScale customer to enter or exit read-only mode, except by remediating the conditions that caused the cluster to enter that mode.

Here are the reasons we'll put the cluster into read-only mode:

## Insufficient space

To protect your cluster from crashing, if a cluster has very little space left (typically 5% or less), we will automatically move it into read-only mode. The specific scenarios are:

| Storage Type                 | Scenario                                                                                                                                                                                                                                                                                                                                 | Remediation                                                                                                                                                                                                                       |
| ---------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Network-attached storage** | Clusters with [Autoscaling](/docs/postgres/cluster-configuration/cluster-storage#enable-autoscaling) disabled or where your current [disk size](/docs/postgres/cluster-configuration/cluster-storage#disk-size) is approaching your configured [Autoscaling maximum](/docs/postgres/cluster-configuration/cluster-storage#storage-limit) | Increase the [cluster disk size](/docs/postgres/cluster-configuration/cluster-storage#disk-size) manually or increase the [storage limit on disk autoscaling](/docs/postgres/cluster-configuration/cluster-storage#storage-limit) |
| **PlanetScale Metal**        | When your data is approaching the storage size of your cluster                                                                                                                                                                                                                                                                           | Increase the [cluster size](/docs/metal)                                                                                                                                                                                          |

Once you have remediated the issue, your cluster will automatically exit read-only mode.

<Note>
  PlanetScale will send notifications via email and webhook (if enabled) when your storage exceeds 60, 75, 85, 90, and 95 percent, respectively. You can also track disk utilization via [Metrics](/docs/postgres/monitoring/metrics). Emails are sent to all Organization and Database administrators. It is critical that these email addresses are monitored regularly.
</Note>

If the issue was temporary, say due to errant data being written by mistake, you could remove the data, perform a [vacuum](/docs/postgres/cluster-configuration/cluster-storage#example-when-and-how-to-run-vacuum), and then reverse the remediation action. See [Managing Storage](/docs/postgres/cluster-configuration/cluster-storage#managing-your-storage) for more on reducing storage usage.

## Single Node Archiver lag

Single node Postgres clusters offer decreased durability compared to HA offerings. In HA, we use Postgres synchronous replication to guarantee every committed transaction reaches the primary and at least one replica.
As part of our usual (we do this on HA too) durability posture, we archive postgres Write-Ahead Logs (WAL) to durable object storage (S3, GCP Cloud Storage). For single-node, we will enter read-only mode if this archiving process falls too far behind writes on your cluster.

If you see this happening frequently, we recommend sizing your cluster up, or moving to an HA cluster.


# PlanetScale Postgres with Bun
Source: https://planetscale.com/docs/postgres/tutorials/planetscale-postgres-bun

Create a new Postgres database and integrate it with Bun.

export const YouTubeEmbed = ({id, title}) => {
  return <Frame>
      <iframe src={`https://www.youtube-nocookie.com/embed/${id}?rel=0`} title={title} className="aspect-video w-full" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" />
    </Frame>;
};

Bun provides native support for Postgres databases.

<Callout icon="fast-forward" color="#47b7f8">
  Already created a PlanetScale Postgres database? [Jump straight to integration instructions](#integrate-with-bun).
</Callout>

We'll cover:

* Creating a new Postgres database
* Cluster configuration options
* Connecting to your database

## Prerequisites

Before you begin, make sure you have a [PlanetScale account](https://auth.planetscale.com/sign-up). After you create an account, you'll be prompted to create a new organization, which is essentially a container for your databases, settings, and members.

After creating your organization, it's important to understand the relationship between databases, branches, and clusters.

* **Database**: Your overall project (e.g., "my-ecommerce-app")
* **Branch**: Isolated database deployments that provide you with separate environments for development and testing, as well as restoring from backups - [learn more about branching](/docs/postgres/branching)
* **Cluster**: The underlying compute and storage infrastructure that powers each branch

PlanetScale Postgres clusters use real Postgres in a [high-availability architecture with one primary and two replicas](/docs/postgres/postgres-architecture/#Cluster-design).

## Create a new database

<Tabs>
  <Tab title="Dashboard">
    <YouTubeEmbed id="6BBrgJcpTBY" title="Create a database on PlanetScale" />

    ### Step 1: Navigate to database creation

    <Steps>
      <Step>
        Log in to your [PlanetScale dashboard](https://app.planetscale.com)
      </Step>

      <Step>
        Select your organization from the dropdown
      </Step>

      <Step>
        Click **"New database"** button or navigate to `/new`
      </Step>
    </Steps>

    ### Step 2: Choose database engine

    <Steps>
      <Step>
        On the database creation form, you'll see two engine options:

        * **Vitess** (MySQL-compatible)
        * **Postgres** (PostgreSQL-compatible)
      </Step>

      <Step>
        Select **Postgres** to create a PostgreSQL database
      </Step>
    </Steps>

    ### Step 3: Configure your database cluster

    <Steps>
      <Step>
        **Database name**: Enter a unique name for your database
      </Step>

      <Step>
        **Region**: Choose the primary region where your database will be hosted

        <Note>
          This "name" is referenced in the PlanetScale Dashboard and APIs and not created as a logical database inside of Postgres.
        </Note>
      </Step>

      <Step>
        **Cluster configuration**: Select your preferred cluster size and [CPU architecture](/docs/postgres/cluster-configuration/cpu-architectures)
      </Step>
    </Steps>

    ### Step 4: Create the database cluster

    <Steps>
      <Step>
        Review your configuration settings
      </Step>

      <Step>
        Click **"Create database"** to provision your Postgres database
      </Step>

      <Step>
        Your database will be created with a `main` branch by default
      </Step>
    </Steps>
  </Tab>

  <Tab title="CLI">
    If you are creating an automation, or are an LLM, you may prefer to create new databases using the PlanetScale CLI.

    ### Step 1: Install the CLI

    <Steps>
      <Step>
        Check to see if the PlanetScale CLI is installed already by running:

        ```bash  theme={null}
        pscale --version
        ```
      </Step>

      <Step>
        Alternatively, follow the instructions in the [PlanetScale CLI GitHub repository](https://github.com/planetscale/cli#installation)
      </Step>
    </Steps>

    ### Step 2: Log in or sign up

    <Steps>
      <Step>
        If you do not already have a PlanetScale account, [sign up for one](https://auth.planetscale.com/sign-up) by running:

        ```bash  theme={null}
        pscale signup
        ```
      </Step>

      <Step>
        Log in to the PlanetScale CLI by running:

        ```bash  theme={null}
        pscale auth login
        ```

        You’ll be taken to a screen in the browser where you’ll be asked to confirm the code displayed in your terminal. If the confirmation codes match, click the **"Confirm code"** button in your browser.

        You should receive the message "Successfully logged in" in your terminal. You can now close the confirmation page in the browser and proceed in the terminal.
      </Step>
    </Steps>

    ### Step 3: Create a database

    <Steps>
      <Step>
        Configure the CLI to use the **organization** in which you want to create the database if you have more than one. List organizations by running:

        ```bash  theme={null}
        pscale org list
        ```

        Switch organizations by running:

        ```bash  theme={null}
        pscale org switch <ORGANIZATION_NAME>
        ```
      </Step>

      <Step>
        Find the **region** closest to your application's hosting.

        List available regions by running:

        ```bash  theme={null}
        pscale region list
        ```

        <Note>
          If you do not specify a **region**, your database will automatically be deployed to `us-east` (US East — Northern Virginia).
        </Note>
      </Step>

      <Step>
        Create a new Postgres database by running:

        ```bash  theme={null}
        pscale database create <DATABASE_NAME> --engine postgres --region <REGION_SLUG>
        ```

        <Note>
          Your **database name** can contain lowercase, alphanumeric characters, or underscores. We allow dashes, but don't recommend them, as they may need to be escaped in some instances.
        </Note>
      </Step>
    </Steps>
  </Tab>
</Tabs>

## What happens during creation

When you create a Postgres database cluster, PlanetScale automatically:

* Provisions a PostgreSQL cluster in your selected region
* Creates the initial `main` branch
* Prepopulates Postgres with required default databases
* Sets up monitoring and metrics collection
* Configures backup and high availability settings

## Create credentials and connect

In this section you'll create the "Default role" in your PlanetScale dashboard to create connection credentials for your database branch.

<Note>
  The "Default role" is meant purely for administrative purposes. You can only create one and it has significant privileges for your database cluster and you should treat these credentials carefully. After completing this quickstart, it is *strongly recommended* that you [create another role](/docs/postgres/connecting/roles) for your application use-cases.
</Note>

<Tabs>
  <Tab title="Dashboard">
    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=9d2832f63cdd6203cfe7f1a55fcdbbdb" alt="Database dashboard" data-og-width="1800" width="1800" data-og-height="760" height="760" data-path="docs/postgres/tutorials/new-database.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=ba2c259b2870598d3a4cf4fa88a18724 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=09c9ae2fce51ddcc013c3b82625bfb6a 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=fbfd0843a6fbb170757655896d8cade4 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=f9046e8ac38a2f60f83a57c2a7df73d3 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=eb873e467af068b415734ddb9f11409a 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=5df4d215dfedb451cf51ec27900ff120 2500w" />
    </Frame>

    <Steps>
      <Step>
        Navigate to your database in the [PlanetScale dashboard](https://app.planetscale.com)
      </Step>

      <Step>
        Click on the **"Connect"** button in the top right
      </Step>

      <Step>
        Select **"Default role"**
      </Step>

      <Step>
        Click **"Create default role"**. A new default role is created for your database branch.
      </Step>

      <Step>
        Record the "Host", "Username", and "Password" for the "Default role" someplace secure.
        <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=6441862bb6d6890244eef761af99cea4" alt="Create a new role" data-og-width="2060" width="2060" data-og-height="1026" height="1026" data-path="docs/postgres/tutorials/create-role.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=4fe62ba845c393c8821f189886b498ff 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=03cb8bd911c01dd8df99d296eaf05c92 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=5880ac93f049e1c055f905f664944d26 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=9ba45ca192ab29c0b12586cf806c4621 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=1c686b956b60806404a54d93b9185cd3 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=05c057853bee8d3ceb5b53c0e82f0025 2500w" />
      </Step>

      <Step>
        You can generate connection strings under **"How are you connecting?"** for major languages, frameworks, and tools.

                <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=761943da846fbb9e1086b1603d002128" alt="Connection strings" data-og-width="1806" width="1806" data-og-height="984" height="984" data-path="docs/postgres/tutorials/langs-and-frames.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=6a2a3aa3ed3c45769ed09c86dbd8e82e 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=ef9f2ab2a8df1d31fd56f4b9f57cec46 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=c86a885c2f951c89c7d3524ce81054ab 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=e2eaa548d3b951b37e1fdfa4ade59ada 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=510aa78544776a2da5172b4ae8568c94 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=8c352e6883f12f79fd4648a9dfd08bb5 2500w" />

        Your connection details will include:

        * **Host**: the DNS name of your database endpoint
        * **Username**: automatically formatted for routing to the correct `branch`
        * **Password**: A securely generated password
        * **Database**: `postgres` (default database)
        * **Port**: `5432` (standard PostgreSQL port) or `6432` (for using [PgBouncer](/docs/postgres/connecting/pgbouncer))
      </Step>
    </Steps>
  </Tab>

  <Tab title="CLI">
    Create a new "Default role" in your PlanetScale CLI to create connection credentials for your database branch.

    <Steps>
      <Step>
        Run the following command to create the "Default role" for your database branch.

        ```bash  theme={null}
        pscale role reset-default <DATABASE_NAME> <BRANCH_NAME>
        ```
      </Step>

      <Step>
        Record the "Host", "Username", and "Password" for the "Default role" somewhere secure.
      </Step>
    </Steps>
  </Tab>
</Tabs>

<Note>
  Passwords are shown only once. If you lose your record of the password, you must [reset the password](/docs/postgres/connecting/roles).
</Note>

## Integrate with Bun

### Step 1: Initialization

You will need to have Bun installed. See the Bun docs for [installation instructions](https://bun.com/docs/installation).

If you do not already have a Bun project, initialize one with the following command:

```bash Terminal theme={null}
bun init
```

### Step 2: Add credentials to .env

For local development, you can place your credentials in a `.env` file. For production, we recommend setting your credentials as environment variables wherever your application is deployed.

Bun will automatically load your credentials from the `.env` file.

Replace the placeholders below with the role credentials created in the previous section.

```bash .env theme={null}
DATABASE_URL='postgresql://{username}:{password}@{host}:{port}/{database}?sslmode=verify-full'
```

Choose the appropriate **port** for your use case. Learn more about [Direct vs PgBouncer connections](/docs/postgres/connecting/quickstart#connection-types%3A-direct-vs-pgbouncer).

<Columns cols={2}>
  <Card title="PgBouncer">
    Port `6432` enables a lightweight connection pooler for PostgreSQL. This facilitates better performance when there are many simultaneous connections.
  </Card>

  <Card title="Direct">
    Port `5432` connects directly to PostgreSQL. Total connections are limited by your cluster's `max_connections` setting.
  </Card>
</Columns>

<Note>
  Both connection types will disconnect when your database restarts or handles a failover scenario.
</Note>

### Step 3: Create a database connection

To run a query, you can use the `sql` template literal. As Postgres is the default choice in Bun, using it requires no additional configuration.

```typescript index.ts theme={null}
import { sql } from 'bun'

const now = await sql`
  SELECT NOW()
`

console.log(now)
```

From the command line, run the following command to execute the script:

```bash Terminal theme={null}
bun index.ts
```

See the [Bun SQL documentation](https://bun.com/docs/runtime/sql#postgresql) for more information about queries and mutations.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale Postgres with Drizzle
Source: https://planetscale.com/docs/postgres/tutorials/planetscale-postgres-drizzle

Create a new Postgres database and integrate it with Drizzle.

export const YouTubeEmbed = ({id, title}) => {
  return <Frame>
      <iframe src={`https://www.youtube-nocookie.com/embed/${id}?rel=0`} title={title} className="aspect-video w-full" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" />
    </Frame>;
};

Drizzle is a modern SQL ORM for TypeScript that allows you to write type-safe SQL queries and migrations.

<Callout icon="fast-forward" color="#47b7f8">
  Already created a PlanetScale Postgres database? [Jump straight to integration instructions](#integrate-with-drizzle).
</Callout>

We'll cover:

* Creating a new Postgres database
* Cluster configuration options
* Connecting to your database

## Prerequisites

Before you begin, make sure you have a [PlanetScale account](https://auth.planetscale.com/sign-up). After you create an account, you'll be prompted to create a new organization, which is essentially a container for your databases, settings, and members.

After creating your organization, it's important to understand the relationship between databases, branches, and clusters.

* **Database**: Your overall project (e.g., "my-ecommerce-app")
* **Branch**: Isolated database deployments that provide you with separate environments for development and testing, as well as restoring from backups - [learn more about branching](/docs/postgres/branching)
* **Cluster**: The underlying compute and storage infrastructure that powers each branch

PlanetScale Postgres clusters use real Postgres in a [high-availability architecture with one primary and two replicas](/docs/postgres/postgres-architecture/#Cluster-design).

## Create a new database

<Tabs>
  <Tab title="Dashboard">
    <YouTubeEmbed id="6BBrgJcpTBY" title="Create a database on PlanetScale" />

    ### Step 1: Navigate to database creation

    <Steps>
      <Step>
        Log in to your [PlanetScale dashboard](https://app.planetscale.com)
      </Step>

      <Step>
        Select your organization from the dropdown
      </Step>

      <Step>
        Click **"New database"** button or navigate to `/new`
      </Step>
    </Steps>

    ### Step 2: Choose database engine

    <Steps>
      <Step>
        On the database creation form, you'll see two engine options:

        * **Vitess** (MySQL-compatible)
        * **Postgres** (PostgreSQL-compatible)
      </Step>

      <Step>
        Select **Postgres** to create a PostgreSQL database
      </Step>
    </Steps>

    ### Step 3: Configure your database cluster

    <Steps>
      <Step>
        **Database name**: Enter a unique name for your database
      </Step>

      <Step>
        **Region**: Choose the primary region where your database will be hosted

        <Note>
          This "name" is referenced in the PlanetScale Dashboard and APIs and not created as a logical database inside of Postgres.
        </Note>
      </Step>

      <Step>
        **Cluster configuration**: Select your preferred cluster size and [CPU architecture](/docs/postgres/cluster-configuration/cpu-architectures)
      </Step>
    </Steps>

    ### Step 4: Create the database cluster

    <Steps>
      <Step>
        Review your configuration settings
      </Step>

      <Step>
        Click **"Create database"** to provision your Postgres database
      </Step>

      <Step>
        Your database will be created with a `main` branch by default
      </Step>
    </Steps>
  </Tab>

  <Tab title="CLI">
    If you are creating an automation, or are an LLM, you may prefer to create new databases using the PlanetScale CLI.

    ### Step 1: Install the CLI

    <Steps>
      <Step>
        Check to see if the PlanetScale CLI is installed already by running:

        ```bash  theme={null}
        pscale --version
        ```
      </Step>

      <Step>
        Alternatively, follow the instructions in the [PlanetScale CLI GitHub repository](https://github.com/planetscale/cli#installation)
      </Step>
    </Steps>

    ### Step 2: Log in or sign up

    <Steps>
      <Step>
        If you do not already have a PlanetScale account, [sign up for one](https://auth.planetscale.com/sign-up) by running:

        ```bash  theme={null}
        pscale signup
        ```
      </Step>

      <Step>
        Log in to the PlanetScale CLI by running:

        ```bash  theme={null}
        pscale auth login
        ```

        You’ll be taken to a screen in the browser where you’ll be asked to confirm the code displayed in your terminal. If the confirmation codes match, click the **"Confirm code"** button in your browser.

        You should receive the message "Successfully logged in" in your terminal. You can now close the confirmation page in the browser and proceed in the terminal.
      </Step>
    </Steps>

    ### Step 3: Create a database

    <Steps>
      <Step>
        Configure the CLI to use the **organization** in which you want to create the database if you have more than one. List organizations by running:

        ```bash  theme={null}
        pscale org list
        ```

        Switch organizations by running:

        ```bash  theme={null}
        pscale org switch <ORGANIZATION_NAME>
        ```
      </Step>

      <Step>
        Find the **region** closest to your application's hosting.

        List available regions by running:

        ```bash  theme={null}
        pscale region list
        ```

        <Note>
          If you do not specify a **region**, your database will automatically be deployed to `us-east` (US East — Northern Virginia).
        </Note>
      </Step>

      <Step>
        Create a new Postgres database by running:

        ```bash  theme={null}
        pscale database create <DATABASE_NAME> --engine postgres --region <REGION_SLUG>
        ```

        <Note>
          Your **database name** can contain lowercase, alphanumeric characters, or underscores. We allow dashes, but don't recommend them, as they may need to be escaped in some instances.
        </Note>
      </Step>
    </Steps>
  </Tab>
</Tabs>

## What happens during creation

When you create a Postgres database cluster, PlanetScale automatically:

* Provisions a PostgreSQL cluster in your selected region
* Creates the initial `main` branch
* Prepopulates Postgres with required default databases
* Sets up monitoring and metrics collection
* Configures backup and high availability settings

## Create credentials and connect

In this section you'll create the "Default role" in your PlanetScale dashboard to create connection credentials for your database branch.

<Note>
  The "Default role" is meant purely for administrative purposes. You can only create one and it has significant privileges for your database cluster and you should treat these credentials carefully. After completing this quickstart, it is *strongly recommended* that you [create another role](/docs/postgres/connecting/roles) for your application use-cases.
</Note>

<Tabs>
  <Tab title="Dashboard">
    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=9d2832f63cdd6203cfe7f1a55fcdbbdb" alt="Database dashboard" data-og-width="1800" width="1800" data-og-height="760" height="760" data-path="docs/postgres/tutorials/new-database.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=ba2c259b2870598d3a4cf4fa88a18724 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=09c9ae2fce51ddcc013c3b82625bfb6a 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=fbfd0843a6fbb170757655896d8cade4 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=f9046e8ac38a2f60f83a57c2a7df73d3 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=eb873e467af068b415734ddb9f11409a 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=5df4d215dfedb451cf51ec27900ff120 2500w" />
    </Frame>

    <Steps>
      <Step>
        Navigate to your database in the [PlanetScale dashboard](https://app.planetscale.com)
      </Step>

      <Step>
        Click on the **"Connect"** button in the top right
      </Step>

      <Step>
        Select **"Default role"**
      </Step>

      <Step>
        Click **"Create default role"**. A new default role is created for your database branch.
      </Step>

      <Step>
        Record the "Host", "Username", and "Password" for the "Default role" someplace secure.
        <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=6441862bb6d6890244eef761af99cea4" alt="Create a new role" data-og-width="2060" width="2060" data-og-height="1026" height="1026" data-path="docs/postgres/tutorials/create-role.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=4fe62ba845c393c8821f189886b498ff 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=03cb8bd911c01dd8df99d296eaf05c92 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=5880ac93f049e1c055f905f664944d26 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=9ba45ca192ab29c0b12586cf806c4621 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=1c686b956b60806404a54d93b9185cd3 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=05c057853bee8d3ceb5b53c0e82f0025 2500w" />
      </Step>

      <Step>
        You can generate connection strings under **"How are you connecting?"** for major languages, frameworks, and tools.

                <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=761943da846fbb9e1086b1603d002128" alt="Connection strings" data-og-width="1806" width="1806" data-og-height="984" height="984" data-path="docs/postgres/tutorials/langs-and-frames.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=6a2a3aa3ed3c45769ed09c86dbd8e82e 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=ef9f2ab2a8df1d31fd56f4b9f57cec46 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=c86a885c2f951c89c7d3524ce81054ab 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=e2eaa548d3b951b37e1fdfa4ade59ada 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=510aa78544776a2da5172b4ae8568c94 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=8c352e6883f12f79fd4648a9dfd08bb5 2500w" />

        Your connection details will include:

        * **Host**: the DNS name of your database endpoint
        * **Username**: automatically formatted for routing to the correct `branch`
        * **Password**: A securely generated password
        * **Database**: `postgres` (default database)
        * **Port**: `5432` (standard PostgreSQL port) or `6432` (for using [PgBouncer](/docs/postgres/connecting/pgbouncer))
      </Step>
    </Steps>
  </Tab>

  <Tab title="CLI">
    Create a new "Default role" in your PlanetScale CLI to create connection credentials for your database branch.

    <Steps>
      <Step>
        Run the following command to create the "Default role" for your database branch.

        ```bash  theme={null}
        pscale role reset-default <DATABASE_NAME> <BRANCH_NAME>
        ```
      </Step>

      <Step>
        Record the "Host", "Username", and "Password" for the "Default role" somewhere secure.
      </Step>
    </Steps>
  </Tab>
</Tabs>

<Note>
  Passwords are shown only once. If you lose your record of the password, you must [reset the password](/docs/postgres/connecting/roles).
</Note>

## Integrate with Drizzle

Installation instructions vary depending on your choice of JavaScript runtime or framework. The following are general instructions for installing Drizzle and connecting to your database.

### Step 1: Install packages

Run the following to install `drizzle-orm` and `pg`:

```bash Terminal theme={null}
npm install drizzle-orm pg
```

Optionally, you can also install `dotenv` to load your credentials from an `.env` file for local development.

```bash Terminal theme={null}
npm install dotenv
```

### Step 2: Add credentials to .env

For local development, you can place your credentials in a `.env` file. For production, we recommend setting your credentials as environment variables wherever your application is deployed.

Replace the placeholders below with the role credentials created in the previous section.

```bash .env theme={null}
DATABASE_URL='postgresql://{username}:{password}@{host}:{port}/{database}?sslmode=verify-full'
```

Choose the appropriate **port** for your use case. Learn more about [Direct vs PgBouncer connections](/docs/postgres/connecting/quickstart#connection-types%3A-direct-vs-pgbouncer).

<Columns cols={2}>
  <Card title="PgBouncer">
    Port `6432` enables a lightweight connection pooler for PostgreSQL. This facilitates better performance when there are many simultaneous connections.
  </Card>

  <Card title="Direct">
    Port `5432` connects directly to PostgreSQL. Total connections are limited by your cluster's `max_connections` setting.
  </Card>
</Columns>

<Note>
  Both connection types will disconnect when your database restarts or handles a failover scenario.
</Note>

### Step 3: Create a database connection

```typescript src/db/index.ts theme={null}
import 'dotenv/config'
import { drizzle } from 'drizzle-orm/node-postgres'

export const db = drizzle(process.env.DATABASE_URL)
```

Where you import this `db` variable will depend on your choice of JavaScript runtime or framework.

For more information on using Drizzle including creating schemas and performing queries, [refer to their documentation](https://orm.drizzle.team/docs).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale Postgres with Kysely
Source: https://planetscale.com/docs/postgres/tutorials/planetscale-postgres-kysely

Create a new Postgres database and integrate it with Kysely.

export const YouTubeEmbed = ({id, title}) => {
  return <Frame>
      <iframe src={`https://www.youtube-nocookie.com/embed/${id}?rel=0`} title={title} className="aspect-video w-full" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" />
    </Frame>;
};

Kysely is a type-safe SQL query builder for TypeScript.

<Callout icon="fast-forward" color="#47b7f8">
  Already created a PlanetScale Postgres database? [Jump straight to integration instructions](#integrate-with-kysely).
</Callout>

We'll cover:

* Creating a new Postgres database
* Cluster configuration options
* Connecting to your database

## Prerequisites

Before you begin, make sure you have a [PlanetScale account](https://auth.planetscale.com/sign-up). After you create an account, you'll be prompted to create a new organization, which is essentially a container for your databases, settings, and members.

After creating your organization, it's important to understand the relationship between databases, branches, and clusters.

* **Database**: Your overall project (e.g., "my-ecommerce-app")
* **Branch**: Isolated database deployments that provide you with separate environments for development and testing, as well as restoring from backups - [learn more about branching](/docs/postgres/branching)
* **Cluster**: The underlying compute and storage infrastructure that powers each branch

PlanetScale Postgres clusters use real Postgres in a [high-availability architecture with one primary and two replicas](/docs/postgres/postgres-architecture/#Cluster-design).

## Create a new database

<Tabs>
  <Tab title="Dashboard">
    <YouTubeEmbed id="6BBrgJcpTBY" title="Create a database on PlanetScale" />

    ### Step 1: Navigate to database creation

    <Steps>
      <Step>
        Log in to your [PlanetScale dashboard](https://app.planetscale.com)
      </Step>

      <Step>
        Select your organization from the dropdown
      </Step>

      <Step>
        Click **"New database"** button or navigate to `/new`
      </Step>
    </Steps>

    ### Step 2: Choose database engine

    <Steps>
      <Step>
        On the database creation form, you'll see two engine options:

        * **Vitess** (MySQL-compatible)
        * **Postgres** (PostgreSQL-compatible)
      </Step>

      <Step>
        Select **Postgres** to create a PostgreSQL database
      </Step>
    </Steps>

    ### Step 3: Configure your database cluster

    <Steps>
      <Step>
        **Database name**: Enter a unique name for your database
      </Step>

      <Step>
        **Region**: Choose the primary region where your database will be hosted

        <Note>
          This "name" is referenced in the PlanetScale Dashboard and APIs and not created as a logical database inside of Postgres.
        </Note>
      </Step>

      <Step>
        **Cluster configuration**: Select your preferred cluster size and [CPU architecture](/docs/postgres/cluster-configuration/cpu-architectures)
      </Step>
    </Steps>

    ### Step 4: Create the database cluster

    <Steps>
      <Step>
        Review your configuration settings
      </Step>

      <Step>
        Click **"Create database"** to provision your Postgres database
      </Step>

      <Step>
        Your database will be created with a `main` branch by default
      </Step>
    </Steps>
  </Tab>

  <Tab title="CLI">
    If you are creating an automation, or are an LLM, you may prefer to create new databases using the PlanetScale CLI.

    ### Step 1: Install the CLI

    <Steps>
      <Step>
        Check to see if the PlanetScale CLI is installed already by running:

        ```bash  theme={null}
        pscale --version
        ```
      </Step>

      <Step>
        Alternatively, follow the instructions in the [PlanetScale CLI GitHub repository](https://github.com/planetscale/cli#installation)
      </Step>
    </Steps>

    ### Step 2: Log in or sign up

    <Steps>
      <Step>
        If you do not already have a PlanetScale account, [sign up for one](https://auth.planetscale.com/sign-up) by running:

        ```bash  theme={null}
        pscale signup
        ```
      </Step>

      <Step>
        Log in to the PlanetScale CLI by running:

        ```bash  theme={null}
        pscale auth login
        ```

        You’ll be taken to a screen in the browser where you’ll be asked to confirm the code displayed in your terminal. If the confirmation codes match, click the **"Confirm code"** button in your browser.

        You should receive the message "Successfully logged in" in your terminal. You can now close the confirmation page in the browser and proceed in the terminal.
      </Step>
    </Steps>

    ### Step 3: Create a database

    <Steps>
      <Step>
        Configure the CLI to use the **organization** in which you want to create the database if you have more than one. List organizations by running:

        ```bash  theme={null}
        pscale org list
        ```

        Switch organizations by running:

        ```bash  theme={null}
        pscale org switch <ORGANIZATION_NAME>
        ```
      </Step>

      <Step>
        Find the **region** closest to your application's hosting.

        List available regions by running:

        ```bash  theme={null}
        pscale region list
        ```

        <Note>
          If you do not specify a **region**, your database will automatically be deployed to `us-east` (US East — Northern Virginia).
        </Note>
      </Step>

      <Step>
        Create a new Postgres database by running:

        ```bash  theme={null}
        pscale database create <DATABASE_NAME> --engine postgres --region <REGION_SLUG>
        ```

        <Note>
          Your **database name** can contain lowercase, alphanumeric characters, or underscores. We allow dashes, but don't recommend them, as they may need to be escaped in some instances.
        </Note>
      </Step>
    </Steps>
  </Tab>
</Tabs>

## What happens during creation

When you create a Postgres database cluster, PlanetScale automatically:

* Provisions a PostgreSQL cluster in your selected region
* Creates the initial `main` branch
* Prepopulates Postgres with required default databases
* Sets up monitoring and metrics collection
* Configures backup and high availability settings

## Create credentials and connect

In this section you'll create the "Default role" in your PlanetScale dashboard to create connection credentials for your database branch.

<Note>
  The "Default role" is meant purely for administrative purposes. You can only create one and it has significant privileges for your database cluster and you should treat these credentials carefully. After completing this quickstart, it is *strongly recommended* that you [create another role](/docs/postgres/connecting/roles) for your application use-cases.
</Note>

<Tabs>
  <Tab title="Dashboard">
    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=9d2832f63cdd6203cfe7f1a55fcdbbdb" alt="Database dashboard" data-og-width="1800" width="1800" data-og-height="760" height="760" data-path="docs/postgres/tutorials/new-database.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=ba2c259b2870598d3a4cf4fa88a18724 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=09c9ae2fce51ddcc013c3b82625bfb6a 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=fbfd0843a6fbb170757655896d8cade4 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=f9046e8ac38a2f60f83a57c2a7df73d3 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=eb873e467af068b415734ddb9f11409a 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=5df4d215dfedb451cf51ec27900ff120 2500w" />
    </Frame>

    <Steps>
      <Step>
        Navigate to your database in the [PlanetScale dashboard](https://app.planetscale.com)
      </Step>

      <Step>
        Click on the **"Connect"** button in the top right
      </Step>

      <Step>
        Select **"Default role"**
      </Step>

      <Step>
        Click **"Create default role"**. A new default role is created for your database branch.
      </Step>

      <Step>
        Record the "Host", "Username", and "Password" for the "Default role" someplace secure.
        <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=6441862bb6d6890244eef761af99cea4" alt="Create a new role" data-og-width="2060" width="2060" data-og-height="1026" height="1026" data-path="docs/postgres/tutorials/create-role.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=4fe62ba845c393c8821f189886b498ff 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=03cb8bd911c01dd8df99d296eaf05c92 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=5880ac93f049e1c055f905f664944d26 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=9ba45ca192ab29c0b12586cf806c4621 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=1c686b956b60806404a54d93b9185cd3 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=05c057853bee8d3ceb5b53c0e82f0025 2500w" />
      </Step>

      <Step>
        You can generate connection strings under **"How are you connecting?"** for major languages, frameworks, and tools.

                <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=761943da846fbb9e1086b1603d002128" alt="Connection strings" data-og-width="1806" width="1806" data-og-height="984" height="984" data-path="docs/postgres/tutorials/langs-and-frames.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=6a2a3aa3ed3c45769ed09c86dbd8e82e 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=ef9f2ab2a8df1d31fd56f4b9f57cec46 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=c86a885c2f951c89c7d3524ce81054ab 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=e2eaa548d3b951b37e1fdfa4ade59ada 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=510aa78544776a2da5172b4ae8568c94 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=8c352e6883f12f79fd4648a9dfd08bb5 2500w" />

        Your connection details will include:

        * **Host**: the DNS name of your database endpoint
        * **Username**: automatically formatted for routing to the correct `branch`
        * **Password**: A securely generated password
        * **Database**: `postgres` (default database)
        * **Port**: `5432` (standard PostgreSQL port) or `6432` (for using [PgBouncer](/docs/postgres/connecting/pgbouncer))
      </Step>
    </Steps>
  </Tab>

  <Tab title="CLI">
    Create a new "Default role" in your PlanetScale CLI to create connection credentials for your database branch.

    <Steps>
      <Step>
        Run the following command to create the "Default role" for your database branch.

        ```bash  theme={null}
        pscale role reset-default <DATABASE_NAME> <BRANCH_NAME>
        ```
      </Step>

      <Step>
        Record the "Host", "Username", and "Password" for the "Default role" somewhere secure.
      </Step>
    </Steps>
  </Tab>
</Tabs>

<Note>
  Passwords are shown only once. If you lose your record of the password, you must [reset the password](/docs/postgres/connecting/roles).
</Note>

## Integrate with Kysely

Installation instructions vary depending on your choice of JavaScript runtime or framework. The following are general instructions for installing Drizzle and connecting to your database.

### Step 1: Install packages

Run the following to install `kysely` and `pg`:

```bash Terminal theme={null}
npm install kysely pg
```

Optionally, you can also install `dotenv` to load your credentials from an `.env` file for local development.

```bash Terminal theme={null}
npm install dotenv
```

### Step 2: Add credentials to .env

For local development, you can place your credentials in a `.env` file. For production, we recommend setting your credentials as environment variables wherever your application is deployed.

Replace the placeholders below with the role credentials created in the previous section.

```bash .env theme={null}
DATABASE_URL='postgresql://{username}:{password}@{host}:{port}/{database}?sslmode=verify-full'
```

Choose the appropriate **port** for your use case. Learn more about [Direct vs PgBouncer connections](/docs/postgres/connecting/quickstart#connection-types%3A-direct-vs-pgbouncer).

<Columns cols={2}>
  <Card title="PgBouncer">
    Port `6432` enables a lightweight connection pooler for PostgreSQL. This facilitates better performance when there are many simultaneous connections.
  </Card>

  <Card title="Direct">
    Port `5432` connects directly to PostgreSQL. Total connections are limited by your cluster's `max_connections` setting.
  </Card>
</Columns>

<Note>
  Both connection types will disconnect when your database restarts or handles a failover scenario.
</Note>

### Step 3: Create a database connection

```typescript app.ts theme={null}
import 'dotenv/config'

import { Kysely, PostgresDialect } from 'kysely'
import { Pool } from 'pg'
import { Database } from './types.ts' // you set this up locally

const db = new Kysely<Database>({
  dialect: new PostgresDialect({
    pool: new Pool({
      connectionString: process.env.DATABASE_URL,
      database: 'postgres'
    })
  })
})
```

Where you import this `db` variable will depend on your choice of JavaScript runtime or framework.

For more information on using Kysely including creating schemas and performing queries, [refer to their documentation](https://kysely.dev/docs).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale Postgres with Node.js
Source: https://planetscale.com/docs/postgres/tutorials/planetscale-postgres-node

Create a new Postgres database and integrate it with Node.js.

export const YouTubeEmbed = ({id, title}) => {
  return <Frame>
      <iframe src={`https://www.youtube-nocookie.com/embed/${id}?rel=0`} title={title} className="aspect-video w-full" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" />
    </Frame>;
};

Node.js is a popular JavaScript runtime for building server-side applications.

<Callout icon="fast-forward" color="#47b7f8">
  Already created a PlanetScale Postgres database? [Jump straight to integration instructions](#integrate-with-node.js).
</Callout>

We'll cover:

* Creating a new Postgres database
* Cluster configuration options
* Connecting to your database

## Prerequisites

Before you begin, make sure you have a [PlanetScale account](https://auth.planetscale.com/sign-up). After you create an account, you'll be prompted to create a new organization, which is essentially a container for your databases, settings, and members.

After creating your organization, it's important to understand the relationship between databases, branches, and clusters.

* **Database**: Your overall project (e.g., "my-ecommerce-app")
* **Branch**: Isolated database deployments that provide you with separate environments for development and testing, as well as restoring from backups - [learn more about branching](/docs/postgres/branching)
* **Cluster**: The underlying compute and storage infrastructure that powers each branch

PlanetScale Postgres clusters use real Postgres in a [high-availability architecture with one primary and two replicas](/docs/postgres/postgres-architecture/#Cluster-design).

## Create a new database

<Tabs>
  <Tab title="Dashboard">
    <YouTubeEmbed id="6BBrgJcpTBY" title="Create a database on PlanetScale" />

    ### Step 1: Navigate to database creation

    <Steps>
      <Step>
        Log in to your [PlanetScale dashboard](https://app.planetscale.com)
      </Step>

      <Step>
        Select your organization from the dropdown
      </Step>

      <Step>
        Click **"New database"** button or navigate to `/new`
      </Step>
    </Steps>

    ### Step 2: Choose database engine

    <Steps>
      <Step>
        On the database creation form, you'll see two engine options:

        * **Vitess** (MySQL-compatible)
        * **Postgres** (PostgreSQL-compatible)
      </Step>

      <Step>
        Select **Postgres** to create a PostgreSQL database
      </Step>
    </Steps>

    ### Step 3: Configure your database cluster

    <Steps>
      <Step>
        **Database name**: Enter a unique name for your database
      </Step>

      <Step>
        **Region**: Choose the primary region where your database will be hosted

        <Note>
          This "name" is referenced in the PlanetScale Dashboard and APIs and not created as a logical database inside of Postgres.
        </Note>
      </Step>

      <Step>
        **Cluster configuration**: Select your preferred cluster size and [CPU architecture](/docs/postgres/cluster-configuration/cpu-architectures)
      </Step>
    </Steps>

    ### Step 4: Create the database cluster

    <Steps>
      <Step>
        Review your configuration settings
      </Step>

      <Step>
        Click **"Create database"** to provision your Postgres database
      </Step>

      <Step>
        Your database will be created with a `main` branch by default
      </Step>
    </Steps>
  </Tab>

  <Tab title="CLI">
    If you are creating an automation, or are an LLM, you may prefer to create new databases using the PlanetScale CLI.

    ### Step 1: Install the CLI

    <Steps>
      <Step>
        Check to see if the PlanetScale CLI is installed already by running:

        ```bash  theme={null}
        pscale --version
        ```
      </Step>

      <Step>
        Alternatively, follow the instructions in the [PlanetScale CLI GitHub repository](https://github.com/planetscale/cli#installation)
      </Step>
    </Steps>

    ### Step 2: Log in or sign up

    <Steps>
      <Step>
        If you do not already have a PlanetScale account, [sign up for one](https://auth.planetscale.com/sign-up) by running:

        ```bash  theme={null}
        pscale signup
        ```
      </Step>

      <Step>
        Log in to the PlanetScale CLI by running:

        ```bash  theme={null}
        pscale auth login
        ```

        You’ll be taken to a screen in the browser where you’ll be asked to confirm the code displayed in your terminal. If the confirmation codes match, click the **"Confirm code"** button in your browser.

        You should receive the message "Successfully logged in" in your terminal. You can now close the confirmation page in the browser and proceed in the terminal.
      </Step>
    </Steps>

    ### Step 3: Create a database

    <Steps>
      <Step>
        Configure the CLI to use the **organization** in which you want to create the database if you have more than one. List organizations by running:

        ```bash  theme={null}
        pscale org list
        ```

        Switch organizations by running:

        ```bash  theme={null}
        pscale org switch <ORGANIZATION_NAME>
        ```
      </Step>

      <Step>
        Find the **region** closest to your application's hosting.

        List available regions by running:

        ```bash  theme={null}
        pscale region list
        ```

        <Note>
          If you do not specify a **region**, your database will automatically be deployed to `us-east` (US East — Northern Virginia).
        </Note>
      </Step>

      <Step>
        Create a new Postgres database by running:

        ```bash  theme={null}
        pscale database create <DATABASE_NAME> --engine postgres --region <REGION_SLUG>
        ```

        <Note>
          Your **database name** can contain lowercase, alphanumeric characters, or underscores. We allow dashes, but don't recommend them, as they may need to be escaped in some instances.
        </Note>
      </Step>
    </Steps>
  </Tab>
</Tabs>

## What happens during creation

When you create a Postgres database cluster, PlanetScale automatically:

* Provisions a PostgreSQL cluster in your selected region
* Creates the initial `main` branch
* Prepopulates Postgres with required default databases
* Sets up monitoring and metrics collection
* Configures backup and high availability settings

## Create credentials and connect

In this section you'll create the "Default role" in your PlanetScale dashboard to create connection credentials for your database branch.

<Note>
  The "Default role" is meant purely for administrative purposes. You can only create one and it has significant privileges for your database cluster and you should treat these credentials carefully. After completing this quickstart, it is *strongly recommended* that you [create another role](/docs/postgres/connecting/roles) for your application use-cases.
</Note>

<Tabs>
  <Tab title="Dashboard">
    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=9d2832f63cdd6203cfe7f1a55fcdbbdb" alt="Database dashboard" data-og-width="1800" width="1800" data-og-height="760" height="760" data-path="docs/postgres/tutorials/new-database.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=ba2c259b2870598d3a4cf4fa88a18724 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=09c9ae2fce51ddcc013c3b82625bfb6a 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=fbfd0843a6fbb170757655896d8cade4 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=f9046e8ac38a2f60f83a57c2a7df73d3 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=eb873e467af068b415734ddb9f11409a 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=5df4d215dfedb451cf51ec27900ff120 2500w" />
    </Frame>

    <Steps>
      <Step>
        Navigate to your database in the [PlanetScale dashboard](https://app.planetscale.com)
      </Step>

      <Step>
        Click on the **"Connect"** button in the top right
      </Step>

      <Step>
        Select **"Default role"**
      </Step>

      <Step>
        Click **"Create default role"**. A new default role is created for your database branch.
      </Step>

      <Step>
        Record the "Host", "Username", and "Password" for the "Default role" someplace secure.
        <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=6441862bb6d6890244eef761af99cea4" alt="Create a new role" data-og-width="2060" width="2060" data-og-height="1026" height="1026" data-path="docs/postgres/tutorials/create-role.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=4fe62ba845c393c8821f189886b498ff 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=03cb8bd911c01dd8df99d296eaf05c92 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=5880ac93f049e1c055f905f664944d26 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=9ba45ca192ab29c0b12586cf806c4621 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=1c686b956b60806404a54d93b9185cd3 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=05c057853bee8d3ceb5b53c0e82f0025 2500w" />
      </Step>

      <Step>
        You can generate connection strings under **"How are you connecting?"** for major languages, frameworks, and tools.

                <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=761943da846fbb9e1086b1603d002128" alt="Connection strings" data-og-width="1806" width="1806" data-og-height="984" height="984" data-path="docs/postgres/tutorials/langs-and-frames.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=6a2a3aa3ed3c45769ed09c86dbd8e82e 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=ef9f2ab2a8df1d31fd56f4b9f57cec46 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=c86a885c2f951c89c7d3524ce81054ab 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=e2eaa548d3b951b37e1fdfa4ade59ada 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=510aa78544776a2da5172b4ae8568c94 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=8c352e6883f12f79fd4648a9dfd08bb5 2500w" />

        Your connection details will include:

        * **Host**: the DNS name of your database endpoint
        * **Username**: automatically formatted for routing to the correct `branch`
        * **Password**: A securely generated password
        * **Database**: `postgres` (default database)
        * **Port**: `5432` (standard PostgreSQL port) or `6432` (for using [PgBouncer](/docs/postgres/connecting/pgbouncer))
      </Step>
    </Steps>
  </Tab>

  <Tab title="CLI">
    Create a new "Default role" in your PlanetScale CLI to create connection credentials for your database branch.

    <Steps>
      <Step>
        Run the following command to create the "Default role" for your database branch.

        ```bash  theme={null}
        pscale role reset-default <DATABASE_NAME> <BRANCH_NAME>
        ```
      </Step>

      <Step>
        Record the "Host", "Username", and "Password" for the "Default role" somewhere secure.
      </Step>
    </Steps>
  </Tab>
</Tabs>

<Note>
  Passwords are shown only once. If you lose your record of the password, you must [reset the password](/docs/postgres/connecting/roles).
</Note>

## Integrate with Node.js

### Step 1: Install packages

First, you will need to install the `pg` package:

```bash  theme={null}
npm install pg
```

Optionally, you can also install `dotenv` to load your credentials from an `.env` file for local development:

```bash  theme={null}
npm install dotenv
```

### Step 2: Add credentials to .env

For local development, you can place your credentials in a `.env` file. For production, we recommend setting your credentials as environment variables wherever your application is deployed.

Replace the placeholders below with the role credentials created in the previous section.

```bash .env theme={null}
DATABASE_URL='postgresql://{username}:{password}@{host}:{port}/{database}?sslmode=verify-full'
```

Choose the appropriate **port** for your use case. Learn more about [Direct vs PgBouncer connections](/docs/postgres/connecting/quickstart#connection-types%3A-direct-vs-pgbouncer).

<Columns cols={2}>
  <Card title="PgBouncer">
    Port `6432` enables a lightweight connection pooler for PostgreSQL. This facilitates better performance when there are many simultaneous connections.
  </Card>

  <Card title="Direct">
    Port `5432` connects directly to PostgreSQL. Total connections are limited by your cluster's `max_connections` setting.
  </Card>
</Columns>

<Note>
  Both connection types will disconnect when your database restarts or handles a failover scenario.
</Note>

### Step 3: Create a database connection

To run a query, you will establish a connection and then use the `query` function:

```javascript index.js theme={null}
require('dotenv').config()

const { Client } = require('pg')

const client = new Client({
  host: process.env.DB_HOST,
  port: process.env.DB_PORT,
  user: process.env.DB_USERNAME,
  password: process.env.DB_PASSWORD,
  database: process.env.DB_DATABASE,
  ssl: {
    rejectUnauthorized: true
  },
})

client
  .connect()
  .then(() => console.log('Connected to PostgreSQL!'))
  .catch((err) => console.error('Connection error', err.stack))

// Example query
client.query('SELECT NOW()', (err, res) => {
  if (err) {
    console.error(err)
  } else {
    console.log(res.rows[0])
  }
  client.end()
})
```

From the command line, run the following command to execute the script:

```bash Terminal theme={null}
node index.js
```

See the [node-postgres documentation](https://node-postgres.com/) for more information.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale Postgres with Prisma
Source: https://planetscale.com/docs/postgres/tutorials/planetscale-postgres-prisma

Create a new Postgres database and integrate it with Prisma.

export const YouTubeEmbed = ({id, title}) => {
  return <Frame>
      <iframe src={`https://www.youtube-nocookie.com/embed/${id}?rel=0`} title={title} className="aspect-video w-full" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" />
    </Frame>;
};

Prisma ORM allows you to write type-safe SQL queries and automatically generate migrations.

<Callout icon="fast-forward" color="#47b7f8">
  Already created a PlanetScale Postgres database? [Jump straight to integration instructions](#integrate-with-prisma).
</Callout>

We'll cover:

* Creating a new Postgres database
* Cluster configuration options
* Connecting to your database

## Prerequisites

Before you begin, make sure you have a [PlanetScale account](https://auth.planetscale.com/sign-up). After you create an account, you'll be prompted to create a new organization, which is essentially a container for your databases, settings, and members.

After creating your organization, it's important to understand the relationship between databases, branches, and clusters.

* **Database**: Your overall project (e.g., "my-ecommerce-app")
* **Branch**: Isolated database deployments that provide you with separate environments for development and testing, as well as restoring from backups - [learn more about branching](/docs/postgres/branching)
* **Cluster**: The underlying compute and storage infrastructure that powers each branch

PlanetScale Postgres clusters use real Postgres in a [high-availability architecture with one primary and two replicas](/docs/postgres/postgres-architecture/#Cluster-design).

## Create a new database

<Tabs>
  <Tab title="Dashboard">
    <YouTubeEmbed id="6BBrgJcpTBY" title="Create a database on PlanetScale" />

    ### Step 1: Navigate to database creation

    <Steps>
      <Step>
        Log in to your [PlanetScale dashboard](https://app.planetscale.com)
      </Step>

      <Step>
        Select your organization from the dropdown
      </Step>

      <Step>
        Click **"New database"** button or navigate to `/new`
      </Step>
    </Steps>

    ### Step 2: Choose database engine

    <Steps>
      <Step>
        On the database creation form, you'll see two engine options:

        * **Vitess** (MySQL-compatible)
        * **Postgres** (PostgreSQL-compatible)
      </Step>

      <Step>
        Select **Postgres** to create a PostgreSQL database
      </Step>
    </Steps>

    ### Step 3: Configure your database cluster

    <Steps>
      <Step>
        **Database name**: Enter a unique name for your database
      </Step>

      <Step>
        **Region**: Choose the primary region where your database will be hosted

        <Note>
          This "name" is referenced in the PlanetScale Dashboard and APIs and not created as a logical database inside of Postgres.
        </Note>
      </Step>

      <Step>
        **Cluster configuration**: Select your preferred cluster size and [CPU architecture](/docs/postgres/cluster-configuration/cpu-architectures)
      </Step>
    </Steps>

    ### Step 4: Create the database cluster

    <Steps>
      <Step>
        Review your configuration settings
      </Step>

      <Step>
        Click **"Create database"** to provision your Postgres database
      </Step>

      <Step>
        Your database will be created with a `main` branch by default
      </Step>
    </Steps>
  </Tab>

  <Tab title="CLI">
    If you are creating an automation, or are an LLM, you may prefer to create new databases using the PlanetScale CLI.

    ### Step 1: Install the CLI

    <Steps>
      <Step>
        Check to see if the PlanetScale CLI is installed already by running:

        ```bash  theme={null}
        pscale --version
        ```
      </Step>

      <Step>
        Alternatively, follow the instructions in the [PlanetScale CLI GitHub repository](https://github.com/planetscale/cli#installation)
      </Step>
    </Steps>

    ### Step 2: Log in or sign up

    <Steps>
      <Step>
        If you do not already have a PlanetScale account, [sign up for one](https://auth.planetscale.com/sign-up) by running:

        ```bash  theme={null}
        pscale signup
        ```
      </Step>

      <Step>
        Log in to the PlanetScale CLI by running:

        ```bash  theme={null}
        pscale auth login
        ```

        You’ll be taken to a screen in the browser where you’ll be asked to confirm the code displayed in your terminal. If the confirmation codes match, click the **"Confirm code"** button in your browser.

        You should receive the message "Successfully logged in" in your terminal. You can now close the confirmation page in the browser and proceed in the terminal.
      </Step>
    </Steps>

    ### Step 3: Create a database

    <Steps>
      <Step>
        Configure the CLI to use the **organization** in which you want to create the database if you have more than one. List organizations by running:

        ```bash  theme={null}
        pscale org list
        ```

        Switch organizations by running:

        ```bash  theme={null}
        pscale org switch <ORGANIZATION_NAME>
        ```
      </Step>

      <Step>
        Find the **region** closest to your application's hosting.

        List available regions by running:

        ```bash  theme={null}
        pscale region list
        ```

        <Note>
          If you do not specify a **region**, your database will automatically be deployed to `us-east` (US East — Northern Virginia).
        </Note>
      </Step>

      <Step>
        Create a new Postgres database by running:

        ```bash  theme={null}
        pscale database create <DATABASE_NAME> --engine postgres --region <REGION_SLUG>
        ```

        <Note>
          Your **database name** can contain lowercase, alphanumeric characters, or underscores. We allow dashes, but don't recommend them, as they may need to be escaped in some instances.
        </Note>
      </Step>
    </Steps>
  </Tab>
</Tabs>

## What happens during creation

When you create a Postgres database cluster, PlanetScale automatically:

* Provisions a PostgreSQL cluster in your selected region
* Creates the initial `main` branch
* Prepopulates Postgres with required default databases
* Sets up monitoring and metrics collection
* Configures backup and high availability settings

## Create credentials and connect

In this section you'll create the "Default role" in your PlanetScale dashboard to create connection credentials for your database branch.

<Note>
  The "Default role" is meant purely for administrative purposes. You can only create one and it has significant privileges for your database cluster and you should treat these credentials carefully. After completing this quickstart, it is *strongly recommended* that you [create another role](/docs/postgres/connecting/roles) for your application use-cases.
</Note>

<Tabs>
  <Tab title="Dashboard">
    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=9d2832f63cdd6203cfe7f1a55fcdbbdb" alt="Database dashboard" data-og-width="1800" width="1800" data-og-height="760" height="760" data-path="docs/postgres/tutorials/new-database.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=ba2c259b2870598d3a4cf4fa88a18724 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=09c9ae2fce51ddcc013c3b82625bfb6a 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=fbfd0843a6fbb170757655896d8cade4 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=f9046e8ac38a2f60f83a57c2a7df73d3 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=eb873e467af068b415734ddb9f11409a 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=5df4d215dfedb451cf51ec27900ff120 2500w" />
    </Frame>

    <Steps>
      <Step>
        Navigate to your database in the [PlanetScale dashboard](https://app.planetscale.com)
      </Step>

      <Step>
        Click on the **"Connect"** button in the top right
      </Step>

      <Step>
        Select **"Default role"**
      </Step>

      <Step>
        Click **"Create default role"**. A new default role is created for your database branch.
      </Step>

      <Step>
        Record the "Host", "Username", and "Password" for the "Default role" someplace secure.
        <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=6441862bb6d6890244eef761af99cea4" alt="Create a new role" data-og-width="2060" width="2060" data-og-height="1026" height="1026" data-path="docs/postgres/tutorials/create-role.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=4fe62ba845c393c8821f189886b498ff 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=03cb8bd911c01dd8df99d296eaf05c92 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=5880ac93f049e1c055f905f664944d26 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=9ba45ca192ab29c0b12586cf806c4621 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=1c686b956b60806404a54d93b9185cd3 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=05c057853bee8d3ceb5b53c0e82f0025 2500w" />
      </Step>

      <Step>
        You can generate connection strings under **"How are you connecting?"** for major languages, frameworks, and tools.

                <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=761943da846fbb9e1086b1603d002128" alt="Connection strings" data-og-width="1806" width="1806" data-og-height="984" height="984" data-path="docs/postgres/tutorials/langs-and-frames.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=6a2a3aa3ed3c45769ed09c86dbd8e82e 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=ef9f2ab2a8df1d31fd56f4b9f57cec46 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=c86a885c2f951c89c7d3524ce81054ab 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=e2eaa548d3b951b37e1fdfa4ade59ada 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=510aa78544776a2da5172b4ae8568c94 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=8c352e6883f12f79fd4648a9dfd08bb5 2500w" />

        Your connection details will include:

        * **Host**: the DNS name of your database endpoint
        * **Username**: automatically formatted for routing to the correct `branch`
        * **Password**: A securely generated password
        * **Database**: `postgres` (default database)
        * **Port**: `5432` (standard PostgreSQL port) or `6432` (for using [PgBouncer](/docs/postgres/connecting/pgbouncer))
      </Step>
    </Steps>
  </Tab>

  <Tab title="CLI">
    Create a new "Default role" in your PlanetScale CLI to create connection credentials for your database branch.

    <Steps>
      <Step>
        Run the following command to create the "Default role" for your database branch.

        ```bash  theme={null}
        pscale role reset-default <DATABASE_NAME> <BRANCH_NAME>
        ```
      </Step>

      <Step>
        Record the "Host", "Username", and "Password" for the "Default role" somewhere secure.
      </Step>
    </Steps>
  </Tab>
</Tabs>

<Note>
  Passwords are shown only once. If you lose your record of the password, you must [reset the password](/docs/postgres/connecting/roles).
</Note>

## Integrate with Prisma

Installation instructions vary depending on your choice of JavaScript runtime or framework. The following are general instructions for installing Prisma and connecting to your database.

### Step 1: Install packages and initialize

Run the following to install `prisma`:

```bash Terminal theme={null}
npm install prisma
```

Next, set up Prisma in your application with the following command:

```bash Terminal theme={null}
npx prisma init
```

### Step 2: Add credentials to .env

For local development, you can place your credentials in a `.env` file. For production, we recommend setting your credentials as environment variables wherever your application is deployed.

Replace the placeholders below with the role credentials created in the previous section.

```bash .env theme={null}
DATABASE_URL='postgresql://{username}:{password}@{host}:{port}/{database}?sslmode=verify-full'
```

Choose the appropriate **port** for your use case. Learn more about [Direct vs PgBouncer connections](/docs/postgres/connecting/quickstart#connection-types%3A-direct-vs-pgbouncer).

<Columns cols={2}>
  <Card title="PgBouncer">
    Port `6432` enables a lightweight connection pooler for PostgreSQL. This facilitates better performance when there are many simultaneous connections.
  </Card>

  <Card title="Direct">
    Port `5432` connects directly to PostgreSQL. Total connections are limited by your cluster's `max_connections` setting.
  </Card>
</Columns>

<Note>
  Both connection types will disconnect when your database restarts or handles a failover scenario.
</Note>

### Step 3: Create a database connection

By default, Prisma should create a `prisma/schema.prisma` file with the `postgresql` provider.

```prisma prisma/schema.prisma theme={null}
generator client {
  provider = "prisma-client"
  output   = "../generated/prisma"
}

datasource db {
  provider = "postgresql"
}
```

As well as a `prisma.config.ts` file which reads `DATABASE_URL` from your `.env` file.

```ts prisma.config.ts theme={null}
import 'dotenv/config'
import { defineConfig, env } from 'prisma/config'

export default defineConfig({
  schema: 'prisma/schema.prisma',
  migrations: {
    path: 'prisma/migrations',
  },
  datasource: {
    url: env('DATABASE_URL'),
  },
})
```

[Learn how to query your replicas directly](/docs/postgres/scaling/replicas)

For more information on using Prisma including creating schemas and performing queries, [refer to their documentation](https://www.prisma.io/docs/orm).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale Postgres quickstart
Source: https://planetscale.com/docs/postgres/tutorials/planetscale-postgres-quickstart

This guide will walk you through how to create a new PlanetScale Postgres database.

export const YouTubeEmbed = ({id, title}) => {
  return <Frame>
      <iframe src={`https://www.youtube-nocookie.com/embed/${id}?rel=0`} title={title} className="aspect-video w-full" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" />
    </Frame>;
};

We'll cover:

* Creating a new Postgres database
* Cluster configuration options
* Connecting to your database

## Prerequisites

Before you begin, make sure you have a [PlanetScale account](https://auth.planetscale.com/sign-up). After you create an account, you'll be prompted to create a new organization, which is essentially a container for your databases, settings, and members.

After creating your organization, it's important to understand the relationship between databases, branches, and clusters.

* **Database**: Your overall project (e.g., "my-ecommerce-app")
* **Branch**: Isolated database deployments that provide you with separate environments for development and testing, as well as restoring from backups - [learn more about branching](/docs/postgres/branching)
* **Cluster**: The underlying compute and storage infrastructure that powers each branch

PlanetScale Postgres clusters use real Postgres in a [high-availability architecture with one primary and two replicas](/docs/postgres/postgres-architecture/#Cluster-design).

## Create a new database

<Tabs>
  <Tab title="Dashboard">
    <YouTubeEmbed id="6BBrgJcpTBY" title="Create a database on PlanetScale" />

    ### Step 1: Navigate to database creation

    <Steps>
      <Step>
        Log in to your [PlanetScale dashboard](https://app.planetscale.com)
      </Step>

      <Step>
        Select your organization from the dropdown
      </Step>

      <Step>
        Click **"New database"** button or navigate to `/new`
      </Step>
    </Steps>

    ### Step 2: Choose database engine

    <Steps>
      <Step>
        On the database creation form, you'll see two engine options:

        * **Vitess** (MySQL-compatible)
        * **Postgres** (PostgreSQL-compatible)
      </Step>

      <Step>
        Select **Postgres** to create a PostgreSQL database
      </Step>
    </Steps>

    ### Step 3: Configure your database cluster

    <Steps>
      <Step>
        **Database name**: Enter a unique name for your database
      </Step>

      <Step>
        **Region**: Choose the primary region where your database will be hosted

        <Note>
          This "name" is referenced in the PlanetScale Dashboard and APIs and not created as a logical database inside of Postgres.
        </Note>
      </Step>

      <Step>
        **Cluster configuration**: Select your preferred cluster size and [CPU architecture](/docs/postgres/cluster-configuration/cpu-architectures)
      </Step>
    </Steps>

    ### Step 4: Create the database cluster

    <Steps>
      <Step>
        Review your configuration settings
      </Step>

      <Step>
        Click **"Create database"** to provision your Postgres database
      </Step>

      <Step>
        Your database will be created with a `main` branch by default
      </Step>
    </Steps>
  </Tab>

  <Tab title="CLI">
    If you are creating an automation, or are an LLM, you may prefer to create new databases using the PlanetScale CLI.

    ### Step 1: Install the CLI

    <Steps>
      <Step>
        Check to see if the PlanetScale CLI is installed already by running:

        ```bash  theme={null}
        pscale --version
        ```
      </Step>

      <Step>
        Alternatively, follow the instructions in the [PlanetScale CLI GitHub repository](https://github.com/planetscale/cli#installation)
      </Step>
    </Steps>

    ### Step 2: Log in or sign up

    <Steps>
      <Step>
        If you do not already have a PlanetScale account, [sign up for one](https://auth.planetscale.com/sign-up) by running:

        ```bash  theme={null}
        pscale signup
        ```
      </Step>

      <Step>
        Log in to the PlanetScale CLI by running:

        ```bash  theme={null}
        pscale auth login
        ```

        You’ll be taken to a screen in the browser where you’ll be asked to confirm the code displayed in your terminal. If the confirmation codes match, click the **"Confirm code"** button in your browser.

        You should receive the message "Successfully logged in" in your terminal. You can now close the confirmation page in the browser and proceed in the terminal.
      </Step>
    </Steps>

    ### Step 3: Create a database

    <Steps>
      <Step>
        Configure the CLI to use the **organization** in which you want to create the database if you have more than one. List organizations by running:

        ```bash  theme={null}
        pscale org list
        ```

        Switch organizations by running:

        ```bash  theme={null}
        pscale org switch <ORGANIZATION_NAME>
        ```
      </Step>

      <Step>
        Find the **region** closest to your application's hosting.

        List available regions by running:

        ```bash  theme={null}
        pscale region list
        ```

        <Note>
          If you do not specify a **region**, your database will automatically be deployed to `us-east` (US East — Northern Virginia).
        </Note>
      </Step>

      <Step>
        Create a new Postgres database by running:

        ```bash  theme={null}
        pscale database create <DATABASE_NAME> --engine postgres --region <REGION_SLUG>
        ```

        <Note>
          Your **database name** can contain lowercase, alphanumeric characters, or underscores. We allow dashes, but don't recommend them, as they may need to be escaped in some instances.
        </Note>
      </Step>
    </Steps>
  </Tab>
</Tabs>

## What happens during creation

When you create a Postgres database cluster, PlanetScale automatically:

* Provisions a PostgreSQL cluster in your selected region
* Creates the initial `main` branch
* Prepopulates Postgres with required default databases
* Sets up monitoring and metrics collection
* Configures backup and high availability settings

## Create credentials and connect

In this section you'll create the "Default role" in your PlanetScale dashboard to create connection credentials for your database branch.

<Note>
  The "Default role" is meant purely for administrative purposes. You can only create one and it has significant privileges for your database cluster and you should treat these credentials carefully. After completing this quickstart, it is *strongly recommended* that you [create another role](/docs/postgres/connecting/roles) for your application use-cases.
</Note>

<Tabs>
  <Tab title="Dashboard">
    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=9d2832f63cdd6203cfe7f1a55fcdbbdb" alt="Database dashboard" data-og-width="1800" width="1800" data-og-height="760" height="760" data-path="docs/postgres/tutorials/new-database.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=ba2c259b2870598d3a4cf4fa88a18724 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=09c9ae2fce51ddcc013c3b82625bfb6a 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=fbfd0843a6fbb170757655896d8cade4 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=f9046e8ac38a2f60f83a57c2a7df73d3 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=eb873e467af068b415734ddb9f11409a 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/new-database.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=5df4d215dfedb451cf51ec27900ff120 2500w" />
    </Frame>

    <Steps>
      <Step>
        Navigate to your database in the [PlanetScale dashboard](https://app.planetscale.com)
      </Step>

      <Step>
        Click on the **"Connect"** button in the top right
      </Step>

      <Step>
        Select **"Default role"**
      </Step>

      <Step>
        Click **"Create default role"**. A new default role is created for your database branch.
      </Step>

      <Step>
        Record the "Host", "Username", and "Password" for the "Default role" someplace secure.
        <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=6441862bb6d6890244eef761af99cea4" alt="Create a new role" data-og-width="2060" width="2060" data-og-height="1026" height="1026" data-path="docs/postgres/tutorials/create-role.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=4fe62ba845c393c8821f189886b498ff 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=03cb8bd911c01dd8df99d296eaf05c92 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=5880ac93f049e1c055f905f664944d26 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=9ba45ca192ab29c0b12586cf806c4621 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=1c686b956b60806404a54d93b9185cd3 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/create-role.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=05c057853bee8d3ceb5b53c0e82f0025 2500w" />
      </Step>

      <Step>
        You can generate connection strings under **"How are you connecting?"** for major languages, frameworks, and tools.

                <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=761943da846fbb9e1086b1603d002128" alt="Connection strings" data-og-width="1806" width="1806" data-og-height="984" height="984" data-path="docs/postgres/tutorials/langs-and-frames.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=6a2a3aa3ed3c45769ed09c86dbd8e82e 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=ef9f2ab2a8df1d31fd56f4b9f57cec46 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=c86a885c2f951c89c7d3524ce81054ab 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=e2eaa548d3b951b37e1fdfa4ade59ada 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=510aa78544776a2da5172b4ae8568c94 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/postgres/tutorials/langs-and-frames.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=8c352e6883f12f79fd4648a9dfd08bb5 2500w" />

        Your connection details will include:

        * **Host**: the DNS name of your database endpoint
        * **Username**: automatically formatted for routing to the correct `branch`
        * **Password**: A securely generated password
        * **Database**: `postgres` (default database)
        * **Port**: `5432` (standard PostgreSQL port) or `6432` (for using [PgBouncer](/docs/postgres/connecting/pgbouncer))
      </Step>
    </Steps>
  </Tab>

  <Tab title="CLI">
    Create a new "Default role" in your PlanetScale CLI to create connection credentials for your database branch.

    <Steps>
      <Step>
        Run the following command to create the "Default role" for your database branch.

        ```bash  theme={null}
        pscale role reset-default <DATABASE_NAME> <BRANCH_NAME>
        ```
      </Step>

      <Step>
        Record the "Host", "Username", and "Password" for the "Default role" somewhere secure.
      </Step>
    </Steps>
  </Tab>
</Tabs>

<Note>
  Passwords are shown only once. If you lose your record of the password, you must [reset the password](/docs/postgres/connecting/roles).
</Note>

### Connection strings

PlanetScale provides connection strings in various formats for different frameworks and languages. Here are some common examples:

<AccordionGroup>
  <Accordion title="General PostgreSQL URL">
    ```bash  theme={null}
    postgresql://postgres.{branch-id}:{password}@{host}.horizon.psdb.cloud:5432/postgres?sslmode=require
    ```
  </Accordion>

  <Accordion title="psql command line">
    ```bash terminal theme={null}
    psql 'host={host} port=5432 user={user} password={password} dbname=postgres sslnegotiation=direct sslmode=verify-full sslrootcert=system'
    ```
  </Accordion>

  <Accordion title="Node.js (node-postgres)">
    ```typescript db.ts theme={null}
    const { Client } = require('pg');
    const client = new Client({
      host: '{host}',
      port: 5432,
      user: '{user}',
      password: '{your-password}',
      database: 'postgres',
      ssl: { rejectUnauthorized: true }
    });
    ```
  </Accordion>

  <Accordion title="Rails">
    ```ruby  theme={null}
    planetscale:
      username: {user}
      host: {host}
      port: 5432
      database: postgres
      password: {your-password}
    ```
  </Accordion>
</AccordionGroup>

## Default databases

When your database branch is first created, there are a number of default databases that are created at the same time.

```sql  theme={null}
postgres=> \l
                    List of databases
      Name       |      Owner       | Removed columns...
------------------+------------------+-------------------
 postgres         | postgres         | ...
 pscale_admin     | pscale_admin     | ...
 pscale_exporter  | pscale_admin     | ...
 pscale_pgbouncer | pscale_admin     | ...
 template0        | pscale_admin     | ...
 template1        | pscale_superuser | ...
(6 rows)
```

These databases include those that are used by various PlanetScale platform features such as metrics and logs collection, backups, Insights, or are used by PGBouncer (as examples). They cannot be removed.

| Database                    | Purpose                                                                                          |
| --------------------------- | ------------------------------------------------------------------------------------------------ |
| postgres                    | Default user database                                                                            |
| pscale\_admin               | PlanetScale platform                                                                             |
| pscale\_exporter            | PlanetScale platform                                                                             |
| pscale\_pgbouncer           | PlanetScale platform                                                                             |
| `template0` and `template1` | [Postgres database defaults](https://www.postgresql.org/docs/current/manage-ag-templatedbs.html) |

## Security requirements

All connections to Postgres databases require:

* **SSL/TLS encryption** - Always use `sslmode=require` or equivalent
* **Certificate verification** - Connections verify PlanetScale's SSL certificates
* **Secure passwords** - Generated passwords use cryptographically secure random generation

### Password management

* **Password reset**: You can [reset your default user password](/docs/postgres/connecting/roles) anytime from the dashboard
* **No password rotation required**: Passwords don't expire unless you set them to
* **Single credential per branch**: Each branch has one default user credential

## Next steps

Once your database is created, you can:

* [Create additional branches](/docs/postgres/branching) for development and testing
* Connect your applications using the connection credentials above
* [Monitor performance and usage](/docs/postgres/monitoring/query-insights) through the dashboard
* [Scale your cluster](/docs/postgres/cluster-configuration) as your needs grow
* Create additional PostgreSQL roles within your database using SQL

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# The proof of concept phase of an enterprise contract
Source: https://planetscale.com/docs/proof-of-concept

We get many customers that come to us with existing large, production databases.

export const YouTubeEmbed = ({id, title}) => {
  return <Frame>
      <iframe src={`https://www.youtube-nocookie.com/embed/${id}?rel=0`} title={title} className="aspect-video w-full" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" />
    </Frame>;
};

## PlanetScale proof of concept

<YouTubeEmbed id="yj7nylLeyog" title="How we do PoCs with customers (Proof of Concept)" />

These databases can be many terabytes of data, serving millions or billions of queries per day. Such customers would like an opportunity to build confidence that PlanetScale will meet and exceed their expectations for database performance, reliability, and compatibility with their existing systems. To help build this confidence, we often include a **Proof of Concept** (PoC) phase as a part of our contract with enterprise customers. These are also sometimes referred to as a **Proof of Value** phases (PoV).

## What is a proof of concept?

We often sign multi-year contracts with our enterprise customers, and typically also include a Proof of Concept phase at the beginning. The PoC phase is a short window of time at the beginning of a longer contract during which our customers work with a Solutions Architect from our team to test our product and ensure it will meet their needs going forward. These can range from a few weeks to a few months depending on the details of the customers database, size, compute requirements, and testing needs.

The last day of the PoC phase is known as the opt-out date. If a customer decides that they no longer want to move forward using PlanetScale before this opt-out date, they must notify our team of this decision in a timely manner. Payment will still be required for all resource usage and services provided during the PoC, and anything else specified in the contract, but the customer will not be on the hook for the full contract amount. This gives you the opportunity to test drive PlanetScale before committing to the full contract period.

## What happens during a Proof of Concept phase?

Before signing a contract, customers typically have discussions with one of our Solutions Architects to gather the information needed to create your contract:

* Which database(s) you want to bring in to PlanetScale
* Your database storage and resource requirements
* Whether or not vertical or horizontal sharding will be needed
* The load that the database will need to be able to handle
* Other details about your database infrastructure

After signing a contract with PlanetScale, you will start working with your Solutions Architects on implementation and testing. Though the details change from customer to customer, the flow of work during the first few weeks or months of an enterprise contract is as follows:

<Steps>
  <Step>
    First, it's good to determine which database you want to start with. Long term, you may intend to move multiple databases and workloads into PlanetScale, but during the proof-of-concept phase it is often useful to focus on one specific database and workload to begin testing with.
  </Step>

  <Step>
    Based on this choice, your Solutions Architect will help you get the infrastructure set up for your database, and configured appropriately.
  </Step>

  <Step>
    If you are going to be using a sharded database, you will also work with us to determine your [sharding strategy](/docs/vitess/sharding), and we will help you decide the best [VSchema](https://vitess.io/docs/reference/features/vschema/) for your tables.
  </Step>

  <Step>
    The next step is to get your data imported. We have a [self-serve tool for importing databases](/docs/vitess/imports/database-imports) into PlanetScale, but we often provide direct assistance with this, and for larger, more complex workloads our team can handle running the appropriate Vitess commands to migrate you over.
  </Step>

  <Step>
    After this, customers typically want to start testing the capabilities of their imported database. Testing can be done in a number of ways. Some customers have testing scripts they have already developed, which can be run against the PlanetScale database. You may also want to do more specific testing with queries or workloads that you know to be problematic to see how PlanetScale handles them. You can use our powerful [Insights panel](/docs/vitess/monitoring/query-insights) as well as your own custom tooling to assess how well these workloads are handled.
  </Step>

  <Step>
    If any issues arise during the testing phase, you can work with our team to come up with solutions before going into production.
  </Step>
</Steps>

Ideally, customers have had sufficient time to test their database and build confidence in our platform before the opt-out date.

We often see customers completely cut over to PlanetScale during this phase as well — making PlanetScale their production database before the opt-out period is even over. The process has been optimized to quickly prove that PlanetScale can handle your workloads while still providing flexibility to ensure you're able to test everything you want to before the opt-out date.

## After the opt-out date

After a phase of testing has completed, our customers continue to work with our Customer Engineering team to continue onboarding. This process includes:

* Migrating other database(s) that you intend to use PlanetScale for
* Continued testing
* Performance tuning where needed
* Setting a cutover date to get your database into production

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Exploring Enterprise - the sales process
Source: https://planetscale.com/docs/sales-process



## PlanetScale sales process

In an effort to make the process as smooth and quick as possible for you, we've outlined what the typical sales process looks like so you know what to expect and can prepare accordingly.

This is the general process we aim for, but there may be some additional back and forth in between depending on your current setup. We are always happy to schedule additional calls or check-ins at any point in the process.

<Steps>
  <Step>
    **Reach out to us**: First, fill out our [contact form](https://planetscale.com/contact) to let us know what type of plan you're looking for. In most cases, filling out the [technical questionnaire and running the pre-checks script](#technical-questionnaire) during this step will help speed up the process, especially if you're looking for pricing. However, it is not mandatory.
  </Step>

  <Step>
    **Gathering information**: We respond via email to ask for some additional information about your workload and application requirements. We may ask you to fill out our [technical questionnaire and run our pre-checks script](#technical-questionnaire). We sometimes will also request slow query log results. This gives us important information about your current workload so that we can begin to prepare pricing information and other answers specific to your application.
  </Step>

  <Step>
    **Call with our Solutions team**: If our solutions engineering team determines that an Enterprise plan is the best fit for your use-case, we reach out to schedule a call with your team. During this call, we discuss timelines, possible configurations, add-on options (Support, sharding, BAAs, etc.), and more. Gathering this information allows us to put together a pricing proposal. This is also your opportunity to ask us any questions about the platform. We are happy to do a short demo here as well or schedule a follow up demo if there are specific features you're interested in.
  </Step>

  <Step>
    **Pricing proposal**: In some cases, we are able to gather sufficient information to prepare a pricing proposal after the first call. The pricing proposal is created based on what we learned in the intro call, your pre-checks script results, and the technical questionnaire answers. We can also include a proof-of-concept commitment with an opt-out period. Once the pricing is prepared, we send it to you and can schedule a call for further discussion if desired.
  </Step>

  <Step>
    **Optionally, gather more information**: Our aim is to get you pricing information as quickly as possible, but in some more complex situations, we need additional information to get a proposal put together. Our pricing is based on a number of factors, primarily: memory, compute, storage, replicas, sharding, and support packages. If you have several databases or environments to move over, or have some other complex situation, usually the fastest way to get to pricing is to share your itemized bill from your current database provider.
  </Step>

  <Step>
    **Conversations with stakeholders**: At this point, your security team or other stakeholders may have additional questions about the platform. We are happy to fill out any questionnaires you may have or get on a call with them. We can also provide additional material and documentation for anyone needing extra convincing.
  </Step>

  <Step>
    **Proof of concept**: If you have chosen to proceed with the proof of concept, we will create a shared Slack channel and work together to spin up, optimize, and benchmark your chosen test workload based on the success criteria we set together. The PoC phase typically lasts 30 to 90 days and includes an opt-out period. In most cases, the PoC fulfills the success criteria much earlier than that, and companies choose to move forward ahead of schedule. In these cases, we roll the PoC straight into the migration for the full workload.
  </Step>
</Steps>

## Technical questionnaire

The [PlanetScale Technical questionnaire](https://docs.google.com/forms/d/e/1FAIpQLSdzmtLykP8hAAqOLfAaIDy1-B5_2XFXSa1-Q5RJgX8ZvOcIvA/viewform) is a short form that asks questions about your production workload, such as:

* Architecture information for your database clusters — Total number of databases, instance sizes/types, disk/IOPs allocations, regions, application stack, and tooling.
* Performance profile — Observed traffic patterns, peak QPS, average QPS, observed resource utilization.
* Team and project — Desired outcome, timeline, security and compliance.
* Pre-checks script results — See below.

## PlanetScale pre-checks

To help us get a better sense of your production workload, please run the [PlanetScale pre-checks script](https://github.com/planetscale/ps-prechecks) against your current database and send back the tarball generated by the script. You can either upload it at the end of the technical questionnaire or email it to your Solutions rep.

The `ps-prechecks` script summarizes the status and configuration of your MySQL database server. It works by connecting to a MySQL database server and querying it for status and configuration information. It saves this information into files in a temporary directory with clean formatting.

It is not a tuning tool or diagnosis tool. It produces a report that is easy to diff and can be pasted into emails without losing the formatting. It should work well on any modern UNIX systems. The `ps-prechecks` script is a heavily modified version of [`pt-mysql-summary`](https://docs.percona.com/percona-toolkit/pt-mysql-summary.html), which is part of the [Percona Toolkit](https://docs.percona.com/percona-toolkit/index.html).

`ps-prechecks` is designed to connect to your live database environment, execute queries to collect the relevant information, and process/summarize the data without further impacting your database environment. While this script is well tested, any database tool can pose a risk to your database environment.

<Note>
  By running the PlanetScale pre-checks script on your database, you understand that there may be a risk to your database environment, and acknowledge and agree that PlanetScale shall not be held liable for anything indirectly or directly arising from or related to such script.
</Note>

### Usage

Before using this tool, please:

* Read the documentation in the [repo README](https://github.com/planetscale/ps-prechecks)
* Test the script on a non-production server first
* Back up your production server and verify the backups

To use, simply execute it. Optionally add a double dash and then the same command-line options you would use to connect to MySQL, such as the following:

```
ps-prechecks --user=root
```

You can find additional options in the [repo README](https://github.com/planetscale/ps-prechecks).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Security and compliance
Source: https://planetscale.com/docs/security

PlanetScale is committed to delivering a powerful and easy-to-use database platform while keeping your data secure.

The security of our systems is of the utmost importance. We consistently aim to improve our security posture by building security into every layer of our products.

Below is a breakdown of common security and compliance requirements by PlanetScale plan:

|                                                                                             | Scaler Pro                         | Enterprise multi-tenant            | Enterprise single-tenant           | PlanetScale Managed                |
| :------------------------------------------------------------------------------------------ | :--------------------------------- | :--------------------------------- | :--------------------------------- | :--------------------------------- |
| [Encryption of data (at rest and in transit)](#encryption-of-data)                          | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| [SOC 1 Type 2 available](#soc-1-type-2--soc-2-type-2-hipaa)                                 | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| [SOC 2 Type 2+ HIPAA available](#soc-1-type-2--soc-2-type-2-hipaa)                          | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| [IP restrictions](/docs/vitess/connecting/connection-strings#ip-restrictions) (Vitess only) | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| [Audit logs](/docs/security/audit-log)                                                      | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| [Security logs](/docs/security/security-log)                                                | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| [Data Processing Addendum available](#data-processing-addendum)                             | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| [Private database connectivity](#private-database-connectivity)                             | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| [Single sign-on (SSO)](/docs/security/sso)                                                  | Available as add-on                | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| [Business Associate Agreements available](#hipaa-and-business-associate-agreements)         | Available as add-on                | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| Dedicated AWS/GCP account                                                                   | <Icon icon="xmark" color="red" />  | <Icon icon="xmark" color="red" />  | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| [PCI compliant](#pci-compliance)                                                            | <Icon icon="xmark" color="red" />  | <Icon icon="xmark" color="red" />  | <Icon icon="xmark" color="red" />  | <Icon icon="check" color="blue" /> |
| [Your own AWS/GCP account](#deploy-in-your-own-aws-or-gcp-account)                          | <Icon icon="xmark" color="red" />  | <Icon icon="xmark" color="red" />  | <Icon icon="xmark" color="red" />  | <Icon icon="check" color="blue" /> |

## Available on all PlanetScale plans

### Private database connectivity

By default, all PlanetScale connections are encrypted and routed through the public Internet. Optionally, you can connect privately to databases through [AWS PrivateLink](/docs/vitess/connecting/private-connections) or [GCP Private Service Connect](/docs/vitess/connecting/private-connections-gcp).

### SOC 1 Type 2 & SOC 2 Type 2+ HIPAA

PlanetScale continuously monitors and reports primarily using System and Organization Controls (SOC) 1 & 2 Type 2 paired with the HIPAA Security Rule. To request access to our latest reports, please visit PlanetScale's [Trust Center](https://trust.planetscale.com/).

### Data security

#### Encryption of data

PlanetScale databases and their client communications are AES encrypted throughout the PlanetScale platform, both in transit and at rest.

##### At rest

Data is encrypted at rest on the underlying storage media that serves database branches and also the underlying storage media that hosts your PlanetScale database backups. This helps mitigate the risk of unintentional or malicious access to user data on storage systems.

##### In transit

Data in transit to PlanetScale databases is encrypted and goes through three major paths:

* The [PlanetScale CLI](/docs/cli), leverages [TLS](https://en.wikipedia.org/wiki/Transport_Layer_Security) when initiating a connection to PlanetScale's API and Edge.
* PlanetScale [connection strings](/docs/vitess/connecting/connection-strings) require the successful establishment of a TLS session before any SQL commands can be issued.
* [TLS](https://en.wikipedia.org/wiki/Transport_Layer_Security) is used to secure all data transmitted between PlanetScale and [clients using PlanetScale Connect](/docs/vitess/etl).

#### Additional data protection controls

Communications to the PlanetScale API and Dashboard are encrypted using TLS 1.3. Certificates are issued by established third-party certificate authorities.

### General Data Protection Regulation (GDPR)

PlanetScale offers database services in Amazon Web Services and Google Cloud Platform regions around the world. PlanetScale complies with the EU General Data Protection Regulation (GDPR) and other global privacy regulations, where applicable. Customers are responsible for their applications' compliance with regulatory requirements, including as they relate to data subjects of their systems.

#### Data Processing Addendum

All PlanetScale plans are covered by our [Data Processing Addendum (DPA)](https://planetscale.com/legal/data-processing-addendum). Markups are accepted for addendums on all PlanetScale Enterprise plans. [Contact us](https://planetscale.com/contact) to talk more about PlanetScale Enterprise plans and changes to our DPA.

#### Data locality

The infrastructure supporting user databases, backups, etc., is in the provider (AWS or GCP) and region where the database is created. Any read-only replicas in other geographies will copy the data set to the selected regions.

The following are two examples of data locality in PlanetScale:

* If you create a database in a US-based region, all data, including customer data, is stored and processed in the US, except in cases where sub-processors are identified as having other locations.
* If you create a database in a Europe-based region, your data does not leave the region the database was created in, unless you create a read-only region in another region.

## Available on all Enterprise plans

### HIPAA and Business Associate Agreements

PlanetScale can enter into Business Associate Agreements (BAAs) with customers who purchase [Business support](/docs/support), an [Enterprise plan](/docs/planetscale-plans#planetscale-enterprise-plan), or qualify for our startup pricing. Please [reach out for more information](https://planetscale.com/contact), and we'll be in touch shortly.

The customer must determine whether they are a Covered Entity — or a Business Associate of a Covered Entity — as defined under HIPAA. If so, the customer may require a BAA with PlanetScale for the purposes of our relationship.

Responsibility around HIPAA compliance between PlanetScale and the customer is implemented using a shared responsibility model. While PlanetScale Enterprise plans provide a secure and compliant infrastructure for the storage and processing of Protected Health Information (PHI), the customer is ultimately responsible for ensuring that the environment and applications that they build on top of PlanetScale are properly configured and secured according to HIPAA requirements.

The Department of Health and Human Services does not recognize any formal certification for HIPAA. PlanetScale systems, software, networks, and procedures are consistent with the controls outlined in the relevant rules.

### Additional audit logging features

In addition to the [audit log](/docs/security/audit-log) feature available to all PlanetScale plans, Enterprise plans can use our EventBridge configuration to send logs to your AWS account. Ask your PlanetScale account manager for more information on how to set it up.

<Note>
  If you have any questions or concerns related to the security and compliance of any PlanetScale Enterprise plans, please [contact us](https://planetscale.com/contact), and we will be happy to discuss them further.
</Note>

## Available on Enterprise single-tenant plans

PlanetScale offers two single-tenant deployment options: Single-tenant and PlanetScale Managed for organizations that require a single-tenant environment. See the [section below](/#available-on-planetscale-managed) for more information on PlanetScale Managed-only security and compliance features.

<Note>
  [Contact us](https://planetscale.com/contact) if you are interested in exploring PlanetScale single-tenant deployment options for your organization.
</Note>

### Dedicated AWS Organizations member account or GCP organization

PlanetScale single-tenant deployment options are deployed into a dedicated AWS Organizations member account or GCP organization owned by PlanetScale. If you want PlanetScale to deploy into your own AWS Organizations member account or GCP organization, owned by your organization, see PlanetScale Managed below.

## Available on PlanetScale Managed

PlanetScale Managed is a single-tenant deployment of PlanetScale within your Amazon Web Services (AWS) or Google Cloud Platform (GCP) account. In this configuration, you can use the same API, CLI, and web interface that PlanetScale offers, with the benefit of running entirely in your own AWS Organizations member account or GCP organization. You can learn more on the [PlanetScale Managed overview page](/docs/vitess/managed).

### Deploy in your own AWS or GCP account

PlanetScale Managed is a packaged [data plane](https://en.wikipedia.org/wiki/Data_plane) that's deployed to an AWS Organizations member account or GCP project that you own and we operate. Your database lives entirely inside a dedicated member account or project within your cloud organization. PlanetScale will not have access to other member accounts or projects nor your organization-level settings within the cloud service provider.

Read more on how PlanetScale Managed works inside either cloud provider:

* [PlanetScale Managed on Amazon Web Services](/docs/vitess/managed/aws)
* [PlanetScale Managed on Google Cloud Platform](/docs/vitess/managed/gcp)

### PCI compliance

PlanetScale Managed, when deployed with the appropriate controls enabled via our Shared Responsibility Matrix, has been issued an Attestation of Compliance (AoC) and Report on Compliance (RoC), certifying our compliance with the PCI DSS 4.0 as a Level 1 Service Provider. This enables PlanetScale Managed to be used via a shared responsibility model across merchants, acquirers, issuers, and other roles in storing and processing cardholder data.

### Human access flow

PlanetScale Managed supports customer counter-approval for access to Managed environment(s) by PlanetScale employees via an integration in the web application. Talk to your Customer Engineer about enabling this feature.

### Other PlanetScale Managed security features

PlanetScale Managed on AWS also supports:

* [Fully private network isolation](/docs/vitess/managed/aws#fully-private-network-isolation)
* [Third-account customer-controlled public key infrastructure (PKI)](/docs/vitess/managed/aws#third-account-customer-controlled-public-key-infrastructure)

## Corporate security

### Background checks

Background checks are performed on new team members during onboarding (within 30 days of their start date) as permitted by local law.

### Security training

All team members complete security awareness training covering company security policies and procedures during onboarding (within 30 days of their hire date). Trainings are required annually after that for all employees.

The training material is designed to assist the employee in identifying and responding to social engineering and other cybersecurity risks they may encounter as part of their role at PlanetScale.

## Security operations

### Endpoints

All company-provided devices are managed with [Kandji](https://kandji.io/). Device configuration is based on the Center for Internet Security (CIS) level 1 benchmark and is continuously enforced. Mobile device management deploys and enables relevant services to ensure corporate endpoints are appropriately monitored.

### Extended Detection and Response (XDR)

PlanetScale administrative endpoints, such as devices used by employees, are monitored via endpoint detection and response systems.

### Detection and response

PlanetScale employs measures for both corporate endpoints and cloud instances to collect, analyze, and store events, connections, and other potentially relevant metadata in real-time. Automated systems match events against internal and known bad patterns and intelligence streams.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Access control
Source: https://planetscale.com/docs/security/access-control



## Organization access control

When you set up your PlanetScale account, you're asked to create an **Organization**.

An organization is essentially a container for your databases, settings, and members. You can create multiple organizations in the same account for different applications or use cases.

Within each organization, you can add members and assign them different roles. This document covers the different roles, the ways you can assign roles, permissions associated with those roles.

## Roles and permissions

We currently support three different roles in your organization:

* `Organization Administrator`
* `Organization Member`
* `Database Administrator`

### Organization Administrator

An `Organization Administrator` can perform all actions in an organization, as well as all actions on *every* database within that organization.

### Organization Member

An `Organization Member` can only perform limited actions within an organization and on all databases in that organization. By default, all users added to an organization have this role.

### Database Administrator

A `Database Administrator` can perform all actions on the database for which they were assigned the `Databases Administrator` role.

This role is assigned at the **database level** and gives elevated permissions for the particular database that an organization member is the `Database Administrator` of. If you want to [grant a member *full* access to manage one or several databases](#assign-roles-at-a-database-level) but not full `Organization Administrator` access, then this is the role you want. Please note, a user that is granted this role must be a member of the organization of which the database exists in, so they will have the permissions associated with `Organization Member` as well.

## Organization-level permissions

Each role has a set of permissions assigned to it, which determines what actions that role is allowed to take within an organization or database.

The following table describes permissions assigned at the organization level for `Organization Administrators` and `Organization Members`. Because `Database Administrators` don't have any organization-level permissions, they are not included in this table.

| Action                                 | Description                                                                    | Member                             | Administrator                      |
| -------------------------------------- | ------------------------------------------------------------------------------ | ---------------------------------- | ---------------------------------- |
| View branches                          | View a database branch                                                         | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| Create branches                        | Create a database branch                                                       | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| Delete non-production branches         | Delete a non-production database branch                                        | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| View databases                         | View one or all databases                                                      | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| Create databases                       | Create a new database                                                          | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| Create deploy requests                 | Create a deploy request for a branch                                           | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| Manage service tokens                  | Create, view, or delete service tokens                                         | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| Manage service token grants            | Create, view, update, or delete service token grants                           | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| View organization members              | View one or all organization members                                           | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| View database members                  | View one or all database members                                               | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| View organization                      | View an organization                                                           | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| View query statistics                  | View query statistics for an organization's databases                          | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| Connect to development branches        | Create passwords or use pscale shell for development branches                  | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| Connect to production branches         | Create passwords or use pscale shell for production branches                   |                                    | <Icon icon="check" color="blue" /> |
| Delete production branches             | Delete a production database branch                                            |                                    | <Icon icon="check" color="blue" /> |
| Promote branches                       | Promote a branch to production                                                 |                                    | <Icon icon="check" color="blue" /> |
| Modify VSchema (Vitess only)           | Edit the VSchema of a keyspace                                                 |                                    | <Icon icon="check" color="blue" /> |
| Manage databases                       | Delete, update settings, or import a database                                  |                                    | <Icon icon="check" color="blue" /> |
| Manage beta features                   | Opt-in or opt-out of a beta feature                                            |                                    | <Icon icon="check" color="blue" /> |
| Create production service token grants | Create a service token grant to connect or delete a production database branch |                                    | <Icon icon="check" color="blue" /> |
| Update an integration                  | Update a third-party integration                                               |                                    | <Icon icon="check" color="blue" /> |
| Manage invitations                     | View, create, or cancel organization invitations                               |                                    | <Icon icon="check" color="blue" /> |
| Manage invoices                        | View or download organization invoices                                         |                                    | <Icon icon="check" color="blue" /> |
| Manage billing                         | View or update billing plans and payment methods                               |                                    | <Icon icon="check" color="blue" /> |
| View audit logs                        | View all audit logs                                                            |                                    | <Icon icon="check" color="blue" /> |
| Manage organization members            | Update member roles or delete organization members                             |                                    | <Icon icon="check" color="blue" /> |
| Manage database members                | Update member roles, add, or remove database members                           |                                    | <Icon icon="check" color="blue" /> |
| Manage organization                    | Update organization settings, SSO, or delete organization                      |                                    | <Icon icon="check" color="blue" /> |

## Database-level permissions

The following table describes the permissions assigned at the **database level** for `Organization Administrators`, `Organization Members`, and `Database Administrators`.

For `Organization Administrators` and `Organization Members`, these permissions apply to every database in the organization. Because the `Database Administrator` role is assigned at the database level, the permissions are for the specific database(s) for which they have the `Database Administrator` role.

| Action                          | Description                                                   | Member                             | Administrator                      |
| ------------------------------- | ------------------------------------------------------------- | ---------------------------------- | ---------------------------------- |
| Create and view branches        | Create or view a database branch                              | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| Delete non-production branches  | Delete a non-production branch of a specific database         | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| View database                   | View a database in an organization                            | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| Create deploy requests          | Create a deploy request for a branch on a specific database   | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| View database members           | View one or all database members                              | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| View query statistics           | View query statistics for an organization's databases         | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| Restore non-production backups  | Restore the backup of a development branch                    | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| Connect to development branches | Create passwords or use pscale shell for development branches | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| Connect to production branches  | Create passwords or use pscale shell for production branches  |                                    | <Icon icon="check" color="blue" /> |
| Manage billing                  | Update the billing plan of a specific database                |                                    | <Icon icon="check" color="blue" /> |
| Delete production branches      | Delete a production database branch of a specific database    |                                    | <Icon icon="check" color="blue" /> |
| Promote branches                | Promote a branch of a specific database to production         |                                    | <Icon icon="check" color="blue" /> |
| Manage database                 | Delete, update settings, or import a database                 |                                    | <Icon icon="check" color="blue" /> |
| Manage beta features            | Opt-in or opt-out of a beta feature for a database            |                                    | <Icon icon="check" color="blue" /> |
| Manage database members         | Update database member roles, add, or remove database members |                                    | <Icon icon="check" color="blue" /> |
| Restore production backups      | Restore the backup of a production branch                     |                                    | <Icon icon="check" color="blue" /> |

An organization may have several databases, and an `Organization Member` may have different access to each database depending on whether or not they also have the `Database Administrator` role.

## Assign organization roles to members

You can follow the steps below to assign roles to your members. You must be an Organization Administrator to modify member roles.

* In the [PlanetScale dashboard](https://app.planetscale.com), click on the Settings tab in the top navigation.
* Click on "Members" in the sidebar on the left.
* From here, you can click on the dropdown on the right under the "Role" column to select the role you want to apply to each member.

You can also invite new members to your organization and assign roles once they accept their invitation. New members will be added with the [`Organization Member`](#organization-member) role by default.

<Note>
  Member role management is issued at the organization level. Each organization in your account may have different
  members with different access levels.
</Note>

## Assign roles at a database level

There are two ways to assign database-level roles to Organization members:

1. Individually using the `Database Administrator` role.
2. Creating a Team, adding member(s), and adding database(s) to that team.

### Individually assign the `Database Administrator` role

To assign a member the role of `Database Administrator`, follow the steps outlined below. You must be an Organization Administrator or an existing Database Administrator to manage the `Database Administrator` role.

<Note>
  Members that create a database are automatically assigned the role of `Database Administrator` for that database.
</Note>

<Steps>
  <Step>
    In the [PlanetScale dashboard](https://app.planetscale.com), click on the name of the database you want to add a Database Administrator to.
  </Step>

  <Step>
    Click on the "**Settings**" tab in the top navigation.
  </Step>

  <Step>
    Click on "**Administrators**" in the sidebar on the left.
  </Step>

  <Step>
    To add an administrator, click on the "**Add administrator**" button and select the member you wish to add as a Database Administrator.
  </Step>

  <Step>
    From here, you can also remove a Database Administrator by clicking the "**Remove**" button next to their name.
  </Step>
</Steps>

### Add Database Administrator role via Teams

If you wish to give several members the Database Administrator role, you may want to [create a Team](/docs/security/teams#create-and-manage-teams). This will allow you to manage the access to that database all in one place.

For instructions, see our [Teams documentation](/docs/security/teams).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Account password security
Source: https://planetscale.com/docs/security/account-password-security

In addition to best practices like [multi-factor authentication](/docs/security/multi-factor-authentication), PlanetScale securely stores your account passwords and validates passwords against known security breaches.

## Password storage

PlanetScale uses [Argon2](https://en.wikipedia.org/wiki/Argon2) as the password hashing function. We use the `Argon2id` variant, which provides protection against side channel attacks and GPU-based cracking attacks.

A password hashing function is a one-way function which means that your password cannot be reversed or decrypted from the stored value in the database.

## Leaked passwords

PlanetScale checks passwords when a user sets them during signup or when changing the password. The first check is that the password needs to have enough entropy. Entropy is a measure for the amount of randomness a password contains. Read more about how we use entropy for [user-friendly strong passwords in the PlanetScale blog](https://planetscale.com/blog/using-entropy-for-user-friendly-strong-passwords).

PlanetScale also checks the password against [Have I been pwned](https://haveibeenpwned.com). *Have I been pwned* is a large database containing passwords seen in security breaches.

PlanetScale does **not** send the password you enter to *Have I been pwned*. The *Have I been pwned* API provides anonymity through \[the Cloudflare k-anonymity implementation]\(https:([https://planetscale.com/blog.cloudflare.com/validating-leaked-passwords-with-k-anonymity/](https://planetscale.com/blog.cloudflare.com/validating-leaked-passwords-with-k-anonymity/)). This ensures that no other provider can identify the password that you have entered.

The password is also not associated in any way with the email address you use to sign up. This information is not shared with *Have I been pwned*, nor is this information needed for the leaked passwords API they provide.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Audit log
Source: https://planetscale.com/docs/security/audit-log

The organization audit log grants [Organization Administrators](/docs/security/access-control#organization-administrator) access to review **actions** performed by individual members of the organization.

In addition, each audit log includes **events** detailing who performed the **action** and when it happened.

Audit log retainment period is [based on your plan](/docs/planetscale-plans):

* **Scaler Pro** — 15 days
* **Enterprise** — 15 days

<Note>
  Organization audit log access is limited to [Organization Administrators](/docs/security/access-control#organization-administrator).
</Note>

## Review your organization audit log

You can review your organization audit log under [your PlanetScale **organization** settings](https://app.planetscale.com/~/settings/audit-log).

Once there, you can filter the audit log by **Actor** and/or **Action**.

Clicking and expanding individual log **event** names empowers you to investigate additional metadata that can provide broader context around what changed.

## Audited organization events

You can track the following organization **events** in your PlanetScale account:

| PlanetScale organization events      | Actions                                                                                                                                                                                                                                            | Database Engine  |
| :----------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------- |
| access\_token                        | created  deleted  token\_leaked                                                                                                                                                                                                                    | Vitess, Postgres |
| backup\_policy                       | created  deleted                                                                                                                                                                                                                                   | Vitess           |
| branch\_maintenance\_schedule        | created  deleted  started\_maintenance\_window  finished\_maintenance\_window                                                                                                                                                                      | Vitess           |
| branch\_maintenance\_window          | created  deleted                                                                                                                                                                                                                                   | Vitess           |
| database                             | created  deleted  requested\_deletion  updated\_default\_branch  updated\_migration\_table\_name  removed\_member  added\_member                                                                                                                   | Vitess, Postgres |
| database\_branch                     | created  deleted  enabled\_safe\_migrations  disabled\_safe\_migrations  enabled\_foreign\_keys  disabled\_foreign\_keys  enabled\_vectors  updated\_cluster\_size  updated\_cluster\_config  updated\_vtgates  updated\_cluster\_update\_strategy | Vitess, Postgres |
| database\_branch\_keyspace           | created  deleted  updated  requested\_deletion                                                                                                                                                                                                     | Vitess           |
| database\_branch\_password           | created  deleted  password\_leaked  updated\_ip\_restrictions                                                                                                                                                                                      | Vitess, Postgres |
| database\_branch\_read\_only\_region | created  deleted                                                                                                                                                                                                                                   | Vitess           |
| database\_deploy\_request            | created  deleted  closed                                                                                                                                                                                                                           | Vitess           |
| database\_webhook                    | created  deleted                                                                                                                                                                                                                                   | Vitess, Postgres |
| deploy\_request\_review              | approved                                                                                                                                                                                                                                           | Vitess           |
| deployment                           | unqueued                                                                                                                                                                                                                                           | Vitess           |
| external\_datasource                 | created  deleted                                                                                                                                                                                                                                   | Vitess, Postgres |
| integration                          | created  deleted                                                                                                                                                                                                                                   | Vitess, Postgres |
| organization                         | created  deleted  joined  removed\_member  left  added\_member  enabled\_sso  disabled\_sso  enabled\_sso\_directory  disabled\_sso\_directory                                                                                                     | Vitess, Postgres |
| organization\_invitation             | created  deleted                                                                                                                                                                                                                                   | Vitess, Postgres |
| organization\_membership             | created  updated\_role                                                                                                                                                                                                                             | Vitess, Postgres |
| organization\_team                   | created  deleted  added\_member  left  removed\_member                                                                                                                                                                                             | Vitess, Postgres |
| postgres\_role                       | created  deleted                                                                                                                                                                                                                                   | PostgreSQL       |
| regional\_failover\_event            | created  deleted                                                                                                                                                                                                                                   | Vitess, Postgres |
| service\_token                       | created  deleted  updated\_bulk\_database\_access  token\_leaked                                                                                                                                                                                   | Vitess, Postgres |
| user                                 | created  deleted  signed\_in                                                                                                                                                                                                                       | Vitess, Postgres |

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Authentication methods
Source: https://planetscale.com/docs/security/authentication-methods

There are three ways to authenticate with PlanetScale: _email address and password_, _single sign-on_, and _OAuth via GitHub_.

## Overview

Let's break down how each of these work.

## Email address and password

This is the only authentication mechanism where PlanetScale maintains user credentials.

Additionally, users can opt to configure [two-factor authentication (2FA)](/docs/security/multi-factor-authentication). This option requires **something you know** *(i.e. your password)* and **something you have** *(i.e. recovery codes)*.

## Single sign-on

Users can authenticate with their chosen corporate identity provider *(i.e. Okta)* instead of maintaining passwords with PlanetScale.

Once [SSO](/docs/security/sso) is enabled for an `organization`, all members are redirected through that identity provider's authentication flow. Moving forward, they must pass through SSO to access their PlanetScale account.

## OAuth via GitHub

Users can authenticate with PlanetScale using their GitHub account.

<Warning>
  PlanetScale doesn't maintain the passwords for these accounts. Losing access to your GitHub account prevents accessing
  your PlanetScale account.
</Warning>

<Note>
  Enabling SSO removes OAuth access for all members of your *organization*, meaning they will no longer be able
  to sign in with their GitHub credentials.
</Note>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Multi-factor authentication
Source: https://planetscale.com/docs/security/multi-factor-authentication

Multi-factor authentication (MFA) provides better safety for your databases and prevents unauthorized access to your user account. 

## Overview

MFA strengthens security by requiring two or more methods *(i.e. authentication factors)* to verify your identity.

PlanetScale allows users logging in with an email address and password to set MFA as a requirement for logging into the user account.

<Note>
  If you're authenticating via GitHub OAuth or [SSO](/docs/security/sso), MFA settings will not be displayed.
</Note>

### Authentication providers

PlanetScale supports login with a unique *time-based one-time password (TOTP)* that is generated for your user account by using TOTP apps such as [1Password](https://docs/support.1password.com/one-time-passwords/), [Authy](https://docs/support.authy.com/hc/en-us/articles/360006303934-Add-a-New-Two-Factor-Authentication-2FA-Account-Token-in-the-Authy-App), or [LastPass Authenticator](https://lastpass.com/auth/).

## Enable MFA

You can enable MFA for your user account under your PlanetScale account settings.

<Steps>
  <Step>
    Go to your [PlanetScale account settings](https://app.planetscale.com/account) page.
  </Step>

  <Step>
    Find the **Security** row and click the **"Setup multi-factor authentication"** button.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/setup.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=f4edfa84b8b413a6ad57cbbc2352e442" alt="Click the &#x22;Setup MFA&#x22; button priority" data-og-width="3074" width="3074" data-og-height="282" height="282" data-path="docs/images/assets/docs/concepts/mfa/setup.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/setup.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=f8dd0dce93d91a0ddc5d5211b23a6b14 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/setup.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=10d6543b46f9d54594519bd44688caf2 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/setup.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=a4a3a4ae917c5d1ab911d1f860fb4d75 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/setup.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=464f1d2b2eb4e6edc94ca6babcd57359 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/setup.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=aaf4e6dc3f1ca854c2648fa430b47747 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/setup.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=e03f2f6cc49fad6c807e0cabb8d90df5 2500w" />
    </Frame>

    This will bring up a pop-up modal with a *QR code* and some `recovery codes` that you will need to copy.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/recovery-codes.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=7dab6f372e3f6c38543cc73f9ede99ac" alt="Pop-up modal with QR code and recovery codes priority" data-og-width="976" width="976" data-og-height="1522" height="1522" data-path="docs/images/assets/docs/concepts/mfa/recovery-codes.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/recovery-codes.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=0ba128fe36918b9681b71ea2fcf23717 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/recovery-codes.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=38f8bcf831d613b7c6a081b8f3bf8d5e 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/recovery-codes.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=6f6d1a3555c3c60ebfd2ffed6e365d27 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/recovery-codes.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=1c93b3f6df89b0cfd4d34109a9ca16f3 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/recovery-codes.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=a1742746f6f220a8adb74fc8d1c84b77 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/recovery-codes.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=95e3236dd79a42f9ca74647d50d963fe 2500w" />
    </Frame>

    <Warning>
      Recovery codes are the only account recovery method accepted when MFA is enabled. If you lose both your TOTP app and the recovery codes, there is no way to regain access to your account.
    </Warning>
  </Step>

  <Step>
    Scan the QR Code with your preferred TOTP app and enter the generated code.
  </Step>

  <Step>
    Press **"Validate OTP"** to ensure that your application setup is correct.
  </Step>

  <Step>
    Once the generated code is validated, click the **Copy** button in the `recovery codes` section.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/copy.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=5a66009cff34f2e90b448b9e0e04204f" alt="Copy the recovery codes" data-og-width="1018" width="1018" data-og-height="196" height="196" data-path="docs/images/assets/docs/concepts/mfa/copy.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/copy.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=4ee38296e61784a81d25ad47b8f5b959 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/copy.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=f0d2b813d2a88bdaf19e5efbc3e39cbc 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/copy.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=e4e0b3a945dede504f3217446c90294f 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/copy.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=4bdea3b0f242a09dbacba99904890ea0 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/copy.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=5678149ae5d243a0f81ec5bbbf84c362 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/copy.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=034d11f659a3aa6e177db1be6e2c1378 2500w" />
    </Frame>

    <Tip>
      Recovery codes are only visible during the MFA setup process. Make sure the recovery codes are copied and saved some place secure before continuing.
    </Tip>
  </Step>

  <Step>
    Click **Done** to close the pop-up modal.
  </Step>
</Steps>

## Login with two-factor authentication

Once you've enabled MFA in your PlanetScale user account, the next time you login, you'll be prompted to enter your two-factor authentication (2FA) code.

* Use the **OTP code** generated by your preferred TOTP app to login to your PlanetScale account.

## Recovery code login

The `recovery codes` shown during MFA setup are **the only way regain access to your account** in the event that you lose the device that generates your authentication codes. PlanetScale will not accept any other method of authentication or identification.

You can use one of the `recovery codes` in the place of a TOTP token on the second screen during login.

## Disable MFA

<Warning>
  We strongly recommend that you do not disable MFA to avoid unauthorized access to your user account.
</Warning>

<Note>
  * Any devices setup with the QR code for your account will no longer be able to produce valid OTP tokens.
  * Any recovery codes that were generated when MFA was enabled will no longer be valid.
</Note>

You can disable MFA for your user account under your PlanetScale account settings.

<Steps>
  <Step>
    Go to your [PlanetScale account settings](https://app.planetscale.com/account) page.
  </Step>

  <Step>
    Click the **Disable** button next to *"Multi-factor authentication enabled"* in the **Security** row.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/disable.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=579199129da39c54b73b13fb9d24ce3c" alt="Click the &#x22;Disable&#x22; button" data-og-width="2418" width="2418" data-og-height="214" height="214" data-path="docs/images/assets/docs/concepts/mfa/disable.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/disable.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=6a71282290304e3da7031f7a1e04d62f 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/disable.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=f91420f8cf1f1562a35a9e5686c399a2 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/disable.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=10737e36df84611ca58c7fe12c726368 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/disable.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=d15e6b8127d7ec19b16fc546e4f662fa 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/disable.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=fdd2ede71e1702c85f75658e252f929b 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/disable.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=44bfb00f7531df7e5a13e4b2b4219d9a 2500w" />
    </Frame>
  </Step>

  <Step>
    Enter an **OTP code** or one of the `recovery codes` generated by your preferred TOTP app to confirm.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/modal.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=9e8817684df875bf178afdac4c2b2ffa" alt="Disable MFA pop-up modal" data-og-width="980" width="980" data-og-height="814" height="814" data-path="docs/images/assets/docs/concepts/mfa/modal.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/modal.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=047a79f38de3d486bf361d231b9e0a5d 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/modal.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=d51085b118ee67036de16e3bb24774c2 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/modal.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=d827299f586334ecfc7a66cbebc11b1d 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/modal.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=c8b6072cd204b741ddb695cf54aa3d1e 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/modal.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=a3c885718acc44a8854a1bd772f72b38 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/mfa/modal.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=b12f0f625a8a5167374bb2e45685f485 2500w" />
    </Frame>
  </Step>

  <Step>
    Click the **Disable** button to close the pop-up modal.
  </Step>
</Steps>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Security log
Source: https://planetscale.com/docs/security/security-log

The Security log gives you insight into recent authentication and other security-related `actions` for your account. Your security log also details when you performed each `action`.

## Overview

Security log events are retained for 6 months in your PlanetScale account.

## Review your security log

You can review your security log in your PlanetScale account settings.

1. Go to your [PlanetScale Security log](https://app.planetscale.com/settings/security-log) page.

Once there, you can filter your Security log by `Action`.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/security-log/filter.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=d33d639dcec6b936c87080b7706239fd" alt="Filter your security log by Action." data-og-width="812" width="812" data-og-height="808" height="808" data-path="docs/images/assets/docs/concepts/security-log/filter.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/security-log/filter.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=8655cca1f6953bd2cdd477d864ef18af 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/security-log/filter.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=afe211633b21d74289cf1f6f6d09a569 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/security-log/filter.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=4029265d83398e6c901854d877f43551 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/security-log/filter.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=eb7d4be6c97ba4100bf1fac96013a95c 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/security-log/filter.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=e0fe1c310ffd9ab0f00282c57b7a242f 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/security-log/filter.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=0e333a5ca4bb2007d5ae64053b6a56f6 2500w" />
</Frame>

## Security log events

You can track the following `events` in your PlanetScale account:

| PlanetScale security log events | Actions                                                                                            |
| :------------------------------ | :------------------------------------------------------------------------------------------------- |
| user                            | signed\_out  signed\_in  accepted\_tos  updated\_name  updated\_email  enabled\_mfa  disabled\_mfa |

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Single sign-on
Source: https://planetscale.com/docs/security/sso

Single sign-on (SSO) provides additional account security by allowing company administrators to require the use of an identity provider when logging into PlanetScale.

## Overview

Users only need to sign in once with a single set of credentials *(i.e. password and email)* to access all of their tools and applications upon joining the company.

Furthermore, SSO allows an administrator to revoke someone’s access to all tools and applications from a single place when they leave a team or the company. PlanetScale uses SAML SSO.

<Note>
  SSO is available as an *add-on* for Scaler Pro plan at \$199/month per organization and included in our [Enterprise plans](/docs/planetscale-plans#planetscale-enterprise-plan). Security is important to us, so we do not profit off of SSO. We only charge enough to cover the WorkOS cost for enrolling a new account.
</Note>

## Implications

It's important to understand how enabling SSO will affect your Organization. Once enabled, the following will happen:

* All non-admin members will be removed from the organization.
* [Organization Administrators](/docs/security/access-control#organization-administrator) will remain in the Organization so they can configure SSO without losing access.
  * All administrators will retain access with their old credentials, until they logout and login through their Identity Provider. Once they've authenticated through their Identity Provider the account will only be usable with SSO authentication.
* Each [Organization Member](https://app.planetscale.com/planetscale/settings/members) must re-authenticate using SSO. Once they've authenticated, they will be automatically added back to the organization.
* Organization Member invites will be disabled when SSO is enabled; all Organization Membership will be managed through SSO.
* Organization Members that were [Database Administrators](/docs/security/access-control#database-administrator) before will no longer have that role upon rejoining. You must assign them the role after they re-authenticate with SSO.
* Organization Members that were on [Teams](/docs/security/teams) will need to be re-added.
* Any database credentials and tokens that were generated by non-admin members will remain active, so you **do not need to regenerate connection credentials**.
* Organization Members removed from your SSO, will still appear in the Organization until they are manually removed.
  * While they are visible in the Organization they will not be able to authenticate to PlanetScale.
  * If you enabled [Directory Sync](/docs/security/#directory-sync), the Member will be removed from the Organization without manual intervention.

<Note>
  If you enable SSO and Directory Sync, the Directory will remain the source of truth, and Teams will map accordingly. Please see the [Directory Sync](/docs/security/#directory-sync) section for more information.
</Note>

## Enable SSO for your organization

To enable SSO for your organization, you must be an [Organization Administrator](/docs/security/access-control). Organization administrators can *enable*, *configure*, and *disable* SSO for all members of your organization.

First, head to your [PlanetScale organization's authentication page](https://app.planetscale.com/~/settings/authentication) under Settings -> Authentication. Type in the email domains that you would like to allow to sign in through SSO, and click "Enable single sign-on".

You can configure your SSO settings by clicking the "identity provider" link on that SSO settings page. This will take you to WorkOS where you can choose your identity provider. You can find documentation for your specific identity provider in the [WorkOS integrations documentation](https://workos.com/docs/integrations).

You also have the option to manage PlanetScale roles through your identity provider's SSO profile. Just check the box next to that message on the settings page to enable.

## Multiple organizations with SSO

You can enable SSO for multiple organizations using the same identity provider. However, users across all your organizations must have unique internal IDs within your identity provider.

For example, if your identity provider has a user with:

* Email: `user@example.com`
* Internal ID: `some_user_id`

You cannot create another user with:

* Email: `user+staging@example.com`
* Internal ID: `some_user_id` (same as above)

This restriction exists because both email addresses would be treated as the same user due to the identical internal ID, which could cause authentication conflicts across organizations.

After enabling SSO for a different organization, you will be able to join the new organization by logging out and using the Single Sign On form to log into the new organization for the first time.

## Disable SSO

When SSO is disabled for an organization, users can log in with the password they initially set for their PlanetScale account. If they don't know their password, users can go through the password reset flow to regain access to their account.

Should a user lose access to the email address associated with that organization, they'll also lose access to their account after SSO is disabled.

## Directory Sync

We also support the use of Directory Sync with SSO. You can use Directory Sync to make the directory the source of truth for organization membership.

### Enable Directory Sync

To enable Directory Sync, you first must have SSO enabled.

Once enabled, go to your Organization settings page, click "**Authentication**", and click the "**Enable directory sync**" button.

You can now configure Directory Sync using your identity provider.

### Directory Sync access control

Directory Sync automatically adds and removes members from your PlanetScale organization to match your SSO directory. If you have groups defined within your SSO provider, it can also automatically create [Teams](/docs/security/teams) within your PlanetScale organization mapped to those groups.

If you wish to have your identity provider determine user roles in PlanetScale, please make sure to select the option for `Manage PlanetScale roles through identity provider` in Settings > Authentication.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/sso/sync.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=e5692ad5b294f96fd1104f85bc6ec4f0" alt="Manage roles" data-og-width="1698" width="1698" data-og-height="372" height="372" data-path="docs/images/assets/docs/concepts/sso/sync.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/sso/sync.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=62212f4a6788cb68ab3967144eeb3589 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/sso/sync.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=96640d0f45789ea088dda7d83faa9424 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/sso/sync.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=d66e90cb0d0b7e97aa966f43692ca849 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/sso/sync.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=5e0f63b77bc24f8a2960bcfefbfc3825 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/sso/sync.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=53fd9a76b5309080974134c2d1fa3852 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/sso/sync.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=3343c9060d6d8289cd4ed4c7f77e8fbb 2500w" />
</Frame>

<Note>
  Once you enable Directory Sync, existing Teams will be cleared, as all Teams must map to a Directory group.
</Note>

Once you enable Directory Sync, existing Teams will be cleared, as all Teams must map to a Directory group.

You can find the directory-managed members under "**Settings**" > "**Members**", and directory-managed Teams under "**Settings**" > "**Teams**".

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/sso/managed.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=ea6e8a6fedd17bed9a398d4c4d20e626" alt="Dashboard UI - Directory-managed Teams page" data-og-width="1800" width="1800" data-og-height="1392" height="1392" data-path="docs/images/assets/docs/concepts/sso/managed.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/sso/managed.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=fdfc6d5163dd5b23a8d6971a86f20e16 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/sso/managed.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=d8ad85a927fcfd4d0d5d14179714a681 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/sso/managed.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=2fa39492d377c6acce22cb0bfaca0543 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/sso/managed.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=25d11d732a3c830f28c921fc73cf7182 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/sso/managed.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=0a8170a05664fddda91741021d1a4fd1 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/sso/managed.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=ec15613c1c96e61e5787255432b6baee 2500w" />
</Frame>

## Using Okta to manage organization admins

There are two different ways to configure Okta to manage the user's role in a PlanetScale organization. The first approach uses SAML assertions during Single Sign On to update the role just-in-time. The second approach uses an attribute defined on the directory user to update the role in real time as it changes. It is up to user to decide which approach works best for their organization.

### Managing roles during single sign-on (SAML)

In order to update roles using SAML, you will first need to configure the SAML application and user profile in Okta before enabling the setting within PlanetScale.

#### Configuring the SAML Okta application

<Steps>
  <Step>
    Click the **Applications** > **Applications** in the Okta dashboard's sidebar.
  </Step>

  <Step>
    Find your PlanetScale SAML application that you configured earlier for WorkOS and find the **SAML Settings** section. Click on the **Edit** link in this section.
  </Step>

  <Step>
    In the **Configure SAML** step, find the **Attribute Statements (optional)** section, which defines the attributes of email, first and last name, and id.
  </Step>

  <Step>
    Add a new Attribute Statement with the following values.

    * Name — `planetscale_role`
    * Name Format — `Unspecified`
    * Value — `user.planetscale_role`
  </Step>
</Steps>

#### Creating user profile attributes

In order to define the custom attribute in Okta, you must update the default User profile to have the `planetscale_role` attribute.

<Steps>
  <Step>
    Click **Directory** > **Profile Editor** in the Okta dashboard's sidebar.
  </Step>

  <Step>
    Click the **User (default)** profile within Okta
  </Step>

  <Step>
    Click **Add attribute** and fill in the following values

    * Data type: `string`
    * Display name: `PlanetScale Role`
    * Variable name: `planetscale_role`
    * Enum: Enable the **Define enumerated list of values** checkbox and add these attributes:
      * Display name: `Member`, Value: `member`
      * Display name: `Administrator`, Value: `administrator`
  </Step>

  <Step>
    Click **Save attribute**
  </Step>
</Steps>

#### Enable in PlanetScale

Lastly, we will have to enable role management via SSO within the PlanetScale application.

<Steps>
  <Step>
    Go to your SSO organization's settings page
  </Step>

  <Step>
    Click on **Authentication** in the side navigation
  </Step>

  <Step>
    Check the **Manage PlanetScale roles through identity provider's SSO profile** checkbox
  </Step>
</Steps>

After following all of these steps, after you change the PlanetScale role for a user in Okta, their role will be updated accordingly the next time they log in via Single Sign On. If the role is changed during an active session, they will need to log out and log back in for it to be updated once again.

### Managing roles using Directory Sync (SCIM)

In order to manage roles using Directory Sync, you must properly configure the SCIM user's profile within Okta and update the Directory Sync custom attributes before enabling the setting within the PlanetScale dashboard.

#### Configuring the SCIM user profile

<Steps>
  <Step>
    Click **Directory** > **Profile Editor** in the Okta dashboard's sidebar.
  </Step>

  <Step>
    Click the user profile that maps to your SCIM application, it should be in the format of **\< SCIM application name > User**
  </Step>

  <Step>
    Click **Add attribute** and fill in the following values

    * Data type: `string`
    * Display name: `PlanetScale Role`
    * Variable name: `planetscale_role`
    * External name: `planetscale_role`
    * The External namespace should be defined as `urn:ietf:params:scim:schemas:core:2.0:User`.
    * Check the box to define an enumerated list of values, add the following:
      * Display name: `member`, Value: `member`
      * Display name: `administrator`, Value: `admin`
  </Step>

  <Step>
    Click **Save attribute**
  </Step>
</Steps>

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/sso/planetscale_role.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=330d20aa2c39a54e704af671aa573155" alt="PlanetScale Role" data-og-width="1678" width="1678" data-og-height="1536" height="1536" data-path="docs/images/assets/docs/concepts/sso/planetscale_role.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/sso/planetscale_role.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=75233fbe45e1c778db8ad7026a0c9242 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/sso/planetscale_role.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=6adf3487a2b89a13ca8297c406c50bc6 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/sso/planetscale_role.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=fb0c7c14481ca4a4ac355035d4a3a65f 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/sso/planetscale_role.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=d841b2a2e4a9bcce4f017df4ab97f797 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/sso/planetscale_role.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=1d58a2757d47b6e266198369c6faaa29 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/sso/planetscale_role.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=0255c222c229300bb13c8bfbed9a02c4 2500w" />
</Frame>

This attribute then needs to be mapped to the PlanetScale application in Okta.

<Steps>
  <Step>
    Open the PlanetScale SCIM application in Okta Admin Console, then select Provisioning
  </Step>

  <Step>
    Scroll down to the bottom of the `Okta to App` provisioning page, and click `Show unmapped attributes`.
  </Step>

  <Step>
    Find `planetscale_role` and click the 🖋️ to map the attribute.
  </Step>

  <Step>
    Select `Map from Okta Profile` as the type and `planetscale_role` as the string.
  </Step>

  <Step>
    Save with Create & Update permission.
  </Step>
</Steps>

#### Configuring custom attributes within WorkOS

Next, you will need to update your WorkOS configuration to detect the custom attribute mappings.

<Steps>
  <Step>
    Go to the **Authentication** within your organization settings page.
  </Step>

  <Step>
    In the **Directory sync** section, click the `identity provider` link to open up the directory configuration page
  </Step>

  <Step>
    Click the **Edit Attribute Map** button and in the form, enter the folllowing:

    * Directory Provider Value: `planetscale_role`
    * PlanetScale Attribute: `planetscale_role`
  </Step>

  <Step>
    Click **Save mappings**
  </Step>
</Steps>

#### Enable in PlanetScale

Lastly, we will have to enable role management via SSO within the PlanetScale application.

<Steps>
  <Step>
    Go to your SSO organization's settings page
  </Step>

  <Step>
    Click on **Authentication** in the side navigation
  </Step>

  <Step>
    Check the **Manage PlanetScale roles through identity provider's directory** checkbox
  </Step>
</Steps>

After following all of these steps, after you change the PlanetScale role for a user in Okta, their role will be updated in PlanetScale without needing to re-authenticate.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Teams
Source: https://planetscale.com/docs/security/teams

PlanetScale allows you to create teams within organizations.

## Overview

This allows you to easily manage administrator access to one or multiple databases all in one spot.

## Create and manage Teams

You can manage teams straight from your PlanetScale dashboard by going to "**Settings**" > "**Teams**".

<Note>
  Only [Organization Administrators](/docs/security/access-control#organization-administrator) can create and
  manage Teams.
</Note>

Once you add databases to a team, any members on that team will have [Database Administrator access](/docs/security/access-control#database-level-permissions) to those databases. Review our [Access control documentation](/docs/security/access-control) to understand the full scope of Database Administrator access.

### Create a team

<Steps>
  <Step>
    On your PlanetScale overview page, click "**Settings**".
  </Step>

  <Step>
    Click "**Teams**" in the left nav.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/teams/create.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=5a76414f2aaa68cf4715cd5bfde59d88" alt="Dashboard UI - Create a PlanetScale team" data-og-width="1800" width="1800" data-og-height="1236" height="1236" data-path="docs/images/assets/docs/concepts/teams/create.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/teams/create.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=03f51e5dadd8b2fd8985435950a6ce2f 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/teams/create.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=5189e60c0b7b56c72b21ad36e5f545f8 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/teams/create.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=6120f683423e37f6188ddfc06d394a9c 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/teams/create.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=9eb11a5459d6b649d8a22e8f7a7f4c4a 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/teams/create.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=f5b8589a78376edba51ce13d45d40862 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/teams/create.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=5c6a49210763325fcc966ff81eddee27 2500w" />
    </Frame>
  </Step>

  <Step>
    Give your team a name and description (*optional*).
  </Step>

  <Step>
    Click "**Create team**".
  </Step>
</Steps>

### Add members

<Steps>
  <Step>
    Click "**Add a member**".
  </Step>

  <Step>
    You'll see a list of your Organization members. Select the member(s) one at a time that you wish to add to the team.
  </Step>
</Steps>

### Add databases

<Steps>
  <Step>
    Click "**Add databases**".
  </Step>

  <Step>
    Select the databases you want this team to have [database administrator access](/docs/security/access-control#database-level-permissions) to.
  </Step>
</Steps>

Now, when you go to the Settings page for any databases you've added to a team, you'll also be able to view and revoke access straight from the database Administrators page.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/teams/settings.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=386d47dd32e1e710e3c97a098c9f8de1" alt="Dashboard UI - Database Administrators settings page" data-og-width="1800" width="1800" data-og-height="825" height="825" data-path="docs/images/assets/docs/concepts/teams/settings.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/teams/settings.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=3dd1aad781d127fd05018f9426d69b21 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/teams/settings.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=9957c4bbab41541e5fe2dffb34d78332 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/teams/settings.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=dfa30d507dcf4f53b46879659a007cc3 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/teams/settings.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=614c92326df3e30cb8b25d6b00e1b947 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/teams/settings.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=8b3f836736aa319dc7e538deb8b22801 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/teams/settings.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=846fc8debf5472cb9cfa27fbdd6175b3 2500w" />
</Frame>

### Remove members and databases

To remove a member from a team, find their name in the member list and click "**Remove**". At this time, you'll also be able to delete any passwords this member has created to ensure you've completely revoked their access to the database.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/teams/member.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=d9546a59b294ee6c754aad0deefc33f6" alt="Dashboard UI - Delete a member from a team" data-og-width="1800" width="1800" data-og-height="1146" height="1146" data-path="docs/images/assets/docs/concepts/teams/member.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/teams/member.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=cda2a415580d69bed26e018e42f253aa 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/teams/member.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=c9ca89ca00ee4b54fd98e600a99acd0a 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/teams/member.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=142f1c81ec6b89aacd2e969bcfbb99be 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/teams/member.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=98b118baaf2c61143f3a44ca21653d62 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/teams/member.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=3e221d1a81bb1f5ae3fa939a441c3cba 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/teams/member.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=2a5fdea6d2df8e493809b411c7557311 2500w" />
</Frame>

To remove a database from a team, click the "**x**" next to the database name under "Administrator permissions". This will remove database administrator access for all members of the team.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/teams/database.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=c5489e95e5970efb19945c91c8b759c8" alt="Dashboard UI - Delete a database from a team" data-og-width="1800" width="1800" data-og-height="1146" height="1146" data-path="docs/images/assets/docs/concepts/teams/database.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/teams/database.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=38a9a0619e6375dc539e2e6480861e8b 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/teams/database.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=8d4f326783d6d6371f6830da1e3eb3a4 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/teams/database.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=83352e1ec4589a307d6fdd8abfce857d 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/teams/database.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=3ebb0fa3b48ebad8b749f1ddd6d0a544 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/teams/database.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=6ff0b8688837a0ca97b7568a57db87ca 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/teams/database.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=64a31a7fdbbdcbc476ae3927302b33e4 2500w" />
</Frame>

## Directory Sync with Teams

If you have [SSO with Directory Sync](/docs/security/sso#directory-sync) enabled, all Teams will be managed by your Directory Sync directory. You can add and remove database access to teams, but member management must be done through your directory.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/sso/managed.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=ea6e8a6fedd17bed9a398d4c4d20e626" alt="Dashboard UI - Directory-managed Teams page" data-og-width="1800" width="1800" data-og-height="1392" height="1392" data-path="docs/images/assets/docs/concepts/sso/managed.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/sso/managed.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=fdfc6d5163dd5b23a8d6971a86f20e16 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/sso/managed.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=d8ad85a927fcfd4d0d5d14179714a681 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/sso/managed.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=2fa39492d377c6acce22cb0bfaca0543 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/sso/managed.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=25d11d732a3c830f28c921fc73cf7182 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/sso/managed.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=0a8170a05664fddda91741021d1a4fd1 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/sso/managed.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=ec15613c1c96e61e5787255432b6baee 2500w" />
</Frame>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Vulnerability disclosure
Source: https://planetscale.com/docs/security/vulnerability-disclosure



## In scope

PlanetScale is actively seeking vulnerability reports for the following components that make up the product and its Production Environment:

* **Dashboard and API**: The website hosted at app.planetscale.com, along with the API hosted at api.planetscale.com
* **Database Operations**: The actions taken within the product to create, branch, backup, and restore databases
* **Database Connectivity and Behavior**: The process of provisioning a password and issuing SQL statements against a PlanetScale database
* **Command-line Interface**: The open source command-line interface hosted at [planetscale/docs/cli](https://github.com/planetscale/docs/cli)

## Out of scope

PlanetScale is not actively seeking the following types of reports:

* **Testing software output**: Output generated from automated testing software like [Burp Suite](https://portswigger.net/burp). These include, but aren't limited to:
  * CSRF on forms that are available to anonymous users or are related to logging out
  * Disclosure of known public files or directories (i.e. `robots.txt`)
  * DNSSEC or other DNS configuration suggestions
  * TLS and security header configuration suggestions
  * Sender Policy Framework (SPF) configuration suggestions
  * Flags on cookies that are not sensitive
* **Software version reports**: Reports notifying PlanetScale that newer versions of software have been released

## Reporting a vulnerability

If you believe you have discovered a security vulnerability in a PlanetScale product or its Production Environment, please let us know immediately.
You can submit your vulnerability findings to [security@planetscale.com](mailto:security@planetscale.com).

If applicable, please include the following pieces of information in your report:

* Steps to reproduce the vulnerability
* The word "mochi" to acknowledge that you have read these guidelines
* Any relevant software (including versions) used to identify the vulnerability

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale Support
Source: https://planetscale.com/docs/support

PlanetScale’s team has a combined 600 years of experience running databases at scale.

Our support goes well beyond what you might have experienced with other providers.
We partner closely with our customers whether they are launching a AAA game or planning brand new architecture.
Even with the Standard tier, you can expect timely, and thorough responses from engineers.
We also have an active [Discord server](https://discord.com/invite/pDUGAAFEJx) where you can chat with staff as well as a [GitHub Discussion board](https://github.com/planetscale/discussion/discussions).
If you’re looking into Vitess or already running it on your own, the security of knowing you have a direct line to the core maintainers and co-creators of Vitess is invaluable.

PlanetScale believes all our users, from those running small databases to those running large, mission-critical workloads, deserve to get the help they need.

We are committed to providing the following support options.

## Standard

* Included for Scaler Pro plans
* Access to support engineers via [email](mailto:support@planetscale.com) and [web ticketing](https://support.planetscale.com/hc/en-us)
* Access to the [PlanetScale GitHub Discussion](https://github.com/planetscale/discussion/discussions) board
* Ability to [submit support tickets](https://support.planetscale.com/hc/en-us)
* Target response time of 2 business days

## Business

* Available as an upgrade for Scaler Pro plans and included on Enterprise plans
* Access to the [PlanetScale GitHub Discussion](https://github.com/planetscale/discussion/discussions) board
* Access to support engineers via email and [web ticketing](https://support.planetscale.com/hc/en-us)
* Enhanced response times including Urgent and High Priority coverage
* [Business Response Times](#initial-response-times)

## Enterprise

* Available upgrade for Enterprise Plan customers
* Enhanced email and [web ticketing](https://support.planetscale.com/hc/en-us) support
* Additional options for technical account management, Slack-based support, and phone escalation
* Mission-critical response times including continuous support coverage
* [Enterprise Response Times](#initial-response-times)

<Note>
  [Contact our sales team](https://planetscale.com/contact) for Business and Enterprise support pricing.
</Note>

## Initial response times

<Note>
  Tickets must be submitted through web ticketing to be eligible for response times.

  12x5 means from 6am to 6pm Pacific Standard Time (UTC -8), Monday through Friday, excluding public holidays in the United States.

  Standard support includes a target first response time of 2 business days.
</Note>

| Support plan                  | P1 (Urgent)       | P2 (High)      | P3 (Normal)           | P4 (Low)               |
| :---------------------------- | :---------------- | :------------- | :-------------------- | :--------------------- |
| Standard (Scaler Pro)         | 12x5              | 12x5           | 12x5                  | 12x5                   |
| Business (Optional Upgrade)   | 1 hour (24x7)     | 4 hours (24x7) | 1 business day (12x5) | 2 business days (12x5) |
| Enterprise (Optional Upgrade) | 15 minutes (24x7) | 1 hour (24x7)  | 4 hours (12x5)        | 12 hours (12x5)        |

## Ticket priorities

P1 (Urgent) and P2 (High) priority tickets are reserved exclusively for production database issues, excepting confirmed security incidents. Development, staging, or non-production environments should use P3 (Normal) or P4 (Low) priority levels.

<Note>
  PlanetScale Support has the ability to update a ticket's priority if it does not generally match these guidelines.
</Note>

| Priority        | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| :-------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **P4 (Low)**    | One or more of:<br /><br /> - General platform and workflow questions<br /><br />- Feature requests<br /><br />- General product feedback                                                                                                                                                                                                                                                                                                                                                                           |
| **P3 (Normal)** | One or more of:<br /><br />- Partial, non-critical loss of functionality of the PlanetScale platform. Users are able to continue using core platform functionality.<br /><br />- A non-production application running against the PlanetScale platform is impaired or down.<br /><br />- There is a partial, non-critical loss of use of the platform with a medium-to-low business impact. Short-term workaround is available.                                                                                     |
| **P2 (High)**   | One or more of:<br /><br />- A production application is experiencing performance issues due to the PlanetScale platform.<br /><br />- Intermittent issues and reduced quality of service for significant features.<br /><br />- Operations can continue in a restricted fashion, although long-term productivity might be adversely affected.<br /><br />- A temporary workaround is available for an issue that was previously a P1 (Urgent).                                                                     |
| **P1 (Urgent)** | One or more of:<br /><br />- Production applications are down due to the PlanetScale platform and no workaround is immediately available.<br /><br />- A complete loss of ability to serve queries, commit transactions, or a significant feature impacting production workflows is completely unavailable.<br /><br />- All or a substantial portion of your data on PlanetScale production branches is at significant risk of loss or corruption.<br /><br />- A confirmed security incident has been identified. |

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale for Vitess
Source: https://planetscale.com/docs/vitess



[Vitess](https://vitess.io) is a battle-hardened open source technology invented at YouTube for deploying, scaling, and managing large clusters of database instances. It now powers some of the largest sites on the Internet.

PlanetScale for Vitess brings you fully-managed Vitess clusters with unlimited scalability through horizontal sharding.

<CardGroup>
  <Card title="Vitess quickstart" href="/docs/vitess/tutorials/planetscale-quick-start-guide" icon="rocket-launch">
    Deploy a Vitess database and learn the basics of using PlanetScale.
  </Card>

  <Card title="Architecture" href="/docs/vitess/architecture" icon="database">
    Learn about Vitess architecture.
  </Card>

  <Card title="Branching" href="/docs/vitess/imports/database-imports" icon="code-branch">
    Schema changes with branching for safe database development.
  </Card>

  <Card title="Import a database" href="/docs/vitess/imports/database-imports" icon="file-import">
    Import your existing MySQL database to PlanetScale.
  </Card>
</CardGroup>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale Vitess architecture
Source: https://planetscale.com/docs/vitess/architecture

PlanetScale's Vitess product is designed for reliability, scalability, and developer productivity.

## Overview

We achieve these goals through a combination of [MySQL](/docs/vitess/terminology#mysql), [Vitess](/docs/vitess/terminology#vitess), and our own application and ecosystem we have built atop these open-source technologies.
There is a great deal of infrastructure that enables our databases to be highly-available, secure, and resilient.
In this article, you'll learn about what powers PlanetScale databases and how you can view your database's configuration on our app.

## The infrastructure diagram

After creating a PlanetScale account and joining at least one organization, you can create a database.
Each new database has a single default [keyspace](/docs/vitess/terminology#keyspace) — a logical database — with the same name as the database.
On the dashboard of every PlanetScale database is a diagram outlining the infrastructure that powers the database.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-darkmode.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=8ff461953de326678c550456a3ca4ace" alt="Architecture diagram for a PlanetScale database" className="hidden dark:block" data-og-width="2752" width="2752" data-og-height="1830" height="1830" data-path="docs/images/architecture/diagram-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-darkmode.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=a2376fe44d17be55bfc4f355680c4739 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-darkmode.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=37980db67a4c23f90fa24e872654dff3 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-darkmode.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=f6e8a9cdebcd60ec614a4ddc4fda288a 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-darkmode.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=e8002abeb1d3beb08a8feb6d2c689a69 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-darkmode.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=1ac4fc381ed9c8be5be52ff431749058 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-darkmode.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=984db971b4593965516b587f438f130f 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=6e9cddee148d4715b51b49c5e768d48e" alt="Architecture diagram for a PlanetScale database" className="block dark:hidden" data-og-width="2752" width="2752" data-og-height="1830" height="1830" data-path="docs/images/architecture/diagram.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=daa7f290d50579e377f1b8a02224ba2b 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=d55648cfda6ddd4da518e462c67d06d9 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=52602bec4a0af285dfc33639ac1858fd 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=28691706e7fc2f9c5ba9aa5318b4ac3c 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=6bd6e447437ee1bacc3c9c7749a88eb0 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=5509812fb8cfae7ee8761830b0e6c2b8 2500w" />
</Frame>

By default, the architecture diagram will show the architecture for the keyspace corresponding to your default branch.
Here's how you can tell what keyspace and branch you are viewing the diagram of:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-labeled-darkmode.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=c221e690867e4e5549921166eb4da63b" alt="Architecture diagram for a PlanetScale database" className="hidden dark:block" data-og-width="2752" width="2752" data-og-height="1830" height="1830" data-path="docs/images/architecture/diagram-labeled-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-labeled-darkmode.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=6a1c6a4a791756b5d7b11ea472a42f05 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-labeled-darkmode.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=7ba53127a8be7b1a3c220677cb03ce5e 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-labeled-darkmode.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=2acf394224d00c4198a26368270ce09a 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-labeled-darkmode.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=cd00f860d17bd567236e75d87942d9f4 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-labeled-darkmode.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=46d4d841a7896f22048e32f25bcf313d 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-labeled-darkmode.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=4844e4a83423f1224ad0ee8f99547d40 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-labeled.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=e0dc87424518edca1be088c095546075" alt="Architecture diagram for a PlanetScale database" className="block dark:hidden" data-og-width="2752" width="2752" data-og-height="1830" height="1830" data-path="docs/images/architecture/diagram-labeled.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-labeled.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=2f18d5afb6aa30eb6626600609632df0 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-labeled.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=ae1ec32b445cbdf2879bee459a59ae65 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-labeled.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=efd116d37e1cd6e56101cc71f75b846e 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-labeled.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=e29af0a0b60055ad6b25c1b1233ad4af 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-labeled.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=c55d06df26eb9e4ae5b708d4683273e2 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-labeled.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=0988504efebd2271aa7df4146de25d95 2500w" />
</Frame>

### Production branches

Production branches are designed for production workloads, and as such are given enough resources to ensure high availability.
By default, every production branch has a single primary MySQL instance and two replicas.
Each primary also comes with 3 [VTGates](/docs/vitess/terminology#vtgate) across 3 availability zones, which act as proxies for your MySQL instances.
These are all pictured in the diagram for a production branch:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-production-branch-darkmode.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=248e2000bda9548ee2fff5c56492d0e7" alt="Production branch architecture" className="hidden dark:block" data-og-width="2752" width="2752" data-og-height="1830" height="1830" data-path="docs/images/architecture/diagram-production-branch-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-production-branch-darkmode.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=3c8c4505eb2d16cf7ed36d2a8cedb647 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-production-branch-darkmode.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=47c8663d518f1e22875eab9204425c77 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-production-branch-darkmode.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=f11fc8850d2429a01bf00e88b8e54101 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-production-branch-darkmode.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=bcb53e4c06f1b21a59933bcf01816a9a 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-production-branch-darkmode.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=68ce599df8ff85e52da7f7a321c1eebc 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-production-branch-darkmode.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=e4823ed8807cf86cbf1c6c1f41f5aab5 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-production-branch.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=6985e505d93f50dc2760686c3e839b09" alt="Production branch architecture" className="block dark:hidden" data-og-width="2752" width="2752" data-og-height="1830" height="1830" data-path="docs/images/architecture/diagram-production-branch.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-production-branch.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=c4b1a6a0261b50c999469358a6502be4 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-production-branch.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=76d90e0b388471e2f0d39765de03f98a 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-production-branch.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=55342d49ffefd27d7306ce4bb7792e13 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-production-branch.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=adde57a391c9ad3f2fbf2e35337cc98a 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-production-branch.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=36a11e21c206ab67f34eb76069592f18 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-production-branch.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=6845332a300c9b94a9f899c3d4aef061 2500w" />
</Frame>

Generally, the application connecting to this database need not be aware of these various components.
One exception to this is if you are specifically trying to [send queries to a replica](/docs/vitess/scaling/replicas#how-to-query-replicas).

### Development branches

Development branches are specced to enable the development and testing of new features and are not designed for production workloads.
When a new development branch is created, a single MySQL node is created along with a VTGate that handles connections to that node.
This is reflected in the diagram of a development branch.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-dev-branch-darkmode.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=78d2f37bc38ec95cdd035564724d88a1" alt="Development branch architecture" className="hidden dark:block" data-og-width="2752" width="2752" data-og-height="1580" height="1580" data-path="docs/images/architecture/diagram-dev-branch-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-dev-branch-darkmode.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=94782ad3469a5e40b76d3b47a8ac0636 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-dev-branch-darkmode.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=a5bb52bf82947f5cee84ae644aa8144e 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-dev-branch-darkmode.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=6d239901e10ebc925ec674218510f68f 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-dev-branch-darkmode.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=c8ad1da53f792173b447b2e79893d540 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-dev-branch-darkmode.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=5924c6c36d20c97a1659f7d2eb04c9bc 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-dev-branch-darkmode.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=ee1da429a6448e40bf6fb8a6c53a3afc 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-dev-branch.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=3f5b2f5dd63a4442c07aa6bd840960a4" alt="Development branch architecture" className="block dark:hidden" data-og-width="2752" width="2752" data-og-height="1580" height="1580" data-path="docs/images/architecture/diagram-dev-branch.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-dev-branch.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=73e58b9e4b4efe1b2fc33fe6e3e1c083 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-dev-branch.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=d9be9b275637f246f3eb46eef131d211 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-dev-branch.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=3f6e9407202d864d27f76fa9da41511f 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-dev-branch.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=ff8ae7d504b40fe76cf25d224849c633 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-dev-branch.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=b166da089c303fdc830ee8b181de388c 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-dev-branch.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=feb90895258113309cd69c995293c8ce 2500w" />
</Frame>

When you promote a development branch to production status, PlanetScale automatically adds additional replicas and VTGates deployed across multiple availability zones in a given region.

### Read-only regions

The primary of your database is the only node that can accept writes, and it resides in a single region.
You can add [read-only regions](/docs/vitess/scaling/read-only-regions) to a branch which adds replicas in another region and can be used to serve read traffic.
This can help reduce read latency for application servers that are distributed around the world.

Below, you can see our database has the primary and two replicas in `us-east-2` with read-only replicas added in both `us-west-2` and `eu-central-1`.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/read-only-regions-darkmode.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=946fd6c98c9f5e7a91627324988c9aaf" alt="Production branch with read-only regions architecture" className="hidden dark:block" data-og-width="2752" width="2752" data-og-height="2146" height="2146" data-path="docs/images/architecture/read-only-regions-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/read-only-regions-darkmode.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=3727ec758b17f33e26c4d22e538f00c1 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/read-only-regions-darkmode.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=683fc5b7b985c68e67ca99db4aac6c21 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/read-only-regions-darkmode.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=fa80def7463b85f52b7d303e0e22e776 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/read-only-regions-darkmode.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=419aedf356c386da9c0f879443da956a 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/read-only-regions-darkmode.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=4c96cfee3e55aaefe8d0ce9d0a993578 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/read-only-regions-darkmode.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=2155b87d1213a9fdc1000f8cb18ed92d 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/read-only-regions.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=0f9b16928bd3303ac9220794332ff5eb" alt="Production branch with read-only regions architecture" className="block dark:hidden" data-og-width="2752" width="2752" data-og-height="2146" height="2146" data-path="docs/images/architecture/read-only-regions.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/read-only-regions.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=b39b991025ebbbec0f8d0ac3d1c86afe 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/read-only-regions.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=6222d63c6e3b754214e4b15f1aab5992 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/read-only-regions.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=ebcb6b58574f9c33a44e91526a387f66 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/read-only-regions.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=b3e6118fa32a7cc77704b649b6fec114 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/read-only-regions.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=a1b10f9fb73a3a1eeba98341f2aec1c2 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/read-only-regions.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=0fd138b584af4e844634f1bf34a8e96b 2500w" />
</Frame>

The read-only replicas can be identified by the blue globe icon.

## Infrastructure metrics

Each element within the infrastructure diagram for PlanetScale database branches can be selected to display additional metrics related to that element.
These metrics are displayed in expandable cards that present themselves when an element is selected.
By default, the cards display metrics from the last 6 hours but can be adjusted if additional data is needed.

### VTGates

The VTGate node displays the total number of VTGates that exist for a given branch, as well as the number of availability zones in which they live.
Selecting the VTGates node will show the following metrics:

* Number of connections.
* Latency.
* Queries received.
* CPU.
* Memory consumption.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/vtgate-metrics-darkmode.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=fe6fcb89a10cdfc68de6a0b571751302" alt="VTGate metrics" className="hidden dark:block" data-og-width="1430" width="1430" data-og-height="1585" height="1585" data-path="docs/images/architecture/vtgate-metrics-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/vtgate-metrics-darkmode.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=8120f1584dac7cb2518e678a8725a557 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/vtgate-metrics-darkmode.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=0e844c912059cd4589fffffbf87c377f 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/vtgate-metrics-darkmode.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=eb6feccdb678e410dbb3dbca19e65cd5 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/vtgate-metrics-darkmode.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=ca51c4776dabd9a2da69090c8d44eb65 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/vtgate-metrics-darkmode.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=8bb7e27617f246ef10c80d3b6ee1f6d0 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/vtgate-metrics-darkmode.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=324bfe0aea03824d194c0e6695de436e 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/vtgate-metrics.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=84aef5fca619f4398c027528216b3767" alt="VTGate metrics" className="block dark:hidden" data-og-width="1430" width="1430" data-og-height="1590" height="1590" data-path="docs/images/architecture/vtgate-metrics.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/vtgate-metrics.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=c469e1f7a36296a6d8ae4e43d3cd1b1d 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/vtgate-metrics.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=92e8d7ffc6f64a0f93d1a9413efc65e5 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/vtgate-metrics.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=9219bc3233481a7b2f6aeb640a23fe6a 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/vtgate-metrics.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=04ff265c7a8668b99ce1015b8e216c7a 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/vtgate-metrics.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=77e041ace6145863f3cef0fc991ad4e2 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/vtgate-metrics.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=5380742e50b49a78c7140b90b76a444a 2500w" />
</Frame>

### MySQL nodes

Each MySQL node in the diagram will display whether it is the primary node or a replica, along with the region where that node is deployed to.
Clicking any of the MySQL nodes will display the following metrics:

* Database reads and writes for that node.
* Queries served.
* IOPS.
* CPU and Memory utilization.
* Storage utilization over the past week.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/vttablet-metrics-darkmode.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=89fa57f12f3ff28a6754050bb27cb7bc" alt="Primary MySQL node metrics" className="hidden dark:block" data-og-width="1430" width="1430" data-og-height="1584" height="1584" data-path="docs/images/architecture/vttablet-metrics-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/vttablet-metrics-darkmode.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=85e417d989b45795369a25af59b1afc2 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/vttablet-metrics-darkmode.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=105e1e2ba94d9a58cd3f29400b9a8270 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/vttablet-metrics-darkmode.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=c51d0f3b3eece3c1d16ee6f19648b965 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/vttablet-metrics-darkmode.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=4040abac1dec0c4eb4aeedbcb82fb500 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/vttablet-metrics-darkmode.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=07638261f61ade5d88b9d505d837333a 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/vttablet-metrics-darkmode.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=79c59cde7e264d6233c892b8b74b36f9 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/vttablet-metrics.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=2807ccfa5a46923d5a71dc4bf509142e" alt="Primary MySQL node metrics" className="block dark:hidden" data-og-width="1430" width="1430" data-og-height="1591" height="1591" data-path="docs/images/architecture/vttablet-metrics.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/vttablet-metrics.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=d93fab022f7c565aa00ebe75076b9038 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/vttablet-metrics.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=2eed96d1be7f062ab6d7545e4f07799a 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/vttablet-metrics.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=ee8c13deca7ef9190308e9248de885ad 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/vttablet-metrics.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=dabad7915c37567f8ad6499b99b2bda4 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/vttablet-metrics.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=c1e63af7f97f553fd00acd17c55ac5e2 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/vttablet-metrics.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=c7ffb7b1c3a84a2d0fa8fb270548b2d6 2500w" />
</Frame>

Selecting a replica will display the replication lag in addition to the other metrics.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/replication-lag-diagram-darkmode.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=18e5076155cb9373c3653537f8d44cd7" alt="Replication lag diagram" className="hidden dark:block" data-og-width="1698" width="1698" data-og-height="570" height="570" data-path="docs/images/architecture/replication-lag-diagram-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/replication-lag-diagram-darkmode.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=aa15bc99c068110594486e9d22ad3ac7 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/replication-lag-diagram-darkmode.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=f665c98cc3adba9e06df0641806adac9 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/replication-lag-diagram-darkmode.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=b3d17abe27f2fcec00ab5b494a15c828 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/replication-lag-diagram-darkmode.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=369035dcf3832e328695d3e4c34a627f 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/replication-lag-diagram-darkmode.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=6d66bbbfe328fed44fb8723d8e052b4e 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/replication-lag-diagram-darkmode.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=b6293d132f6d3323dad60065ec92162d 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/replication-lag-diagram.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=cfca029fc14d0776e69f45e5e09c530e" alt="Replication lag diagram" className="block dark:hidden" data-og-width="1698" width="1698" data-og-height="570" height="570" data-path="docs/images/architecture/replication-lag-diagram.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/replication-lag-diagram.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=67e3d43e9cf5f68cb4782839fe66480b 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/replication-lag-diagram.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=5856afb2604fa8aa3ca7b9453ffdefdb 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/replication-lag-diagram.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=96e337cc6d755d49d021d5008e01e709 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/replication-lag-diagram.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=8fc0ef4f54b9d7548bf2812bcffd22cf 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/replication-lag-diagram.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=3e1895d144e4c0b743568652da34a6ed 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/replication-lag-diagram.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=e4068f5d6888eb651002147dd7572f8d 2500w" />
</Frame>

### Replication lag at a glance

Within the infrastructure diagram, you'll also notice that there is a number near the connection points for each replica.
These numbers are a way to read the replication lag between the Primary node and that given node at a glance.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/replication-lag-darkmode.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=53b4da3de7945142a1a0e9763e6109d5" alt="Replication lag" className="hidden dark:block" data-og-width="2936" width="2936" data-og-height="1614" height="1614" data-path="docs/images/architecture/replication-lag-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/replication-lag-darkmode.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=fdafa63fc9fe14a0a7af26ef6bff2e6d 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/replication-lag-darkmode.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=868e9876b6436fbda649755457f3893e 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/replication-lag-darkmode.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=dfdb719f551d0a844c614d8cbb63146e 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/replication-lag-darkmode.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=2c3f7dd95ded8550f470001219f7e2ee 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/replication-lag-darkmode.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=023ef2d76b58340380b84b1aafd05a0a 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/replication-lag-darkmode.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=67c70922cf93819d3019403a9f5cb386 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/replication-lag.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=6eebc912c88aa168e8329126b3ca9391" alt="Replication lag" className="block dark:hidden" data-og-width="2934" width="2934" data-og-height="1612" height="1612" data-path="docs/images/architecture/replication-lag.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/replication-lag.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=66bfa6c04e6e51d2f08cd38a3091c914 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/replication-lag.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=7f796d463a5efc41fc83c51ee0e18f01 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/replication-lag.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=c402cb30622ae04280b35b5dde49af76 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/replication-lag.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=c43cb11be81ea3fcab7eaf1bf56e9994 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/replication-lag.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=376156e4de012975675cd67f7b134b73 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/replication-lag.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=e21228126d2d58489c3d851763794100 2500w" />
</Frame>

### Database shards

If your database is [sharded](/docs/vitess/sharding), the infrastructure diagram will represent that as a green stack of shards.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-stack-darkmode.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=49ca1c508f46e4384b8edc520be1464b" alt="Stacked shards" className="hidden dark:block" data-og-width="2936" width="2936" data-og-height="1810" height="1810" data-path="docs/images/architecture/shard-stack-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-stack-darkmode.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=45d319b7f6dd2c9e8ea912b1278a9184 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-stack-darkmode.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=ecadadd26f83f084491f2054622de353 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-stack-darkmode.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=575e6216f66a05e53a743177c58c1645 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-stack-darkmode.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=7edcdc8e75432bd5ca39307157200d6b 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-stack-darkmode.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=90230cd0a13ad3c439fe2cfd3d1b5a82 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-stack-darkmode.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=dd3aaccaec8edafc18f6c55b7a7a7f58 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-stack.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=59b797580764d61b3992da0703e7bf56" alt="Stacked shards" className="block dark:hidden" data-og-width="2936" width="2936" data-og-height="1810" height="1810" data-path="docs/images/architecture/shard-stack.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-stack.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=f1fdac6b01e18cf09fa08dc12a475a45 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-stack.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=9a16b97fa11b06946d73513b03c399c7 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-stack.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=d02a05a8f36da6d085a1fe76bc667282 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-stack.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=d66db1b2472fa7c25f9379ad0c1afa92 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-stack.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=19149a5c0c320a2a374323a3fc7af87b 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-stack.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=736f055f4d6c6e909a2e3936ca9102bb 2500w" />
</Frame>

Selecting the stack from the diagram will open a card displaying all of the shards belonging to that keyspace.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-list-darkmode.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=8ce63cf5bb54afe5e49fddc39693afc3" alt="Shard list" className="hidden dark:block" data-og-width="1850" width="1850" data-og-height="2124" height="2124" data-path="docs/images/architecture/shard-list-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-list-darkmode.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=2bcc3f44d60b5189a7c9b64e42689c31 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-list-darkmode.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=92ceb27e357fd2c90e4e0e1ebf7e99dd 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-list-darkmode.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=8c8216152c83e08e75f5a1c06d302f4c 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-list-darkmode.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=7ec646e3e53d82b6113770ca732f8c07 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-list-darkmode.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=a4e16079e0a829dd47ba2c7c7e281fa4 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-list-darkmode.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=02aa0aa4ecc68cf07a305b4ad84a0368 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-list.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=bac1d60c412c34f8a81920b7e8def601" alt="Shard list" className="block dark:hidden" data-og-width="1850" width="1850" data-og-height="2130" height="2130" data-path="docs/images/architecture/shard-list.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-list.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=26bb8fbf2b3f2b929223106196f11352 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-list.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=5b391577764b1a644195ad427a175b48 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-list.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=a5cb0cef3ffc55eff565be345ecc0ced 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-list.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=02482f077d8d7d937eab53d9e9c8b7f7 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-list.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=fec63927cc845a19677bb097f8fdba20 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-list.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=a896faf6c23f7e06382530e60011b66c 2500w" />
</Frame>

After selecting a shard, you'll be able to choose to look at metrics for either that shard's primary or one of its replicas.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-primary-replicas-list-darkmode.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=3c07a572ca54cf9ef1b39294c46dfd7c" alt="Shard list" className="hidden dark:block" data-og-width="1850" width="1850" data-og-height="2124" height="2124" data-path="docs/images/architecture/shard-primary-replicas-list-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-primary-replicas-list-darkmode.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=57508b0eea87b7174685618366831cf9 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-primary-replicas-list-darkmode.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=06fbb0fbed40a2016eb5f8e126cee004 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-primary-replicas-list-darkmode.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=079b7d9975927dcf561d45d76fe67711 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-primary-replicas-list-darkmode.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=903ce01b9a0b2ff9eddb889d3589f141 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-primary-replicas-list-darkmode.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=1dfaa53209abf7aca2c1c7bacab38934 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-primary-replicas-list-darkmode.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=1240f52cee27d089e346251e7ac4f322 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-primary-replicas-list.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=d27e29bba924d3a456d59a56ff2243cd" alt="Shard list" className="block dark:hidden" data-og-width="1850" width="1850" data-og-height="2130" height="2130" data-path="docs/images/architecture/shard-primary-replicas-list.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-primary-replicas-list.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=08fa833b7b90a8c9befa5d35cc4a96c0 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-primary-replicas-list.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=5621a0cebd60afb6a321dbfe9dd9315c 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-primary-replicas-list.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=a28d5347439058e1467009f65a6ec9d9 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-primary-replicas-list.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=86b258224119e8506283e5ba35ab5b0e 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-primary-replicas-list.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=00ae8ac3aea0b7a403776be9edaeb38e 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-primary-replicas-list.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=f410973b16b275d6be14f7cf6040bbde 2500w" />
</Frame>

Selecting one will show you the metrics for that specific node in your database architecture.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-primary-metrics-darkmode.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=59b9f35fa7f64183343fb73038ab047b" alt="Shard" className="hidden dark:block" data-og-width="1850" width="1850" data-og-height="2184" height="2184" data-path="docs/images/architecture/shard-primary-metrics-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-primary-metrics-darkmode.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=9c25ba30bdca6104752c1fa29531853f 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-primary-metrics-darkmode.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=516d267b6c0ea35dc0e41736392902f1 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-primary-metrics-darkmode.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=86ae36755050fff4a2f3806b838c3fd6 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-primary-metrics-darkmode.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=b940d88ef483925d00d2c94183543d95 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-primary-metrics-darkmode.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=335ddf4b8731bf95dfb94f68a254b70a 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-primary-metrics-darkmode.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=89e90b58ad3b5819f9f45cb327aa6db3 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-primary-metrics.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=828657f74aba7b4cb60e48c297a52aff" alt="Shard" className="block dark:hidden" data-og-width="1850" width="1850" data-og-height="2198" height="2198" data-path="docs/images/architecture/shard-primary-metrics.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-primary-metrics.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=2b51fb71b8305c7e071d5a35e898f1f3 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-primary-metrics.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=0616f26060913ac2be8d5e2126426e89 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-primary-metrics.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=640c025b43b41062fcdbf0fdd236733d 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-primary-metrics.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=412bfebecc9a81451cf9a4c9ef2c5146 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-primary-metrics.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=3b1a39f3b6fcc5d6b66b5be812455702 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/shard-primary-metrics.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=8f678af5d1087c212d1ec38b093165e2 2500w" />
</Frame>

### Resizing

You can use the [Clusters page](/docs/vitess/cluster-configuration) menu to resize your keyspaces.
When a resize is in progress, this will be indicated at the top of the diagram.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-resize-darkmode.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=5f55648ced6c5f66e3614777156e1f51" alt="Architecture diagram with resize indicator" className="hidden dark:block" data-og-width="2666" width="2666" data-og-height="1546" height="1546" data-path="docs/images/architecture/diagram-resize-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-resize-darkmode.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=fff504c00ae7b3d73355c3501f26ded0 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-resize-darkmode.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=280b9976d927ca70695656f334f958c3 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-resize-darkmode.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=290b4f494b2c2dad0d0e84fd2531c192 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-resize-darkmode.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=861843cd286593f37981beb44d57fa52 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-resize-darkmode.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=6dbe44d12f63b837b75a49ae456e023f 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-resize-darkmode.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=07f5c648a727cb500c989534ef6d9e6c 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-resize.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=a7b30d905a4347941fa0d84e7f0e478b" alt="Architecture diagram with resize indicator" className="block dark:hidden" data-og-width="2666" width="2666" data-og-height="1546" height="1546" data-path="docs/images/architecture/diagram-resize.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-resize.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=8a6c43e5998a909fce27c49385246200 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-resize.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=a1fe0ec6cd2b257a610a6aa7366fb82f 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-resize.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=331136a3e92ab4ef3794da8c93a094b3 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-resize.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=60f4796dd8c1cf5a87d5c1df5fd6a94b 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-resize.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=f6297b5c02342e4ef453292d3e0ec1c6 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/diagram-resize.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=35786008821459d2255e0c5ffde757d4 2500w" />
</Frame>

Click on "**View**" to see the status for each shard being resized:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/resize-progress-darkmode.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=d98344833cfbbcf0132aed9f9860f12d" alt="Per-shard resize status" className="hidden dark:block" data-og-width="2410" width="2410" data-og-height="1784" height="1784" data-path="docs/images/architecture/resize-progress-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/resize-progress-darkmode.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=bc5fd9fca6b661d46c1abb8609038f2e 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/resize-progress-darkmode.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=fb5e1323998afff651f17111140659f2 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/resize-progress-darkmode.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=d34edb597976ab123177522a8a7ba12e 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/resize-progress-darkmode.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=65b31d56bc4994553da59d27cee0c117 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/resize-progress-darkmode.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=9b83853c3acb90b6a62589bb3a233208 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/resize-progress-darkmode.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=332c4b7c05033cb86b851d584f8b4a37 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/resize-progress.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=1b820c2aacae5d4ff0898477a59c08bd" alt="Per-shard resize status" className="block dark:hidden" data-og-width="2282" width="2282" data-og-height="1652" height="1652" data-path="docs/images/architecture/resize-progress.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/resize-progress.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=9fbea2e9341740272203549a57f2396b 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/resize-progress.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=1487deadbd733adb31dfa2c36f3ad8a6 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/resize-progress.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=263c748c0502baa81eff38cf5b0e8ef2 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/resize-progress.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=55b38da80247b4c02bd67685ff4189f1 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/resize-progress.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=4fd4431b46d01666bb52900dc406e381 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/architecture/resize-progress.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=2b2b1965d034b1f60d151950ee9b71ad 2500w" />
</Frame>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Audit log
Source: https://planetscale.com/docs/vitess/audit-log

The organization audit log grants [Organization Administrators](/docs/security/access-control#organization-administrator) access to review **actions** performed by individual members of the organization.

In addition, each audit log includes **events** detailing who performed the **action** and when it happened.

Audit log retainment period is [based on your plan](/docs/planetscale-plans):

* **Scaler Pro** — 15 days
* **Enterprise** — 15 days

<Note>
  Organization audit log access is limited to [Organization Administrators](/docs/security/access-control#organization-administrator).
</Note>

## Review your organization audit log

You can review your organization audit log under [your PlanetScale **organization** settings](https://app.planetscale.com/~/settings/audit-log).

Once there, you can filter the audit log by **Actor** and/or **Action**.

Clicking and expanding individual log **event** names empowers you to investigate additional metadata that can provide broader context around what changed.

## Audited organization events

You can track the following organization **events** in your PlanetScale account:

| PlanetScale organization events      | Actions                                                                                                                                                                                                                                            | Database Engine  |
| :----------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------- |
| access\_token                        | created  deleted  token\_leaked                                                                                                                                                                                                                    | Vitess, Postgres |
| backup\_policy                       | created  deleted                                                                                                                                                                                                                                   | Vitess           |
| branch\_maintenance\_schedule        | created  deleted  started\_maintenance\_window  finished\_maintenance\_window                                                                                                                                                                      | Vitess           |
| branch\_maintenance\_window          | created  deleted                                                                                                                                                                                                                                   | Vitess           |
| database                             | created  deleted  requested\_deletion  updated\_default\_branch  updated\_migration\_table\_name  removed\_member  added\_member                                                                                                                   | Vitess, Postgres |
| database\_branch                     | created  deleted  enabled\_safe\_migrations  disabled\_safe\_migrations  enabled\_foreign\_keys  disabled\_foreign\_keys  enabled\_vectors  updated\_cluster\_size  updated\_cluster\_config  updated\_vtgates  updated\_cluster\_update\_strategy | Vitess, Postgres |
| database\_branch\_keyspace           | created  deleted  updated  requested\_deletion                                                                                                                                                                                                     | Vitess           |
| database\_branch\_password           | created  deleted  password\_leaked  updated\_ip\_restrictions                                                                                                                                                                                      | Vitess, Postgres |
| database\_branch\_read\_only\_region | created  deleted                                                                                                                                                                                                                                   | Vitess           |
| database\_deploy\_request            | created  deleted  closed                                                                                                                                                                                                                           | Vitess           |
| database\_webhook                    | created  deleted                                                                                                                                                                                                                                   | Vitess, Postgres |
| deploy\_request\_review              | approved                                                                                                                                                                                                                                           | Vitess           |
| deployment                           | unqueued                                                                                                                                                                                                                                           | Vitess           |
| external\_datasource                 | created  deleted                                                                                                                                                                                                                                   | Vitess, Postgres |
| integration                          | created  deleted                                                                                                                                                                                                                                   | Vitess, Postgres |
| organization                         | created  deleted  joined  removed\_member  left  added\_member  enabled\_sso  disabled\_sso  enabled\_sso\_directory  disabled\_sso\_directory                                                                                                     | Vitess, Postgres |
| organization\_invitation             | created  deleted                                                                                                                                                                                                                                   | Vitess, Postgres |
| organization\_membership             | created  updated\_role                                                                                                                                                                                                                             | Vitess, Postgres |
| organization\_team                   | created  deleted  added\_member  left  removed\_member                                                                                                                                                                                             | Vitess, Postgres |
| postgres\_role                       | created  deleted                                                                                                                                                                                                                                   | PostgreSQL       |
| regional\_failover\_event            | created  deleted                                                                                                                                                                                                                                   | Vitess, Postgres |
| service\_token                       | created  deleted  updated\_bulk\_database\_access  token\_leaked                                                                                                                                                                                   | Vitess, Postgres |
| user                                 | created  deleted  signed\_in                                                                                                                                                                                                                       | Vitess, Postgres |

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Back up and restore
Source: https://planetscale.com/docs/vitess/backups



## Overview

PlanetScale provides the ability to create, schedule, and restore backups for production and development database branches.

<Note>
  Our [Scaler Pro plan](/docs/planetscale-plans) includes automated backups every 12 hours.
</Note>

## View backups

To view backups for all of your branches, go to your database backups page: `app.planetscale.com/<org>/<database>/backups`.

Once there, you'll find additional details about your backup history.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/view-backups.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=56a6cd014e2c1759913d5c77f07ed457" alt="View backups for your database" data-og-width="2756" width="2756" data-og-height="1626" height="1626" data-path="docs/images/assets/docs/concepts/back-up-and-restore/view-backups.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/view-backups.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=c1d30b456afe41dcb1c4d84e58942c2a 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/view-backups.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=b4c436b1ddc10fc7ae536eab3d64b705 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/view-backups.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=f2727fc50076134c561aa1e7fcf04bae 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/view-backups.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=c69d6a044d9e2c6b650267b312d4ebad 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/view-backups.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=ae5f39fbbc723b9e6ed0c3d1dda660de 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/view-backups.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=fca0a73078b9c7a8665138a14b040183 2500w" />
</Frame>

## Create manual backups

In addition to the daily default backups that PlanetScale schedules for your database branches, you can create additional **manual** backups.

To create a manual backup, follow the steps outlined below:

<Steps>
  <Step>
    Go to your database backups page: `app.planetscale.com/<org>/<database>/backups`.
  </Step>

  <Step>
    Click the **Create new backup** button.

    This will bring up a pop-up modal that prompts you to pick a **branch** to backup, *name* your backup, and select how long you wish to keep the backup.

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/create-new-backup.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=cb9bf694360f157ed0ca044dad1b878e" alt="Manual backup pop-up modal" data-og-width="1006" width="1006" data-og-height="950" height="950" data-path="docs/images/assets/docs/concepts/back-up-and-restore/create-new-backup.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/create-new-backup.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=a7aca2a7d28a13dd2b630d95effaaf5e 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/create-new-backup.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=af1267d98c18c5661d0da341d77f939a 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/create-new-backup.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=83a1b590c55581dab67d9b656eb137cd 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/create-new-backup.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=e03b306844e7390efd9bc3e9f16a0672 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/create-new-backup.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=619775abf40d6a0b28b613cc63fd56a3 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/create-new-backup.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=554e8da8a921c0d2880c316305da9a20 2500w" />
    </Frame>
  </Step>

  <Step>
    Click the **Create backup** button to finish the backup and close the pop-up modal.
  </Step>

  <Step>
    Manual backups are not free and are denoted by the `$` icon next to their name in the list view on the backups page.

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/manual-backup-row.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=28daf076ab38beb6b1a63235b2c8b212" alt="Manual backups are shown with a $ sign" data-og-width="1712" width="1712" data-og-height="172" height="172" data-path="docs/images/assets/docs/concepts/back-up-and-restore/manual-backup-row.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/manual-backup-row.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=06f16c950fe14422cfec9405e054b80e 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/manual-backup-row.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=2293002b6e26455aef8494bfcc3aea54 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/manual-backup-row.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=055f375dcbbcb3fd993ee27f22eb4f68 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/manual-backup-row.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=28754ba6a6387bd329b7da06d5fb5dc3 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/manual-backup-row.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=f7794d1b940245f341f8d6e4fbf850b3 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/manual-backup-row.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=272a3b3247369bc346e92cfb334dda10 2500w" />
    </Frame>
  </Step>

  <Step>
    To see the cost associated with storage of a manual backup, click on the backup name. You will see the cost per month on the details page.

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/manual-backup-cost.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=ab73d34990b26eef95385f0b0c7c072f" alt="Monthly storage cost for a Manual backup" data-og-width="1000" width="1000" data-og-height="818" height="818" data-path="docs/images/assets/docs/concepts/back-up-and-restore/manual-backup-cost.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/manual-backup-cost.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=dfe2a7cd1673e4677f08c15b80d63741 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/manual-backup-cost.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=38d085ba95b1a42075e44cc0f3745ce5 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/manual-backup-cost.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=eb0ff2a20dd07ace199d2a3c9ecd0a0b 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/manual-backup-cost.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=c00d9142cfd459e99666b3448b091724 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/manual-backup-cost.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=2ba8f240c8c7f734a10b6b8fbdd57a9e 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/manual-backup-cost.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=213586c56f5673103d88cdafe5af651a 2500w" />
    </Frame>
  </Step>
</Steps>

## Schedule backups

You can add additional **scheduled backups** for your branches, billed at \$0.023 per GB per month.

<Steps>
  <Step>
    Go to your database backups page: `app.planetscale.com/<org>/<database>/backups`.
  </Step>

  <Step>
    Select the type of branch (`Production branches` | `Development branches` ) you'd like to be backed up by the new backup schedule.

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/new-backup-schedule.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=3f1d1574ad2b45ba5a4ae8467ef2b5b0" alt="Schedule backup pop-up modal" data-og-width="990" width="990" data-og-height="1134" height="1134" data-path="docs/images/assets/docs/concepts/back-up-and-restore/new-backup-schedule.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/new-backup-schedule.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=8f391fc9db38254840f907b3cb374dc8 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/new-backup-schedule.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=348e90d124eb2b0e0a58d9da687dc329 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/new-backup-schedule.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=1bc3d88794a890f515a90fea7bb2f2ed 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/new-backup-schedule.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=20f11405194530384cd78e2691f9cf46 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/new-backup-schedule.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=79d5674cf88757f785a07e4944a12753 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/new-backup-schedule.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=ce74e69d0a384633492bad7f1cf3979b 2500w" />
    </Frame>
  </Step>

  <Step>
    Click the **Add new schedule** button.

    This will bring up a pop-up modal that prompts you to configure backup frequency, select how long you wish to keep the backup, and to *name* your **schedule**.

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/new-backup-schedule.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=3f1d1574ad2b45ba5a4ae8467ef2b5b0" alt="Schedule backup pop-up modal" data-og-width="990" width="990" data-og-height="1134" height="1134" data-path="docs/images/assets/docs/concepts/back-up-and-restore/new-backup-schedule.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/new-backup-schedule.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=8f391fc9db38254840f907b3cb374dc8 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/new-backup-schedule.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=348e90d124eb2b0e0a58d9da687dc329 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/new-backup-schedule.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=1bc3d88794a890f515a90fea7bb2f2ed 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/new-backup-schedule.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=20f11405194530384cd78e2691f9cf46 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/new-backup-schedule.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=79d5674cf88757f785a07e4944a12753 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/new-backup-schedule.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=ce74e69d0a384633492bad7f1cf3979b 2500w" />
    </Frame>
  </Step>

  <Step>
    Click the **Save schedule** button to save your new scheduling configurations and to close the pop-up modal.
  </Step>
</Steps>

<Note>
  For additional scheduled backups beyond the included default (every 12 hours for the
  [Scaler Pro plan](/docs/planetscale-plans), you will be billed
  **\$0.023 per GB per month**.
</Note>

## Restore from a backup

To restore a backup to a new branch, click on the individual backup to see the option to restore them.

<Steps>
  <Step>
    Go to your database backups page: `app.planetscale.com/<org>/<database>/backups`.
  </Step>

  <Step>
    Select the backup you wish to restore.
  </Step>

  <Step>
    Click the **Restore backup** button.
    This will bring up a pop-up modal that prompts you to name your branch and select a cluster size.

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/backups/restore.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=38e52111180f3d93482e73f67ad98d2c" alt="Restore backup pop-up modal" data-og-width="877" width="877" data-og-height="1063" height="1063" data-path="docs/images/backups/restore.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/backups/restore.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=0a18407f87674f360e86c9a5b81e753c 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/backups/restore.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=852f86ba97a27e9813302184e14874fc 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/backups/restore.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=4dc47fa81377c2002fef51083c428112 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/backups/restore.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=93a2d33a04a24171b6d03751e183ab0c 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/backups/restore.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=f355ce0bc569686c4d3053bc1c505712 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/backups/restore.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=b2e224c1792fc7dfb0f72f69859d90a1 2500w" />
    </Frame>
  </Step>

  <Step>
    Click the **Restore backup** button to finish restoring your backup and to close the pop-up modal.
  </Step>

  <Step>
    To see all branches that are `restored` from a backup, head to the backup details and you'll see a list of those branches.

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/restored-branches-list.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=fac0e032e05cfd1c6acb8f57143cb4a6" alt="Branches restored from a backup" data-og-width="1016" width="1016" data-og-height="1058" height="1058" data-path="docs/images/assets/docs/concepts/back-up-and-restore/restored-branches-list.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/restored-branches-list.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=0a68baf3a5a48b9e9dcecc50631b5e2a 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/restored-branches-list.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=afa12c3e1a5c3d696a1e9a9ca2df08c3 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/restored-branches-list.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=2fa41463914364fbec05c79a9a34606e 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/restored-branches-list.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=ebc01b0c5814d3925461a666f8f237e0 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/restored-branches-list.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=1ab0c0090af8144aedffd394e985e947 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/back-up-and-restore/restored-branches-list.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=2a97f0279111f2789d2c79ae5deb62f2 2500w" />
    </Frame>
  </Step>
</Steps>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale workflow for Vitess
Source: https://planetscale.com/docs/vitess/best-practices



<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/planetscale-workflow.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=3248003ec557ff31fd529bf8e2eb717c" alt="Diagram showing PlanetScale workflow" data-og-width="1511" width="1511" data-og-height="407" height="407" data-path="docs/images/planetscale-workflow.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/planetscale-workflow.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=a0a07a5c2c44f8b140e904ba8e968b0f 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/planetscale-workflow.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=550934b3f690b2c665e1278effac6b68 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/planetscale-workflow.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=f0c0f8e6e8e4aaa56138460392d44e67 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/planetscale-workflow.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=dfd39c2550b147efff8542d7ba1d8af6 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/planetscale-workflow.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=c9d5fa068ed136fd1d0a570a8d382ce3 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/planetscale-workflow.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=8294de9fea0486702918a8cd142f1a0c 2500w" />
</Frame>

PlanetScale databases are designed for developers and developer workflows. Deploy a fully managed database cluster with the reliability of MySQL (our databases run on MySQL 8) and the scale of open source Vitess in just minutes.

Deploy, branch, and query your database directly from the UI, download our [CLI](https://github.com/planetscale/cli#installation) and run commands there, or automate your deployments using our [GitHub Actions](/docs/vitess/integrations/github-actions) and [API](/docs/api/reference/getting-started-with-planetscale-api).

Built-in connection pooling means you’ll never run into connection limits for your database.

## PlanetScale branching

The PlanetScale branching feature allows you to treat your databases like code by creating a branch of your production database schema to serve as an isolated development environment.

PlanetScale provides two types of database branches: **development** and **production**.

Development branches provide isolated copies of your database schema where you can make changes, experiment, or run CI against. Instantly branch your production database to create a staging environment for testing out your schema changes.

Production branches are highly available databases intended for production traffic. They include an additional replica for high availability and are automatically backed up daily.

Branches can also have [safe migrations](/docs/vitess/schema-changes/safe-migrations) enabled for zero-downtime schema migrations, protection against accidental schema changes, and enhanced team collaboration through [deploy requests](/docs/vitess/schema-changes/deploy-requests).

We also offer a [Data Branching®](/docs/vitess/schema-changes/data-branching) feature, which allows you to create an isolated replica of your database for development that includes both the schema **and** data.

Learn more about [database branching](/docs/vitess/schema-changes/branching).

## Non-blocking schema changes

PlanetScale makes it safe to deploy schema changes to production and easy to automate schema management as a part of your CI/CD process. Schema changes to production branches with safe migrations enabled are applied online and protect against changes that block databases, lock individual tables, or slow down production during the migration.

Use a development branch to apply schema changes and view the schema diff in the UI or the CLI. Once you’re satisfied with your schema changes, you can open a deploy request.

Learn more about [non-blocking schema changes](/docs/vitess/schema-changes).

## Deploy requests

<Note>
  Your database must have a branch with [safe migrations](/docs/vitess/schema-changes/safe-migrations) enabled before you can create a deploy request.
</Note>

[Deploy requests](/docs/vitess/schema-changes/deploy-requests) allow you to propose schema changes and get feedback from your team. The deploy requests display DDL statements (`CREATE`, `ALTER`, and `DROP`) for each table changed, with a line-by-line schema diff, making it easy to review the changes.

For example, you can pair deploy requests with GitHub pull requests so that your teammates can review the code and the schema changes in parallel.

PlanetScale also analyzes your schema changes for conflicts when you open a deploy request. It checks against the production schema at the time the branch was created and against the current production schema which may have changed in the interim. This ensures your changes can be deployed safely without impacting production.

## Deploy a schema change

Once a deploy request has been approved, it gets added to the deploy queue. Schema changes are applied to your production database branch in the order in which they are added to the deploy queue.

As a part of the process of adding the schema changes to the deploy queue, PlanetScale analyzes the schema changes in the deploy requests ahead of your deploy request for any potential conflicts. If a conflict exists, your deploy request is rejected from the queue, and you are immediately notified of the conflict. This means you’ll never wait for your schema change to deploy only to learn that there’s an unanticipated conflict. This is especially useful if long running schema changes are being applied.

## Revert a schema change

If you ever deploy a schema change, only to realize you need to undo it, PlanetScale can handle that. With our [schema revert feature](/docs/vitess/schema-changes/deploy-requests#revert-a-schema-change), you have 30 minutes to "undo" a schema change deployment. Simply click the "Revert changes" button on the affected deploy request page and your production database will instantly revert back to the previous schema. Additionally, any data that was added to your production database in the time between deployment and reverting will be retained. Learn more about this process in our [Revert a schema change section](/docs/vitess/schema-changes/deploy-requests#revert-a-schema-change) of our deploy requests documentation.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Cluster configuration
Source: https://planetscale.com/docs/vitess/cluster-configuration

The Clusters page in your PlanetScale dashboard allows you to configure your PlanetScale cluster.

From here, you can:

* Adjust the instance sizes for [keyspaces](/docs/vitess/sharding/keyspaces)
* Create sharded or unsharded keyspaces
* Switch between [Metal](/docs/plans/planetscale-skus#metal) and [network-attached storage](/docs/plans/planetscale-skus#network-attached-storage)
* Adjust the number of replicas for each keyspace
* Adjust the VSchema
* Adjust the number and size of [VTGates](/docs/vitess/scaling/vtgates)
* Turn on [VTGate autoscaling](/docs/vitess/scaling/vtgates#autoscaling-vtgates)
* View the CPU and memory utilization of your VTGates
* View any changes to your VTGates
* Adjust the [memory allocated to vector indexes](/docs/vitess/vectors#resource-requirements) and the InnoDB buffer pool, for any branch with [vectors](/docs/vitess/vectors) enabled
* Configure [VReplication settings](#vreplication-settings)
* Configure [replication durability constraints](#replication-durability-constraints)

This documentation will cover how to use everything in this Clusters page. For a full walkthrough with an example of setting up a sharded keyspace, refer to the [Sharding quickstart](/docs/vitess/sharding/sharding-quickstart).

If you would like additional support configuring your sharded keyspaces, our [Enterprise plan](/docs/planetscale-plans#planetscale-enterprise-plan) may be a good fit. [Get in touch](https://planetscale.com/contact) for a quick assessment.

## Adjust your cluster size

To adjust your cluster size, first select the [keyspace](/docs/vitess/sharding/keyspaces) you'd like to adjust. If you only have one keyspace, it will be selected by default.

Click on the "Cluster size" dropdown. Select the new cluster size, ([Metal](/docs/metal) or network-attached storage). If you select Metal, make sure you select the correct storage size. Finally. click "Save".

A network-attached storage cluster (Amazon Elastic Block Storage or Google Persistent Disk) may take a few minutes to change sizes. And a Metal cluster can take hours, depending on the size.

You can check the status of the resize from the [database homepage](/docs/vitess/architecture#resizing) in the dashboard, [CLI](/docs/cli/keyspace), or [API](/docs/api/reference/get_keyspace_rollout_status). There will be no downtime or locking during this process.

For more information about selecting a cluster size, see the [Cluster sizing documentation](/docs/plans/cluster-sizing).

## Create a keyspace

<Warning>
  Misconfiguration to keyspaces can cause availability issues. We recommend thoroughly reading through the documentation in the [Sharding section](/docs/vitess/sharding) of the docs prior to adding additional keyspaces. If you have any questions, please [reach out to our support team](https://docs/support.planetscale.com).
</Warning>

<Warning>
  If you are using [Vitess global routing](https://vitess.io/docs/api/reference/features/global-routing/), you must take extra care when adding a sharded keyspace to your database (for example, if you are using `@primary`).
  Before creating one, you must ensure that all tables from your first *unsharded keyspace* are added to the `VSchema` of that *unsharded keyspace*. Eg:

  ```
  {
    "tables": {
      "users": { }
      ...
    }
  }
  ```

  Otherwise, queries will fail. [Learn more about VSchema.](/docs/vitess/sharding/vschema)
</Warning>

<Note>
  Sharded keyspaces are not currently supported on databases that have foreign key constraints enabled.
</Note>

To create a new [keyspace](/docs/vitess/sharding/keyspaces):

<Steps>
  <Step>Select the database you want to configure.</Step>
  <Step>Click "Clusters" in the left nav.</Step>
  <Step>You should see the existing unsharded keyspace for your database here.</Step>
  <Step>Click "New keyspace".</Step>

  <Step>
    Enter the keyspace name. For example, if your existing unsharded keyspace is named `metal`, you may create a sharded
    keyspace named `metal-sharded`.
  </Step>

  <Step>
    Select whether you want to keep it unsharded, or, if not, select the number of shards you want to exist in this
    keyspace. In most cases, you will be adding a new sharded keyspace. Adding a new unsharded keyspace is not a common
    use case, but is an option if you're looking to do [vertical
    sharding](https://planetscale.com/learn/courses/database-scaling/sharding/vertical-sharding).
  </Step>
</Steps>

<Note>
  The cost of adding this additional keyspace largely depends on the number of shards you choose, the cluster size, and if you'd like to add additional replicas.
</Note>

<Steps>
  <Step>
    Choose the cluster sizes you would like to use for this keyspace. Keep in mind, if you are creating a sharded
    keyspace, this will spin up multiple clusters of the selected size. For example, if you are creating 4 shards and
    choose the `PS-80` cluster size, we will create 4 `PS-80`s, each with 1 primary and 2 replicas.
  </Step>

  <Step>
    Select the number of *additional* replicas, if any, that you'd like to add to each cluster. Each cluster comes with
    2 replicas by default, so any number you choose will be in addition to those 2.
  </Step>

  <Step>
    Review the new monthly cost for this keyspace below. This is in addition to your existing unsharded keyspace, as
    well as any other keyspaces you add.
  </Step>

  <Step>Once satisfied, click "Create keyspace".</Step>
</Steps>

## Modify the VSchema of a keyspace via the Clusters page

<Note>
  You can modify the VSchema on your development branch either in the Clusters page, using the [`ALTER VSCHEMA` command](/docs/vitess/sharding/vschema#modifying-vschema), or with the pscale CLI using [`pscale keyspace vschema update`](/docs/cli/keyspace).
</Note>

Once you have created your keyspace, you will see a new tab: **VSchema**. The VSchema contains information about how the keyspace is sharded, sequence tables, and other Vitess schema information. The VSchema tab allows you to configure the Vschema for your new keyspace or modify it for existing keyspaces.

We do not recommend modifying the VSchema directly on your production branch. In fact, it is not possible to do if you have [safe migrations](/docs/vitess/schema-changes/safe-migrations) enabled (as recommended). Instead, to modify the VSchema, you should first [create a new development branch](/docs/vitess/schema-changes/branching). Once you have your branch ready, follow these steps:

<Steps>
  <Step>
    To update the VSchema on the Clusters page, select your new development branch from the dropdown at the top, and
    then select the keyspace below that has the VSchema you'd like to modify.
  </Step>

  <Step>Next, click the tab labeled "VSchema".</Step>

  <Step>
    Modify the VSchema configuration JSON file as needed. Refer to the [VSchema
    documentation](/docs/vitess/sharding/vschema) for more information about the available options.
  </Step>

  <Step>
    When finished, click "Save changes". We will validate your VSchema, and if it is valid, the changes will be saved.
    If there are errors, we will warn you here to change them before saving.
  </Step>

  <Step>
    Go back to your "Branches" tab and click on the development branch that you modified. You should see a note on the
    right that says "Updated VSchema configuration" which lets you know the VSchema(s) for this branch has been
    modified.
  </Step>

  <Step>
    From here, go through the normal [deploy request process](/docs/vitess/schema-changes/deploy-requests) to deploy
    this change to production.
  </Step>
</Steps>

Once your change is deployed to production, you can come back to the Clusters page, switch to your production branch, and view the updates to your VSchema. You can also click the "Changes" tab to see information, such as the resize event, status, and start/end time for any previous changes to the VSchema.

## Modify routing rules

This configuration setting is currently only available for some Enterprise customers. To modify your routing rules, click "Manage routing rules" on the bottom left of the keyspace configuration panel.

Again, you will need to create a new branch to modify routing rules, as described in the "Modify the VSchema of a keyspace" section above.

## Modify keyspace settings

There are a number of keyspace-specific settings you can use to customize keyspace behavior.

### Replication durability constraints

By default, replication to replica and read-only VTTablets is configured to maximize safety and data integrity. This behavior can be relaxed for performance improvements and reduced replication lag.

* **Maximum** — Default setting; use maximum durability constraints.
* **Dynamic** — Use maximum durability constraints when replication lag is under 5s, and automatically relax durability constraints when replication lag exceeds 5s. Durability constraints are automatically set back to maximum when replication lag reduces to under 5s.
* **Minimum** — Reduce durability constraints. Optimizes for replica and read-only performance, but has highest risk of data loss on crashed instances.

### VReplication settings

These settings improve performance during [VReplication](https://vitess.io/docs/api/reference/vreplication/vreplication/) processes like deploy requests and workflows.

* **Optimize inserts** — Enabled by default. When enabled, during binlog replication catch-up, skip sending insert events for rows that have yet to be copied. For more technical details, see the corresponding [Vitess implementation](https://github.com/vitessio/docs/vitess/pull/7708).
* **Allow NOBLOB binlog row image** — Enabled by default. When enabled, then we support enabling MySQL’s NOBLOB binlog mode, to omit unchanged BLOB and TEXT columns from replication events, reducing binlog size. For more technical details, see the corresponding [Vitess Implementation](https://github.com/vitessio/docs/vitess/pull/14502).
* **Batch binlog statements** — Disabled by default. If enabled, batches binlog statements and transactions to limit the number of round-trips to MySQL. For more technical details, see the corresponding [Vitess implementation](https://github.com/vitessio/docs/vitess/pull/14502).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Connection strings
Source: https://planetscale.com/docs/vitess/connecting/connection-strings



## Creating a password

<Steps>
  <Step>
    To create a password, head to your database dashboard page at `https://app.planetscale.com/<ORGANIZATION>/<DATABASE_NAME>` and click on the "**Connect**" button.
  </Step>

  <Step>
    On the **Connect page**, select the branch you wanted to create a password for, pick a [password role](/docs/vitess/security/password-roles), and provide a recognizable name for the new credentials. Clicking `Create password` will then generate a **unique username and password pair** that can only be used to access the designated branch of your database. Take note of this password, as you won't be able to see it again.
  </Step>

  <Step>
    Once created, you can browse the connection string in different framework formats by selecting framework in the "Select your language or framework" section. This will also show you all of the files you need to modify to get connected with PlanetScale in your framework or language of choice.
  </Step>
</Steps>

<Note>
  There are two connection types for a password: `Primary` and `Replica`. The `Primary` connection type is used to connect to the primary region of your database, while the `Replica` connection type is used to route queries to your branch's replicas and read-only regions. You can create multiple passwords for a branch, each with a different connection type. [Read more about replicas](/docs/vitess/scaling/replicas).
</Note>

<Tip>
  Make sure you copy the credentials for your application and the "Other" format. We do not save the password in plaintext, so there will be no way to retrieve the password after you leave this page.
</Tip>

## Managing passwords

Once you've created the password, you can head over to the "**Passwords**" settings page available at `Organization > Database > Settings > Passwords` to manage them.

<Tip>
  You can also create passwords for branches other than `main` on this page.
</Tip>

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/connection-strings/manage-2.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=07f620508e2a9ebd153dc80a3dcf7629" alt="Manage passwords page" data-og-width="1761" width="1761" data-og-height="604" height="604" data-path="docs/images/assets/docs/concepts/connection-strings/manage-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/connection-strings/manage-2.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=33d63bc6fe96ee960fac37e93b7e0853 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/connection-strings/manage-2.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=24cf5aaab28af798045f57cfbf2c7cbb 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/connection-strings/manage-2.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=6ff1f113234e375d4fc75f00839bcd04 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/connection-strings/manage-2.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=e0279fb5a9bf953bf266f70e336d59c3 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/connection-strings/manage-2.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=c2cc871b1f3425d99314ecffe6d83b5a 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/connection-strings/manage-2.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=8216abe43a4fe2abffa93a3909f1a0d9 2500w" />
</Frame>

Clicking on the `...` icon on the row for your password allows you rename or delete the password.

## Renaming a password

Since the **username & password** pair is unique, the only metadata you can edit is the `display name` of the password.

## Deleting a password

Deleting a password will invalidate the username & password pair and **disconnect any active clients using this password**.

<Note>
  Any open database connections authenticated with a deleted password will be disconnected within five minutes.
</Note>

## Native MySQL authentication support

Use the tools you're familiar with to connect to PlanetScale databases.
PlanetScale supports both [MySQL native authentication](https://dev.mysql.com/doc/refman/8.0/en/native-pluggable-authentication.html), which is widely used to provide a secure connection to MySQL servers,
and [MySQL Caching SHA-2 authentication](https://dev.mysql.com/doc/refman/8.0/en/caching-sha2-pluggable-authentication.html), which is the most secure authentication mechanism to connect to MySQL.
Based on your application needs and platform support, you can switch between the authentication modes, with the same password.

For a list of tested MySQL GUI clients, review our article on [how to connect MySQL GUI applications](/docs/vitess/tutorials/connect-mysql-gui).

## Strong security model

PlanetScale Passwords are created for use with a single database branch.
This strong security model allows you to generate passwords that are tied to a branch, and cannot access data/schema from another branch.

## IP restrictions

You can restrict database connections to specific IP ranges for a single password by updating its IP restrictions. For example, if you have a database for a web application and you create a password for use in the deployed application, you can restrict usage of that specific password to the IP ranges of the deployed application. If somebody attempts to connect to the database from outside of the deployed application, the connection will be refused. IP restrictions work on a per-password basis, so if you want to use the same restriction across passwords, they must be applied to each password separately.

Some passwords are incompatible with IP restrictions, and you will need to create a new password to use IP restrictions.

Examples of when you may want to use IP restrictions:

* You want to segment database access so that the production database can only be connected to from production environments or development branches.
* You use a bastion in production and want to ensure that all database connections originate or pass through the bastion.
* You want to allow a single client to be able to access your database (e.g., for debugging) and want to provide the least amount of access for them to do so.
* You have compliance requirements that require implementing a more stringent access control list in your database.

### Updating the IP restrictions for a password

1. Go to your database's "**Settings**" tab.
2. Click "**Passwords**."
3. You can update the IP restrictions for a password in two different ways: The first way is by opening the dropdown menu to the right of any password on the Passwords page and clicking "**Manage IP restrictions**." The second way is by clicking on the password and scrolling to the bottom of its page to update IP restrictions.
4. Add the IP ranges that you want to allow to connect using the selected password.

<Note>
  If your password has no IP restrictions, it is set to **allow all traffic**. Similarly, when you add a new IP range to the restrictions, all IP addresses out of this range cannot connect to your database using that password.
</Note>

## Disconnect clients by deleting passwords

PlanetScale automatically disconnects clients that are using a deleted password.
Head on over to the `Organization > Database > Settings > Passwords` page on your database branch to delete passwords for that branch.
It may take up to five minutes for all active clients to be disconnected.

## No plain text password storage

PlanetScale only stores hashes and metadata about your database passwords.
To add an extra layer of security to your database, we do not store any passwords in plaintext.

<Note>
  In the event that you lose a password, we cannot recover it for you. We recommend creating a new password with the
  same access level.
</Note>

## GitHub Secret Scanning integration

All passwords and service tokens generated for use with PlanetScale databases are part of [GitHub's Secret Scanning](https://docs.github.com/en/code-security/secret-security/about-secret-scanning) program. If any database passwords or service tokens are committed in plaintext to any public GitHub repository, we will be notified and take corrective action to delete the access tokens and cut off their access.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale Model Context Protocol (MCP)
Source: https://planetscale.com/docs/vitess/connecting/mcp



With PlanetScale MCP support, AI tools like Claude or Cursor can directly access read-only database information to answer your questions about your database.

## Installation

We have two new [CLI Commands](/docs/cli/mcp) you can use to get up and running:

```
pscale mcp install --target claude # Installs PlanetScale MCP server in Claude Desktop
pscale mcp server                  # Enables Claude to perform read-only database interactions
```

The `--target` flag accepts `claude` or `cursor`.

You can get started using Claude or Cursor. Other tools should accept a configuration similar to the Cursor configuration.

### Claude Desktop:

<Steps>
  <Step>
    Upgrade to the latest version of pscale CLI
  </Step>

  <Step>
    Run `pscale mcp install --target claude`
  </Step>

  <Step>
    Restart Claude Desktop
  </Step>
</Steps>

### Cursor

Add the following to your `.cursor/mcp.json` in your project or home directory:

```json  theme={null}
{
  "mcpServers": {
    "planetscale": {
      "args": [
        "mcp",
        "server"
      ],
      "command": "pscale"
    }
  }
}
```

## Use cases

Using this functionality allows you to interact with your PlanetScale organizations and databases right from your AI tooling. You can ask questions like "How many users do I have in North Carolina?" or "How many rows are in my messages table?". Claude or your preferred AI coding tool will then be able to access road-only information from your PlanetScale account to give bespoke answers.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Network latency
Source: https://planetscale.com/docs/vitess/connecting/network-latency

When connecting to PlanetScale, it's important to understand how network latency can impact query speed.

## What is network latency, and why is it important?

Network latency is the time it takes for data to travel across a network between your application and your database. Minimizing network latency for databases is critical because it adds additional time to your application's queries.

One of the primary causes of network latency is the distance between two endpoints. For example, if an application is hosted in Virginia and is communicating to a database in Paris, each query will spend around 80ms traveling between the two servers. For an application that does many queries, this network time adds up quickly and can greatly impact the application's performance.

### Choosing a region for your database

The most optimal location for your database is in the same region and cloud provider as your application. PlanetScale has [regions available in both AWS and GCP](/docs/vitess/regions). If your application is hosted outside of AWS or GCP, then we recommend
choosing the PlanetScale region that is geographically closest to your application servers.

#### Using `pscale ping`

You can use the [`pscale ping`](/docs/cli/ping) command to determine the best PlanetScale region for your application. It will measure the latency to each PlanetScale region
and display the results in an ordered list from fastest to slowest.

It's best to run this command directly from your application's server to get a realistic measure.

```shell  theme={null}
  NAME                          LATENCY   ENDPOINT                                         TYPE
 ----------------------------- --------- ------------------------------------------------ -----------
  AWS us-west-2                 34.6ms    aws.connect.psdb.cloud                           optimized
  AWS us-west-2                 34.8ms    us-west.connect.psdb.cloud                       direct
  GCP us-central1               57.5ms    gcp.connect.psdb.cloud                           optimized
  GCP us-central1               57.9ms    gcp-us-central1.connect.psdb.cloud               direct
  AWS us-east-2                 60.5ms    aws-us-east-2.connect.psdb.cloud                 direct
  GCP us-east4                  69.2ms    gcp-us-east4.connect.psdb.cloud                  direct
  AWS us-east-1                 70.2ms    us-east.connect.psdb.cloud                       direct
  GCP northamerica-northeast1   80.9ms    gcp-northamerica-northeast1.connect.psdb.cloud   direct
```

### Network latency and serverless applications

In traditional applications, a single region hosts both the application and database. With these both collocated in the same region, network latency is minimized. For serverless or "edge" deployment models, this can become more complex. In these scenarios, the application is often deployed to several different regions while the primary database remains in a single region. This can result in high network latency between the application and database.

When looking at network latency, there are two important dimensions to consider. First, the **distance between the application and PlanetScale's edge network**. And second, the **distance between the edge and the database**.

While there is no solution to completely eliminating latency, the path the connection takes over the internet can be optimized for these serverless applications. PlanetScale does this by connecting to the nearest edge location and having the traffic backhauled via PlanetScale's network rather than traversing the public internet.

For example, consider an application running in `us-east` that is connecting to a database in `eu-west`. This example is not optimized, and traffic is directed over public internet until it connects to PlanetScale's edge in `eu-west` before being directed to the database.

```
<client (us-east)> <-------> <edge (eu-west)> <-> <database (eu-west)>
```

The following example shows a more optimal traffic path. The application connects to PlanetScale's edge, and then the traffic is sent over PlanetScale's network to the database. This minimizes the time spent traversing the public internet.

```
<client (us-east)> <-> <edge (us-east)> <------> <database (eu-west)>
```

## How to tell if network latency is impacting my application

When experiencing slow query times, it's important to rule out any potential networking issues. First, you need to be able to measure the time your application is spending waiting on the query. The best sources of this data are application performance monitoring (APM) services or query-level logging from your application. Since applications can be complex and many different factors can influence a performance problem, it's important to isolate and measure only the time spent on the query from the application's perspective.

Once you have a measurement of the query in your application, you can then compare the query to the data in [Query Insights](/docs/vitess/monitoring/query-insights). The difference between the numbers can give you an idea of how much time was spent transferring the data across the network. If you see a large difference, it is likely due to network latency between the application and database.

If you can access your application's host machine, you can also use `netcat` to understand the latency between the host and PlanetScale.

```shell  theme={null}
time nc -z aws.connect.psdb.cloud 3306

Connection to aws.connect.psdb.cloud port 3306 [tcp/mysql] succeeded!
nc -z aws.connect.psdb.cloud 3306  0.01s user 0.00s system 24% cpu 0.044 total
```

In this example, you can see establishing a connection took 44ms.

## PlanetScale hostnames

When connecting to PlanetScale, you have the option of two different hostnames: optimized or direct. For almost every case, we recommend using the optimized hostname. But there are some circumstances where using the direct hostname may work better for your application.

<Note>
  This does not apply to [PlanetScale Managed](/docs/vitess/managed) and single-tenant customers who have a single hostname for their account.
</Note>

## Optimized hostname

PlanetScale's optimized hostname uses Route 53's [latency-based](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-latency.html) routing to connect to the closest edge in PlanetScale's network. Using this hostname optimizes latency and improves reliability as it can route around regional network outages.

Examples of optimized hostnames:

* `aws.connect.psdb.cloud`
* `gcp.connect.psdb.cloud`

We recommend everyone use this hostname by default as it provides the best connection in most cases.

## Direct hostname

In some rare cases, we have found that PlanetScale's optimized hostname may not direct traffic along the most optimal path. If you are experiencing this, one solution is to use the direct hostname. We only recommend using this if your application and database are in the same region and you have confirmed the optimized hostname is directing traffic incorrectly.

To test if the direct hostname is better for your application, you can compare it to the optimized hostname by running `netcat` from your application's host machine.

```shell  theme={null}
time nc -z us-east.connect.psdb.cloud 3306

Connection to us-east.connect.psdb.cloud port 3306 [tcp/mysql] succeeded!
nc -z us-east.connect.psdb.cloud 3306  0.01s user 0.00s system 26% cpu 0.037 total
```

<Note>
  If you have latency issues with the optimized hostname, we'd appreciate [hearing about your experience](https://docs/support.planetscale.com/hc/en-us/requests/new) so that we can improve.
</Note>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Connecting to PlanetScale privately on AWS
Source: https://planetscale.com/docs/vitess/connecting/private-connections



## Connecting to PlanetScale privately via AWS PrivateLink

When your compliance mandates that your connections do not route through the public Internet, PlanetScale provides private connection endpoints to AWS regions via [AWS PrivateLink](https://aws.amazon.com/privatelink/). AWS PrivateLink is a form of *VPC peering* that does not send your traffic over the public internet. Private connections are included on Scaler Pro plans. There is no additional charge on PlanetScale's end, but this may impact your AWS bill.

Below is a list of instructions to set up your Virtual Private Cloud (VPC) to utilize a VPC endpoint when communicating with PlanetScale databases.

## Establishing a VPC endpoint

<Steps>
  <Step>
    Identify the AWS region that your VPC lives in, which we will refer to as `<aws-region>` for the rest of this document.
  </Step>

  <Step>
    Navigate to the "Endpoints" section on the VPC page and select "**Create Endpoint**."

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/private-connections/create-new-endpoint.png?fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=5aa9d629a8171ed0a24924aa9bdfe3f9" alt="Create a new endpoint" data-og-width="3260" width="3260" data-og-height="1416" height="1416" data-path="docs/images/private-connections/create-new-endpoint.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/private-connections/create-new-endpoint.png?w=280&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=04cccfae2d06bb554b93ddb72d526739 280w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/private-connections/create-new-endpoint.png?w=560&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=d87e4b4df0a620e92e0cfba108de5db3 560w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/private-connections/create-new-endpoint.png?w=840&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=708cc295c15f88c77b011fc840cecf68 840w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/private-connections/create-new-endpoint.png?w=1100&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=90822113c5b31ef85fea041e0fea54ee 1100w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/private-connections/create-new-endpoint.png?w=1650&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=4dd17b94ddf4c59a7ff5ec11e4c008cf 1650w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/private-connections/create-new-endpoint.png?w=2500&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=5e07d735eae86bf4bea5cf1ba87ac4d6 2500w" />
    </Frame>
  </Step>

  <Step>
    Select "Endpoint services that use NLBs and GWLBs"

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/private-connections/type-of-endpoint.png?fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=240ffb56c6adc56c8291a756636ef4f1" alt="Menu to select endpoint type" data-og-width="3134" width="3134" data-og-height="2080" height="2080" data-path="docs/images/private-connections/type-of-endpoint.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/private-connections/type-of-endpoint.png?w=280&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=8e3ce9478f9ad1cb414d3dc89920bed1 280w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/private-connections/type-of-endpoint.png?w=560&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=af7d46ce33a13620c7d373c3c7276a23 560w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/private-connections/type-of-endpoint.png?w=840&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=1a361dfe9358990e754ad9daefadce48 840w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/private-connections/type-of-endpoint.png?w=1100&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=c694a8505444fe795ad387e13999191b 1100w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/private-connections/type-of-endpoint.png?w=1650&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=61b5a769dcec93c68ed9415e94ad8624 1650w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/private-connections/type-of-endpoint.png?w=2500&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=7eb27018ed8001cd46275bb5cd8f6743 2500w" />
    </Frame>

    | AWS Region Name           | AWS Region     | VPC Endpoint Service Name                                      |
    | :------------------------ | :------------- | :------------------------------------------------------------- |
    | US East (Ohio)            | us-east-2      | `com.amazonaws.vpce.us-east-2.vpce-svc-069f88c102c1a7fba`      |
    | US East (N. Virginia)     | us-east-1      | `com.amazonaws.vpce.us-east-1.vpce-svc-02fef31be60d3fd35`      |
    | US West (Oregon)          | us-west-2      | `com.amazonaws.vpce.us-west-2.vpce-svc-0f63a383cb2d41919`      |
    | Asia Pacific (Mumbai)     | ap-south-1     | `com.amazonaws.vpce.ap-south-1.vpce-svc-06556ed2371c5fdd2`     |
    | Asia Pacific (Singapore)  | ap-southeast-1 | `com.amazonaws.vpce.ap-southeast-1.vpce-svc-046d8feae38660302` |
    | Asia Pacific (Sydney)     | ap-southeast-2 | `com.amazonaws.vpce.ap-southeast-2.vpce-svc-03e5578eeaf446c90` |
    | Asia Pacific (Tokyo)      | ap-northeast-1 | `com.amazonaws.vpce.ap-northeast-1.vpce-svc-099c246fa320e54d1` |
    | Europe (Frankfurt)        | eu-central-1   | `com.amazonaws.vpce.eu-central-1.vpce-svc-091260498e58d4dc3`   |
    | Europe (Ireland)          | eu-west-1      | `com.amazonaws.vpce.eu-west-1.vpce-svc-049577caa775e8648`      |
    | Europe (London)           | eu-west-2      | `com.amazonaws.vpce.eu-west-2.vpce-svc-0f69e183c9a555f03`      |
    | South America (São Paulo) | sa-east-1      | `com.amazonaws.vpce.sa-east-1.vpce-svc-09b11604d399b5c58`      |
    | Canada (Montreal)         | ca-central-1   | `com.amazonaws.vpce.ca-central-1.vpce-svc-0617a00ea4e327520`   |

    * Canada (Central) |
  </Step>

  <Step>
    Fill the "Service name" text box according to which region you want to establish AWS PrivateLink for. Once you have filled in the Service Name text box, click "Verify service".

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/private-connections/service-name-and-verification.png?fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=4b1d3bb6215941d86393a8a9619d8785" alt="Endpoint service name and verification" data-og-width="3772" width="3772" data-og-height="1276" height="1276" data-path="docs/images/private-connections/service-name-and-verification.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/private-connections/service-name-and-verification.png?w=280&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=9b2b153ed5ae25f9b6667d06a091b5e7 280w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/private-connections/service-name-and-verification.png?w=560&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=660e249f5da6c677e2c85e3bce95e2be 560w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/private-connections/service-name-and-verification.png?w=840&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=bd9fb24799b1b029b9221f1183265cd5 840w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/private-connections/service-name-and-verification.png?w=1100&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=06c35eefe97f1f35dfc783d373cf7041 1100w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/private-connections/service-name-and-verification.png?w=1650&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=608a6b83f64188e5d258f9f9a35f2c89 1650w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/private-connections/service-name-and-verification.png?w=2500&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=e6ddf72865e0abeca07d46676ec2a26d 2500w" />
    </Frame>
  </Step>

  <Step>
    Choose the VPC and subnets that should be peered with the PlanetScale service endpoint.
  </Step>

  <Step>
    Click the "Additional settings" dropdown arrow to reveal the "DNS name" configuration, and select the "Enable DNS name" checkbox.
  </Step>

  <Step>
    Choose the security group of your choice to control what can send traffic to the PlanetScale service endpoint.
  </Step>

  <Step>
    Click "Create endpoint" and verify that the VPC endpoint's status reports "Available" after a few minutes.
  </Step>
</Steps>

## Verifying the connectivity of your VPC endpoint

<Steps>
  <Step>
    In the AWS UI, confirm that the endpoint has successfully been created by verifying that the Status section of the endpoint reads "Available".

    <Warning>
      Some PlanetScale regions are named differently than AWS Provider regions. We will refer to the PlanetScale region as `<planetscale-region>` for the rest of this document.
    </Warning>
  </Step>

  <Step>
    Confirm that the Private DNS Names reads: `<planetscale-region>.private-connect.psdb.cloud`.
  </Step>

  <Step>
    Log into any EC2 instance in the configured VPC and run `dig +short <planetscale-region>.private-connect.psdb.cloud` to confirm that DNS resolution is producing IP Addresses in the range of your VPC's CIDR.

    ```bash  theme={null}
    dig +short us-east.private-connect.psdb.cloud
    172.31.16.197
    172.31.13.7
    ```
  </Step>

  <Step>
    Run `curl https://<planetscale-region>.private-connect.psdb.cloud` to verify your connectivity. A successful response will yield `Welcome to PlanetScale`.

    ```
    curl https://us-east.private-connect.psdb.cloud
    Welcome to PlanetScale.
    ```
  </Step>
</Steps>

## Modifying your Connection Strings to utilize your VPC endpoint.

By default, PlanetScale provides users with a connection string that reads `<planetscale-region>.connect.psdb.cloud`.

To utilize your newly configured VPC endpoint, prepend `private-` to the `connect` subdomain as shown above, yielding a connection string that reads `<planetscale-region>.private-connect.psdb.cloud`.

With this configured, you can leverage VPC peering to communicate between your AWS account and PlanetScale.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Connecting to PlanetScale privately on GCP
Source: https://planetscale.com/docs/vitess/connecting/private-connections-gcp



## Connecting to PlanetScale privately via GCP Private Service Connect

When your compliance mandates that your connections do not route through the public Internet, PlanetScale provides private connection endpoints to GCP regions via [GCP Private Service Connect](https://cloud.google.com/vpc/docs/private-service-connect). GCP Private Service Connect is a form of *VPC peering* that keeps your traffic within Google Cloud. Private connections are included on Scaler Pro plans. There is no additional charge on PlanetScale's end, but this may impact your GCP bill.

Below is a list of instructions to set up your VPC network to utilize a Private Service Connect endpoint when communicating with PlanetScale databases.

## Establishing a Private Service Connect Endpoint

<Steps>
  <Step>
    Identify the GCP region that your VPC lives in, which we will refer to as `<gcp-region>` for the rest of this document.
  </Step>

  <Step>
    In the GCP console, go to ["Network Service > Private Service Connect"](https://console.cloud.google.com/net-services/psc) page, select the "**Connected endpoints**" tab, and select the "**+ Connect endpoint**" button.
  </Step>

  <Step>
    Add a Private Service Connect Endpoint with the following details:

    * **Target**: Published Service.
    * **Target Service**: Select the target service from the table below for the region you want to establish an endpoint in.
    * **Name**: Pick any endpoint name. The examples in this document use `"edge"`.
    * **Network and subnet**: Select the network (VPC) to create the endpoint in. The endpoint will reserve a static IP address in the subnet. The VPC and subnet must be reachable by the applications you intend to connect to your PlanetScale databases from.
    * **Create an IP Address**: Create a reserved IP address for the endpoint. This is the address your applications will use to access your PlanetScale databases.
    * **Enable Global Access**: PlanetScale recommends enabling this option. When enabled, this allows applications in other regions of your VPC to reach the PSC endpoint.
    * Finally, click **Add Endpoint** to start the process. Setup will take approximately 1-2 minutes.

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/multi/gcp/private-service-connect/connect_endpoint_details.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=4c5a06499d0e6700029f4c4c693d6778" alt="Setup endpoint details" data-og-width="825" width="825" data-og-height="892" height="892" data-path="docs/images/assets/docs/multi/gcp/private-service-connect/connect_endpoint_details.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/multi/gcp/private-service-connect/connect_endpoint_details.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=661660327bd31b9f39de96cf74adb06a 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/multi/gcp/private-service-connect/connect_endpoint_details.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=0e3b52977a8d6eea944d0604d31b6f87 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/multi/gcp/private-service-connect/connect_endpoint_details.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=ec2f13018a0ec2f418c76828baff1a7e 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/multi/gcp/private-service-connect/connect_endpoint_details.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=434f024cff833835162d1d64fd191879 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/multi/gcp/private-service-connect/connect_endpoint_details.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=5334ef996c6e408c69d6660e94c2252d 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/multi/gcp/private-service-connect/connect_endpoint_details.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=03750181ffa1c4d5e70d93e87a7250dd 2500w" />
    </Frame>

    | GCP Region              | Target Service                                                                                                                | Domain Name                                            |
    | :---------------------- | :---------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------- |
    | asia-northeast3         | `projects/planetscale-production/regions/asia-northeast3/serviceAttachments/edge-gateway-gcp-asia-northeast3`                 | gcp-asia-northeast3.private-connect.psdb.cloud         |
    | europe-west1            | `projects/planetscale-production/regions/europe-west1/serviceAttachments/edge-gateway-gcp-europe-west1`                       | gcp-europe-west1.private-connect.psdb.cloud            |
    | northamerica-northeast1 | `projects/planetscale-production/regions/northamerica-northeast1/serviceAttachments/edge-gateway-gcp-northamerica-northeast1` | gcp-northamerica-northeast1.private-connect.psdb.cloud |
    | us-central1             | `projects/planetscale-production/regions/us-central1/serviceAttachments/edge-gateway-gcp-us-central1`                         | gcp-us-central1.private-connect.psdb.cloud             |
    | us-east4                | `projects/planetscale-production/regions/us-east4/serviceAttachments/edge-gateway-gcp-us-east4`                               | gcp-us-east4.private-connect.psdb.cloud                |
  </Step>

  <Step>
    The endpoint creation process will take a minute or two. When finished, select the endpoint and verify the status is **Accepted**:

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/multi/gcp/private-service-connect/endpoint_status.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=a49e348b96a865b131b575fd8d125f33" alt="Showing endpoint status as 'Accepted'" data-og-width="1115" width="1115" data-og-height="433" height="433" data-path="docs/images/assets/docs/multi/gcp/private-service-connect/endpoint_status.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/multi/gcp/private-service-connect/endpoint_status.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=f18e3a36bd630ae16f29eb24f1f4ed29 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/multi/gcp/private-service-connect/endpoint_status.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=958a90fc5720a9f985bfef8f366c9cf5 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/multi/gcp/private-service-connect/endpoint_status.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=70970e7b1b8648bf93de9dc072af02e9 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/multi/gcp/private-service-connect/endpoint_status.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=3e98e88bd496f411c69c07b6d7f17b39 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/multi/gcp/private-service-connect/endpoint_status.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=d2bc6ebf7c80b04b683a5b01a3dbdba3 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/multi/gcp/private-service-connect/endpoint_status.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=c7d3d2acc05a2c0a02b6ed8ac1cfebfe 2500w" />
    </Frame>
  </Step>
</Steps>

## Verifying the connectivity of your Private Service Connect endpoint

GCP will automatically create a private Cloud DNS zone in the project where the PSC consumer endpoints are created.

The domain name depends on the region the consumer endpoint was created in. Refer to the table above. The format of the domain name will be:

* `<Endpoint-Name>.<Domain-Name>`

For example, if you chose `edge` as the endpoint name in the `us-central1` region, the domain name for the endpoint would be:

* `edge.gcp-us-central1.private-connect.psdb.cloud`

<Steps>
  <Step>
    Log into any VM instance in the configured VPC and run `dig +short <Endpoint-Name>.<Domain-Name>` to confirm that DNS resolution resolves to the static IP address reserved during endpoint creation.

    ```
    $ dig +short edge.gcp-us-central1.private-connect.psdb.cloud
    10.128.0.17
    ```
  </Step>

  <Step>
    Run `curl https://<Endpoint-Name>.<Domain-Name>` to verify your connectivity. A successful response will yield `Welcome to PlanetScale`.

    ```
    curl https://edge.gcp-us-central1.private-connect.psdb.cloud
    Welcome to PlanetScale.
    ```
  </Step>
</Steps>

## Modifying your Connection Strings to utilize your Private Service Connect endpoint

By default, PlanetScale provides connection strings based on the `connect.psdb.cloud` domain name. To access your databases over the private endpoint change your connection string to match the `<Endpoint-Name>.<Domain-Name>` pattern.

For example, a connection string such as `gcp-us-central1.connect.psdb.cloud` would be changed to `edge.gcp-us-central1.private-connect.psdb.cloud` assuming `edge` was the Endpoint Name chosen during creation of the endpoint.

With this configured, you can leverage VPC peering to communicate between your GCP account and PlanetScale.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Connecting to PlanetScale securely
Source: https://planetscale.com/docs/vitess/connecting/secure-connections



When you are using passwords to connect to PlanetScale, it is essential to correctly validate the server side certificate that PlanetScale provides. If you don't configure it properly, your connection will be vulnerable to man-in-the-middle attacks. The server certificates that PlanetScale uses are signed by a commonly available system root. How to configure this properly depends on the client you are using to connect to MySQL.

<Note>
  We don't guarantee we will continue to sign our certificates using the same CA. We reserve the right to change our CA but guarantee that we will use one that is provided on all common systems by default.
</Note>

## MySQL command line client

With the MySQL command line client, you need to use the `VERIFY_IDENTITY` mode and provide the path to the system roots on your system. The examples below use the most common Linux configuration, but the path might be different in your case. Please check the [CA root configuration](#ca-root-configuration) below to find the appropriate path for your operating system and distribution.

```bash  theme={null}
mysql --ssl-mode=VERIFY_IDENTITY --ssl-ca=/etc/ssl/certs/ca-certificates.crt
```

## libmysqlclient based clients

Many clients are based on `libmysqlclient`. These clients require configuration options similar to the command line MySQL client, as they are using the same driver to connect to PlanetScale. Please reference the documentation of the driver that you are using for how to configure the SSL CA path. The value provided can be found in the [CA root configuration](#ca-root-configuration).

Again, if you don't configure this, things will work but your connection to PlanetScale will be vulnerable to man-in-the-middle attacks.

For a list of tested MySQL GUI clients, review our article on [how to connect MySQL GUI applications](/docs/vitess/tutorials/connect-mysql-gui).

<Note>
  Some MySQL clients, like Sequel Pro, do not have full SSL support and will not work with PlanetScale.
</Note>

## Certificate Authorities

### What is a Certificate Authority (CA)?

A [Certificate Authority (CA)](https://en.wikipedia.org/wiki/Certificate_authority) is a trusted party that signs digital certificates used to identify websites and other services. Their most well known use is signing the certificates used by websites serving content over [HTTPS](https://en.wikipedia.org/wiki/HTTPS). Using a signed certificate means that you can trust that you really are communicating with the website that you think you are, without someone else listening in.

### Why is a Certificate Authority (CA) needed for TLS / SSL?

Encryption alone won't provide trust, only confidentiality. Without a way to verify the entity at the other end of a secure connection, you risk establishing an encrypted connection to a malicious party. This means that TLS / SSL only provides real security when certificates are validated against specific trusted Certificate Authorities.

### What is a CA root store?

In day-to-day browsing you don't need to specify which CAs to trust, your operating system and / or browser come with a list of trusted authorities. These trusted authorities verify the identities of domain owners creating HTTPS certificates, and sign those certificates so that you can trust them as well. The root store of certificates from Certificate Authorities is a collection of certificates that are trusted to sign other certificates.

### Why do I need to care about a CA when connecting to PlanetScale?

PlanetScale uses certificates from a [Certificate Authority (CA)](https://en.wikipedia.org/wiki/Certificate_authority) that is part of the system root available on almost all platforms. We do this so that we can provide the easiest possible way for you to connect securely to PlanetScale. No one should be able to listen in on traffic, and no one should be able to impersonate PlanetScale to trick you into connecting to them.

Operating systems all come with CA root stores, but not every MySQL driver uses these. Drivers like those for Go, Java or .NET will use the system roots. This means they can provide an experience similar to your browser when connecting to PlanetScale. For these drivers you only have to indicate that you want to use TLS / SSL and that you want to run it in a mode that verifies the identity of PlanetScale. That way you can trust that you are safely connected to PlanetScale when the handshake completes.

Many other drivers don't automatically load the system roots. Most notably, the `libmysqlclient` C driver and all other drivers that use it, such as the drivers often used in Ruby (`mysql2`) and Python (`mysqlclient`). This means you will need to specify the path to the CA certificates you want to trust. The best option is to point the driver at your system roots.

Below we have listed common paths on which the CA root store can be found in various operating systems and distributions. Only configure this path if your driver absolutely needs it and it won't work without it. If you are using a driver where the system roots are loaded by default, such as Go, Java or .NET, we strongly recommend not configuring this option and depending on the default behavior of your driver.

### CA root configuration

#### Linux

On Linux, the path to the system CA roots depends on the distribution that you are using.

#### Debian / Ubuntu / Gentoo / Arch / Slackware

This path also applies to Debian or Ubuntu derivatives. You need to make sure the `ca-certificates` package is installed.

```
/etc/ssl/certs/ca-certificates.crt
```

#### RedHat / Fedora / CentOS / Mageia / Vercel / Netlify

This path also applies to RedHat or Fedora derivatives like Amazon Linux and Oracle Linux. This is the path to use for applications deployed on Vercel and Netlify.

```
/etc/pki/tls/certs/ca-bundle.crt
```

#### Alpine

This is a commonly used distribution for Docker containers.

```
/etc/ssl/cert.pem
```

#### OpenSUSE

This also applies to OpenSUSE derivatives.

```
/etc/ssl/ca-bundle.pem
```

#### MacOS / FreeBSD / OpenBSD

MacOS provides an extracted version of the system roots on disk that can be used for the CA roots. On FreeBSD you need to install the `ca_root_nss` package for this path to be available.

```
/etc/ssl/cert.pem
```

#### Windows

Windows does not provide a file with the CA roots that can be used by your driver. Many languages often used on Windows like C#, Java or Go do not need the CA root path and will use the Windows internal system roots by default. In those environments, you don't need to specify a root CA list.

If you are using a language that requires specifying the CA root path, like C or PHP, the [`curl`](https://curl.se) project provides an extracted bundle of root certificates from the [Mozilla CA Certificate program](https://wiki.mozilla.org/CA). You can download the bundle at [https://curl.se/docs/caextract.html](https://curl.se/docs/caextract.html). Once you download the file, you can point at it with the correct configuration options for the driver that you are using.

<Note>
  We strongly discourage downloading only the current CA that we are using, as PlanetScale reserves the right to change
  our CA. We will however always use a CA that is commonly available.
</Note>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Deleting a database
Source: https://planetscale.com/docs/vitess/delete-a-database

How to remove a database from PlanetScale.

To access the settings for deleting a database:

<Steps>
  <Step>
    From the PlanetScale organization dashboard, select your database
  </Step>

  <Step>
    Navigate to **Settings** on the left sidebar menu
  </Step>
</Steps>

## Delete Database

The settings page provides the option to permanently delete your database. This action is irreversible and will:

* Delete the entire database and all of its branches
* Remove all data and backups permanently
* Disconnect any applications currently connected to the database
* Include usage charges in your invoice through the deletion date

<Warning>
  Database deletion cannot be undone. Make sure you have proper backups and that no critical applications depend on this database before proceeding.
</Warning>

To delete a database:

<Steps>
  <Step>
    Scroll to the bottom of the Settings page
  </Step>

  <Step>
    Click the red **Delete database** button
  </Step>

  <Step>
    Enter the database name
  </Step>

  <Step>
    Click **Delete database** to confirm
  </Step>
</Steps>

Only organization administrators and database administrators have permission to delete databases. See the [Access Control documentation](/docs/security/access-control) for more information about user permissions.

## IP Restrictions

If you would like to 'soft delete' your database first, you can instead enable IP restrictions on the passwords for that database.

To enable IP restrictions:

<Steps>
  <Step>
    From the **Settings** page, select **Passwords** from the list
  </Step>

  <Step>
    Click the three dots button next to the password you wish to restrict
  </Step>

  <Step>
    Click the **Manage IP restrictions** button
  </Step>

  <Step>
    Add in an IP range that is NOT used by any clients and click the **Add** button
  </Step>
</Steps>

This will restrict the IP range to what is specified and can be used to test if connection are going to the database via the password in question.
This is a useful test for determining if the password in question is still in use, before deleting either the database or the password.
You should also confirm in [Insights](/docs/vitess/monitoring/query-insights) that no queries are executing on the database.

This helps avoid remaking passwords that are still in use or deleting a database that is still in use.

Once you've confirmed that no traffic is using any of your database passwords, you can follow the steps above to delete the database if desired and safe.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale Connect
Source: https://planetscale.com/docs/vitess/etl

PlanetScale Connect is one of the underlying mechanisms we use to transfer data between parties.

With PlanetScale Connect, you can integrate with supported ETL platforms to extract data from your PlanetScale database and safely load it into other destinations for analysis, transformation, and more.

## ETL integrations

We currently support the following connectors:

<Columns cols={2}>
  <Card title="Airbyte" icon="angles-right" horizontal href="/docs/vitess/integrations/airbyte" />

  <Card title="Debezium" icon="angles-right" horizontal href="/docs/vitess/integrations/debezium" />

  <Card title="Fivetran" icon="angles-right" horizontal href="/docs/vitess/integrations/fivetran" />

  <Card title="Hightouch" icon="angles-right" horizontal href="/docs/vitess/integrations/hightouch" />

  <Card title="Stitch" icon="angles-right" horizontal href="/docs/vitess/integrations/stitch" />
</Columns>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Foreign key constraints
Source: https://planetscale.com/docs/vitess/foreign-key-constraints



## What is a foreign key constraint?

A **foreign key** is a logical association of rows between two tables in a parent-child relationship. A row in a “parent” table may be referenced by one or more rows in a “child” table. A foreign key typically suggests how you should `JOIN` tables in most queries.

A **`FOREIGN KEY` *constraint*** is a database construct, an implementation that *forces* the foreign key relationship's integrity (referential integrity). Namely, it ensures that a child table can only reference a parent table when the appropriate row *exists* in the parent table. A constraint also prevents the existence of “orphaned rows” in different methods.

### Advantages and disadvantages of foreign key constraints

Foreign key constraints have advantages and disadvantages. While foreign key constraints can help ensure referential integrity, they will cause degraded performance in high concurrency workloads and introduce more complexity in the database. Often, foreign key constraints become problematic when operating on a large scale. You can read more [why we do not recommend foreign key constraints](/docs/vitess/operating-without-foreign-key-constraints#why-does-planetscale-not-recommend-constraints) for some applications.

We recommend weighing the advantages and disadvantages for your specific application when using foreign key constraints. If you decide to enforce referential integrity at the application level instead of at the database level, see our documentation on [how to design systems that maintain referential integrity without foreign key constraints](/docs/vitess/strategies-for-maintaining-referential-integrity).

## Foreign key constraints on PlanetScale

### Prerequisites

<Note>
  You must enable foreign key constraint support on your database in the PlanetScale database settings.
</Note>

Before you enable foreign key constraint support, there are a few important things to know:

* **No open deploy requests:** You cannot have any open deploy requests before enable foreign key constraint support.
* **Possible orphaned rows on reverts:** Some deploy requests may result in orphaned rows if you revert them. You will be warned before you deploy changes.
* **Database upgrades:** When you enable foreign key constraint support, we upgrade the MySQL version in some cases. You should experience no downtime during this upgrade, but it can take a few minutes to complete. Older databases may take longer.

### How to enable foreign key constraints

Foreign key constraints can be enabled on a **per database** level, not the organization level.

<Steps>
  <Step>
    Navigate to the database you want to enable foreign key constraint support for, and select the **“Settings”** tab.
  </Step>

  <Step>
    Under **"General"**, click the check box next to "Allow foreign key constraints", and click **"Save database settings"**.
  </Step>

  <Step>
    After enable foreign key constraint support, PlanetScale may upgrade your database in the background, which could take several seconds. As always, database upgrades do not involve any downtime or locking and require no action on your part.
  </Step>
</Steps>

## Database imports with foreign key constraints

You can import a database with foreign key constraints to PlanetScale. We will automatically detect them after successfully connecting to your external database. If we find any foreign key constraints, we automatically enable foreign key constraint support and continue the import process.

We recommend using a replica as your source when doing database imports with foreign key constraints. Read more in the [database import documentation](/docs/vitess/imports/database-imports#foreign-key-constraints).

## Limitations

For most cases, foreign key constraints should work as expected in PlanetScale. There are a few cases to be aware of as they are currently unsupported or result in less ideal behavior.

### Sharded versus unsharded environments

Currently, the foreign key constraints are only supported in unsharded environments. If you use PlanetScale in a sharded environment, contact your PlanetScale account manager for more information.

### Deploy request limitations

Deploy requests do not validate the referential integrity of *existing* columns. `ALTER TABLE… ADD FOREIGN KEY…` does not validate existing row relations within the context of a deploy request. Unlike standard MySQL, it is possible to add the foreign key constraint to a table with orphaned rows, and they will remain orphaned. In standard MySQL, adding a foreign key is a blocking operation, and it fails if any orphaned rows are found.

#### Revert limitations

In some cases, a revert of a deploy request can result in orphaned rows. When you revert:

* Dropping a foreign key constraint: Once a foreign key constraint is dropped, new data written to the table is less constrained. Reverting this change may result in data that is inconsistent with the dropped foreign key constraint.
* Dropping a table with foreign key constraints: When a table with foreign key constraints is dropped, the parent table(s) will continue to be written to. If this change is reverted, data in the table that was dropped may no longer be consistent with its foreign key constraints.

### Unsupported queries

The following queries are currently unsupported when the query leads to `child` table `CASCADE` or `SET NULL`:

#### `INSERT... ON DUPLICATE KEY UPDATE` statement

For example:

```sql  theme={null}
INSERT INTO tbl (id, col) values (1, 2) ON DUPLICATE KEY UPDATE SET col = 3;
```

#### `REPLACE INTO... SELECT...` statement

For example:

```sql  theme={null}
REPLACE INTO tbl SELECT * FROM tbl2;
```

#### `UPDATE` for a foreign key column statement and referencing the column being updated in the same `UPDATE` statement

For example:

```sql  theme={null}
UPDATE tbl SET col = 5, fk_col = col + 1;
```

The workaround is to separate the updates for conflicting columns into separate schema changes.

#### Cyclic foreign keys

It is possible to create self-referencing tables as well as a groups of tables which compose a cyclic foreign key (where tables reference each other in a loop). There is no restriction to the schema design, but in some scenarios queries will be rejected:

* A cycle where the participating foreign keys all have `ON DELETE CASCADE` rule. Example:

```sql  theme={null}
CREATE TABLE `employee` (
	`id` bigint unsigned NOT NULL AUTO_INCREMENT,
	`manager_id` bigint unsigned NOT NULL,
	PRIMARY KEY (`id`),
	KEY `idx_manager_id` (`manager_id`),
	CONSTRAINT `self_referencing_key_with_cascade` FOREIGN KEY (`manager_id`) REFERENCES `employee` (`id`) ON DELETE CASCADE
);
```

* A cycle where all rules are either `SET NULL` or `CASCADE`, and the loop ends up having columns reference themselves. Example:

```sql  theme={null}
CREATE TABLE `t1` (
	`id` int NOT NULL,
	`i` int,
	PRIMARY KEY (`id`),
	KEY `i_fk` (`i`),
	CONSTRAINT `i_fk` FOREIGN KEY (`i`) REFERENCES `t2` (`j`) ON UPDATE CASCADE ON DELETE SET NULL
);
CREATE TABLE `t2` (
	`id` int NOT NULL,
	`j` int,
	PRIMARY KEY (`id`),
	KEY `j_fk` (`j`),
	CONSTRAINT `j_fk` FOREIGN KEY (`j`) REFERENCES `t1` (`i`) ON UPDATE SET NULL ON DELETE CASCADE
);
```

#### Foreign key constraint names change on every deployment

This is mainly due to the MySQL limitation (compatible with ANSI SQL specification), where constraint names must be unique to the schema.

For example, in your database branch, you may run the following:

```sql  theme={null}
create table parent (id int primary key);
create table child (id int primary key, pid int, constraint pchild_fk foreign key (pid) references parent (id));
```

But after you deploy your schema changes:

```sql  theme={null}
show create table child \G
CREATE TABLE `child` (
  `id` int NOT NULL,
  `pid` int DEFAULT NULL,
  PRIMARY KEY (`id`),
  KEY `pchild_fk_5vtaqz7kepok6wa91vryrkrje` (`pid`),
  CONSTRAINT `pchild_fk_5vtaqz7kepok6wa91vryrkrje` FOREIGN KEY (`pid`) REFERENCES `parent` (`id`)
)
```

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Sending Prometheus Metrics to Datadog
Source: https://planetscale.com/docs/vitess/guides/prometheus-metrics-datadog

If you're looking for more metrics than PlanetScale's native Datadog integration provides, this tutorial will show how to configure your [Datadog agent](https://docs.datadoghq.com/agent/) to scrape PlanetScale's [Prometheus infrastructure](/docs/vitess/integrations/prometheus) automatically, allowing you to collect detailed metrics for all of your PlanetScale branches.

## Overview

In this tutorial, we'll assume that you have a Datadog Agent Version 7 running. For more information on what the Datadog Agent is and how to install it, start with the [Datadog Agent documentation](https://docs.datadoghq.com/getting_started/agent/).

For the purposes of this guide, we'll be using a Datadog agent running with the recommended installation steps on a Linux system.

## Prerequisites

You'll need a working Datadog agent and access to add a [Custom Agent Check](https://docs.datadoghq.com/developers/custom_checks/) to that instance. This may require `root` or `sudo` access on the machine running the Datadog agent.

You'll also need a [Service token](/docs/api/reference/service-tokens) in your Organization, with the `read_metrics_endpoints` permission granted.

## Adding the Plugin to the Datadog Agent

Go to [https://github.com/planetscale/planetscale-datadog](https://github.com/planetscale/planetscale-datadog), which is the repository that has our custom OpenMetrics Check.

Place the unedited `planetscale.py` in the `checks.d` directory of your Datadog Agent.

* On Linux, that is `/etc/datadog-agent/checks.d/`
* On macOS, that is `/opt/datadog-agent/etc/checks.d/`

Make sure that it belongs to the appropriate user. If you're using the recommended Linux installation steps, it will have created a `dd-agent` user:

```
$ pwd
/etc/datadog-agent/checks.d
$ ls -al planetscale.py
-rw-r--r-- 1 dd-agent dd-agent 9261 Apr  2 22:54 planetscale.py
```

This file is owned by the `dd-agent` user and group in the `/etc/datadog-agent/checks.d` directory.

If you're on macOS, it will depend on whether you installed the agent as a 'Single User Agent' or a 'Systemwide Agent'. If you picked Single User, there should be no additional permission changes needed. If you installed it as a Systemwide agent, make sure the user and group you installed the agent with as ownership of the file.

## Configuring the Datadog Agent

Now that we have the plugin installed, we need to configure it. In the `conf.d` directory of the Datadog agent take the `conf.d/planetscale.yaml.example` file and edit it with your organization name and Service Token information. It should look like this:

```bash expandable theme={null}
instances:
  - planetscale_organization: 'nick' # Required: Your PlanetScale organization ID
    ps_service_token_id: '${TOKEN_ID}' # Required: Your PlanetScale Service Token ID
    ps_service_token_secret: '${TOKEN}' # Required: Your PlanetScale Service Token Secret. Consider using Datadog secrets management: https://docs.datadoghq.com/agent/guide/secrets-management/

    namespace: 'planetscale' # Required: Namespace for the metrics
    metrics: # Required: List of metrics to collect. Use mapping for renaming/type overrides.
      - planetscale_vtgate_queries_duration: vtgate_query_duration

    min_collection_interval: 60
    send_distribution_buckets: true
    collect_counters_with_distributions: true
```

This configures the integration to look for all of the branches in the `"nick"` PlanetScale organization, only collect the `planetscale_vtgate_queries_duration` metric, which it will rename `vtgate_query_duration` and put it inside of the `planetscale` namespace.

Save the file at `planetscale.yaml`, making sure to double check permissions:

```
$ pwd
/etc/datadog-agent/conf.d
$ ls -al planetscale.yaml
-rw-r--r-- 1 root root 1518 Apr  2 22:57 planetscale.yaml
```

## Restart the Datadog Agent

Now that this is configured and installed, restart the Agent:

```
$ sudo systemctl restart datadog-agent
```

## Validating the PlanetScale Plugin

Now that the Datadog Agent is running the PlanetScale plugin, metrics should start flowing into Datadog within a couple of minutes. To validate, we can ask the Datadog Agent:

```
sudo -u dd-agent -- datadog-agent check planetscale
```

If the plugin is installed successfuly, this should output the scrape targets for your branches, as well as metadata about when it was last run and how many metrics were emitted:

```bash expandable theme={null}
$ sudo -u dd-agent -- datadog-agent check planetscale
=== Service Checks ===
[
  {
    "check": "planetscale.api.can_connect",
    "host_name": "ubuntu",
    "timestamp": 1743638192,
    "status": 0,
    "message": "",
    "tags": [
      "planetscale_org:nick"
    ]
  },
  {
    "check": "planetscale.prometheus.health",
    "host_name": "ubuntu",
    "timestamp": 1743638192,
    "status": 0,
    "message": "",
    "tags": [
      "endpoint:https://metrics.psdb.cloud/metrics/branch/7wxuxewx4l0p?..."
    ]
  },
  {
    "check": "planetscale.prometheus.health",
    "host_name": "ubuntu",
    "timestamp": 1743638192,
    "status": 0,
    "message": "",
    "tags": [
      "endpoint:https://metrics.psdb.cloud/metrics/branch/6o0rr27785fl?..."
    ]
  }
]


  Running Checks
  ==============

    planetscale (unversioned)
    -------------------------
      Instance ID: planetscale:planetscale:8d4d64f696d967be [OK]
      Configuration Source: file:/etc/datadog-agent/conf.d/planetscale.yaml
      Total Runs: 1
      Metric Samples: Last Run: 0, Total: 0
      Events: Last Run: 0, Total: 0
      Service Checks: Last Run: 3, Total: 3
      Histogram Buckets: Last Run: 77, Total: 77
      Average Execution Time : 809ms
      Last Execution Date : 2025-04-02 23:56:32 UTC (1743638192000)
      Last Successful Execution Date : 2025-04-02 23:56:32 UTC (1743638192000)


  Metadata
  ========
    config.hash: planetscale:planetscale:8d4d64f696d967be
    config.provider: file
```

The Service Checks show that it has successfully connected to the PlanetScale API to request information about how to scrape for the branches in my organization, and it has successfully scraped both of what it discovered.

We can also see that it successfully executed at `2025-04-02 23:56:32 UTC` and produced 77 Histogram Buckets.

## Adding Metrics

In our earlier configuration, we only added one metric. For a complete list of what PlanetScale exposes, please take a look at our [Metrics Reference Documentation](/docs/vitess/integrations/prometheus-metrics).

Note that the Datadog agent [normalizes metrics with certain suffixes starting in v7.32.0](https://github.com/DataDog/integrations-core/blob/master/openmetrics/README.md):

> Starting in Datadog Agent v7.32.0, in adherence to the OpenMetrics specification standard, counter names ending in \_total must be specified without the \_total suffix. For example, to collect promhttp\_metric\_handler\_requests\_total, specify the metric name promhttp\_metric\_handler\_requests. This submits to Datadog the metric name appended with .count, promhttp\_metric\_handler\_requests.count.

This means that to scrape a metric such as `planetscale_mysql_bytes_received_total`, you would configure the Datadog agent for `planetscale_mysql_bytes_received`.

If I want to collect additional metrics, I can add them to the list:

```
    metrics: # Required: List of metrics to collect. Use mapping for renaming/type overrides.
      - planetscale_vtgate_queries_duration: vtgate_query_duration
      - planetscale_edge_active_connections: active_connections
```

Then, restart the Datadog Agent:

```
$ sudo systemctl restart datadog-agent
```

If I check the status of the PlanetScale Plugin, I can see our last run added a Metric Sample:

```bash  theme={null}
  Running Checks
  ==============

    planetscale (unversioned)
    -------------------------
      Instance ID: planetscale:planetscale:fde586b60a54a38f [OK]
      Configuration Source: file:/etc/datadog-agent/conf.d/planetscale.yaml
      Total Runs: 1
      Metric Samples: Last Run: 1, Total: 1
      Events: Last Run: 0, Total: 0
      Service Checks: Last Run: 3, Total: 3
      Histogram Buckets: Last Run: 77, Total: 77
      Average Execution Time : 826ms
      Last Execution Date : 2025-04-03 00:01:45 UTC (1743638505000)
      Last Successful Execution Date : 2025-04-03 00:01:45 UTC (1743638505000)
```

In the Datadog UI, I can see data for the `planetscale.active_connections` metric:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/prometheus-datadog-graph.png?fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=af198d4ffbb6a7ca9bd821afd5d79d83" alt="Datadog Connections Metric" data-og-width="2770" width="2770" data-og-height="1206" height="1206" data-path="docs/vitess/tutorials/prometheus-datadog-graph.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/prometheus-datadog-graph.png?w=280&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=ba4fabc1030fe00f62d363eb04438b17 280w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/prometheus-datadog-graph.png?w=560&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=5d9107c8dc8cf34d267a57fc59179006 560w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/prometheus-datadog-graph.png?w=840&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=80385f6b51c3a50fcc2c507d8317a061 840w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/prometheus-datadog-graph.png?w=1100&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=1dd84603686131e0547c6931a5e68045 1100w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/prometheus-datadog-graph.png?w=1650&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=7bd1a120cff8194ad5069748dd7b5ef5 1650w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/prometheus-datadog-graph.png?w=2500&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=8b08666bcb27fcc7d57ec5c0aca1b2ba 2500w" />
</Frame>

## What's Next?

Now that you're sending a couple of metrics from PlanetScale to Datadog, take a look at our [full list](/docs/vitess/integrations/prometheus-metrics) and start building dashboards!

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Grafana Dashboard for PlanetScale Branches
Source: https://planetscale.com/docs/vitess/guides/prometheus-metrics-grafana

In this tutorial, you'll learn how to set up Grafana and connect it to a Prometheus instance to see metrics about your PlanetScale database.

## Introduction

This guide requires that you've set up a Prometheus instance from our documentation.

If you're already running Grafana in production and you're just looking for our standard dashboard template, you can find it [on GitHub](https://github.com/planetscale/grafana-dashboard).

## Install Grafana

Grafana's [installation documentation](https://grafana.com/docs/grafana/latest/setup-grafana/installation/) contains information for their supported platforms. For this guide, we'll be setting this up locally on a macOS machine.

If you're using a hosted Grafana option such as [Grafana Cloud](https://grafana.com/products/cloud/) or [AWS Managed Grafana](https://aws.amazon.com/grafana/) you can skip this step.

On macOS, Grafana is availabile via [homebrew](https://brew.sh/), and I can install it with:

```bash  theme={null}
$ brew install grafana
```

This will download and install Grafana, and I can start it with:

```bash  theme={null}
$ brew services start grafana
```

When that succeeds, I can go to `http://localhost:3000/` and I should see the Grafana welcome page:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-grafana-welcome.png?fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=569ab1148e332bf9c8817988ff8ebea9" alt="Grafana Welcome Page" data-og-width="3008" width="3008" data-og-height="2326" height="2326" data-path="docs/vitess/tutorials/metrics-grafana-welcome.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-grafana-welcome.png?w=280&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=e0d475c9afa651fdf3ccc9f26422ce4b 280w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-grafana-welcome.png?w=560&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=a61e8cebd18725e1b7b3d9d8cbe8622c 560w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-grafana-welcome.png?w=840&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=1fb013d5340d8023da5389c25ce7096c 840w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-grafana-welcome.png?w=1100&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=8af773900d52f7d8d8ce88abde83e50d 1100w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-grafana-welcome.png?w=1650&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=ff9ac6e12979c3d37286c50f707ad258 1650w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-grafana-welcome.png?w=2500&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=2af627b979fe0f362c793eed6f9d61d5 2500w" />
</Frame>

The default username and password for a new install is `admin` and `admin`. Grafana will ask you to change the password the first time you log in, please pick something more secure than `admin`.

### Adding a Prometheus Endpoint

You can skip this step as well if you're already running a managed Prometheus or have added your datasource to Grafana already.

If you're running Prometheus locally, you'll need to add that as a datasource. To do this:

<Steps>
  <Step>
    Open the menu in the top left and click "Connections"
  </Step>

  <Step>
    Search for "Prometheus" and pick the plain "Prometheus" option
  </Step>

  <Step>
    Click "Add new data source" in the top right of the page
  </Step>
</Steps>

Now, you should look see a page that looks like this:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-add-prometheus-connection.png?fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=5c6624bb11d3b92dcd327686426ebb0d" alt="Grafana Add Datasource" data-og-width="3024" width="3024" data-og-height="3010" height="3010" data-path="docs/vitess/tutorials/metrics-add-prometheus-connection.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-add-prometheus-connection.png?w=280&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=45864b735b894467268eecba79c38aac 280w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-add-prometheus-connection.png?w=560&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=4fd1bb03b34fba84acbef64779ff564e 560w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-add-prometheus-connection.png?w=840&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=ee754e8a82184144b5df8439522cf404 840w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-add-prometheus-connection.png?w=1100&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=cb50a1ba2da6ce96cbea57bd32fe066d 1100w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-add-prometheus-connection.png?w=1650&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=72f59be560278c017ffda052ad3f12a0 1650w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-add-prometheus-connection.png?w=2500&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=27fadd371e63ddf51c10b29ec9fa1b5a 2500w" />
</Frame>

You can call this whatever you want, we'll use the following:

* Name: "PlanetScale"
* Prometheus server URL: `http://localhost:9090/`

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-prometheus-configuration.png?fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=2d4dd7f8dbef421801fcebcbbb03055b" alt="Grafana Prometheus Configuration" data-og-width="2936" width="2936" data-og-height="2922" height="2922" data-path="docs/vitess/tutorials/metrics-prometheus-configuration.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-prometheus-configuration.png?w=280&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=82ce937c41081ec21e76c79774373054 280w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-prometheus-configuration.png?w=560&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=d782a494cfea2b0973e5c678f9df2b0d 560w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-prometheus-configuration.png?w=840&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=335befdb3d314dcd710d5d8d0197ead6 840w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-prometheus-configuration.png?w=1100&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=d40ea531005719f722d5a7163f98ec54 1100w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-prometheus-configuration.png?w=1650&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=ba8bd36ea388996be473fc20257a7f89 1650w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-prometheus-configuration.png?w=2500&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=0b6c7d8940c745fe3b4537bef8af8a1a 2500w" />
</Frame>

Because this is running on your local machine, we do not need to use any Authentication or TLS

Scroll down to the "Interval behaviour" section and set the "Scrape interval" to `1m`.

Finally, scroll to the bottom and click "Save & test".

## Import the PlanetScale Dashboard

Now that we have our datasource added, let's import the PlanetScale Dashboard. This is a starter dashboard that PlanetScale has produced which shows an overview of your branch with the metrics that we expose.

From the Grafana homepage, go to the top left menu and pick "Dashboards".

In the top right, click "New" and then Import":

PlanetScale maintains the latest version of the dashboard located here:

[https://github.com/planetscale/grafana-dashboard/blob/main/overview.json](https://github.com/planetscale/grafana-dashboard/blob/main/overview.json)

Download this file to your computer, and then click "Upload dashboard JSON file".

Find the JSON file you downloaded in the previous step, and configure it with the Prometheus datasource that we added in an earlier:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-prometheus-configuration.png?fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=2d4dd7f8dbef421801fcebcbbb03055b" alt="Grafana Prometheus Configuration" data-og-width="2936" width="2936" data-og-height="2922" height="2922" data-path="docs/vitess/tutorials/metrics-prometheus-configuration.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-prometheus-configuration.png?w=280&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=82ce937c41081ec21e76c79774373054 280w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-prometheus-configuration.png?w=560&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=d782a494cfea2b0973e5c678f9df2b0d 560w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-prometheus-configuration.png?w=840&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=335befdb3d314dcd710d5d8d0197ead6 840w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-prometheus-configuration.png?w=1100&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=d40ea531005719f722d5a7163f98ec54 1100w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-prometheus-configuration.png?w=1650&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=ba8bd36ea388996be473fc20257a7f89 1650w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-prometheus-configuration.png?w=2500&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=0b6c7d8940c745fe3b4537bef8af8a1a 2500w" />
</Frame>

Click 'Import' and you should be directed to the dashboard, configured to query your local Prometheus with the data it's been scraping!

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Sending Prometheus Metrics to New Relic
Source: https://planetscale.com/docs/vitess/guides/prometheus-metrics-newrelic

If you're looking for your PlanetScale database metrics in your New Relic account, this tutorial will show how to configure a [Prometheus](https://prometheus.io/) instance to scrape PlanetScale's [Prometheus infrastructure](/docs/vitess/integrations/prometheus) automatically, allowing you to collect detailed metrics for all of your PlanetScale branches.

While this tutorial is written for New Relic, using Prometheus' remote write is a common pattern for sending metrics to [AWS Managed Prometheus](https://aws.amazon.com/prometheus/), [Google Cloud Managed Service for Prometheus](https://cloud.google.com/stackdriver/docs/managed-prometheus), [Grafana hosted Prometheus](https://grafana.com/products/cloud/metrics/) and many other tools.

For more information on Prometheus Remote Write and New Relic, see the [New Relic documentation on sending Prometheus metric data](https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/).

## Overview

In this tutorial, we will be using an instance of [Prometheus](https://prometheus.io/) running on a Linux VM to scrape metrics from PlanetScale and then forward them to New Relic using [Remote Write](https://prometheus.io/docs/specs/prw/remote_write_spec/). We will make sure that Prometheus stays running by creating a [Systemd Unit File](https://www.digitalocean.com/community/tutorials/understanding-systemd-units-and-unit-files).

The default configuration we will create will send all PlanetScale metrics to New Relic, and we will cover how to filter to drop certain metrics that may not be desired.

In order to proceed, you'll need:

* A [New Relic API Key](https://docs.newrelic.com/docs/apis/intro-apis/new-relic-api-keys), make sure it is the `Ingest - License` type.
* PlanetScale [Service token](/docs/api/reference/service-tokens) with `read_metrics_endpoints` permissions.

## Prometheus Installation

First, let's download the latest release of Prometheus and create our user that is going to run it. We'll be using the latest 3.x release from the [GitHub Releases Page](https://github.com/prometheus/prometheus/releases).

Create a `prometheus` user:

```bash  theme={null}
$ sudo useradd -M -U prometheus
```

```bash  theme={null}
$ wget https://github.com/prometheus/prometheus/releases/download/v3.2.1/prometheus-3.2.1.linux-amd64.tar.gz
$ tar xf prometheus-3.2.1.linux-amd64.tar.gz
$ sudo mv prometheus-3.2.1.linux-amd64/ /opt/prometheus
$ sudo chown prometheus:prometheus -R /opt/prometheus
```

This has put the Prometheus binary in `/opt/prometheus` along with the example configuration file that we can use.

## Create our Systemd Unit File

Now that we have the binary in place, let's setup Systemd to run Prometheus by creating a Unit File in `/etc/systemd/system/prometheus.service` with the following contents:

```ini expandable theme={null}
[Unit]
Description=Prometheus Agent
Documentation=https://prometheus.io/
After=network-online.target

[Service]
User=prometheus
Group=prometheus
Restart=on-failure
ExecStart=/opt/prometheus/prometheus \
  --config.file=/opt/prometheus/prometheus.yml \
  --storage.agent.path=/opt/prometheus/data \
  --web.listen-address=0.0.0.0:9091 \
  --agent

[Install]
WantedBy=multi-user.target
```

## Configure Prometheus

Now that we've got Prometheus installed and a unit file present, let's configure Prometheus. We will be borrowing some of our configuration from the [Prometheus Guide](/docs/vitess/integrations/prometheus), and adding some New Relic specific configuration. Edit `/opt/prometheus/prometheus.yml` in your editor of choice so that it contains this, making sure to replace your org name, service token information, and New Relic API key:

```yaml  theme={null}
global:
  scrape_interval: 1m
scrape_configs:
  - job_name: "${ORG}"
    http_sd_configs:
    - url: https://api.planetscale.com/v1/organizations/${ORG}/metrics
      authorization:
        type: "token"
        credentials: "${TOKEN_ID}:${TOKEN}"
      refresh_interval: 10m
remote_write:
  - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=planetscale
    authorization:
      credentials: ${NEW_RELIC_API_TOKEN}
```

This configuration file does the following:

* Configures Prometheus to discover scraping endpoints from the PlanetScale API using a service token
* Points Prometheus to write the metrics it scrapes from PlanetScale to the New Relic API

## Starting Prometheus

Now that we have a Systemd unit file and a configured Prometheus, let's run it!

```bash  theme={null}
$ sudo systemctl daemon-reload
$ sudo systemctl start prometheus.service
```

We can also tell Systemd to run Prometheus when my VM boots:

```bash  theme={null}
$ sudo systemctl enable prometheus.service
```

Now, let's check to make sure everything is running properly:

```bash expandable theme={null}
$ sudo systemctl status prometheus.service
● prometheus.service - Prometheus Agent
     Loaded: loaded (/etc/systemd/system/prometheus.service; enabled; preset: enabled)
     Active: active (running) since Fri 2025-04-04 17:36:57 UTC; 38s ago
       Docs: https://prometheus.io/
   Main PID: 745542 (prometheus)
      Tasks: 9 (limit: 9486)
     Memory: 21.2M (peak: 21.9M)
        CPU: 264ms
     CGroup: /system.slice/prometheus.service
             └─745542 /opt/prometheus/prometheus --config.file=/opt/prometheus/prometheus.yml --storage.agent.path=/opt/prometheus/data --web.listen-address=0.0.0.0:9091 --agent

Apr 04 17:36:57 ubuntu prometheus[745542]: time=2025-04-04T17:36:57.804Z level=INFO source=main.go:1305 msg=EXT4_SUPER_MAGIC
Apr 04 17:36:57 ubuntu prometheus[745542]: time=2025-04-04T17:36:57.804Z level=INFO source=main.go:1308 msg="Agent WAL storage started"
Apr 04 17:36:57 ubuntu prometheus[745542]: time=2025-04-04T17:36:57.804Z level=INFO source=main.go:1437 msg="Loading configuration file" filename=/opt/prometheus/prometheus.yml
Apr 04 17:36:57 ubuntu prometheus[745542]: time=2025-04-04T17:36:57.805Z level=INFO source=watcher.go:225 msg="Starting WAL watcher" component=remote remote_name=fbe64a url="https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=planetscal>
Apr 04 17:36:57 ubuntu prometheus[745542]: time=2025-04-04T17:36:57.805Z level=INFO source=metadata_watcher.go:90 msg="Starting scraped metadata watcher" component=remote remote_name=fbe64a url="https://metric-api.newrelic.com/prometheus/v1/write?prometh>
Apr 04 17:36:57 ubuntu prometheus[745542]: time=2025-04-04T17:36:57.805Z level=INFO source=watcher.go:277 msg="Replaying WAL" component=remote remote_name=fbe64a url="https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=planetscale" queu>
Apr 04 17:36:57 ubuntu prometheus[745542]: time=2025-04-04T17:36:57.806Z level=INFO source=main.go:1476 msg="updated GOGC" old=100 new=75
Apr 04 17:36:57 ubuntu prometheus[745542]: time=2025-04-04T17:36:57.806Z level=INFO source=main.go:1486 msg="Completed loading of configuration file" db_storage=791ns remote_storage=610.171µs web_handler=897ns query_engine=301ns scrape=517.175µs scrape_s>
Apr 04 17:36:57 ubuntu prometheus[745542]: time=2025-04-04T17:36:57.806Z level=INFO source=main.go:1213 msg="Server is ready to receive web requests."
```

This reports that prometheus is `active (running)`, and I can see the logs showing that it started successfully. Great!

## Querying on New Relic

After a couple of minutes, head over to your New Relic dashboard and we can query for your database metrics. First, let's get a list of the database branches in the `nick` organization that I'm using to test:

```bash  theme={null}
$ pscale branch list test --org nick
  ID             NAME         PARENT BRANCH   REGION    PRODUCTION   SAFE MIGRATIONS   READY   CREATED AT     UPDATED AT
 -------------- ------------ --------------- --------- ------------ ----------------- ------- -------------- ----------------
  7wxuxewx4l0p   main         n/a             us-east   Yes          No                Yes     2 years ago    50 minutes ago
  6o0rr27785fl   partitions   main            us-east   No           No                Yes     2 months ago   7 minutes ago
```

For this, we'll use the `7wxuxewx4l0p` branch.

Using New Relic's NRQL, we can visualize the memory usage of my VTTablet instances with the following query:

```sql  theme={null}
FROM Metric SELECT average(planetscale_pods_cpu_util_percentages) WHERE planetscale_database_branch_id = '7wxuxewx4l0p' AND planetscale_component='vttablet' SINCE 30 minutes AGO TIMESERIES FACET planetscale_pod
```

Because my `main` branch is production, we will see the memory usage for my primary and both my replicas over the last 30 minutes:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-new-relic-dashboard.png?fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=8335f48aba48d9d38c5392fbe8c7bd24" alt="New Relic Memory Query" data-og-width="2758" width="2758" data-og-height="1450" height="1450" data-path="docs/vitess/tutorials/metrics-new-relic-dashboard.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-new-relic-dashboard.png?w=280&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=3dabb0eae8d0f066945a1ad2cc5a03b4 280w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-new-relic-dashboard.png?w=560&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=ef446159597e8d24a2dff6826b6bb42c 560w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-new-relic-dashboard.png?w=840&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=778694ba8466edd29b7b37ab64215302 840w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-new-relic-dashboard.png?w=1100&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=a16ff496cce5ce1b000c39883ca24319 1100w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-new-relic-dashboard.png?w=1650&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=264e6dbe4a57d9b5f6cf7b3351cfcd64 1650w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-new-relic-dashboard.png?w=2500&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=af0dc6f709b8c625765036ffbe630441 2500w" />
</Frame>

## Filtering Metrics

If you don't want to ingest every metric into New Relic, you can tell Prometheus to drop certain metrics. For more information, see the [New Relic Documentation](https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/#allow-deny).

If we adjust our Prometheus configuration that we have in `/opt/prometheus/prometheus.yml` we can instruct Prometheus to drop all metrics unless they match a certain naming convention:

```yaml  theme={null}
remote_write:
  - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=planetscale
    authorization:
      credentials: ${NEW_RELIC_API_TOKEN}
  - source_labels: [__name__]
    regex: "planetscale_pods_(.*)"
    action: keep
```

If you replace your `remote_write` block with what's above, Prometheus will only forward the timeseries that match the `planetscale_pods_*` name. For a full list of metrics, see our [Metric List](/docs/vitess/integrations/prometheus-metrics).

## What's Next?

Now that you have your branch metrics in New Relic, you can create dashboards and alerts for conditions such as high CPU or replication delay.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Handling table and column renames
Source: https://planetscale.com/docs/vitess/handling-table-and-column-renames



## Overview

In SQL, it is possible to `RENAME` either a column or a table using an [`ALTER` statement](https://dev.mysql.com/doc/refman/8.0/en/alter-table.html). This keeps the data and type of the column or entire table the same and changes only the name.

PlanetScale does not support this method of table and column renames.

If you do need to rename a column or table, the following section covers how you should do it with PlanetScale using [Deploy Requests](/docs/vitess/schema-changes/deploy-requests).

## How to rename a column on PlanetScale

<Steps>
  <Step>
    Create a new column with the new name.
  </Step>

  <Step>
    Update the application to write to both columns with new data.
  </Step>

  <Step>
    Backfill all the data in the new column for rows that are still missing that information.
  </Step>

  <Step>
    Optionally, add constraints like `NOT NULL` to the new column once all the data is backfilled.
  </Step>

  <Step>
    Update the application to only use the new column, and remove any references to the old column name.
  </Step>

  <Step>
    Drop the old column.
  </Step>
</Steps>

This means at least two deploy requests are needed (potentially more if you want to enforce `NOT NULL` without a `DEFAULT`), where you first add the newly named column and then drop the old one.

## Why not support renames?

There are two reasons why renames are not supported. The first reason is that PlanetScale uses a declarative model for determining the changes between schemas. This means that there is no way to know for sure if something is a rename or if it was an add and drop column. The second reason is that safely performing a rename requires downtime for your application.

### Declarative model for schema migrations

PlanetScale uses a declarative model to determine schema changes. This means that we look at the end state of two branches and compare them to find the difference. When you rename a column, there is no way to know whether there was a rename or if a column was added and dropped in a development branch.

It would be possible to apply heuristics to determine if something looks like a rename, but this is not 100% guaranteed to be correct. For example, if a column is renamed and a second column is dropped adjacent to the one that was renamed, it is impossible to determine which column to rename with certainty.

Because of this, we only detect simple cases that look like a rename and alert the user on that in a deploy request. While it would be possible to generate a `RENAME` in such a case, we don't know if that is correct, and do not want to execute any schema changes that are not requested.

### Downtime with renames

When using a `RENAME`, downtime for your application can only be avoided with significant and complex logic on the application side. While the application is running, suddenly a column disappears and another one appears that contains the same data. This means that the application both needs to be able to handle the state when this happens and know that the data is the same.

Usually the only way to handle a column rename is to simultaneously deploy the application, but that is very difficult as it races with the database changes and is not perfectly atomic and synchronized with the schema change.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# How to make different types of schema changes
Source: https://planetscale.com/docs/vitess/how-to-make-different-types-of-schema-changes

Your database has to grow and change with your application and product.

## Overview

So schema changes are inevitable. Luckily, PlanetScale has features like [branching](/docs/vitess/schema-changes/branching), [deploy requests](/docs/vitess/schema-changes/deploy-requests), and [revert](/docs/vitess/schema-changes/deploy-requests#revert-a-schema-change) to help give you more confidence when making schema changes. But you still have to consider though how to make the schema change. Some schema changes are more complex than others. This document will help you understand how to make different types of schema changes and associated risks.

<Info>
  Most of the following applies to any relational database and not only to PlanetScale.
</Info>

## Adding a table

Adding a table is a low risk since it does not affect the existing schema. Once you add the table to your schema and deploy the database to production, if you want to backfill the data, now would be the time. Otherwise, you add it to your application code after completing your database deployment. Deploying the database first and then changing your application code to avoid downtime is best.

## Adding a column

Adding a column without a `NOT NULL` constraint is a low risk and can be handled similarly to [adding a table](#adding-a-table).

The one exception is if you want to add a column with a `NOT NULL` constraint and/or no `DEFAULT` value. The risk increases because the previously inserted rows will be expected to be `NOT NULL` or have no default values; this will cause possible errors in your application.

To work around this, you can do the following:

<Steps>
  <Step>
    Add the new column without defining a DEFAULT value, allowing NULL values.
  </Step>

  <Step>
    Write a script to backfill the new column for all missing data rows.
  </Step>

  <Step>
    Add the `NOT NULL` constraint once the data is backfilled.
  </Step>
</Steps>

## Changing a column or table

Some examples of changing a column or table include:

* Renaming an existing column or table
* Changing the data type of an existing column
* Splitting and other modifications to the data of an existing column or table

This includes modifying your application to use a different column or table in existing code. For example, if you are using `username` in the code but instead want to start using a new `user_id` column in the existing code.

Changing a column or table your current application uses is a high risk. It can cause disruptions to users and possible downtime. If you want to change a column or table, we recommend following the expand, migrate, and contract pattern described in the [backward compatible schema changes](https://planetscale.com/blog/backward-compatible-databases-changes) blog post. The blog post walks you through each step in both your database and application code and gives a detailed example.

## Removing a column or table

### In use

Removing a column or table your current application uses is a high risk. Dropping a column or table can cause disruptions to users and possible downtime. If you want to remove a column or table in use, we recommend following the expand, migrate, and contract pattern described in the [backward compatible schema changes](https://planetscale.com/blog/backward-compatible-databases-changes) blog post. The blog post walks you through each step in both your database and application code and gives a detailed example.

### Not in use

Removing a column or table that is no longer in use by your current application is a low risk. The only danger is potentially needing the removed data in the future.

## Other schema changes

This is not an exhaustive list of schema changes. Remember, making smaller incremental schema changes is always safer than larger, more complex ones. If you want to read more on making safer schema changes with PlanetScale, read this blog post on [safely making database schema changes](https://planetscale.com/blog/safely-making-database-schema-changes).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Amazon Aurora migration guide
Source: https://planetscale.com/docs/vitess/imports/amazon-aurora-migration-guide



## Overview

This document will demonstrate how to migrate a database from Amazon Aurora (MySQL compatible) to PlanetScale.

<Note>
  This guide assumes you are using Amazon Aurora (MySQL compatible) on RDS. If you are using MySQL on Amazon RDS, follow the [Amazon RDS for MySQL migration guide](/docs/vitess/imports/aws-rds-migration-guide). Other database systems (non-MySQL or MariaDB databases) available through RDS will not work with the PlanetScale import tool.
</Note>

We recommend reading through the [Database import documentation](/docs/vitess/imports/database-imports) to learn how our import tool works before proceeding.

## Prerequisites

Gather the following information from the AWS Console:

* **Database cluster endpoint address** - Located in "**Connectivity & security**" tab (use the regional cluster endpoint, not reader or writer instances)
* **Port number** - Typically 3306
* **Master username and password** - Your Aurora root credentials

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-connectivity-and-security-tab-of-the-database-in-aurora.jpg?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=56d4fa1510f417a6fe99e10aa4b3b9ae" alt="The Connectivity & security tab of the database in RDS." data-og-width="2230" width="2230" data-og-height="1356" height="1356" data-path="docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-connectivity-and-security-tab-of-the-database-in-aurora.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-connectivity-and-security-tab-of-the-database-in-aurora.jpg?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=3435cb44733b8e832b17eea2d64cabc6 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-connectivity-and-security-tab-of-the-database-in-aurora.jpg?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=62a94e0cb1875444df3953b43c1e786f 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-connectivity-and-security-tab-of-the-database-in-aurora.jpg?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=36dbb0ab9720036fed8df7d2e6642564 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-connectivity-and-security-tab-of-the-database-in-aurora.jpg?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=b4cf72c8cee58b663768046d9e6a018d 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-connectivity-and-security-tab-of-the-database-in-aurora.jpg?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=4c6cefc5818fc78ef8b9dd6a3e9de336 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-connectivity-and-security-tab-of-the-database-in-aurora.jpg?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=63aaa1d68457d381c9213d26dfa654c6 2500w" />
</Frame>

## Step 1: Configure server settings

Your Aurora database needs specific server settings configured before you can import. Follow these steps to configure GTID mode, binlog format, and sql\_mode.

### Check your current parameter group

Your Amazon Aurora database is either using the default DB cluster parameter group (e.g., default.aurora-mysql8.0) or a custom one. You can view it in the "**Configuration**" tab of your regional database cluster (not reader or writer instances).

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-configuration-tab-of-the-database-view-in-aurora.jpg?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=b10172341894e593ad7d4260cf505cca" alt="The Configuration tab of the database view in RDS." data-og-width="2232" width="2232" data-og-height="1302" height="1302" data-path="docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-configuration-tab-of-the-database-view-in-aurora.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-configuration-tab-of-the-database-view-in-aurora.jpg?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=836dc59ec2d0bcf2ae6691010f877401 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-configuration-tab-of-the-database-view-in-aurora.jpg?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=82b5eeb7c231b282c706e53260e8eaf3 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-configuration-tab-of-the-database-view-in-aurora.jpg?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=f328e5d7a91bba7cd404e82af99c8adb 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-configuration-tab-of-the-database-view-in-aurora.jpg?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=9b672d57ae4fd703a9503f4da6507f47 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-configuration-tab-of-the-database-view-in-aurora.jpg?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=9ab285a313ce44361e2e1b4ceed24c28 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-configuration-tab-of-the-database-view-in-aurora.jpg?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=9494d911a860ccfe16909d6c8cfe4560 2500w" />
</Frame>

### Configure the parameter group

<Steps>
  <Step>
    If you are using the default DB cluster parameter group, you'll need to create a new parameter group to reconfigure settings.

    To create a parameter group, select "**Parameter groups**" from the left nav and then "**Create parameter group**".

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-parameter-groups-view-in-rds.png?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=8795d1b9c39dc35098d6a7c7ad6cfb1e" alt="The Parameter groups view in RDS." data-og-width="1715" width="1715" data-og-height="729" height="729" data-path="docs/images/assets/docs/imports/aws-rds-migration-guide/the-parameter-groups-view-in-rds.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-parameter-groups-view-in-rds.png?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=8a6a08244de4f421f9a9bf398d3ac394 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-parameter-groups-view-in-rds.png?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=7b1c2cb6e0743e250108cb12923a12f8 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-parameter-groups-view-in-rds.png?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=2821d5901e5847f9d60b88cf242f0cfc 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-parameter-groups-view-in-rds.png?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=183a6cc3efc5d55075e3396176bb7afe 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-parameter-groups-view-in-rds.png?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=1eb294b513db63c06bc3d09fb0a06459 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-parameter-groups-view-in-rds.png?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=1fc4a1b5250f596aee5653160c34a435 2500w" />
    </Frame>

    Specify the **Parameter group family**, **Type**, **Group name**, and **Description**. All fields are required.

    * Parameter group family: aurora-mysql8.0
    * Type: DB Cluster Parameter Group (Note: Not "DB Parameter Group" type)
    * Group name: psmigrationgroup (or your choice)
    * Description: Parameter group for PlanetScale migration

    You'll be brought back to the list of available parameter groups when you save.
  </Step>

  <Step>
    Edit the settings in your custom DB cluster parameter group. Select your parameter group from the list.

    Click "**Edit parameters**" to unlock editing.

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-header-of-the-view-when-editing-a-parameter-group.png?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=08119cc3d744db24b15237d4f15f457a" alt="The header of the view when editing a parameter group." data-og-width="1374" width="1374" data-og-height="293" height="293" data-path="docs/images/assets/docs/imports/aws-rds-migration-guide/the-header-of-the-view-when-editing-a-parameter-group.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-header-of-the-view-when-editing-a-parameter-group.png?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=00baeaf3bac5809c23f19760f1a93f0a 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-header-of-the-view-when-editing-a-parameter-group.png?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=a0e8a0d6f172998869c40d81ca810681 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-header-of-the-view-when-editing-a-parameter-group.png?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=7a4c06d0ec5d068c7ca41c07cc67746b 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-header-of-the-view-when-editing-a-parameter-group.png?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=728ab3b88fdc122275609a986206edcf 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-header-of-the-view-when-editing-a-parameter-group.png?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=34898994ff2c9df7142a1bc273d0d825 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-header-of-the-view-when-editing-a-parameter-group.png?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=1c1a820d2e98fad946ddac9ad8dc6132 2500w" />
    </Frame>

    Search for "**gtid**" and update:

    * gtid-mode: ON
    * enforce\_gtid\_consistency: ON

    Search for "**sql\_mode**" and update:

    * sql\_mode: NO\_ZERO\_IN\_DATE,NO\_ZERO\_DATE,ONLY\_FULL\_GROUP\_BY

    Search for "**binlog\_format**" and update:

    * binlog\_format: ROW

    Click "**Save changes**".
  </Step>

  <Step>
    Associate the DB cluster parameter group to your database. Select "**Databases**" from the left nav, select your regional cluster (not writer or reader instance), and click "**Modify**".

    Scroll to **Additional configuration** section. Update the **DB cluster parameter group** to your new parameter group. Click "**Continue**".

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-additional-configuration-section-of-the-database-configuration-view.jpg?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=82138d9a79c3812f3cf7e5c3d8c764ec" alt="The Additional configuration section of the database configuration view." data-og-width="1576" width="1576" data-og-height="808" height="808" data-path="docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-additional-configuration-section-of-the-database-configuration-view.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-additional-configuration-section-of-the-database-configuration-view.jpg?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=9aedef4e72bdde152946f45737149445 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-additional-configuration-section-of-the-database-configuration-view.jpg?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=3b121c208be069162796fad087cc89f9 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-additional-configuration-section-of-the-database-configuration-view.jpg?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=029b44de7c22cadd9f61d2b3201bef2b 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-additional-configuration-section-of-the-database-configuration-view.jpg?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=deaeff2a02c3c6b3e04f40efa38488fe 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-additional-configuration-section-of-the-database-configuration-view.jpg?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=574a3e511b876c08bbe3fd53966550b2 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-additional-configuration-section-of-the-database-configuration-view.jpg?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=c78db817555447c52ca24d742206b50d 2500w" />
    </Frame>

    Choose when to apply:

    * **Apply during the next scheduled maintenance window** - Applied during maintenance window
    * **Apply immediately** - Applied now, but requires manual reboot

    Click "**Modify DB instance**".
  </Step>

  <Step>
    Reboot your database's writer instance to apply the settings. Click "**Actions**" > "**Reboot**". (Make sure you're selecting the writer instance, not the regional cluster.)

    <Warning>
      This will briefly disconnect active users! The parameter group changes won't take effect without a reboot.
    </Warning>

    Confirm the reboot. You can check the status in the databases list (click refresh to update).
  </Step>
</Steps>

## Step 2: Enable binary logging

Binary logging must be enabled for the import to work. On Aurora/RDS, binary logging is tied to automated backups.

To enable binary logging, [enable automated backups](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html#USER_WorkingWithAutomatedBackups.Enabling) by setting the backup retention period to any value greater than zero days.

Verify binary logging is enabled:

```sql  theme={null}
mysql> show variables like 'log_bin';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| log_bin       | ON    |
+---------------+-------+
```

## Step 3: Configure binlog retention

Set the binary log retention period to ensure logs aren't purged during the import. For most cases, 48 hours is sufficient, but larger imports may need more time.

<Warning>
  Longer retention periods use more disk space. Evaluate your binlog size to avoid running out of disk space. Contact [PlanetScale Support](https://support.planetscale.com/hc/en-us) if you need assistance.
</Warning>

Set the retention period using the `mysql.rds_set_configuration()` procedure:

```sql  theme={null}
CALL mysql.rds_set_configuration('binlog retention hours', 48);
```

Verify the setting:

```sql  theme={null}
CALL mysql.rds_show_configuration;
```

Expected output:

```
+------------------------+-------+-----------------------------------------------------------------------------------------------------------+
| name                   | value | description                                                                                               |
+------------------------+-------+-----------------------------------------------------------------------------------------------------------+
| binlog retention hours | 48    | binlog retention hours specifies the duration in hours before binary logs are automatically deleted.      |
+------------------------+-------+-----------------------------------------------------------------------------------------------------------+
```

## Step 4: Ensure database is publicly accessible

PlanetScale needs to connect to your Aurora database over the internet. Check that your database is publicly accessible.

In the writer instance, go to "**Connectivity & security**" tab. Under "**Security**", check if **Publicly accessible** is set to "Yes". If it says "No", you'll need to modify the database settings to enable public access.

If you cannot make the database publicly accessible, [contact us](https://planetscale.com/contact) to discuss alternative import options.

## Step 5: Create a migration user

Create a dedicated user with limited privileges for the import process.

Connect to your Aurora database using the MySQL command line with your master credentials:

```bash  theme={null}
mysql -u admin -p -h [your-aurora-endpoint]
```

Run the following script, replacing the placeholders:

* `<SUPER_STRONG_PASSWORD>` - Password for the migration\_user account
* `<DATABASE_NAME>` - Name of the database you're importing

```sql  theme={null}
CREATE USER 'migration_user'@'%' IDENTIFIED BY '<SUPER_STRONG_PASSWORD>';
GRANT PROCESS, REPLICATION SLAVE, REPLICATION CLIENT, RELOAD ON *.* TO 'migration_user'@'%';
GRANT SELECT, INSERT, UPDATE, DELETE, SHOW VIEW, LOCK TABLES ON `<DATABASE_NAME>`.* TO 'migration_user'@'%';
GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, ALTER ON `ps\_import\_%`.* TO 'migration_user'@'%';
GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, ALTER ON `_vt`.* TO 'migration_user'@'%';
GRANT EXECUTE ON PROCEDURE mysql.rds_show_configuration TO 'migration_user'@'%';
GRANT SELECT ON mysql.db TO 'migration_user'@'%';
GRANT SELECT ON mysql.func TO 'migration_user'@'%';
GRANT SELECT ON mysql.tables_priv TO 'migration_user'@'%';
GRANT SELECT ON mysql.user TO 'migration_user'@'%';
GRANT SELECT ON performance_schema.* TO 'migration_user'@'%';
FLUSH PRIVILEGES;
```

Save the username and password securely - you'll need them for the import.

## Step 6: Configure RDS security group

Allow PlanetScale to connect by adding PlanetScale's IP addresses to your security group.

The specific IP addresses depend on your PlanetScale database region. These will be shown during the import workflow on the **Connect to external database** step. See the [Import public IP addresses](/docs/vitess/imports/import-tool-migration-addresses) page for more details.

### Add IP addresses to security group

1. Navigate to "**Connectivity & security**" tab of your writer instance
2. Click the VPC security group link

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-connectivity-and-security-tab-of-the-database-view-in-aurora.jpg?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=6a7b857e848b0af7e7f5937365489c1a" alt="The Connectivity & security tab of the database view in RDS." data-og-width="2230" width="2230" data-og-height="1346" height="1346" data-path="docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-connectivity-and-security-tab-of-the-database-view-in-aurora.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-connectivity-and-security-tab-of-the-database-view-in-aurora.jpg?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=121d0c141db1e1b2cac3c9309c5b89d6 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-connectivity-and-security-tab-of-the-database-view-in-aurora.jpg?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=eff3ed1dcd20cd6043838b09262533fb 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-connectivity-and-security-tab-of-the-database-view-in-aurora.jpg?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=10477f61a03b78d09682cabdd1ffca60 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-connectivity-and-security-tab-of-the-database-view-in-aurora.jpg?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=c6df3fc2c1ed1268312f5e08db69f309 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-connectivity-and-security-tab-of-the-database-view-in-aurora.jpg?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=bbc1925d231063f51495a5c2427c9f77 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/amazon-aurora-migration-guide/the-connectivity-and-security-tab-of-the-database-view-in-aurora.jpg?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=9b44971e88332cd55682c8dcec17156b 2500w" />
</Frame>

3. Select "**Inbound rules**" tab, then "**Edit inbound rules**"

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-view-of-security-groups-associated-with-the-rds-instance.png?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=3b08f2a427d49727fc071e4ed86628e5" alt="The view of security groups associated with the RDS instance." data-og-width="1440" width="1440" data-og-height="981" height="981" data-path="docs/images/assets/docs/imports/aws-rds-migration-guide/the-view-of-security-groups-associated-with-the-rds-instance.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-view-of-security-groups-associated-with-the-rds-instance.png?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=4361d5f6db9992f61cff08bf4fca6b00 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-view-of-security-groups-associated-with-the-rds-instance.png?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=8898bad933120556731dd9697c830c7d 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-view-of-security-groups-associated-with-the-rds-instance.png?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=875859633e5875a7b367ff14c82160d8 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-view-of-security-groups-associated-with-the-rds-instance.png?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=ae2a1524936bb8d70e73ef517cc305ee 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-view-of-security-groups-associated-with-the-rds-instance.png?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=d6228cb6a412d00dea0ef241691dfb63 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-view-of-security-groups-associated-with-the-rds-instance.png?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=14195e865041f44f93373829eac5a713 2500w" />
</Frame>

4. Click "**Add rule**"
5. **Type**: Select `MYSQL/Aurora`
6. **Source**: Enter the first PlanetScale IP address (AWS will format it as `x.x.x.x/32`)
7. Repeat for each IP address in your region
8. Click "**Save rules**"

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-edit-inbound-rules-view-where-source-traffic-can-be-allowed.png?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=6f47823af9b1ac30ac4539403d1a0215" alt="The Edit inbound rules view where source traffic can be allowed." data-og-width="1419" width="1419" data-og-height="597" height="597" data-path="docs/images/assets/docs/imports/aws-rds-migration-guide/the-edit-inbound-rules-view-where-source-traffic-can-be-allowed.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-edit-inbound-rules-view-where-source-traffic-can-be-allowed.png?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=299ad1895dd115798b1f074186f94665 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-edit-inbound-rules-view-where-source-traffic-can-be-allowed.png?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=9a4db15d937996c2536f0f8146dc5b62 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-edit-inbound-rules-view-where-source-traffic-can-be-allowed.png?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=282321b8f65b2b2103eacc14078e4a12 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-edit-inbound-rules-view-where-source-traffic-can-be-allowed.png?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=ef4dd2841399ed274fde95bf2284a2af 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-edit-inbound-rules-view-where-source-traffic-can-be-allowed.png?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=e9134bff585bde08831853c5f760b064 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-edit-inbound-rules-view-where-source-traffic-can-be-allowed.png?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=0b4a9650b59e4d07cc63cba65021873d 2500w" />
</Frame>

## Importing your database

Now that your Aurora database is configured, follow the [Database Imports guide](/docs/vitess/imports/database-imports) to complete your import.

When filling out the connection form in the import workflow, use:

* **Host name** - Your Aurora cluster endpoint address (from Prerequisites)
* **Port** - 3306 (or your custom port)
* **Database name** - The exact database name to import
* **Username** - `migration_user`
* **Password** - The password you set in Step 5
* **SSL verification mode** - Select based on your Aurora SSL configuration

The Database Imports guide will walk you through:

* Creating your PlanetScale database
* Connecting to your Aurora database
* Validating your configuration
* Selecting tables to import
* Monitoring the import progress
* Switching traffic and completing the import

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# AWS RDS migration guide
Source: https://planetscale.com/docs/vitess/imports/aws-rds-migration-guide



## Overview

This document will demonstrate how to migrate a database from AWS Relational Database Services (RDS) to PlanetScale.

<Note>
  This guide assumes you are using MySQL on Amazon RDS. If you are using Amazon Aurora (MySQL compatible) on RDS, follow the [Amazon Aurora migration guide](/docs/vitess/imports/amazon-aurora-migration-guide). Other database systems (non-MySQL or MariaDB databases) available through RDS will not work with the PlanetScale import tool.
</Note>

We recommend reading through the [Database import documentation](/docs/vitess/imports/database-imports) to learn how our import tool works before proceeding.

## Prerequisites

Gather the following information from the AWS Console:

* **Database endpoint address** - Located in "**Connectivity & security**" tab of your database instance
* **Port number** - Typically 3306
* **Master username and password** - Your RDS root credentials

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-connectivity-and-security-tab-of-the-database-in-rds.png?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=f494ca71daef8abd319078a18d2b3ec9" alt="The Connectivity & security tab of the database in RDS." data-og-width="1394" width="1394" data-og-height="970" height="970" data-path="docs/images/assets/docs/imports/aws-rds-migration-guide/the-connectivity-and-security-tab-of-the-database-in-rds.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-connectivity-and-security-tab-of-the-database-in-rds.png?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=e2bfaccfab57a3ab7f04c75c32a42afc 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-connectivity-and-security-tab-of-the-database-in-rds.png?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=0200bfd0239ca5855079e6a429f98bc1 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-connectivity-and-security-tab-of-the-database-in-rds.png?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=c3b4adea68ef69f8627189e202f373b1 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-connectivity-and-security-tab-of-the-database-in-rds.png?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=cc77bb96590d2a2a22423555965b931f 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-connectivity-and-security-tab-of-the-database-in-rds.png?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=23a433819344efa6d6b6896b86c29052 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-connectivity-and-security-tab-of-the-database-in-rds.png?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=d4924d8090823e86bc42e510ad42702f 2500w" />
</Frame>

## Step 1: Configure server settings

Your RDS database needs specific server settings configured before you can import. Follow these steps to configure GTID mode, binlog format, and sql\_mode.

### Check your current parameter group

Your Amazon RDS database is either using the default DB parameter group (e.g., default.mysql8.0) or a custom one. You can view it in the "**Configuration**" tab of your database instance.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-configuration-tab-of-the-database-view-in-rds.png?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=17e6b06053635fa8cdc53a25352c841c" alt="The Configuration tab of the database view in RDS." data-og-width="1362" width="1362" data-og-height="885" height="885" data-path="docs/images/assets/docs/imports/aws-rds-migration-guide/the-configuration-tab-of-the-database-view-in-rds.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-configuration-tab-of-the-database-view-in-rds.png?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=dc6ae79957ea6ae6ef45e2a05a5e050b 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-configuration-tab-of-the-database-view-in-rds.png?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=49096617798791817260f45ba6c82068 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-configuration-tab-of-the-database-view-in-rds.png?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=ad4b49fe29add1a38122f4c2723672c6 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-configuration-tab-of-the-database-view-in-rds.png?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=e78c833db058be60e5ee45f22794c5cc 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-configuration-tab-of-the-database-view-in-rds.png?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=1fa9ec4b16695b72a55132f5b8777cb2 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-configuration-tab-of-the-database-view-in-rds.png?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=5fbcb7bf7531cc6154a3b144d6f4f021 2500w" />
</Frame>

### Configure the parameter group

<Steps>
  <Step>
    If you are using the default DB parameter group, you'll need to create a new parameter group to reconfigure settings.

    To create a parameter group, select "**Parameter groups**" from the left nav and then "**Create parameter group**".

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-parameter-groups-view-in-rds.png?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=8795d1b9c39dc35098d6a7c7ad6cfb1e" alt="The Parameter groups view in RDS." data-og-width="1715" width="1715" data-og-height="729" height="729" data-path="docs/images/assets/docs/imports/aws-rds-migration-guide/the-parameter-groups-view-in-rds.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-parameter-groups-view-in-rds.png?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=8a6a08244de4f421f9a9bf398d3ac394 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-parameter-groups-view-in-rds.png?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=7b1c2cb6e0743e250108cb12923a12f8 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-parameter-groups-view-in-rds.png?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=2821d5901e5847f9d60b88cf242f0cfc 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-parameter-groups-view-in-rds.png?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=183a6cc3efc5d55075e3396176bb7afe 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-parameter-groups-view-in-rds.png?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=1eb294b513db63c06bc3d09fb0a06459 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-parameter-groups-view-in-rds.png?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=1fc4a1b5250f596aee5653160c34a435 2500w" />
    </Frame>

    Specify the **Parameter group family**, **Type**, **Group name**, and **Description**. All fields are required.

    * Parameter group family: mysql8.0 (or your MySQL version)
    * Type: DB Parameter Group (Note: Not "DB Cluster Parameter Group" type)
    * Group name: psmigrationgroup (or your choice)
    * Description: Parameter group for PlanetScale migration

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-form-used-to-create-a-parameter-group.png?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=dd2d6d551bbf121b67db6f1bea6af761" alt="The form used to create a parameter group." data-og-width="851" width="851" data-og-height="625" height="625" data-path="docs/images/assets/docs/imports/aws-rds-migration-guide/the-form-used-to-create-a-parameter-group.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-form-used-to-create-a-parameter-group.png?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=243521b4880e7246d561e8534d0fede3 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-form-used-to-create-a-parameter-group.png?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=f73d6b599275bb198ba31041d115171e 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-form-used-to-create-a-parameter-group.png?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=a1a2f076186dffeb59a2c86d43f9b9c9 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-form-used-to-create-a-parameter-group.png?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=19d0f482501e4f1a28873e95f01caa21 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-form-used-to-create-a-parameter-group.png?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=5426eee226a619781d098649963b6162 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-form-used-to-create-a-parameter-group.png?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=a6fa1846653b8fb669427cc636d756cb 2500w" />
    </Frame>

    You'll be brought back to the list of available parameter groups when you save.
  </Step>

  <Step>
    Edit the settings in your custom DB parameter group. Select your parameter group from the list.

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-list-of-parameter-groups-in-rds.png?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=2e5df4ee64d051a761d4dac61bccb402" alt="The list of parameter groups in RDS." data-og-width="1398" width="1398" data-og-height="296" height="296" data-path="docs/images/assets/docs/imports/aws-rds-migration-guide/the-list-of-parameter-groups-in-rds.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-list-of-parameter-groups-in-rds.png?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=0c9830a2300a8367386c3c15189b1e51 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-list-of-parameter-groups-in-rds.png?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=3d2664f422302e241eff62f3e60f6ff2 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-list-of-parameter-groups-in-rds.png?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=0fe73391304d28a2f7fce64ec866f526 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-list-of-parameter-groups-in-rds.png?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=bc0404939d1aebe6ea383de31c60cd23 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-list-of-parameter-groups-in-rds.png?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=1b9dbdc40d713b858fa9658aeb97a307 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-list-of-parameter-groups-in-rds.png?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=e21fe21bac05d00b82613b92de2bb69e 2500w" />
    </Frame>

    Click "**Edit parameters**" to unlock editing.

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-header-of-the-view-when-editing-a-parameter-group.png?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=08119cc3d744db24b15237d4f15f457a" alt="The header of the view when editing a parameter group." data-og-width="1374" width="1374" data-og-height="293" height="293" data-path="docs/images/assets/docs/imports/aws-rds-migration-guide/the-header-of-the-view-when-editing-a-parameter-group.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-header-of-the-view-when-editing-a-parameter-group.png?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=00baeaf3bac5809c23f19760f1a93f0a 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-header-of-the-view-when-editing-a-parameter-group.png?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=a0e8a0d6f172998869c40d81ca810681 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-header-of-the-view-when-editing-a-parameter-group.png?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=7a4c06d0ec5d068c7ca41c07cc67746b 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-header-of-the-view-when-editing-a-parameter-group.png?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=728ab3b88fdc122275609a986206edcf 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-header-of-the-view-when-editing-a-parameter-group.png?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=34898994ff2c9df7142a1bc273d0d825 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-header-of-the-view-when-editing-a-parameter-group.png?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=1c1a820d2e98fad946ddac9ad8dc6132 2500w" />
    </Frame>

    Search for "**binlog\_format**" and update:

    * binlog\_format: ROW

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-binlog_format-configuration-required.png?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=f1b3bd8971ffcf58dc357a310e21a0e0" alt="The binlog_format configuration required." data-og-width="1389" width="1389" data-og-height="347" height="347" data-path="docs/images/assets/docs/imports/aws-rds-migration-guide/the-binlog_format-configuration-required.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-binlog_format-configuration-required.png?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=dac891bcd462c138468ca2dd01de4933 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-binlog_format-configuration-required.png?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=793872209ea6a2be557f996849a73bf4 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-binlog_format-configuration-required.png?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=664c5c79ed73161b9be6de2b2d7ec12d 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-binlog_format-configuration-required.png?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=50e5ade40c608e28bf1b13d44e3f0a1e 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-binlog_format-configuration-required.png?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=86136925c73293f699e4b206ac6ca625 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-binlog_format-configuration-required.png?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=5613c34282fe236222d7540f8ed56268 2500w" />
    </Frame>

    Search for "**gtid**" and update:

    * gtid-mode: ON
    * enforce\_gtid\_consistency: ON

    Search for "**sql\_mode**" and update:

    * sql\_mode: NO\_ZERO\_IN\_DATE,NO\_ZERO\_DATE,ONLY\_FULL\_GROUP\_BY

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-gtid-configurations-that-are-required.png?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=55edf3b3c4455e133be34024547e390c" alt="The GTID configurations that are required." data-og-width="1377" width="1377" data-og-height="977" height="977" data-path="docs/images/assets/docs/imports/aws-rds-migration-guide/the-gtid-configurations-that-are-required.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-gtid-configurations-that-are-required.png?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=4456f5b16ad17fcd2c82848e05e30ea2 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-gtid-configurations-that-are-required.png?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=08338d920e050cf67a2daf99eca3b545 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-gtid-configurations-that-are-required.png?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=87c61a4e4fbbf89c5194b0ad1e5028b0 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-gtid-configurations-that-are-required.png?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=9e57f4be6d349c121dd9c2f717c328fa 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-gtid-configurations-that-are-required.png?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=b3213ef7eaa9f71d407f6c75bb594757 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-gtid-configurations-that-are-required.png?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=99bdbca00d7c7341c6435459e0cb6540 2500w" />
    </Frame>

    Click "**Save changes**".
  </Step>

  <Step>
    Associate the parameter group to your database. Select "**Databases**" from the left nav, check the **select box** next to your database instance, and click "**Modify**".

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-list-of-databases-in-rds.png?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=96eeb7470cdab8debb088431bd19dfcf" alt="The list of databases in RDS." data-og-width="1406" width="1406" data-og-height="782" height="782" data-path="docs/images/assets/docs/imports/aws-rds-migration-guide/the-list-of-databases-in-rds.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-list-of-databases-in-rds.png?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=b068c468f0ee86a1b41b5377e2a47747 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-list-of-databases-in-rds.png?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=dfb55df507086ec41e5da6ad370d02ea 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-list-of-databases-in-rds.png?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=0b594c9ea0dba0e21f1093c49b75f11f 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-list-of-databases-in-rds.png?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=0fdb4eb5c94b3844337176efda0a1650 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-list-of-databases-in-rds.png?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=90bf8770dcf1b6ff28c9ec69fdedd240 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-list-of-databases-in-rds.png?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=ee687367d153fbeb45f08f0a48ed0fe7 2500w" />
    </Frame>

    Scroll to **Additional configuration** section. Update the **DB parameter group** to your new parameter group. Click "**Continue**".

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-additional-configuration-section-of-the-database-configuration-view.png?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=9ad37f2425a5f9dcd1dff618d683689a" alt="The Additional configuration section of the database configuration view." data-og-width="816" width="816" data-og-height="307" height="307" data-path="docs/images/assets/docs/imports/aws-rds-migration-guide/the-additional-configuration-section-of-the-database-configuration-view.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-additional-configuration-section-of-the-database-configuration-view.png?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=dfae9771e8f0607d24eb671c4b5aaa82 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-additional-configuration-section-of-the-database-configuration-view.png?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=1ead96bd4d4e2897fb3c6d631e463718 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-additional-configuration-section-of-the-database-configuration-view.png?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=3feaac5cc81f88822f736cad33f03b65 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-additional-configuration-section-of-the-database-configuration-view.png?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=c7a533ddedfddfffee5d347e9ec8c987 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-additional-configuration-section-of-the-database-configuration-view.png?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=3531db2c381faf6295e74a56e3d68e0f 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-additional-configuration-section-of-the-database-configuration-view.png?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=d5c9c7eaf15ef58dc92165054b6bf311 2500w" />
    </Frame>

    Choose when to apply:

    * **Apply during the next scheduled maintenance window** - Applied during maintenance window
    * **Apply immediately** - Applied now, but requires manual reboot

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-confirmation-view-that-is-displayed-when-modifying-rds-database-settings.png?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=1ee70e0553a9158d61f67317f530bd56" alt="The confirmation view that is displayed when modifying RDS database settings." data-og-width="805" width="805" data-og-height="643" height="643" data-path="docs/images/assets/docs/imports/aws-rds-migration-guide/the-confirmation-view-that-is-displayed-when-modifying-rds-database-settings.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-confirmation-view-that-is-displayed-when-modifying-rds-database-settings.png?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=e8ff88d65cd326a7151057a07b6d5e8e 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-confirmation-view-that-is-displayed-when-modifying-rds-database-settings.png?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=cc30674f7a363dbe0fb9d110bf2d0f05 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-confirmation-view-that-is-displayed-when-modifying-rds-database-settings.png?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=9844c443eb94bb6ee803a88a6c9d6c40 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-confirmation-view-that-is-displayed-when-modifying-rds-database-settings.png?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=c036b5d81df84dc52d6c8f65da610b78 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-confirmation-view-that-is-displayed-when-modifying-rds-database-settings.png?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=d356b434c83d9844a353788906991286 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-confirmation-view-that-is-displayed-when-modifying-rds-database-settings.png?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=b79870916839e51e2edeeed5dd1655f3 2500w" />
    </Frame>

    Click "**Modify DB instance**".
  </Step>

  <Step>
    Reboot your database instance to apply the settings. Click "**Actions**" > "**Reboot**".

    <Warning>
      This will briefly disconnect active users! The parameter group changes won't take effect without a reboot.
    </Warning>

    You'll be presented with a page to confirm the database you want to reboot. Click "**Confirm**" and the database will begin rebooting.

    If you opted to apply changes immediately, monitor the status on the "**Configuration**" tab. The page does not automatically update, so refresh to check status.

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-configuration-tab-with-the-new-parameter-group-applying.png?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=189da81d821db253e547ac212270178a" alt="The configuration tab with the new parameter group applying." data-og-width="1378" width="1378" data-og-height="888" height="888" data-path="docs/images/assets/docs/imports/aws-rds-migration-guide/the-configuration-tab-with-the-new-parameter-group-applying.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-configuration-tab-with-the-new-parameter-group-applying.png?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=de27ad5c4f397f20dff9abb112752993 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-configuration-tab-with-the-new-parameter-group-applying.png?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=67f8bda0d1224ff2709cbe890ffc2fea 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-configuration-tab-with-the-new-parameter-group-applying.png?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=cee1585da284ea0fea4b1c8613f38509 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-configuration-tab-with-the-new-parameter-group-applying.png?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=5c5e50029370069cf34e160b8837a6de 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-configuration-tab-with-the-new-parameter-group-applying.png?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=b91215045b11fa8064a398621f9d6a63 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-configuration-tab-with-the-new-parameter-group-applying.png?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=15f2c9de6f4dc231d24662c2d97e9ee9 2500w" />
    </Frame>
  </Step>
</Steps>

## Step 2: Enable binary logging

Binary logging must be enabled for the import to work. On RDS, binary logging is tied to automated backups.

To enable binary logging, [enable automated backups](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html#USER_WorkingWithAutomatedBackups.Enabling) by setting the backup retention period to any value greater than zero days.

Verify binary logging is enabled:

```sql  theme={null}
mysql> show variables like 'log_bin';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| log_bin       | ON    |
+---------------+-------+
```

## Step 3: Configure binlog retention

Set the binary log retention period to ensure logs aren't purged during the import. For most cases, 48 hours is sufficient, but larger imports may need more time.

<Warning>
  Longer retention periods use more disk space. Evaluate your binlog size to avoid running out of disk space. Contact [PlanetScale Support](https://support.planetscale.com/hc/en-us) if you need assistance.
</Warning>

Set the retention period using the `mysql.rds_set_configuration()` procedure:

```sql  theme={null}
CALL mysql.rds_set_configuration('binlog retention hours', 48);
```

Verify the setting:

```sql  theme={null}
CALL mysql.rds_show_configuration;
```

Expected output:

```
+------------------------+-------+-----------------------------------------------------------------------------------------------------------+
| name                   | value | description                                                                                               |
+------------------------+-------+-----------------------------------------------------------------------------------------------------------+
| binlog retention hours | 48    | binlog retention hours specifies the duration in hours before binary logs are automatically deleted.      |
+------------------------+-------+-----------------------------------------------------------------------------------------------------------+
```

## Step 4: Ensure database is publicly accessible

PlanetScale needs to connect to your RDS database over the internet. Check that your database is publicly accessible.

In your database instance, go to "**Connectivity & security**" tab. Under "**Security**", check if **Publicly accessible** is set to "Yes". If it says "No", you'll need to modify the database settings to enable public access.

If you cannot make the database publicly accessible, [contact us](https://planetscale.com/contact) to discuss alternative import options.

## Step 5: Create a migration user

Create a dedicated user with limited privileges for the import process.

Connect to your RDS database using the MySQL command line with your master credentials:

```bash  theme={null}
mysql -u admin -p -h [your-rds-endpoint]
```

Run the following script, replacing the placeholders:

* `<SUPER_STRONG_PASSWORD>` - Password for the migration\_user account
* `<DATABASE_NAME>` - Name of the database you're importing

```sql  theme={null}
CREATE USER 'migration_user'@'%' IDENTIFIED BY '<SUPER_STRONG_PASSWORD>';
GRANT PROCESS, REPLICATION SLAVE, REPLICATION CLIENT, RELOAD ON *.* TO 'migration_user'@'%';
GRANT SELECT, INSERT, UPDATE, DELETE, SHOW VIEW, LOCK TABLES ON `<DATABASE_NAME>`.* TO 'migration_user'@'%';
GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, ALTER ON `ps\_import\_%`.* TO 'migration_user'@'%';
GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, ALTER ON `_vt`.* TO 'migration_user'@'%';
GRANT EXECUTE ON PROCEDURE mysql.rds_show_configuration TO 'migration_user'@'%';
GRANT SELECT ON mysql.db TO 'migration_user'@'%';
GRANT SELECT ON mysql.func TO 'migration_user'@'%';
GRANT SELECT ON mysql.tables_priv TO 'migration_user'@'%';
GRANT SELECT ON mysql.user TO 'migration_user'@'%';
GRANT SELECT ON performance_schema.* TO 'migration_user'@'%';
FLUSH PRIVILEGES;
```

Save the username and password securely - you'll need them for the import.

## Step 6: Configure RDS security group

Allow PlanetScale to connect by adding PlanetScale's IP addresses to your security group.

The specific IP addresses depend on your PlanetScale database region. These will be shown during the import workflow on the **Connect to external database** step. See the [Import public IP addresses](/docs/vitess/imports/import-tool-migration-addresses) page for more details.

### Add IP addresses to security group

Navigate to "**Connectivity & security**" tab of your database instance and click the VPC security group link.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-connectivity-and-security-tab-of-the-database-view-in-rds.png?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=870e66547a25ea0fe921cf9f76434045" alt="The Connectivity & security tab of the database view in RDS." data-og-width="1382" width="1382" data-og-height="970" height="970" data-path="docs/images/assets/docs/imports/aws-rds-migration-guide/the-connectivity-and-security-tab-of-the-database-view-in-rds.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-connectivity-and-security-tab-of-the-database-view-in-rds.png?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=01072f83f4a0c3b2c218dbaa3a8dd8b6 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-connectivity-and-security-tab-of-the-database-view-in-rds.png?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=952f795452721e33f4b1a30c1d25e560 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-connectivity-and-security-tab-of-the-database-view-in-rds.png?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=5978d00aa27bf9a2fd417e91e5bc8f4e 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-connectivity-and-security-tab-of-the-database-view-in-rds.png?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=d3ddfff1680a82a748bb36cbdac364be 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-connectivity-and-security-tab-of-the-database-view-in-rds.png?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=7c4695ec2d28bbb69338d71d8398ff25 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-connectivity-and-security-tab-of-the-database-view-in-rds.png?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=31e2b24301f8349cfbfd23f5766ddd68 2500w" />
</Frame>

Select "**Inbound rules**" tab, then "**Edit inbound rules**".

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-view-of-security-groups-associated-with-the-rds-instance.png?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=3b08f2a427d49727fc071e4ed86628e5" alt="The view of security groups associated with the RDS instance." data-og-width="1440" width="1440" data-og-height="981" height="981" data-path="docs/images/assets/docs/imports/aws-rds-migration-guide/the-view-of-security-groups-associated-with-the-rds-instance.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-view-of-security-groups-associated-with-the-rds-instance.png?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=4361d5f6db9992f61cff08bf4fca6b00 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-view-of-security-groups-associated-with-the-rds-instance.png?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=8898bad933120556731dd9697c830c7d 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-view-of-security-groups-associated-with-the-rds-instance.png?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=875859633e5875a7b367ff14c82160d8 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-view-of-security-groups-associated-with-the-rds-instance.png?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=ae2a1524936bb8d70e73ef517cc305ee 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-view-of-security-groups-associated-with-the-rds-instance.png?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=d6228cb6a412d00dea0ef241691dfb63 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-view-of-security-groups-associated-with-the-rds-instance.png?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=14195e865041f44f93373829eac5a713 2500w" />
</Frame>

Click "**Add rule**", then:

* **Type**: Select `MYSQL/Aurora`
* **Source**: Enter the first PlanetScale IP address (AWS will format it as `x.x.x.x/32`)

Repeat for each IP address in your region, then click "**Save rules**".

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-edit-inbound-rules-view-where-source-traffic-can-be-allowed.png?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=6f47823af9b1ac30ac4539403d1a0215" alt="The Edit inbound rules view where source traffic can be allowed." data-og-width="1419" width="1419" data-og-height="597" height="597" data-path="docs/images/assets/docs/imports/aws-rds-migration-guide/the-edit-inbound-rules-view-where-source-traffic-can-be-allowed.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-edit-inbound-rules-view-where-source-traffic-can-be-allowed.png?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=299ad1895dd115798b1f074186f94665 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-edit-inbound-rules-view-where-source-traffic-can-be-allowed.png?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=9a4db15d937996c2536f0f8146dc5b62 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-edit-inbound-rules-view-where-source-traffic-can-be-allowed.png?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=282321b8f65b2b2103eacc14078e4a12 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-edit-inbound-rules-view-where-source-traffic-can-be-allowed.png?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=ef4dd2841399ed274fde95bf2284a2af 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-edit-inbound-rules-view-where-source-traffic-can-be-allowed.png?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=e9134bff585bde08831853c5f760b064 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/aws-rds-migration-guide/the-edit-inbound-rules-view-where-source-traffic-can-be-allowed.png?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=0b4a9650b59e4d07cc63cba65021873d 2500w" />
</Frame>

## Importing your database

Now that your RDS database is configured, follow the [Database Imports guide](/docs/vitess/imports/database-imports) to complete your import.

When filling out the connection form in the import workflow, use:

* **Host name** - Your RDS endpoint address (from Prerequisites)
* **Port** - 3306 (or your custom port)
* **Database name** - The exact database name to import
* **Username** - `migration_user`
* **Password** - The password you set in Step 5
* **SSL verification mode** - Select based on your RDS SSL configuration

The Database Imports guide will walk you through:

* Creating your PlanetScale database
* Connecting to your RDS database
* Validating your configuration
* Selecting tables to import
* Monitoring the import progress
* Switching traffic and completing the import

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Azure Database for MySQL migration guide
Source: https://planetscale.com/docs/vitess/imports/azure-database-for-mysql-migration-guide



## Overview

This document will demonstrate how to migrate a database from Azure Database for MySQL to PlanetScale. We recommend reading through the [Database import documentation](/docs/vitess/imports/database-imports) to learn how our import tool works before proceeding.

## Prerequisites

Before you can perform the migration, you’ll need to gather the following information from you MySQL instance in Azure:

* Server name
* Server admin login name
* Server admin password
* Database name

The server name and admin login name can be located on the **Overview** tab of the MySQL instance in Azure.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/the-server-name-and-server-admin-login-name-located-in-the-azure-dashboard.png?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=ab06902db06f591fb7bba9cb32d54240" alt="The server name and server admin login name located in the Azure dashboard." data-og-width="1718" width="1718" data-og-height="498" height="498" data-path="docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/the-server-name-and-server-admin-login-name-located-in-the-azure-dashboard.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/the-server-name-and-server-admin-login-name-located-in-the-azure-dashboard.png?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=0325c637fd86580988221ea7793647b9 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/the-server-name-and-server-admin-login-name-located-in-the-azure-dashboard.png?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=4fbd300ea20a99c25ec9b7e4b182bd54 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/the-server-name-and-server-admin-login-name-located-in-the-azure-dashboard.png?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=66a68acb59302ecb0e4bce676573e525 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/the-server-name-and-server-admin-login-name-located-in-the-azure-dashboard.png?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=0b847cff2e57589b65a5ec30b39767eb 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/the-server-name-and-server-admin-login-name-located-in-the-azure-dashboard.png?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=57cd45105387c49785c91f914fc8a5b6 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/the-server-name-and-server-admin-login-name-located-in-the-azure-dashboard.png?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=3553eccd702281809393f43625d9102a 2500w" />
</Frame>

The server admin password is the same password you set when initially creating the database instance.

To view your available databases, select the **Databases** tab from the sidebar.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/the-databases-tab-of-the-azure-dashboard.png?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=c4f0137e8d4d1e74c6c14a30f1a7ff13" alt="The databases tab of the Azure dashboard." data-og-width="865" width="865" data-og-height="502" height="502" data-path="docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/the-databases-tab-of-the-azure-dashboard.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/the-databases-tab-of-the-azure-dashboard.png?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=1ec22cfd71ba14b2974094f902de2ce2 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/the-databases-tab-of-the-azure-dashboard.png?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=e40edfe07bc174966fb80581733b8882 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/the-databases-tab-of-the-azure-dashboard.png?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=8315091123225c46e82a24dad9f38946 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/the-databases-tab-of-the-azure-dashboard.png?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=292f473dc25a690150d4ac857a4dbade 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/the-databases-tab-of-the-azure-dashboard.png?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=e2dffd1fff7f45765df1b4d557064a92 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/the-databases-tab-of-the-azure-dashboard.png?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=3829e160532e4160e236f132606b625e 2500w" />
</Frame>

## Configure firewall rules

In order for PlanetScale to connect to your Azure database, you must allow traffic into the database through the associated security group. The specific IP addresses you will need to allow depend on the region you plan to host your PlanetScale database. Check the [Import tool public IP addresses page](/docs/vitess/imports/import-tool-migration-addresses) to determine the IP addresses to allow before continuing. This guide will use the **AWS us-east-1 (North Virginia)** region so we’ll allow the following addresses:

```
3.209.149.66
3.215.97.46
34.193.111.15
23.23.187.137
52.6.141.108
52.70.2.89
50.17.188.76
52.2.251.189
52.72.234.74
35.174.68.24
52.5.253.172
54.156.81.4
34.200.24.255
35.174.79.154
44.199.177.24
```

To allow traffic into your Azure database, navigate to the “**Networking**” section from the sidebar and locate the **Firewall rules** section. There are already a series of inputs allowing you to add entries into the Firewall rules, each of which will permit network traffic from that IP address. Add a new entry for each address required, then click “Save” from the toolbar.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/the-networking-tab-of-the-azure-dashboard.png?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=b4c43428b328d74e2b04fe4aac038a99" alt="The networking tab of the Azure dashboard." data-og-width="1709" width="1709" data-og-height="939" height="939" data-path="docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/the-networking-tab-of-the-azure-dashboard.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/the-networking-tab-of-the-azure-dashboard.png?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=69283b905bddd01b09112b3cd01bda82 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/the-networking-tab-of-the-azure-dashboard.png?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=10d08764b66f8016ac3a80aa92f48cd5 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/the-networking-tab-of-the-azure-dashboard.png?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=88d6034957b727585a9f69e268fc58fb 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/the-networking-tab-of-the-azure-dashboard.png?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=d451e19cc31a5c4abf52f2028a612046 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/the-networking-tab-of-the-azure-dashboard.png?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=9667387b6c046de943001ba45147e19c 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/the-networking-tab-of-the-azure-dashboard.png?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=d6abcd587ae30b5f90362159af195723 2500w" />
</Frame>

## Configure MySQL server settings

There are three settings that need to be configured before you can import your database:

* gtid\_mode
* enforce\_gtid\_consistency
* binlog\_row\_image

To access these settings in Azure, select “**Server parameters**” from the sidebar and enter “**gtid**” in the search bar. Set both “**enforce\_gtid\_consistency**” and “**gtid\_mode**” to “**ON**”. Next, search for “**binlog\_row\_image**” and set to “**full**”. Click “**Save**”.

<Note>
  For “**gtid\_mode**”, you’ll need to update the value in sequence displayed in the dropdown until it is set to “**ON**”. For example, if the current setting is “**OFF\_PERMISSIVE**”, you’ll need to first change it to “**ON\_PERMISSIVE**”, save the changes, then set it to “**ON**” in that order.
</Note>

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/how-to-access-gtid-settings-in-the-azure-dashboard.png?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=953cfd080082e485f14b880cf1f0b0f1" alt="How to access gtid settings in the Azure dashboard." data-og-width="2132" width="2132" data-og-height="1274" height="1274" data-path="docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/how-to-access-gtid-settings-in-the-azure-dashboard.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/how-to-access-gtid-settings-in-the-azure-dashboard.png?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=b3f2d8018eb69cba4fd43a11dbbad2af 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/how-to-access-gtid-settings-in-the-azure-dashboard.png?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=b9c9ca3fe51779243fa5356f4a840ef2 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/how-to-access-gtid-settings-in-the-azure-dashboard.png?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=2d196faebb8ca2ea838da32858d085e0 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/how-to-access-gtid-settings-in-the-azure-dashboard.png?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=088b295f20f50b12b3d7cb0fec18f1ff 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/how-to-access-gtid-settings-in-the-azure-dashboard.png?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=73954ad2f4b9fe663f954f35edb9a3f3 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/azure-database-for-mysql-migration-guide/how-to-access-gtid-settings-in-the-azure-dashboard.png?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=b98bdd22315bbbd7cd3be29cad590c57 2500w" />
</Frame>

## Import your database

Now that your Azure Database for MySQL is configured and ready, follow the [Database Imports guide](/docs/vitess/imports/database-imports) to complete your import.

When filling out the connection form in the import workflow, use the following information:

* **Host name** - Your Azure server name (from Prerequisites)
* **Port** - 3306 (default for Azure MySQL)
* **Database name** - The exact database name to import
* **Username** - Your server admin login name
* **Password** - Your server admin password
* **SSL verification mode** - Select "**Verify Identity**" (Verify certificate and hostname)

The Database Imports guide will walk you through:

* Creating your PlanetScale database
* Connecting to your Azure MySQL database
* Validating your configuration
* Selecting tables to import
* Monitoring the import progress
* Switching traffic and completing the import

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Database Imports
Source: https://planetscale.com/docs/vitess/imports/database-imports



## Overview

PlanetScale provides an import tool in the dashboard that allows you to painlessly import an **existing internet-accessible MySQL or MariaDB database** into a PlanetScale database with *no downtime*.

<Note>
  You must be an [Organization Administrator](/docs/security/access-control#organization-administrator) to use this feature.
</Note>

Before you begin, it may be helpful to check out our [general MySQL compatibility guide](/docs/vitess/troubleshooting/mysql-compatibility).

## Import process overview

The import workflow gives you visibility into every step of your database migration. You'll see real-time progress, detailed logs, and replication metrics throughout. Here's what the process looks like:

1. **Create database** - Set up your PlanetScale database
2. **Connect to external database** - Add connection credentials and SSL/TLS settings
3. **Validate connection and schema** - We check connectivity, server configuration, and schema compatibility
4. **Select tables** - Pick which tables to import (all tables imported if foreign keys detected)
5. **Start workflow** - Kick off the import
6. **Monitor import** - Watch progress with real-time logs, per-table progress, and replication lag information
7. **Complete import** - Finalize and detach from your external database

<Note>
  **Note**

  It's recommended to avoid all schema changes / DDL (Data Definition Language) statements during an import on both your source database and the PlanetScale database. This includes `CREATE`, `DROP`, `ALTER`, `TRUNCATE`, etc.
</Note>

## Step 1: Create your PlanetScale database

<Steps>
  <Step>
    Head to your PlanetScale dashboard and click on "**New database**" > "**Import database**".

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows/import-database-dropdown.png?fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=dc2f02fdae66525265d68a569e5d4770" alt="Import database dropdown." data-og-width="1898" width="1898" data-og-height="666" height="666" data-path="docs/images/assets/docs/imports/import-workflows/import-database-dropdown.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows/import-database-dropdown.png?w=280&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=0a45d2068d298febb277ec7c2bc407f9 280w, https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows/import-database-dropdown.png?w=560&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=3395254e30c5b0309543fcd51b124838 560w, https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows/import-database-dropdown.png?w=840&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=ad136fcd63be93eb8b840e431ce7ce7e 840w, https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows/import-database-dropdown.png?w=1100&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=184ddc9c79ad3576042ea6efef1e8918 1100w, https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows/import-database-dropdown.png?w=1650&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=c0fdbc9cebe1f9b222f7b37e6c6c389b 1650w, https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows/import-database-dropdown.png?w=2500&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=381259e00da6dc00ce074b48ae222cee 2500w" />
    </Frame>
  </Step>

  <Step>
    Give your imported database a name and [select a region](/docs/vitess/regions) from the dropdown.

    We recommend using the same name as the database you're importing from to avoid updating any database name references throughout your application code. If you'd prefer to use a different database name, make sure to update your app where applicable once you fully switch over to PlanetScale.
  </Step>

  <Step>
    Click "**Create database**" to proceed to the import workflow setup.
  </Step>
</Steps>

## Step 2: Connect to your external database

You'll be taken to the import workflow page where you can configure the connection to your external MySQL or MariaDB database.

### Connection settings

Fill in your connection info:

**Host name** - The address where your database is hosted.

**Port** - The port where your database is hosted. The default MySQL port is `3306`.

**Database name** - The exact database name you want to import.

**SSL verification mode** - Choose from these options:

* **Disabled** - No SSL encryption (not recommended for production)
* **Preferred** - Use SSL if available, otherwise connect without SSL
* **Required** - SSL is required, but certificate is not verified
* **Verify CA** - SSL is required and the certificate is verified against the CA
* **Verify Identity** - SSL is required and the certificate hostname is verified

If your database server has a valid SSL certificate, set this to `Required` or higher.
For more information about certificates from a Certificate Authority, check out our [Secure connections documentation](/docs/vitess/connecting/secure-connections#certificate-authorities).

**Username** - The username to connect with. This user needs proper permissions. See our [import tool user requirements guide](/docs/vitess/imports/import-tool-user-requirements) for the full list of required grants.

### Authentication method

Pick your authentication method:

**Authenticate with password:**
Provide the password for the username you entered.

**Authenticate with mTLS (mutual TLS):**

* **SSL client certificate** - Certificate to authenticate PlanetScale with your database server
* **SSL client key** - The private key for the client certificate

### Advanced settings (optional)

Click "**Show advanced settings**" for more options:

* **Import connections** - Maximum number of concurrent connections for the import (max 100)
* **Minimum TLS version** - Choose from TLS 1.0, 1.1, 1.2, or 1.3
* **SSL server name override** - Override the server name for SSL certificate verification
* **SSL CA certificate chain** - If your database server has a certificate with a non-trusted root CA, provide the full CA certificate chain here

<Note>
  **Note**

  You must have [binary logs](https://dev.mysql.com/doc/refman/8.0/en/binary-log.html) enabled on the database you're importing. See our [provider-specific migration guides](/docs/vitess/imports/database-imports#provider-specific-guides) for instructions on enabling binary logging.
</Note>

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/4VhQ3O-86dqtVudH/docs/images/assets/docs/imports/import-workflows/external-database-connection-settings.png?fit=max&auto=format&n=4VhQ3O-86dqtVudH&q=85&s=c24607fa1f6dbd1ab7f6a049f26bdc20" alt="The connection form with SSL/TLS settings." data-og-width="2670" width="2670" data-og-height="3488" height="3488" data-path="docs/images/assets/docs/imports/import-workflows/external-database-connection-settings.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/4VhQ3O-86dqtVudH/docs/images/assets/docs/imports/import-workflows/external-database-connection-settings.png?w=280&fit=max&auto=format&n=4VhQ3O-86dqtVudH&q=85&s=da7681e3794b71233690bf73bf01222b 280w, https://mintcdn.com/planetscale-cad1a68a/4VhQ3O-86dqtVudH/docs/images/assets/docs/imports/import-workflows/external-database-connection-settings.png?w=560&fit=max&auto=format&n=4VhQ3O-86dqtVudH&q=85&s=23792d85ae1ba08829796fc940c5d81d 560w, https://mintcdn.com/planetscale-cad1a68a/4VhQ3O-86dqtVudH/docs/images/assets/docs/imports/import-workflows/external-database-connection-settings.png?w=840&fit=max&auto=format&n=4VhQ3O-86dqtVudH&q=85&s=2b17379a88e324eade2119555a11df52 840w, https://mintcdn.com/planetscale-cad1a68a/4VhQ3O-86dqtVudH/docs/images/assets/docs/imports/import-workflows/external-database-connection-settings.png?w=1100&fit=max&auto=format&n=4VhQ3O-86dqtVudH&q=85&s=9313a79fa523bf0f154303b4760acf47 1100w, https://mintcdn.com/planetscale-cad1a68a/4VhQ3O-86dqtVudH/docs/images/assets/docs/imports/import-workflows/external-database-connection-settings.png?w=1650&fit=max&auto=format&n=4VhQ3O-86dqtVudH&q=85&s=108f858da96f92161317b46f3c4ec6f9 1650w, https://mintcdn.com/planetscale-cad1a68a/4VhQ3O-86dqtVudH/docs/images/assets/docs/imports/import-workflows/external-database-connection-settings.png?w=2500&fit=max&auto=format&n=4VhQ3O-86dqtVudH&q=85&s=7bd663d1a05a2e94f5632e376498b214 2500w" />
</Frame>

## Step 3: Validate connection and schema

Once you've filled in your connection info, click "**Connect to database**". PlanetScale will run some checks on your external database.

### Connectivity check

We'll make sure we can connect to your database with the credentials and SSL/TLS settings you provided.

### Server configuration check

These server configuration values need to be set correctly for the import to work:

| Variable                       | Required Value | Documentation                                                                                                                  |
| :----------------------------- | :------------- | :----------------------------------------------------------------------------------------------------------------------------- |
| `gtid_mode`                    | `ON`           | [Documentation](https://dev.mysql.com/doc/refman/5.7/en/replication-options-gtids.html#sysvar_gtid_mode)                       |
| `binlog_format`                | `ROW`          | [Documentation](https://dev.mysql.com/doc/refman/5.7/en/replication-options-binary-log.html#sysvar_binlog_format)              |
| `binlog_row_image`             | `FULL`         | [Documentation](https://dev.mysql.com/doc/refman/8.0/en/replication-options-binary-log.html#sysvar_binlog_row_image)           |
| `expire_logs_days`\*           | `> 2`          | [Documentation](https://dev.mysql.com/doc/refman/8.0/en/replication-options-binary-log.html#sysvar_expire_logs_days)           |
| `binlog_expire_logs_seconds`\* | `> 172800`     | [Documentation](https://dev.mysql.com/doc/refman/8.0/en/replication-options-binary-log.html#sysvar_binlog_expire_logs_seconds) |

**\*** Either `expire_logs_days` or `binlog_expire_logs_seconds` needs to be set. If both are set, `binlog_expire_logs_seconds` takes precedence.

### Schema compatibility check

We'll look for any compatibility issues with your schema:

* **Missing unique key** - All tables must have a unique, not-null key. See our [Changing unique keys documentation](/docs/vitess/schema-changes/onlineddl-change-unique-keys) for more info.
* **Invalid charset** - We support `utf8`, `utf8mb4`, `utf8mb3`, `latin1`, and `ascii`. Tables with other charsets will be flagged.
* **Table names with special characters** - Tables with characters outside the standard ASCII set aren't supported.
* **Views** - Views are detected but won't be imported. You can create them manually after the import finishes.
* **Unsupported storage engines** - Only `InnoDB` is supported.
* **Foreign key constraints** - Detected and flagged for special handling (see below).

### Handling validation errors

If validation fails, you'll see error messages with links to troubleshooting docs. You have two options:

1. **Fix the issues** - Go back to your external database, fix the configuration or schema issues, and try connecting again. [Contact support](https://planetscale.com/contact?initial=support) if you encounter trouble addressing the incompatibilities.
2. **Skip and continue** - For certain failures, you can proceed anyway. Not recommended since this may cause the import to fail later.

<Warning>
  **Warning**

  If you choose to skip validation errors and proceed, all tables will be imported automatically (you won't be able to select specific tables). This may result in unexpected behavior or import failures.
</Warning>

## Step 4: Foreign key constraints

If your database uses foreign key constraints, we'll detect them during validation and automatically enable foreign key support.

### Important things to know

When importing with foreign keys:

* **All tables will be imported** - You can't select a subset of tables when foreign keys are present. This keeps referential integrity intact.
* **Use a replica if possible** - The foreign key import holds a long-running transaction on the source database, which can increase load. We recommend connecting to a replica instead of your primary.
* **Import retries** - If your import fails, it starts over from the beginning. Unlike regular imports, we can't resume from where we left off.

For more information about foreign key support and limitations, see our [foreign key constraints documentation](/docs/vitess/foreign-key-constraints).

## Step 5: Select tables to import

After validation passes, you'll see a workflow form with your source database on the left (with provider logo and table list) and PlanetScale on the right (with target keyspace and shard count). In Vitess a [keyspace](/docs/vitess/sharding/keyspaces) is the equivalent of a single, logical MySQL databases.

### Table selection

**If foreign keys were detected:**

* All tables are automatically selected
* You can't deselect individual tables
* You'll see: "All tables will be replicated due to foreign key constraints usage"

**If no foreign keys:**

* Select all tables or pick specific ones
* You can start with a subset of tables for testing if you want

### Workflow validation

Before creating the workflow, click "**Validate**" to run pre-migration checks:

* Safe migrations is enabled
* [VSchema](/docs/vitess/sharding/vschema) is valid
* Tables will be created automatically in the target keyspace (PlanetScale database)
* Enough storage is available

Once these checks pass, the "**Create workflow**" button will light up.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/4VhQ3O-86dqtVudH/docs/images/assets/docs/imports/import-workflows/validate-import-workflow.png?fit=max&auto=format&n=4VhQ3O-86dqtVudH&q=85&s=4d330febc731c349473ef66a2cfb741a" alt="The validation results showing checks passed and table list." data-og-width="2670" width="2670" data-og-height="2950" height="2950" data-path="docs/images/assets/docs/imports/import-workflows/validate-import-workflow.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/4VhQ3O-86dqtVudH/docs/images/assets/docs/imports/import-workflows/validate-import-workflow.png?w=280&fit=max&auto=format&n=4VhQ3O-86dqtVudH&q=85&s=7b7e4b93f023ff231dc5a5b7ab351346 280w, https://mintcdn.com/planetscale-cad1a68a/4VhQ3O-86dqtVudH/docs/images/assets/docs/imports/import-workflows/validate-import-workflow.png?w=560&fit=max&auto=format&n=4VhQ3O-86dqtVudH&q=85&s=eeb62fa716d33204058df9de906421d5 560w, https://mintcdn.com/planetscale-cad1a68a/4VhQ3O-86dqtVudH/docs/images/assets/docs/imports/import-workflows/validate-import-workflow.png?w=840&fit=max&auto=format&n=4VhQ3O-86dqtVudH&q=85&s=987b14e380151c0bfaebb6fb07df8477 840w, https://mintcdn.com/planetscale-cad1a68a/4VhQ3O-86dqtVudH/docs/images/assets/docs/imports/import-workflows/validate-import-workflow.png?w=1100&fit=max&auto=format&n=4VhQ3O-86dqtVudH&q=85&s=70a50330bcd3e014009dc990dc2556f3 1100w, https://mintcdn.com/planetscale-cad1a68a/4VhQ3O-86dqtVudH/docs/images/assets/docs/imports/import-workflows/validate-import-workflow.png?w=1650&fit=max&auto=format&n=4VhQ3O-86dqtVudH&q=85&s=55bf42b4230bdf68049ff5fafbe5ca08 1650w, https://mintcdn.com/planetscale-cad1a68a/4VhQ3O-86dqtVudH/docs/images/assets/docs/imports/import-workflows/validate-import-workflow.png?w=2500&fit=max&auto=format&n=4VhQ3O-86dqtVudH&q=85&s=3802f42b8658eb23a9292cc3a30285ab 2500w" />
</Frame>

### Advanced options

Click "**Advanced options**" to see additional settings that can optimize your import:

**Defer secondary index creation**

Checked by default. Creates secondary indexes (non-primary indexes) after copying data instead of during the initial copy.

* Why this helps: Maintaining many indexes while inserting data is slow. By deferring index creation until after all data is copied, your import can be significantly faster (often 2-3x faster for tables with multiple indexes).

* When it's disabled: Import will run slower. Automatically disabled for imports with foreign keys, since foreign key constraints require indexes to exist during the copy phase.

**DDL handling**

Controls what happens if schema changes (like `ALTER TABLE`, `ADD INDEX`, etc.) occur on your external database while the import is running.

* **STOP** (default, recommended) - The workflow stops immediately when schema changes are detected. You'll need to manually restart the workflow after reviewing the changes. This is the safest option because it lets you verify the schema changes won't cause issues before continuing.

* **IGNORE** - Schema changes are skipped and won't be applied to your PlanetScale database. Your import continues without interruption, but your schemas will diverge. Only use this if you're confident you don't need these changes or plan to apply them manually to your PlanetScale database later.

* **EXEC** - Schema changes are automatically applied to your PlanetScale database while the import continues running. If applying a schema change fails (for example, if it's not compatible with Vitess), the workflow stops and you'll need to restart it. Use this if you need schema changes to sync automatically but want safety checks.

* **EXEC\_IGNORE** - Attempts to apply schema changes but keeps running even if they fail.

<Warning>
  `EXEC_IGNORE` can lead to schema mismatches between your external database and PlanetScale database, potentially causing data inconsistencies or unexpected behavior. Only use this if you understand the risks and have a plan to handle failures.
</Warning>

<Note>
  **Important**

  Schema changes during an active import can cause problems. This setting is a safety mechanism for unexpected changes, not a way to intentionally modify schemas mid-import. If possible, avoid making schema changes until the import completes.
</Note>

**Global keyspace**

Not applicable for external database imports. This setting is only used when moving tables between keyspaces within PlanetScale.
When moving tables with `AUTO_INCREMENT` columns from an unsharded to a sharded keyspace, Vitess needs a place to store "sequence tables" that coordinate ID generation across shards. This setting specifies which unsharded keyspace should hold those sequence tables.
You can ignore this setting for external database imports.

## Step 6: Start the import workflow

After validation passes. click "**Create workflow**" to start the import process. You'll be redirected to the workflow monitoring page where you can track your import in real-time.

## Step 7: Monitor your import

The monitoring page shows you real-time progress of your import.

### Connection status

At the top, you'll see:

* A live connection indicator (green pulsing dot when connected)
* Your external database name and hostname
* Workflow info (name, who started it, when)

### Visual replication flow

The main view shows data flowing from your external database to PlanetScale:

**Source keyspace (left):**

* List of tables being imported
* Progress donuts for each table during the copy phase (0-100%)
* Row counts per table

**Replication arrow (center):**

* Animated arrow showing data flow direction
* Current phase ("Copying data" or "Replicating data")
* Replication lag graph with current lag in seconds

**Target keyspace (right):**

* Your PlanetScale shards (only one shard in most cases)
* Traffic serving status

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows/copying-phase.png?fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=d6ca23c52a73a8ea5c8d83ddd2e2d3aa" alt="The visual replication flow with progress indicators." data-og-width="2670" width="2670" data-og-height="2948" height="2948" data-path="docs/images/assets/docs/imports/import-workflows/copying-phase.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows/copying-phase.png?w=280&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=636d700ae4a68753187e4d0fe24f1913 280w, https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows/copying-phase.png?w=560&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=930a3bde595bcec0b8158698ba3ce835 560w, https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows/copying-phase.png?w=840&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=195d4ed670d3b6edd2b882c3ca0c4117 840w, https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows/copying-phase.png?w=1100&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=8eb2a46f4405344406a0746fbb4d397d 1100w, https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows/copying-phase.png?w=1650&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=6c9369b506093a81dd5da04c1b46eb69 1650w, https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows/copying-phase.png?w=2500&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=d3a48b52294441091ef509c617065a54 2500w" />
</Frame>

### Workflow phases

Your import will go through these states:

1. **Pending** - Workflow created, not started yet
2. **Copying** - Copying initial data (you'll see per-table progress here)
3. **Running** - Replicating changes to keep databases in sync
4. **Verifying data** - Optional data verification
5. **Verified data** - Verification complete
6. **Switching replicas** - Moving replica traffic to PlanetScale
7. **Switched replicas** - Replica traffic now on PlanetScale
8. **Switching primaries** - Moving primary traffic to PlanetScale
9. **Switched primaries** - Primary traffic now on PlanetScale
10. **Completed** - Import done
11. **Error** - Something went wrong, check error messages or logs

**You can now connect your application to PlanetScale**

Once the workflow enters the **Running** (replication) phase, bidirectional replication is active. This means you can safely connect your application to PlanetScale for testing while your external database remains the authoritative source. Any writes to either database will be replicated to the other, allowing you to validate your application's behavior against PlanetScale without risk.

This is the ideal time to test your application end-to-end before switching traffic.

### Adding a replica host name (optional)

If your external database has read replicas, you can route read traffic to them instead of your primary database. This helps reduce load on your primary during the import.

**How this works:**

If your application is configured to send read traffic to replicas, you can continue this pattern while testing PlanetScale. Adding a replica hostname allows PlanetScale to proxy traffic to your external replicas during the import. This is useful when you want to test PlanetScale with read traffic going to your replicas while writes continue to your primary.

<Note>
  **Important**

  PlanetScale doesn't automatically detect or route read-only transactions. You control which queries go to replicas through your application's database connection configuration. PlanetScale simply acts as a proxy, forwarding the traffic you send to replica connections through to your external replica databases.
</Note>

On the workflow monitoring page, under the connection status:

<Steps>
  <Step>
    Click "**Add a replica host name**" below your primary connection info
  </Step>

  <Step>
    Enter the hostname of your read replica (e.g., `replica.myserver.example.com`)
  </Step>

  <Step>
    Click "**Add**" to save
  </Step>
</Steps>

Once added, you'll see the replica connection listed below your primary. You can edit or delete it anytime during the import.

**Why use a replica:**

* Reduces load on your primary database during the copy phase
* Especially useful for large imports or high-traffic databases
* The replica must have the same data as your primary (replication lag should be minimal)

<Frame>
  <img src="https://mintlify.s3.us-west-1.amazonaws.com/planetscale-cad1a68a/docs/images/assets/docs/imports/import-workflows/add-replica-hostname.png" alt="Connection status add replica hostname." />
</Frame>

### Verify data (optional)

Once the initial copy completes and replication catches up, you can optionally verify that your data matches between the external database and PlanetScale.

Click "**Verify data**" on the workflow monitoring page to run a comparison. This checks that the copied data is identical between your external database and PlanetScale, giving you confidence before switching traffic.

### Switching traffic

Once you've verified your data, you can control how traffic is routed between your external database and PlanetScale:

1. **Switch replica traffic** - Serve read queries from PlanetScale while writes still go to your external database. This is an optional intermediate step that lets you test read traffic separately.
2. **Switch primary traffic** - Serve both reads and writes from PlanetScale. This switches all traffic at once, so you don't need to switch replica traffic first.
3. **Complete** - Finalize the migration

<Note>
  **Note**

  You can skip directly to switching primary traffic if you prefer. Switching primary traffic handles both reads and writes simultaneously, so switching replica traffic first is optional.
</Note>

<Warning>
  **Critical: Update connection strings before switching primary traffic**

  You must update your application's connection string to point to PlanetScale **before** switching primary traffic. If you switch primary traffic while your application is still connected to your external database, you will create a split-brain scenario where:

  * PlanetScale believes it is serving all traffic (reads and writes)
  * Your application continues writing to the external database
  * The two databases diverge, causing data inconsistency and potential data loss

  Always verify your application is connected to PlanetScale before proceeding with the primary traffic switch.
</Warning>

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows/switch-traffic-dropdown.png?fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=4acd8f91a560c2534e6ba9045e321fa9" alt="Switch replica and primary traffice." data-og-width="1978" width="1978" data-og-height="902" height="902" data-path="docs/images/assets/docs/imports/import-workflows/switch-traffic-dropdown.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows/switch-traffic-dropdown.png?w=280&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=0983f6e91c7d8f768df6c89e579843ba 280w, https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows/switch-traffic-dropdown.png?w=560&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=72dbead6c02d184f997e46b214afa7bb 560w, https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows/switch-traffic-dropdown.png?w=840&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=0de24d207ff3c64a861eeb5d0c5756f8 840w, https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows/switch-traffic-dropdown.png?w=1100&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=aab5a541cb9d973b7e83dea353661e0c 1100w, https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows/switch-traffic-dropdown.png?w=1650&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=c873dbbd90f99c3c22aa3c5312513f0a 1650w, https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows/switch-traffic-dropdown.png?w=2500&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=a739def00db91b1205f2de37e0574584 2500w" />
</Frame>

### Monitoring replication lag

The lag graph shows how far behind PlanetScale is from your external database. During the initial copy, lag will be high. Once the copy finishes and replication catches up, lag should drop.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows/replication-phase.png?fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=ad50823ab164d211c33fa655e6b1c47d" alt="The replication lag graph." data-og-width="1978" width="1978" data-og-height="854" height="854" data-path="docs/images/assets/docs/imports/import-workflows/replication-phase.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows/replication-phase.png?w=280&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=fae2bbe6bf73345becfe87d4a998bc1b 280w, https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows/replication-phase.png?w=560&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=233ef6fe6ff09d0baa9678d778a63cc2 560w, https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows/replication-phase.png?w=840&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=81b3f11a2aed564114cca76bf5f90201 840w, https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows/replication-phase.png?w=1100&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=8ad33cdbf01dad527dbf50089fc3a25e 1100w, https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows/replication-phase.png?w=1650&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=55b27bf3642496b3997881b0687f8a78 1650w, https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows/replication-phase.png?w=2500&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=92725637599901b0d0eb66cd1b7923c7 2500w" />
</Frame>

## Step 8: Complete the import

Once you've switched all traffic to PlanetScale and verified everything is working:

<Steps>
  <Step>
    Monitor your application for any issues. Check:

    * Application logs for errors
    * Replication lag (should be near zero)
    * Error tracking tools
    * Application performance metrics
  </Step>

  <Step>
    When you're confident everything is working correctly, click "**Complete workflow**" on the workflow monitoring page.
  </Step>

  <Step>
    PlanetScale will detach your external database and the connection will be closed.
  </Step>
</Steps>

**What happens when you complete:**

* Replication from PlanetScale back to your external database stops
* The connection to your external database is closed
* All external database credentials are removed from PlanetScale
* The workflow is marked as complete

<Warning>
  **Important**

  Completing the workflow is not reversible. Make sure your application is running smoothly on PlanetScale before completing the import.
</Warning>

## Next steps

You just migrated your database to PlanetScale. Here are some things you can do next:

* [Create a development branch](/docs/vitess/schema-changes/branching) - Use branching in your development workflow.
* [Create a deploy request](/docs/vitess/schema-changes/branching#1-create-a-deploy-request) - Test schema changes in dev branches before pushing to production.

## Provider-specific migration guides

For detailed instructions on preparing your external database for import, see our provider-specific guides:

* [Amazon Aurora](/docs/vitess/imports/amazon-aurora-migration-guide)
* [AWS RDS for MySQL](/docs/vitess/imports/aws-rds-migration-guide)
* [Azure Database for MySQL](/docs/vitess/imports/azure-database-for-mysql-migration-guide)
* [DigitalOcean MySQL](/docs/vitess/imports/digitalocean-database-migration-guide)
* [Google Cloud SQL](/docs/vitess/imports/gcp-cloudsql-migration-guide)
* [MariaDB](/docs/vitess/imports/mariadb-migration-guide)

## Troubleshooting

For detailed troubleshooting guidance, see our [Import Troubleshooting guide](/docs/vitess/imports/import-troubleshooting).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# DigitalOcean database migration guide
Source: https://planetscale.com/docs/vitess/imports/digitalocean-database-migration-guide



## Introduction

In this article, we’ll walk through migrating a MySQL database from DigitalOcean to PlanetScale using the [Import tool](/docs/vitess/imports/database-imports).

<Note>
  This guide assumes you are using MySQL on DigitalOcean. Other database systems available through DigitalOcean will not work with the PlanetScale import tool.
</Note>

We recommend reading through the [Database import documentation](/docs/vitess/imports/database-imports) to learn how our import tool works before proceeding.

## Prerequisites

Before you can start migrating your database, you’ll also need to collect the following information from your DigitalOcean cluster:

* The admin username and password
* The database host
* The port
* Database name

Most of this information is located on the landing page of your cluster.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-overview-of-the-database-cluster.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=c9e7e15e82cf7202d443b36e1c6f70e4" alt="The Overview of the database cluster." data-og-width="1342" width="1342" data-og-height="1059" height="1059" data-path="docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-overview-of-the-database-cluster.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-overview-of-the-database-cluster.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=7df4fb8892c73001128aa6d6d56f52b2 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-overview-of-the-database-cluster.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=64437172759b90063a9bb7e9e6b0a680 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-overview-of-the-database-cluster.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=48643c53388ef8c235bdd63e37050ff0 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-overview-of-the-database-cluster.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=bd02cb6cdf5b1da6161c374c3c63f1c9 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-overview-of-the-database-cluster.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=f2611e7eb2547be58e478cd2409e6fa7 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-overview-of-the-database-cluster.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=8c57f3afe13a6f2b074437af1126c306 2500w" />
</Frame>

You can view the list of databases in the "**Users & Databases**" tab. This article will use the default database created when the cluster was initialized, named `defaultdb`.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-users-and-databases-view-of-the-cluster-with-the-databases-section-highlighted.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=d1988d9296dd60f89781ea4be7491f64" alt="The Users & Databases view of the cluster with the Databases section highlighted." data-og-width="1255" width="1255" data-og-height="939" height="939" data-path="docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-users-and-databases-view-of-the-cluster-with-the-databases-section-highlighted.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-users-and-databases-view-of-the-cluster-with-the-databases-section-highlighted.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=96f9165d512fb484e881405266d68ce3 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-users-and-databases-view-of-the-cluster-with-the-databases-section-highlighted.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=a9a3fd91ff59df0607196c747ac30eea 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-users-and-databases-view-of-the-cluster-with-the-databases-section-highlighted.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=8844885233da239fc9851726f84edb7f 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-users-and-databases-view-of-the-cluster-with-the-databases-section-highlighted.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=861ed4aaaed4bf79e66696e3893269b0 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-users-and-databases-view-of-the-cluster-with-the-databases-section-highlighted.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=353028bec14f5ef8bbc9446460cb65c8 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-users-and-databases-view-of-the-cluster-with-the-databases-section-highlighted.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=fd42ee81d8cecfbbb727200b7d9fd740 2500w" />
</Frame>

<Note>
  If you don’t know the admin password, you can create a new set of credentials using the information on the [Import
  tool user permissions page](/docs/vitess/imports/import-tool-user-requirements) to create an account that can be used to
  import your database.
</Note>

## Update trusted sources

In order for PlanetScale to connect to your DigitalOcean database, you must allow network traffic into the database by adding the necessary IP addresses to the trusted sources list in DigitalOcean. The specific IP addresses you will need to allow depend on the region you plan to host your PlanetScale database. Check the [Import tool public IP addresses page](/docs/vitess/imports/import-tool-migration-addresses) to determine the IP addresses to allow before continuing. This guide will use the **AWS us-east-1 (North Virginia)** region so we’ll allow the following addresses:

```
3.209.149.66
3.215.97.46
34.193.111.15
23.23.187.137
52.6.141.108
52.70.2.89
50.17.188.76
52.2.251.189
52.72.234.74
35.174.68.24
52.5.253.172
54.156.81.4
34.200.24.255
35.174.79.154
44.199.177.24
```

In the DigitalOcean dashboard, navigate to the “**Settings”** tab of your database and locate **Trusted sources** in the list of configuration items. Click “**Edit”** and the row should change to allow edits to the setting.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-settings-tab-of-the-database-cluster-in-digital-ocean.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=876280400c684db84ca3a19175791793" alt="The Settings tab of the database cluster in Digital Ocean." data-og-width="1238" width="1238" data-og-height="1138" height="1138" data-path="docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-settings-tab-of-the-database-cluster-in-digital-ocean.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-settings-tab-of-the-database-cluster-in-digital-ocean.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=cbb5f3e7ca96add99d099eca7818d56b 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-settings-tab-of-the-database-cluster-in-digital-ocean.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=f8a813ec71c54eb8a87c0d4d65e40686 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-settings-tab-of-the-database-cluster-in-digital-ocean.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=20d823bbae7a4ec5d1a350b7e8fb7e4b 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-settings-tab-of-the-database-cluster-in-digital-ocean.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=d0cf323fa24748dab67f242924a9ac44 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-settings-tab-of-the-database-cluster-in-digital-ocean.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=eda0d8da872cee5617daa5d0c987402a 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-settings-tab-of-the-database-cluster-in-digital-ocean.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=43715ce6485db29b4fd93034c6914dd5 2500w" />
</Frame>

When you enter an IP address from the list, a message will appear below the input box asking if you want to add that IP as an address. Click that message to add it to the list. Repeat this step for each IP address that needs to be added, then click “**Save**” once you are done.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-trusted-sources-section-of-the-settings-tab.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=1c6f421babeb8f50dd29d236b4b6ae91" alt="The Trusted sources section of the Settings tab." data-og-width="959" width="959" data-og-height="217" height="217" data-path="docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-trusted-sources-section-of-the-settings-tab.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-trusted-sources-section-of-the-settings-tab.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=7db4cb24bb79c0e7ec89dc0085037776 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-trusted-sources-section-of-the-settings-tab.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=b7ee64abcb3f544e5d44874979e6312f 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-trusted-sources-section-of-the-settings-tab.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=e28835e70e272e956116885b683d1cd9 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-trusted-sources-section-of-the-settings-tab.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=20e6f58b843b4ff96fa7cfc7840d56c8 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-trusted-sources-section-of-the-settings-tab.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=9106142f106dff2fad18e8388cc2811f 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-trusted-sources-section-of-the-settings-tab.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=712a76156c9a446ec24e06e134637e92 2500w" />
</Frame>

### Disable ANSI\_QUOTES setting

The default settings for MySQL databases on DigitalOcean is to have `ANSI_QUOTES` enabled in the global MySQL settings, which is not supported by PlanetScale. To remove this setting, navigate to the "**Settings**" tab of your cluster and locate the section titled **Global SQL mode**. Click “**Edit**” in that section to change the configuration settings.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-settings-tab-of-the-database-cluster-in-digitalocean-with-the-global-sql-mode-section-highlighted.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=46a07a4b638e150e5e915f9df9190c55" alt="The Settings tab of the database cluster in DigitalOcean with the Global SQL mode section highlighted." data-og-width="1284" width="1284" data-og-height="1123" height="1123" data-path="docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-settings-tab-of-the-database-cluster-in-digitalocean-with-the-global-sql-mode-section-highlighted.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-settings-tab-of-the-database-cluster-in-digitalocean-with-the-global-sql-mode-section-highlighted.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=e0fd887db72f0c8449732a5d51736dc3 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-settings-tab-of-the-database-cluster-in-digitalocean-with-the-global-sql-mode-section-highlighted.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=9e8ab0aae3f9191178fd93224aaa9ee5 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-settings-tab-of-the-database-cluster-in-digitalocean-with-the-global-sql-mode-section-highlighted.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=7d87626df046bc513a366854be38ac82 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-settings-tab-of-the-database-cluster-in-digitalocean-with-the-global-sql-mode-section-highlighted.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=29c9fdb902871c72c86789cbe0cccc0a 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-settings-tab-of-the-database-cluster-in-digitalocean-with-the-global-sql-mode-section-highlighted.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=c67678e09413f45e306b886d3eda1fc4 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/imports/digitalocean-database-migration-guide/the-settings-tab-of-the-database-cluster-in-digitalocean-with-the-global-sql-mode-section-highlighted.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=d629e6f430ad2bb51fef85f4f3b7a11b 2500w" />
</Frame>

To remove the `ANSI_QUOTES` setting, click the “**x**” next to the tag and click “**Save**.” The change should apply immediately.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/digitalocean-database-migration-guide/an-example-of-removing-the-ansi_quotes-setting-from-the-global-sql-mode-settings.png?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=46afdb19de1802b33a4d7d1c3b244aff" alt="An example of removing the ANSI_QUOTES setting from the Global SQL mode settings." data-og-width="1197" width="1197" data-og-height="517" height="517" data-path="docs/images/assets/docs/imports/digitalocean-database-migration-guide/an-example-of-removing-the-ansi_quotes-setting-from-the-global-sql-mode-settings.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/digitalocean-database-migration-guide/an-example-of-removing-the-ansi_quotes-setting-from-the-global-sql-mode-settings.png?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=50e6ea837ad6c5a2ea8fa48309ecb1b0 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/digitalocean-database-migration-guide/an-example-of-removing-the-ansi_quotes-setting-from-the-global-sql-mode-settings.png?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=ca1c83660b8782fe58e55dd71bcf0c1f 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/digitalocean-database-migration-guide/an-example-of-removing-the-ansi_quotes-setting-from-the-global-sql-mode-settings.png?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=f9cbc7dac22a3e23756bdc3992fba68e 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/digitalocean-database-migration-guide/an-example-of-removing-the-ansi_quotes-setting-from-the-global-sql-mode-settings.png?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=d61300f8358d8186bd3eae13e12445ce 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/digitalocean-database-migration-guide/an-example-of-removing-the-ansi_quotes-setting-from-the-global-sql-mode-settings.png?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=3f8819115ee251c66df4181340fe3b2c 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/imports/digitalocean-database-migration-guide/an-example-of-removing-the-ansi_quotes-setting-from-the-global-sql-mode-settings.png?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=71c7c554c673ca661ce0998a857994d2 2500w" />
</Frame>

### Set the Binlog Retention Period

The binary log related [MySQL server variables](/docs/vitess/imports/database-imports#server-configuration-issues) required for PlanetScale's importer are already set to acceptable values by default on Digital Ocean managed MySQL servers but there's one more variable to check on the "**Settings**" tab of your existing cluster.

Scroll down to "**Advanced configurations**" and ensure the "**Binlog Retention Period**" is set to the maximum value of `86400` seconds.

<img src="https://mintcdn.com/planetscale-cad1a68a/fA2eqC-FBQpMQJbO/docs/vitess/imports/digitalocean-binlog-retention.png?fit=max&auto=format&n=fA2eqC-FBQpMQJbO&q=85&s=c61467df267e15c65b1893f4f153d662" alt="Setting the binlog retention period under Advanced Configurations" data-og-width="1998" width="1998" data-og-height="582" height="582" data-path="docs/vitess/imports/digitalocean-binlog-retention.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/fA2eqC-FBQpMQJbO/docs/vitess/imports/digitalocean-binlog-retention.png?w=280&fit=max&auto=format&n=fA2eqC-FBQpMQJbO&q=85&s=4579a5703be560eb0ba103b98ef18eea 280w, https://mintcdn.com/planetscale-cad1a68a/fA2eqC-FBQpMQJbO/docs/vitess/imports/digitalocean-binlog-retention.png?w=560&fit=max&auto=format&n=fA2eqC-FBQpMQJbO&q=85&s=f2dba67341d67840b29ba44481936346 560w, https://mintcdn.com/planetscale-cad1a68a/fA2eqC-FBQpMQJbO/docs/vitess/imports/digitalocean-binlog-retention.png?w=840&fit=max&auto=format&n=fA2eqC-FBQpMQJbO&q=85&s=75bac6e6b6d11501a37ae009823d0e64 840w, https://mintcdn.com/planetscale-cad1a68a/fA2eqC-FBQpMQJbO/docs/vitess/imports/digitalocean-binlog-retention.png?w=1100&fit=max&auto=format&n=fA2eqC-FBQpMQJbO&q=85&s=a6e0fab28f5bb6c5aab6562b3d2595cc 1100w, https://mintcdn.com/planetscale-cad1a68a/fA2eqC-FBQpMQJbO/docs/vitess/imports/digitalocean-binlog-retention.png?w=1650&fit=max&auto=format&n=fA2eqC-FBQpMQJbO&q=85&s=4d18aec047292186c60cbf629599b10d 1650w, https://mintcdn.com/planetscale-cad1a68a/fA2eqC-FBQpMQJbO/docs/vitess/imports/digitalocean-binlog-retention.png?w=2500&fit=max&auto=format&n=fA2eqC-FBQpMQJbO&q=85&s=4fd0886af7792b3e30e01fa65c74f201 2500w" />

## Importing your database

Now that your DigitalOcean database is configured and ready, follow the [Database Imports guide](/docs/vitess/imports/database-imports) to complete your import.

When filling out the connection form in the import workflow, use the following information:

* **Host name** - Your DigitalOcean database host (from Prerequisites)
* **Port** - Your port (typically 25060 for DigitalOcean)
* **Database name** - The exact database name to import (e.g., `defaultdb`)
* **Username** - Your admin username
* **Password** - Your admin password
* **SSL verification mode** - Select based on your DigitalOcean SSL configuration

The Database Imports guide will walk you through:

* Creating your PlanetScale database
* Connecting to your DigitalOcean database
* Validating your configuration
* Selecting tables to import
* Monitoring the import progress
* Switching traffic and completing the import

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# GCP Cloud SQL Migration Guide
Source: https://planetscale.com/docs/vitess/imports/gcp-cloudsql-migration-guide



## Overview

This document will demonstrate how to migrate a database from Google Cloud Platform (GCP) Cloud SQL MySQL Cluster to PlanetScale using our [Import tool](/docs/vitess/imports/database-imports).

<Note>
  This guide assumes you are using MySQL on GCP. Other database systems available through GCP will not work with the
  PlanetScale import tool.
</Note>

We recommend reading through the [Database import documentation](/docs/vitess/imports/database-imports) to learn how our import tool works before proceeding.

## Prerequisites

Before you can perform a migration, gather the following information from the GCP Console:

* **Public IP address** - Found in the **Overview** tab of your Cloud SQL cluster under the **Connect to this instance** section
* **Database name** - The name of the database you want to import
* **Root username and password** - You'll need these to create the migration user

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-ip-address.png?fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=d998a6198377ed0d0bba597d5b0bedde" alt="The GCP Cloud SQL console with the IP address highlighted." data-og-width="1754" width="1754" data-og-height="1330" height="1330" data-path="docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-ip-address.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-ip-address.png?w=280&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=d7bbcd497e902ce4e5d6c4e2f06a71ae 280w, https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-ip-address.png?w=560&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=8e345b8b6712bbf8f767a46f2b7951e2 560w, https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-ip-address.png?w=840&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=ef043d2d0307cd13547991b3793baf8c 840w, https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-ip-address.png?w=1100&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=406e6ace4cc2422eb09a46ec7e478180 1100w, https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-ip-address.png?w=1650&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=9a75e188035a8e579cd0758d58581e8b 1650w, https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-ip-address.png?w=2500&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=346f7c911abf90732276c1ad5db7e1f9 2500w" />
</Frame>

A list of your databases can be found in the **Databases** tab. In this guide, we'll be using the `prod` database.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-databases.png?fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=d32ce30657df2225b95e0326424dd58d" alt="The Databases list in the GCP console." data-og-width="1982" width="1982" data-og-height="1038" height="1038" data-path="docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-databases.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-databases.png?w=280&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=376a9be77b70ec19742fa4b12cdf7129 280w, https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-databases.png?w=560&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=7bb062a6938817e3abc4d23fbb45aed7 560w, https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-databases.png?w=840&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=ebba9a3ea16cc1c570c55b2a2bb65c38 840w, https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-databases.png?w=1100&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=99efbf396af676b815daea92d9b98e48 1100w, https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-databases.png?w=1650&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=c1a98c2aaac4e1bb63d754690f5f8826 1650w, https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-databases.png?w=2500&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=06c11421fd2d5122f310cde0335dfa6d 2500w" />
</Frame>

## Create a migration user

Create a migration user account with limited privileges for the import process. **You must run this as the root user or another user with admin privileges.** Connect to your Cloud SQL instance as root using the MySQL command line:

```bash  theme={null}
mysql -u root -p -h [your-cloud-sql-ip]
```

Then run the following script, making sure to update the placeholders:

* `<SUPER_STRONG_PASSWORD>` - The password for the `migration_user` account
* `<DATABASE_NAME>` - The name of the database you will import into PlanetScale

```sql  theme={null}
CREATE USER 'migration_user'@'%' IDENTIFIED BY '<SUPER_STRONG_PASSWORD>';
GRANT PROCESS, REPLICATION SLAVE, REPLICATION CLIENT, RELOAD ON *.* TO 'migration_user'@'%';
GRANT SELECT, INSERT, UPDATE, DELETE, SHOW VIEW, LOCK TABLES ON `<DATABASE_NAME>`.* TO 'migration_user'@'%';
GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, ALTER ON *.* TO 'migration_user'@'%';
GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, ALTER ON `_vt`.* TO 'migration_user'@'%';
GRANT SELECT ON mysql.db TO 'migration_user'@'%';
GRANT SELECT ON mysql.func TO 'migration_user'@'%';
GRANT SELECT ON mysql.tables_priv TO 'migration_user'@'%';
GRANT SELECT ON mysql.user TO 'migration_user'@'%';
GRANT SELECT ON performance_schema.* TO 'migration_user'@'%';
FLUSH PRIVILEGES;
```

Verify the grants were applied correctly:

```sql  theme={null}
SHOW GRANTS FOR 'migration_user'@'%';
```

You should see all the GRANT statements listed. If you only see one or two lines, the grants didn't apply correctly.

<Note>
  **Important**

  You must create the migration user on the MySQL command line and not in the GCP console. Creating users through the GCP console automatically grants the `cloudsqlsuperuser` role, which will cause the import to fail.
</Note>

## Allow PlanetScale to connect to your Cloud SQL instance

For PlanetScale to connect to your database, you'll need to update the Authorized networks for your cluster. The specific IP addresses to permit are shown during the import workflow on the **Connect to external database** step. The list includes IP addresses specific to your PlanetScale database region.

See the [Import public IP addresses](/docs/vitess/imports/import-tool-migration-addresses) page for more details on where to find these IP addresses in the workflow. To permit traffic from these IP addresses to your database in GCP, select **Connections** from the navigation on the left. Under **Authorized networks**, click “**Add network**”. This will display an inline form for you to add a network. The name of the field is arbitrary, but the **Network** field should contain the IP address that needs access to your database. Click “**Done**” to add the new entry. Perform this step for each IP address for the selected region, then click “**Save**” to apply the settings.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-networking.png?fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=306bf5676a73e89c2da04bb62a6f053a" alt="The form to add a new authorized network in the GCP console." data-og-width="1448" width="1448" data-og-height="1556" height="1556" data-path="docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-networking.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-networking.png?w=280&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=93b7f00da2fe783e178ad120d511369e 280w, https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-networking.png?w=560&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=3dfb907738a537b395852911523bf521 560w, https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-networking.png?w=840&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=925d61023753732ed109a189c112910d 840w, https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-networking.png?w=1100&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=177d735a386200dd0ea9128e9fd51ace 1100w, https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-networking.png?w=1650&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=1e5f688725d34724d7d643dc0bf58db6 1650w, https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-networking.png?w=2500&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=12767ab87731d206cc1faa16f15a8e8e 2500w" />
</Frame>

## Configure MySQL server settings

Certain MySQL server settings may need to be changed before you can begin the import. The initial connection test will fail if these settings are not configured correctly.

* binlog\_expire\_logs\_seconds

To set a flag in your GCP console, go to your database's “**Overview**” page, select the “**Edit**” button, and then scroll down to the “**Flags**” section.

You want to select the “**binlog\_expire\_logs\_seconds**” flag and set it to `172800` seconds.

Make sure to select the “**Done**” button.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-set-flags.png?fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=3a395537ace05e3139646ece64609af9" alt="The form to set MySQL flags." data-og-width="1490" width="1490" data-og-height="1560" height="1560" data-path="docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-set-flags.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-set-flags.png?w=280&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=885eb6f3f1b27646f0b3e6965146b9c9 280w, https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-set-flags.png?w=560&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=0b234630cc0316a99a07e7312ed8f9e5 560w, https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-set-flags.png?w=840&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=c7ea320059bd97e129dbfa38bbaf3a6a 840w, https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-set-flags.png?w=1100&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=d1e56bfb9691564e8b485624eb25cf72 1100w, https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-set-flags.png?w=1650&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=82f4102c710f8262f663eab4492bba87 1650w, https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-set-flags.png?w=2500&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=c0660ba0016f3b040868861c3d17d516 2500w" />
</Frame>

* log\_bin

If `log_bin` is set to OFF you may need to [enable Point in Time Recovery (PITR)](https://cloud.google.com/sql/docs/mysql/backup-recovery/pitr#enablingpitr) from the GCP console to start binary logging.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-enable-pitr.png?fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=9b8b6493d1058801ce03636d3c05ec9e" alt="The form to set enable point in time recovery." data-og-width="2118" width="2118" data-og-height="1644" height="1644" data-path="docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-enable-pitr.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-enable-pitr.png?w=280&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=8fe84b1b1f5c65b5ee7df83bdab3dc74 280w, https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-enable-pitr.png?w=560&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=1142df9a5e2992ed95c68fa74ed24992 560w, https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-enable-pitr.png?w=840&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=a9e64e02310fac0fdfa55f4b2f1d81c0 840w, https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-enable-pitr.png?w=1100&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=a936663d41d864e5f65e8ae7dc93302b 1100w, https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-enable-pitr.png?w=1650&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=091aae0d6079e5241f99a1279184617a 1650w, https://mintcdn.com/planetscale-cad1a68a/AspOsmxsYasYOxA4/docs/images/assets/docs/imports/gcp-cloudsql-migration-guide/cloudsql-enable-pitr.png?w=2500&fit=max&auto=format&n=AspOsmxsYasYOxA4&q=85&s=3d8be00078be6e4906b54d8767b7c488 2500w" />
</Frame>

## Importing your database

Now that your GCP Cloud SQL database is configured and ready, follow the [Database Imports guide](/docs/vitess/imports/database-imports) to complete your import.

When filling out the connection form in the import workflow, use the following information:

* **Host name** - Your GCP Cloud SQL public IP address (from Prerequisites)
* **Port** - 3306 (default for Cloud SQL)
* **Database name** - The exact database name to import
* **Username** - `migration_user`
* **Password** - The password you set for the migration user
* **SSL verification mode** - Select based on your Cloud SQL SSL configuration

The Database Imports guide will walk you through:

* Creating your PlanetScale database
* Connecting to your Cloud SQL database
* Validating your configuration
* Selecting tables to import
* Monitoring the import progress
* Switching traffic and completing the import

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Import public IP addresses
Source: https://planetscale.com/docs/vitess/imports/import-tool-migration-addresses

When importing a database using our [Import tool](/docs/vitess/imports/database-imports), you need to grant a set of IP addresses access to your external MySQL database so that PlanetScale can make the connection.

## Overview

To import your external database into PlanetScale, you need to allowlist PlanetScale's IP addresses in your database firewall or security group. This lets PlanetScale connect to your database during the import process.

## Where to find your IP addresses

The IP addresses you need to allowlist are shown during the import workflow on the **Connect to external database** step. You'll see a blue info box on the connection page that lists all the IP addresses that need access to your external database.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows-ip-addresses/import-ips.png?fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=51adf62afe74c5afa2d007872c634a6c" alt="IP addresses displayed on the connection step of import workflow" data-og-width="1568" width="1568" data-og-height="1108" height="1108" data-path="docs/images/assets/docs/imports/import-workflows-ip-addresses/import-ips.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows-ip-addresses/import-ips.png?w=280&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=c84c4251bbf4dbf0cb359a37ef24becf 280w, https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows-ip-addresses/import-ips.png?w=560&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=d1a5bdf99d8f7866ab46a1fc1b0affe3 560w, https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows-ip-addresses/import-ips.png?w=840&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=a3268e6deb7c6b562227d1f838321a12 840w, https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows-ip-addresses/import-ips.png?w=1100&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=13295745ac6d799a06abdb173ceca0f9 1100w, https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows-ip-addresses/import-ips.png?w=1650&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=b346ee0c98c7ebbc1f00ec9b7c65202e 1650w, https://mintcdn.com/planetscale-cad1a68a/CkuVTAfsTSnAmv2e/docs/images/assets/docs/imports/import-workflows-ip-addresses/import-ips.png?w=2500&fit=max&auto=format&n=CkuVTAfsTSnAmv2e&q=85&s=a6b91f5f2ac8993e930c1dc7d9477657 2500w" />
</Frame>

## Provider-specific firewall guides

If PlanetScale detects that you're connecting to a known database provider (like Amazon RDS, Aurora, Azure, GCP Cloud SQL, or DigitalOcean), you'll also see a direct link to that provider's firewall configuration documentation.

## Important notes

* **Region-specific IPs** - The IP addresses differ by region. Make sure you're using the IPs shown for your selected region.
* **IPs may change** - IP addresses can change occasionally. Always use the IPs shown in the import workflow.
* **All IPs required** - You need to allowlist all the IP addresses shown, not just one.

<Note>
  **Note**

  This guide is meant to be used alongside the [Database Imports guide](/docs/vitess/imports/database-imports) or one of the provider-specific migration guides.
</Note>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Import user permissions
Source: https://planetscale.com/docs/vitess/imports/import-tool-user-requirements

When importing a database using our [Import tool](/docs/vitess/imports/database-imports), you will need to connect to your existing database with a user that has the proper permissions to set up the necessary configurations to start importing data.

Below is the minimum set of permissions needed and what each allows the user to do:

| Scope  | Databases                                               | Grant                | Description                                                                  |
| :----- | :------------------------------------------------------ | :------------------- | :--------------------------------------------------------------------------- |
| Global | n/a                                                     | `PROCESS`            | Enable the user to see all processes with SHOW PROCESSLIST.                  |
| Global | n/a                                                     | `REPLICATION SLAVE`  | Enable replicas to read binary log events from the source.                   |
| Global | n/a                                                     | `REPLICATION CLIENT` | Enable the user to ask where source or replica servers are.                  |
| Global | n/a                                                     | `RELOAD`             | Enable use of FLUSH operations.                                              |
| Table  | `<DATABASE_NAME>`, `ps_import_*`, `_vt`, `mysql`.`func` | `SELECT`             | Enable use of SELECT.                                                        |
| Table  | `<DATABASE_NAME>`, `ps_import_*`, `_vt`                 | `INSERT`             | Enable use of INSERT.                                                        |
| Table  | `<DATABASE_NAME>`                                       | `LOCK TABLES`        | Enable use of LOCK TABLES on tables for which you have the SELECT privilege. |
| Table  | `<DATABASE_NAME>`                                       | `SHOW VIEW`          | Enable use of SHOW VIEW.                                                     |
| Table  | `<DATABASE_NAME>`, `ps_import_*`, `_vt`                 | `UPDATE`             | Enable use of UPDATE.                                                        |
| Table  | `<DATABASE_NAME>`, `ps_import_*`, `_vt`                 | `DELETE`             | Enable use of DELETE.                                                        |
| Table  | `ps_import_*`, `_vt`                                    | `CREATE`             | Enable database and table creation.                                          |
| Table  | `ps_import_*`, `_vt`                                    | `DROP`               | Enable databases, tables, and views to be dropped.                           |
| Table  | `ps_import_*`, `_vt`                                    | `ALTER`              | Enable use of ALTER TABLE.                                                   |

<Note>
  The descriptions in the table above were taken from the MySQL docs. For a full list of all possible grants and their
  impact, please refer to the [GRANT Statement page](https://dev.mysql.com/doc/refman/8.0/en/grant.html) of the MySQL
  docs, and locate the section titled **Privileges Supported by MySQL**.
</Note>

## Script to create user

This MySQL script can be used to create a user with the necessary permissions inside of your database. You will need appropriate database permissions to run the script. The username will be `migration_user`. Make sure to update the following variables:

* `<SUPER_STRONG_PASSWORD>` — The password for the `migration_user` account.
* `<DATABASE_NAME>` — The name of the database you will import into PlanetScale.

```sql  theme={null}
CREATE USER 'migration_user'@'%' IDENTIFIED BY '<SUPER_STRONG_PASSWORD>';
GRANT PROCESS, REPLICATION SLAVE, REPLICATION CLIENT, RELOAD ON *.* TO 'migration_user'@'%';
GRANT SELECT, INSERT, UPDATE, DELETE, SHOW VIEW, LOCK TABLES ON `<DATABASE_NAME>`.* TO 'migration_user'@'%';
GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, ALTER ON `ps\_import\_%`.* TO 'migration_user'@'%';
GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, ALTER ON `_vt`.* TO 'migration_user'@'%';
GRANT EXECUTE ON PROCEDURE mysql.rds_show_configuration TO 'migration_user'@'%';
GRANT SELECT ON mysql.db TO 'migration_user'@'%';
GRANT SELECT ON mysql.func TO 'migration_user'@'%';
GRANT SELECT ON mysql.tables_priv TO 'migration_user'@'%';
GRANT SELECT ON mysql.user TO 'migration_user'@'%';
```

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Import troubleshooting
Source: https://planetscale.com/docs/vitess/imports/import-troubleshooting



## Overview

This guide covers common issues you might run into when importing a database to PlanetScale and how to fix them.

## Connection issues

### Can't connect to external database

If the connection test fails, here's what to check:

**Verify your credentials locally**

Try connecting with the same credentials using the MySQL CLI:

```sql  theme={null}
mysql -u <USERNAME> -p <PASSWORD> -h <HOST> -P <PORT> -D <DATABASE>
```

If this works locally but not in PlanetScale, the issue is likely network-related.

**Check IP allowlist**

Make sure you've added all PlanetScale IP addresses to your database's firewall or security group. The specific IPs depend on which region you selected for your PlanetScale database. See our [Import tool public IP addresses](/docs/vitess/imports/import-tool-migration-addresses) page.

**Verify database is publicly accessible**

PlanetScale needs to reach your database over the internet. Check that:

* Your database has a public IP address
* Public access is enabled in your database settings
* No VPN or private network is required

**SSL/TLS issues**

If you're getting SSL-related errors:

* Try setting SSL verification mode to `Disabled` to test if SSL is the issue
* If your database uses self-signed certificates, provide the full CA certificate chain
* For managed databases (RDS, Azure, etc.), use `Required` or `Verify CA` mode

### Connection times out

If the connection attempt times out:

**Firewall rules**

Most timeouts are caused by firewall rules blocking PlanetScale IPs. Double-check:

* All IPs for your region are allowlisted
* The port (usually 3306) is open
* Any cloud provider security groups are configured correctly

**Database not running**

Verify your database server is running and accepting connections.

**Hostname/port incorrect**

Make sure you're using the correct hostname and port. For cloud providers:

* Use the cluster endpoint, not individual instance endpoints
* Default MySQL port is 3306, but some providers use different ports (DigitalOcean uses 25060)

## Server configuration errors

### GTID mode is OFF

**Error:** `external database settings are not compatible with PlanetScale: "gtid_mode" must be "ON", but found: "OFF"`

**Solution:**

You need to enable GTID mode in your database configuration.

For AWS RDS/Aurora:

1. Create a custom DB parameter group
2. Set `gtid-mode` to `ON`
3. Set `enforce_gtid_consistency` to `ON`
4. Apply the parameter group to your database
5. Reboot the database

For Azure:

1. Go to Server parameters
2. Set `gtid_mode` to `ON` (you may need to go through intermediate states: `OFF_PERMISSIVE` → `ON_PERMISSIVE` → `ON`)
3. Set `enforce_gtid_consistency` to `ON`
4. Save changes

For self-hosted MySQL/MariaDB:
Add to your `my.cnf` or `my.ini`:

```
gtid_mode = ON
enforce_gtid_consistency = ON
```

Then restart MySQL.

### Binary logging not enabled

**Error:** `external database settings are not compatible with PlanetScale: "log_bin" must be "ON", but found: "OFF"`

**Solution:**

For AWS RDS/Aurora:
Binary logging is tied to automated backups. Enable automated backups with a retention period >= 2 days.

For GCP Cloud SQL:
Enable Point in Time Recovery (PITR) from the console.

For self-hosted:
Add to your configuration:

```
log_bin = /var/log/mysql/mysql-bin.log
```

Restart MySQL.

### Wrong binlog format

**Error:** `"binlog_format" must be "ROW", but found: "MIXED"` or `"STATEMENT"`

**Solution:**

Set `binlog_format` to `ROW` in your database configuration, then restart.

For managed databases, update this in your parameter group or server parameters.

### Binlog retention too short

**Error:** `"binlog_expire_logs_seconds" must be > 172800` (or similar for `expire_logs_days`)

**Solution:**

You need at least 48 hours of binlog retention for the import to work.

For AWS RDS/Aurora:

```sql  theme={null}
CALL mysql.rds_set_configuration('binlog retention hours', 48);
```

Verify with:

```sql  theme={null}
CALL mysql.rds_show_configuration;
```

For other platforms:
Set in your database configuration:

```
binlog_expire_logs_seconds = 172800
```

Or:

```
expire_logs_days = 3
```

## Schema compatibility issues

### No unique key on table

**Error:** Table has no unique key

**Solution:**

All tables must have a unique, not-null key. This is required for replication to work correctly.

Add a primary key or unique index to the table:

```sql  theme={null}
ALTER TABLE your_table ADD PRIMARY KEY (id);
```

Or add a unique index:

```sql  theme={null}
ALTER TABLE your_table ADD UNIQUE KEY unique_index (column1, column2);
```

See our [Changing unique keys documentation](/docs/vitess/schema-changes/onlineddl-change-unique-keys) for more details.

### Invalid charset

**Error:** Table uses unsupported charset

**Solution:**

PlanetScale supports: `utf8`, `utf8mb4`, `utf8mb3`, `latin1`, and `ascii`.

Convert your table to a supported charset:

```sql  theme={null}
ALTER TABLE your_table CONVERT TO CHARACTER SET utf8mb4;
```

We recommend `utf8mb4` as it has the widest character support.

### Table names with special characters

**Error:** Table name contains unsupported characters

**Solution:**

Rename tables that have characters outside the standard ASCII set:

```sql  theme={null}
RENAME TABLE `special-table-name` TO `special_table_name`;
```

Ensure that any queries using the table name get updated as well.

### Views detected

Views aren't imported automatically. After your import completes, you'll need to manually recreate any views in your PlanetScale database.

### Unsupported storage engine

**Error:** Table uses non-InnoDB storage engine

**Solution:**

Convert your tables to InnoDB:

```sql  theme={null}
ALTER TABLE your_table ENGINE=InnoDB;
```

Changing storage engines has significant performance impact. [Contact us](https://planetscale.com/contact?initial=support)iIf you are using a different storage engine on your source database and cannot change it prior to migrating.

## Foreign key import issues

### Import slower than expected

Foreign key imports hold a long-running transaction, which can be slow on large databases.

**Solution:**

Connect to a read replica instead of your primary database. This reduces load and can improve performance.

### Import failed and won't resume

Unlike regular imports, foreign key imports must start from the beginning if they fail.

**Solution:**

Before retrying:

1. Fix any errors that caused the failure
2. Make sure your binlog retention is long enough for the full import
3. Consider importing during off-peak hours
4. Ensure your replica (if using one) is healthy and has minimal replication lag

### Can't select specific tables

When foreign keys are detected, all tables are automatically selected to maintain referential integrity. This is expected behavior.

If you really only need specific tables, you'll need to:

1. Remove foreign key constraints from your source database
2. Import only the tables you need
3. Recreate foreign key constraints in PlanetScale after import

Note: We recommend importing all tables to avoid referential integrity issues.

## Validation errors with skip option

### Validation failed but can skip

For certain validation failures, you'll see an option to skip and continue. This forces all tables to be imported.

**When to skip:**

* You understand the risks
* You'll fix the issues in PlanetScale after import
* The validation is a false positive

**When NOT to skip:**

* Server configuration issues (GTID, binlog) - these will cause the import to fail later
* You're not sure what the error means
* Production import (always fix issues first)

If you skip validation errors, you won't be able to select specific tables - everything gets imported.

## Import monitoring issues

### Replication lag is high

During the initial copy phase, high replication lag is normal. The lag should drop once the copy finishes.

**If lag stays high after copy completes:**

1. **Check source database load** - High write activity on source can cause lag
2. **Slow queries** - Look for slow queries or locks on the source database
3. **Network issues** - Check for network latency between source and PlanetScale
4. **Large transactions** - Very large transactions take time to replicate

**Solutions:**

* Reduce write load on source during import
* Wait for off-peak hours
* Check binlog retention isn't expiring before lag catches up

### Logs show errors

Check the logs section for specific error messages. Common ones:

**"Access denied"** - Permission issues. See [user requirements](/docs/vitess/imports/import-tool-user-requirements).

**"Table doesn't exist"** - Schema may have changed during import. Don't modify schema during import.

**"Deadlock found"** - Usually temporary. The import will retry.

**Connection lost** - Network issue or source database restarted. The import will retry.

### Import stuck in "Copying" phase

The copy phase can take a while for large databases. Check:

* Look at per-table progress indicators to see if it's actually stuck or just slow
* Check logs for any errors
* Verify source database is responding

If truly stuck:

1. Check source database for locks or slow queries
2. Verify network connectivity
3. Look for errors in logs

## Permission errors

### MySQL error 1045: Access denied

**Error:** `Access denied for user 'migration_user'@'%'`

**Solution:**

Check that your migration user has all required permissions. See our [import tool user requirements](/docs/vitess/imports/import-tool-user-requirements).

For foreign key imports, the user needs either:

* `FLUSH_TABLES` or `RELOAD` privileges (preferred)
* `LOCK TABLES` privilege (minimum)

Verify grants:

```sql  theme={null}
SHOW GRANTS FOR 'migration_user'@'%';
```

### Can't create *vt or ps\_import* databases

**Solution:**

Grant the migration user permissions on these databases:

```sql  theme={null}
GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, ALTER
  ON `ps\_import\_%`.* TO 'migration_user'@'%';
GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, ALTER
  ON `_vt`.* TO 'migration_user'@'%';
```

## Traffic switching issues

### Can't switch replica traffic

Make sure:

* Replication lag is low (under a few seconds)
* Import is in "Running" state
* No errors in logs

### Can't switch primary traffic

Make sure:

* Replica traffic has been switched first
* Your application is connected to PlanetScale
* Replication lag is minimal

### Data inconsistency after switching

If you notice missing or stale data after switching traffic:

1. Check replication lag - it may still be catching up
2. Verify your application is actually connecting to PlanetScale
3. Check for any errors in workflow logs

Don't complete the import until you've verified data consistency.

## Common provider-specific issues

### AWS RDS

**Problem:** Can't modify GTID settings on default parameter group

**Solution:** Create a custom DB parameter group with your MySQL version, modify settings there, then apply to your database.

**Problem:** Binary logs not enabled

**Solution:** Enable automated backups with retention >= 2 days.

### Azure

**Problem:** Can't set gtid\_mode directly to ON

**Solution:** Change through intermediate states: `OFF_PERMISSIVE` → `ON_PERMISSIVE` → `ON`

### DigitalOcean

**Problem:** ANSI\_QUOTES mode enabled

**Solution:** Remove ANSI\_QUOTES from Global SQL mode in Settings.

**Problem:** Binlog retention too short

**Solution:** Set Binlog Retention Period to 86400 seconds (24 hours minimum, max available).

### GCP Cloud SQL

**Problem:** Binary logging disabled

**Solution:** Enable Point in Time Recovery (PITR) from the GCP console.

## Still stuck?

If you've tried the solutions above and are still having issues:

1. Check your database's error logs
2. Review our [general MySQL compatibility guide](/docs/vitess/troubleshooting/mysql-compatibility)
3. Look at the specific provider guide for your database
4. Check workflow logs in PlanetScale for detailed error messages
5. [Contact PlanetScale support](https://planetscale.com/contact?initial=support)

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# MariaDB migration guide
Source: https://planetscale.com/docs/vitess/imports/mariadb-migration-guide



## Overview

In this article, you’ll learn how to migrate a database from MariaDB, a fork of MySQL, into PlanetScale.

<Warning>
  The steps outlined in this guide used MariaDB version 10.6.12 on an Ubuntu host. Depending on the version of MariaDB you are using, your results may vary. Don't hesitate to [reach out to us](https://planetscale.com/contact) for further assistance.
</Warning>

We recommend reading through the [Database import documentation](/docs/vitess/imports/database-imports) to learn how our import tool works before proceeding.

### Prerequisites

* A PlanetScale account
* A MariaDB server with traffic permitted from our [import tool IP addresses](/docs/vitess/imports/import-tool-migration-addresses)

## Configure MariaDB

Before you can start migrating data, there are a number of configuration options that need to be in place for our import tool to work properly:

* `binlog_format`
* `log_bin`
* `sql_mode`

You may run the following query to check these values:

```sql  theme={null}
SHOW variables WHERE Variable_name IN ('binlog_format','log_bin','sql_mode');

+---------------+-------------------------------------------------------------------------------------------+
| Variable_name | Value                                                                                     |
+---------------+-------------------------------------------------------------------------------------------+
| binlog_format | MIXED                                                                                     |
| log_bin       | OFF                                                                                       |
| sql_mode      | STRICT_TRANS_TABLES,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION |
+---------------+-------------------------------------------------------------------------------------------+
```

In the results listed above, none of the options are configured properly, so let’s set them up now. The exact path to your configuration file varies by operating system. This demo uses Ubuntu, so the MariaDB configuration file is located at `/etc/mysql/mariadb.conf.d/50-server.cnf`. Edit the configuration file and add the following values at the end of the file:

```
binlog_format = ROW
log_bin = /var/log/mysql/mysql-bin.log
sql_mode = 'NO_ZERO_IN_DATE,NO_ZERO_DATE,ONLY_FULL_GROUP_BY'
```

With the configuration updated, restart the MariaDB service. The exact command varies by the service manager you are using on your host, with this demo using `systemctl`:

```bash  theme={null}
sudo systemctl restart mariadb
```

## Configure a migration account

The PlanetScale import tool requires a user account with a specific set of permissions on the database you wish to migrate, as well as the server itself to set up the necessary database that tracks replication changes. To create a user named `migration_user`, run the following:

```sql  theme={null}
CREATE USER 'migration_user'@'%' IDENTIFIED BY '<SUPER_STRONG_PASS>';
```

Next, configure the proper grants to allow `migration_user` to set up replication:

```sql  theme={null}
GRANT PROCESS, REPLICATION SLAVE, REPLICATION CLIENT, RELOAD ON *.* TO 'migration_user'@'%';
```

Now you can configure the necessary permissions on the database you wish to migrate. Replace `<DATABASE_NAME>` with the name of your database in MariaDB:

```sql  theme={null}
GRANT SELECT, INSERT, UPDATE, DELETE, SHOW VIEW, LOCK TABLES ON `<DATABASE_NAME>`.* TO 'migration_user'@'%';
```

Finally, you’ll need to configure permissions for a database named `ps_import_<id>` (the last portion of the name will vary) that will be created by the import tool to track replication between MariaDB and PlanetScale.

```sql  theme={null}
GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, ALTER ON `ps\_import\_%`.* TO 'migration_user'@'%';
```

For a full explanation on what each of these grants do, [our article on configuring a migration account for MySQL databases](/docs/vitess/imports/import-tool-user-requirements) details each requirement.

## Importing your database

Now that your MariaDB database is configured and ready, follow the [Database Imports guide](/docs/vitess/imports/database-imports) to complete your import.

When filling out the connection form in the import workflow, use the following information:

* **Host name** - Your MariaDB server hostname or IP address
* **Port** - 3306 (default for MariaDB)
* **Database name** - The exact database name to import
* **Username** - `migration_user` (created in previous section)
* **Password** - The password you set for the migration user
* **SSL verification mode** - Select based on your MariaDB SSL configuration

The Database Imports guide will walk you through:

* Creating your PlanetScale database
* Connecting to your MariaDB database
* Validating your configuration
* Selecting tables to import
* Monitoring the import progress
* Switching traffic and completing the import

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Postgres to Vitess imports
Source: https://planetscale.com/docs/vitess/imports/postgres



export const YouTubeEmbed = ({id, title}) => {
  return <Frame>
      <iframe src={`https://www.youtube-nocookie.com/embed/${id}?rel=0`} title={title} className="aspect-video w-full" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" />
    </Frame>;
};

<Warning>
  **PlanetScale now supports Postgres**. This guide is for migrating from Postgres to PlanetScale's Vitess product. You can still use these scripts if you would like to utilize [Vitess](/docs/vitess). If you prefer to stay on Postgres, refer to the [Postgres import guides](/docs/postgres/imports/postgres-imports).
</Warning>

Many customers choose to migrate from Postgres to PlanetScale for Vitess in order to leverage the performance and scalability offered by PlanetScale and Vitess.

Use this guide if you are importing from platforms like Aurora Postgres, RDS Postgres, Neon, Supabase, and other Postgres instances.

<YouTubeEmbed id="UPMinBnujmo" title="Data drop: Migrate from Postgres → PlanetScale" />

## Postgres to MySQL

Vitess is built on MySQL. MySQL and Postgres are similar in that they are both relational databases, broadly function similarly, and are used for similar purposes by many organizations. When you look more closely, there are a number of small differences in types, indexing, and SQL language features. When added up, it makes for enough difference that migrating between the two is typically not straightforward.

We provide scripts to make moving your data from Postgres to PlanetScale for Vitess easier. However, moving the data is only a part of the challenge. It is up to you to ensure that your application servers, which connect to your database, are also set up to cut over to MySQL. If you are using an ORM, this could be as simple as changing the target database engine. In some cases, you will have to update your queries manually to follow MySQL syntax.

## Migration Guides

We have two recommendations for how to migrate your Postgres database to PlanetScale for Vitess. Both of these solutions leverage the AWS Database Migration Service to handle conversions between Postgres and MySQL types.

1. Use our [Postgres to PlanetScale for Vitess](/docs/vitess/imports/postgres-planetscale-migration-guide) guide along with our [postgres-planetscale scripts](https://github.com/planetscale/migration-scripts/tree/main/postgres-planetscale) to import directly from a Postgres source to a PlanetScale target. This technique is simpler than option 2, as it does not require an intermediate MySQL database, but operates more slowly at a rate of only a few gigabytes per hour.
2. Use our [Postgres to MySQL + PlanetScale for Vitess](/docs/vitess/imports/postgres-mysql-planetscale-migration-guide) guide along with our [postgres-mysql-planetscale scripts](https://github.com/planetscale/migration-scripts/tree/main/postgres-mysql-planetscale) to import from Postgres to an RDS MySQL instance, and then use PlanetScale's built-in [import tool](/docs/vitess/imports/database-imports) to bring the data in from MySQL. This technique is more complex since it requires an intermediary MySQL database. However, this technique can be 10x or more faster. This is recommended for larger databases.

In some cases, using these scripts and following the necessary steps from our docs are all it takes to get your data imported to PlanetScale.

However, there are many possible edge cases and specific Postgres configurations that may cause the scripts to fail. We encourage customers to make modifications to the scripts as needed to support their specific database environment.

If you encounter issues while importing from a Postgres database, please [reach out to support](https://planetscale.com/contact?initial=support) for assistance.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Postgres to MySQL to PlanetScale migration guide
Source: https://planetscale.com/docs/vitess/imports/postgres-mysql-planetscale-migration-guide



<Warning>
  **PlanetScale now supports Postgres**. This guide is for migrating from Postgres to PlanetScale's Vitess product. You can still use these scripts if you would like to utilize [Vitess](/docs/vitess). If you prefer to stay on Postgres, refer to the [Postgres import guides](/docs/postgres/imports/postgres-imports).
</Warning>

This guide covers how to import a Postgres database to PlanetScale for Vitess using an intermediate Aurora MySQL database.
For this, we will be using the [postgres-mysql-planetscale scripts](https://github.com/planetscale/migration-scripts/tree/main/docs/postgres-mysql-planetscale).
This technique is good for larger databases that need a fast import, but requires an intermediate MySQL instance.
If you have a smaller database or don't mind a slower import, consider [an alternate approach](/docs/vitess/imports/postgres-planetscale-migration-guide).
You are encouraged to modify these scripts to suit your needs if need be.

## Methodology

The scripts in this guide leverage [AWS Database Migration Service](https://aws.amazon.com/dms/) to handle conversions between Postgres and MySQL types.
It also creates a new Aurora MySQL database to use as a go-between for Postgres and PlanetScale.
Even if you are migrating from a non-AWS Postgres provider, such as Neon or Supabase, you will still need an AWS account to perform the migration.

When using this script, your data will take the following path:

* Data flows from your Postgres source into DMS
* DMS does necessary type conversions and copies the data into the Aurora MySQL database
* Using the PlanetScale import tool, your data will flow from Aurora MySQL into your destination PlanetScale database
* After the initial copy, changes will continue to flow form Postgres, to Aurora MySQL, to PlanetScale so that your data stays in sync, even if the migration takes several hours or days.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/postgres-mysql-planetscale-darkmode.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=71ccc0d17313bf965ab3310e1ebb9bfd" alt="Import data flow" data-og-width="3202" width="3202" data-og-height="812" height="812" data-path="docs/images/postgres-mysql-planetscale-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/postgres-mysql-planetscale-darkmode.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=5cf87871f42ed88da517e046924076fd 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/postgres-mysql-planetscale-darkmode.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=9edd29457a5c3f4b8b236e09f7b6cb5e 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/postgres-mysql-planetscale-darkmode.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=286980bf36c1e5d597c6509c30feca2d 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/postgres-mysql-planetscale-darkmode.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=cef7752d2771c5b74c295a292a82c909 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/postgres-mysql-planetscale-darkmode.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=c2012d7f89d1d38f13b2292fe908e35a 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/postgres-mysql-planetscale-darkmode.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=f6d63c0f8d48c05a908288bb0c597b15 2500w" />
</Frame>

It is up to you to determine how to handle the cut over between the two in your application.

### Prerequisites

* An AWS account
* An empty PlanetScale database as the target
* The [AWS CLI](https://aws.amazon.com/docs/cli/)

<Warning>
  These import scripts create and modify resources in your AWS account.
  Before executing, you should read through the scripts to ensure you are comfortable with the actions they will take.
  You will also be billed in AWS for the resources it creates, which include:

  * 1 DMS replication task
  * 1 DMS replication instance
  * 1 DMS replication subnet group
  * 2 DMS endpoints
  * 1 Aurora MySQL database
</Warning>

## Importing a database

### 1. Prepare Postgres for migration

Before beginning a migration to PlanetScale, you should ensure the following flags are set on your Postgres database.

| flag name                  | flag value |
| -------------------------- | ---------- |
| logical\_replication       | 1          |
| shared\_preload\_libraries | pglogical  |

Given the variety of Postgres options and versions, there may be additional flags that you will need to adjust to make the import work.
If you encounter errors when using the script below, it may be caused by other Postgres options not shown here.
You can either update your Postgres configuration based on the error you see, or [contact support](https://planetscale.com/contact?initial=support) for additional assistance.

<Warning>
  You should not make any schema changes to the source database during an import.
</Warning>

### 2. Create an EC2 instance

These scripts are designed to be run from an EC2 instance on the same account that you will authenticate into.
Create one, and then log in to the instance.
Ensure that both Postgres and MySQL are installed:

```
sudo apt update
sudo apt install postgresql
sudo apt install mysql-server
```

### 3. Install the AWS CLI

The migration scripts we provide rely on [AWS Database Migration Service](https://aws.amazon.com/dms/).
To use the scripts, you will need to download and install the [AWS CLI](https://aws.amazon.com/docs/cli/).
You will also need to authenticate into the AWS account that you would like to run the migration from.
This step is necessary, even if you are importing from a non-AWS Postgres provider.

Go ahead and download and install the AWS CLI on the EC2 instance you created in the last step.

For example, on **Ubuntu**, you would run:

```
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
```

Full instructions can be found on the [AWS documentation](https://docs.aws.amazon.com/docs/cli/latest/userguide/getting-started-install.html).

### 4. Authenticate into AWS

After installing the CLI, you must authenticate into the AWS account that you intend to run the import scripts in.
There are several ways to authenticate.
You can find instructions in the [AWS documentation](https://docs.aws.amazon.com/docs/cli/v1/userguide/cli-chap-authentication.html).
We recommend you authenticate with [short-term credentials](https://docs.aws.amazon.com/docs/cli/v1/userguide/cli-authentication-short-term.html).

The authenticated account will need permissions to both create and modify DMS resources, RDS / Aurora databases, security groups, and parameter groups

### 5. Prepare the import

Check out the [migration-scripts](https://github.com/planetscale/migration-scripts) repository, and navigate to the `postgres-mysql-planetscale` directory.

```
git clone https://github.com/planetscale/migration-scripts
cd migration-scripts/docs/postgres-mysql-planetscale
```

Since these scripts will be running in your AWS account, it is important that you understand what the scripts are doing before executing them.
Before running, look through the scripts and confirm what they will be doing in your AWS account is acceptable to you and your organization.
At a high level, this script does the following:

1. Creates a DMS source using the Postgres credentials you provide
2. Creates a new Aurora MySQL database and sets it as the target for the DMS import
3. Creates a DMS import instance (a server to handle the migration)
4. Sets up rules for how to handle the migration

If there are any concerns, you should modify the scripts to suit your needs.

<Warning>
  This script prepares for a migration between your postgres source and a new MySQL database. The MySQL database will be accessible from all IPs.

  If you want a tighter security configuration, modify the script to make the database only accessible from the [required PlanetScale IPs](/docs/vitess/imports/import-tool-migration-addresses).
</Warning>

If you are comfortable proceeding, the next step is to execute the `prepare.sh` script.
You will need to provide this with a unique identifier for this import, as well as the connection credentials for the source Postgres database.

<Note>
  If you are importing from **Supabase**, the scripts will not work with a transaction pooler or session pooler connection.
  You must use a direct connection over ipv4.
  In order to use this, you must be on the pro plan or greater, and pay for the ipv4 connection upgrade.
  After doing so, use the direct connection credentials and host when using `import.sh`.

  If you are importing from **Neon**, You must use `--tls` mode when importing from Neon.
  This sets `SSL_MODE="require"` on the connection, a necessity for Neon.
</Note>

Here's an example of executing this:

```
 sh prepare.sh --identifier "PGtoPSImport01" \
   --source "${PG_USER}:${PG_PASSWORD}@${PG_HOST}/${PG_DB}/${PG_SCHEMA}" \
   --ips "us-east-1"
```

You can choose whatever identifier you want in place of `PGtoPSImport01`.
The variables prefixed with `PG_` are for the Postgres source.

Running the script like this will give you occasional log messages indicating which phase of the import process it is at.
If you want full debug mode, including each command the script executes, add the `--debug` flag:

```
 sh prepare.sh --identifier "PGtoPSImport01" \
   --source "${PG_USER}:${PG_PASSWORD}@${PG_HOST}/${PG_DB}/${PG_SCHEMA}" \
   --ips "us-east-1" \
   --debug
```

This script can take upwards of 15 minutes, particularly because of the step to set up the DMS import server.

When this script completes, it will provide the connection information for the MySQL instance and instructions for next steps.
For example:

```
======================================================================================
SETUP COMPLETED SUCCESSFULLY
======================================================================================

MySQL RDS instance information:
Hostname: TARGET_HOSTNAME
Database: TARGET_DATABASE

Admin user:
Username: TARGET_USERNAME
Password: TARGET_PASSWORD

Migration user (for PlanetScale):
Username: migration_user
Password: MIGRATION_PASSWORD

======================================================================================
NEXT STEPS:
======================================================================================
1. Log into your new MySQL instance using the credentials above
2. Set up your schema manually
3. Once your schema is ready, run the start.sh script with the same identifier:
   sh start.sh --identifier \"$IDENTIFIER\"
```

Notably, this step does not actually begin the migration, but sets up the necessary AWS and DMS resources.

<Note>
  If your database is on a PlanetScale cloud or managed plan, you will need to manually provide your IP addresses.
  For this, use `--ips "manual"` and then give the script a comma-separated list of IPs as instructed by the script.
</Note>

### 6. Copy your schema

We have configured these scripts so that they do not automatically copy the schema from Postgres to MySQL.
This is intentional, as DMS sometimes does not make good choices for how to convert Postgres types to MySQL.
Therefore, we leave it up to you to copy the schema before we begin the migration via `import.sh`.

There are several ways you can do this, but one option is to use `pg_dump` to get your schema, convert to MySQL types / syntax, the apply it to the MySQL target.
First, you'd run a `pg_dump` command:

```
PGPASSWORD=${PG_PASSWORD} pg_dump -h ${PG_HOST} -U ${PG_USERNAME} ${PG_DATABASE} --schema-only --format=p > schema.sql
```

Next, modify `schema.sql`.
Remove all excessive lines from the dump, and update all column types to use ones supported by MySQL.

Finally, apply this schema to a new database in the MySQL target created by the `prepare.sh` script:

```
echo "CREATE DATABASE ${MYSQL_DATABASE}" | mysql -u ${MYSQL_USER} -p${MYSQL_PASSWORD} -h${MYSQL_HOST}
cat schema.sql | mysql -u ${MYSQL_USER} -p${MYSQL_PASSWORD} -h${MYSQL_HOST} ${MYSQL_DATABASE}
```

Then, log in to the MySQL instance to confirm the schema was applied correctly.

### 7. Migration from Postgres to Aurora MySQL

Next, we need to run `start.sh`.
This initiates the migration between the Postgres source and the Aurora MySQL target.
To start, just run:

```
start.sh --identifier "PGtoPSImport01"
```

You can monitor the progress of the migration by comparing row counts between the source and target database.
The initial data copy may range from several minutes to several hours depending on the size of your database.
After the initial copy, data changes to your Postgres database will replicate to Aurora MySQL.
You do not need to connect to the Aurora MySQL database form your application.
We are only using it as a temporary holding-place while moving into PlanetScale.

### 8. The PlanetScale import tool

To get your data into PlanetScale, we will use the import tool to migrate the data from the Aurora MySQL instance created in the previous steps into PlanetScale.
Log into PlanetScale, select your organization, click "New database", and then "Import database."

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/planetscale-new-import.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=fe88a5f1091b7c879fb4d77b32e07896" alt="PlanetScale new database via import" data-og-width="3024" width="3024" data-og-height="1122" height="1122" data-path="docs/vitess/imports/planetscale-new-import.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/planetscale-new-import.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=9454f709043320c12ee7c47de0fe35e0 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/planetscale-new-import.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=93930359cfe642324ab2028bc66aa2bf 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/planetscale-new-import.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=970c3f819892fa890474c805b8c4845e 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/planetscale-new-import.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=9bc27a2d4a0480657a371f0fe3256946 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/planetscale-new-import.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=ee730466e894993347d2244dfd24b6db 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/planetscale-new-import.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=6b5e4ef8555c7fbe8b55fa77711f7624 2500w" />
</Frame>

Enter the name of your database and choose the type and size of database you want to import to.
For large imports, we recommend using Metal for improved import speed.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/planetscale-database-name-type.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=ddefc55b4d0a83e3ae17ec6ecfd6db80" alt="PlanetScale set database name and type for import" data-og-width="3024" width="3024" data-og-height="1420" height="1420" data-path="docs/vitess/imports/planetscale-database-name-type.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/planetscale-database-name-type.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=75aafab8dd0de6c1ab2f2f1dc50b62d4 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/planetscale-database-name-type.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=166d7836aee7b6bf2fb69c259c6cf46b 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/planetscale-database-name-type.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=28da7624c7708827c8ac51d259ad9939 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/planetscale-database-name-type.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=074e106fb9b5190ef4a90f00c863ba5f 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/planetscale-database-name-type.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=48617a29b8c6fb0ca265439664f9aae0 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/planetscale-database-name-type.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=f6c04c1364af971f698318893e610d72 2500w" />
</Frame>

Scroll down and you will see a section to add the connection information for the database to import.
Enter all of the credentials that the script printed out, and use the username and password for the `migration_user`.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/planetscale-import-connection-info.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=2527223b990f6afbe197d8d9009d1f39" alt="PlanetScale import connection info" data-og-width="2060" width="2060" data-og-height="1774" height="1774" data-path="docs/vitess/imports/planetscale-import-connection-info.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/planetscale-import-connection-info.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=c5c97e0a08ad16cffc6a87bcdc1a15e7 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/planetscale-import-connection-info.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=7738ecfb50a3c40713416a7817dc3fec 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/planetscale-import-connection-info.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=c4006d7433de6480cf200916ddb86d09 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/planetscale-import-connection-info.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=8f9dab2f67ddada6b78610eac6c03e03 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/planetscale-import-connection-info.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=84d0a0210f0e1c3009f7cd2415e488c0 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/planetscale-import-connection-info.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=b47519543b34689ac98cee61ace94ee5 2500w" />
</Frame>

Click "Connect to Database."
If you encounter any errors at this step, look carefully at the error message and address any connection or schema issues as needed.
When the import is ready, click "Begin import."

### 9. Complete the import

This full import flow not only copies data, but does continuous replication of traffic from Postgres, to DMS, to Aurora MySQL, and finally to PlanetScale.
Replication between these continues until you stop the DMS task using `cleanup.sh` and complete the PlanetScale import flow.

It is up to you to determine how you want to cut over your application to use PlanetScale as your primary database instead of the old Postgres source.
Before doing this, you should ensure all of your queries and/or your ORM are updated to work properly with PlanetScale.
We also recommend doing some performance testing, and adding indexes if you encounter slow queries.

### 10. Clean up import

After you have switched all of your traffic over to PlanetScale and are comfortable wrapping up the import, you can clean up the resources that the script created.
This includes the DMS migration instance, the Aurora database, and the DMS source / targets.

<Warning>
  Do not run this until you are absolutely sure you no longer need the migration set up.
</Warning>

To clean up the resources, you can run:

```
sh cleanup.sh --identifier "PGtoPSImport01"
```

We recommend double checking that all of the resources were properly cleaned up in your AWS console after running this.

## Resolving errors

We have designed these scripts to run as generally as possible and have tested them on a variety of platforms.
Even so, you may encounter errors for a variety of reasons.

In some cases, the `aws` command that fails will produce a useful error message in the script output, which you can take action on as needed.

If the script is able to set up all of the resources without error but the import is not working, we recommend you take a look at the migration task logs.
You can view and search through these in the AWS web console.

Log in to the web console, and go to the AWS DMS service page.
In the sidebar, click on "Database migration tasks."

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=043d6ed57996bee3e7dea3d8ddbc9742" alt="AWS DMS landing page" data-og-width="3010" width="3010" data-og-height="1720" height="1720" data-path="docs/vitess/imports/aws-dms.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=95afa3e1bd99ef86e4b73c1442169c8c 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=f671f6018c7d8063d0d5b4550c8db3ce 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=ea9137d816fdcb1a09f536e47f872380 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=6fd3fd5b3f498ce3d5a9836f3dc20b2d 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=a741b789aa76433d82e188c939359f0d 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=7bd3276128a63bab08844441c8894786 2500w" />
</Frame>

You should see a migration task in the list with a name that corresponds to the identifier you chose.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-tasks.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=6cb6b624de6db0acc2477e58394d287f" alt="AWS DMS migration tasks" data-og-width="3012" width="3012" data-og-height="1722" height="1722" data-path="docs/vitess/imports/aws-dms-migration-tasks.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-tasks.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=0bbd50cd5f87541036c2e2a746ed70cc 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-tasks.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=de1724f34c7f6e8b957e1c565732f12c 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-tasks.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=faa4d773b55caf3852874b0f637e3a46 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-tasks.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=b9562e40782af7958bb9ef005f1bac64 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-tasks.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=414626fcf29d15ef77c1ec79d2a9a1d3 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-tasks.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=30a316341d98c6a3d29cd44e4da2f78b 2500w" />
</Frame>

Click on the migration task, and then click "View logs" in the top right corner.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-task-choice.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=94c5ea0c4959a1ff036648e4c6ad11b3" alt="AWS DMS migration task choice" data-og-width="3012" width="3012" data-og-height="1722" height="1722" data-path="docs/vitess/imports/aws-dms-migration-task-choice.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-task-choice.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=701b8f700944ec49040a3f9d5e8e4152 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-task-choice.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=9cc94346aeb525cef62bb320d479cffb 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-task-choice.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=1554029b5d2b36a39af3669ee20fd25d 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-task-choice.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=778b453f5b222c43e07bfc9676a676da 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-task-choice.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=28d11f55c053f78fb5c6c1f79df96535 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-task-choice.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=08d87c32dbebaea640a9166863238018 2500w" />
</Frame>

This will bring you to your CloudWatch logs for this replication task, and you can search through it for error and warning messages to help you pinpoint the issue.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-task-logs.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=41502e80d17eba6b2952ebdfebc96163" alt="AWS DMS migration task logs" data-og-width="2334" width="2334" data-og-height="1722" height="1722" data-path="docs/vitess/imports/aws-dms-migration-task-logs.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-task-logs.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=1f1b5b436c39ca2e400e7656fb460e80 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-task-logs.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=2fc493aa0b2c4e384b94fa760d19885d 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-task-logs.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=e8dba2bb51359b4830a7619a8f6809fd 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-task-logs.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=75c6a5949c6f71b936029e9ff976f77c 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-task-logs.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=77d458d7cd04a304df982c048e462c6f 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-task-logs.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=cea6b9ce7879057d21803d05e926f18e 2500w" />
</Frame>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Postgres to PlanetScale for Vitess migration guide
Source: https://planetscale.com/docs/vitess/imports/postgres-planetscale-migration-guide



<Warning>
  **PlanetScale now supports Postgres**. This guide is for migrating from Postgres to PlanetScale's Vitess product. You can still use these scripts if you would like to utilize [Vitess](/docs/vitess). If you prefer to stay on Postgres, refer to the [Postgres import guides](/docs/postgres/imports/postgres-imports).
</Warning>

This guide covers how to do an import directly between a Postgres source database and a PlanetScale for Vitess target.
For this, we will be using the [postgres-planetscale scripts](https://github.com/planetscale/migration-scripts).
This method of importing can be slow.
If you have a large database, you might consider trying [an alternative, faster approach](/docs/vitess/imports/postgres-mysql-planetscale-migration-guide).
You are encouraged to modify these scripts to suit your needs if need be.

## Methodology

The scripts in this guide leverage [AWS Database Migration Service](https://aws.amazon.com/dms/) to handle conversions between Postgres and MySQL types.
Even if you are migrating from a non-AWS Postgres provider, such as Neon or Supabase, you will still need an AWS account to perform the migration.

DMS acts as a middle-man between your Postgres source database and PlanetScale.
All of your data will pass through the DMS server which will handle type conversions before passing the data on to your PlanetScale database.
After the initial data copy, DMS will continue to replicate changes from your source to the target until you stop it.
It is up to you to determine how to handle the cut over between the two in your application.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/postgres-planetscale-darkmode.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=b25cdd0826775f1940fc1d157a6bee40" alt="Import data flow" data-og-width="2424" width="2424" data-og-height="812" height="812" data-path="docs/images/postgres-planetscale-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/postgres-planetscale-darkmode.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=26c1636f243437e310fd800284ba7088 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/postgres-planetscale-darkmode.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=a18802bdeff9cf36d8a7f987dc8b3474 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/postgres-planetscale-darkmode.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=eeea225deca228b171f93baf9b38aedf 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/postgres-planetscale-darkmode.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=cf047a3be7bc9ae7a442f5d66001ad03 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/postgres-planetscale-darkmode.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=cdfdadeabd55d6eb4d90e17eadbee5c2 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/postgres-planetscale-darkmode.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=5cc77a4e3e69cd2555465392fd1ceb36 2500w" />
</Frame>

### Prerequisites

* An AWS account
* An empty PlanetScale database as the target
* The [AWS CLI](https://aws.amazon.com/docs/cli/)

<Warning>
  These import scripts create and modify resources in your AWS account.
  Before executing, you should read through the scripts to ensure you are comfortable with the actions they will take.
  You will also be billed in AWS for the resources it creates, which include:

  * 1 DMS replication task
  * 1 DMS replication instance
  * 1 DMS replication subnet group
  * 2 DMS endpoints
</Warning>

## Importing a database

### 1. Prepare Postgres for migration

Before beginning a migration to PlanetScale, you should ensure the following flags are set on your Postgres database.

| flag name                  | flag value |
| -------------------------- | ---------- |
| logical\_replication       | 1          |
| shared\_preload\_libraries | pglogical  |

Given the variety of Postgres options and versions, there may be additional flags that you will need to adjust to make the import work.
If you encounter errors when using the script below, it may be caused by other Postgres options not shown here.
You can either update your Postgres configuration based on the error you see, or [contact support](https://planetscale.com/contact?initial=support) for additional assistance.

<Warning>
  You should not make any schema changes to the source database during an import.
</Warning>

### 2. Install the AWS CLI

The migration scripts we provide rely on [AWS Database Migration Service](https://aws.amazon.com/dms/).
To use the scripts, you need to download and install the [AWS CLI](https://aws.amazon.com/docs/cli/).
You will also need to authenticate into the AWS account that you would like to run the migration from.
This step is necessary, even if you are importing from a non-AWS Postgres provider.

Go ahead and download and install the AWS CLI.
On **MacOS**, you can run:

```
curl "https://awscli.amazonaws.com/AWSCLIV2.pkg" -o "AWSCLIV2.pkg"
sudo installer -pkg AWSCLIV2.pkg -target /
```

On **Linux**, you run:

```
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install
```

Full instructions can be found on the [AWS documentation](https://docs.aws.amazon.com/docs/cli/latest/userguide/getting-started-install.html).

### 3. Authenticate into AWS

After installing the CLI, you must authenticate into the AWS account that you intend to run the import scripts in.
There are several ways to authenticate.
You can find instructions in the [AWS documentation](https://docs.aws.amazon.com/docs/cli/v1/userguide/cli-chap-authentication.html).
We recommend you authenticate with [short-term credentials](https://docs.aws.amazon.com/docs/cli/v1/userguide/cli-authentication-short-term.html).
The authenticated account will need permissions to both create and modify DMS resources, security groups, and parameter groups.

### 4. Customizing import of large tables

If you have large tables that are in need of a [parallel import](https://aws.amazon.com/blogs/database/speed-up-database-migration-by-using-aws-dms-with-parallel-load-and-filter-options/), you should specify custom rules for these to give to DMS.

For example, perhaps we have a large table named `items` that contains 1 million rows.
We would like DMS to import this in parallel with multiple threads.
To configure this, create a json file named `custom-table-mappings.json` and place this in there:

```json expandable theme={null}
{
  "rules": [
    {
      "object-locator": {
        "schema-name": "public",
        "table-name": "items"
      },
      "parallel-load": {
        "boundaries": [
          ["250000"],
          ["500000"],
          ["750000"]
        ],
        "columns": ["item_id"],
        "type": "ranges"
      },
      "rule-id": "3",
      "rule-name": "parallel-load-settings",
      "rule-type": "table-settings"
    }
  ]
}
```

These rules will be used to tell DMS that it can load the table in 4 parallel threads.
It can load rows with `item_id` ranges 0-250k, 250k-500k, 500k-750k, and 750k+ in separate threads.

You can add multiple rules here if there are multiple large tables.
You do not need to specify custom rules for small tables.

If you use custom rules, ensure you pass this file to the `import.sh` script in the next step via `--table-mappings custom-table-mappings.json`.

Learn more about the options available to you here in the [AWS table mappings documentation](https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TableMapping.html).

### 5. Begin the import

Check out the [migration-scripts](https://github.com/planetscale/migration-scripts) repository, and navigate to the `postgres-planetscale` directory.

```bash  theme={null}
git clone https://github.com/planetscale/migration-scripts
cd migration-scripts/docs/postgres-planetscale
```

Since these scripts will be running in your AWS account, it is important that you understand what the scripts are doing before executing them.
Before running, look through the scripts and confirm what they will be doing in your AWS account is acceptable to you and your organization.
At a high level, the `import.sh` script does the following:

<Steps>
  <Step>
    Creates a new DMS source and target using the credentials you provide
  </Step>

  <Step>
    Creates a DMS import instance (a server to handle the migration)
  </Step>

  <Step>
    Creates / modifies the subnets so the databases can communicate with DMS
  </Step>

  <Step>
    Sets up rules for how to handle the migration and how to map tables between the source and target
  </Step>

  <Step>
    Begins the DMS migration task to copy data between the two instances
  </Step>

  <Step>
    After initial data copy is complete, continues to replicate the data between the instances until you are ready to stop the task
  </Step>
</Steps>

If there are any concerns, you should modify the scripts to suit your needs.

If you are comfortable proceeding, the next step is to execute the `import.sh` script.
You will need to provide this with a unique identifier for this import, as well as the connection credentials for both the source Postgres database and the target PlanetScale database.

<Note>
  If you are importing from **Supabase**, the scripts will not work with a transaction pooler or session pooler connection.
  You must use a direct connection over ipv4.
  In order to use this, you must be on the pro plan or greater, and pay for the ipv4 connection upgrade.
  After doing so, use the direct connection credentials and host when using `import.sh`.

  If you are importing from **Neon**, You must use `--tls` mode when importing from Neon.
  This sets `SSL_MODE="require"` on the connection, a necessity for Neon.
</Note>

Here's an example of executing this:

```
 sh import.sh --identifier "PGtoPSImport01" \
   --source "${PG_USER}:${PG_PASSWORD}@${PG_HOST}/${PG_DB}/${PG_SCHEMA}" \
   --target "${PS_USER}:${PS_PASSWORD}@${PS_HOST}/${PS_DB}"
```

You can choose whatever identifier you want in place of `PGtoPSImport01`.
The variables prefixed with `PG_` are for the Postgres source, and the ones prefixed with `PS_` are for the PlanetScale target.

Running the script like this will give you occasional log messages indicating which phase of the import process it is at.
If you want full debug mode, including each command the script executes, add the `--debug` flag:

```
 sh import.sh --identifier "PGtoPSImport01" \
   --source "${PG_USER}:${PG_PASSWORD}@${PG_HOST}/${PG_DB}/${PG_SCHEMA}" \
   --target "${PS_USER}:${PS_PASSWORD}@${PS_HOST}/${PS_DB}" \
   --debug
```

If you are running the script for the first time, we recommend using `--debug` in case you encounter any issues needing debugging.

### 6. Completing the import

The `import.sh` script can take upwards of 20 minutes to prepare all of the resources before even beginning the import.
After this, the time for the import itself varies widely depending on the size and load of your database.
You should monitor the progress of the migration by comparing row counts in PlanetScale to ones from the source.

Once the migration is complete, it is up to you to determine when you want to cut over your application to use PlanetScale as your primary database instead of the old Postgres source.
Before doing this, you should ensure all of your queries are updated to work properly with PlanetScale and do some performance testing.
If you encounter performance issues, you likely need to add indexes. Use [PlanetScale Query Insights](/docs/vitess/monitoring/query-insights) to discover and improve poor performing queries.

### 7. Cleaning up the import

When you have fully switched all traffic over to PlanetScale and are comfortable stopping the replication between Postgres and PlanetScale, you can use the `cleanup.sh` script to delete the DMS resources that `import.sh` created.
All you have to provide is the `--identifier` that you used when starting the import.

```
sh cleanup.sh --identifier "PGtoPSImport01"
```

We recommend double checking that all of the resources were properly cleaned up in your AWS console after running this.

## Resolving errors

We have designed these scripts to run as generally as possible and have tested them on a variety of platforms.
Even so, you may encounter errors for a variety of reasons.

In some cases, the `aws` command that fails will produce a useful error message in the script output, which you can take action on as needed.

If the script is able to set up all of the resources without error but the import is not working, we recommend you take a look at the migration task logs.
You can view and search through these in the AWS web console.

Log in to the web console, and go to the AWS DMS service page.
In the sidebar, click on "Database migration tasks."

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=043d6ed57996bee3e7dea3d8ddbc9742" alt="AWS DMS landing page" data-og-width="3010" width="3010" data-og-height="1720" height="1720" data-path="docs/vitess/imports/aws-dms.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=95afa3e1bd99ef86e4b73c1442169c8c 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=f671f6018c7d8063d0d5b4550c8db3ce 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=ea9137d816fdcb1a09f536e47f872380 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=6fd3fd5b3f498ce3d5a9836f3dc20b2d 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=a741b789aa76433d82e188c939359f0d 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=7bd3276128a63bab08844441c8894786 2500w" />
</Frame>

You should see a migration task in the list with a name that corresponds to the identifier you chose.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-tasks.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=6cb6b624de6db0acc2477e58394d287f" alt="AWS DMS migration tasks" data-og-width="3012" width="3012" data-og-height="1722" height="1722" data-path="docs/vitess/imports/aws-dms-migration-tasks.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-tasks.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=0bbd50cd5f87541036c2e2a746ed70cc 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-tasks.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=de1724f34c7f6e8b957e1c565732f12c 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-tasks.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=faa4d773b55caf3852874b0f637e3a46 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-tasks.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=b9562e40782af7958bb9ef005f1bac64 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-tasks.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=414626fcf29d15ef77c1ec79d2a9a1d3 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-tasks.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=30a316341d98c6a3d29cd44e4da2f78b 2500w" />
</Frame>

Click on the migration task, and then click "View logs" in the top right corner.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-task-choice.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=94c5ea0c4959a1ff036648e4c6ad11b3" alt="AWS DMS migration task choice" data-og-width="3012" width="3012" data-og-height="1722" height="1722" data-path="docs/vitess/imports/aws-dms-migration-task-choice.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-task-choice.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=701b8f700944ec49040a3f9d5e8e4152 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-task-choice.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=9cc94346aeb525cef62bb320d479cffb 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-task-choice.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=1554029b5d2b36a39af3669ee20fd25d 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-task-choice.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=778b453f5b222c43e07bfc9676a676da 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-task-choice.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=28d11f55c053f78fb5c6c1f79df96535 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-task-choice.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=08d87c32dbebaea640a9166863238018 2500w" />
</Frame>

This will bring you to your CloudWatch logs for this replication task, and you can search through it for error and warning messages to help you pinpoint the issue.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-task-logs.png?fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=41502e80d17eba6b2952ebdfebc96163" alt="AWS DMS migration task logs" data-og-width="2334" width="2334" data-og-height="1722" height="1722" data-path="docs/vitess/imports/aws-dms-migration-task-logs.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-task-logs.png?w=280&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=1f1b5b436c39ca2e400e7656fb460e80 280w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-task-logs.png?w=560&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=2fc493aa0b2c4e384b94fa760d19885d 560w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-task-logs.png?w=840&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=e8dba2bb51359b4830a7619a8f6809fd 840w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-task-logs.png?w=1100&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=75c6a5949c6f71b936029e9ff976f77c 1100w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-task-logs.png?w=1650&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=77d458d7cd04a304df982c048e462c6f 1650w, https://mintcdn.com/planetscale-cad1a68a/WcazQYbZzMHxCEvC/docs/vitess/imports/aws-dms-migration-task-logs.png?w=2500&fit=max&auto=format&n=WcazQYbZzMHxCEvC&q=85&s=cea6b9ce7879057d21803d05e926f18e 2500w" />
</Frame>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Airbyte integration
Source: https://planetscale.com/docs/vitess/integrations/airbyte

With PlanetScale Connect, you can extract data from your PlanetScale database and safely load it into other destinations for analysis, transformation, and more.

We implemented an [Airbyte](https://airbyte.com/) connector as the pipeline between your PlanetScale source and selected destination. This document will walk you through how to connect your PlanetScale database to Airbyte.

## Connect to Airbyte

Only [Airbyte Open Source](https://docs.airbyte.com/quickstart/deploy-airbyte) supports the PlanetScale data source. In this section, you'll learn how to set up Airbyte and connect your PlanetScale source.

### Requirements

* A PlanetScale database
* [Docker Desktop](https://www.docker.com/products/docker-desktop/) (Docker terms apply)

### Set up Airbyte locally

<Steps>
  <Step>
    Install [Docker Desktop](https://www.docker.com/products/docker-desktop/).
  </Step>

  <Step>
    Follow the related installation instructions included within the [Airbyte Quickstart Documentation](https://docs.airbyte.com/using-airbyte/getting-started/oss-quickstart).
  </Step>

  <Step>
    Open Airbyte in the browser at [http://localhost:8000](http://localhost:8000).
  </Step>
</Steps>

### Set up PlanetScale source

Now that Airbyte is running locally, let's set up the custom PlanetScale source.

<Steps>
  <Step>
    In the Airbyte dashboard, click "**Settings**" on the bottom left.
  </Step>

  <Step>
    Click "**Sources**" on the left sidebar.
  </Step>

  <Step>
    Click the "**New connector**" button.
  </Step>

  <Step>
    Click the "**Add a new Docker connector**" option.
    Fill in the connector values as follows:

    * **Connector display name**: PlanetScale
    * **Docker repository name**: planetscale/airbyte-source
    * **Docker image tag**: `latest`
    * **Connector Documentation URL**:(/docs/vitess/integrations/airbyte
  </Step>
</Steps>

You can find the [PlanetScale Airbyte Source Dockerhub release page here](https://hub.docker.com/r/planetscale/airbyte-source).

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/modal.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=ef6bfa8dca1b88bd8a26a6358756c201" alt="Airbyte new PlanetScale connector" data-og-width="2594" width="2594" data-og-height="2378" height="2378" data-path="docs/images/assets/docs/integrations/airbyte/modal.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/modal.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=110ee71f12cb600875234630992f542f 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/modal.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=1dfee5945b8f2a5cae3b0c67faaf5511 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/modal.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=ba3ed772204ff90901eac87c33f16be9 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/modal.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=4608782f2938240339295f8a07b19ab1 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/modal.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=a7b4e9e5cf23c24b10c8feb6ff52d70b 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/modal.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=d3b824c8bd15d8e9dde4585d76486ab1 2500w" />
</Frame>

### Fill in PlanetScale connection information

You're now ready to connect your PlanetScale database to Airbyte.

<Steps>
  <Step>
    Click on the database and branch you want to connect to.
  </Step>

  <Step>
    Click "**Connect**", select "**General**" from the "**Connect with**" dropdown.
  </Step>

  <Step>
    Leave this tab open, as you'll need to copy these credentials shortly.
  </Step>

  <Step>
    Back in Airbyte, click "**Sources**" in the main left sidebar > "**New source**".
  </Step>

  <Step>
    Select the new PlanetScale source you created from the dropdown.
  </Step>

  <Step>
    Fill in the "**Set up the source**" values as follows:

    * **Name**: Any name of your choice
    * **Source type**: Select "PlanetScale"
    * **Host**: Paste in the copied value for `host`
    * **Database**: Paste in the copied value for `database`
    * **Username**: Paste in the copied value for `username`
    * **Password**: Paste in the copied value for `password`

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/db-info.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=9156ee6a9a926f94dc2371974e3005a2" alt="Airbyte - PlanetScale source setup" data-og-width="4324" width="4324" data-og-height="2644" height="2644" data-path="docs/images/assets/docs/integrations/airbyte/db-info.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/db-info.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=b75e136d3d83196bf22f4a1ebd3b952d 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/db-info.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=ea3e7fd4b8fce00802d3ebd9b2dc8157 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/db-info.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=377c6f0ce5c6cea656dcd9abc69339e1 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/db-info.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=f0588dc2df2cfee1abb7485f6c1cf88d 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/db-info.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=f39ecce6d26eb435368ac0b9c629a835 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/db-info.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=03c48e7b2c429dd31df5be8a60d9eaa6 2500w" />
    </Frame>
  </Step>

  <Step>
    You can also provide some optional values:

    * **Replicas**: Select whether or not you want to collect data from replica nodes.
    * **Shards**: Map your shards.
    * **Starting GTIDs**: Start replication from a specific GTID per keyspace shard.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/optional.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=2f794ce4c4d22e5d758ad3a7a023fa09" alt="Airbyte - PlanetScale optional setup" data-og-width="3606" width="3606" data-og-height="892" height="892" data-path="docs/images/assets/docs/integrations/airbyte/optional.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/optional.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=2101603af5c5d9ea7ed0043d15fefa16 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/optional.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=5aa5a36c49d5e279fbc04910a8e55305 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/optional.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=c3044a4ff72063ccee4a85170d19bb1d 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/optional.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=8a0a6a4bf487f046e58afec5d5d64d69 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/optional.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=bd2b40c5361158ede3454f442850fdc9 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/optional.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=90d0140fe65976240cbb99382c2505f5 2500w" />
    </Frame>

    You can see the [PlanetScale airbyte-source README](https://github.com/planetscale/airbyte-source/blob/main/README.md) for more details on these options.
  </Step>

  <Step>
    Click "**Set up source**" to connect.
  </Step>
</Steps>

You should get a success message that the connection test passed.

### Choose your destination

With the connection complete, you can now choose your destination.

<Steps>
  <Step>
    Click "**Destinations**" in the sidebar or the "**New destination**" button on the source connection page.
  </Step>

  <Step>
    Set up the destination you want to sync your data to.
  </Step>
</Steps>

Each destination should have a Setup Guide linked on its destination setup page.

### Configure a connection

Now to get the connection fully set up.
Click on "Connections" on the left side bar.
If you have not yet set up any connectors, you should see this:

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/create.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=57549ae292673f7ca90957f6dd42e190" alt="Airbyte - New connection" data-og-width="2552" width="2552" data-og-height="1326" height="1326" data-path="docs/images/assets/docs/integrations/airbyte/create.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/create.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=3d64b2ee1c49472102ff37b60b45d1c8 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/create.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=4cedbe097f5f50777d915a038b62c812 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/create.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=329e8d1d5ecba847d241b4f36a19a1c1 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/create.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=206ec13c826bcd8d225b482748e7d9f1 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/create.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=88b11bdea316e0e93e417a9133c4e87f 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/create.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=1bd27e0595fc879ec5e1620627868a17 2500w" />
</Frame>

Click the button to set up a connection.
Otherwise, click "**New Connection**" in the top right corner.
From here, follow these steps:

<Steps>
  <Step>
    On the "**Define source**" page, choose your PlanetScale source as the **source**.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/source.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=122d64604916ed970c722c9c8a8224bb" alt="Airbyte - Source" data-og-width="2252" width="2252" data-og-height="575" height="575" data-path="docs/images/assets/docs/integrations/airbyte/source.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/source.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=732bb48274fc60a831deef96f7f3a76c 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/source.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=ef8a06c77ac9231efef47bb68c9847cc 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/source.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=bb1b69abe4de4eb885a88ec6bc7e8b15 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/source.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=f1f31552437842b083d61a6bfae0b3da 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/source.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=22fcc667d657276bf8d182438365090a 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/source.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=caf26f2ac9dd1e99f601cc72bf33bf6d 2500w" />
    </Frame>
  </Step>

  <Step>
    On the "**Define destination**" page, select the **destination** you want to sync your PlanetScale data to.
    For this demo, we are using a CSV destination.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/destination.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=9e666c58a217148bc5eeb50ee7af5781" alt="Airbyte - Source" data-og-width="2253" width="2253" data-og-height="638" height="638" data-path="docs/images/assets/docs/integrations/airbyte/destination.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/destination.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=817c7746574cb2d96b7cb2716bb9be68 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/destination.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=2100502590cd938fe514a33dc534b369 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/destination.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=d528b4e08a0062fe61859e9cb375d86a 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/destination.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=5c44c77506555de4fc103e91899aa318 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/destination.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=d764edee220bf7efb47d4383cdc5685d 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/destination.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=1113b0e555ba96677279e48cc997e5a1 2500w" />
    </Frame>
  </Step>

  <Step>
    On the "**Select streams**" page, select a sync mode.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/streams.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=1b421fd50ce7af795468970b1a5b65e3" alt="Airbyte - Source" data-og-width="2252" width="2252" data-og-height="1019" height="1019" data-path="docs/images/assets/docs/integrations/airbyte/streams.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/streams.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=e17c18e63dfc972e4174a31ae885734f 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/streams.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=46815452786a2293cf68af06509b4491 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/streams.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=da45da2c551116b61b685bd70384c3de 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/streams.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=7173fc0c3ab932af03024b5cb5c2ec8e 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/streams.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=307500b930fa91fd16f614142be48d52 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/streams.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=cf4c9fe7aaa25401603c1e0ca3a09ca1 2500w" />
    </Frame>
  </Step>

  <Step>
    Also on this page, you will need to select the specific tables and columns you want to sync. For each, choose what type of sync mode you'd like to use for each source table.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/sync.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=42dc150525ebf7d9153f427e4dda03c8" alt="Airbyte - Sync" data-og-width="1259" width="1259" data-og-height="718" height="718" data-path="docs/images/assets/docs/integrations/airbyte/sync.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/sync.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=7cbea68ebcca591800938d4e3727f300 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/sync.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=9974d2bb3cda98079ee69cbe41975f09 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/sync.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=0111c403cb4baaa491ae40d104fd0b81 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/sync.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=9261a8fcf4c4f25553a4efbb7537dd70 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/sync.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=b079bb2ef5e55820ca31414968e7d77e 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/sync.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=dfed6a2e1823f2fdddddb1bfa91a4845 2500w" />
    </Frame>

    * **Incremental** — Incremental sync pulls *only* the data that has been modified/added since the last sync. We use [Vitess VStream](https://vitess.io/docs/concepts/vstream/) to track the stopping point of the previous sync and only pull any changes since then.
    * **Full refresh** — Full refresh pulls *all* data at every scheduled sync frequency.
  </Step>

  <Step>
    On the "**Configure connection**" page, choose a sync frequency, which is how often we will connect to your PlanetScale database to download data.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/connection.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=7f8c857355fc78335edf1f5530e113aa" alt="Airbyte - Connection" data-og-width="4498" width="4498" data-og-height="2580" height="2580" data-path="docs/images/assets/docs/integrations/airbyte/connection.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/connection.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=6ecea76451147ff86116041d09fb54fd 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/connection.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=00a123c6e1acf6eed73f694c01324520 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/connection.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=2a85a04a3f67500aa4ee4827e7c33162 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/connection.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=6c6519255a181eb101f5d671ba986e18 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/connection.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=8f716411648dc8d6e344356a9f1dacad 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/connection.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=9b0db125a4d92fbb9a216b61e0172403 2500w" />
    </Frame>
  </Step>

  <Step>
    Click "**Finish and sync**".
  </Step>
</Steps>

Everything is now configured to pull your PlanetScale data into Airbyte and sync it to the selected destination on the schedule you chose. To run the connection, click "**Connections**" > "**Launch**".

## Handling schema changes

Airbyte will not automatically detect when you make schema changes to your PlanetScale database. If you drop a column, your sync should throw an error as it looks for a column that doesn't exist. However, if you add a column, the sync will continue without any errors. Airbyte will be unaware of the new column altogether. This is known as schema drift.

Whenever you perform a schema change, you need to notify Airbyte of it:

<Steps>
  <Step>
    In the Airbyte dashboard, click "**Connections**", select the connection, then navigate to the "**Schema**" tab.
  </Step>

  <Step>
    Click "**Refresh source schema**".
  </Step>

  <Step>
    Click "**Save changes**". Keep in mind, this might delete all data for the connection and start a new sync from scratch.
  </Step>
</Steps>

## Stopping Airbyte

At any point, you can disable any incremental or full syncs by going to the 'Connection' settings page and clicking 'Delete this connection'. This will not touch any of the source or destination data, but will prevent Airbyte from doing any further operations.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/delete.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=709a4f7998328d6720fb7fe8f84254be" alt="Airbyte - PlanetScale disconnection" data-og-width="2428" width="2428" data-og-height="1288" height="1288" data-path="docs/images/assets/docs/integrations/airbyte/delete.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/delete.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=27e4d0c7a87604c466f70977b71f0b64 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/delete.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=2e1847c2ba136082c014e78c5f5060b5 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/delete.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=da39b6b2f9d3c2bb576ece90d0d3c4cc 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/delete.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=558138c0c9d44171f93e777d6ea23ab2 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/delete.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=35de7064cf934e4b7d0aa95118d3382e 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/airbyte/delete.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=54be9992cd4a4112b9688eeb5a778f89 2500w" />
</Frame>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Cloudflare Workers database integration
Source: https://planetscale.com/docs/vitess/integrations/cloudflare-workers



## Introduction

[Cloudflare Workers database integration](https://developers.cloudflare.com/workers/learning/integrations/databases/#planetscale) is designed to connect your Cloudflare Workers to data sources automatically by generating connection strings and storing them in the worker's secrets.

This article will utilize a sample repository that is a preconfigured Cloudflare Worker you can use to deploy to your Cloudflare account.

## Prerequisites

* [NodeJS](https://nodejs.org) installed
* A [PlanetScale account](https://auth.planetscale.com/sign-up)
* The [PlanetScale CLI](https://github.com/planetscale/cli)
* A [Cloudflare account](https://www.cloudflare.com)

## Set up the database

<Steps>
  <Step>
    Create a database in your PlanetScale account named `bookings_db`.

    ```bash  theme={null}
    pscale database create bookings_db
    ```
  </Step>

  <Step>
    Connect to the `main` branch of the new database.

    ```bash  theme={null}
    pscale shell bookings_db main
    ```
  </Step>

  <Step>
    Run the following commands to create a table in the database and populate it with some data.

    ```sql  theme={null}
    CREATE TABLE hotels (
      id INT UNSIGNED PRIMARY KEY AUTO_INCREMENT,
      name VARCHAR(50) NOT NULL,
      address VARCHAR(50) NOT NULL,
      stars FLOAT(2) UNSIGNED
    );

    INSERT INTO hotels (name, address, stars) VALUES
      ('Hotel California', '1967 Can Never Leave Ln, San Fancisco CA, 94016', 7.6),
      ('The Galt House', '140 N Fourth St, Louisville, KY 40202', 8.0);
    ```
  </Step>
</Steps>

## Deploy the Cloudflare Worker

<Steps>
  <Step>
    Clone the sample repository.

    ```bash  theme={null}
    git clone https://github.com/planetscale/cloudflare-workers-quickstart.git
    ```
  </Step>

  <Step>
    Navigate to the `worker` folder of the repository and install the dependencies.

    ```bash  theme={null}
    cd cloudflare-workers-quickstart/worker
    npm install
    ```
  </Step>

  <Step>
    Deploy the Worker to your Cloudflare account.

    ```bash  theme={null}
    npx wrangler publish
    ```
  </Step>
</Steps>

## Configure the Cloudflare PlanetScale integration

<Steps>
  <Step>
    Log into the Cloudflare dashboard and navigate to **"Workers"** > **"Overview"**. You should see a service in the list named **"planetscale-worker"**. Select it from the list.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.52.48.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=e09de5f7e8e98c67842fd0ef10c72315" alt="PlanetScale Cloudflare integration wizard - step 1" data-og-width="1760" width="1760" data-og-height="903" height="903" data-path="docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.52.48.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.52.48.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=9e82d236471d950865b0ddefa84b2ef6 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.52.48.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=0b4c709602f247d8c6f9c9c53dd4d4c7 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.52.48.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=807190b415c630258900e2f94b05407b 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.52.48.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=cba0647d673ed32e6ef960d9f31bd783 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.52.48.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=6cc676932997cf48d019c6b7edaccfc6 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.52.48.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=7eaa2a858225ca039b7afaa610fdc67c 2500w" />
    </Frame>
  </Step>

  <Step>
    Select the **"Settings"** tab, then **"Integrations"**, and finally **"Add Integration"** in the PlanetScale card.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.51.19.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=c3414a459fa818eedadd1d922c5517f2" alt="PlanetScale Cloudflare integration wizard - step 2" data-og-width="1758" width="1758" data-og-height="901" height="901" data-path="docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.51.19.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.51.19.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=7f801daa32a939f8b67674c22343ba18 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.51.19.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=016647eb7a825d1643ec98bcb4c6463d 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.51.19.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=22e0a31442f969e1c9fe1c89fb5c75df 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.51.19.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=217eecee3a12613db8e513785aa284da 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.51.19.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=2bf3f34b10a977fa0259e2ddd03583c8 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.51.19.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=d5d035320351e206ba29636c56967796 2500w" />
    </Frame>
  </Step>

  <Step>
    Click **"Accept"** under **Review and grant permissions** to allow the wizard to write the database connection details to the Worker secrets.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.55.06.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=e6c13f25e08ea22dae40c66076ba35c6" alt="PlanetScale Cloudflare integration wizard - step 3" data-og-width="1474" width="1474" data-og-height="1011" height="1011" data-path="docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.55.06.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.55.06.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=9ee1a368214784e5e6e0a12c27872c41 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.55.06.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=b52bf6e6a50c556da6b51f8d3b35044c 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.55.06.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=b07f79dcd6fa950dbc735226bcd55f75 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.55.06.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=2c1992f7cf524ebfee17c0270670d5ed 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.55.06.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=7e7d81b47a9ff64b82096beeb1cc8805 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.55.06.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=a70a2f20732738d0504962d14eddd714 2500w" />
    </Frame>
  </Step>

  <Step>
    Under **Connect to PlanetScale**, click **"Connect"** to start the process of connecting your PlanetScale and Cloudflare accounts.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.56.11.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=f2bee6ceff513f228261d8bb2bf575d0" alt="PlanetScale Cloudflare integration wizard - step 4" data-og-width="1467" width="1467" data-og-height="801" height="801" data-path="docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.56.11.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.56.11.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=90ae58d9dbbb40907c1277400b84af9b 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.56.11.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=c5d3113f80618170ab0b3533da4cbe1b 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.56.11.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=ded7904da2a3d12bd727f9f66818b2af 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.56.11.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=b51e5b3846bd7bfc582717255410dbc5 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.56.11.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=1849489e699fed6acf31ee4286bb441c 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.56.11.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=94ddd8b631a271ccc6fca41d34a02d5f 2500w" />
    </Frame>
  </Step>

  <Step>
    A modal will appear allowing you to grant access to your organization, database, and branch. Start by selecting your organization from the list. This demonstration uses an organization named “ps-deved”.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.59.44.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=945738456262d053fd1eaafe363b8bb4" alt="PlanetScale Cloudflare integration wizard - step 5" data-og-width="748" width="748" data-og-height="883" height="883" data-path="docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.59.44.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.59.44.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=a23568968d710918c8071d769d7bff40 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.59.44.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=4963680e4bdf3a780c0c8f0cc89a39f2 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.59.44.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=4375a4a56f1b1a69cfe8d75e9f7895f3 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.59.44.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=f3af118b8fb20d61b8e5304a605841a9 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.59.44.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=2b46d1c5bfa99604a8bef2811fb77cd4 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.59.44.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=2a748ef8a2b9c55110fda4713c1f7dac 2500w" />
    </Frame>
  </Step>

  <Step>
    Select the “bookings\_db” database from the list in the **Databases** card, and the “main” branch from the list in the **Branches** card. Finally, click **"Authorize access"**.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.00.34.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=d4008ffa166bc2f36c97360b2ee0c7be" alt="PlanetScale Cloudflare integration wizard - step 6" data-og-width="759" width="759" data-og-height="824" height="824" data-path="docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.00.34.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.00.34.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=f976c879e7f3c576bb0cb0dae6ca8f42 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.00.34.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=b770c9bd360aa8693e517c536e828371 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.00.34.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=f729e75b9db77d391687d471d7b94c0b 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.00.34.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=398fca821d8cd31958f6e6ff65a3dbfd 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.00.34.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=cb65a80d695c4831410a54d55ba02595 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.00.34.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=31bb75c37b752a6ad8ed70dea58e95d4 2500w" />
    </Frame>
  </Step>

  <Step>
    Select your organization again from the list and click **"Continue"**.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.01.32.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=e184b3f8c80b32d42421f27c59f44b82" alt="PlanetScale Cloudflare integration wizard - step 7" data-og-width="1383" width="1383" data-og-height="960" height="960" data-path="docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.01.32.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.01.32.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=1cd7184b03261d7470af344ca055c395 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.01.32.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=f699f7641313c9b626bc0695322eda03 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.01.32.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=31cac4bc09fe6ab27fd656535c2f7826 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.01.32.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=ae9ee322013959014789a517773233d4 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.01.32.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=fc70027807521177c9a2ff736d2b1a90 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.01.32.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=286310a2226098dbc616702b8b76af2f 2500w" />
    </Frame>
  </Step>

  <Step>
    Select your database and the [user role](/docs/vitess/security/password-roles) you want the integration to have.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.02.29.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=75a3f9ca4131241956cdb3fef2a1a9b3" alt="PlanetScale Cloudflare integration wizard - step 8" data-og-width="1402" width="1402" data-og-height="907" height="907" data-path="docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.02.29.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.02.29.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=b4b69fde8db00d0faddcb068da3f9604 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.02.29.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=05f21b792e07a0ecbee3f0f1d4b666e3 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.02.29.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=4136bcbe4c486ccb4604a6d6254949c1 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.02.29.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=d21b05ba311f0803bd65dbdab3f44ea3 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.02.29.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=7fcc9e4b96c6042d656a371719dd8bab 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.02.29.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=e5dab734feb249aea958762ff55638b3 2500w" />
    </Frame>
  </Step>

  <Step>
    Select the “main” branch from the list and click **"Continue"**.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.03.07.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=3953c32e12c0ca5d384c5796e3c64208" alt="PlanetScale Cloudflare integration wizard - step 9" data-og-width="1406" width="1406" data-og-height="754" height="754" data-path="docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.03.07.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.03.07.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=9ddb6c3c065cb5433c357dbcc6e740fb 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.03.07.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=15e092e97fcee20c60efac6f66ff0938 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.03.07.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=66a5ebfb214cfb09fce78d95a1e8b247 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.03.07.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=7af23600587fc9f3d9fe1c528c34b193 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.03.07.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=57f8b0a52a96538f76113d7d19581034 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.03.07.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=f61718a70e12c4ff50e8b1856ecf2c20 2500w" />
    </Frame>
  </Step>

  <Step>
    You’ll be given the option to rename the secrets that will be configured on your behalf. These can be left as is. Click **"Add Integration"** to complete the process.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.33.05.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=e493ffc599cda78930738030eb5ff2d4" alt="PlanetScale Cloudflare integration wizard - step 10" data-og-width="1441" width="1441" data-og-height="681" height="681" data-path="docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.33.05.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.33.05.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=ada2f8018465e4a3d0a4f404d4f80999 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.33.05.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=55388da6408fc4baf106b989bcceb19c 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.33.05.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=cb870130ce4c96c29a5241e45a9691b4 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.33.05.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=10eb66b595928771b35967bf3595c2d3 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.33.05.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=859b47970a4b02dad289a4202d3f85af 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.33.05.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=026533cc1d16951e861b6736b770ba33 2500w" />
    </Frame>
  </Step>
</Steps>

## Test the integration

Back in the overview of the Worker, there is a preview URL that you can use to open a new tab in your browser that runs the Worker and displays the results. Once you’ve located the preview URL, click it to test the Worker.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.03.41.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=7d06084c3df1da6de44710146a652871" alt="Cloudflare Worker preview URL in the dashboard" data-og-width="1436" width="1436" data-og-height="546" height="546" data-path="docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.03.41.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.03.41.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=49fe3df15b50f25358221e182bd2d492 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.03.41.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=746e7901b61b9346d6827e59ec2b25cb 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.03.41.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=b6ccddd8b3184f804665a4f385ad4351 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.03.41.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=2c4e0edd7560939c1eaf7cf9eefe343f 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.03.41.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=fc4aae1fd497db3af8f963de47b9d306 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.03.41.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=0e5c28355077c1e17862d2de8afe11a9 2500w" />
</Frame>

Once the integration is configured, you can also run the project on your computer using:

```bash  theme={null}
npx wrangler dev
```

This will automatically use the secrets defined in Cloudflare to run the Worker on your computer.

### Test other database operations (optional)

To test other database operations that are mapped to HTTP methods, you may use the provided `tests.http` file which is designed to work with the [VSCode REST client plugin](https://marketplace.visualstudio.com/items?itemName=humao.rest-client). The file is preconfigured to work with the local environment, or you can change the `@host` variable to match the URL provided in the Cloudflare dashboard that cooresponds with your Worker project.

| Method      | Operation                 |
| :---------- | :------------------------ |
| GET /       | Get a list of all hotels. |
| POST /      | Create a hotel.           |
| PUT /:id    | Update a hotel.           |
| DELETE /:id | Delete a hotel.           |

## What's next?

Once you're done with development, it is highly recommended that [safe migrations](/docs/vitess/schema-changes/safe-migrations) be turned on for your `main` production branch to protect from accidental schema changes and enable zero-downtime deployments.

When you're ready to make more schema changes, you'll [create a new branch](/docs/vitess/schema-changes/branching) off of your production branch. Branching your database creates an isolated copy of your production schema so that you can easily test schema changes in development. Once you're happy with the changes, you'll open a [deploy request](/docs/vitess/schema-changes/deploy-requests). This will generate a diff showing the changes that will be deployed, making it easy for your team to review.

Learn more about how PlanetScale allows you to make [non-blocking schema changes](/docs/vitess/schema-changes) to your database tables without locking or causing downtime for production databases.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Datadog integration
Source: https://planetscale.com/docs/vitess/integrations/datadog

PlanetScale can push metrics to Datadog to assist your team with understanding your database usage and performance.

<Warning>
  This Datadog integration is no longer receiving updates or new metric additions. You should instead use our [Datadog Agent Integration](/docs/vitess/tutorials/prometheus-metrics-datadog), which provides more metrics than this native integration.

  If you have any questions about migrating to the Datadog Agent Integration, [reach out to support](https://planetscale.com/contact?initial=support).
</Warning>

## Prerequisites

* A [Datadog](https://www.datadoghq.com/) account

## Configuring the Datadog integration

<Steps>
  <Step>
    In Datadog, install the [PlanetScale integration](https://app.datadoghq.com/account/settings#integrations/planetscale).
  </Step>

  <Step>
    Create a Datadog API key in your [Datadog Organization Settings](https://app.datadoghq.com/organization-settings/api-keys) and copy the key.
  </Step>

  <Step>
    In PlanetScale, go to your organization's [Integrations settings](https://app.planetscale.com/settings/integrations), and select **Configure** for the Datadog integration. Paste your Datadog API key into the field.
  </Step>
</Steps>

Once complete, a "PlanetScale" dashboard will be available with incoming metrics from PlanetScale.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/datadog/dashboard.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=82eface6b002bf541dfbdfd3872fcb5f" alt="PlanetScale Default Dashboard in Datadog" data-og-width="2728" width="2728" data-og-height="1988" height="1988" data-path="docs/images/assets/docs/integrations/datadog/dashboard.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/datadog/dashboard.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=854a1bf1a8734829cbef18fb81e9d792 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/datadog/dashboard.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=168cdb12f985df9a270efb4f30f37bb6 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/datadog/dashboard.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=c0ddbfa415ac9c1e0171056e83d7cb7a 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/datadog/dashboard.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=893cd14f67d8e83c73532e8626137a0b 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/datadog/dashboard.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=c56a588d6fbf265d0ba56b864e1f6b61 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/datadog/dashboard.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=e7352e48985d34ec6cad22cfb7a6f507 2500w" />
</Frame>

## Metrics We Collect

Once configured, PlanetScale collects the following metrics from every branch in your organization.

| **Metric name**                            | **Metric type** | **Description**                                                                                     |
| :----------------------------------------- | :-------------- | :-------------------------------------------------------------------------------------------------- |
| planetscale.connections                    | gauge           | Number of active connections to a database branch. *Shown as connection.*                           |
| planetscale.primary.cpu\_usage             | gauge           | Percentage of CPU utilized on a database branch's primary. *Shown as percent.*                      |
| planetscale.primary.memory\_usage          | gauge           | Percentage of memory utilized on a database branch's primary. *Shown as percent.*                   |
| planetscale.queries.latency                | gauge           | Query times in the p50, p95, p99 and p999 percentiles. *Shown as millisecond.*                      |
| planetscale.replication\_lag               | gauge           | Replication lag in seconds between a database branch's primary and each replica. *Shown as second.* |
| planetscale.rows\_read                     | count           | Number of rows read from a database branch. *Shown as row.*                                         |
| planetscale.rows\_written                  | count           | Number of rows written to a database branch. *Shown as row.*                                        |
| planetscale.tables.cumulative\_query\_time | count           | Cumulative active query time in a database branch, by table and statement. *Shown as nanosecond.*   |
| planetscale.tables.queries                 | count           | Number of queries issued to a database branch, by table and statement. *Shown as query.*            |
| planetscale.tables.rows\_deleted           | count           | Number of rows deleted from a database branch, by table. *Shown as row.*                            |
| planetscale.tables.rows\_inserted          | count           | Number of rows inserted into a database branch, by table. *Shown as row.*                           |
| planetscale.tables.rows\_selected          | count           | Number of rows selected in a database branch, by table. *Shown as row.*                             |
| planetscale.tables.rows\_updated           | count           | Number of rows updated in a database branch, by table. *Shown as row.*                              |
| planetscale.tables.storage                 | gauge           | Total bytes stored in a database branch, by table. *Shown as byte.*                                 |
| planetscale.vtgate.errors                  | count           | Number of errors encountered by a database branch's vtgate. *Shown as error.*                       |
| planetscale.vttablet.mem\_util.max         | gauge           | Maximum memory utilization of a database branch's vttablet. *Shown as percent.*                     |
| planetscale.vttablet.mem\_util.avg         | gauge           | Average memory utilization of a database branch's vttablet. *Shown as percent.*                     |
| planetscale.vttablet.iops                  | gauge           | Number of IOPS performed by a database branch's vttablet. *Shown as operation.*                     |

## Billing

The Datadog integration is available on all of our [paid plans](https://planetscale.com/pricing).

## Frequently asked questions

### How do I track replication lag in Datadog?

You can use the following formula to set alerts for replication lag:

```bash  theme={null}
(max:planetscale.replication_lag{ps_database:<DATABASE_NAME> ps_tablet_type:replica, ps_branch:<MAIN>})
```

Make sure you replace `<DATABASE_NAME>` with your PlanetScale database name and `<MAIN>` with the name of the branch for which you'd like to track replication lag.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Debezium connector for PlanetScale
Source: https://planetscale.com/docs/vitess/integrations/debezium

The Debezium connector for PlanetScale is a connector for [Debezium](https://debezium.io/), an open-source distributed platform for change data capture.

The [Debezium Connector for PlanetScale](https://github.com/planetscale/debezium-connector-planetscale?tab=readme-ov-file) is a fork of the 2.4.x release of the [Debezium connector for Vitess](https://debezium.io/documentation/reference/connectors/vitess.html).

This documentation shows you how to set up the Debezium connector for PlanetScale. This will allow you to get the `debezium-server` running on your machine, connect to PlanetScale, and send messages to a webhook endpoint.

## Install Java

1. First, you’ll need the Java Development SDK installed on your machine. You can find this at [https://www.oracle.com/java/technologies/downloads/#jdk22-mac](https://www.oracle.com/java/technologies/downloads/#jdk22-mac).

## Running standalone in Debezium Server

[Debezium Server](https://debezium.io/documentation/docs/api/reference/stable/operations/debezium-server.html) is a standalone application that can test a Debezium connector end-to-end by hosting the Debezium core as an in-process library and pass data from the source to the sink.

<Steps>
  <Step>
    Download Debezium Server from the [distribution link](https://repo1.maven.org/maven2/io/debezium/debezium-server-dist/2.4.1.Final/debezium-server-dist-2.4.1.Final.tar.gz).
  </Step>

  <Step>
    Create a directory on your machine where you want to run it.
  </Step>

  <Step>
    Move the download to that directory.
  </Step>

  <Step>
    Extract it by running `tar -xvf <path to file>`
  </Step>

  <Step>
    You’ll now have a `debezium-server` directory.
  </Step>

  <Step>
    `cd debezium-server`
  </Step>

  <Step>
    Create a `data` folder.
  </Step>

  <Step>
    Download the JAR with dependencies for the `debezium-vitess-planetscale` from [GitHub](https://github.com/planetscale/debezium-connector-planetscale/releases/download/v2.4.0.Final.PS20241031.1/debezium-connector-planetscale-2.4.0.Final-jar-with-dependencies.jar), and place in `lib/`.
  </Step>
</Steps>

### Configure the Debezium connector for PlanetScale

Create a file `conf/application.properties`. This is where your config will go.

In this example config, we are going to have the sink send HTTP requests to `webhook.site`.

Go to [`http://webhook.site`](http://webhook.site) to get your own endpoint.

Place the sample config below in `conf/application.properties`, replacing the following placeholders:

* `<webhook>` with your webhook.site endpoint.
* `<planetscale-database-name>` with your PlanetScale database name.
* `<planetscale-hostname>` with your PlanetScale connection string hostname.
* `<planetscale-username>` with your PlanetScale connection string username.
* `<planetscale-password>` with your PlanetScale connection string password.

```java  theme={null}
debezium.sink.type=http
quarkus.log.level=DEBUG
debezium.format.value=json
debezium.sink.http.url=<webhook>
log4j.logger.io.debezium.relational.history=DEBUG, stdout
debezium.source.offset.storage.file.filename=data/offsets.dat
debezium.source.offset.flush.interval.ms=0
debezium.source.schema.history.internal=io.debezium.storage.file.history.FileSchemaHistory
debezium.source.schema.history.internal.file.filename=data/schema_history.dat
debezium.source.connector.class=io.debezium.connector.planetscale.PlanetScaleConnector
debezium.source.vitess.keyspace=<planetscale-database-name>
debezium.source.vitess.tablet.type=MASTER
debezium.source.database.hostname=<planetscale-hostname>
debezium.source.database.port=443
debezium.source.database.user=<planetscale-username>
debezium.source.database.password=<planetscale-password>
debezium.source.topic.prefix=connector-test
```

## Run it

Once the config is set, you can start it by running `./run.sh`

Any existing rows in any table of `<planetscale-database-name>` will show up as events in your `webhook.site` endpoint. Adding/modifying/deleting rows will also show up as events in your endpoint.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Fivetran integration
Source: https://planetscale.com/docs/vitess/integrations/fivetran

With PlanetScale Connect, you can extract data from your PlanetScale database and safely load it into other destinations for analysis, transformation, and more.

We implemented a [Fivetran](https://fivetran.com/) connector as the pipeline between your PlanetScale source and selected destination. This document will walk you through connecting your PlanetScale database to Fivetran.

## Connect to Fivetran

### Sign up for the PlanetScale connector private preview

Currently, the PlanetScale connector is in private preview mode in Fivetran. To get access:

<Steps>
  <Step>
    Log into your [Fivetran dashboard](https://fivetran.com/dashboard/)
  </Step>

  <Step>
    Click on "**Add connector**" and set up your destination (if needed)
  </Step>

  <Step>
    On the "**Browse our data sources**" page, type `planetscale` in the search box
  </Step>

  <Step>
    Click on the "**Contact us**" button in the `PlanetScale` search result
  </Step>

  <Step>
    Fill out the form and submit
  </Step>
</Steps>

The Fivetran team will reach out to grant you access to the private preview.

### Requirements

* A PlanetScale database
* A Fivetran account
* Access to the private preview of the PlanetScale connector in Fivetran

#### Connect using private networking

<Note>
  You must be using the [PlanetScale Enterprise single-tenant deployment option](/docs/plans/deployment-options#single-tenancy-deployment-on-planetscale) and on a [Fivetran Business Critical plan](https://resources.fivetran.com/datasheets/fivetran-business-critical-product-overview) to use private networking.
</Note>

If you are using private networking and not connecting directly to your PlanetScale database, the Fivetran integration supports AWS PrivateLink and GCP Private Service Connect. See the [Fivetran AWS Private Link setup guide](https://fivetran.com/docs/databases/connection-options#awsprivatelink) or [Fivetran GCP Private Service Connect setup guide](https://fivetran.com/docs/databases/connection-options#googlecloudprivateserviceconnect) for details.

### Set up Fivetran

<Steps>
  <Step>
    In the [Fivetran dashboard](https://fivetran.com/dashboard/), click on "**Add connector**," set up your destination (if needed), and search for `planetscale` on the “**Browse our data sources**” page. Once selected, you should see the PlanetScale connector settings page.
  </Step>

  <Step>
    In [PlanetScale](https://app.planetscale.com), navigate to the database you want to connect to Fivetran and click the "**Connect**" button.
  </Step>

  <Step>
    Create a new password for your main branch with [read-only permissions](/docs/vitess/security/password-roles#overview).
  </Step>

  <Step>
    Select "**General**" from the "**Connect with**" dropdown and leave this tab open, as you'll need to copy these credentials shortly.
  </Step>

  <Step>
    Back in Fivetran, in your [connector setup form](https://fivetran.com/docs/getting-started/fivetran-dashboard/connectors#addanewconnector), enter the connector values as follows:

    * **Destination schema**: This prefix applies to each replicated schema and cannot be changed once your connector is created. Note: Each replicated schema is appended with `_planetscale` at the end of your chosen name.
    * **Database host name**: Paste in the copied value for `host`
    * **Database name**: Paste in the copied value for `database`
    * **Database username**: Paste in the copied value for `username`
    * **Database password**: Paste in the copied value for `password`
    * **Comma-separated list of shards to sync (optional)**: If your PlanetScale database is *not* sharded, ignore this field. If the database is sharded, by default, the PlanetScale connector will download rows from all shards in the database. To pick which shards are synced by the connector, you can optionally provide a comma-separated list of shards in the connector configuration.
    * **Use replica?**: In PlanetScale, VStream will connect to the primary tablet for your database, which also serves queries to your database. To lessen the load on the primary tablet, set this to `true` to make Vstream read from a replica of your database.
    * **Treat tinyint(1) as boolean (optional)**: You can choose to have the connector transform tinyint(1) type columns in your database to either `true` or `false`.
    * **Fivetran IPs (optional)**: If your connection string was created with [IP restrictions](/docs/vitess/connecting/connection-strings#ip-restrictions), ensure that the [Fivetran IP ranges](https://fivetran.com/docs/using-fivetran/ips) are added to the password.
  </Step>

  <Step>
    Click "**Save & Test**". Fivetran tests and validates our connection to your PlanetScale database. Upon successfully completing the setup tests, you can sync your data using Fivetran.
  </Step>
</Steps>

## Sync overview

Once Fivetran is connected to your PlanetScale primary or read replica, we pull a complete dump of all selected data from your database. The connector then connects to your database's [VStream](https://vitess.io/docs/concepts/vstream/) to pull all your new and changed data at regular intervals. VStream is Vitess' change tracking mechanism that is underneath every PlanetScale database. If data in the source changes (for example, you add new tables or change a data type), the connector automatically detects and persists these changes into your destination.

### Syncing empty tables and columns

Fivetran can sync empty tables and columns for your PlanetScale connector. For more information, see the [Fivetran features documentation](https://fivetran.com/docs/getting-started/features#syncingemptytablesandcolumns).

## Schema information

Fivetran tries to replicate the exact schema and tables from your PlanetScale source database to your destination according to our [standard database update strategies](https://fivetran.com/docs/databases#transformationandmappingoverview). For every schema in the PlanetScale database you connect, we create a schema in your destination that maps directly to its native schema. This ensures that the data in your destination is in a familiar format to work with.

### Fivetran-generated columns

Fivetran adds the following columns to every table in your destination:

* `_fivetran_deleted` (BOOLEAN) marks deleted rows in the source database.
* `_fivetran_synced` (UTC TIMESTAMP) indicates when Fivetran last successfully synced the row.
* `_fivetran_index` (INTEGER) shows the order of updates for tables that do not have a primary key.
* `_fivetran_id` (STRING) is the hash of the non-Fivetran values of each row. It's a unique ID that Fivetran uses to avoid duplicate rows in tables that do not have a primary key.

We add these columns to give you insight into the state of your data and the progress of your data syncs.

### Type transformations and mapping

As the connector extracts your data, the connector matches MySQL data types in your PlanetScale database to types that Fivetran supports. If the connector doesn't support a specific data type, the connector automatically changes that type to the closest supported type or, for some types, does not load that data at all. Our system automatically skips columns with data types we do not accept or transform.

The following table illustrates how we transform your MySQL data types into Fivetran-supported types:

|     MySQL Type     | Fivetran Data Type | Fivetran Supported |                                                                                                                              Notes                                                                                                                             |
| :----------------: | :----------------: | :----------------: | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
|       BINARY       |       BINARY       |        True        |                                                                                                                                                                                                                                                                |
|       BIGINT       |        LONG        |        True        |                                                                                                                                                                                                                                                                |
|         BIT        |       BOOLEAN      |        True        |                                                                                                           BIT type with a single digit is supported.                                                                                                           |
|        BLOB        |       BINARY       |        True        |                                                                                                                                                                                                                                                                |
|        CHAR        |       STRING       |        True        |                                                                                                                                                                                                                                                                |
|        DATE        |        DATE        |        True        |                                                                                          Invalid values will be loaded as NULL or EPOCH if the type is a primary key.                                                                                          |
|      DATETIME      |   TIMESTAMP\_NTZ   |        True        |                                                                                          Invalid values will be loaded as NULL or EPOCH if the type is a primary key.                                                                                          |
|  DECIMAL/ NUMERIC  |     BIGDECIMAL     |        True        |                                                                                                                                                                                                                                                                |
|       DOUBLE       |       DOUBLE       |        True        |                                                                                                                                                                                                                                                                |
|        ENUM        |       STRING       |        True        |                                                                                                                                                                                                                                                                |
|        FLOAT       |       DOUBLE       |        True        |                                                                                                                                                                                                                                                                |
|      GEOMETRY      |        JSON        |        True        |                                                                                                                                                                                                                                                                |
| GEOMETRYCOLLECTION |        JSON        |        True        |                                                                                                                                                                                                                                                                |
|        JSON        |        JSON        |        True        |                                                                                                                                                                                                                                                                |
|         INT        |       INTEGER      |        True        |                                                                                                                                                                                                                                                                |
|     LINESTRING     |        JSON        |        True        |                                                                                                                                                                                                                                                                |
|      LONGBLOB      |       BINARY       |        True        |                                                                                                                                                                                                                                                                |
|      LONGTEXT      |       STRING       |        True        |                                                                                                                                                                                                                                                                |
|     MEDIUMBLOB     |       BINARY       |        True        |                                                                                                                                                                                                                                                                |
|      MEDIUMINT     |       INTEGER      |        True        |                                                                                                                                                                                                                                                                |
|     MEDIUMTEXT     |       STRING       |        True        |                                                                                                                                                                                                                                                                |
|   MULTILINESTRING  |        JSON        |        True        |                                                                                                                                                                                                                                                                |
|     MULTIPOINT     |        JSON        |        True        |                                                                                                                                                                                                                                                                |
|    MULTIPOLYGON    |        JSON        |        True        |                                                                                                                                                                                                                                                                |
|        POINT       |        JSON        |        True        |                                                                                                                                                                                                                                                                |
|       POLYGON      |        JSON        |        True        |                                                                                                                                                                                                                                                                |
|         SET        |        JSON        |        True        |                                                                                                                                                                                                                                                                |
|      SMALLINT      |       INTEGER      |        True        |                                                                                                                                                                                                                                                                |
|        TIME        |       STRING       |        True        |                                                                                                                                                                                                                                                                |
|      TIMESTAMP     |      TIMESTAMP     |        True        |                                                                       MYSQL always stores timestamps in UTC. Invalid values will be loaded as NULL or EPOCH if the type is a primary key.                                                                      |
|      TINYBLOB      |       BINARY       |        True        |                                                                                                                                                                                                                                                                |
|       TINYINT      |       BOOLEAN      |        True        |                                                If you select `Treat TinyInt(1) as boolean` in the connector configuration, we will enforce that the tinyint is either 1 or 0 and return true/false accordingly.                                                |
|       TINYINT      |       INTEGER      |        True        | In all other cases, the destination type for TINYINT columns will be INTEGER. If the width isn't specified to be exactly 1 (either no specification or a value other than 1), the destination type will be INTEGER, even if the column contains only 1s or 0s. |
|      TINYTEXT      |       STRING       |        True        |                                                                                                                                                                                                                                                                |
|   UNSIGNED BIGINT  |     BIGDECIMAL     |        True        |                                                                                                                                                                                                                                                                |
|    UNSIGNED INT    |        LONG        |        True        |                                                                                                                                                                                                                                                                |
|  UNSIGNED SMALLINT |       INTEGER      |        True        |                                                                                                                                                                                                                                                                |
|       VARCHAR      |       STRING       |        True        |                                                                                                                                                                                                                                                                |
|      VARBINARY     |       BINARY       |        True        |                                                                                                                                                                                                                                                                |
|        YEAR        |       INTEGER      |        True        |                                                                                                                                                                                                                                                                |

<Note>
  If the connector is missing an important data type that you need, please [contact us](https://planetscale.com/contact).
</Note>

In some cases, when loading data into your destination, the connector may need to convert Fivetran data types into data types supported by the destination. For more information, see the [individual data destination pages](https://fivetran.com/docs/destinations).

### Unparsable values

When the connector encounters [an unparsable value](https://fivetran.com/docs/databases/mysql#unparsablevalues) of one of the following data types, the connector substitutes it with a default value. The default value the connector uses depends on whether the unparsable value is in a primary key column or non-primary key column:

| MySQL Type | Primary Key Value    | Non-Primary Key Value |
| :--------- | :------------------- | :-------------------- |
| DATE       | 1970-01-01           | null                  |
| DATETIME   | 1970-01-01T00:00:00  | null                  |
| TIMESTAMP  | 1970-01-01T00:00:00Z | null                  |

Although we may be able to read some values outside the supported DATE, DATETIME, and TIMESTAMP ranges as defined by [MySQL's documentation](https://dev.mysql.com/doc/refman/8.0/en/datetime.html), there is no guarantee. Additionally, the special zero value 0000-00-00 00:00:00 is subject to this rule.

### Excluding source data

If you don’t want to sync all the data from your database, you can exclude schemas, tables, or columns from your syncs on your Fivetran dashboard. To do so, go to your connector details page and uncheck the objects you want to omit from syncing. For more information, see the [Fivetran column Blocking documentation](https://fivetran.com/docs/getting-started/features/column-blocking-hashing).

## Initial sync

When Fivetran connects to a new database, the connector first copies all rows from every table in every schema for which we have `SELECT` permission (except those you have excluded in your Fivetran dashboard) and add [Fivetran-generated columns](https://fivetran.com/docs/databases/mysql#fivetrangeneratedcolumns). Tables are copied in ascending size order (from smallest to largest). The connector copies rows by performing a `SELECT` statement on each table. For large tables, we copy a limited number of rows at a time so that we don't have to start the sync again from the beginning if our connection is lost midway.

The duration of initial syncs can vary depending on the number and size of tables to be imported. We, therefore, interleave incremental updates with the table imports during the initial sync.

## Updating data

Fivetran performs incremental updates of any new or modified data from your source database. The connector uses Vitess's inbuilt VStream VGtids, which allows Fivetran to update only the data that has changed since our last sync.

### Deleted rows

The connector does not delete rows from the destination. It handles deletes as part of streaming changes from VStream. Note: We only process `DELETE` events from the stream.

### Deleted columns

The connector does not delete columns from your destination. When a column is deleted from the source table, it replaces the existing values in the corresponding destination column with `NULL` values.

### Adding and dropping columns

When you add or drop a column, the connector attempts to migrate your destination schema to the new table structure automatically. In some cases, it will be unable to do this and instead perform an automatic re-sync of the changed table.

In the following scenarios, Fivetran will re-sync your table instead of automatically migrating it:

* Changing column order
* Changing primary keys
* Modifying `ENUM` or `SET` columns

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale GitHub Actions
Source: https://planetscale.com/docs/vitess/integrations/github-actions



See our [tech talk on Databases + CI/CD](https://planetscale.com/blog/databases-ci-cd-pipeline) to see pscale + GitHub Actions
used in a real application.

With GitHub Actions, you can automate the creation of branches and deploy requests all within your CI workflow.

<Columns cols={2}>
  <Card title="Getting Started" href="#getting-started" icon="rocket-launch" horizontal />

  <Card title="Authentication" href="#authentication" icon="key" horizontal />

  <Card title="Convert GitHub branch name to PlanetScale branch name" href="#convert-github-branch-name-to-planetscale-branch-name" icon="code-branch" horizontal />

  <Card title="Create a PlanetScale branch" href="#create-a-planetscale-branch" icon="code-branch" horizontal />

  <Card title="Create a password for a branch" href="#create-a-password-for-a-branch" icon="key" horizontal />

  <Card title="Open a deploy request" href="#open-a-deploy-request" icon="code-branch" horizontal />

  <Card title="Get deploy request by branch name" href="#get-deploy-request-by-branch-name" icon="code-branch" horizontal />

  <Card title="Get deploy request diff and comment on pull request" href="#get-deploy-request-diff-and-comment-on-pull-request" icon="code-branch" horizontal />

  <Card title="Check for dropped columns" href="#check-for-dropped-columns" icon="code-branch" horizontal />

  <Card title="Submit a deploy request by branch name" href="#submit-a-deploy-request-by-branch-name" icon="code-branch" horizontal />
</Columns>

## Getting started

The best way to use PlanetScale within GitHub Actions is via the `pscale` CLI.

Use [`planetscale/setup-pscale-action`](https://github.com/planetscale/setup-pscale-action) to make pscale available within your GitHub Actions.

```yaml  theme={null}
- name: Setup pscale
  uses: planetscale/setup-pscale-action@v1
```

The action works with Linux, Windows, and Mac runners. Once installed it will be added to your tool cache for subsequent runs.

## Authentication

Authentication for pscale is via service token environment variables.

You will need to [create a service token](/docs/cli/service-token). Make sure to give your service token the proper permissions to the database you'll be using in your workflow.

Add your `PLANETSCALE_SERVICE_TOKEN_ID` and `PLANETSCALE_SERVICE_TOKEN` to your [Actions secrets](https://docs.github.com/en/actions/security-guides/using-secrets-in-github-actions#creating-secrets-for-a-repository).

In your Actions workflow, you will need to make the secrets available as environment variables.

```yaml  theme={null}
- name: Run pscale command
  env:
    PLANETSCALE_SERVICE_TOKEN_ID: ${{ secrets.PLANETSCALE_SERVICE_TOKEN_ID }}
    PLANETSCALE_SERVICE_TOKEN: ${{ secrets.PLANETSCALE_SERVICE_TOKEN }}
  run: pscale database list
```

## Examples

The following examples show how to accomplish common Actions workflows with PlanetScale. In each example, notice that we use secrets for the service token, database and organization names.
You will need to set them in your GitHub repository to target your own database.

```
PLANETSCALE_ORG_NAME
PLANETSCALE_DATABASE_NAME
PLANETSCALE_SERVICE_TOKEN_ID
PLANETSCALE_SERVICE_TOKEN
```

### Convert GitHub branch name to PlanetScale branch name

PlanetScale branch names must be lowercase, alphanumeric characters and hyphens are allowed.

Since git branch names allow more possibilities, you can use the following code to transform a git branch name into an acceptable PlanetScale branch name.

```yaml  theme={null}
- name: Rename branch name
  run: echo "PSCALE_BRANCH_NAME=$(echo ${{ github.head_ref }} | tr -cd '[:alnum:]-'| tr '[:upper:]' '[:lower:]')" >> $GITHUB_ENV
```

This makes `${{ env.PSCALE_BRANCH_NAME }}` available for use in the rest of the workflow. This is useful to run in any scenario where you are creating
a PlanetScale branch to correspond with a git branch.

### Create a PlanetScale branch

You can use `pscale branch create` to create a branch that matches your GitHub branch name.

```yaml  theme={null}
- name: Create branch
  env:
    PLANETSCALE_SERVICE_TOKEN_ID: ${{ secrets.PLANETSCALE_SERVICE_TOKEN_ID }}
    PLANETSCALE_SERVICE_TOKEN: ${{ secrets.PLANETSCALE_SERVICE_TOKEN }}
  run: |
    set +e
    pscale branch show ${{ secrets.PLANETSCALE_DATABASE_NAME }} ${{ env.PSCALE_BRANCH_NAME }} --org ${{ secrets.PLANETSCALE_ORG_NAME }}
    exit_code=$?
    set -e

    if [ $exit_code -eq 0 ]; then
      echo "Branch exists. Skipping branch creation."
    else
      echo "Branch does not exist. Creating."
      pscale branch create ${{ secrets.PLANETSCALE_DATABASE_NAME }} ${{ env.PSCALE_BRANCH_NAME }} --wait --org ${{ secrets.PLANETSCALE_ORG_NAME }}
    fi
```

Notice that we first check if the branch exists. If it does, we do nothing. Otherwise we create it and pass the `--wait` flag.

This is useful when running in CI, as the workflow may run multiple times and you'll want the branch ready if you are running schema migrations immediately after creating the branch.

### Create a password for a branch

You can use `pscale password create` to generate credentials for your database branch.

```yaml expandable theme={null}
- name: Generate password for branch
  env:
    PLANETSCALE_SERVICE_TOKEN_ID: ${{ secrets.PLANETSCALE_SERVICE_TOKEN_ID }}
    PLANETSCALE_SERVICE_TOKEN: ${{ secrets.PLANETSCALE_SERVICE_TOKEN }}
  run: |
    response=$(pscale password create ${{ secrets.PLANETSCALE_DATABASE_NAME }} ${{ env.PSCALE_BRANCH_NAME }} "" -f json --org ${{ secrets.PLANETSCALE_ORG_NAME }})

    id=$(echo "$response" | jq -r '.id')
    host=$(echo "$response" | jq -r '.access_host_url')
    username=$(echo "$response" | jq -r '.username')
    password=$(echo "$response" | jq -r '.plain_text')
    ssl_mode="verify_identity"  # Assuming a default value for ssl_mode
    ssl_ca="/etc/ssl/certs/ca-certificates.crt"  # Assuming a default value for ssl_ca

    # Set the password ID, allows us to later delete it if wanted.
    echo "PASSWORD_ID=$id" >> $GITHUB_ENV

    # Create the DATABASE_URL
    database_url="mysql://$username:$password@$host/${{ secrets.PLANETSCALE_DATABASE_NAME }}?sslmode=$ssl_mode&sslca=$ssl_ca"
    echo "DATABASE_URL=$database_url" >> $GITHUB_ENV
    echo "::add-mask::$DATABASE_URL"
- name: Use the DATABASE_URL in a subsequent step
  run: |
    echo "Using DATABASE_URL: $DATABASE_URL"
```

This example shows creating the password and getting back a response in json. The json is then parsed to create a `DATABASE_URL` which can be used in later steps.

<Note>
  `pscale password create` can also accept a `ttl` flag which lets you limit the number of minutes the password is valid for.
</Note>

### Open a deploy request

You can use `pscale deploy-request create` to open a new deploy request from GitHub Actions.
This can be useful after running migrations against a branch.

```yaml  theme={null}
- name: Open DR if migrations
  env:
    PLANETSCALE_SERVICE_TOKEN_ID: ${{ secrets.PLANETSCALE_SERVICE_TOKEN_ID }}
    PLANETSCALE_SERVICE_TOKEN: ${{ secrets.PLANETSCALE_SERVICE_TOKEN }}
  run: pscale deploy-request create ${{ secrets.PLANETSCALE_DATABASE_NAME }} ${{ env.PSCALE_BRANCH_NAME }}
```

### Get deploy request by branch name

You can use `pscale deploy-requests show` to grab the latest deploy request by branch name.

This can be useful when deploying your application. You can first check if there are any deploy requests open for the branch being deployed. If there are, you can
trigger the deploy request to run before you deploy your application.

```yaml  theme={null}
- name: Get Deploy Requests
  env:
    PLANETSCALE_SERVICE_TOKEN_ID: ${{ secrets.PLANETSCALE_SERVICE_TOKEN_ID }}
    PLANETSCALE_SERVICE_TOKEN: ${{ secrets.PLANETSCALE_SERVICE_TOKEN }}
  run: |
    deploy_request_number=$(pscale deploy-request show ${{ secrets.PLANETSCALE_DATABASE_NAME }} ${{ env.PSCALE_BRANCH_NAME }} --org ${{ secrets.PLANETSCALE_ORG_NAME }} -f json | jq -r '.number')
    echo "DEPLOY_REQUEST_NUMBER=$deploy_request_number" >> $GITHUB_ENV
```

This example also makes the deploy request number available as an `env` var so that it can be used in later steps.

### Get deploy request diff and comment on pull request

We can use `pscale deploy-request diff` to see the full schema diff of a deploy request.

This example is useful when combined with opening a deploy request for a git branch. You can then automatically comment the diff back to the GitHub pull request.

```yaml  theme={null}
- name: Comment on PR
  env:
    PLANETSCALE_SERVICE_TOKEN_ID: ${{ secrets.PLANETSCALE_SERVICE_TOKEN_ID }}
    PLANETSCALE_SERVICE_TOKEN: ${{ secrets.PLANETSCALE_SERVICE_TOKEN }}
  run: |
    echo "Deploy request opened: https://app.planetscale.com/${{ secrets.PLANETSCALE_ORG_NAME }}/${{ secrets.PLANETSCALE_DATABASE_NAME }}/deploy-requests/${{ env.DEPLOY_REQUEST_NUMBER }}" >> migration-message.txt
    echo "" >> migration-message.txt
    echo "\`\`\`diff" >> migration-message.txt
    pscale deploy-request diff ${{ secrets.PLANETSCALE_DATABASE_NAME }} ${{ env.DEPLOY_REQUEST_NUMBER }} --org ${{ secrets.PLANETSCALE_ORG_NAME }} -f json | jq -r '.[].raw' >> migration-message.txt
    echo "\`\`\`" >> migration-message.txt
- name: Comment PR - db migrated
  uses: thollander/actions-comment-pull-request@v2
  with:
    filePath: migration-message.txt
```

This writes the diff to the `migration-message.txt` file. And then creates a comment on the pull request that triggered the workflow.

### Check for dropped columns

PlanetScale sets a `can_drop_data` boolean for any schema change that drop a column or table. We can make use of this to emit a warning into our pull requests.

In this example, we first wait for the deployment check to be `ready`. During this time, PlanetScale is examining the schema change, verifying that it is safe and
generating the DDL statements to make the change. Once it's done, we then use this information to put a comment on the deploy request with tips on how to deploy it safely.

```yaml expandable theme={null}
- name: Check deployment state
    id: check-state
    env:
      PLANETSCALE_SERVICE_TOKEN_ID: ${{ secrets.PLANETSCALE_SERVICE_TOKEN_ID }}
      PLANETSCALE_SERVICE_TOKEN: ${{ secrets.PLANETSCALE_SERVICE_TOKEN }}
    run: |
      for i in {1..10}; do
        deployment_state=$(pscale deploy-request show ${{ secrets.PLANETSCALE_ORG_NAME }} ${{ env.PSCALE_BRANCH_NAME }} --org ${{ secrets.PLANETSCALE_ORG_NAME }} --format json | jq -r '.deployment_state')
        echo "Deployment State: $deployment_state"

        if [ "$deployment_state" = "ready" ]; then
          echo "Deployment state is ready. Continuing."
          break
        fi

        echo "Deployment state is not ready. Waiting 2 seconds before checking again."
        sleep 2
      done
  - name: Comment PR - db migrated
    if: ${{ env.DR_OPENED }}
    env:
      PLANETSCALE_SERVICE_TOKEN_ID: ${{ secrets.PLANETSCALE_SERVICE_TOKEN_ID }}
      PLANETSCALE_SERVICE_TOKEN: ${{ secrets.PLANETSCALE_SERVICE_TOKEN }}
    run: |
      deploy_data=$(pscale api organizations/${{ secrets.PLANETSCALE_ORG_NAME }}/databases/${{ secrets.PLANETSCALE_DATABASE_NAME }}/deploy-requests/${{ env.DEPLOY_REQUEST_NUMBER }}/deployment --org planetscale)
      can_drop_data=$(echo "$deploy_data" | jq -r '.deploy_operations[] | select(.can_drop_data == true) | .can_drop_data')

      echo "Deploy request opened: https://app.planetscale.com/${{ secrets.PLANETSCALE_ORG_NAME }}/${{ secrets.PLANETSCALE_DATABASE_NAME }}/deploy-requests/${{ env.DEPLOY_REQUEST_NUMBER }}" >> migration-message.txt
      echo "" >> migration-message.txt

      if [ "$can_drop_data" = "true" ]; then
        echo ":rotating_light: You are dropping a column. Before running the migration make sure to do the following:" >> migration-message.txt
        echo "" >> migration-message.txt

        echo "1. [ ] Deploy app changes to ensure the column is no longer being used." >> migration-message.txt
        echo "2. [ ] Once you've verified it's no used, run the deploy request." >> migration-message.txt
        echo "" >> migration-message.txt
      else
        echo "When adding to the schema, the Deploy Request must be run **before** the code is deployed." >> migration-message.txt
        echo "Please ensure your schema changes are compatible with the application code currently running in production." >> migration-message.txt
        echo "" >> migration-message.txt

        echo "1. [ ] Successfully run the Deploy Request" >> migration-message.txt
        echo "2. [ ] Deploy this PR" >> migration-message.txt
        echo "" >> migration-message.txt
      fi

      echo "\`\`\`diff" >> migration-message.txt
      pscale deploy-request diff ${{ secrets.PLANETSCALE_ORG_NAME }} ${{ env.DEPLOY_REQUEST_NUMBER }} -f json | jq -r '.[].raw' >> migration-message.txt
      echo "\`\`\`" >> migration-message.txt
  - name: Comment PR - db migrated
    uses: thollander/actions-comment-pull-request@v2
    if: ${{ env.DR_OPENED }}
    with:
      filePath: migration-message.txt
```

### Submit a deploy request by branch name

To trigger a deploy, we can use `pscale deploy-request deploy`. This command will accept either the deploy request number, or the name of the branch that the deploy request was created from.

When using with GitHub Actions, it's often easier to use the branch name.

The `--wait` flag will let the command run until the deployment is complete. This is important if you want your schema change to run before the next step in your workflow.

```yaml  theme={null}
- name: Deploy schema migrations
  env:
    PLANETSCALE_SERVICE_TOKEN_ID: ${{ secrets.PLANETSCALE_SERVICE_TOKEN_ID }}
    PLANETSCALE_SERVICE_TOKEN: ${{ secrets.PLANETSCALE_SERVICE_TOKEN }}
  run: |
    pscale deploy-request deploy ${{ secrets.PLANETSCALE_DATABASE_NAME }} ${{ env.PSCALE_BRANCH_NAME }} --org ${{ secrets.PLANETSCALE_ORG_NAME }} --wait
```

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Hightouch integration
Source: https://planetscale.com/docs/vitess/integrations/hightouch

Hightouch is a platform that allows you to sync data between systems using a simple and intuitive UI.

## Overview

Hightouch supports PlanetScale databases as a data source and a destination to send data to. This article will provide an overview of setting up a PlanetScale database as a source and destination.

Connecting your PlanetScale database to a Hightouch workflow can be used in a number of different ways, but here are a few ideas to get you started:

* Automatically update customer records in your CRM when they are created or changed in your database.
* Send a notification to your Slack workspace when data is changed in your PlanetScale database.
* Execute asynchronous operations by sending data from your PlanetScale database to a data queue like AWS SQS.
* Automate paid ad targeting, suppression, and conversion uploads by syncing audiences from your database to ad platforms.
* Automatically sync account and user health scores to your customer success platforms.

Before you can proceed, you’ll need to create a set of credentials as covered in our [Connection strings doc](/docs/vitess/connecting/connection-strings). Take note of the hostname, username, password, and database name.

## PlanetScale as a source

If you wish to sync data from your PlanetScale database to another system, you’ll need to configure your PlanetScale database as a source in Hightouch. I will be using a database named `bookings_db` in this demo.

### 1. Create the source

In Hightouch, start by selecting **"Sources"** from the sidebar.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/sources.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=2d60dc3c77ad4ae0b8f6b18397608d60" alt="Sources" data-og-width="1999" width="1999" data-og-height="1327" height="1327" data-path="docs/images/assets/docs/integrations/hightouch/sources.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/sources.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=b74692d0819c92ddcbdce86347d6f26d 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/sources.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=bcb1be0bf22d0c9d96a46eda0985c621 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/sources.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=157f2eb1ea1eb63f803e63caaf7bf3c9 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/sources.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=d86529bc7b169ebd21cea06e8b2e40b5 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/sources.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=1f1164ab8df8ef8385bf7746e7a686f2 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/sources.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=758bb77f7657cb17192c068413e81d88 2500w" />
</Frame>

Click **"Add Source"**.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/hightouch/add-source.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=1c82a88640af94a718096239e36ce740" alt="Add source" data-og-width="1999" width="1999" data-og-height="1327" height="1327" data-path="docs/images/assets/docs/integrations/hightouch/add-source.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/hightouch/add-source.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=2376abaae95c1c48c756dd87684a6ef9 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/hightouch/add-source.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=7b340d19a1a948c7a483b06d7917fa8d 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/hightouch/add-source.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=e0f930f747763a91128e66400151f34a 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/hightouch/add-source.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=fa2ebf926a5a114e5d440ed5e4bcbad8 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/hightouch/add-source.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=1504a98e851b49807484366ab1a7f9f7 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/hightouch/add-source.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=2cfaeaa48cba39968a7c954f86db382f 2500w" />
</Frame>

Select **"PlanetScale"** from the list of supported data sources.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/planetscale-as-a-data-source.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=a26f941c5dc49307ef11c1ada4d4a82f" alt="PlanetScale as a data source" data-og-width="1949" width="1949" data-og-height="1285" height="1285" data-path="docs/images/assets/docs/integrations/hightouch/planetscale-as-a-data-source.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/planetscale-as-a-data-source.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=96d1a33ee8b7f10178a4325cd15b5424 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/planetscale-as-a-data-source.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=1046b96807b0bb4db11a6e6da832d3e1 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/planetscale-as-a-data-source.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=f524daa4e0a90a066e81ba3c74db0ad7 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/planetscale-as-a-data-source.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=c0144ef90860121d39a7ff8cd3e0aeb8 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/planetscale-as-a-data-source.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=18bf5dac518249584fd634e43aad7e8c 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/planetscale-as-a-data-source.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=ed70b202450b72db11f0d6548a4f204c 2500w" />
</Frame>

On the Configure your PlanetScale source, populate Step 1 of the form using a set of credentials generated for your database. The **Port** will always be **3306**.

<Note>
  As a reminder, PlanetScale credentials can be generated by clicking **"Connect"** in the PlanetScale dashboard overview of your database. Review our [Connection strings doc](/docs/vitess/connecting/connection-strings) for more details.
</Note>

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/connect-to-planetscale-data-source.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=dd4b960d0264e89092bc2b90eb3ec9a0" alt="Connect to PlanetScale data source" data-og-width="1406" width="1406" data-og-height="981" height="981" data-path="docs/images/assets/docs/integrations/hightouch/connect-to-planetscale-data-source.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/connect-to-planetscale-data-source.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=758984669632637c611d7a61aed8dd3c 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/connect-to-planetscale-data-source.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=3c755b6cb19a6013742da1f1171e85f8 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/connect-to-planetscale-data-source.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=18309c18fd7338bef9f9e1b2c9c84e37 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/connect-to-planetscale-data-source.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=432965c919fdcffe6083f2a38482c0d8 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/connect-to-planetscale-data-source.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=7cf9b10e6e7bc881bf39f8fa4dcabdd7 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/connect-to-planetscale-data-source.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=1ca2a3ca6278252c8d9fe728bde3fb2d 2500w" />
</Frame>

Scroll down to Step 2, and populate your **User** and **Password** before clicking **"Continue"**.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/connect-to-planetscale-data-source-2.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=f55fad1dd550507e520353779d02d609" alt="Connect to PlanetScale data source 2" data-og-width="1406" width="1406" data-og-height="1011" height="1011" data-path="docs/images/assets/docs/integrations/hightouch/connect-to-planetscale-data-source-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/connect-to-planetscale-data-source-2.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=13de7edb6f2ba44b479f7161af6be4fe 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/connect-to-planetscale-data-source-2.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=fc8e5add7cc0d33939d93b5f9261d841 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/connect-to-planetscale-data-source-2.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=41bab15cff3e469de2cb75391caad85c 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/connect-to-planetscale-data-source-2.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=5c145ead249d256152bda171291442f0 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/connect-to-planetscale-data-source-2.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=e26677c8ee711d0647bda7f79e040e49 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/connect-to-planetscale-data-source-2.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=f8474a934249d07d07930a076e21339a 2500w" />
</Frame>

In the next view, Hightouch will ensure that it can properly connect to your database. Provided it connects successfully, click **"Continue"**.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/validate-connection.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=447c065067c1a9cf85e403d639cf9221" alt="Validate connection" data-og-width="1750" width="1750" data-og-height="1251" height="1251" data-path="docs/images/assets/docs/integrations/hightouch/validate-connection.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/validate-connection.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=aa9d45d60f33f7b97b7c762fc50c47dd 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/validate-connection.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=6987049ac3d3e8196d5afd44fe613533 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/validate-connection.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=ca701938cba07b68d5a0d271a8997fe4 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/validate-connection.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=b39ce759d13917c2a3a30f145d6eaf33 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/validate-connection.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=bb9cb85a947cbb4df280eb096cad5b6b 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/validate-connection.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=b2c2be15dfd4700a62ac1adb9b2bea4b 2500w" />
</Frame>

Finally, name your source and click **"Finish"**.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/finalize-source-settings.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=3e2a755dc892ed80929ae77f3f79516d" alt="Finalize source settings" data-og-width="1693" width="1693" data-og-height="1232" height="1232" data-path="docs/images/assets/docs/integrations/hightouch/finalize-source-settings.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/finalize-source-settings.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=7fe08e3b29feab58acd70ae1d175d53f 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/finalize-source-settings.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=a6665bc88f2d13be9511d3a96d9b2cbd 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/finalize-source-settings.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=6752365fdc83d16196bb67a970a1078c 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/finalize-source-settings.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=eb55d2f24fb6cd91ef779da49aae6b56 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/finalize-source-settings.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=a290577045b28b7ea782649011d1787e 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/finalize-source-settings.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=dcae5926155600d3ea8b1077a9e5d91b 2500w" />
</Frame>

### 2. Create a model

Before you can start syncing data, you need to tell Hightouch how the data looks with a Model. Start by selecting **"Models"** from the navigation.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/create-a-model.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=33055fa91b5975e44e2c5e896940d466" alt="Create a model" data-og-width="2156" width="2156" data-og-height="1322" height="1322" data-path="docs/images/assets/docs/integrations/hightouch/create-a-model.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/create-a-model.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=07d5eee4b6ab6dc405a0f518abef9f01 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/create-a-model.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=314de11cf18707c934c43604afff6a69 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/create-a-model.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=acb245479d9ca4e93221805aa1b39791 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/create-a-model.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=2642d697024aff0faa2c0cb4efc1329c 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/create-a-model.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=f20c98890313c075a3d8ad78c58ed282 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/create-a-model.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=d7cef0a51dc005bf6025741c07d05a02 2500w" />
</Frame>

Select the Source you created in the previous step.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/planetscale-data-source-for-the-model.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=4320e940b6a42c25ee86327b201886d2" alt="PlanetScale data source for the model" data-og-width="1580" width="1580" data-og-height="1327" height="1327" data-path="docs/images/assets/docs/integrations/hightouch/planetscale-data-source-for-the-model.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/planetscale-data-source-for-the-model.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=131faf6247c4af751d110d2e6ce98f26 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/planetscale-data-source-for-the-model.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=459ac2d69a8d418cb2d17d1cad5d8985 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/planetscale-data-source-for-the-model.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=ca858e9eda4f4fae82d4fad501e80a23 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/planetscale-data-source-for-the-model.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=15f586541c2bee91e5636996f55b8505 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/planetscale-data-source-for-the-model.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=c235378db1353b0f9725ff32fc3f4659 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/planetscale-data-source-for-the-model.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=8941fc1f8b76a5169dd7796a26d18061 2500w" />
</Frame>

There are two available methods to model your data from a PlanetScale database:

* SQL query - allows you to write a SQL query that returns the data you want to sync.
* Table selector - allows you to select a table and read every row in it.

I’ll use **"Table selector"** for this demo.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/query-vs-table-selector.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=f6301214512b41b0e4f01d0ae4d7682d" alt="Query vs table selector" data-og-width="2279" width="2279" data-og-height="1322" height="1322" data-path="docs/images/assets/docs/integrations/hightouch/query-vs-table-selector.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/query-vs-table-selector.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=fddc8e855c7c8d50f823adff8b208e84 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/query-vs-table-selector.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=632959b18d20f57717ee77db88a2153f 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/query-vs-table-selector.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=1d883c6a8d566b611b5130d1fc3b2c96 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/query-vs-table-selector.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=436eb7ece7673608dca97eaf015b8efe 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/query-vs-table-selector.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=721542bbe6b823074fb533ba7adba565 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/query-vs-table-selector.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=4d6b5bd327aa3ea05e56fe6fdf9c39b6 2500w" />
</Frame>

Use the search to find the `hotels` table, select it from the list, and click **"Preview results"** to validate the data in that table. Once satisfied with the results, click **"Continue"**.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/new-model-from-hotels-table.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=5d91745221ea860cb95b53bf39de8c3a" alt="New model from hotels table" data-og-width="1830" width="1830" data-og-height="1322" height="1322" data-path="docs/images/assets/docs/integrations/hightouch/new-model-from-hotels-table.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/new-model-from-hotels-table.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=03f0907137f248fea62906e381212fed 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/new-model-from-hotels-table.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=20d7b85d486f44633516e7948b6c26af 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/new-model-from-hotels-table.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=c79fea955afaaec3b69c643f83712692 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/new-model-from-hotels-table.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=e34adee0859addf1903cc747dcebc5ef 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/new-model-from-hotels-table.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=3bc4fdffde6d2f4b93256424364e674d 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/new-model-from-hotels-table.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=5f4eef0537a07aa583ffdf92a8bc5d9c 2500w" />
</Frame>

Finally, name the model “Hotel” and set the Primary key to `id` before clicking **"Finish"**.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/configure-model.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=3e6a08e8f9acae2d34a74a5355e72838" alt="Configure model" data-og-width="1830" width="1830" data-og-height="1322" height="1322" data-path="docs/images/assets/docs/integrations/hightouch/configure-model.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/configure-model.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=a4dd1da3e9f5aa2be9f045a239492168 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/configure-model.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=c54d4b398cba84a91262fc1d8cbaad84 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/configure-model.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=9f3f41ec38b1e96b7d5f5d45b5109c7c 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/configure-model.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=51c3d0023271fe84a6ca556e66d0729a 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/configure-model.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=809655effa20668a041920b13ab550fd 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/configure-model.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=acd3bf39872715eb770b6c4d538b8abf 2500w" />
</Frame>

### 3. Create a sync

Now that the Source and Model are configured, a Sync can be created which will sync data from PlanetScale to another system. For this demo, I’ll be using a spreadsheet in my Google Drive account that I’ve already configured as a destination.

Select **"Syncs"** from the left and click **"Add sync"**.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/create-a-sync.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=a62e2c218a9100a29a185ea683474aaf" alt="Create a sync" data-og-width="2156" width="2156" data-og-height="1322" height="1322" data-path="docs/images/assets/docs/integrations/hightouch/create-a-sync.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/create-a-sync.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=1c5061228cc3a6d6b4246ef6c7776cfc 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/create-a-sync.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=943da78f28b705637f8e109f7fa36bd8 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/create-a-sync.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=9b321ed0377c4cd5d17731155efee991 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/create-a-sync.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=60df51e8ab08e728b281a8e9985e4107 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/create-a-sync.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=001b26e9ee17b735da49be363de09f53 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/create-a-sync.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=c7532e08e6e3b6aafa7b18178a793150 2500w" />
</Frame>

Select the “Hotel” model created in the previous step.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/select-the-hotel-model-for-the-sync.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=cccf197e6181037b6c9b391de1f70c9e" alt="Select the hotel model for the sync" data-og-width="2156" width="2156" data-og-height="1322" height="1322" data-path="docs/images/assets/docs/integrations/hightouch/select-the-hotel-model-for-the-sync.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/select-the-hotel-model-for-the-sync.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=fac832dbcf19485a327f1d61b7f71747 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/select-the-hotel-model-for-the-sync.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=17ef4037d07b31d35523b255fac31abd 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/select-the-hotel-model-for-the-sync.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=8455a0b9012c693d1e80593906cc1d85 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/select-the-hotel-model-for-the-sync.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=720b738041e1894c995b8e7736929530 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/select-the-hotel-model-for-the-sync.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=955fd94acede38649ad13276c78e1dab 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/select-the-hotel-model-for-the-sync.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=79a5e088be3180a1011aab4c4c9fc7de 2500w" />
</Frame>

For my demo, I’ll select my Google Sheets destination.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/the-google-sheets-destination.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=653e38b97a3893776540db12c1d7dec7" alt="The Google Sheets destination" data-og-width="1844" width="1844" data-og-height="1322" height="1322" data-path="docs/images/assets/docs/integrations/hightouch/the-google-sheets-destination.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/the-google-sheets-destination.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=5a90964649285d9610f37b895e3e86d8 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/the-google-sheets-destination.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=44b19c36b49a48d0a0de4e6a34ce919f 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/the-google-sheets-destination.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=7d316ab587c078b605fb9f6a659bbe68 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/the-google-sheets-destination.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=e41309e1aa9360e514a29ef55cb449c4 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/the-google-sheets-destination.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=b7c19825e268f786d2f88f887b55f90d 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/the-google-sheets-destination.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=cda5b5016068120a4096333181a823b9 2500w" />
</Frame>

I’m mirroring data into the default sheet in a Sheets doc named “Hightouch demo” for the Google Sheets settings.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/mirror-data-to-the-google-sheet.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=80d51ee6d524a7cd10a8bc288ba55010" alt="Mirror data to the Google Sheet" data-og-width="1550" width="1550" data-og-height="1280" height="1280" data-path="docs/images/assets/docs/integrations/hightouch/mirror-data-to-the-google-sheet.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/mirror-data-to-the-google-sheet.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=cf59218ad240bf468ff11a8ed1d0c9b2 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/mirror-data-to-the-google-sheet.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=6fec222448116e1d2f1252200356caba 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/mirror-data-to-the-google-sheet.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=9c2008c9179eec1f2e77c52fe15bbc45 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/mirror-data-to-the-google-sheet.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=82af49f42fda93c2aa058a9346dc0bee 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/mirror-data-to-the-google-sheet.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=14d3f4d39a7ada37ca3207b124249c8b 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/mirror-data-to-the-google-sheet.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=a1756439a18f2c87ad3878535b3dd2cd 2500w" />
</Frame>

A sync can be performed using a number of different methods. For the demo, I’ll leave **"Manual"** selected and click **"Finish"**.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/finalize-settings-for-first-sync.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=de094bb1295bd24fdc0abaceb8a29a7f" alt="Finalize settings for first sync" data-og-width="1550" width="1550" data-og-height="1280" height="1280" data-path="docs/images/assets/docs/integrations/hightouch/finalize-settings-for-first-sync.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/finalize-settings-for-first-sync.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=4e496334a8f08077c1f7de222d7698d2 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/finalize-settings-for-first-sync.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=9ef298043c66163939f46ba12194a485 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/finalize-settings-for-first-sync.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=b8a4ee236c024e43be1ee9e368dae70b 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/finalize-settings-for-first-sync.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=b7a86428d909fec15f1996d63cee3fbc 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/finalize-settings-for-first-sync.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=2096cd66278b41956078ba7e05d4810e 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/finalize-settings-for-first-sync.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=640627962dd2f661f7eba6396e647476 2500w" />
</Frame>

Since this is a manual sync, I’ll also need to click **"Run sync"** to execute it.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/manually-run-first-sync.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=38ea4cbc5654d8be8da7fd25cc42148d" alt="Manually run the first sync" data-og-width="1550" width="1550" data-og-height="1280" height="1280" data-path="docs/images/assets/docs/integrations/hightouch/manually-run-first-sync.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/manually-run-first-sync.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=300d6afb5f233a6b829b883c12e42ccf 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/manually-run-first-sync.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=826213212ccaa89c344a294adefeaa28 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/manually-run-first-sync.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=ef728241b43d9418d11b40fa7fc8ded0 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/manually-run-first-sync.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=3d9529c0522db5907f1f3519844aa953 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/manually-run-first-sync.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=2aa4ccedb511815c851a2ef67669ac1f 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/manually-run-first-sync.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=695427fe298120e29e3985af322cae57 2500w" />
</Frame>

### 4. Review the synced data

If I log into my Google Drive account and review that spreadsheet, there are now three rows that exist which match the data from the `hotels` table of my PlanetScale database.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/the-synced-data-in-google-sheets.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=7a7feb0f8082114a011ba470b6c7b752" alt="The synced data in Google Sheets" data-og-width="1550" width="1550" data-og-height="1280" height="1280" data-path="docs/images/assets/docs/integrations/hightouch/the-synced-data-in-google-sheets.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/the-synced-data-in-google-sheets.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=42b92547b318e4acccc807fc1bb0065d 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/the-synced-data-in-google-sheets.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=9763563df123a8905fca055bdb140db1 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/the-synced-data-in-google-sheets.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=17d526a5617c5d461fdde1617043c058 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/the-synced-data-in-google-sheets.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=8eb0792e0fc165c41dafd5b9938dac40 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/the-synced-data-in-google-sheets.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=cd36f5da3f7f0a83ad0327b57d898a88 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/the-synced-data-in-google-sheets.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=0a600d1299083f3f063739d11ee79583 2500w" />
</Frame>

## PlanetScale as a destination

Hightouch can also be configured to send data to a PlanetScale database. In this demo, I’ll perform the inverse of the above demo and configure the `new hotels` sheet from my Google Sheets spreadsheet to add new rows to my PlanetScale database.

Here is that sheet with a single new hotel added to it.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/the-new-hotels-sheet-in-google-sheets.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=4c3c50da4af937565fa6aa81ad399285" alt="The new hotels sheet in Google Sheets" data-og-width="1550" width="1550" data-og-height="1280" height="1280" data-path="docs/images/assets/docs/integrations/hightouch/the-new-hotels-sheet-in-google-sheets.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/the-new-hotels-sheet-in-google-sheets.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=f779c9886ad95248b8b5b7cb9ef3bbb2 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/the-new-hotels-sheet-in-google-sheets.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=3d0409daeab844197d663800274b60f6 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/the-new-hotels-sheet-in-google-sheets.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=c9612f386aef4e9d46c3495d5e6c6334 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/the-new-hotels-sheet-in-google-sheets.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=b5119a33675c68594434b25523891f31 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/the-new-hotels-sheet-in-google-sheets.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=06487c604650a3b24195cc92ba8b8ac6 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/the-new-hotels-sheet-in-google-sheets.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=e036a7b401ee01cc0238463d6f8bfa0b 2500w" />
</Frame>

### 1. Create the Destination

Configuring the source and model for Google Sheets will not be covered in this demo. I’ll start by creating a new destination for my PlanetScale database by selecting **"Destinations"** from the left navigation, then **"Add destination"**.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/hightouch/add-destination.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=fa2011e731734b714a052fd1e82dd9dd" alt="Add Destination" data-og-width="1550" width="1550" data-og-height="1280" height="1280" data-path="docs/images/assets/docs/integrations/hightouch/add-destination.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/hightouch/add-destination.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=1adad77680f1a72a1f167766e933ba36 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/hightouch/add-destination.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=f88f5b5d362d53eb34ad014b64e48a43 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/hightouch/add-destination.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=347b85e743854a824459bf30ac537ec2 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/hightouch/add-destination.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=697468c01237a735cf5620955718ccd2 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/hightouch/add-destination.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=f419bb077d2e704b6dcea9f19c73f475 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/hightouch/add-destination.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=15e352fafb152bf2af8c347eb208f44c 2500w" />
</Frame>

Search for the PlanetScale destination, select it, and click **"Continue"**.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/select-planetscale-as-the-destination.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=262444106452a60ae9b4c581ff5b67d3" alt="Select PlanetScale as the destination" data-og-width="1550" width="1550" data-og-height="1280" height="1280" data-path="docs/images/assets/docs/integrations/hightouch/select-planetscale-as-the-destination.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/select-planetscale-as-the-destination.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=8ec86fb2473510ecd4ce43b0ebcb63b9 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/select-planetscale-as-the-destination.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=ac75ba147d7092735c7b266683d8724d 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/select-planetscale-as-the-destination.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=97d5dd89317cb989e19239e788294db6 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/select-planetscale-as-the-destination.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=6782ba73afccebc798da9f9978c50f4d 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/select-planetscale-as-the-destination.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=8315c99b1187940750ceea79f0d45656 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/select-planetscale-as-the-destination.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=b6921037d35c718a10b9ae58daffcc78 2500w" />
</Frame>

In the next view, populate the details in the form using the connection details for your PlanetScale database. Click **"Continue"**.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/connect-to-planetscale-as-a-destination.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=1a3763104618d29b74ea1df4be1c09d6" alt="Connect to PlanetScale as a destination" data-og-width="1596" width="1596" data-og-height="1280" height="1280" data-path="docs/images/assets/docs/integrations/hightouch/connect-to-planetscale-as-a-destination.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/connect-to-planetscale-as-a-destination.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=c1821ffeb8f1e4bc8bc24a70205cded0 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/connect-to-planetscale-as-a-destination.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=ebe1b594cd16bf7f610d01b66c1f27b9 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/connect-to-planetscale-as-a-destination.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=804d766a3dfd34c4e44cbbe2708c1b4b 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/connect-to-planetscale-as-a-destination.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=f342a26bc930e93edabc2800f44ce175 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/connect-to-planetscale-as-a-destination.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=967d8065fa3c7b6d3ee1257d24671080 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/connect-to-planetscale-as-a-destination.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=d391b6d663f8bf325af048b10c643d15 2500w" />
</Frame>

Click **"Finish"** to save the Destination.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/finalize-planetscale-destination.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=47e143161c3056c2945db17034d9785e" alt="Finalize PlanetScale destination" data-og-width="1596" width="1596" data-og-height="1280" height="1280" data-path="docs/images/assets/docs/integrations/hightouch/finalize-planetscale-destination.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/finalize-planetscale-destination.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=57b1d5be094d7607a7669e2445d8b06c 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/finalize-planetscale-destination.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=42a3056438709162d25b486a7081ddd4 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/finalize-planetscale-destination.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=c9eaa30033195910dbaa04ae1f8e8008 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/finalize-planetscale-destination.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=8e095d20eff3627090bada8a060e23f3 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/finalize-planetscale-destination.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=af970eb7c5c5b2fc1b12a26021785bb6 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/finalize-planetscale-destination.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=64903feccca6a8d8c081f3c3da5eda6c 2500w" />
</Frame>

### 2. Create the Sync

To create a new Sync, select **"Syncs"** from the left navigation and click **"Add sync"**.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/hightouch/add-second-sync.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=33bdf9089b3f27a6c0a0dc39d4d8e797" alt="Add second sync" data-og-width="2007" width="2007" data-og-height="1280" height="1280" data-path="docs/images/assets/docs/integrations/hightouch/add-second-sync.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/hightouch/add-second-sync.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=8f7c09b1299490a18a9db85239586746 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/hightouch/add-second-sync.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=47f28a778da227fe45990e2541e0a1a4 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/hightouch/add-second-sync.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=30e46e72e8191d2c42b003a22f0dde78 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/hightouch/add-second-sync.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=6e798fc36dc1ec309a39d4584871d889 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/hightouch/add-second-sync.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=3c11387e8d214e4e3c9dcaeda66bcf05 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/hightouch/add-second-sync.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=973b6514dfec5e39ad2a50cfb7cf728b 2500w" />
</Frame>

The model name for my Google Sheets worksheet is `new_hotel` so I’ll select that from the list.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/select-google-sheets-as-a-source.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=e3939cdbda63ebcfa71e9f110789186c" alt="Select Google Sheets as a source" data-og-width="1806" width="1806" data-og-height="1291" height="1291" data-path="docs/images/assets/docs/integrations/hightouch/select-google-sheets-as-a-source.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/select-google-sheets-as-a-source.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=bec2b1d8b39a83f837c4c92b31cdddc6 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/select-google-sheets-as-a-source.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=395e3a68d5729cf9236abe80bdd811ff 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/select-google-sheets-as-a-source.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=31c198d1ba47cbd0930076c94228c197 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/select-google-sheets-as-a-source.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=2cba66e45b8490d5e13c8101f6380ea2 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/select-google-sheets-as-a-source.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=9b5ffa9fc3b86948bd19f8260268eb53 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/select-google-sheets-as-a-source.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=61672103e7b206ec4b4a1ad41276c1c7 2500w" />
</Frame>

Select the Destination created in the previous step.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/select-the-planetscale-destination.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=c96e07fbd3cdb95d55768b24b65176b3" alt="Select the PlanetScale destination" data-og-width="1806" width="1806" data-og-height="1291" height="1291" data-path="docs/images/assets/docs/integrations/hightouch/select-the-planetscale-destination.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/select-the-planetscale-destination.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=7ee0d724e0f0ed00bf2cd216dfd6bfae 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/select-the-planetscale-destination.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=02b46ea3c2c28ddd128f19733d9bff36 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/select-the-planetscale-destination.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=a80448a78ec588874ce717ab1d12637c 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/select-the-planetscale-destination.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=a7d4c77e249b2d157474d5373f4b5f97 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/select-the-planetscale-destination.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=e62f4bafa30464739cf7ea2082a24530 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/select-the-planetscale-destination.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=09fed7db378d2d20648b274ab2865466 2500w" />
</Frame>

Next, I can select the table I wish to sync data to. I’ll choose `hotels` from the dropdown. Hightouch only supports upserting data at this time, so that option is preselected. I’ll also need to map the fields between my Google Sheets worksheet and my PlanetScale database, starting with the unique identifiers.

Both have an `id` column so I can select that for both sides of the sync. Since the other column names match, I can click **"Suggest mappings"** to let Hightouch automatically figure out which fields map from the Source to the Destination. Once that’s done, I’ll click **"Continue"** to move forward.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/configure-the-field-mapping.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=4851815fdbf35776fcd54c17902cf357" alt="Configure the field mapping" data-og-width="1502" width="1502" data-og-height="1378" height="1378" data-path="docs/images/assets/docs/integrations/hightouch/configure-the-field-mapping.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/configure-the-field-mapping.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=09bda23987a8b13d670663776f788fa8 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/configure-the-field-mapping.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=1d8deb9b72744c13f4d010f9b34bf9c3 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/configure-the-field-mapping.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=b94bbcdbc7740245c9ed55c867695072 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/configure-the-field-mapping.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=d2319586c24955e34e73429a84151af0 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/configure-the-field-mapping.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=f1906326918a666abc1c9dc0bdd0a6f2 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/configure-the-field-mapping.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=2e9e609d1334b8b04d5e756f32127499 2500w" />
</Frame>

Finally, I can specify the Schedule type for when my data sync should occur. I’ll leave it set to **"Manual"** and click **"Continue"**.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/finalize-the-second-sync.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=27ce228e5ad43b21f936d93f9b42c27c" alt="Finalize the second sync" data-og-width="1502" width="1502" data-og-height="1378" height="1378" data-path="docs/images/assets/docs/integrations/hightouch/finalize-the-second-sync.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/finalize-the-second-sync.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=458b2b475c34dc31f48d2a814d32daef 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/finalize-the-second-sync.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=8a3d3905d26126df8e4d36f2b2a8bfac 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/finalize-the-second-sync.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=13abaa505c39268bc431b4d9aeaf42e9 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/finalize-the-second-sync.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=60f64bb276907aa160623b8328330a78 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/finalize-the-second-sync.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=943d89813d65e15fd6c0f0aefdeafe0d 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/finalize-the-second-sync.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=a6508956a5744d0ee0ac1bfea353eb3f 2500w" />
</Frame>

Since this Sync is configured to execute manually, I’ll click **"Run sync"**.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/run-the-second-sync.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=44e79d86124ffdc688f6ef6c48a7de0d" alt="Run the second sync" data-og-width="1502" width="1502" data-og-height="1378" height="1378" data-path="docs/images/assets/docs/integrations/hightouch/run-the-second-sync.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/run-the-second-sync.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=b2b5b843cccfcdaffca79ca7242e90a7 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/run-the-second-sync.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=e6e4df469fe9d92a483c4b07ef71390b 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/run-the-second-sync.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=996bda49058f3133987fdb6550c5406e 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/run-the-second-sync.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=1def61e8b8177c04bed9dfe1c04e8e92 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/run-the-second-sync.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=2c53573121f3a5c1a966405641d07d45 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/run-the-second-sync.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=19e39c7a7ece8bd5d2f44938707ff1d4 2500w" />
</Frame>

### 3. Review the synced data

I can verify that the sync was successful by querying my PlanetScale database to ensure the new hotel was added successfully.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/the-select-query-from-the-planetscale-console.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=caac39d8abb61533af341333b6abb873" alt="The select query from the PlanetScale console" data-og-width="1502" width="1502" data-og-height="1378" height="1378" data-path="docs/images/assets/docs/integrations/hightouch/the-select-query-from-the-planetscale-console.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/the-select-query-from-the-planetscale-console.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=51f1d72201effaa73e0e2f190a0f442b 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/the-select-query-from-the-planetscale-console.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=9f54495db0061050adc9ef6c9ab80af1 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/the-select-query-from-the-planetscale-console.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=27065987d49750e3cc99001d51e3f85d 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/the-select-query-from-the-planetscale-console.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=a343364dc0afb95f2d15635724d2a8d9 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/the-select-query-from-the-planetscale-console.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=a634020a7a56390ac18134483a56f815 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/hightouch/the-select-query-from-the-planetscale-console.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=cf44abb4565110d7c5239361f52cda03 2500w" />
</Frame>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Netlify
Source: https://planetscale.com/docs/vitess/integrations/netlify



## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Prometheus
Source: https://planetscale.com/docs/vitess/integrations/prometheus

PlanetScale exposes Prometheus-compatible metrics endpoints for scraping metrics about your database branches. This, along with our API-driven service discovery, allow you to automatically get in-depth information about all of the databases in your organization.

In order to collect and store these, you will need to use Prometheus or a Prometheus-compatible metrics engine (such as VictoriaMetrics) that is capable of using the [HTTP SD](https://prometheus.io/docs/prometheus/latest/http_sd/) protocol.

## Prerequisites

This document assumes we'll be configuring a Prometheus 3.x instance via a configuration file running on our local machine.

If you are using managed Prometheus via AWS, GCP or another provider, you will have to deploy Prometheus to scrape and forward metrics via `remote_write`, as these services do not support scraping metrics.

## Getting Started

First, provision a new PlanetScale [Service token](/docs/api/reference/service-tokens) in your Organization settings. Make sure to save the ID and token, as they will not be visible after they've been generated.

When that's created, grant the token `read_metrics_endpoints` permissions and click "Save permissions". Your token should look like the following:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-service-token-configuration.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=6788de4f7c33b53bae655708c4baff9f" alt="Service Token configuration for Metrics Exporting" data-og-width="1348" width="1348" data-og-height="1136" height="1136" data-path="docs/images/metrics-service-token-configuration.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-service-token-configuration.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=64c0aa40bda297dd71fcdc604c7593c8 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-service-token-configuration.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=0f82feb485f8046096d6dc4c9fde8cde 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-service-token-configuration.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=a507b57eef83b96e43ef16ad9c29afcd 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-service-token-configuration.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=2f04f5ebb7f7f148dc809654a8b192da 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-service-token-configuration.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=aa009d73fa64ae6d3fd60d37ba0bd94c 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-service-token-configuration.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=45f8237641306c452d6e3293fec320c1 2500w" />
</Frame>

## Configuring Prometheus

Now that we have a Service Token, we can add a scrape configuration for your PlanetScale organization. A minimal Prometheus configuration should look like the following:

```yaml  theme={null}
scrape_configs:
  - job_name: "${ORG}"
    http_sd_configs:
    - url: https://api.planetscale.com/v1/organizations/${ORG}/metrics
      authorization:
        type: "token"
        credentials: "${TOKEN_ID}:${TOKEN}"
      refresh_interval: 10m
```

Fill in your organization name in the `job_name` and `url`, and place the Service Token and ID that we created in the previous step for the credentials.

Save this file to `prometheus.yml` in your working directory.

## Start Prometheus

Run Prometheus pointed at this configuration file:

```bash  theme={null}
$ prometheus --config.file=prometheus.yml
```

By default, Prometheus will listen at `0.0.0.0:9090`, which means you can access it in your browser at [http://127.0.0.1:9090](http://127.0.0.1:9090).

### Validating Service Discovery

First, let's make sure that Prometheus is properly querying the PlanetScale API for the right branches. If you go to `http://127.0.0.1:9090/service-discovery` you should see the job that we created earlier, with all of your branches listed under `Discovered labels`. In this example, our organization is called `nick`, so it looks like the following:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-targets.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=eb5b15d46086cd686d2e3cdc27a81655" alt="Prometheus Target List" data-og-width="2886" width="2886" data-og-height="1456" height="1456" data-path="docs/images/metrics-prometheus-targets.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-targets.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=e6cb52a5d4cfccd6692995ee06e6c17d 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-targets.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=91f2153cfefce2e2517e04e47459e658 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-targets.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=4d22ffa22fa8569ff0fae3be6c848ab0 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-targets.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=4e43bea221193dd4858ea72417cdd4b0 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-targets.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=3ea68d9b55697ad22d7dd93c4aa58190 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-targets.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=0fa878b0e86c25df0a3b99ea55d34ba4 2500w" />
</Frame>

Here, I have two branches that have been discovered. I can confirm that this matches what's in my organization:

```bash  theme={null}
$ pscale branch list test --org nick
  ID             NAME         PARENT BRANCH   REGION    PRODUCTION   SAFE MIGRATIONS   READY   CREATED AT    UPDATED AT
 -------------- ------------ --------------- --------- ------------ ----------------- ------- ------------- ---------------
  7wxuxewx4l0p   main         n/a             us-east   Yes          Yes               Yes     2 years ago   7 minutes ago
  6o0rr27785fl   partitions   main            us-east   No           No                Yes     1 month ago   9 minutes ago
```

Now, if I go to my list of targets I should see each branch as an Endpoint:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-endpoints.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=e0c332509b19a9bb2ed2b63f14e1020d" alt="Prometheus Endpoint List" data-og-width="2384" width="2384" data-og-height="660" height="660" data-path="docs/images/metrics-prometheus-endpoints.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-endpoints.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=ecde7c2e1f790d7eb6730781b3b07224 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-endpoints.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=c5886bea0a0f432f7ac971d6e1e8de2b 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-endpoints.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=408a725b71e31d934868b64a2919a8c7 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-endpoints.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=88a517da96ac66ad8f2662d9dd1fb502 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-endpoints.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=cd421d5b722f15fa176793ec2f0143b9 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-endpoints.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=a0717a3d71c7bd84bafa55f801912964 2500w" />
</Frame>

This screenshot shows that they're being correctly scraped, and I can start to query my Prometheus instance.

## Querying Prometheus

Now that we're collecting metrics for my branches, our [reference guide](/docs/vitess/integrations/prometheus-metrics) has a list of everything that we export. If I want to see how many `vtgate` pods are running per AZ for my branch, I can query:

```
planetscale_vtgate_total_pods{planetscale_database_branch_id="7wxuxewx4l0p"}
```

Make sure the graph is set to stacked, and it should look like this:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-querying.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=bd02102ef5e2bbc4893da279353807cb" alt="Querying Prometheus for VTGate Count" data-og-width="2710" width="2710" data-og-height="2422" height="2422" data-path="docs/images/metrics-prometheus-querying.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-querying.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=150f556daa4de597267520394eadf38e 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-querying.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=f5e4451baa8d463ad094d56ca63e45bf 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-querying.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=532753037cda6f5f7dcc62ffe07e456e 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-querying.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=e69c7becc71cd28b7403b518ad44df0d 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-querying.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=0be7dd37cb8970f9c3d16cef92f8ae77 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-querying.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=ec9705a66b171b308f34f32c008836ff 2500w" />
</Frame>

## Next Steps

If you keep this Prometheus instance running, it will collect metrics every 30 seconds, and refresh the list of branches every 10 minutes.

For more information, see:

* [Metrics reference](/docs/vitess/integrations/prometheus-metrics) for a list of metrics we expose
* [Grafana and Prometheus](/docs/vitess/tutorials/prometheus-metrics-grafana) tutorial for using PlanetScale's provided dashboard to visualize these metrics in Grafana.
* [Sending metrics to New Relic](/docs/vitess/tutorials/prometheus-metrics-newrelic) tutorial for using Prometheus to forward metrics to New Relic.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Prometheus metrics
Source: https://planetscale.com/docs/vitess/integrations/prometheus-metrics

PlanetScale exposes a Prometheus-compatible endpoint per-branch that allows you to scrape metrics for your database.

## Overview

See our [Prometheus integration](/docs/vitess/integrations/prometheus) documentation for how to set Prometheus up to automatically discover and scrape metrics for your database branches.

If you're using Datadog, see our [Datadog tutorial](/docs/vitess/tutorials/prometheus-metrics-datadog) for how to setup your Datadog agent to scrape metrics for your branch.

## Metrics

PlanetScale emits the following metrics to be scraped.

## Database metrics

| **Name & Description**                                                                                                                | **Type**  | **Tags**                                                                                                                                                                                                                            |
| :------------------------------------------------------------------------------------------------------------------------------------ | :-------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **planetscale\_edge\_active\_connections**  The number of active MySQL connections to the branch                                      | Gauge     | cluster, planetscale\_database\_branch\_id, planetscale\_region                                                                                                                                                                     |
| **planetscale\_mysql\_bytes\_received\_total**  Total number of bytes received from MySQL clients                                     | Counter   | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_mysql\_bytes\_sent\_total**  Total number of bytes sent to MySQL clients                                               | Counter   | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_mysql\_innodb\_data\_writes\_total**  Total number of InnoDB data write operations                                     | Counter   | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_mysql\_innodb\_row\_lock\_time\_total**  Total time spent acquiring row locks in InnoDB                                | Counter   | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_mysql\_innodb\_row\_lock\_waits\_total**  Number of times InnoDB had to wait for a row lock                            | Counter   | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_mysql\_innodb\_row\_operations\_total**  Total number of row operations (inserts/updates/deletes) in InnoDB            | Counter   | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_mysql\_operation\_type, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                             |
| **planetscale\_mysql\_auto\_increment\_util\_percentage** The percentage of auto increment size being utilized                        | Gauge     | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_shard, planetscale\_table                                                                                                                           |
| **planetscale\_mysql\_replica\_lag\_seconds**  Replica lag in fine-grained seconds from MySQL                                         | Gauge     | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_mysql\_slow\_queries\_total**  Number of queries that exceeded the slow query threshold                                | Counter   | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_mysql\_threads\_running**  Current number of threads executing in MySQL                                                | Gauge     | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_pods\_container\_waiting\_reason**  Container waiting reason (CrashLoopBackOff, ImagePullBackOff, etc)                 | Gauge     | cluster, planetscale\_cell, planetscale\_component, planetscale\_container, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type, planetscale\_waiting\_reason |
| **planetscale\_pods\_cpu\_util\_percentages**  CPU utilization percentage of database pods                                            | Gauge     | cluster, planetscale\_cell, planetscale\_component, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                       |
| **planetscale\_pods\_iops\_total**  Total IOPS (Input/Output Operations Per Second) of database pods                                  | Counter   | cluster, planetscale\_cell, planetscale\_component, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                       |
| **planetscale\_pods\_mem\_util\_percentages**  Memory utilization percentage of database pods                                         | Gauge     | cluster, planetscale\_cell, planetscale\_component, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                       |
| **planetscale\_pods\_status\_phase**  Pod status phase (Running, Pending, Failed, Succeeded, Unknown)                                 | Gauge     | cluster, planetscale\_cell, planetscale\_component, planetscale\_database\_branch\_id, planetscale\_phase, planetscale\_pod, planetscale\_keyspace, planetscale\_shard, planetscale\_tablet\_type                                   |
| **planetscale\_vtgate\_affected\_rows\_total**  Number of rows affected by queries through vtgate                                     | Counter   | cluster, planetscale\_cell, planetscale\_database\_branch\_id, planetscale\_pod                                                                                                                                                     |
| **planetscale\_vtgate\_commands\_total**  Number of commands processed by vtgate                                                      | Counter   | cluster, planetscale\_cell, planetscale\_command, planetscale\_database\_branch\_id, planetscale\_pod                                                                                                                               |
| **planetscale\_vtgate\_errors\_total**  Total number of errors encountered by vtgate                                                  | Counter   | cluster, planetscale\_vtgate\_code, planetscale\_database\_branch\_id, planetscale\_tablet\_type, planetscale\_keyspace, planetscale\_cell, planetscale\_pod, planetscale\_vtgate\_operation                                        |
| **planetscale\_vtgate\_queries\_duration**  Distribution of query execution times through vtgate                                      | Histogram | cluster, le, planetscale\_cell, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_tablet\_type, planetscale\_vtgate\_operation                                                                                      |
| **planetscale\_vtgate\_returned\_rows\_total**  Number of rows returned by queries through vtgate                                     | Counter   | cluster, planetscale\_cell, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_tablet\_type                                                                                                                          |
| **planetscale\_vtgate\_total\_pods**  Total number of vtgate pods                                                                     | Gauge     | cluster, planetscale\_cell, planetscale\_database\_branch\_id                                                                                                                                                                       |
| **planetscale\_vtorc\_failed\_recoveries\_total**  Number of failed replication/recovery attempts by vtorc                            | Counter   | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_vtorc\_recovery\_type                                                                                                                                    |
| **planetscale\_vtorc\_successful\_recoveries\_total**  Number of successful replication/recovery attempts by vtorc                    | Counter   | cluster, planetscale\_database\_branch\_id, planetscale\_pod, planetscale\_vtorc\_recovery\_type                                                                                                                                    |
| **planetscale\_vttablet\_connection\_pool\_active**  Number of active connections in the vttablet connection pool                     | Gauge     | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_vttablet\_connection\_pool\_capacity**  Total capacity of the vttablet connection pool                                 | Gauge     | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_vttablet\_connection\_pool\_get\_total**  Number of connection get operations from the vttablet pool                   | Counter   | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_vttablet\_connection\_pool\_in\_use**  Number of connections currently in use in the vttablet pool                     | Gauge     | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_vttablet\_connection\_pool\_wait\_time\_total**  Total time spent waiting for connections from the vttablet pool       | Counter   | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_vttablet\_connection\_pool\_wait\_total**  Number of waits for an available connection in the vttablet pool            | Counter   | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_vttablet\_errors\_total**  Number of errors encountered by vttablet                                                    | Counter   | cluster, planetscale\_cell, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type, planetscale\_vttablet\_code                                                  |
| **planetscale\_vttablet\_found\_rows\_pool\_active**  Number of active connections in the vttablet found rows pool                    | Gauge     | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_vttablet\_found\_rows\_pool\_capacity**  Total capacity of the vttablet found rows pool                                | Gauge     | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_vttablet\_found\_rows\_pool\_get\_total**  Number of connection get operations from the found rows pool                | Counter   | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_vttablet\_found\_rows\_pool\_in\_use**  Number of connections currently in use in the found rows pool                  | Gauge     | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_vttablet\_found\_rows\_pool\_wait\_time\_total**  Total time spent waiting for connections from the found rows pool    | Counter   | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_vttablet\_found\_rows\_pool\_wait\_total**  Number of waits for an available connection in the found rows pool         | Counter   | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_vttablet\_is\_serving**  If a vttablet is or is not serving traffic                                                    | Gauge     | cluster, planetscale\_cell, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                               |
| **planetscale\_vttablet\_queries\_affected\_rows\_total**  Total number of rows affected by queries through vttablet                  | Counter   | cluster, planetscale\_command, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_table, planetscale\_tablet\_type                                                        |
| **planetscale\_vttablet\_queries\_duration**  Distribution of query execution times through vttablet                                  | Histogram | cluster, le, planetscale\_command, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                        |
| **planetscale\_vttablet\_queries\_duration\_by\_table\_total**  Total of query execution times through vttablet, broken down by table | Counter   | cluster, planetscale\_command, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_shard, planetscale\_table, planetscale\_tablet\_type                                                                          |
| **planetscale\_vttablet\_queries\_read\_rows\_total**  Total number of rows read by queries through vttablet                          | Counter   | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_vttablet\_queries\_returned\_rows\_total**  Total number of rows returned by queries through vttablet                  | Counter   | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_table, planetscale\_tablet\_type                                                                              |
| **planetscale\_vttablet\_queries\_total**  Total number of queries processed by vttablet                                              | Counter   | cluster, planetscale\_command, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_table, planetscale\_tablet\_type                                                        |
| **planetscale\_vttablet\_replication\_lag**  MySQL replication lag in seconds                                                         | Gauge     | cluster, planetscale\_cell, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                               |
| **planetscale\_vttablet\_stream\_connection\_pool\_active**  Number of active connections in the vttablet stream connection pool      | Gauge     | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_vttablet\_stream\_connection\_pool\_capacity**  Total capacity of the vttablet stream connection pool                  | Gauge     | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_vttablet\_stream\_connection\_pool\_get\_total**  Number of connection get operations from the stream connection pool  | Counter   | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_vttablet\_stream\_connection\_pool\_in\_use**  Number of connections currently in use in the stream connection pool    | Gauge     | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_vttablet\_stream\_connection\_pool\_wait\_time\_total**  Total time spent waiting for connections from the stream pool | Counter   | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_vttablet\_stream\_connection\_pool\_wait\_total**  Number of waits for an available connection in the stream pool      | Counter   | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_vttablet\_table\_storage\_bytes**  Storage size of tables in bytes                                                     | Gauge     | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_shard, planetscale\_table                                                                                                                           |
| **planetscale\_vttablet\_transaction\_pool\_active**  Number of active transactions in the vttablet transaction pool                  | Gauge     | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_vttablet\_transaction\_pool\_capacity**  Total capacity of the vttablet transaction pool                               | Gauge     | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_vttablet\_transaction\_pool\_get\_total**  Number of transaction get operations from the transaction pool              | Counter   | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_vttablet\_transaction\_pool\_in\_use**  Number of transactions currently in use in the transaction pool                | Gauge     | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_vttablet\_transaction\_pool\_wait\_time\_total**  Total time spent waiting for transactions from the transaction pool  | Counter   | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_vttablet\_transaction\_pool\_wait\_total**  Number of waits for an available transaction in the transaction pool       | Counter   | cluster, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                                                  |
| **planetscale\_vttablet\_volume\_available\_bytes**  Available storage space in bytes on vttablet volumes                             | Gauge     | cluster, planetscale\_cell, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                               |
| **planetscale\_vttablet\_volume\_capacity\_bytes**  Total storage capacity in bytes on vttablet volumes                               | Gauge     | cluster, planetscale\_cell, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_pod, planetscale\_shard, planetscale\_tablet\_type                                                                               |
| **planetscale\_vttablet\_vstreamer\_errors\_total**  Total number of vttablet vstreamer errors.                                       | Counter   | cluster, planetscale\_cell, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_shard, planetscale\_vstreamer\_code                                                                                              |
| **planetscale\_vttablet\_vstreamer\_events\_total**  Total number of vttablet vstreamer streamed events.                              | Counter   | cluster, planetscale\_cell, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_shard                                                                                                                            |
| **planetscale\_workflow\_vreplication\_lag**  VReplication lag in seconds for workflow operations                                     | Gauge     | cluster, planetscale\_cell, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_shard, planetscale\_source\_keyspace, planetscale\_source\_shard, planetscale\_workflow                                          |
| **planetscale\_workflow\_vreplication\_stream\_state**  State of the VReplication stream for workflow operations                      | Gauge     | cluster, planetscale\_cell, planetscale\_database\_branch\_id, planetscale\_keyspace, planetscale\_shard, planetscale\_state, planetscale\_workflow                                                                                 |

## Edge metrics (Single Tenant & Managed only)

| **Name & Description**                                                                                  | **Type** | **Tags**                                                                                                              |
| ------------------------------------------------------------------------------------------------------- | -------- | --------------------------------------------------------------------------------------------------------------------- |
| **planetscale\_edge\_cpu\_util\_percentages**  CPU utilization percentage of Edge instances             | Gauge    | cluster, planetscale\_component, planetscale\_instance                                                                |
| **planetscale\_edge\_instance\_connections**  Number of active connections to Edge instances            | Gauge    | cluster, planetscale\_component, planetscale\_instance, planetscale\_region                                           |
| **planetscale\_edge\_mem\_util\_percentages**  Memory utilization percentage of Edge instances          | Gauge    | cluster, planetscale\_component, planetscale\_instance                                                                |
| **planetscale\_edge\_queries\_total**  Total number of queries processed by Edge instances              | Counter  | cluster, planetscale\_component, planetscale\_instance, planetscale\_region                                           |
| **planetscale\_psdb\_api\_cpu\_util\_percentages**  CPU utilization percentage of PSDB API instances    | Gauge    | cluster, planetscale\_availability\_zone, planetscale\_component, planetscale\_pod                                    |
| **planetscale\_psdb\_api\_mem\_util\_percentages**  Memory utilization percentage of PSDB API instances | Gauge    | cluster, planetscale\_availability\_zone, planetscale\_component, planetscale\_pod                                    |
| **planetscale\_psdb\_api\_requests\_total**  Total number of requests processed by PSDB API instances   | Counter  | cluster, planetscale\_availability\_zone, planetscale\_component, planetscale\_pod, planetscale\_psdb\_api\_operation |

## Tag glossary

| **Tag Name**                        | **Description**                                                    |
| ----------------------------------- | ------------------------------------------------------------------ |
| cluster                             | The database cluster identifier                                    |
| le                                  | Histogram bucket upper bound (less than or equal to)               |
| planetscale\_availability\_zone     | AWS availability zone where the component is running               |
| planetscale\_cell                   | Availability zone or cell where the component is running           |
| planetscale\_command                | VTGate command type (e.g., Select, Insert, Update)                 |
| planetscale\_component              | Vitess component type (vtgate, vttablet, vtctld, vtorc)            |
| planetscale\_database\_branch\_id   | Unique identifier for the database branch                          |
| planetscale\_instance               | Edge instance identifier                                           |
| planetscale\_keyspace               | Database keyspace (logical database name)                          |
| planetscale\_mysql\_operation\_type | MySQL row operation type (inserted, read, updated, deleted)        |
| planetscale\_pod                    | Kubernetes pod name where the component is running                 |
| planetscale\_psdb\_api\_operation   | PSDB API operation type                                            |
| planetscale\_region                 | Geographic region where the database is hosted                     |
| planetscale\_shard                  | Database shard identifier                                          |
| planetscale\_source\_keyspace       | Source keyspace for VReplication workflow operations               |
| planetscale\_source\_shard          | Source shard for VReplication workflow operations                  |
| planetscale\_state                  | VReplication stream state (e.g., Running, Stopped)                 |
| planetscale\_table                  | Database table name                                                |
| planetscale\_tablet\_type           | Vitess tablet type (primary, replica, rdonly)                      |
| planetscale\_vtgate\_operation      | VTGate operation type (Execute, ExecuteBatch, etc.)                |
| planetscale\_vtgate\_code           | VTGate error code (INVALID\_ARGUMENT, etc.)                        |
| planetscale\_vttablet\_code         | VTTablet error code (NOT\_FOUND, etc.)                             |
| planetscale\_vstreamer\_code        | VTTablet VStreamer error code (StreamEnded, etc.)                  |
| planetscale\_vtorc\_recovery\_type  | Type of recovery operation performed by VTOrc (planned, unplanned) |
| planetscale\_workflow               | VReplication workflow identifier                                   |

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Stitch integration
Source: https://planetscale.com/docs/vitess/integrations/stitch

With PlanetScale Connect, you can extract data from your PlanetScale database and safely load it into other destinations for analysis, transformation, and more.

We implemented an [Stitch Singer Tap](https://stitchdata.com/) as the pipeline between your PlanetScale source and selected destination. This document will walk you through how to connect your PlanetScale database to Stitch.

## How to connect

### Step 1 : Setup an Import API integration in Stitch.

PlanetScale's Stitch tap outputs records and metadata to stdout so that the `http tap` can import them into Stitch via [Stitch Import API](https://www.stitchdata.com/docs/developers/import-api/).
into Stitch via [Stitch Import API](https://www.stitchdata.com/docs/developers/import-api/)

<Steps>
  <Step>
    Sign up for a [StitchData](https://app.stitchdata.com/signup) account
  </Step>

  <Step>
    Once you've signed up, create an Integration by clicking on **Add Integration**.
  </Step>

  <Step>
    On the marketplace screen, type in **import** to narrow the list down to the **Import API**

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/integration.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=cf44062fff5c6e1c1b33ad3cf25028c6" alt="Add Stitch Integration" data-og-width="2416" width="2416" data-og-height="950" height="950" data-path="docs/images/assets/docs/integrations/stitch/integration.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/integration.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=29df37f8a0e7948265e54e3742c28f35 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/integration.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=eb06af4050721748e3ce4f01a7921f1e 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/integration.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=ba03ec7e67f2b7e388355325cb76ff11 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/integration.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=551bdfefbdaf9e0b9d3b1baa9650caa0 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/integration.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=66dbc96ff1e33e6c03e380e1f3e555c3 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/integration.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=b6a7b53e0e42e99c4f2e2fb1f03f1641 2500w" />
    </Frame>
  </Step>

  <Step>
    On the next screen, configure your integration name and destination.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/configure.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=ab151ca2cdce023f173779a9d66bc9e4" alt="Configure Stitch Integration" data-og-width="1618" width="1618" data-og-height="1834" height="1834" data-path="docs/images/assets/docs/integrations/stitch/configure.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/configure.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=88e95c19553eda7e21bd1b660c41ea89 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/configure.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=5b2f47231ab669bfbaf34b742c0d8e04 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/configure.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=b4f4a3a6afb6099051261563e2761467 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/configure.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=c946304e33d6259894d386ea9e6c82b3 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/configure.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=ac6aaccdf5eab4cd9759b50743c7a8e4 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/configure.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=92410f36f2050aea65a64b304936fe2f 2500w" />
    </Frame>
  </Step>

  <Step>
    Once the integration is created, save the access token for use with the PlanetScale tap.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/api-token.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=3a942c46b78b5236ba0a4f56cc2df8a8" alt="Save Stitch API Token" data-og-width="1478" width="1478" data-og-height="760" height="760" data-path="docs/images/assets/docs/integrations/stitch/api-token.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/api-token.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=ad8bb6c3ced02f65643be5cb8238f1bf 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/api-token.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=ba25a7c639b3d13ae594638bd0fecef9 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/api-token.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=2c4d600369495ca2d419eabeabbcec1d 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/api-token.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=44a64a867975a5b2df1cbd970c235d39 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/api-token.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=4168322e436e009dbab7d7eef8689a86 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/api-token.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=b25b4403108ba3520df8d627acafb06a 2500w" />
    </Frame>
  </Step>
</Steps>

### Step 2 : Configure the PlanetScale Stitch Tap

In this step, we will connect your PlanetScale database to the PlanetScale Singer Tap.

<Steps>
  <Step>
    Click on the database and branch you want to connect to.
  </Step>

  <Step>
    Click "Connect", and select "Stitch source" from the "Connect with" dropdown.
  </Step>

  <Step>
    Leave this tab open, as you'll need to copy these credentials shortly.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/connect.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=1a3e1e44268d2fc19ef5379f286dd988" alt="Stitch Source config" data-og-width="1148" width="1148" data-og-height="634" height="634" data-path="docs/images/assets/docs/integrations/stitch/connect.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/connect.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=1f4bab3b21c7b331f1c85c33977e600b 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/connect.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=7e8f28ac253ae633560bb1524a1b96eb 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/connect.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=a2ba284821850894faf9360b256c8059 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/connect.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=37d23a9496b20554d28f80995fb89a69 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/connect.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=cc4b2c5e079acfc6b294d6c324417a04 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/connect.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=02a43d173615fed1833069459763aa4e 2500w" />
    </Frame>
  </Step>

  <Step>
    Copy the contents of `source.json` as a file on your local file system, and save it as `source.json`. This will now act
    as the `PlanetScale source config` when connecting the `PlanetScale Stitch Tap` to your database.
  </Step>
</Steps>

### Step 3: Run the PlanetScale Stitch Tap

<Steps>
  <Step>
    Install the PlanetScale Singer tap by running:

    ```bash  theme={null}
    brew install planetscale/tap/ps-singer-tap
    ```
  </Step>

  <Step>
    Install the PlanetScale Http tap by running

    ```bash  theme={null}
    brew install planetscale/tap/ps-http-tap
    ```
  </Step>

  <Step>
    Save the schema for your PlanetScale database.

    ```bash  theme={null}
    ps-singer-tap --config source.json  --discover > schema.json
    ```
  </Step>

  <Step>
    The `schema.json` file you saved in the previous step is a JSON document
    that describes all tables & columns available in your PlanetScale database. By default, no tables/columns are selected.
    You can select a column or table by setting its `selected` property in the table's `metadata` element in the JSON document to be true.
    Here's an example of selecting the `dept_no` property in a table.

    ```json  theme={null}
    {
      "metadata": {
        "selected": true,
        "inclusion": "available",
        "breadcrumb": ["properties", "dept_no"]
      }
    }
    ```
  </Step>

  <Step>
    Sync your PlanetScale database to Stitch by running the following command:

    ```bash  theme={null}
    ps-singer-tap --config source.json  --catalog schema.json | ps-http-tap  --api-token $(cat access_token)
    ```
  </Step>

  <Step>
    You should see an output similar to this:

    ```bash expandable theme={null}
    PlanetScale Tap : INFO : Syncing records for PlanetScale database : import-on-scaler
    PlanetScale Tap : INFO : syncing rows from stream "departments" from shard "-"
    PlanetScale Tap : INFO : [departments shard : -] peeking to see if there's any new rows
    PlanetScale Tap : INFO : new rows found, syncing rows for 1m0s
    PlanetScale Tap : INFO : [departments shard : -] syncing rows with cursor [shard:"-" keyspace:"import-on-scaler"]
    PlanetScale Tap : INFO : Syncing with cursor position : [], using last known PK : false, stop cursor is : [MySQL56/e42292e8-e28f-11ec-9c5b-d680f5d655b3:1-705,e4e20f06-e28f-11ec-8d20-8e7ac09cb64c:1-26,eba743a8-e28f-11ec-9227-62aa711d33c6:1-20]
    PlanetScale Tap : INFO : [departments shard : -] Continuing with cursor after server timeout
    PlanetScale Tap : INFO : [departments shard : -] peeking to see if there's any new rows
    HTTP Tap : INFO : flushing [20] messages for stream "departments"
    PlanetScale Tap : INFO : [departments shard : -] no new rows found, exiting
    HTTP Tap : INFO : Server response status : "OK", message : "Batch accepted"
    HTTP Tap : INFO : flushing [1] messages for stream "departments"
    HTTP Tap : INFO : Server response status : "OK", message : "Batch accepted"
    HTTP Tap : INFO : saving state to path : state/state-1656850746251.json
    ```
  </Step>

  <Step>
    Any state outputted by the PlanetScale Tap will be saved and you can look at the logs for the location.
    Here is an example of outputted state:

    ```bash  theme={null}
    HTTP Tap : INFO : saving state to path : state/state-1656850746251.json
    ```
  </Step>

  <Step>
    In this example, you should see that Stitch loaded `21` records to be replicated.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/success.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=af1bafbe3119ba224a421720b537085e" alt="Completed Stitch import" data-og-width="1630" width="1630" data-og-height="988" height="988" data-path="docs/images/assets/docs/integrations/stitch/success.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/success.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=fd59096872b00ed3ff6c36d5c24ae998 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/success.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=390fef253157c1dd13b51151bdbd8b30 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/success.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=fee7607b2a2843bffac67183c4d26acc 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/success.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=bf5ec5bce2f9276389c80e2c051113ba 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/success.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=d80688c49f8083411b1fda51c6c191a1 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/stitch/success.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=11093e3b77900646d13b06a454dba8f5 2500w" />
    </Frame>
  </Step>

  <Step>
    To incrementally sync from this last sync position, pass the path to last saved state in step 7 as the `--state` argument when you run sync.
  </Step>
</Steps>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Vantage integration
Source: https://planetscale.com/docs/vitess/integrations/vantage

With [Vantage](https://www.vantage.sh/), you can set up [PlanetScale cost management](https://vantage.sh/integrations/planetscale) to report on PlanetScale costs alongside other infrastructure providers, such as AWS or GCP.

After integrating, you can create [cost reports](https://docs.vantage.sh/cost_reports) to break down costs per database and branch.

Beyond reporting, set up budget alerts, forecast usage, and view active database costs in Vantage. Vantage connects to your PlanetScale organizations using an OAuth flow.

## Prerequisites

* The [Organization Admin role](/docs/security/access-control) in PlanetScale
* A [Vantage account](https://console.vantage.sh/signup)

<Note>
  Database cost reporting in Vantage is not available for [PlanetScale Managed](/docs/vitess/managed) customers via the integration.
</Note>

## Configure the Vantage integration

<Steps>
  <Step>
    From the Vantage console, navigate to the [Integrations page](https://console.vantage.sh/settings/integrations).
  </Step>

  <Step>
    Select **PlanetScale**, then click **Connect PlanetScale Account**.
  </Step>

  <Step>
    The PlanetScale login screen is displayed. Log in to your PlanetScale account and select the organizations you want to connect with.
  </Step>

  <Step>
    Click **Authorize access**.
  </Step>

  <Step>
    On the [PlanetScale Settings](https://console.vantage.sh/settings/planetscale/) page in Vantage, you should see the status of your connection change to **Importing**.
  </Step>
</Steps>

Costs will be ingested and processed in Vantage once you add the integration. It typically takes less than 15 minutes to ingest PlanetScale costs. The costs will be available on your **All Resources** Cost Report in Vantage as soon as they are processed.

<Note>
  PlanetScale data refreshes daily in Vantage.
</Note>

## View PlanetScale costs in Vantage

In Vantage, you can create cost reports to drill down into your costs. Vantage displays PlanetScale costs by Organization, Service, Category, and Resource.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/vantage/vantage-console.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=46b4fe1abdb755f67231f53402352c7f" alt="Image of a PlanetScale Cost Report in Vantage showing costs per database" data-og-width="2792" width="2792" data-og-height="1878" height="1878" data-path="docs/images/assets/docs/integrations/vantage/vantage-console.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/vantage/vantage-console.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=b001625dbe59b47c07774ad1284fd129 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/vantage/vantage-console.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=5aa483c616a9a0625a89e14637137ddd 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/vantage/vantage-console.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=2faa0509aaf6a18ed782d10e8ca41b01 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/vantage/vantage-console.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=fb76560995d65e83cbfac4e1cf5ec213 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/vantage/vantage-console.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=106ba6087edd5e910c04fbccbef41ff7 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/integrations/vantage/vantage-console.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=9e57eb2651cd3fe225c3740607b52171 2500w" />
</Frame>

In the graphic above, PlanetScale costs are grouped by database for the month. For complete cost reporting dimensions and more information, see the [PlanetScale documentation](https://docs.vantage.sh/connecting_planetscale) for Vantage.

## Billing

The Vantage integration is available on all our [plans](/docs/planetscale-plans).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Maintenance schedules
Source: https://planetscale.com/docs/vitess/maintenance-schedules

Enterprise customers have the option to enable scheduled maintenance windows on databases.

If you have a maintenance schedule enabled, any changes to your cluster configuration will only roll out during the set maintenance period. This includes MySQL and Vitess version upgrades, increasing or decreasing cluster size, changes to the size of VTTablets and VTGates, and anything else that modifies your cluster configuration.

The only exceptions to this are the following:

* Changes to the *number* of VTGates

Modifications to your schema, such as [deploy requests](/docs/vitess/schema-changes/deploy-requests) and [workflows](/docs/vitess/scaling/workflows), are not included in the maintenance schedule and will complete as soon as you apply the changes.

## Enabling maintenance schedules

To enable the use of maintenance schedules, you must first reach out to our [Support team](https://planetscale.com/contact).

Once the feature is enabled on a database, you can configure the maintenance windows for each of your databases by clicking the database > Settings > Maintenance.

Maintenance schedules can be set up on a daily, weekly, or monthly basis at a specific time in UTC. When occurring weekly, you can set the day of the week. When occurring monthly, you can set the day of the week and which week of the month over which the maintenance will occur.

To modify a maintenance schedule, click on the available schedule, and then adjust via the dropdowns.

If you need to make a change during an emergency, you can disable the maintenance schedule. That will result in the branch immediately running any queued tasks. If you are making a change in an emergency, we recommend that you perform the sizing operation first to queue the change, and then disable the maintenance schedule.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale Managed overview
Source: https://planetscale.com/docs/vitess/managed



## What is PlanetScale Managed?

PlanetScale Managed is a single-tenant deployment of PlanetScale within your Amazon Web Services (AWS) or Google Cloud Platform (GCP) account. In this configuration, you can use the same API, CLI, and web interface that PlanetScale offers, with the benefit of running entirely in your own AWS or GCP account.

We have packaged the best parts of PlanetScale into a container and can deploy and operate them in your own account, bringing you the best of SaaS with the added benefit of a deployment free of noisy neighbors, enhanced support, and additional security guarantees.

With PlanetScale Managed, it is more than just an on-premises deployment of your database; you are getting the PlanetScale expert team operating your database alongside your team for a *truly* fully managed database solution. The PlanetScale team is on-call for your databases.

## How does PlanetScale Managed work?

PlanetScale Managed is a packaged [data plane](https://en.wikipedia.org/wiki/Data_plane), built on [Vitess and Kubernetes](https://planetscale.com/blog/scaling-hundreds-of-thousands-of-database-clusters-on-kubernetes), that's deployed to an AWS Organizations member account or GCP project that you own and we operate. Your database lives entirely inside a dedicated member account or project within your cloud organization. PlanetScale will not have access to other member accounts or projects nor your organization-level settings within the cloud service provider. At the same time, you still get to interact with your databases through the web application, pscale CLI, or the PlanetScale API, as you usually would with our hosted product. This includes developer experience features such as non-blocking schema changes, safe migrations, database branching, query insights, and more.

If you are an existing PlanetScale user, moving to PlanetScale Managed requires no changes to your existing developer workflows.

The database is deployed in a single-tenant environment and isolated in an AWS Organizations member account or GCP project from the rest of your organization's infrastructure. By default, all connections are encrypted, but public. You have the option to use private database connectivity through [AWS PrivateLink](/docs/vitess/managed/aws/privatelink) or [GCP Private Service Connect](/docs/vitess/managed/gcp/private-service-connect), which are only available on single-tenancy deployment options, including PlanetScale Managed.

Read more on how PlanetScale Managed works inside either cloud provider:

<Columns cols={2}>
  <Card title="PlanetScale Managed on Amazon Web Services" icon="aws" horizontal href="/docs/vitess/managed/aws" />

  <Card title="PlanetScale Managed on Google Cloud Platform" icon="google" horizontal href="/docs/vitess/managed/gcp" />
</Columns>

## Benefits of PlanetScale Managed

Single-tenancy is one of many benefits when it comes to PlanetScale Managed. Still, with this PlanetScale Enterprise service, you also get:

* Assistance with [horizontal sharding](/docs/vitess/sharding)
* Option to sign BAAs for [HIPAA compliance](https://planetscale.com/blog/planetscale-and-hipaa)
* Deployment to additional regions
* [PCI compliance](https://planetscale.com/blog/planetscale-managed-is-now-pci-compliant) (AWS only)
* Additional [support options](/docs/support#enterprise)
* Available on [AWS Marketplace](https://aws.amazon.com/marketplace/pp/prodview-luy3krhkpjne4). Your PlanetScale purchase through the AWS Marketplace and the resources you use on PlanetScale will qualify against your EDP commitment.

## How do I get PlanetScale Managed?

If you are interested in seeing if PlanetScale Managed fits your needs, [contact us](https://planetscale.com/contact), and we can chat more about your requirements and see if PlanetScale Managed is a good fit for you.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale Managed on AWS overview
Source: https://planetscale.com/docs/vitess/managed/aws

PlanetScale Managed on Amazon Web Services (AWS) is a single-tenant deployment of PlanetScale in your AWS organization within an isolated AWS Organizations member account.

## Overview

In this configuration, you can use the same API, CLI, and web interface that PlanetScale offers, with the benefit of running entirely in an AWS Organizations member account that you own and PlanetScale manages for you.

## Architecture

The PlanetScale data plane is deployed inside of a PlanetScale-controlled AWS Organizations member account in your AWS organization.
The Vitess cluster will run within this member account, orchestrated via Kubernetes.

We distribute components of the cluster across three AWS availability zones within your selected region to ensure high availability.
You can deploy PlanetScale Managed to any AWS region with at least three availability zones, including those not supported by the PlanetScale self-serve product.

Backups, part of the data plane, are stored in S3 inside the same member account.
PlanetScale Managed uses isolated Amazon Elastic Compute Cloud (Amazon EC2) instances as part of the deployment.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/aws-arch-diagram.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=dc1fa5d01017c5e105d9f2fb59aa5946" alt="Architecture diagram for PlanetScale Managed in AWS" data-og-width="1664" width="1664" data-og-height="1118" height="1118" data-path="docs/images/assets/docs/managed/aws/aws-arch-diagram.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/aws-arch-diagram.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=27ad4832e817b50d5b855fe9b7b8e8df 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/aws-arch-diagram.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=b89cd6bb954ea158e4ecfb8f00ebf135 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/aws-arch-diagram.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=c03abc4f85055d93c4696c4b64a8beef 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/aws-arch-diagram.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=8b249ab8f8116bd35c59d9011a57e84e 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/aws-arch-diagram.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=62b371d1d5b163b0684c6b24a32baa22 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/aws-arch-diagram.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=fca36834076de28e021bdf546e0fd490 2500w" />
</Frame>

Your database lives entirely inside a dedicated AWS Organizations member account within your AWS organization.
PlanetScale will not have access to other member accounts nor your organization-level settings within AWS.
Outside of your AWS organization, we run the PlanetScale control plane, which includes the PlanetScale API and web application, including the dashboard you see at `app.planetscale.com`.

The Vitess cluster running inside Kubernetes is composed of a number of Vitess Components.
All incoming queries are received by one of the **VTGates**, which then routes them to the appropriate **VTTablet**.
The VTGates, VTTablets, and MySQL instances are distributed across 3 availability zones.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/aws-vitess.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=ee039f8d95d7fea1226360744d2e0f94" alt="Diagram of Vitess cluster on AWS" data-og-width="2184" width="2184" data-og-height="1626" height="1626" data-path="docs/images/assets/docs/managed/aws/aws-vitess.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/aws-vitess.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=96d2ad24d694a21b1d2d36bcf49b2a53 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/aws-vitess.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=9c6dc87d00e2d360b19ac3cd4ca60347 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/aws-vitess.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=25b679054b4364a7e567227820821a89 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/aws-vitess.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=6c5a37d48033097ec81c9d2db79ee0b6 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/aws-vitess.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=5f226e36dbcbe4ffabdaf9b704bd939e 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/aws-vitess.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=d3617b2d36b0c085b66ddc4e7f38e16b 2500w" />
</Frame>

Several additional required Vitess components are run in the Kubernetes cluster as well.
The topology server keeps track of cluster configuration.
**VTOrc** monitors cluster health and handles repairs, including managing automatic failover in case of an issue with a primary.
**vtctld** along with the client **vtctl** can be used to make changes to the cluster configuration and run workflows.

## Security and compliance

PlanetScale Managed is an excellent option for organizations with specific security and compliance requirements.

You own the AWS organization and member account that PlanetScale is deployed within an isolated architecture. This differs from when your PlanetScale database is deployed within our AWS organizations.

<Note>
  PlanetScale manages the entire member account and can NOT support customers running Terraform or other configuration management in the member account.
</Note>

### PCI compliance

Along with System and Organization Controls (SOC) 2 Type 2 and other [security and compliance](/docs/security) practices that PlanetScale has been issued and follows, PlanetScale Managed has been issued an Attestation of Compliance (AoC) and Report on Compliance (RoC), certifying our compliance with the PCI DSS 4.0 as a [Level 1 Service Provider](https://www.pcisecuritystandards.org/glossary/service-provider/). This enables PlanetScale Managed to be used via a shared responsibility model across merchants, acquirers, issuers, and other roles in storing and processing cardholder data.

<Note>
  If you have any questions or concerns related to the security and compliance of PlanetScale Managed, please [contact us](https://planetscale.com/contact), and we will be happy to discuss them further.
</Note>

### AWS PrivateLink

By default, all connections are encrypted, but public. Optionally, you also have the option to use private database connectivity through [AWS PrivateLink](/docs/vitess/managed/aws/privatelink).

### Fully private network isolation

You can also turn off public database access with a dual AWS PrivateLink setup. PlanetScale's control plane will talk to your member account over AWS PrivateLink, and your VPCs will also communicate with your database over AWS PrivateLink. Please get in touch with your PlanetScale Account Manager for more information on how to set up fully private network isolation.

### Third-account customer-controlled public key infrastructure

PlanetScale Managed on AWS supports public key infrastructure (PKI) services. PlanetScale Managed customers can provide PlanetScale the use of a set of customer-managed keys in a third AWS account inside your organization. This third account is controlled by you, the customer. PlanetScale has no administrative access. Your organization is the custodian for this key material. PlanetScale uses the customer-managed keys to encrypt EBS volumes, S3 buckets, and for envelope encryption of backups.

## Billing

With any of the PlanetScale Enterprise offerings, including PlanetScale Managed, you have the option to purchase PlanetScale through the [AWS Marketplace](https://aws.amazon.com/marketplace/pp/prodview-luy3krhkpjne4). In addition to this, the resources you use on PlanetScale will qualify against your EDP commitment.

<Note>
  If you have any billing-related questions for PlanetScale Managed, please [contact us](https://planetscale.com/contact), and we will be happy to discuss them further.
</Note>

## Getting started with PlanetScale Managed in AWS

If you want to see what is involved in getting set up with PlanetScale Managed in AWS, you can see the [AWS set up documentation](/docs/vitess/managed/aws/getting-started).

If you are interested in exploring PlanetScale Managed further, please [contact us](https://planetscale.com/contact), and we can chat more about your requirements and see if PlanetScale Managed is a good fit for you.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Back up and restore in AWS
Source: https://planetscale.com/docs/vitess/managed/aws/back-up-and-restore

PlanetScale Managed backup and restore functions like the hosted PlanetScale product. For more info, see [how to create, schedule, and restore backups for your PlanetScale databases](/docs/vitess/backups).

To learn more about the backup and restore access levels, see the [database level permissions documentation](/docs/security/access-control#database-level-permissions).

By default, databases are automatically backed up once per day to an S3 bucket in the customer's AWS Organizations member account. This default can be adjusted when working with PlanetScale Support. Configuring and validating additional backup frequencies is the customer's responsibility.

During the initial provisioning process, PlanetScale applies an S3 configuration to ensure that backups are encrypted at rest on Amazon S3.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Set up PlanetScale Managed in AWS
Source: https://planetscale.com/docs/vitess/managed/aws/getting-started

The following guide will walk you through setting up a PlanetScale Managed cluster in your Amazon Web Services (AWS) organization.

## Overview

If you have any questions while working through this documentation, contact your PlanetScale Solutions Engineer for assistance.

<Note>
  This guide is only intended for PlanetScale Managed customers currently working with the PlanetScale team. You cannot set PlanetScale Managed up on your own without PlanetScale enabling it for your organization. If you are interested in [PlanetScale Managed](/docs/vitess/managed), please [contact us](https://planetscale.com/contact).
</Note>

## Step 1: Account requirements

A new AWS Organizations member account must be set up following this documentation to successfully bootstrap a new PlanetScale Managed cluster. An existing AWS organization is required to proceed with this guide.

### Dedicated AWS Organizations member account

PlanetScale Managed requires the use of a standalone AWS Organizations member account in Amazon Web Services. This account should not have any existing resources running within it.

The [creating a member account in your organization document](https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_create.html#orgs_manage_accounts_create-new) covers how to create a new member account in an existing AWS organization. The document also includes the required permissions to create a member account in your AWS organization.

### Modification of accounts

Once the member account is handed over to PlanetScale via granting IAM permissions, it should not be modified. Issues caused by modifications of the member account or its resources void the PlanetScale Managed SLA. Contact [support@planetscale.com](/docs/vitess/managed/aws/) to discuss configuration changes or customization.

### Recommendations

During the initial provisioning process, PlanetScale applies the following recommendations to the AWS Organizations member account and we recommend that a customer not change these once the member account has been created:

* **Encryption by default:** PlanetScale enables EBS encryption by default using the AWS-managed keys in the relevant regions in the member account. If you want to change this behavior, please consult PlanetScale before the initial deployment process.
* **AWS CloudTrail + AWS Config:** Enable AWS CloudTrail for management events and resource tracking using AWS Config.

### PCI Compliance

Customers of PlanetScale Managed should ensure the following additional configurations are applied and maintained to ensure that the customer environment remains PCI-compliant for the storage and protection of cardholder data:

#### Local Authentication Parameters

PlanetScale does not set specific authentication parameters for accessing the customer database during initial provisioning. To maintain compliance with PCI requirements 8.2 and 8.3, it is the customer's responsibility to set the following authentication parameters in line with the respective requirements set by the PCI Data Security Standards:

* Account lockout
* Account lockout duration
* Password minimum length
* Password complexity
* Password expiration
* Password history

#### Log Level Configuration

The PlanetScale-controlled AWS Organizations member account will be pre-configured by PlanetScale with [AWS CloudTrail](https://aws.amazon.com/cloudtrail/) enabled and configured to emit logging events from the customer application. As PlanetScale does not retain access to these logs after the account is configured, to maintain compliance with PCI requirement 10.2.1.1 (audit logs capture all individual user access to cardholder data), it is the customer's responsibility to ensure this logging remains enabled and to regularly review and verify the following events:

* All administrative action
* Accessing cardholder data
* Accessing audit trails
* Invalid access attempts
* Successful access attempts
* Elevation of privileges
* Creation/deletion/changing an account with admin privileges
* Start/stop/pausing of audit logs

As a best practice, it is recommended that these logs be captured and continuously analyzed by a Security Information & Event Management (SIEM) platform.

## Step 2: Bootstrap with CloudFormation

We've created a CloudFormation template to complete the setup of required permissions in your AWS Organizations member account.

Save the following as `planetscale-bootstrap.json`:

```json expandable theme={null}
{
  "Resources": {
    "GrantTerraformRunnerAccess": {
      "Type": "AWS::IAM::Role",
      "DeletionPolicy": "Retain",
      "Properties": {
        "RoleName": "TerraformRunner",
        "ManagedPolicyArns": ["arn:aws:iam::aws:policy/AdministratorAccess"],
        "AssumeRolePolicyDocument": {
          "Statement": [
            {
              "Effect": "Allow",
              "Principal": {
                "AWS": [
                  "arn:aws:iam::313573332105:role/aws-reserved/sso.amazonaws.com/AWSReservedSSO_Ops_feec88bc3aad314d"
                ]
              },
              "Action": ["sts:AssumeRole"]
            }
          ]
        }
      }
    },
    "GrantOpsAccess": {
      "Type": "AWS::IAM::Role",
      "DeletionPolicy": "Retain",
      "Properties": {
        "RoleName": "PlanetscaleOps",
        "ManagedPolicyArns": ["arn:aws:iam::aws:policy/AdministratorAccess"],
        "AssumeRolePolicyDocument": {
          "Statement": [
            {
              "Effect": "Allow",
              "Principal": {
                "AWS": [
                  "arn:aws:iam::867309876077:role/aws-reserved/sso.amazonaws.com/AWSReservedSSO_Ops_f1d00b216d43a785"
                ]
              },
              "Action": ["sts:AssumeRole"]
            }
          ]
        }
      }
    }
  }
}
```

Next, apply the CloudFormation template as a new stack:

```bash  theme={null}
aws cloudformation create-stack --stack-name planetscale-bootstrap \\
  --template-body file://planetscale-bootstrap.json \\
  --capabilities CAPABILITY_NAMED_IAM
```

Let your Solutions Engineer know once the new stack reaches the `CREATED` state in AWS.

## Step 3: Requesting an initial quota increase

By default, AWS may provision new member accounts with EC2 On-Demand quotas that may be too small for:

* PlanetScale's initial provisioning process
* The databases you may want to provision on your PlanetScale Managed cluster

Although the PlanetScale Support and Operations teams will have the ability to request quota increases on your behalf after you give us access to the AWS Organizations member account, we recommend that you review the following quotas and request increases as necessary, as requesting quota increases later will delay the process:

* [Running On-Demand Standard (A, C, D, H, I, M, R, T, Z) instances](https://console.aws.amazon.com/servicequotas/home/services/ec2/quotas/L-1216C47A) — Since PlanetScale Managed typically runs small instances by default, it is generally best to set this high enough to avoid any later issues. At least **300** is sufficient for most customers.
* [Storage for General Purpose SSD (gp3) volumes, in TiB](https://console.aws.amazon.com/servicequotas/home/services/ebs/quotas/L-7A658B76) — Note that we typically will keep 3 copies of all data (primary plus 2 replicas), so you have to consider that here. We will also create volumes at backup time, which could be a temporary 4th copy for quota purposes. **50** TiB should be sufficient for most customers.
* [Storage modifications for General Purpose SSD (gp3) volumes, in TiB](https://console.aws.amazon.com/servicequotas/home/services/ebs/quotas/L-59C8FC87) — Ensure this is large enough, if possible, to cover your largest database so that storage volume performance modifications can be made (if necessary), without replacing volumes. Again, **50** TiB or more should be sufficient in most cases.

You can read more about how to request a quota increase in the [AWS requesting a quota increase documentation](https://docs.aws.amazon.com/servicequotas/latest/userguide/request-quota-increase.html).

<Note>
  If you have AWS Enterprise Support, you can contact your account manager to expedite quota requests; otherwise, quota requests above your current limit can take at least one business day. There is also a limit on how often you can make quota requests. A quota request can only be made once every 6 hours.
</Note>

## Step 4: Initiating the provisioning process

Once the CloudFormation stack has returned as `CREATED`, notify your Solutions Engineer, providing them the following information:

* The name of the organization that you have created on `app.planetscale.com`.
* The AWS Account ID of the member account, which can be found by using one of the choices in the [AWS account ID and alias documentation](https://docs.aws.amazon.com/IAM/latest/UserGuide/console_account-alias.html).
* A confirmation of the region(s) that you have chosen for the deployment to reside in. The canonical list of regions can be found in the [AWS Regions and Zones documentation](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#concepts-available-regions).

Once your Solutions Engineer receives this information, they will forward it to the team responsible for provisioning your deployment. Provisioning the deployment takes PlanetScale, on average, one business day.

Once the deployment has been provisioned, your Solutions Engineer will contact you to confirm that your team can start creating databases.

<Note>
  Optionally, PlanetScale can connect you to your databases via [AWS PrivateLink](https://aws.amazon.com/privatelink/) with PlanetScale Managed. See the [AWS PrivateLink documentation](/docs/vitess/managed/aws/privatelink) for more information on establishing a PrivateLink connection.
</Note>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Set up AWS PrivateLink with PlanetScale Managed
Source: https://planetscale.com/docs/vitess/managed/aws/privatelink

PlanetScale Managed can connect you to your databases via [AWS PrivateLink](https://aws.amazon.com/privatelink/). The following guide describes how PlanetScale Managed with AWS PrivateLink works and how to set it up.

## Overview

If you are on a Scaler Pro plan and would like to set up AWS PrivateLink, see our [Private connections documentation](/docs/vitess/connecting/private-connections).

## How PlanetScale Managed and AWS PrivateLink work

AWS PrivateLink requires two components:

* A [VPC endpoint service](https://docs.aws.amazon.com/vpc/latest/privatelink/endpoint-service-overview.html) deployed in the AWS Organizations member account that PlanetScale controls.
* A [VPC endpoint interface](https://docs.aws.amazon.com/vpc/latest/privatelink/vpce-interface.html), sometimes referred to as a "VPC endpoint" in AWS, deployed in the account that your applications operate in.

Once both components are operating correctly, the EC2 instances in the VPC that the VPC endpoint has been assigned to will leverage [Private DNS](https://docs.aws.amazon.com/vpc/latest/privatelink/vpce-interface.html#vpce-private-dns) to connect to your VPC endpoint instead of the publicly accessible endpoint.

The connection strings that PlanetScale provides will operate successfully inside and outside your VPC, creating PrivateLink connections inside of your VPC and regular connections outside of your VPC.

## Step 1: Initiating the setup process

There is no fully automated way to establish a PrivateLink connection. If you would like to initiate the process, please get in touch with your Solutions Engineer and let them know the [AWS Account ID](https://docs.aws.amazon.com/IAM/latest/UserGuide/console_account-alias.html) that you intend to create the [VPC endpoint](https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints.html) in.

Once they receive your AWS Account ID and forward it to the team responsible for provisioning your deployment, the team will provide the Solutions Engineer (and ultimately you) with the Service Name of the [VPC endpoint service](https://docs.aws.amazon.com/vpc/latest/privatelink/endpoint-service-overview.html) that will be responsible for accepting your connection.

<Note>
  It is important to keep the service name in your records. It is the only piece of information you need to input when creating your VPC endpoint.
</Note>

## Step 2: Establishing a VPC endpoint connection

<Warning>
  Only proceed to the next steps once a PlanetScale Solutions Engineer has provided the service name and confirmed cross-account authentication has been configured.
</Warning>

The following steps are an example of establishing a VPC endpoint connection in the AWS Console. In this example, the customer has requested that their deployment be in the `eu-west-1` region.

When you go through the steps, make sure that you have selected the region that matches the region that your PlanetScale Managed cluster deployment has been provisioned into.

<Steps>
  <Step>
    Navigate to the Endpoints section on the Virtual Private Cloud page and select "**Create Endpoint**."

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/nav_to_splash.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=410a8cc156a78a51d8e5f8ea48a78643" alt="nav_to_splash" data-og-width="1572" width="1572" data-og-height="1090" height="1090" data-path="docs/images/assets/docs/managed/aws/privatelink/nav_to_splash.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/nav_to_splash.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=b5ac93ebfdc41198a7a0a1cef542348d 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/nav_to_splash.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=e62c2706799fb13f22008dde7a3d1714 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/nav_to_splash.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=d5da2ea8fabc40805aa7bfcd26fdad2f 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/nav_to_splash.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=e29d28e4406495aa718adac5ee86976d 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/nav_to_splash.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=1201e6dae57dc8a8cfba3fdeb2438523 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/nav_to_splash.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=08ec647abfea620358d53f7cc9de5a4d 2500w" />
    </Frame>
  </Step>

  <Step>
    Select the "**Find service by name**" selector, input the provided Service Name, and select the "**Verify**" button.

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/verified.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=9fc8c13b137c6316427fbd7386bbed5b" alt="verified" data-og-width="996" width="996" data-og-height="650" height="650" data-path="docs/images/assets/docs/managed/aws/privatelink/verified.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/verified.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=a112f9dbf7fa47fa30a1c271d41ce1b3 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/verified.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=811903960eb22be2519097ab702fdb79 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/verified.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=078ca796d8f3967f2cda0faa9e8ef133 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/verified.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=2c0ee8d9a3dfed3f0c74c0ff796e1bb5 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/verified.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=969b2d29bcad6224189b7bb7423ed43c 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/verified.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=0904bea0888bae224575eba102bbf52f 2500w" />
    </Frame>
  </Step>

  <Step>
    Select the VPC in the drop-down where you wish to provision this VPC endpoint and the relevant subnets inside your VPC.

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/cyo_vpc.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=7c83fac18a4586b519ceac3de61cba6b" alt="cyo_vpc" data-og-width="1274" width="1274" data-og-height="548" height="548" data-path="docs/images/assets/docs/managed/aws/privatelink/cyo_vpc.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/cyo_vpc.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=dfa500fe0bca2819e6f780dedde981aa 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/cyo_vpc.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=bf772e5f881647fe1d1299b76b68cef9 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/cyo_vpc.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=5789eda2e4fbe18b34c2fbcf10926bb1 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/cyo_vpc.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=14f22d02e7f597c0f309fb7c84f11a31 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/cyo_vpc.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=d5401b743509e15e12fa4144d6645790 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/cyo_vpc.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=1ee58d0ac57df6d6d8c0495956e88d8b 2500w" />
    </Frame>
  </Step>

  <Step>
    Select the "**Enable DNS Name**" checkbox. Take note of the value of your "**Private DNS Name**" field. That is how we will verify that the connection is operating successfully.

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/enable_dns_name.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=5076b63698f59daff686dea822ed84fd" alt="enable_dns_name" data-og-width="966" width="966" data-og-height="438" height="438" data-path="docs/images/assets/docs/managed/aws/privatelink/enable_dns_name.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/enable_dns_name.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=e40f181d7aa32649c2956d1a8776cafb 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/enable_dns_name.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=eeea9704de4430e098f226dfee20ce27 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/enable_dns_name.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=e64ad338e8787ce8b92849db48408f91 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/enable_dns_name.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=fdaf45c01259dfb65249ac5459fc36c6 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/enable_dns_name.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=9168612958492030b7e2d608524e2436 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/enable_dns_name.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=12ae8174c2ea21d94cec152997d3fb97 2500w" />
    </Frame>
  </Step>

  <Step>
    Select the relevant Security Groups you want your VPC endpoint to adhere to.

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/select_sgs.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=77fa4fc01f3426cc2061917468bfc018" alt="select_sgs" data-og-width="1654" width="1654" data-og-height="726" height="726" data-path="docs/images/assets/docs/managed/aws/privatelink/select_sgs.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/select_sgs.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=9da44222e7be9b41a260c511c4c1be0f 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/select_sgs.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=5007ad5420a2c81308baaae4b981f581 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/select_sgs.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=3bc65832787751cbfb4cef2fa6e9e34e 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/select_sgs.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=3d6ed64b1ee79fd95ecbf95afd75cc04 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/select_sgs.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=5791657e835a772fb75d60d6a8fae37a 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/select_sgs.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=d28da7cfe21579e80698f183b6d629b8 2500w" />
    </Frame>
  </Step>

  <Step>
    Add as many tags as your heart desires (up to 50) and select "**Create endpoint**."

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/click_it.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=166bab6ba2d78b66b7e1af9263e176ac" alt="click_it" data-og-width="1226" width="1226" data-og-height="412" height="412" data-path="docs/images/assets/docs/managed/aws/privatelink/click_it.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/click_it.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=6abcefddababd2a0360a63502e7444c7 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/click_it.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=cc3e5f7e2896ff32eeef541d3e65791b 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/click_it.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=ffff1430be432c2d637671bce1bcd522 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/click_it.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=283a0ec0e315c96d32def3b63f052f8c 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/click_it.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=286070e9498b13efecf8c49e26cc7d69 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/click_it.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=b23a5cf96045ceeaed859c268ed3a4af 2500w" />
    </Frame>
  </Step>

  <Step>
    The "Creating" spinner will spin momentarily and then deliver you the news of the endpoint creation. You should see a VPC endpoint in the `pending` state if it was successful. If the creation failed, record the reason and consult your Solutions Engineer.

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/pending.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=eb05aa70bcff44fdaab0bd6e9c208abe" alt="pending" data-og-width="740" width="740" data-og-height="240" height="240" data-path="docs/images/assets/docs/managed/aws/privatelink/pending.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/pending.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=19aea33d536adf90f9d08b0a28b4f662 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/pending.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=fb09861e3d2a522927f3d6cb3e6fe727 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/pending.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=98c7a5e21556f6b58e8159c33675b5b5 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/pending.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=c2bede53482994d46ae12065fe790264 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/pending.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=10c582399a04b04efc22a0596cd408a0 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/pending.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=99e6c2855c127163eda8b972140d89a2 2500w" />
    </Frame>
  </Step>

  <Step>
    After 2-10 minutes (make sure to refresh), your VPC endpoint will report an `available` state.

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/available.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=3653f9ed7f50fe9c27bd2a796e3078dd" alt="available" data-og-width="766" width="766" data-og-height="210" height="210" data-path="docs/images/assets/docs/managed/aws/privatelink/available.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/available.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=4ef5fe4b690fc9879b34d4a080333d6d 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/available.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=f884cea3db162cfca24e08e538167d35 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/available.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=5d20504baaa97fd2a057b94c10b3931d 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/available.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=c8489f7c25e7a3a1b8432558e12cd598 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/available.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=776db12391e5ca1c516b424d6493cff4 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/available.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=b8f805c01c9bdcf4e68ff2848570a831 2500w" />
    </Frame>
  </Step>
</Steps>

## Step 3: Verifying a VPC endpoint connection

PlanetScale publishes a [wildcard DNS record](https://en.wikipedia.org/wiki/Wildcard_DNS_record) for your private region. AWS PrivateLink will override the DNS record in your VPC to point to your VPC endpoint instead of the publicly published record.

To verify that the DNS override is working correctly, issue the following `dig` command using the value of your "Private DNS Name" instead of the value in the example:

```shell  theme={null}
dig +short wildcard.frzzbztuqm3h-euwest1-1.psdb.cloud
172.31.16.197
172.31.13.7
```

If your `dig` command returns a set of static IP addresses, your VPC Endpoint connection is operating successfully. If it returns a `CNAME` to an ELB record (for example, something like `something.elb.region.amazoneaws.com`), your connection is not operating successfully, and you should consult your Solutions Engineer.

Once you've verified that your connection is operating successfully, you will need to verify that you can reach a database you've provisioned:

<Steps>
  <Step>
    [Create a connection string](/docs/vitess/connecting/connection-strings#creating-a-password) for a PlanetScale database using the "**Connect**" button. Select "**MySQL CLI**" and copy the command.
  </Step>

  <Step>
    Paste your MySQL CLI command into a command prompt of an EC2 instance running in your VPC with the `mysql-client` package installed:

    ```shell  theme={null}
    mysql -h <HOST_NAME> -u <USERNAME> -p --ssl-mode=VERIFY_IDENTITY --ssl-ca=/etc/pki/tls/certs/ca-bundle.crt
    Enter password:
    ...

    mysql>
    ```
  </Step>
</Steps>

<Info>
  The correct path for the CA root configuration for the `--ssl-ca` flag depends on your operating system. See the [CA root configuration documentation](/docs/vitess/connecting/secure-connections#ca-root-configuration) for more the correct path.
</Info>

If you receive the `mysql>` prompt, your connection is operating successfully, and you have just confirmed that your connections to PlanetScale will be established through AWS PrivateLink. If you do not receive the `mysql>` prompt, please consult your Solutions Engineer.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Set up AWS Reverse PrivateLink with PlanetScale Managed
Source: https://planetscale.com/docs/vitess/managed/aws/reverse-privatelink

[PlanetScale Managed](/docs/vitess/managed/aws) can connect to your existing databases via AWS PrivateLink for the purposes of data imports.

Often, one of the first tasks for a new Managed deployment is to import data from an existing MySQL database housed in a separate AWS Organizations member account.

This guide explains how to set up AWS PrivateLink components that enable cross-account communication to facilitate this process. In PrivateLink parlance, the MySQL database that serves as the import source is known as the "producer", and PlanetScale is the "consumer".

While there are a number of different ways to set up and configure PrivateLink components, in this guide we'll be using the AWS CLI tool.

## How PlanetScale Managed and AWS PrivateLink work

Broadly speaking, there are three major components to this PrivateLink setup:

* A [VPC Endpoint Service](https://docs.aws.amazon.com/vpc/latest/privatelink/endpoint-service-overview.html) conceptually lives in the producer's VPC and AWS account. Once configured, it allows authorized principals (e.g. AWS accounts, IAM users) to establish a connection to an existing VPC Endpoint Service ID.

* A [VPC Endpoint Interface](https://docs.aws.amazon.com/vpc/latest/privatelink/vpce-interface.html) exists on the consumer side, and once configured with an endpoint service address exposes an internal IP address and port which consumers may use for cross-account communication.

* A [Network Load Balancer](https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html) (NLB) is created on the producer side, and by managing the IP target group on this NLB you can choose which internal services to expose via the PrivateLink Endpoint Service.

## Starting state

Below is a simplified diagram of our initial state. We've got two separate accounts, each with their own VPCs and availability zones. We're using AWS [AZ IDs](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html#az-ids) as well as of AZ names to ensure that we're always referring to the correct AZ, as AZ names are not consistent across AWS accounts. On the producer side, we have a network subnet inside each AZ.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/starting-state.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=5deee8a67f89d51fa74135640d3bbe9a" alt="starting state" data-og-width="1488" width="1488" data-og-height="1478" height="1478" data-path="docs/images/assets/docs/managed/aws/privatelink/starting-state.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/starting-state.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=00a77eb5a5fc95c2e68b2de6e0db73da 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/starting-state.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=b9500773b6bc70c6159f9344ce919de3 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/starting-state.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=e213ec13620b59b9f60f82244a8bdb14 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/starting-state.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=f11e87eaae534618eb57eb53aeb79338 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/starting-state.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=ee9b65ee2504663086851148f5856894 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/starting-state.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=5de25c745ba323f49369f419a442dc88 2500w" />
</Frame>

## Create and configure the NLB

First, create the Network Load Balancer in the producer AWS account. You want the NLB to be available in each of the three AZs that the two accounts share. This is done by adding the ids of the three subnets associated with those AZs using the `--subnets` option.

```bash  theme={null}
aws elbv2 create-load-balancer \
  --name pl-nlb \
  --scheme internal \
  --type network \
  --subnets subnet-cafed00d subnet-c0ffee subnet-cafef00d
  --security-groups sg-f00
```

If you want to attach a security group to your NLB, you must do so at creation time. This allows you to restrict the inbound protocol and port range to `TCP:3306`. This command will return an ARN that uniquely identifies the newly created NLB. You'll need this later.

### Create an IP target group

Run the following to create an empty IP target group on `TCP:3306` in the producer VPC:

```bash  theme={null}
aws elbv2 create-target-group \
  --name pl-nlb-target-group \
  --protocol TCP \
  --port 3306 \
  --vpc-id vpc-f00 \
  --target-type ip
```

This will return an ARN that uniquely identifies the target group.

### Register a target

This step adds the IP address of the existing Primary RDS instance to the target group. You want to make sure that only the RDS writer is registered as a target.

```bash  theme={null}
aws elbv2 register-targets \
  --target-group-arn <arn>
  --targets Id=10.0.0.1
```

### Create a listener

Finally, associate the target group NLB by creating a listener.

```bash  theme={null}
aws elbv2 create-listener \
   --load-balancer-arn <arn>\
   --protocol TCP \
   --port 3306 \
   --default-actions Type=forward,TargetGroupArn=<arn>
```

Now that you've created the NLB and configured it, you can add the NLB to the diagram. Although on the diagram it looks like there are three separate NLBs, it's meant to represent a single NLB object that has network interfaces in three different subnets.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/create-and-configure-nlb.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=112c3d0ba385b85f81533d16c84c30ff" alt="create and configure nlb" data-og-width="1752" width="1752" data-og-height="1630" height="1630" data-path="docs/images/assets/docs/managed/aws/privatelink/create-and-configure-nlb.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/create-and-configure-nlb.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=976c99c5481760e33889dc382e889689 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/create-and-configure-nlb.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=2a4ad955209557756fbdfc6aaf9ca922 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/create-and-configure-nlb.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=b4ed0378ff16f03c08f48813d1228374 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/create-and-configure-nlb.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=d4d06f6f4b9ecf1a8ea9765b8a587a92 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/create-and-configure-nlb.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=1c1243a4da82e114b7cab71e6e8e1da6 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/create-and-configure-nlb.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=a1a9dc6424e909834d34753ca1afcd4c 2500w" />
</Frame>

## Create and configure the PrivateLink VPC Endpoint Service

Now, let's configure the endpoint service. You'll need the ARN of your load balancer for this.

```bash  theme={null}
aws ec2 create-vpc-endpoint-service-configuration \
  --network-load-balancer-arn <arn> \
  --no-acceptance-required
```

This will return a VPC service id.

## Allow inbound traffic from VPC's subnet

After setting up the NLB and the VPC Endpoint Service, you need to ensure that the security group attached to the NLB permits inbound traffic from the subnet(s) of the consumer VPC where the interface VPC endpoint resides.

<Steps>
  <Step>
    Locate the security group associated with your NLB
  </Step>

  <Step>
    Edit Inbound Rules:

    * Protocol: TCP
    * Port Range: Specify the port your MySQL database is listening on (default is 3306)
    * Source: Enter the CIDR block of the consumer VPC's subnet (e.g., 10.0.1.0/24)
  </Step>

  <Step>
    Save changes to apply the updated rules
  </Step>
</Steps>

### Configure service permissions

You only want to allow incoming connections on this endpoint service from the PlanetScale Managed AWS Organizations member account. This step requires the account number.

```bash  theme={null}
aws ec2 modify-vpc-endpoint-service-permissions \
  --service-id vpce-svc-f00 \
  --add-allowed-principals '["arn:aws:iam::123456789012:root"]'
```

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/create-and-configure-vpce-svc.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=39abee3e37dd45e21badebdda42c98e2" alt="create and configure vpce svc" data-og-width="2102" width="2102" data-og-height="1630" height="1630" data-path="docs/images/assets/docs/managed/aws/privatelink/create-and-configure-vpce-svc.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/create-and-configure-vpce-svc.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=82bb056ccc1767ef869bb714cd3985e2 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/create-and-configure-vpce-svc.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=ec2505a1ac59a309b1ae84c6c6d935cf 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/create-and-configure-vpce-svc.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=794a182c9d5b9aed881a900e1bcc1017 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/create-and-configure-vpce-svc.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=5db59c752c91a658f5c54af37b8c82e7 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/create-and-configure-vpce-svc.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=9c23e06ded7ceb964dfd70d692d2c0d8 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/create-and-configure-vpce-svc.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=2480fbcaf1f5af77cbcac722abd076b0 2500w" />
</Frame>

## Wrapping Up

Once you've communicated your newly created VPC Service Endpoint ID to your Solutions Engineer, the PlanetScale Engineering team can then complete the rest of the process. This involves creating, configuring, and testing the PrivateLink VPC Interface Endpoint. The diagram below illustrates the completed system.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/wrapping-up.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=4d12fa0711cc37762b0b6f03760e6318" alt="wrapping up" data-og-width="2470" width="2470" data-og-height="2218" height="2218" data-path="docs/images/assets/docs/managed/aws/privatelink/wrapping-up.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/wrapping-up.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=13a870bfd4efbf456aab6d158c516ee5 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/wrapping-up.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=bdf7d9967162ded1ddf9a0128fe7693f 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/wrapping-up.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=db83ddf96c82de2b0ea979c79974909e 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/wrapping-up.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=25caca1dc84219490dd2f5bcde43d8d0 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/wrapping-up.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=ba3eb7f4b549a4a2983b104bec8af01d 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/aws/privatelink/wrapping-up.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=3943c1f3dc0261b433a5cc8f0f1c68e0 2500w" />
</Frame>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Cloud accounts and contents
Source: https://planetscale.com/docs/vitess/managed/aws/security-and-access/cloud-accounts-and-contents



## Cloud accounts

PlanetScale is not responsible for the general configuration of services shared across the cloud organization in which the AWS Organizations member account or GCP project is provisioned. The customer is solely responsible for managing account access outside that granted to PlanetScale.

## Content restrictions

The data stored in PlanetScale Managed databases is contained entirely in the customer's AWS Organizations member account or GCP project.

Customers are responsible for all content that is stored in the databases they have created.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Data requests
Source: https://planetscale.com/docs/vitess/managed/aws/security-and-access/data-requests



## End-user data requests

Some laws require you and your company to allow your end-users to control their personal data. PlanetScale allows you to access, export, or delete their personal data.

Reach out to [support@planetscale.com](/docs/vitess/managed/) for additional support with accessing, exporting, or deleting personal data.

## Data portability

PlanetScale does not offer the migration of specific metadata between regions. PlanetScale does support migrating a database between regions for PlanetScale Managed.

## Data deletion

Users can request to have their personal data removed by contacting [support@planetscale.com](/docs/vitess/managed/).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Schema snapshots
Source: https://planetscale.com/docs/vitess/managed/aws/security-and-access/schema-snapshots



Snapshots of database schemas within PlanetScale Managed accounts are transferred to PlanetScale's control plane to facilitate features requiring visibility into schemas. Schema snapshots include the structure of databases managed by PlanetScale, but do not include any row-level data.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# User management
Source: https://planetscale.com/docs/vitess/managed/aws/security-and-access/user-management



## Initial onboarding

### Administrative onboarding

The customer's initial administrative user creates an organization on PlanetScale. Administrative accounts have the `Organization Administrator` role assigned.

### User onboarding

Users can be onboarded either manually or using single sign-on. Manual onboarding is handled by the Administrator once they are initially granted access. Users managed via SSO are onboarded once the SSO provider is connected and configured.

### Single sign-on

PlanetScale Managed requires single sign-on (SSO) for the API and web interface, enabling organizations to manage access through their existing directory services.

You can read more about single sign-on and how to set it up in the [PlanetScale single sign-on documentation](/docs/security/sso).

## Access levels

PlanetScale currently supports three different roles inside of organizations:

* `Organization Administrator`
* `Organization Member`
* `Database Administrator`

See the [PlanetScale access control documentation](/docs/security/access-control) for a further breakdown of each role's permissions.

## Separation of accounts

PlanetScale Managed provides integration with numerous single sign-on providers. Users can have entirely separate personal and corporate accounts with PlanetScale when their organization uses SSO.

It is up to the customer to ensure that they maintain their SSO setup and do not invite or allow employees to use any other authentication method to access PlanetScale.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale Managed on GCP overview
Source: https://planetscale.com/docs/vitess/managed/gcp

PlanetScale Managed on Google Cloud Platform (GCP) is a single-tenant deployment of PlanetScale in your GCP organization within an isolated project.

## Overview

In this configuration, you can use the same API, CLI, and web interface that PlanetScale offers, with the benefit of running entirely in a GCP project that you own and PlanetScale manages for you.

## Architecture

As you can see in the architecture diagram below, the PlanetScale data plane is deployed inside of a PlanetScale-controlled project in your GCP organization.
The Vitess cluster will run within this project, orchestrated by Kubernetes.

We distribute components of the cluster across three GCP zones within a region to ensure high availability.
You can deploy PlanetScale Managed to any GCP region with at least three zones, including zones not supported by the PlanetScale self-serve product, so long as the region supports the required GCP services (including but not limited to Google Compute Engine (GCE), Google Kubernetes Engine (GKE), Cloud Storage, Persistent Disk, Cloud Key Management Service (Cloud KMS), Cloud Logging).

Backups, part of the data plane, are stored in Cloud Storage inside the same project.
PlanetScale Managed uses isolated GCE instances as part of the deployment.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/gcp/gcp-arch-diagram.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=a2db14dd5cef1e279da0be5e44839df8" alt="Architecture diagram for PlanetScale Managed in GCP" data-og-width="1664" width="1664" data-og-height="1118" height="1118" data-path="docs/images/assets/docs/managed/gcp/gcp-arch-diagram.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/gcp/gcp-arch-diagram.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=0069c5641129d032f7530619cefb8f37 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/gcp/gcp-arch-diagram.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=b8197c865136ae812ff1d765215ef4f8 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/gcp/gcp-arch-diagram.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=9f14313dc14e9f2b0c42ee2951061ba9 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/gcp/gcp-arch-diagram.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=4099f851877c7aff01e7f4a9e7a8e693 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/gcp/gcp-arch-diagram.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=3e792a579cdccfb0bb33ca7d33e129b2 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/gcp/gcp-arch-diagram.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=f4fbd8cef20c375faa7872adfb5a8297 2500w" />
</Frame>

Your database lives entirely inside a dedicated project within GCP. PlanetScale will not have access to other projects nor your organization-level settings within GCP. Outside of your GCP organization, we run the PlanetScale control plane, which includes the PlanetScale API and web application, including the dashboard you see at `app.planetscale.com`.

The Vitess cluster running inside Kubernetes is composed of a number of Vitess Components.
All incoming queries are received by one of the **VTGates**, which then routes them to the appropriate **VTTablet**.
The VTGates, VTTablets, and MySQL instances are distributed across 3 availability zones.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/gcp/gcp-vitess.png?fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=8c50c23071733bc6438efad7c983fced" alt="Diagram of Vitess cluster on GCP" data-og-width="2184" width="2184" data-og-height="1626" height="1626" data-path="docs/images/assets/docs/managed/gcp/gcp-vitess.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/gcp/gcp-vitess.png?w=280&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=76d49e0a03673caa209113739d8af319 280w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/gcp/gcp-vitess.png?w=560&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=63cf4ddd75ca92a065d4693cdb5e371b 560w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/gcp/gcp-vitess.png?w=840&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=f73f061fe6374be4a52281e4f7500914 840w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/gcp/gcp-vitess.png?w=1100&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=329222cdd7cef430edfde0c4990db003 1100w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/gcp/gcp-vitess.png?w=1650&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=0460e9b1cf6392054c5d5e6e42b06485 1650w, https://mintcdn.com/planetscale-cad1a68a/vjR5C2sTgdIGKQJc/docs/images/assets/docs/managed/gcp/gcp-vitess.png?w=2500&fit=max&auto=format&n=vjR5C2sTgdIGKQJc&q=85&s=5d1b34dadcbdde55900d77e90327ad76 2500w" />
</Frame>

Several additional required Vitess components are run in the Kubernetes cluster as well.
The topology server keeps track of cluster configuration.
**VTOrc** monitors cluster health and handles repairs, including managing automatic failover in case of an issue with a primary.
**vtctld** along with the client **vtctl** can be used to make changes to the cluster configuration and run workflows.

## Security and compliance

PlanetScale Managed is an excellent option for organizations with specific security and compliance requirements.

You own the GCP organization and project that PlanetScale is deployed within an isolated architecture. This differs from when your PlanetScale database is deployed within our GCP organizations.

Along with System and Organization Controls (SOC) 2 Type 2 and PlanetScale [security and compliance](/docs/security) practices that PlanetScale has been issued and follows, we can also sign BAAs for [HIPAA compliance](https://planetscale.com/blog/planetscale-and-hipaa) on PlanetScale Managed.

<Note>
  PlanetScale manages the entire project and can NOT support customers running Terraform or other configuration management in the project.
</Note>

### GCP Private Service Connect

By default, all connections are encrypted, but public. Optionally, you also have the option to use private database connectivity through [GCP Private Service Connect](/docs/vitess/managed/gcp/private-service-connect), which is only available on single-tenancy deployment options, including PlanetScale Managed.

<Note>
  If you have any questions or concerns related to the security and compliance of PlanetScale Managed, please [contact us](https://planetscale.com/contact), and we will be happy to discuss them further.
</Note>

## Getting started with PlanetScale Managed in GCP

If you want to see what is involved in getting set up with PlanetScale Managed in GCP, you can see the [GCP set up documentation](/docs/vitess/managed/gcp/getting-started).

If you are interested in exploring PlanetScale Managed further, please [contact us](https://planetscale.com/contact), and we can chat more about your requirements and see if PlanetScale Managed is a good fit for you.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Back up and restore in GCP
Source: https://planetscale.com/docs/vitess/managed/gcp/back-up-and-restore

PlanetScale Managed backup and restore functions like the hosted PlanetScale product. For more info, see [how to create, schedule, and restore backups for your PlanetScale databases](/docs/vitess/backups).

To learn more about the backup and restore access levels, see the [database level permissions documentation](/docs/security/access-control#database-level-permissions).

By default, databases are automatically backed up once per day to a Cloud Storage bucket in the customer's GCP project. This default can be adjusted when working with PlanetScale Support. However, configuring and validating additional backup frequencies is the customer's responsibility.

During the initial provisioning process, PlanetScale applies a Cloud Storage configuration to ensure backups are encrypted at rest on GCP Cloud Storage.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Set up PlanetScale Managed in GCP
Source: https://planetscale.com/docs/vitess/managed/gcp/getting-started

The following guide will walk you through setting up a PlanetScale Managed cluster in your Google Cloud Platform (GCP) organization.

## Overview

If you have any questions while working through this documentation, contact your PlanetScale Solutions Engineer for assistance.

<Note>
  This guide is only intended for PlanetScale Managed customers currently working with the PlanetScale team. You cannot set PlanetScale Managed up on your own without PlanetScale enabling it for your organization. If you are interested in [PlanetScale Managed](/docs/vitess/managed), please [contact us](https://planetscale.com/contact).
</Note>

## Step 1: Account requirements

A new GCP project must be set up following this documentation to successfully bootstrap a new PlanetScale Managed cluster. To proceed with this guide, an existing GCP organization and an active Cloud Billing account are required.

Further information on creating GCP organizations can be found in the [creating and managing organization resources documentation](https://cloud.google.com/resource-manager/docs/creating-managing-organization).

## Dedicated GCP project

PlanetScale Managed requires the use of a standalone project in GCP. This project should not have any existing resources running within it, as PlanetScale will request a set of permissions as defined in step 2.

## Modification of accounts

Once the GCP project is handed over to PlanetScale via granting IAM permissions, it should not be modified. Issues caused by modifications of the GCP project or its resources void the PlanetScale Managed SLA. Contact [support@planetscale.com](/docs/vitess/managed/gcp/) to discuss configuration changes or customization.

## Step 2: Bootstrap GCP project

Before setting up the IAM roles, you must create a new GCP project, assign it to a GCP Billing Account, and enable the Compute Engine API.

### Create a new GCP project

A new GCP project can be created via the command line if the [gcloud](https://cloud.google.com/sdk/docs/install) SDK is installed and configured:

```bash  theme={null}
gcloud projects create <project-name>
```

Projects can also be created through the [GCP console](https://console.cloud.google.com/projectcreate).

Further information on creating GCP projects is available in the [Google Cloud Resource Manager documentation](https://cloud.google.com/resource-manager/docs/creating-managing-projects).

### Assign the new project to a GCP Billing Account

Next, assign the new project to a GCP Billing Account inside your organization. The account to use will depend on your organization and its policies.

<Note>
  If the user who created the project has the Billing Administrator role, the project may already have billing enabled. Please review the settings to ensure it is attached to the intended Billing Account.
</Note>

Further information on assigning projects to Billing Accounts is available [here](https://cloud.google.com/docs/billing/docs/how-to/modify-project).

### Enable Compute Engine API

The Compute Engine API must be enabled on the new project. This can be done via the command line:

```bash  theme={null}
gcloud services enable compute.googleapis.com --project "<project-name>"
```

Further information on enabling an API is available [here](https://cloud.google.com/apis/docs/getting-started#enabling_apis).

### Assign IAM Roles

For PlanetScale to provision resources in the project, the following IAM roles must be granted to the following service accounts:

* `terraform-planner@planetscale-operations.iam.gserviceaccount.com` service account:
  * `roles/viewer` - Viewer
* `terraform-runner@planetscale-operations.iam.gserviceaccount.com` service account:
  * `roles/cloudkms.admin` - Cloud KMS Admin
  * `roles/compute.admin` - Compute Admin
  * `roles/container.admin` - Kubernetes Engine Admin
  * `roles/container.clusterAdmin` - Kubernetes Engine Cluster Admin
  * `roles/iam.roleAdmin` - IAM Role Admin
  * `roles/iam.securityAdmin` - Security Admin
  * `roles/iam.serviceAccountAdmin` - Service Account Admin
  * `roles/iam.serviceAccountKeyAdmin` - Service Account Key Admin
  * `roles/logging.admin` - Logging Admin
  * `roles/serviceusage.serviceUsageAdmin` - Service Usage Admin
  * `roles/storage.admin` - Storage Admin
  * `roles/viewer` - Viewer

These can be assigned using the `gcloud` command line tool:

```bash  theme={null}
gcloud projects add-iam-policy-binding "<project-name>" --member serviceAccount:terraform-planner@planetscale-operations.iam.gserviceaccount.com --role roles/viewer
gcloud projects add-iam-policy-binding "<project-name>" --member serviceAccount:terraform-runner@planetscale-operations.iam.gserviceaccount.com --role roles/cloudkms.admin
gcloud projects add-iam-policy-binding "<project-name>" --member serviceAccount:terraform-runner@planetscale-operations.iam.gserviceaccount.com --role roles/compute.admin
gcloud projects add-iam-policy-binding "<project-name>" --member serviceAccount:terraform-runner@planetscale-operations.iam.gserviceaccount.com --role roles/container.admin
gcloud projects add-iam-policy-binding "<project-name>" --member serviceAccount:terraform-runner@planetscale-operations.iam.gserviceaccount.com --role roles/container.clusterAdmin
gcloud projects add-iam-policy-binding "<project-name>" --member serviceAccount:terraform-runner@planetscale-operations.iam.gserviceaccount.com --role roles/iam.roleAdmin
gcloud projects add-iam-policy-binding "<project-name>" --member serviceAccount:terraform-runner@planetscale-operations.iam.gserviceaccount.com --role roles/iam.securityAdmin
gcloud projects add-iam-policy-binding "<project-name>" --member serviceAccount:terraform-runner@planetscale-operations.iam.gserviceaccount.com --role roles/iam.serviceAccountAdmin
gcloud projects add-iam-policy-binding "<project-name>" --member serviceAccount:terraform-runner@planetscale-operations.iam.gserviceaccount.com --role roles/iam.serviceAccountKeyAdmin
gcloud projects add-iam-policy-binding "<project-name>" --member serviceAccount:terraform-runner@planetscale-operations.iam.gserviceaccount.com --role roles/logging.admin
gcloud projects add-iam-policy-binding "<project-name>" --member serviceAccount:terraform-runner@planetscale-operations.iam.gserviceaccount.com --role roles/serviceusage.serviceUsageAdmin
gcloud projects add-iam-policy-binding "<project-name>" --member serviceAccount:terraform-runner@planetscale-operations.iam.gserviceaccount.com --role roles/storage.admin
gcloud projects add-iam-policy-binding "<project-name>" --member serviceAccount:terraform-runner@planetscale-operations.iam.gserviceaccount.com --role roles/viewer
```

Alternatively, they can be assigned through the GCP console under the project's "**IAM & Admin > IAM**" section of the [GCP console](https://console.cloud.google.com/iam-admin/iam).

Further information on assigning IAM roles to projects is available in the [GCP IAM documentation](https://cloud.google.com/iam/docs/granting-changing-revoking-access).

## Step 3: Requesting an initial quota increase

By default, GCP provides most new projects with quotas that are too small for PlanetScale's initial provisioning process.

Submit increase requests for the following quotas. This must be done for all regions in which PlanetScale will provision resources. Depending on your organization, the default quotas may already be at or above these levels:

1. `compute.googleapis.com/ssd_total_storage`: 10000 GB
2. `compute.googleapis.com/disks_total_storage`: 10000 GB
3. `compute.googleapis.com/n2_cpus`: 256
4. `compute.googleapis.com/n2d_cpus`: 256
5. `compute.googleapis.com/cpus_all_regions`: 256
6. `compute.googleapis.com/instances`: 100

You can submit GCP quota increase requests via the project's "**IAM & Admin > Quotas**" section of the [GCP console](https://console.cloud.google.com/iam-admin/quotas). Copy and paste the quota metrics from above into the table to search for them in the quota interface.

While PlanetScale does not immediately consume all requested resources, we recommend these values to ensure enough resources are available for autoscaling, growth, and upgrades.

PlanetScale will request the quota increase if the customer does not but recommends that the customer initiate the request due to an unknown turnaround time for quota requests.

Further information on requesting and managing GCP quotas can be found in the [Google Cloud Allocation quotas documentation](https://cloud.google.com/compute/quotas).

<Note>
  After initial provisioning, PlanetScale will manage quotas on behalf of the customer. Customers do not need to request quota increases for future upgrades or scaling and should not restrict quotas. Limiting quotas may result in service interruptions.
</Note>

## Step 4: Initiating the provisioning process

Once the GCP project has been created, the IAM roles have been applied, and the quota increases have been granted, notify your Solutions Engineer, providing them the following information:

* The name of the organization that you have created on `app.planetscale.com`.
* The GCP project name
* A confirmation of the region(s) that you have chosen for the deployment to reside in. The canonical list of regions can be found in the [Google Cloud Regions and Zones documentation](https://cloud.google.com/compute/docs/regions-zones).

Once your Solutions Engineer receives this information, they will forward it to the team responsible for provisioning your deployment. Provisioning the deployment takes PlanetScale, on average, one business day.

Once the deployment has been provisioned, your Solutions Engineer will contact you to confirm that your team can start creating databases.

<Note>
  Optionally, PlanetScale can connect you to your databases via [GCP Private Service Connect](https://cloud.google.com/vpc/docs/private-service-connect) with PlanetScale Managed. See the [GCP Private Service Connect documentation](/docs/vitess/managed/gcp/private-service-connect) for more information on establishing a Private Service Connect connection.
</Note>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Set up GCP Private Service Connect with PlanetScale Managed
Source: https://planetscale.com/docs/vitess/managed/gcp/private-service-connect

PlanetScale Managed can connect you to your databases via [GCP Private Service Connect](https://cloud.google.com/vpc/docs/private-service-connect).

## Overview

The following guide describes how PlanetScale Managed with GCP Private Service Connect works and how to set it up.

<Note>
  If you are on a Scaler Pro plan and would like to set up GCP Private Service Connect endpoint, see our [Private connections documentation](/docs/vitess/connecting/private-connections-gcp).
</Note>

## How PlanetScale Managed and GCP Private Service Connect work

Private Service Connect (PSC) allows a service producer (PlanetScale) offer services to a service consumer without the consumer being a member of the service producer's organization.

The service producer is the Google Cloud project controlled by PlanetScale, and the service consumer is the project(s) where your applications operate. Your applications connect to a private IP you allocate in your project, which is routed to your PlanetScale databases in the project that PlanetScale controls.

GCP PSC requires multiple components:

* A Private Service Connect [Service Attachment](https://cloud.google.com/vpc/docs/private-service-connect#service-attachments) (also known as a Published Service) deployed in the project that PlanetScale controls on your behalf.
* A Private Service Connect [Endpoint](https://cloud.google.com/vpc/docs/private-service-connect#endpoints) deployed in the project(s) that your applications operate in.

Once all components are operating correctly, the applications in the project with the endpoint configured will connect to the service attachment using private IP addresses instead of the publicly accessible endpoint.

## Step 1: Initiating the setup process

If you would like to initiate the process, please contact your Solutions Engineer and let them know the Google Cloud project ID(s) in which you intend to create Private Service Connect endpoints. If you need to add additional projects to the allowlist, please get in touch with your Solutions Engineer.

<Warning>
  Google Cloud project IDs cannot be changed after initial setup. Please be sure to choose an ID that you will continue to use.
</Warning>

Once they receive your project IDs and forward them to the team responsible for provisioning your deployment, the team will provide them (and ultimately you) with the Private Service Connect Service Attachment URI, which will be in the form `projects/PROJECT/regions/REGION/serviceAttachments/SERVICE_NAME`.

<Warning>
  If you use VPC Service Controls in your VPC, you must ensure that the policy allows access to the PlanetScale-controlled project.
</Warning>

Your Solutions Engineer will provide you the following information when the setup is complete:

* `Target Service` (example: `projects/PROJECT/regions/REGION/serviceAttachments/SERVICE_NAME`)

You will use these values when configuring the Private Service Connect in your application projects.

If you have databases in multiple regions, each region will have a unique `Target Service`, and you will need to configure consumer endpoints for each region.

## Step 2: Establishing Private Service Connect

<Warning>
  Only proceed to the next steps once a PlanetScale Solutions Engineer has provided the `Target Service`.
</Warning>

Refer to Google Cloud's [Access published services through endpoints](https://cloud.google.com/vpc/docs/configure-private-service-connect-services) document for more information on connecting to services via Private Service Connect. This document covers additional details not covered here, including the IAM roles required to perform the configuration process.

### Using the GCP console

The following steps are an example of establishing a Private Service Connect endpoint in the [GCP Console](https://console.cloud.google.com/).

<Steps>
  <Step>
    Obtain the Private Service Connect Attachment URI (`Target Service`) from your Solutions Engineer. It will be in the format: `projects/PROJECT/regions/REGION/serviceAttachments/SERVICE_NAME`.
  </Step>

  <Step>
    Create a Private Service Connect Endpoint. In the GCP console, go to ["Network Service > Private Service Connect"](https://console.cloud.google.com/net-services/psc) page, select the "**Connected endpoints**" tab, and select the "**+ Connect endpoint**" button.
  </Step>

  <Step>
    Add a Private Service Connect Endpoint with the following details:

    * **Target**: Published Service.
    * **Target Service**: Paste the `Target Service` attachment URI provided by your Solutions Engineer in step 1.
    * **Name**: Pick any `Endpoint Name`. The examples in this document use `"edge"`.
    * **Network and subnet**: Select the network (VPC) to create the endpoint in. The endpoint will reserve a static IP address in the subnet. The VPC and subnet must be reachable by the applications you intend to connect to your PlanetScale databases from.
    * **Create an IP Address**: Create a reserved IP address. This is the address your applications will use to access your PlanetScale databases.
    * **Enable Global Access**: PlanetScale recommends enabling this option. When enabled, this allows applications in other regions of your VPC to reach the PSC endpoint.

    Finally, click **Add Endpoint** to start the process. Setup will take approximately 1-2 minutes.

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/managed/gcp/private-service-connect/connect_endpoint_details.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=3a628afc3368f200ef7a5e041beb99ba" alt="connect_endpoint_details" data-og-width="825" width="825" data-og-height="892" height="892" data-path="docs/images/assets/docs/managed/gcp/private-service-connect/connect_endpoint_details.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/managed/gcp/private-service-connect/connect_endpoint_details.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=cf9ff0d935370188b95fe2a8b0b24453 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/managed/gcp/private-service-connect/connect_endpoint_details.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=766dd3780f3307b1ae0f93b25486d10e 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/managed/gcp/private-service-connect/connect_endpoint_details.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=9a3844e39979f16172a784fcb3b1622f 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/managed/gcp/private-service-connect/connect_endpoint_details.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=05e8194f73732bef3604ddc579443881 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/managed/gcp/private-service-connect/connect_endpoint_details.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=f234b0063fc305a18c31e25fdd0a499e 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/managed/gcp/private-service-connect/connect_endpoint_details.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=59e63a64a90bbfe33400b7fe925e8f6c 2500w" />
    </Frame>
  </Step>

  <Step>
    The endpoint creation process will take a minute or two. When finished, select the endpoint and verify the status is **Accepted**:

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/managed/gcp/private-service-connect/endpoint_status.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=28bfb2a2c7528b4392751a850fdf9dbe" alt="Showing endpoint status as &#x22;Accepted&#x22;" data-og-width="1098" width="1098" data-og-height="432" height="432" data-path="docs/images/assets/docs/managed/gcp/private-service-connect/endpoint_status.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/managed/gcp/private-service-connect/endpoint_status.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=186d95e6b201bddda1edbe17af98d1aa 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/managed/gcp/private-service-connect/endpoint_status.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=706ec8e1f8bf805a4587b6557a56c903 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/managed/gcp/private-service-connect/endpoint_status.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=5706d2ab1fc7bcb1ed4056d19c954560 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/managed/gcp/private-service-connect/endpoint_status.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=8b477ef93fd6b79777ffb26a743241c1 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/managed/gcp/private-service-connect/endpoint_status.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=af7dd63e330a3efac285f81c96286ed8 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/managed/gcp/private-service-connect/endpoint_status.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=e13749a236a5ed9cd7babb731b9182aa 2500w" />
    </Frame>

    <Note>
      You must provide the list of projects you wish to connect from to your Solutions Engineer. Your endpoint will only function once they have been added to the allowlist in the published service.
    </Note>

    Repeat steps 2-4 to create an endpoint in **each project** you wish to connect to the Private Service Connect Attachment.
  </Step>
</Steps>

## Step 3: Verifying Connectivity

## DNS

<Note>
  Private Service Connect services created after **May 8, 2024** automatically create private Cloud DNS records in the project where the PSC consumer endpoints are created.

  PSC services published before **May 8, 2024** may need to create a private Cloud DNS zone and configure records pointing to the PSC endpoint IP's manually if you wish to use DNS names to connect to your PlanetScale databases.

  Google maintains additional documentation covering DNS and Private Service Connect here:

  * [Automatic DNS configuration for Service Consumers](https://cloud.google.com/vpc/docs/dns-vpc-hosted-services#auto-dns-consumer)
  * [Other ways to configure DNS for Service Consumers](https://cloud.google.com/vpc/docs/configure-private-service-connect-services#other-dns)
</Note>

Private Service Connect endpoints automatically create a private DNS records in the project where the PSC consumer endpoints are created that resolve to the endpoint's reserved IP.

The domain name used varies by region. You can view the domain name by clicking on `Network Services > Cloud DNS`. If Google was able to set up automatic DNS, you will see a new private DNS zone labeled by `DNS Name`:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/managed/gcp/private-service-connect/cloud_dns.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=ce3038a503038fdaa93c940e207824f5" alt="cloud dns zone list" data-og-width="1098" width="1098" data-og-height="373" height="373" data-path="docs/images/assets/docs/managed/gcp/private-service-connect/cloud_dns.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/managed/gcp/private-service-connect/cloud_dns.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=625d33992c50f7454fa855c07d7c6fa7 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/managed/gcp/private-service-connect/cloud_dns.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=a0ff32536b71e10f7aed0ffa7aa83d6d 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/managed/gcp/private-service-connect/cloud_dns.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=a6c51f4efed2501f6fce0861a91fcb0d 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/managed/gcp/private-service-connect/cloud_dns.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=22a4e9fca6b1ac2fc9ccff305eca6b82 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/managed/gcp/private-service-connect/cloud_dns.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=1a9a8be9c084ee54805c2b334ec7682a 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/managed/gcp/private-service-connect/cloud_dns.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=42b72b52352f466ef45d59547399c3e3 2500w" />
</Frame>

Your consumer endpoints will be available via DNS records visible only within your VPC using the format:

* `<Endpoint-Name>.<Domain-Name>`

If your endpoint was created with automatic DNS or your created your own DNS records manually, you can verify resolution with `dig`. In this example, the endpoint was created with the name `edge` and the service's domain name was `izkpm55j334u-uscentral1.private-connect.psdb.cloud`:

```shell  theme={null}
$ dig +short edge.izkpm55j334u-uscentral1.private-connect.psdb.cloud
10.128.0.14
```

## Test connectivity

Run `curl https://<Endpoint-Name>.<Domain-Name>` to verify your connectivity. A successful response will yield `Welcome to PlanetScale`.

```shell  theme={null}
curl https://edge.izkpm55j334u-uscentral1.private-connect.psdb.cloud
Welcome to PlanetScale.
```

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Cloud accounts and contents
Source: https://planetscale.com/docs/vitess/managed/gcp/security-and-access/cloud-accounts-and-contents



## Cloud accounts

PlanetScale is not responsible for the general configuration of services shared across the cloud organization in which the AWS Organizations member account or GCP project is provisioned. The customer is solely responsible for managing account access outside that granted to PlanetScale.

## Content restrictions

The data stored in PlanetScale Managed databases is contained entirely in the customer's AWS Organizations member account or GCP project.

Customers are responsible for all content that is stored in the databases they have created.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Data requests
Source: https://planetscale.com/docs/vitess/managed/gcp/security-and-access/data-requests



## End-user data requests

Some laws require you and your company to allow your end-users to control their personal data. PlanetScale allows you to access, export, or delete their personal data.

Reach out to [support@planetscale.com](/docs/vitess/managed/) for additional support with accessing, exporting, or deleting personal data.

## Data portability

PlanetScale does not offer the migration of specific metadata between regions. PlanetScale does support migrating a database between regions for PlanetScale Managed.

## Data deletion

Users can request to have their personal data removed by contacting [support@planetscale.com](/docs/vitess/managed/).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Schema snapshots
Source: https://planetscale.com/docs/vitess/managed/gcp/security-and-access/schema-snapshots



Snapshots of database schemas within PlanetScale Managed accounts are transferred to PlanetScale's control plane to facilitate features requiring visibility into schemas. Schema snapshots include the structure of databases managed by PlanetScale, but do not include any row-level data.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# User management
Source: https://planetscale.com/docs/vitess/managed/gcp/security-and-access/user-management



## Initial onboarding

### Administrative onboarding

The customer's initial administrative user creates an organization on PlanetScale. Administrative accounts have the `Organization Administrator` role assigned.

### User onboarding

Users can be onboarded either manually or using single sign-on. Manual onboarding is handled by the Administrator once they are initially granted access. Users managed via SSO are onboarded once the SSO provider is connected and configured.

### Single sign-on

PlanetScale Managed requires single sign-on (SSO) for the API and web interface, enabling organizations to manage access through their existing directory services.

You can read more about single sign-on and how to set it up in the [PlanetScale single sign-on documentation](/docs/security/sso).

## Access levels

PlanetScale currently supports three different roles inside of organizations:

* `Organization Administrator`
* `Organization Member`
* `Database Administrator`

See the [PlanetScale access control documentation](/docs/security/access-control) for a further breakdown of each role's permissions.

## Separation of accounts

PlanetScale Managed provides integration with numerous single sign-on providers. Users can have entirely separate personal and corporate accounts with PlanetScale when their organization uses SSO.

It is up to the customer to ensure that they maintain their SSO setup and do not invite or allow employees to use any other authentication method to access PlanetScale.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Anomalies
Source: https://planetscale.com/docs/vitess/monitoring/anomalies

Anomalies are defined as periods with a substantially elevated percentage of slow-running queries.

## Overview

PlanetScale Insights continuously analyzes your query performance to establish a baseline for expected performance. When a high enough percentage of queries are running more slowly than the baseline expectation, we call this an anomaly.

## Using the Anomalies graph

The graph shown under the Anomalies tab shows the percentage of queries executing slower than the 97.7th (2-sigma) percentile baseline on the y-axis and the period of time on the x-axis. The "expected" line shows the percent of queries that are statistically expected in a database with uniform query performance over time. Slight deviations from the expected value are normal. Only substantial and sustained deviations from the expected value are considered an anomaly.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/anomalies/database-health-graph.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=580a4e81e18689435ede8b8dc369a5d3" alt="Database health graph showing two anomalies" data-og-width="2342" width="2342" data-og-height="1294" height="1294" data-path="docs/images/assets/docs/concepts/anomalies/database-health-graph.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/anomalies/database-health-graph.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=aa7cd0fc5248ddeeafd7fb674ae46bde 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/anomalies/database-health-graph.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=b3f3c7ab58430026771d9584d4037587 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/anomalies/database-health-graph.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=b7d06c37f44fbb46de877510f1710460 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/anomalies/database-health-graph.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=23e23aa35f109df83f87825f781863d0 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/anomalies/database-health-graph.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=645c9bea0a4d4cd1c407ff196b2bd5f8 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/anomalies/database-health-graph.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=560f66942e734c6bec489a3ce94d8b2b 2500w" />
</Frame>

Any periods where your database was unhealthy will be highlighted with a red icon representing a performance anomaly. Each anomaly on the graph is clickable. Clicking on it will pull up more details about it in the table below the graph, such as: duration, percentage of increase, and when the anomaly occurred. We also overlay any deploy requests that happened during that period over the anomaly graph.

On top of this, we also surface any impact to the following:

* The query that triggered the anomaly
* CPU utilization
* Memory
* IOPS
* Queries per second
* Rows written per second
* Rows read per second
* Errors per second

## Anomalies vs query latency

You may notice a correlation between some areas in the query latency graph and the anomalies graph. Conversely, in some cases, you may see a spike in query latency, but no corresponding anomaly.

Increased query latency *can* be indicative of an anomaly, but not always. Query latency may increase and decrease in ways that don't always indicate an actual problem with your database.

For example, you may run a weekly report that consists of a few slow-running queries. These queries are always slow. Every week, you'll see a spike on your query latency graph during the time that your weekly report is generated, but not on your anomaly violations graph. The queries are running at their *expected* latency, so this is not considered an anomaly.

## What should I do if my database has an anomaly?

The purpose of the Anomalies tab is to show you relevant information so you can determine what caused an anomaly and correct the issue.

Let's look at an example scenario. You deploy a feature in your application that contains a new query. This query is slow, running frequently, and is hogging database resources. This new slow query is running so often that it's slowing down the rest of your database. Because your other queries are now running slower than expected, an anomaly is triggered.

In this case, we will surface the new slow-running query so that you can find ways to optimize it to free up some of the resources it's using. Adding an index will often solve the problem. You can test this by adding the index, creating a deploy request, and deploying it. If it's successful, you'll quickly see the anomaly end.

On the other hand, an anomaly does not necessarily mean you need to take any action. One common example where you may see an anomaly is in the case of large active-running backups. In this case, we will tell you that a backup was running during the time of the anomaly.

<Note>
  Even if it causes an anomaly, we do not recommend you turn off backups to prevent possible data loss.
</Note>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Datadog integration
Source: https://planetscale.com/docs/vitess/monitoring/datadog

PlanetScale can push metrics to Datadog to assist your team with understanding your database usage and performance.

<Warning>
  This Datadog integration is no longer receiving updates or new metric additions. You should instead use our [Datadog Agent Integration](/docs/vitess/tutorials/prometheus-metrics-datadog), which provides more metrics than this native integration.

  If you have any questions about migrating to the Datadog Agent Integration, [reach out to support](https://planetscale.com/contact?initial=support).
</Warning>

## Prerequisites

* A [Datadog](https://www.datadoghq.com/) account

## Configuring the Datadog integration

<Steps>
  <Step>
    In Datadog, install the [PlanetScale integration](https://app.datadoghq.com/account/settings#integrations/planetscale).
  </Step>

  <Step>
    Create a Datadog API key in your [Datadog Organization Settings](https://app.datadoghq.com/organization-settings/api-keys) and copy the key.
  </Step>

  <Step>
    In PlanetScale, go to your organization's [Integrations settings](https://app.planetscale.com/settings/integrations), and select **Configure** for the Datadog integration. Paste your Datadog API key into the field.
  </Step>
</Steps>

Once complete, a "PlanetScale" dashboard will be available with incoming metrics from PlanetScale.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/datadog/dashboard.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=82eface6b002bf541dfbdfd3872fcb5f" alt="PlanetScale Default Dashboard in Datadog" data-og-width="2728" width="2728" data-og-height="1988" height="1988" data-path="docs/images/assets/docs/integrations/datadog/dashboard.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/datadog/dashboard.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=854a1bf1a8734829cbef18fb81e9d792 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/datadog/dashboard.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=168cdb12f985df9a270efb4f30f37bb6 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/datadog/dashboard.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=c0ddbfa415ac9c1e0171056e83d7cb7a 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/datadog/dashboard.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=893cd14f67d8e83c73532e8626137a0b 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/datadog/dashboard.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=c56a588d6fbf265d0ba56b864e1f6b61 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/datadog/dashboard.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=e7352e48985d34ec6cad22cfb7a6f507 2500w" />
</Frame>

## Metrics We Collect

Once configured, PlanetScale collects the following metrics from every branch in your organization.

| **Metric name**                            | **Metric type** | **Description**                                                                                     |
| :----------------------------------------- | :-------------- | :-------------------------------------------------------------------------------------------------- |
| planetscale.connections                    | gauge           | Number of active connections to a database branch. *Shown as connection.*                           |
| planetscale.primary.cpu\_usage             | gauge           | Percentage of CPU utilized on a database branch's primary. *Shown as percent.*                      |
| planetscale.primary.memory\_usage          | gauge           | Percentage of memory utilized on a database branch's primary. *Shown as percent.*                   |
| planetscale.queries.latency                | gauge           | Query times in the p50, p95, p99 and p999 percentiles. *Shown as millisecond.*                      |
| planetscale.replication\_lag               | gauge           | Replication lag in seconds between a database branch's primary and each replica. *Shown as second.* |
| planetscale.rows\_read                     | count           | Number of rows read from a database branch. *Shown as row.*                                         |
| planetscale.rows\_written                  | count           | Number of rows written to a database branch. *Shown as row.*                                        |
| planetscale.tables.cumulative\_query\_time | count           | Cumulative active query time in a database branch, by table and statement. *Shown as nanosecond.*   |
| planetscale.tables.queries                 | count           | Number of queries issued to a database branch, by table and statement. *Shown as query.*            |
| planetscale.tables.rows\_deleted           | count           | Number of rows deleted from a database branch, by table. *Shown as row.*                            |
| planetscale.tables.rows\_inserted          | count           | Number of rows inserted into a database branch, by table. *Shown as row.*                           |
| planetscale.tables.rows\_selected          | count           | Number of rows selected in a database branch, by table. *Shown as row.*                             |
| planetscale.tables.rows\_updated           | count           | Number of rows updated in a database branch, by table. *Shown as row.*                              |
| planetscale.tables.storage                 | gauge           | Total bytes stored in a database branch, by table. *Shown as byte.*                                 |
| planetscale.vtgate.errors                  | count           | Number of errors encountered by a database branch's vtgate. *Shown as error.*                       |
| planetscale.vttablet.mem\_util.max         | gauge           | Maximum memory utilization of a database branch's vttablet. *Shown as percent.*                     |
| planetscale.vttablet.mem\_util.avg         | gauge           | Average memory utilization of a database branch's vttablet. *Shown as percent.*                     |
| planetscale.vttablet.iops                  | gauge           | Number of IOPS performed by a database branch's vttablet. *Shown as operation.*                     |

## Billing

The Datadog integration is available on all of our [paid plans](https://planetscale.com/pricing).

## Frequently asked questions

### How do I track replication lag in Datadog?

You can use the following formula to set alerts for replication lag:

```bash  theme={null}
(max:planetscale.replication_lag{ps_database:<DATABASE_NAME> ps_tablet_type:replica, ps_branch:<MAIN>})
```

Make sure you replace `<DATABASE_NAME>` with your PlanetScale database name and `<MAIN>` with the name of the branch for which you'd like to track replication lag.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Sending Prometheus Metrics to Datadog
Source: https://planetscale.com/docs/vitess/monitoring/prometheus-metrics-datadog

If you're looking for more metrics than PlanetScale's native Datadog integration provides, this tutorial will show how to configure your [Datadog agent](https://docs.datadoghq.com/agent/) to scrape PlanetScale's [Prometheus infrastructure](/docs/vitess/integrations/prometheus) automatically, allowing you to collect detailed metrics for all of your PlanetScale branches.

## Overview

In this tutorial, we'll assume that you have a Datadog Agent Version 7 running. For more information on what the Datadog Agent is and how to install it, start with the [Datadog Agent documentation](https://docs.datadoghq.com/getting_started/agent/).

For the purposes of this guide, we'll be using a Datadog agent running with the recommended installation steps on a Linux system.

## Prerequisites

You'll need a working Datadog agent and access to add a [Custom Agent Check](https://docs.datadoghq.com/developers/custom_checks/) to that instance. This may require `root` or `sudo` access on the machine running the Datadog agent.

You'll also need a [Service token](/docs/api/reference/service-tokens) in your Organization, with the `read_metrics_endpoints` permission granted.

## Adding the Plugin to the Datadog Agent

Go to [https://github.com/planetscale/planetscale-datadog](https://github.com/planetscale/planetscale-datadog), which is the repository that has our custom OpenMetrics Check.

Place the unedited `planetscale.py` in the `checks.d` directory of your Datadog Agent.

* On Linux, that is `/etc/datadog-agent/checks.d/`
* On macOS, that is `/opt/datadog-agent/etc/checks.d/`

Make sure that it belongs to the appropriate user. If you're using the recommended Linux installation steps, it will have created a `dd-agent` user:

```
$ pwd
/etc/datadog-agent/checks.d
$ ls -al planetscale.py
-rw-r--r-- 1 dd-agent dd-agent 9261 Apr  2 22:54 planetscale.py
```

This file is owned by the `dd-agent` user and group in the `/etc/datadog-agent/checks.d` directory.

If you're on macOS, it will depend on whether you installed the agent as a 'Single User Agent' or a 'Systemwide Agent'. If you picked Single User, there should be no additional permission changes needed. If you installed it as a Systemwide agent, make sure the user and group you installed the agent with as ownership of the file.

## Configuring the Datadog Agent

Now that we have the plugin installed, we need to configure it. In the `conf.d` directory of the Datadog agent take the `conf.d/planetscale.yaml.example` file and edit it with your organization name and Service Token information. It should look like this:

```bash expandable theme={null}
instances:
  - planetscale_organization: 'nick' # Required: Your PlanetScale organization ID
    ps_service_token_id: '${TOKEN_ID}' # Required: Your PlanetScale Service Token ID
    ps_service_token_secret: '${TOKEN}' # Required: Your PlanetScale Service Token Secret. Consider using Datadog secrets management: https://docs.datadoghq.com/agent/guide/secrets-management/

    namespace: 'planetscale' # Required: Namespace for the metrics
    metrics: # Required: List of metrics to collect. Use mapping for renaming/type overrides.
      - planetscale_vtgate_queries_duration: vtgate_query_duration

    min_collection_interval: 60
    send_distribution_buckets: true
    collect_counters_with_distributions: true
```

This configures the integration to look for all of the branches in the `"nick"` PlanetScale organization, only collect the `planetscale_vtgate_queries_duration` metric, which it will rename `vtgate_query_duration` and put it inside of the `planetscale` namespace.

Save the file at `planetscale.yaml`, making sure to double check permissions:

```
$ pwd
/etc/datadog-agent/conf.d
$ ls -al planetscale.yaml
-rw-r--r-- 1 root root 1518 Apr  2 22:57 planetscale.yaml
```

## Restart the Datadog Agent

Now that this is configured and installed, restart the Agent:

```
$ sudo systemctl restart datadog-agent
```

## Validating the PlanetScale Plugin

Now that the Datadog Agent is running the PlanetScale plugin, metrics should start flowing into Datadog within a couple of minutes. To validate, we can ask the Datadog Agent:

```
sudo -u dd-agent -- datadog-agent check planetscale
```

If the plugin is installed successfuly, this should output the scrape targets for your branches, as well as metadata about when it was last run and how many metrics were emitted:

```bash expandable theme={null}
$ sudo -u dd-agent -- datadog-agent check planetscale
=== Service Checks ===
[
  {
    "check": "planetscale.api.can_connect",
    "host_name": "ubuntu",
    "timestamp": 1743638192,
    "status": 0,
    "message": "",
    "tags": [
      "planetscale_org:nick"
    ]
  },
  {
    "check": "planetscale.prometheus.health",
    "host_name": "ubuntu",
    "timestamp": 1743638192,
    "status": 0,
    "message": "",
    "tags": [
      "endpoint:https://metrics.psdb.cloud/metrics/branch/7wxuxewx4l0p?..."
    ]
  },
  {
    "check": "planetscale.prometheus.health",
    "host_name": "ubuntu",
    "timestamp": 1743638192,
    "status": 0,
    "message": "",
    "tags": [
      "endpoint:https://metrics.psdb.cloud/metrics/branch/6o0rr27785fl?..."
    ]
  }
]


  Running Checks
  ==============

    planetscale (unversioned)
    -------------------------
      Instance ID: planetscale:planetscale:8d4d64f696d967be [OK]
      Configuration Source: file:/etc/datadog-agent/conf.d/planetscale.yaml
      Total Runs: 1
      Metric Samples: Last Run: 0, Total: 0
      Events: Last Run: 0, Total: 0
      Service Checks: Last Run: 3, Total: 3
      Histogram Buckets: Last Run: 77, Total: 77
      Average Execution Time : 809ms
      Last Execution Date : 2025-04-02 23:56:32 UTC (1743638192000)
      Last Successful Execution Date : 2025-04-02 23:56:32 UTC (1743638192000)


  Metadata
  ========
    config.hash: planetscale:planetscale:8d4d64f696d967be
    config.provider: file
```

The Service Checks show that it has successfully connected to the PlanetScale API to request information about how to scrape for the branches in my organization, and it has successfully scraped both of what it discovered.

We can also see that it successfully executed at `2025-04-02 23:56:32 UTC` and produced 77 Histogram Buckets.

## Adding Metrics

In our earlier configuration, we only added one metric. For a complete list of what PlanetScale exposes, please take a look at our [Metrics Reference Documentation](/docs/vitess/integrations/prometheus-metrics).

Note that the Datadog agent [normalizes metrics with certain suffixes starting in v7.32.0](https://github.com/DataDog/integrations-core/blob/master/openmetrics/README.md):

> Starting in Datadog Agent v7.32.0, in adherence to the OpenMetrics specification standard, counter names ending in \_total must be specified without the \_total suffix. For example, to collect promhttp\_metric\_handler\_requests\_total, specify the metric name promhttp\_metric\_handler\_requests. This submits to Datadog the metric name appended with .count, promhttp\_metric\_handler\_requests.count.

This means that to scrape a metric such as `planetscale_mysql_bytes_received_total`, you would configure the Datadog agent for `planetscale_mysql_bytes_received`.

If I want to collect additional metrics, I can add them to the list:

```
    metrics: # Required: List of metrics to collect. Use mapping for renaming/type overrides.
      - planetscale_vtgate_queries_duration: vtgate_query_duration
      - planetscale_edge_active_connections: active_connections
```

Then, restart the Datadog Agent:

```
$ sudo systemctl restart datadog-agent
```

If I check the status of the PlanetScale Plugin, I can see our last run added a Metric Sample:

```bash  theme={null}
  Running Checks
  ==============

    planetscale (unversioned)
    -------------------------
      Instance ID: planetscale:planetscale:fde586b60a54a38f [OK]
      Configuration Source: file:/etc/datadog-agent/conf.d/planetscale.yaml
      Total Runs: 1
      Metric Samples: Last Run: 1, Total: 1
      Events: Last Run: 0, Total: 0
      Service Checks: Last Run: 3, Total: 3
      Histogram Buckets: Last Run: 77, Total: 77
      Average Execution Time : 826ms
      Last Execution Date : 2025-04-03 00:01:45 UTC (1743638505000)
      Last Successful Execution Date : 2025-04-03 00:01:45 UTC (1743638505000)
```

In the Datadog UI, I can see data for the `planetscale.active_connections` metric:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/prometheus-datadog-graph.png?fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=af198d4ffbb6a7ca9bd821afd5d79d83" alt="Datadog Connections Metric" data-og-width="2770" width="2770" data-og-height="1206" height="1206" data-path="docs/vitess/tutorials/prometheus-datadog-graph.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/prometheus-datadog-graph.png?w=280&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=ba4fabc1030fe00f62d363eb04438b17 280w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/prometheus-datadog-graph.png?w=560&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=5d9107c8dc8cf34d267a57fc59179006 560w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/prometheus-datadog-graph.png?w=840&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=80385f6b51c3a50fcc2c507d8317a061 840w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/prometheus-datadog-graph.png?w=1100&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=1dd84603686131e0547c6931a5e68045 1100w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/prometheus-datadog-graph.png?w=1650&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=7bd1a120cff8194ad5069748dd7b5ef5 1650w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/prometheus-datadog-graph.png?w=2500&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=8b08666bcb27fcc7d57ec5c0aca1b2ba 2500w" />
</Frame>

## What's Next?

Now that you're sending a couple of metrics from PlanetScale to Datadog, take a look at our [full list](/docs/vitess/integrations/prometheus-metrics) and start building dashboards!

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Grafana Dashboard for PlanetScale Branches
Source: https://planetscale.com/docs/vitess/monitoring/prometheus-metrics-grafana

In this tutorial, you'll learn how to set up Grafana and connect it to a Prometheus instance to see metrics about your PlanetScale database.

## Introduction

This guide requires that you've set up a Prometheus instance from our documentation.

If you're already running Grafana in production and you're just looking for our standard dashboard template, you can find it [on GitHub](https://github.com/planetscale/grafana-dashboard).

## Install Grafana

Grafana's [installation documentation](https://grafana.com/docs/grafana/latest/setup-grafana/installation/) contains information for their supported platforms. For this guide, we'll be setting this up locally on a macOS machine.

If you're using a hosted Grafana option such as [Grafana Cloud](https://grafana.com/products/cloud/) or [AWS Managed Grafana](https://aws.amazon.com/grafana/) you can skip this step.

On macOS, Grafana is availabile via [homebrew](https://brew.sh/), and I can install it with:

```bash  theme={null}
$ brew install grafana
```

This will download and install Grafana, and I can start it with:

```bash  theme={null}
$ brew services start grafana
```

When that succeeds, I can go to `http://localhost:3000/` and I should see the Grafana welcome page:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-grafana-welcome.png?fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=569ab1148e332bf9c8817988ff8ebea9" alt="Grafana Welcome Page" data-og-width="3008" width="3008" data-og-height="2326" height="2326" data-path="docs/vitess/tutorials/metrics-grafana-welcome.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-grafana-welcome.png?w=280&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=e0d475c9afa651fdf3ccc9f26422ce4b 280w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-grafana-welcome.png?w=560&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=a61e8cebd18725e1b7b3d9d8cbe8622c 560w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-grafana-welcome.png?w=840&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=1fb013d5340d8023da5389c25ce7096c 840w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-grafana-welcome.png?w=1100&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=8af773900d52f7d8d8ce88abde83e50d 1100w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-grafana-welcome.png?w=1650&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=ff9ac6e12979c3d37286c50f707ad258 1650w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-grafana-welcome.png?w=2500&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=2af627b979fe0f362c793eed6f9d61d5 2500w" />
</Frame>

The default username and password for a new install is `admin` and `admin`. Grafana will ask you to change the password the first time you log in, please pick something more secure than `admin`.

### Adding a Prometheus Endpoint

You can skip this step as well if you're already running a managed Prometheus or have added your datasource to Grafana already.

If you're running Prometheus locally, you'll need to add that as a datasource. To do this:

<Steps>
  <Step>
    Open the menu in the top left and click "Connections"
  </Step>

  <Step>
    Search for "Prometheus" and pick the plain "Prometheus" option
  </Step>

  <Step>
    Click "Add new data source" in the top right of the page
  </Step>
</Steps>

Now, you should look see a page that looks like this:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-add-prometheus-connection.png?fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=5c6624bb11d3b92dcd327686426ebb0d" alt="Grafana Add Datasource" data-og-width="3024" width="3024" data-og-height="3010" height="3010" data-path="docs/vitess/tutorials/metrics-add-prometheus-connection.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-add-prometheus-connection.png?w=280&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=45864b735b894467268eecba79c38aac 280w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-add-prometheus-connection.png?w=560&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=4fd1bb03b34fba84acbef64779ff564e 560w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-add-prometheus-connection.png?w=840&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=ee754e8a82184144b5df8439522cf404 840w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-add-prometheus-connection.png?w=1100&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=cb50a1ba2da6ce96cbea57bd32fe066d 1100w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-add-prometheus-connection.png?w=1650&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=72f59be560278c017ffda052ad3f12a0 1650w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-add-prometheus-connection.png?w=2500&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=27fadd371e63ddf51c10b29ec9fa1b5a 2500w" />
</Frame>

You can call this whatever you want, we'll use the following:

* Name: "PlanetScale"
* Prometheus server URL: `http://localhost:9090/`

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-prometheus-configuration.png?fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=2d4dd7f8dbef421801fcebcbbb03055b" alt="Grafana Prometheus Configuration" data-og-width="2936" width="2936" data-og-height="2922" height="2922" data-path="docs/vitess/tutorials/metrics-prometheus-configuration.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-prometheus-configuration.png?w=280&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=82ce937c41081ec21e76c79774373054 280w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-prometheus-configuration.png?w=560&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=d782a494cfea2b0973e5c678f9df2b0d 560w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-prometheus-configuration.png?w=840&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=335befdb3d314dcd710d5d8d0197ead6 840w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-prometheus-configuration.png?w=1100&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=d40ea531005719f722d5a7163f98ec54 1100w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-prometheus-configuration.png?w=1650&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=ba8bd36ea388996be473fc20257a7f89 1650w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-prometheus-configuration.png?w=2500&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=0b6c7d8940c745fe3b4537bef8af8a1a 2500w" />
</Frame>

Because this is running on your local machine, we do not need to use any Authentication or TLS

Scroll down to the "Interval behaviour" section and set the "Scrape interval" to `1m`.

Finally, scroll to the bottom and click "Save & test".

## Import the PlanetScale Dashboard

Now that we have our datasource added, let's import the PlanetScale Dashboard. This is a starter dashboard that PlanetScale has produced which shows an overview of your branch with the metrics that we expose.

From the Grafana homepage, go to the top left menu and pick "Dashboards".

In the top right, click "New" and then Import":

PlanetScale maintains the latest version of the dashboard located here:

[https://github.com/planetscale/grafana-dashboard/blob/main/overview.json](https://github.com/planetscale/grafana-dashboard/blob/main/overview.json)

Download this file to your computer, and then click "Upload dashboard JSON file".

Find the JSON file you downloaded in the previous step, and configure it with the Prometheus datasource that we added in an earlier:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-prometheus-configuration.png?fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=2d4dd7f8dbef421801fcebcbbb03055b" alt="Grafana Prometheus Configuration" data-og-width="2936" width="2936" data-og-height="2922" height="2922" data-path="docs/vitess/tutorials/metrics-prometheus-configuration.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-prometheus-configuration.png?w=280&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=82ce937c41081ec21e76c79774373054 280w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-prometheus-configuration.png?w=560&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=d782a494cfea2b0973e5c678f9df2b0d 560w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-prometheus-configuration.png?w=840&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=335befdb3d314dcd710d5d8d0197ead6 840w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-prometheus-configuration.png?w=1100&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=d40ea531005719f722d5a7163f98ec54 1100w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-prometheus-configuration.png?w=1650&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=ba8bd36ea388996be473fc20257a7f89 1650w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-prometheus-configuration.png?w=2500&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=0b6c7d8940c745fe3b4537bef8af8a1a 2500w" />
</Frame>

Click 'Import' and you should be directed to the dashboard, configured to query your local Prometheus with the data it's been scraping!

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Sending Prometheus Metrics to New Relic
Source: https://planetscale.com/docs/vitess/monitoring/prometheus-metrics-newrelic

If you're looking for your PlanetScale database metrics in your New Relic account, this tutorial will show how to configure a [Prometheus](https://prometheus.io/) instance to scrape PlanetScale's [Prometheus infrastructure](/docs/vitess/integrations/prometheus) automatically, allowing you to collect detailed metrics for all of your PlanetScale branches.

While this tutorial is written for New Relic, using Prometheus' remote write is a common pattern for sending metrics to [AWS Managed Prometheus](https://aws.amazon.com/prometheus/), [Google Cloud Managed Service for Prometheus](https://cloud.google.com/stackdriver/docs/managed-prometheus), [Grafana hosted Prometheus](https://grafana.com/products/cloud/metrics/) and many other tools.

For more information on Prometheus Remote Write and New Relic, see the [New Relic documentation on sending Prometheus metric data](https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/).

## Overview

In this tutorial, we will be using an instance of [Prometheus](https://prometheus.io/) running on a Linux VM to scrape metrics from PlanetScale and then forward them to New Relic using [Remote Write](https://prometheus.io/docs/specs/prw/remote_write_spec/). We will make sure that Prometheus stays running by creating a [Systemd Unit File](https://www.digitalocean.com/community/tutorials/understanding-systemd-units-and-unit-files).

The default configuration we will create will send all PlanetScale metrics to New Relic, and we will cover how to filter to drop certain metrics that may not be desired.

In order to proceed, you'll need:

* A [New Relic API Key](https://docs.newrelic.com/docs/apis/intro-apis/new-relic-api-keys), make sure it is the `Ingest - License` type.
* PlanetScale [Service token](/docs/api/reference/service-tokens) with `read_metrics_endpoints` permissions.

## Prometheus Installation

First, let's download the latest release of Prometheus and create our user that is going to run it. We'll be using the latest 3.x release from the [GitHub Releases Page](https://github.com/prometheus/prometheus/releases).

Create a `prometheus` user:

```bash  theme={null}
$ sudo useradd -M -U prometheus
```

```bash  theme={null}
$ wget https://github.com/prometheus/prometheus/releases/download/v3.2.1/prometheus-3.2.1.linux-amd64.tar.gz
$ tar xf prometheus-3.2.1.linux-amd64.tar.gz
$ sudo mv prometheus-3.2.1.linux-amd64/ /opt/prometheus
$ sudo chown prometheus:prometheus -R /opt/prometheus
```

This has put the Prometheus binary in `/opt/prometheus` along with the example configuration file that we can use.

## Create our Systemd Unit File

Now that we have the binary in place, let's setup Systemd to run Prometheus by creating a Unit File in `/etc/systemd/system/prometheus.service` with the following contents:

```ini expandable theme={null}
[Unit]
Description=Prometheus Agent
Documentation=https://prometheus.io/
After=network-online.target

[Service]
User=prometheus
Group=prometheus
Restart=on-failure
ExecStart=/opt/prometheus/prometheus \
  --config.file=/opt/prometheus/prometheus.yml \
  --storage.agent.path=/opt/prometheus/data \
  --web.listen-address=0.0.0.0:9091 \
  --agent

[Install]
WantedBy=multi-user.target
```

## Configure Prometheus

Now that we've got Prometheus installed and a unit file present, let's configure Prometheus. We will be borrowing some of our configuration from the [Prometheus Guide](/docs/vitess/integrations/prometheus), and adding some New Relic specific configuration. Edit `/opt/prometheus/prometheus.yml` in your editor of choice so that it contains this, making sure to replace your org name, service token information, and New Relic API key:

```yaml  theme={null}
global:
  scrape_interval: 1m
scrape_configs:
  - job_name: "${ORG}"
    http_sd_configs:
    - url: https://api.planetscale.com/v1/organizations/${ORG}/metrics
      authorization:
        type: "token"
        credentials: "${TOKEN_ID}:${TOKEN}"
      refresh_interval: 10m
remote_write:
  - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=planetscale
    authorization:
      credentials: ${NEW_RELIC_API_TOKEN}
```

This configuration file does the following:

* Configures Prometheus to discover scraping endpoints from the PlanetScale API using a service token
* Points Prometheus to write the metrics it scrapes from PlanetScale to the New Relic API

## Starting Prometheus

Now that we have a Systemd unit file and a configured Prometheus, let's run it!

```bash  theme={null}
$ sudo systemctl daemon-reload
$ sudo systemctl start prometheus.service
```

We can also tell Systemd to run Prometheus when my VM boots:

```bash  theme={null}
$ sudo systemctl enable prometheus.service
```

Now, let's check to make sure everything is running properly:

```bash expandable theme={null}
$ sudo systemctl status prometheus.service
● prometheus.service - Prometheus Agent
     Loaded: loaded (/etc/systemd/system/prometheus.service; enabled; preset: enabled)
     Active: active (running) since Fri 2025-04-04 17:36:57 UTC; 38s ago
       Docs: https://prometheus.io/
   Main PID: 745542 (prometheus)
      Tasks: 9 (limit: 9486)
     Memory: 21.2M (peak: 21.9M)
        CPU: 264ms
     CGroup: /system.slice/prometheus.service
             └─745542 /opt/prometheus/prometheus --config.file=/opt/prometheus/prometheus.yml --storage.agent.path=/opt/prometheus/data --web.listen-address=0.0.0.0:9091 --agent

Apr 04 17:36:57 ubuntu prometheus[745542]: time=2025-04-04T17:36:57.804Z level=INFO source=main.go:1305 msg=EXT4_SUPER_MAGIC
Apr 04 17:36:57 ubuntu prometheus[745542]: time=2025-04-04T17:36:57.804Z level=INFO source=main.go:1308 msg="Agent WAL storage started"
Apr 04 17:36:57 ubuntu prometheus[745542]: time=2025-04-04T17:36:57.804Z level=INFO source=main.go:1437 msg="Loading configuration file" filename=/opt/prometheus/prometheus.yml
Apr 04 17:36:57 ubuntu prometheus[745542]: time=2025-04-04T17:36:57.805Z level=INFO source=watcher.go:225 msg="Starting WAL watcher" component=remote remote_name=fbe64a url="https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=planetscal>
Apr 04 17:36:57 ubuntu prometheus[745542]: time=2025-04-04T17:36:57.805Z level=INFO source=metadata_watcher.go:90 msg="Starting scraped metadata watcher" component=remote remote_name=fbe64a url="https://metric-api.newrelic.com/prometheus/v1/write?prometh>
Apr 04 17:36:57 ubuntu prometheus[745542]: time=2025-04-04T17:36:57.805Z level=INFO source=watcher.go:277 msg="Replaying WAL" component=remote remote_name=fbe64a url="https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=planetscale" queu>
Apr 04 17:36:57 ubuntu prometheus[745542]: time=2025-04-04T17:36:57.806Z level=INFO source=main.go:1476 msg="updated GOGC" old=100 new=75
Apr 04 17:36:57 ubuntu prometheus[745542]: time=2025-04-04T17:36:57.806Z level=INFO source=main.go:1486 msg="Completed loading of configuration file" db_storage=791ns remote_storage=610.171µs web_handler=897ns query_engine=301ns scrape=517.175µs scrape_s>
Apr 04 17:36:57 ubuntu prometheus[745542]: time=2025-04-04T17:36:57.806Z level=INFO source=main.go:1213 msg="Server is ready to receive web requests."
```

This reports that prometheus is `active (running)`, and I can see the logs showing that it started successfully. Great!

## Querying on New Relic

After a couple of minutes, head over to your New Relic dashboard and we can query for your database metrics. First, let's get a list of the database branches in the `nick` organization that I'm using to test:

```bash  theme={null}
$ pscale branch list test --org nick
  ID             NAME         PARENT BRANCH   REGION    PRODUCTION   SAFE MIGRATIONS   READY   CREATED AT     UPDATED AT
 -------------- ------------ --------------- --------- ------------ ----------------- ------- -------------- ----------------
  7wxuxewx4l0p   main         n/a             us-east   Yes          No                Yes     2 years ago    50 minutes ago
  6o0rr27785fl   partitions   main            us-east   No           No                Yes     2 months ago   7 minutes ago
```

For this, we'll use the `7wxuxewx4l0p` branch.

Using New Relic's NRQL, we can visualize the memory usage of my VTTablet instances with the following query:

```sql  theme={null}
FROM Metric SELECT average(planetscale_pods_cpu_util_percentages) WHERE planetscale_database_branch_id = '7wxuxewx4l0p' AND planetscale_component='vttablet' SINCE 30 minutes AGO TIMESERIES FACET planetscale_pod
```

Because my `main` branch is production, we will see the memory usage for my primary and both my replicas over the last 30 minutes:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-new-relic-dashboard.png?fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=8335f48aba48d9d38c5392fbe8c7bd24" alt="New Relic Memory Query" data-og-width="2758" width="2758" data-og-height="1450" height="1450" data-path="docs/vitess/tutorials/metrics-new-relic-dashboard.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-new-relic-dashboard.png?w=280&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=3dabb0eae8d0f066945a1ad2cc5a03b4 280w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-new-relic-dashboard.png?w=560&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=ef446159597e8d24a2dff6826b6bb42c 560w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-new-relic-dashboard.png?w=840&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=778694ba8466edd29b7b37ab64215302 840w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-new-relic-dashboard.png?w=1100&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=a16ff496cce5ce1b000c39883ca24319 1100w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-new-relic-dashboard.png?w=1650&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=264e6dbe4a57d9b5f6cf7b3351cfcd64 1650w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/tutorials/metrics-new-relic-dashboard.png?w=2500&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=af0dc6f709b8c625765036ffbe630441 2500w" />
</Frame>

## Filtering Metrics

If you don't want to ingest every metric into New Relic, you can tell Prometheus to drop certain metrics. For more information, see the [New Relic Documentation](https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/#allow-deny).

If we adjust our Prometheus configuration that we have in `/opt/prometheus/prometheus.yml` we can instruct Prometheus to drop all metrics unless they match a certain naming convention:

```yaml  theme={null}
remote_write:
  - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=planetscale
    authorization:
      credentials: ${NEW_RELIC_API_TOKEN}
  - source_labels: [__name__]
    regex: "planetscale_pods_(.*)"
    action: keep
```

If you replace your `remote_write` block with what's above, Prometheus will only forward the timeseries that match the `planetscale_pods_*` name. For a full list of metrics, see our [Metric List](/docs/vitess/integrations/prometheus-metrics).

## What's Next?

Now that you have your branch metrics in New Relic, you can create dashboards and alerts for conditions such as high CPU or replication delay.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Query Insights
Source: https://planetscale.com/docs/vitess/monitoring/query-insights

PlanetScale Insights gives you a detailed look into **all active queries** running against your database.

## Overview

This in-dashboard tool allows you to identify queries that are running too often, too long, returning too much data, producing errors, and more. You can scroll through the performance graph to detect the time that a query was impacted and, if applicable, the [Deploy Request](/docs/vitess/schema-changes/deploy-requests) that affected it.

You can also see a [list of all queries](#queries-overview) performed on your database in the last 24 hours. For further analysis, you can sort these by metrics like amount of rows read, time per query, and more.

With this built-in tool, you can easily diagnose issues with your queries, allowing you to optimize individual queries without much digging. We will also alert you of any active issues your database may be having in the [Anomalies](/docs/vitess/monitoring/anomalies) tab. This feature flags queries that are running significantly slower than expected.

Insights will also automatically recommend schema changes to improve database performance, reduce memory and storage, and improve your schema based on production database traffic. Read more about the supported recommendations and how to use them in the [schema recommendations documentation](/docs/vitess/monitoring/schema-recommendations).

## Insights page overview

To view Insights for your database, head to the [PlanetScale dashboard](https://app.planetscale.com), select your database, and click the "**Insights**" tab.

<Note>
  If you are a single-tenant or PlanetScale Managed customer, you will need to request access to Insights through your PlanetScale account manager.
</Note>

The dropdown on the top right lets you select which branch you want to analyze. You can also choose which servers you want to view insights for: primary or replicas.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/query-insights/query-insights-overview.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=c8a1d8c0e80e71efb25a5c67fa6bcead" alt="PlanetScale Insights overview page" data-og-width="3018" width="3018" data-og-height="1772" height="1772" data-path="docs/images/assets/docs/concepts/query-insights/query-insights-overview.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/query-insights/query-insights-overview.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=014a25374ee7e376a0e920f0acf83c20 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/query-insights/query-insights-overview.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=a4750fee720940c0e90f7af556fb91ee 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/query-insights/query-insights-overview.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=003044f3e8af8bcf91d9863dda5fa515 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/query-insights/query-insights-overview.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=4a22fec129c477326121b826717d0203 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/query-insights/query-insights-overview.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=4f9bb2610cef3320fb35f121edfde0bd 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/query-insights/query-insights-overview.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=d286f45bb36246d73b1cfca4ca629cc4 2500w" />
</Frame>

You can click the dates listed above the graph to scroll through the past seven days. To further narrow down query analysis, you can select a time range by clicking on the graph and dragging the cursor across. This will zoom in on the selected timeframe.

You also have the option to save a screenshot of the graph by clicking "Save".

If any deploy requests were deployed during the selected period, you will also see an overlay with a link to the deploy request. This can help you quickly assess any impact a deploy request had on your database.

### Queries overview table

The table underneath the graph shows all queries performed on your database in the selected timeframe (last 24 hours by default).

For more information about how to read and interpret this data, see the [Queries overview](#queries-overview) section.

### Insights graph tabs

Once you have selected the branch and server you want to analyze, you can begin exploring the insights for them in the following tabs:

<Columns cols={2}>
  <Card title="Query latency" icon="messages-question" horizontal href="#query-latency" />

  <Card title="Anomalies" icon="code-branch" horizontal href="/docs/vitess/monitoring/anomalies" />

  <Card title="Queries" icon="messages-question" horizontal href="#queries" />

  <Card title="Rows read" icon="glasses" horizontal href="#rows-read" />

  <Card title="Rows written" icon="pen" horizontal href="#rows-written" />

  <Card title="Errors" icon="bug" horizontal href="#errors" />
</Columns>

The remaining sections of this doc walk through how to interpret and act on the data in each tab. If you'd like to see a practical example of how to use Insights to debug a performance issue, check out our [Announcing Insights blog post](https://planetscale.com/blog/introducing-planetscale-insights-advanced-query-monitoring) or [this YouTube video](https://www.youtube.com/watch?v=kkjAxSViOAA) walking you through an example.

## Query latency

The default tab depicts your database's query latency in milliseconds over the last 24 hours.

By default, the graph contains two line charts showing `p50` and `p95` latency. This means 50% and 95% of requests, respectively, completed faster than the time listed. You can also click on the `p99` and `p99.9` pills to toggle those on, or click `p50` or `p95` to toggle those off.

## Queries

The Queries tab displays insights about all active running queries in your database. The graph displays total queries per second against the specified time period.

## Rows read

The Rows read tab displays the total number of rows read per second across the selected time period.

## Rows written

The Rows written tab displays the total number of rows written per second across the selected time period.

## Errors

The Errors tab surfaces any errors that have been captured on your database in a 24 hour period.

Underneath the graph, you'll find a list of database error messages that have been captured over the selected period.

You can click on any of the error messages on the Errors tab to open a more detailed view. This view shows you the individual queries that produced the error, when they ran, how long they ran, and any query tags attached to them.

## Queries overview

The table underneath the graph shows queries performed on your database in the selected timeframe (last 24 hours by default).

<Note>
  The queries table does not show following statements types: `BEGIN`, `COMMIT`, `RELEASE`, `ROLLBACK`, `SAVEPOINT`, `SAVEPOINT_ROLLBACK`, `SET`.
</Note>

You may see some placeholder values in the queries, such as `:v1`. This is because we consider the actual data private and normalize it away.

<Note>
  You have the option to [opt in to complete query collection](#complete-query-collection) to see the full SQL statements.
</Note>

You may also see one or more orange icons next to some queries.

* A shard icon indicates that the query requires execution across multiple shards.
* An exclamation point icon indicates that the query is not currently using an index and requires a full table scan.

Hovering over the icon will show a tooltip with information about the meaning of the icon.

<Note>
  The queries table may also show internal PlanetScale queries that are used to monitor the state of your cluster.
</Note>

This query overviews table shows the same data for all graph tabs except for [Anomalies](/docs/vitess/monitoring/anomalies) and [Errors](#errors). For more information about the content for each of those, refer to each Anomalies and Errors sections above.

### Available query statistics

You can customize the metrics that show up on the Queries list by selecting columns in the "View options" dropdown.

* **Query** - The query that was run.
* **Keyspace** — The default keyspace associated with the connection that issued the query, if set.
* **Qualified table** — The table(s) referenced in the query, in the format `keyspace_name.table_name`.
* **Table keyspace** — The keyspace(s) associated with the tables referenced in the query. (This may differ from the connection keyspace.)
* **Table** — The table(s) being queried or modified.
* **% of runtime** — The percent of the total runtime the query pattern is responsible for (query pattern time divided by the cumulative time of all query patterns on your database).
* **Count** — The number of times this query has run.
* **Total time (s)** — The total time the query has run in seconds.
* **`p50` latency** — The `p50` latency for the query in milliseconds. This means that 50% of requests completed faster than the time listed.
* **`p99` latency** — The `p99` latency for the query in milliseconds. This means that 99% of requests completed faster than the time listed.
* **Max latency** — The maximum observed latency for the query in milliseconds.
* **Rows returned** — The total number of rows fetched by a `SELECT` statement. This includes all times the query has run in the displayed time frame.
* **Rows read** — The total number of rows read. This includes all times the query has run in the displayed time frame.
* **Rows read/rows returned** — The result of dividing total rows read by rows returned in a query. A high number can indicate that your database is reading unnecessary rows, and the query may be improved by adding an index.
* **Rows affected** — The total number of rows modified by an `INSERT`, `UPDATE`, or `DELETE` statement. This includes all times the query has run in the displayed time frame.
* **Tablet calls per query** (only visible for databases with at least one sharded keyspace) — The average number of VTTablet calls/queries for each execution of the query pattern. Calculated as the sum of VTTablet calls divided by the number of queries issued by the client in the selected time period. Query patterns where this value is equal to the number of shards are known as "scatter gather queries".
* **Last run** — The last time a query was run.

You can also sort the columns for quick analysis by clicking on the title at the top of each column.

If `Show sparklines` is selected, numeric columns in the queries table show a time series graph of the value within the selected time period.

### Query filtering

The search bar above the table allows you to filter queries as needed. You can filter for query SQL, keyspace (connection keyspace, and/or keyspace of tables referenced by the query), table name, query count, query latency, multisharded queries, index name, and if the query was indexed. Click on the `?` next to the search bar for the full list of search syntax.

### Query deep dive

Clicking on a query in the Queries list will open a new page with more information about that query.

You'll first see the full query pattern, which displays the query with data normalized away. This query may run several times with different values, which Insights combines into a single query pattern.

You can display an LLM-generated summary of the query by clicking "Summarize query."

You can also display a [query `EXPLAIN` plan](https://planetscale.com/blog/how-read-mysql-explains) by clicking "Show explain plan", which generates the [execution plan](https://dev.mysql.com/blog-archive/mysql-explain-analyze/) for the selected query. You may have to fill in some sample values designated with placeholders like `:v1`. We use placeholders in the patterns both so you can look at whole patterns at once and so the literal values remain private.

Note, if you're viewing the `EXPLAIN` plan on a production branch, this button will be disabled unless you enable production web console access in your database Settings page.

If you'd like to further interact with the query, click "Open query in web console", and you'll be taken to your in-dashboard web console, where you can run the `EXPLAIN` plan.

#### Additional query information

Beneath the query pattern is a graph with more information about the query. The set of available metrics/tabs include: Query latency, Queries, Rows read, Rows written, Errors and Indexes. The Indexes graph (which is not shown on the database-level page) shows the percentage of queries that used each of the listed indexes in each time bucket. Currently only `SELECT` queries show index usage information

Beneath the time series graphs you will see summary statistics for the query pattern. These data are scoped to the same time period shown in the main query pattern graphs. The available metrics have the same definitions as the query statistics listed in the main insights tab.

`SELECT` queries include a horizontal bar graph that shows the cumulative usage of each index over the complete time period shown in the main query pattern graphs.

To change the time period reflected in the graphs and summary statistics, click and drag to restrict the time window, or click on one of the day icons above the graph to select a different day.

#### Notable queries

Underneath the graph, you'll see a table with more information about notable instances of the query, which are defined as queries that took longer than 1s, read more than 10,000 rows, or produced an error.

If any of the selected queries have [SQL comment tags](https://google.github.io/sqlcommenter/) attached, you'll see the key-value pairs in the table under `Tags`.

<Note>
  If you're sending queries with comments using the MySQL shell, make sure you have [enabled comments with the `-c` flag](https://dev.mysql.com/doc/refman/8.0/en/mysql-command-options.html#option_mysql_comments).
</Note>

The table also surfaces when the query started, rows returned, rows read, rows affected, the time it took the query to run (in ms), and the user associated with the query.

### Complete query collection

If you would prefer to view the raw query data using Insights, you can enable this option in your database settings page. In the dashboard, select your database, click "**Settings**", scroll down until you see "**Complete query collection**", and click "**Enable**" to opt in.

With this enabled, Insights will gather the complete raw SQL statements and display them when a query deep dive is selected and in the `EXPLAIN` plan. For example, if you select a query from the table on the Insights page, the query pattern and the selected queries below it will display the full query.

Enabling complete query collection is beneficial when performance varies significantly within the same query pattern, and you need to see the full SQL statement, without placeholders, to identify the correct source of the performance issue.

However, full queries may contain personally identifiable information, so it's important to consider this before opting in to the feature. Because of this, only [Organization Administrators](/docs/security/access-control#organization-administrator) can choose to opt a database into complete query collection. Please read our [Privacy Policy](https://planetscale.com/legal/privacy) for more information on how we collect, process, and disclose your personally identifiable information.

#### Complete query collection and prepared statements

With prepared statements, complete query collection will not display the query values. That is because the MySQL query that is collected is parameterized at the database level, and we cannot extract the bind variables. PlanetScale does not recommend using prepared statements, we recommend disabling them in order to use complete query collection.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Schema recommendations
Source: https://planetscale.com/docs/vitess/monitoring/schema-recommendations

With schema recommendations inside of PlanetScale Insights, you will automatically receive recommendations to improve database performance, reduce memory and storage, and improve your schema based on production database traffic.

## Overview

Schema recommendations uses query-level telemetry to generate tailored recommendations in the form of DDL statements that can be applied directly to a database branch and then deployed to production.

<Note>
  If you are a PlanetScale Enterprise customer, please get in touch with your account manager to learn how you can fully benefit from schema recommendations.
</Note>

## How to use schema recommendations

To find the schema recommendations for your database, go to the “**Insights**” tab in your PlanetScale database and click “**View recommendations**.”

You will see the current open recommendations that may help improve database performance. Select a recommendation to learn more.

Each recommendation will have the following:

* An explanation of the recommended changes, including some of the benefits of the recommended change (E.g., reduced memory and storage, decreased execution time, prevent ID exhaustion)
* The schema or query that it will affect
* The exact DDL that will apply the recommendation
* The option to apply the recommended change to a branch for testing and a safe migration

<Note>
  Schema recommendations that depend on your database traffic run **once per day**. Recommendations that depend only on database schema are run whenever the the schema of your default branch is modified. Schema recommendations are generated only for the database's default branch.
</Note>

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/schema-recommendations/add-index-recommendation.jpg?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=1806d99add1cdb61015145b490c922e3" alt="Example of a recommendation to add an index" data-og-width="3232" width="3232" data-og-height="2112" height="2112" data-path="docs/images/assets/docs/concepts/schema-recommendations/add-index-recommendation.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/schema-recommendations/add-index-recommendation.jpg?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=d3e451c115c0d4888bef6671ef775ba2 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/schema-recommendations/add-index-recommendation.jpg?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=75c3a14fe33d0a1050d528691a0e2153 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/schema-recommendations/add-index-recommendation.jpg?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=1f47cdfa7c94823c43fc356b140ada0f 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/schema-recommendations/add-index-recommendation.jpg?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=f107a47d651dae7deafa620477490040 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/schema-recommendations/add-index-recommendation.jpg?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=ef42e13313e2457d9074f3ed3bb823eb 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/schema-recommendations/add-index-recommendation.jpg?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=57062cf536f3d8f319266ae1753ccd53 2500w" />
</Frame>

### Applying a recommendation

Once you better understand the recommendation, you can apply the recommendation by either:

* Applying it directly through a database branch
* Making the schema change directly in your application or ORM code

We recommend making the schema change directly in your application or ORM code if it is where you manage your schema outside of PlanetScale. We look for different migration framework’s table names in your schema to alert you when you might want to change the schema directly in your application or ORM code. If you manage the schema directly in the database, you can use the following method.

#### Applying a recommendation directly through a database branch

When directly applying through a database branch, you can apply the recommendation by creating and applying it to a new or existing branch.

After applying the recommendation, click on the branch name to see the recommended schema changes. At this point, you can connect to this branch to do your own testing of the change. You can create a connection string for this branch using the “**Connect**” button.

Once ready, you can use the “**Create deploy request**” button on the branch page to open a deploy request to merge the changes with your base branch.

Once it is reviewed and ready to be deployed, you can deploy the changes to production. If you realize you have made a mistake, you can still [revert the schema change](/docs/vitess/schema-changes/deploy-requests#revert-a-schema-change) in the deploy request for up to 30 minutes.

#### Applying a recommendation directly in your application or ORM code

When directly applying through your application or ORM code, you can take the recommended change from PlanetScale and make the equivalent change in your code. The exact DDL provided in each recommendation will help you know what change to make. The exact change will depend on how you manage your schema in your application code.

Then, once you have made the change, you will run the same migration process you would run for any other schema changes with PlanetScale. We recommend doing this inside a development branch with [safe migrations](/docs/vitess/schema-changes/safe-migrations). This will allow you to do your own testing of the change inside the isolated branch. You can create a connection string for this branch using the “**Connect**” button.

Once ready, you can use the “**Create deploy request**” button on the branch page to open a deploy request to merge the changes with your base branch.

Once it is reviewed and ready to be deployed, you can deploy the changes to production. If you realize you have made a mistake, you can still [revert the schema change](/docs/vitess/schema-changes/deploy-requests#revert-a-schema-change) in the deploy request for up to 30 minutes.

### Closing a recommendation

Recommendations are automatically closed when:

* The changes have been deployed to the default branch in a deploy request
* The SQL is applied to the default branch by some other means
* Other schema changes to the default branch make the recommendation unnecessary

Once a recommendation is closed, PlanetScale will never re-suggest it.

## Supported schema recommendations

The following are the currently supported schema recommendations:

<Columns cols={2}>
  <Card title="Adding indexes for inefficient queries" icon="plus" horizontal href="#adding-indexes-for-inefficient-queries" />

  <Card title="Removing redundant indexes" icon="minus" horizontal href="#removing-redundant-indexes" />

  <Card title="Preventing primary key ID exhaustion" icon="key" horizontal href="#preventing-primary-key-id-exhaustion" />

  <Card title="Dropping unused tables" icon="trash" horizontal href="#dropping-unused-tables" />

  <Card title="Upgrading legacy character sets and collations" icon="rocket-launch" horizontal href="#upgrading-legacy-character-sets-and-collations" />
</Columns>

The impact of schema recommendations can vary by recommendation. In the following sections, we will inform you of each recommendation’s potential impacts and explain the recommendation further.

<Note>
  Schema recommendations may not be in line with your desired outcomes. PlanetScale shall not be held liable for any actions you take based on these recommendations.
</Note>

### Adding indexes for inefficient queries

Indexes are crucial for relational database performance. With no indexes or suboptimal indexes, MySQL may have to scan a large number of rows to satisfy queries that only match a few records. This results in slow queries and poor database performance. The right index can reduce query execution time from hours to milliseconds. You can read more about [how database indexes work in this blog post](https://planetscale.com/blog/how-do-database-indexes-work).

#### How PlanetScale recommends adding indexes

To find missing indexes, Insights scans your query performance data to identify queries over the past 24 hours that consume significant resources and have a high aggregate ratio of rows read compared to rows returned. It will then parse the query to extract indexable columns, estimate each column’s cardinality (number of unique values) to determine optimal column order, and suggest a suitable index. In multi-column index suggestions, we first order columns with the most selective (highest cardinality).

Our index recommendation engine doesn’t yet support all queries types, and some queries cannot benefit from an index.

#### Caveats

* Indexes do require both more memory and storage. Evaluating these additional costs against your application’s improvement in read performance is always a good idea. This is also why we offer recommendations to [remove redundant indexes](#removing-redundant-indexes).
* Indexing is a complicated topic and depends on many factors, such as the distribution of values in your database and the particular queries your database receives. Every effort is made to ensure our suggestions improve performance, but verifying and measuring is important.
* If you’re unsure about the impact of adding an index, we recommend benchmarking the index in a non-production environment. If you can simulate production-level traffic, you can do this inside a development branch.
* If you decide to deploy a suggested index to production, it is a good idea to use Insights to verify that your index has the desired effect on relevant queries. If you realize it is not the desired effect, you can still [revert the schema change](/docs/vitess/schema-changes/deploy-requests#revert-a-schema-change) in the deploy request for up to 30 minutes.

### Removing redundant indexes

While indexes can drastically improve query performance, having unnecessary indexes slows down writes and consumes additional storage and memory.

#### How PlanetScale recommends removing indexes

Insights scans your schema every time it is changed to find redundant indexes. We suggest removing two types of indexes:

* Exact duplicate indexes - an index that has the same columns in the same order
* Left prefix duplicate indexes - an index that has the same columns in the same order as the prefix of another index

There are differences between the two, so note your exact recommendation and the following caveats.

#### Caveats

Removing redundant indexes is more nuanced than adding an index.

* Exact duplicate indexes are *always* safe to remove.
* Left prefix duplicate indexes are *almost always* safe to remove, but in some cases can lead to a performance regression. Usually, the larger index can be used instead of the left prefix duplicate indexes. Read the following section for more details on how this works.

#### Left prefix duplicate indexes

Since MySQL can use the leftmost elements of a multi-column index to efficiently find rows, an index that is a left-prefix duplicate of another index can *usually* be removed. To understand this, consider the following table.

```sql  theme={null}
CREATE TABLE `t` (
  `id` bigint unsigned NOT NULL AUTO_INCREMENT,
  `a` bigint,
  `b` bigint,
  `c` bigint,
  PRIMARY KEY (`id`),
  KEY idx_a_b (`a`, `b`),
  KEY idx_a_b_c(`a`, `b`, `c`)
)
```

Consider the following query:

```sql  theme={null}
SELECT * FROM t WHERE a = ? and b = ?
```

This query can use either `idx_a_b` or `id_a_b_c` to quickly find results because both indexes have `a` and `b` as the leftmost columns. In general, removing left prefix duplicates, `idx_a_b` in our case, will result in lower memory and storage usage and improve the performance of inserts, updates, and deletes to this table because there is one less index to be maintained. However, there are a few important caveats.

##### Queries that use the primary key

MySQL implicitly appends the primary key to the end of indexes. An index declared as `KEY idx (a, b)` can be thought of as `KEY idx (a, b, id)`, assuming `id` is the primary key. This has the potential to affect queries that make use of the indexed columns and the primary key. For example:

```sql  theme={null}
SELECT * FROM t WHERE a = ? AND b = ? ORDER BY id
```

This query can be served very efficiently by an `idx_a_b` because the `ORDER` clause is satisfied by the implicitly appended primary key. Since `idx_a_b_c` includes an additional column (`c`), MySQL must perform additional work to return rows in the correct order when using this index.

##### Index size

Even though MySQL can use either index in our table to efficiently look up rows by a and b, searching through the larger `idx_a_b_c` index may take longer simply because the index is larger and will require more pages to be scanned.

In most cases, you can safely remove redundant left-prefix indexes, resulting in better performance. However, ensure that your application doesn’t issue performance critical queries that the above mentioned issues will significantly impact. If you’re unsure about this, we recommend looking through your Insights dashboard to find queries that may be affected.

### Preventing primary key ID exhaustion

As new rows are inserted, it’s possible for auto-incremented primary keys to exceed [maximum allowable value](https://planetscale.com/blog/mysql-data-types-integers#different-integer-data-types) for the underlying column type. When the column reaches the maximum value, subsequent inserts into the table will fail, which can cause a severe outage to your application.

#### How PlanetScale detects primary key ID exhaustion

Insights scans all of the `AUTO INCREMENT` primary keys in your database schema and checks the current `AUTO INCREMENT` value daily to identify where you might be approaching primary key ID exhaustion. If Insights detects that one of the columns is above 60% of the maximum allowable type, it will recommend changing the underlying column to a larger type.

Additionally, Insights scans queries to parse joins and correlated subqueries to find foreign keys and suggests increasing the column size for those columns.

#### Caveats

* It is always a good idea to manually check your database and application for foreign key references to the column you are increasing to ensure none were missed.

### Dropping unused tables

Dropping unused tables can help clean up data that is no longer needed and reduce storage. If the table is large, it can also decrease backup and restore time.

If you are unsure if a table should be retained but decide to drop the table, make sure to [create a manual backup](/docs/vitess/backups#create-manual-backups) of your database before you deploy the change.

If you realize after dropping the table that it is needed, you can still [revert the schema change](/docs/vitess/schema-changes/deploy-requests#revert-a-schema-change) in the deploy request for up to 30 minutes with no data loss. If you are outside the 30-minute period, you must [restore and recover the data inside a database backup branch](/docs/vitess/backups#restore-from-a-backup).

If you determine that the table should be retained, close the recommendation, preventing the suggestion from being remade.

#### How PlanetScale recommends dropping unused tables

Insights scans your query performance data daily to identify if any tables are more than four weeks old and haven’t been queried in the last four weeks.

#### Caveats

* Only you can know if the table’s data is no longer needed. Ensure that the table is never used (even infrequently) and does not contain important data before removing it.
* Once a drop unused table recommendation is opened, it will remain open even if it is subsequently queried. Check your Insights data to verify that the table is still unused before permanently dropping it.

### Upgrading legacy character sets and collations

All non-binary string columns, such as `CHAR`, `VARCHAR` and `TEXT`, have an associated character set and collation. The character set represents the range of valid characters and how they are stored. The collation controls how values are compared, such as in `ORDER BY` clauses and `UNIQUE KEY`s. Over time MySQL has added support for new character sets and collations. The most recent character sets and collations have numerous advantages over their legacy counterparts.

* Full Unicode support including emoji
* Improved performance
* Improved sort order for multi-byte characters
* Awareness of trailing spaces in comparisons

For these reasons we suggest using the `uftmb4` character set and the `utf8mb4_0900_*` collations. Unless you need byte-level comparisons (`utf8mb4_0900_bin`) or language-specific comparisons (e.g. `utf8mb4_0900_es_*` for spanish), we recommend using MySQL's default collation `utf8mb4_0900_ai_ci`.

<Note>
  `0900` in the name of MySQL's modern collations [refers to](https://dev.mysql.com/blog-archive/mysql-character-sets-unicode-and-uca-compliant-collations/) version 9.0.0 of the Unicode Collation Algorithm. `ai` and `ci` stand for accent insensitive and case insensitive, respectively. Accent and case sensitive collations are also available. To see a list of all available character sets and collations, connect to your database and run `show collation;`. When in doubt, use `utf8mb4_0900_ai_ci`.
</Note>

<Note>
  The `utf8` character set is, for compatibility reasons, an alias for `utf8mb3`. We suggest the `utf8mb4` character set instead.
</Note>

#### Caveats

##### Joins

Before upgrading from a legacy character set/collation, it is important to ensure there are no joins on string columns that will become incompatible after the upgrade. For example, if you issue the query

```sql  theme={null}
select * from t1 inner join t2 on t1.name = t2.name
```

both `t1.name` and `t2.name` need to have have identical character sets and collations. If the character sets are the same but the collations are different, the query will fail. If the character sets are different, the query will succeed but will be unable to make use of indexes, which can cause unexpected table scans and dramatically degrade performance for large tables.

Character set/collation upgrade recommendations are not created for tables with a recent history of joins on string columns. However, we cannot automatically detect all join types so it is important to verify that your application does not join on a table's string columns prior to deploying a recommendation that would alter its character set or encoding.

To upgrade the character sets or collations for tables that have string column joins, we recommend upgrading both tables to the same character set and collation in a single deploy request to minimize disruption.

##### Length limits

<Note>
  Length limit issues only apply when upgrading from `utf8mb3` to `utf8mb4` character sets. Collation-only changes are unaffected.
</Note>

When upgrading from `utf8mb3` to `utf8mb4`, it is possible that some adjustments will have to be made to column types. Because `utf8mb3` is a subset of `utf8mb4`, existing data will not increase in size. However, `utf8mb4` requires *up to* four bytes per character instead of *up to* three. Because of the this, some column type changes may be required.

Some data type changes occur automatically when applying the recommendation. `TEXT`-type columns increase to the next largest size (`TEXT` becomes `MEDIUMTEXT`, `MEDIUMTEXT` becomes `LONGTEXT` etc). Because `utf8mb4` possibly requires an extra byte per character, and `TEXT`-type columns are defined by the maximum number of *bytes*, not characters, these columns are upgraded to ensure the ability to store at least the same number of *characters* in the column before and after the `CONVERT TABLE` command in the recommendation. If storing fewer maximum-length characters is acceptable, you can alter the deploy request produced by the recommendation to restore the original `TEXT`-type column definitions.

Other data type changes may need to be made manually. For example, the column definition `VARCHAR(20000) CHARACTER SET utf8mb3` is legal, because 20,000 characters \* 3 bytes per character = 60,000 bytes and `VARCHAR` columns can accommodate up to 65,535 bytes. However, when attempting to convert this column's character set to `utf8mb4`, the number of bytes required to store 20,000 4-byte characters (80,000 bytes) is over the `VARCHAR` byte limit. In this case, the `VARCHAR` length can be decreased, or the column can be changed to `MEDIUMTEXT`.

Migrating from `utf8mb3` to `utf8mb4` may cause indexes on string columns to exceed the maximum number of bytes per entry (the maximum is dependent on the row format). If this happens, an index prefix length can be added or the existing prefix length can be reduced to bring the number of bytes under the limit.

For more details, see the [MySQL docs for utf8mb3 to utf8mb4 conversion](https://dev.mysql.com/doc/refman/8.0/en/charset-unicode-conversion.html).

##### Padding

Legacy MySQL collations ignore trailing whitespace. For example `SELECT * FROM t WHERE a = 'a'` and `SELECT * FROM t where a = 'a      '` (note trailing whitespace) are functionally identical if column `a` has a legacy MySQL character set. Usually trailing space-aware comparisons are more in line with developer expectations, but it is important to verify that this change in behavior won't adversely affect your application.

For more details, wee the [MySQL docs for collation padding](https://dev.mysql.com/doc/refman/8.0/en/charset-unicode-sets.html#charset-unicode-sets-pad-attributes)

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# MySQL compatibility
Source: https://planetscale.com/docs/vitess/mysql-compatibility

PlanetScale's Vitess product is built on top of open-source Vitess, a database clustering system for horizontal scaling of MySQL. Consequently, PlanetScale is only compatible with MySQL databases.

## Overview

PlanetScale databases run on MySQL 8. If you're [importing an existing database](/docs/vitess/imports/database-imports), PlanetScale supports MySQL database versions `5.7` through `8.0`.

New PlanetScale databases are created on MySQL 8 with character set `utf8mb4_0900_ai_ci`. PlanetScale supports `utf8`, `utf8mb4`, and `utf8mb3`, character sets. We also support `latin1` and `ascii` character sets, but do not recommend them.

## MySQL compatibility limitations

The following reference guide will cover some MySQL syntax, features, and more that PlanetScale either does not support or has limitations around. We are actively working on driving up compatibility, but it's an ongoing effort and will take some time to complete. See this [project board on GitHub](https://github.com/vitessio/docs/vitess/projects/4) to learn what the Vitess team is currently focusing on.

If you're attempting to import a database using our Import tool, there are some additional requirements that you can find in our [Database imports documentation](/docs/vitess/imports/database-imports#import-limitations).

### Queries, functions, syntax, data types, and SQL modes

<Note>
  <Icon icon="exclamation" color="orange" /> = *Limitations in support*

  <Icon icon="xmark" color="red" /> = *Not supported*
</Note>

| Statement                     | Support                           | Description                                                                                                                                                                                                                                                |
| ----------------------------- | --------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `ALTER TABLE...RENAME COLUMN` | <Icon icon="xmark" color="red" /> | Renaming columns and tables may be destructive. See our [guide for column rename recommendations](/docs/vitess/schema-changes/handling-table-and-column-renames).                                                                                          |
| `CREATE DATABASE`             | <Icon icon="xmark" color="red" /> | You cannot `CREATE` a PlanetScale database from the MySQL command line, however, this is supported in the [PlanetScale CLI](/docs/cli/database).                                                                                                           |
| `DROP DATABASE`               | <Icon icon="xmark" color="red" /> | You cannot `DROP` a PlanetScale database from the MYSQL command line, however, this is supported in the [PlanetScale CLI](/docs/cli/database).                                                                                                             |
| `JSON_TABLE`                  | <Icon icon="xmark" color="red" /> | The [`JSON_TABLE` function](https://dev.mysql.com/doc/refman/8.0/en/json-table-functions.html#function_json-table) is not yet supported. All other [JSON SQL functions](https://dev.mysql.com/doc/refman/8.0/en/json-function-reference.html) should work. |
| `PROCEDURE`                   | <Icon icon="xmark" color="red" /> | We do not support any form of [stored routines](https://dev.mysql.com/doc/refman/8.0/en/stored-routines.html).                                                                                                                                             |
| `FUNCTION`                    | <Icon icon="xmark" color="red" /> | We do not support any form of [stored routines](https://dev.mysql.com/doc/refman/8.0/en/stored-routines.html).                                                                                                                                             |
| `TRIGGER`                     | <Icon icon="xmark" color="red" /> | We do not support any form of [stored routines](https://dev.mysql.com/doc/refman/8.0/en/stored-routines.html).                                                                                                                                             |
| `EVENT`                       | <Icon icon="xmark" color="red" /> | We do not support any form of [stored routines](https://dev.mysql.com/doc/refman/8.0/en/stored-routines.html).                                                                                                                                             |
| `LOAD DATA INFILE`            | <Icon icon="xmark" color="red" /> | Loading data via [`LOAD DATA INFILE` is not supported](https://github.com/vitessio/docs/vitess/issues/2976).                                                                                                                                               |
| `KILL`                        | <Icon icon="xmark" color="red" /> | We do not support killing queries or shards from the command line.                                                                                                                                                                                         |
| `:=`                          | <Icon icon="xmark" color="red" /> | The `:=` assignment operator is not yet supported.                                                                                                                                                                                                         |
| `SET GLOBAL time_zone`        | <Icon icon="xmark" color="red" /> | The global time zone is set to UTC and can not be modified.                                                                                                                                                                                                |
| `SET GLOBAL sql_mode`         | <Icon icon="xmark" color="red" /> | The global SQL mode can not be changed permanently. Set each new session's mode instead with `SET sql_mode`.                                                                                                                                               |
| `PIPES_AS_CONCAT`             | <Icon icon="xmark" color="red" /> | Enabling this SQL mode can interfere with Vitess' evalengine parsing the SQL queries so enabling it may result in incorrect or unexpected results. Please use MySQL's standard dialect instead, e.g. `CONCAT()`.                                           |
| `ANSI_QUOTES`                 | <Icon icon="xmark" color="red" /> | Enabling this SQL mode can interfere with Vitess' evalengine parsing the SQL queries so enabling it may result in incorrect or unexpected results. Please use MySQL's standard quotation instead.                                                          |
| `WITH RECURSIVE`              | <Icon icon="xmark" color="red" /> | Experimental support for recursive common table expressions (CTEs) was introduced in Vitess 21 for `SELECT` queries.                                                                                                                                       |

## Miscellaneous

| Action                        | Support                                    | Description                                                                                                                                                                                                                               |
| ----------------------------- | ------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Empty schemas**             | <Icon icon="xmark" color="red" />          | Databases with empty schemas are invalid. You cannot deploy a schema change to production if no tables exist.                                                                                                                             |
| **Non-InnoDB Storage engine** | <Icon icon="xmark" color="red" />          | We only support [InnoDB](https://dev.mysql.com/doc/refman/8.0/en/innodb-storage-engine.html) storage engine.                                                                                                                              |
| **No applicable unique key**  | <Icon icon="xmark" color="red" />          | We require all tables have a [unique, non-null key](/docs/vitess/schema-changes/onlineddl-change-unique-keys) and that respective covered columns are shared between old and new schema.                                                  |
| **Direct DDL**                | <Icon icon="xmark" color="red" />          | We do [not allow Direct DDL](/docs/vitess/schema-changes/how-online-schema-change-tools-work) on production branches when [safe migrations](/docs/vitess/schema-changes/safe-migrations) is enabled. This includes `TRUNCATE` statements. |
| **Binary log access**         | <Icon icon="xmark" color="red" />          | PlanetScale does not currently support binlog replication to external databases.                                                                                                                                                          |
| **Large JSON documents**      | <Icon icon="exclamation" color="orange" /> | MySQL supports JSON documents up to 1 GB in size. However, we do not recommend to store more than a few MB in a JSON document for performance reasons.                                                                                    |

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Online DDL change unique keys
Source: https://planetscale.com/docs/vitess/onlineddl-change-unique-keys

It is possible to modify or replace a table's `PRIMARY KEY`, or any other `UNIQUE KEY`s according to the limitation described below, followed by examples.

## Overview

To migrate data safely and [without downtime](/docs/vitess/schema-changes), PlanetScale requires that all tables have a unique, not-null key. Note that a `PRIMARY KEY` satisfies this condition, and it is generally recommended to always have a `PRIMARY KEY` on all tables.

When you modify a table, both the old and the new schema must have a unique key as described, and the columns covered by those keys must exist in both the old and the new schema.

Essentially this makes it possible for PlanetScale to unambiguously identify and correlate a row between the two schemas.

If you attempt to deploy a schema change which does not comply with the above restriction, the deploy request will fail with the error `Table ... has no shared columns covered by non-null unique keys between both branches.`.

## Examples: allowed changes

In our examples, we assume the base schema to be:

```sql  theme={null}
CREATE TABLE `users` (
	`id` int,
	`other_info` int,
	`username` varchar(128),
	`email` varchar(128),
	PRIMARY KEY (`id`)
);
```

The following are all valid changes to the schema:

### Expanding the PRIMARY KEY

```sql  theme={null}
CREATE TABLE `users` (
	`id` int,
	`other_info` int,
	`username` varchar(128),
	`email` varchar(128),
	PRIMARY KEY (`id`, `other_info`)
);
```

In the above we modified the `PRIMARY KEY` to include `other_info`. This is allowed since both `id` and `other_info` columns exist in both the old and the new schema.

### Moving PRIMARY KEY to a different column

```sql  theme={null}
CREATE TABLE `users` (
	`id` int,
	`other_info` int,
	`username` varchar(128),
	`email` varchar(128),
	PRIMARY KEY (`email`)
);
```

Since both `id` and `email` columns exist in both old and new schema, the deploy request will be allowed. The success of the operation depends on whether `email` actually contains unique values. If there's duplication in `email` values, the deployment will fail with error.

### Moving PRIMARY KEY to different columns

Likewise, there is no problem if the new `PRIMARY KEY` covers multiple columns. Again, the success of the operation depends on the actual uniqueness of the combination of columns.

```sql  theme={null}
CREATE TABLE `users` (
	`id` int,
	`other_info` int,
	`username` varchar(128),
	`email` varchar(128),
	PRIMARY KEY (`username`, `other_info`)
);
```

### Changing PRIMARY KEY and adding/removing other UNIQUE KEYs

```sql  theme={null}
CREATE TABLE `users` (
	`id` int,
	`other_info` int,
	`username` varchar(128),
	`email` varchar(128),
	PRIMARY KEY (`username`, `other_info`),
	UNIQUE KEY `email` (`email`)
);
```

## Examples: invalid changes

Consider the next scenarios and the ways to work around them:

### Changing a PRIMARY KEY to include a new column

```sql  theme={null}
CREATE TABLE `users` (
	`id` int,
	`other_info` int,
	`username` varchar(128),
	`email` varchar(128),
	`new_info` int,
	PRIMARY KEY (`username`, `new_info`)
);
```

This is an invalid change because in the new schema, the `PRIMARY KEY` covers the `new_info` column. But this column does not exist in the old schema.

Consider splitting into two distinct schema changes and deploy requests:

1. First, introduce the `new_info` column.
2. Next, change the `PRIMARY KEY`.

### Changing a PRIMARY KEY and also dropping the old covered column

```sql  theme={null}
CREATE TABLE `users` (
	`other_info` int,
	`username` varchar(128),
	`email` varchar(128),
	PRIMARY KEY (`email`)
);
```

The above is invalid because `id` column, covered by the `PRIMARY KEY` in the old schema, does not exist in the new schema.

Again, consider splitting into two distinct changes:

1. First, change the `PRIMARY KEY`.
2. Next, drop the `id` column.

## Summary

We've seen how, in many scenarios, it's straightforward to modify your table's `PRIMARY KEY` or other keys. For some scenarios, it might take two or more steps to achieve the new schema.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Operating without foreign key constraints
Source: https://planetscale.com/docs/vitess/operating-without-foreign-key-constraints

A **foreign key** is a logical association of rows between two tables, in a parent-child relationship.

## Overview

A row in a "parent" table may be referenced by one or more rows in a "child" table. A foreign key typically suggests how you should `JOIN` tables in most queries. A table can also refer to itself, as a special case.

A **`FOREIGN KEY` *constraint* is a database construct**, an implementation that *forces* the foreign key relationship's integrity (referential integrity). Namely, it ensures that a child table can only reference a parent table when the appropriate row *exists* in the parent table. A constraint also prevents the existence of "orphaned rows" in different methods, as you'll see described soon.

At PlanetScale, we don't *recommend* using foreign key constraints. However, if you still want to use them, you can enable [support for foreign key constraints](/docs/vitess/foreign-key-constraints) in your database settings page.

We still encourage you to use the relational model and associate tables by "pointing" rows from one table to another with foreign keys, just not with the `CONSTRAINT ... FOREIGN KEY` definition.

We'll soon cover an example of a schema with and without foreign key constraints to clarify this slight difference.

## Why does PlanetScale not recommend constraints?

There are a few significant technical reasons why we do not recommend foreign key constraints for some applications:

* MySQL foreign keys introduce increased locking for data and metadata changes. Expect degraded performance, especially in high concurrency workloads.
* Once two tables engage in a foreign key constraint relationship, it is not possible to change the data types for the columns used by the foreign key. For example, it is impossible to change a column from `INT` to `BIGINT`, neither on a parent nor child table.
* As applications evolve, there is a need for schema refactoring. It is a more complex process involving more steps to refactor a schema containing foreign key constraints.
* `FOREIGN KEY` constraints are difficult to maintain once your data grows and is split over multiple database servers. This typically happens when you introduce functional partitioning/sharding and/or horizontal sharding.

<Note>
  You can still think in terms of foreign key relationships; of parent tables and child tables; of rows referencing each
  other. You can structure your tables in the exact same way without FOREIGN KEY constraints as you would with the
  constraints. It's just how these relationships are enforced that changes.
</Note>

To understand how to work without foreign key constraints, we must first understand foreign key functionality.

## Types and behavior of FOREIGN KEY constraints

Consider the following trivial parent-child table relationship:

```sql  theme={null}
CREATE TABLE parent_table (
  id INT NOT NULL,
  PRIMARY KEY (id)
);

CREATE TABLE child_table (
  id INT NOT NULL,
  parent_id INT,
  PRIMARY KEY (id),
  KEY parent_id_idx (parent_id),
  CONSTRAINT `child_parent_fk` FOREIGN KEY (parent_id) REFERENCES parent_table(id) ON DELETE NO ACTION
);
```

<Tip>
  In MySQL, foreign keys are only implemented by the storage engine layer, namely the default and popular InnoDB engine.
  A FOREIGN KEY constraint isn't a separate entity. It lives within the child's table space.
</Tip>

`FOREIGN KEY` constraints are available for row deletion (`ON DELETE`) and row updates (`ON UPDATE`). In this document, we discuss `ON DELETE` as it is the more impactful of the two. The discussion is relevant to `ON UPDATE` constraints, as well.

Foreign keys further support three types of action (illustrated below for `ON DELETE`):

### `ON DELETE CASCADE`

This is the most greedy, or ambitious action type. If you `DELETE` a row from a parent table, any referencing rows in a child table are subsequently deleted within the same transaction. This operation runs recursively for all of a parent's children, as well as for their children, should they also employ `ON DELETE CASCADE`.

`ON DELETE CASCADE` is a risky and resource-consuming action. You intend to `DELETE` a single row, but end up deleting hundreds, thousands, or more, rows in multiple tables. What seemed like a simple transaction now turns into a massive operation, that involves excessive locking, increased MVCC overhead, impact on replication lag, and more.

But perhaps the greatest danger is the potential unexpected loss of data. Whether an unsuspecting developer simply assumes a `DELETE FROM parent_table WHERE id=3` will at most delete one row, down to surprising behavior such as in `REPLACE INTO` queries, which actually run an implicit `DELETE`, leading to mass destruction of data.

The use of `ON DELETE CASCADE` is controversial. Use it with great care. Consider using `NO ACTION` instead.

### `ON DELETE SET NULL`

With this setup, a `DELETE` on a parent (e.g. `DELETE FROM parent_table WHERE id=3`) will set the referencing column on children (e.g. `parent_id` column in `child_table`) to `NULL` for matching rows. It effectively leads to orphaned rows, not very differently from having no foreign key constraints at all.

One advantage is that it's easy to identify the orphan rows: those, and only those rows, will have `NULL` for parent-referencing columns.

Like `CASCADE`, a single row deletion on the parent may lead to multiple rows updated on child tables. This again may cause large transactions, excessive locking, and replication lag.

### `ON DELETE NO ACTION`

Possibly the single most important feature of foreign keys, a `DELETE` on a parent will fail if child rows exist that reference the parent's row. To `DELETE` a row from a parent, the app/user must first `DELETE` the referencing rows from all children. Recursively, if those are further referenced by other tables, the app/user must first `DELETE` rows from grandchildren, and so forth.

This action (or lack thereof) type forces the app to have stronger ownership of its data. An app written to work with `ON DELETE NO ACTION` will organically evolve *knowing* which tables reference which other tables, and will have established `DELETE`/`UPDATE` flows that iterate through tables in the correct order to satisfy referential integrity.

## How does your schema look without FOREIGN KEY constraints?

The above schema would look exactly the same, minus the `CONSTRAINT` clause:

```sql  theme={null}
CREATE TABLE parent_table (
  id INT NOT NULL,
  PRIMARY KEY (id)
);

CREATE TABLE child_table (
  id INT NOT NULL,
  parent_id INT,
  PRIMARY KEY (id),
  KEY parent_id_idx (parent_id)
);
```

<Tip>
  Each FOREIGN KEY constraint requires an index covering the referenced column(s) on both sides of the connection. The index `_parent_id_idx` is **required** by the constraint. We can drop that key in our constraint-free table, depending on the type of queries we might use to retrieve data from the tables.
</Tip>

## Developing an app with no FOREIGN KEY constraints

Consider an app's behavior in an `ON DELETE NO ACTION` setup again: the app *knows* the table's dependencies, handles iteration order for `DELETE`/`UPDATE`. It does everything right.

What happens if we take away the `CONSTRAINT` at this point?

The app remains unchanged. It already runs the proper actions in the proper order. The app operation that ends up running the `DELETE FROM parent_table WHERE id=3` succeeds the app operations that `DELETE` the child table(s). While we lose the database's safety guarantee enforcing this behavior, we are perfectly capable of managing it on our own.

Likewise, an app that grows with a constraint-less schema organically learns to handle `DELETE` and `UPDATE` scenarios. It is in fact given some extra freedom because the *order* of operations is not enforced. This is an advantage because the app is not forced to `DELETE` thousands of dependent rows for each parent row deletion *at that same transaction*. The app may well postpone deletion as we discuss shortly.

Referential integrity is but one of many logical data integrity constraints. It just happens to be one that databases can enforce. Any sizeable application will maintain the integrity of its data with rules the database is unaware of.

## Cleaning up orphaned rows

Consider an `ON DELETE SET NULL` constraint. What happens to the child rows that were set to `NULL`? Typically, there is little or no damage in keeping them around. A query can, for example, `SELECT` rows `WHERE parent_id IS NOT NULL`. Better yet, child rows are often `JOIN`ed with their respective parent rows. Any query running such a `JOIN` will return empty since the parent row does not exist.

Eventually, those rows will pile up, and you will want to reclaim the space.

Without `FOREIGN KEY` constraints, the situation is very much the same, It is possible to `DELETE` a parent row without deleting its dependent children rows. A child row `JOIN`ed with a respective (deleted) parent row comes out empty. There is no `IS NOT NULL` to help you, but identifying those rows is still trivial. Similarly, there is little or no damage in keeping those rows around for a while.

And similarly, those rows eventually pile up.

A common practice, where appropriate, is to `DELETE` rows on a parent table, or perhaps also on a subset of children, but leave some other tables for offline batch processing.

At some convenient time, such as low traffic hours, the app or some batch job will purge orphaned rows. Consider this simplified query:

```sql  theme={null}
DELETE FROM child_table LEFT JOIN parent_table ON (child_table.parent_id=parent_table.id) WHERE parent_table.id IS NULL
```

A single `DELETE` is likely a massive operation, which is to be avoided. A good practice is to break the statement into multiple small-scope statements, e.g. deleting `100` rows at a time.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Pricing
Source: https://planetscale.com/docs/vitess/pricing

A single Vitess database cluster includes the resources equivalent to 5 always-on instances within the base monthly plan cost.

Vitess clusters are charged based on the following:

* Cluster size and [region](#region-pricing)
* Storage size (for network-attached storage clusters)
* [Branch](/docs/vitess/schema-changes/branching) hours beyond the included 1440 hours
* Number of [replicas](/docs/vitess/scaling/replicas)
* Number and size of [VTGates](/docs/vitess/scaling/vtgates)
* [Read-only regions](/docs/vitess/scaling/read-only-regions)

## Branches

Each database includes one production branch that provides a primary instance and two replica instances spread across availability zones.
The primary serves all queries by default, but the replicas can be used to serve read traffic, and are also used to maintain high-availability of your cluster.

In addition to the three instances used by your production branch, \~1440 hours of [development branch](/docs/planetscale-plans#development-branches) time per month is included.
The development branch time works out to two "always on" database instances, though we do recommend spinning them up and down and as needed during your development cycle.

These development branches can replace your existing development or staging environment on other providers.
Instead of purchasing 2-3 databases on PlanetScale to recreate your environment, you can just purchase one database cluster and use [branching](/docs/vitess/schema-changes/branching) for development.

You can upsize and downsize your cluster at any time. Pricing is prorated to the millisecond, so if you temporarily upsize, you will only be charged for the larger cluster size for the time that it was running. Billing for a new cluster size begins as soon as you begin the resize. You can also spin up additional production branches at any timing for additional cost. The pricing for these is also prorated.

## Read-only regions

The read-only region prices below are an additional cost if you choose to utilize [read replicas](/docs/vitess/scaling/read-only-regions) across multiple regions.

The first 10 GB of storage is included in your plan.
After this, you will be charged $1.50 per additional 1 GB used.
Since this data will also reside on your two replicas, this comes out to $0.50 per gigabyte, per database instance.
Also, the binary log space is not counted against your storage usage calculation.
For example, if you have a `PS-10` storing 20 GB of data for the full month, your bill would be $39 + ($1.50 \* 10) = \$54.

<Note>
  Cluster size options are capped at `PS-160` until you have a successfully paid an invoice of at least \$100. If you need larger sizes immediately, please [contact us](https://planetscale.com/contact) to unlock all sizes. You can find the full list of cluster sizes in our [Plans documentation](/docs/planetscale-plans#base).
</Note>

## Region pricing

### Amazon Web Services

<CardGroup>
  <Card title="ap-northeast-1 (Tokyo)" href="https://planetscale.com/pricing?region=ap-northeast" icon="angles-right" horizontal />

  <Card title="ap-south-1 (Mumbai)" href="https://planetscale.com/pricing?region=ap-south" icon="angles-right" horizontal />

  <Card title="ap-southeast-1 (Singapore)" href="https://planetscale.com/pricing?region=ap-southeast" icon="angles-right" horizontal />

  <Card title="ap-southeast-2 (Sydney)" href="https://planetscale.com/pricing?region=aws-ap-southeast-2" icon="angles-right" horizontal />

  <Card title="eu-central-1 (Frankfurt)" href="https://planetscale.com/pricing?region=eu-central" icon="angles-right" horizontal />

  <Card title="ca-central-1 (Montreal)" href="https://planetscale.com/pricing?region=aws-ca-central-1" icon="angles-right" horizontal />

  <Card title="eu-west-1 (Dublin)" href="https://planetscale.com/pricing?region=eu-west" icon="angles-right" horizontal />

  <Card title="eu-west-2 (London)" href="https://planetscale.com/pricing?region=aws-eu-west-2" icon="angles-right" horizontal />

  <Card title="sa-east-1 (Sao Paulo)" href="https://planetscale.com/pricing?region=aws-sa-east-1" icon="angles-right" horizontal />

  <Card title="us-east-1 (N. Virginia)" href="https://planetscale.com/pricing?region=us-east" icon="angles-right" horizontal />

  <Card title="us-east-2 (Ohio)" href="https://planetscale.com/pricing?region=us-east-2" icon="angles-right" horizontal />

  <Card title="us-west-2 (Oregon)" href="https://planetscale.com/pricing?region=us-west" icon="angles-right" horizontal />
</CardGroup>

### Google Cloud

<CardGroup>
  <Card title="asia-northeast3 (Seoul, South Korea)" href="https://planetscale.com/pricing?region=gcp-asia-northeast3" icon="angles-right" horizontal />

  <Card title="northamerica-northeast1 (Montréal, Québec)" href="https://planetscale.com/pricing?region=gcp-northamerica-northeast1" icon="angles-right" horizontal />

  <Card title="us-central1 (Council Bluffs, Iowa)" href="https://planetscale.com/pricing?region=gcp-us-central1" icon="angles-right" horizontal />

  <Card title="us-east4 (Ashburn, Virginia)" href="https://planetscale.com/pricing?region=gcp-us-east4" icon="angles-right" horizontal />
</CardGroup>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Regions
Source: https://planetscale.com/docs/vitess/regions



## Overview

PlanetScale currently offers database deployment in multiple regions. Select the region closest to your application servers to reduce latency between your database and application. Deploy development branches in the region closest to your own location to reduce latency when working with the branch.

You may also add read-only regions to your production database. See our [Read-only regions documentation](/docs/vitess/scaling/read-only-regions) for more information.

A number of resources exist to help find which region has the lowest latency from your location – such as [CloudPing](https://www.cloudping.co/grid#).

## Available regions

<Note>
  If you don't see your preferred region(s) in the following list, [get in touch](https://planetscale.com/contact) to let us know what region(s) you would like to see added. Also, Enterprise plans can be deployed in any region(s) with three availability zones. See the [Deployment options documentation](/docs/plans/deployment-options#single-tenancy-highlights) for more information.
</Note>

Currently, the following regions are supported, with their respective PlanetScale slugs:

### AWS regions

* AWS ap-northeast-1 (Tokyo) — `ap-northeast`
* AWS ap-south-1 (Mumbai) — `ap-south`
* AWS ap-southeast-1 (Singapore) — `ap-southeast`
* AWS ap-southeast-2 (Sydney) — `aws-ap-southeast-2`
* AWS ca-central-1 (Montreal) — `aws-ca-central-1`
* AWS eu-central-1 (Frankfurt) — `eu-central`
* AWS eu-west-1 (Dublin) — `eu-west`
* AWS eu-west-2 (London) — `aws-eu-west-2`
* AWS sa-east-1 (Sao Paulo) — `aws-sa-east-1`
* AWS us-east-1 (Northern Virginia) — `us-east`
* AWS us-east-2 (Ohio) — `aws-us-east-2`
* AWS us-west-2 (Oregon) — `us-west`

### GCP regions

* GCP us-central1 (Council Bluffs, Iowa) — `gcp-us-central1`
* GCP us-east4 (Ashburn, Virginia) — `gcp-us-east4`
* GCP northamerica-northeast1 (Montréal, Québec, Canada) — `gcp-northamerica-northeast1`
* GCP asia-northeast3 (Seoul, South Korea) — `gcp-asia-northeast3`
* GCP europe-west1 (St Ghislain, Belgium) — `gcp-europe-west1`

## Selecting the database region

PlanetScale allows you to select the region for the [`main` branch](/docs/vitess/schema-changes/branching) of your database during database creation. By default, all database branches created within this database will also be created in this region. Once you select a region for your `main` branch, it cannot be changed.

You can also select the region while creating a database via the CLI by using
the `--region` flag with the region's slug.

<Note>
  The default region for all new databases is AWS us-east-2.
</Note>

Here's an example command for creating a database with a different region:

```shell  theme={null}
pscale database create <DATABASE_NAME> --region us-west
```

## Selecting the branch region

PlanetScale allows you to select a region for development branches during
creation as well. By default, it is set to the same region as its database.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/regions/branch.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=eba391015d86aa737ccb15fb097ec0d2" alt="Select your branch region" data-og-width="2160" width="2160" data-og-height="1684" height="1684" data-path="docs/images/assets/docs/concepts/regions/branch.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/regions/branch.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=59c028aa00a9f300d35182ceace15d39 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/regions/branch.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=db5e1cc92f5a5c06dd8aed23fc6cff88 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/regions/branch.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=7ccc2c3bf3243ffd13484d8930481061 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/regions/branch.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=e45170bd7de61ec7d76129304ccd081e 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/regions/branch.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=7bafc6bd620fea40af12569443ba124e 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/regions/branch.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=2d4e9925be1dea65a798a6f2e6ec573a 2500w" />
</Frame>

<Note>
  Once you select a branch region, it cannot be changed.
</Note>

You can also select the region while creating a branch via the CLI by using the
`--region` flag with the region's slug.

Here's an example command for creating a branch with a different region:

```shell  theme={null}
pscale branch create my-production-database add-tables --region eu-west
```

## Restricting the branch regions

[Organization Administrators](/docs/security/access-control#organization-administrator) can restrict branches to only being created in the same region as the one selected during database creation. To enable this setting, check the *Restrict region* setting in the settings page for the database: `app.planetscale.com/<org>/<database>/settings`.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/regions/restrict-2.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=c11d99455ead1c10cd262e59aa6e4aa2" alt="Restrict your branches to one region" data-og-width="1658" width="1658" data-og-height="984" height="984" data-path="docs/images/assets/docs/concepts/regions/restrict-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/regions/restrict-2.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=44f76e329ef6096932b3931eae61e48d 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/regions/restrict-2.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=4da8a68494629cbc66986ac6cfe48f3f 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/regions/restrict-2.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=b6e73db587a61dcc55164b76348bec09 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/regions/restrict-2.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=b918bebe8da232b523755ce37a405cde 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/regions/restrict-2.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=42b156825e80ede9fce3d94e969964c2 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/regions/restrict-2.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=9bf5f9438d21ab6e580d46e9155c5457 2500w" />
</Frame>

## Changing branch and database regions

Once you select a region for a production or development branch, it cannot be changed.

If you do need to move to a different region, we recommend taking the following steps:

<Steps>
  <Step>
    Create a new branch in the new region.
  </Step>

  <Step>
    [Backup and dump](/docs/cli/database) the original branch with:

    ```bash  theme={null}
    pscale database dump <DATABASE_NAME> <BRANCH_NAME>
    ```
  </Step>

  <Step>
    Restore the dump to the new branch with:

    ```bash  theme={null}
    pscale database restore-dump <DATABASE_NAME> <BRANCH_NAME>
    ```
  </Step>

  <Step>
    If this is for a production branch, [promote the new branch](/docs/cli/branch) to production:

    ```bash  theme={null}
    pscale branch promote <DATABASE_NAME> <BRANCH_NAME>
    ```
  </Step>

  <Step>
    Swap out the credentials in your app with the new branch.

    It's important to note that this will require downtime if done on a production branch, as the dump and restore process will take time to complete. To avoid data loss, you can temporarily block writes in your application before doing the dump, and re-enable them after the final credential swap.
  </Step>
</Steps>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Database replicas
Source: https://planetscale.com/docs/vitess/replicas

A replica is a continuously updated copy of your Vitess database.

## Overview

Replicas serve two main purposes:

* They provide an additional source for your data to be queried.
* They increase database availability by enabling fast failovers for maintenance or unexpected failure.

<Warning>
  Before utilizing replicas for reducing load on the primary, it's important to understand the trade-offs. For more information, see the [Data consistency and replication lag](#data-consistency-and-replication-lag) section.
</Warning>

For information on replicas running in additional regions, see: [Read-only regions](/docs/vitess/scaling/read-only-regions).

## How to query replicas

PlanetScale replicas can be used to read data and reduce load on the primary. PlanetScale does not automatically route queries to replicas unless you explicitly use a replica credential or tell your application to do so.

### 1. Create a global replica credential (recommended)

With global replica credentials, you can have one credential that will automatically route queries to your branch's replicas and read-only regions. If you do not have any read-only regions, it will route to the included replicas on [your plan](/docs/planetscale-plans). If you do have any read-only regions, the queries will route to the closest replica out of your read-only region replicas and included failover replicas.

To use this, create a new credential in the PlanetScale dashboard and select "**Replica**" as the connection type. You can then use the new credential's connection string to setup a replica-only database
connection.

If you have `pscale` installed locally, you can create a replica credential with the following command:

```shell  theme={null}
pscale password create <database> <branch> <password_name> --replica
```

All queries made using this credential will be routed to the nearest replica, even as you add and remove read-only regions.

### 2. `USE @replica` (not recommended)

<Warning>
  We do not recommend using this method to query replicas. If you are currently using this method, we recommend switching to method 1.
</Warning>

If you are not using a global replica credential, then, by default, all queries are served by the primary. However you may route any `SELECT` queries to replicas by issuing the following command on a connection:

```sql  theme={null}
USE @replica
```

Once this command is run, all subsequent `SELECT` queries on that connection will be served by your branch's replicas instead of the primary node in your cluster. Please note that this does not send queries to any read-only regions, if you want to route queries all replicas, including read-only regions, you should use a global replica credential (method 1 above).

<Tip>
  For querying replicas in separate regions, see: [Read-only regions](/docs/vitess/scaling/read-only-regions).
</Tip>

We highly recommend using a global replica credential to ensure that your queries are always routed to the nearest replica.

## High availability

Replicas within PlanetScale are used to enable high availability of your database. This is a part of the reason all production branches in PlanetScale are provided at least one replica. In situations where the underlying hardware or service hosting the primary MySQL node fails, our system will automatically elect a new primary node from the available replicas and reroute traffic to that new primary. This process is know as **reparenting** and typically is all handled within milliseconds or seconds.

Querying the primary during a reparent typically goes unnoticed, other than a bit of additional query latency.
This is because Vitess [buffers queries](https://vitess.io/docs/api/reference/features/vtgate-buffering) during this time.
You can incorporate retry logic into your application to handle the rare instances where an issue arises during reparenting.

When querying a replica during a reparent, you may encounter this error:

```
no healthy tablet available for 'keyspace:"${keyspace}" shard:"${shard}" tablet_type:REPLICA'
```

Incorporating retry logic into your application can help with this scenario as well.

## Multiple availability zones

In cloud architecture, regions are further broken down into data centers known as availability zones (or AZs for short). For example, the `us-east-1` region on AWS contains 6 default AZs available to customers starting with `us-east-1a` through `us-east-1f`. The infrastructure for all Scaler Pro and Enterprise PlanetScale databases are distributed across 3 availability zones. In the instance of an AZ failure, your database will auto failover to an available AZ.

## Data consistency and replication lag

Whenever data is updated (`INSERT`, `UPDATE`, `DELETE`) on the primary node, those changes are synchronized to the replicas shortly after. The delay between when a primary is updated and the changes are applied to the replica is known as `replication lag`. Your databases replication lag is viewable on your [database dashboard](/docs/vitess/architecture#replication-lag-at-a-glance). Replication lag can also be viewed on Datadog if you set up the [PlanetScale - Datadog integration](/docs/vitess/integrations/datadog).

In Datadog, you can use the following formula to set alerts for replication lag:

```
(max:planetscale.replication_lag{ps_database:<DATABASE_NAME> ps_tablet_type:replica, ps_branch:<MAIN>})
```

Make sure you replace `<DATABASE_NAME>` with your PlanetScale database name and `<MAIN>` with the name of the branch for which you'd like to track replication lag.

It is important to be aware of replication lag whenever querying data from your replicas. For example, if you make an update and then immediately try to query for that updated data via a replica, it may not be available yet due to replication lag.

## When should you use replicas in your application?

Replicas are useful for offloading read-heavy workloads from the primary node. By using replicas, you can distribute the read load across multiple nodes, which can help improve the performance of your application. Some examples of where you might want to query a replica are scheduled jobs, analytics jobs, search features, or aggregate queries. Replicas can also be used to provide a read-only view of your data to users or applications that do not need to write data, such as when a user is logged out or writing one-off queries for debugging purposes.

## When are replicas used in PlanetScale?

We use replicas for every production database branch. The number of replicas for a given database depends on the selected plan for that database:

* **Scaler Pro** — Scaler Pro databases include 2 replicas per production branch distributed across multiple AZs in a given region.
* **Enterprise** — The Enterprise plan is customizable to fit the needs of your organization, and as such can have as many replicas as needed.

[Read-only regions](/docs/vitess/scaling/read-only-regions) use the same replica configuration as their respective database, they are just hosted in a different geographical region. It is important to note that the MySQL nodes in read-only regions are replicas intended only for reading data and are not eligible for failover if the primary node experiences an outage.

<Warning>
  Development branches DO NOT have replicas as they are intended for development only and are not designed with the same resiliency as production branches.
</Warning>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Cluster configuration
Source: https://planetscale.com/docs/vitess/scaling/cluster-configuration

The Clusters page in your PlanetScale dashboard allows you to configure your PlanetScale cluster.

From here, you can:

* Adjust the instance sizes for [keyspaces](/docs/vitess/sharding/keyspaces)
* Create sharded or unsharded keyspaces
* Switch between [Metal](/docs/plans/planetscale-skus#metal) and [network-attached storage](/docs/plans/planetscale-skus#network-attached-storage)
* Adjust the number of replicas for each keyspace
* Adjust the VSchema
* Adjust the number and size of [VTGates](/docs/vitess/scaling/vtgates)
* Turn on [VTGate autoscaling](/docs/vitess/scaling/vtgates#autoscaling-vtgates)
* View the CPU and memory utilization of your VTGates
* View any changes to your VTGates
* Adjust the [memory allocated to vector indexes](/docs/vitess/vectors#resource-requirements) and the InnoDB buffer pool, for any branch with [vectors](/docs/vitess/vectors) enabled
* Configure [VReplication settings](#vreplication-settings)
* Configure [replication durability constraints](#replication-durability-constraints)

This documentation will cover how to use everything on this Clusters page. For a full walkthrough with an example of setting up a sharded keyspace, refer to the [Sharding quickstart](/docs/vitess/sharding/sharding-quickstart).

If you would like additional support configuring your sharded keyspaces, our [Enterprise plan](/docs/planetscale-plans#planetscale-enterprise-plan) may be a good fit. [Get in touch](https://planetscale.com/contact) for a quick assessment.

## Adjust your cluster size

To adjust your cluster size, first select the [keyspace](/docs/vitess/sharding/keyspaces) you'd like to adjust. If you only have one keyspace, it will be selected by default.

Click on the "Cluster size" dropdown. Select the new cluster size, ([Metal](/docs/metal) or network-attached storage). If you select Metal, make sure you select the correct storage size. Finally. click "Save".

A network-attached storage cluster (Amazon Elastic Block Storage or Google Persistent Disk) may take a few minutes to change sizes. And a Metal cluster can take hours, depending on the size.

You can check the status of the resize from the [database homepage](/docs/vitess/architecture#resizing) in the dashboard, [CLI](/docs/cli/keyspace), or [API](https://planetscale.com/docs/api/reference/get_keyspace_rollout_status). There will be no downtime or locking during this process.

For more information about selecting a cluster size, see the [Cluster sizing documentation](/docs/plans/cluster-sizing).

## Create a keyspace

<Warning>
  Misconfiguration to keyspaces can cause availability issues. We recommend thoroughly reading through the documentation
  in the [Sharding section](/docs/vitess/sharding) of the docs prior to adding additional keyspaces. If you have any
  questions, please [reach out to our support team](https://docs/support.planetscale.com).
</Warning>

<Warning>
  If you are using [Vitess global routing](https://vitess.io/docs/api/reference/features/global-routing/), you must take extra care when adding a sharded keyspace to your database (for example, if you are using `@primary`).
  Before creating one, you must ensure that all tables from your first *unsharded keyspace* are added to the `VSchema` of that *unsharded keyspace*. Eg:

  ```
  {
    "tables": {
      "users": { }
      ...
    }
  }
  ```

  Otherwise, queries will fail. [Learn more about VSchema.](/docs/vitess/sharding/vschema)
</Warning>

<Note>
  Sharded keyspaces are not currently supported on databases that have foreign key constraints enabled.
</Note>

To create a new [keyspace](/docs/vitess/sharding/keyspaces):

<Steps>
  <Step>Select the database you want to configure.</Step>
  <Step>Click "Clusters" in the left nav.</Step>
  <Step>You should see the existing unsharded keyspace for your database here.</Step>
  <Step>Click "New keyspace".</Step>

  <Step>
    Enter the keyspace name. For example, if your existing unsharded keyspace is named `metal`, you may create a sharded
    keyspace named `metal-sharded`.
  </Step>

  <Step>
    Select whether you want to keep it unsharded, or, if not, select the number of shards you want to exist in this
    keyspace. In most cases, you will be adding a new sharded keyspace. Adding a new unsharded keyspace is not a common
    use case, but is an option if you're looking to do [vertical
    sharding](https://planetscale.com/learn/courses/database-scaling/sharding/vertical-sharding).
  </Step>
</Steps>

<Note>
  The cost of adding this additional keyspace largely depends on the number of shards you choose, the cluster size, and if you'd like to add additional replicas.
</Note>

<Steps>
  <Step>
    Choose the cluster sizes you would like to use for this keyspace. Keep in mind, if you are creating a sharded
    keyspace, this will spin up multiple clusters of the selected size. For example, if you are creating 4 shards and
    choose the `PS-80` cluster size, we will create 4 `PS-80`s, each with 1 primary and 2 replicas.
  </Step>

  <Step>
    Select the number of *additional* replicas, if any, that you'd like to add to each cluster. Each cluster comes with
    2 replicas by default, so any number you choose will be in addition to those 2.
  </Step>

  <Step>
    Review the new monthly cost for this keyspace below. This is in addition to your existing unsharded keyspace, as
    well as any other keyspaces you add.
  </Step>

  <Step>Once satisfied, click "Create keyspace".</Step>
</Steps>

## Modify the VSchema of a keyspace via Clusters page

<Note>
  You can modify the VSchema on your development branch either on the Clusters page , using the [`ALTER VSCHEMA` command](/docs/vitess/sharding/vschema#modifying-vschema), or with the pscale CLI using [`pscale keyspace vschema update`](/docs/cli/keyspace).
</Note>

Once you have created your keyspace, you will see a new tab: **VSchema**. The VSchema contains information about how the keyspace is sharded, sequence tables, and other Vitess schema information. The VSchema tab allows you to configure the Vschema for your new keyspace or modify it for existing keyspaces.

We do not recommend modifying the VSchema directly on your production branch. In fact, it is not possible to do if you have [safe migrations](/docs/vitess/schema-changes/safe-migrations) enabled (as recommended). Instead, to modify the VSchema, you should first [create a new development branch](/docs/vitess/schema-changes/branching). Once you have your branch ready, follow these steps:

<Steps>
  <Step>
    To update the VSchema on the Clusters page, select your new development branch from the dropdown at the top, and
    then select the keyspace below that has the VSchema you'd like to modify.
  </Step>

  <Step>Next, click the tab labeled "VSchema".</Step>

  <Step>
    Modify the VSchema configuration JSON file as needed. Refer to the [VSchema
    documentation](/docs/vitess/sharding/vschema) for more information about the available options.
  </Step>

  <Step>
    When finished, click "Save changes". We will validate your VSchema, and if it is valid, the changes will be saved.
    If there are errors, we will warn you here to change them before saving.
  </Step>

  <Step>
    Go back to your "Branches" tab and click on the development branch that you modified. You should see a note on the
    right that says "Updated VSchema configuration" which lets you know the VSchema(s) for this branch has been
    modified.
  </Step>

  <Step>
    From here, go through the normal [deploy request process](/docs/vitess/schema-changes/deploy-requests) to deploy
    this change to production.
  </Step>
</Steps>

Once your change is deployed to production, you can come back to the Clusters page, switch to your production branch, and view the updates to your VSchema. You can also click the "Changes" tab to see information, such as the resize event, status, and start/end time for any previous changes to the VSchema.

## Modify routing rules

This configuration setting is currently only available for some Enterprise customers. To modify your routing rules, click "Manage routing rules" on the bottom left of the keyspace configuration panel.

Again, you will need to create a new branch to modify routing rules, as described in the "Modify the VSchema of a keyspace" section above.

## Modify keyspace settings

There are a number of keyspace-specific settings you can use to customize keyspace behavior.

### Replication durability constraints

By default, replication to replica and read-only VTTablets is configured to maximize safety and data integrity. This behavior can be relaxed for performance improvements and reduced replication lag.

* **Maximum** — Default setting; use maximum durability constraints.
* **Dynamic** — Use maximum durability constraints when replication lag is under 5s, and automatically relax durability constraints when replication lag exceeds 5s. Durability constraints are automatically set back to maximum when replication lag reduces to under 5s.
* **Minimum** — Reduce durability constraints. Optimizes for replica and read-only performance, but has highest risk of data loss on crashed instances.

### VReplication settings

These settings improve performance during [VReplication](https://vitess.io/docs/api/reference/vreplication/vreplication/) processes like deploy requests and workflows.

* **Optimize inserts** — Enabled by default. When enabled, during binlog replication catch-up, skip sending insert events for rows that have yet to be copied. For more technical details, see the corresponding [Vitess implementation](https://github.com/vitessio/docs/vitess/pull/7708).
* **Allow NOBLOB binlog row image** — Enabled by default. When enabled, then we support enabling MySQL’s NOBLOB binlog mode, to omit unchanged BLOB and TEXT columns from replication events, reducing binlog size. For more technical details, see the corresponding [Vitess Implementation](https://github.com/vitessio/docs/vitess/pull/14502).
* **Batch binlog statements** — Disabled by default. If enabled, batches binlog statements and transactions to limit the number of round-trips to MySQL. For more technical details, see the corresponding [Vitess implementation](https://github.com/vitessio/docs/vitess/pull/14502).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Cluster sizing
Source: https://planetscale.com/docs/vitess/scaling/cluster-sizing



export const YouTubeEmbed = ({id, title}) => {
  return <Frame>
      <iframe src={`https://www.youtube-nocookie.com/embed/${id}?rel=0`} title={title} className="aspect-video w-full" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" />
    </Frame>;
};

You can easily upsize and downsize your database cluster from within the PlanetScale dashboard. This documentation covers some information about selecting a cluster size upon database creation as well as how to upsize and downsize.

<Note>
  If you are on a consumption commitment plan, please be aware that any changes in cluster size will be reflected against your monthly or annual consumption commitment amount. Changes to the originally selected cluster size may cause you to utilize this amount either more quickly or slowly. If you have further questions, please reach out to your account manager or our [Support](https://planetscale.com/contact) team.
</Note>

<YouTubeEmbed id="y94VExata3A" title="Resize your database" />

## Selecting a cluster size

Selecting the correct cluster size for your database can have a dramatic impact on how it performs and how much it costs.

A good rule of thumb is when you notice CPU usage is consistently at or close to 100% for an extended period of time, you may benefit from [upsizing your cluster](/docs/plans/planetscale-skus/#upsizing-and-downsizing-clusters). Conversely, if your CPU usage is consistently below 50%, you may be able to downsize. You can monitor your CPU usage by clicking on your database, clicking "Primary" in your architecture diagram, and referencing the chart under "Metrics and performance".

<Warning>
  For Metal instances, you have to consider both the compute and the storage, as storage does not autoscale. For more information about adjusting a Metal instance, see [Upgrading an existing database to Metal](/docs/metal/create-a-metal-database#upgrading-an-existing-database-to-metal).
</Warning>

There are also special cases where you may want to temporarily upsize out of caution if you're anticipating a large spike in traffic, such as during a launch or event. In these cases, you can easily [upsize](/docs/plans/planetscale-skus#upsizing-and-downsizing-clusters) ahead of your event, and then downsize after. Changing cluster sizes is a seamless operation that requires no downtime.

If you are switching between [network-attached storage](/docs/plans/planetscale-skus#network-attached-storage) (Amazon Elastic Block Storage or Google Persistent Disk) and [Metal](/docs/plans/planetscale-skus#metal), or changing the size of your Metal instance, be aware this switch takes additional time. Again, no downtime is required.

### Comparing PlanetScale to other database providers

If you are migrating from an existing cloud provider with resource-based pricing, be sure to compare your currently selected instance with our available cluster sizes.

Keep in mind, each database comes with a production branch with two replicas, as well as 1,440 hours worth of development branches. The development branches essentially equate to two extra "always on" databases. In many cases, you can deprecate your dev/staging databases that you pay extra for with other providers in favor of the development branches. In the end, this usually results in significant cost savings.

Databases in PlanetScale also come with additional beneficial infrastructure that is not easily configured or available in other hosted database solutions. For more information on what is provisioned with each database, read our [Architecture](/docs/vitess/architecture) doc.

If you are unsure which plan or cluster size is right for your application, [contact us](https://planetscale.com/contact) to get further assistance.

Our self-serve plans are flexible enough to handle the majority of customers. However, there are several use cases where you may need a more custom plan. This is where our Enterprise offerings shine.

### Upsizing and downsizing clusters

As your application scales, upgrading or downgrading your database cluster is a seamless operation that does not involve any downtime.

To change cluster sizes, go to your PlanetScale dashboard, click on your database, click ["Clusters"](/docs/vitess/cluster-configuration), select the new cluster size for the [keyspace](/docs/vitess/sharding/keyspaces) you wish to configure, and click "Update".

<Note>
  If you have [maintenance schedules](/docs/vitess/maintenance-schedules) enabled, changes to cluster size will roll out during your scheduled maintenance window.
</Note>

The time it takes to change sizes depends on the size and region of your database. Larger databases may take 20 minutes to upsize/downsize. However, this is all done online, so you will not experience any downtime. Keep in mind, once you update your cluster size, you cannot change sizes again until the first size change completes.

When you choose to change cluster size on network-attached storage drives, we upgrade each of your replicas one by one: delete the tablet container, create a new tablet container of the new size, attach the persistent volume, start it up, and connect it to the primary. Once that's complete, we fail the primary over to one of those new replicas, and do the same thing to the old primary.

On Metal, this works a little differently. Instead of upgrading replicas one by one, we spin up 3 new Metal instances in addition to your existing replicas. We catch the three new instances up by restoring a backup and then catching them up via MySQL replication. When those replicas are fully caught up, we reparent the primary to one of them, and begin to remove the old tablets. For this reason, you may notice a minimum of 5 replicas as you're upgrading your Metal instance size.

## Sharding

You can create sharded keyspaces on any plan by adding a new sharded keyspace using the [Clusters page](/docs/vitess/cluster-configuration) and running an [unsharded to sharded workflow](/docs/vitess/sharding/sharding-quickstart) in your dashboard.

If you would like additional support from our expert team, our [Enterprise plan](/docs/planetscale-plans#planetscale-enterprise-plan) may be a good fit. [Get in touch](https://planetscale.com/contact) for a quick assessment.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Vitess system limits
Source: https://planetscale.com/docs/vitess/scaling/planetscale-system-limits



<Note>
  We can sometimes manually adjust limits on a per-database level. If you are facing issues or have questions, the best course of action is to [open a support ticket](https://planetscale.com/contact).

  Additionally, you can find some solutions to specific error codes in the [Errors documentation](/docs/vitess/troubleshooting/errors).
</Note>

## Table limits

Database schemas are limited to a total of `2048` tables, including views.

Individual tables are limited to a maximum of `1017` columns each.

## Disk limits

For [**network-attached storage**](/docs/plans/planetscale-skus#network-attached-storage) databases, the disk size will auto-scale up to a maximum of 4 TB.
For [**Metal** databases](/docs/plans/planetscale-skus#metal), the disk size is dictated by what you choose when the database is created.
Disk sizes range from 110 GB to 7.4 TB.
Which sizes are available depends on the compute instance chosen.

This space is used both for user data as well as system data required to run database operations.

If your database is near this limit, some deploy request operations may not be available as they require adequate disk space to complete.

When near this limit, we recommend either [sharding your data](/docs/vitess/sharding) or distributing tables across multiple keyspaces.
For Metal databases, you can try upgrading to a larger disk size.
Please [contact us](https://planetscale.com/contact) to discuss options.

## Connection lifetime limits

Database client connections that are held open longer than `24 hours` may be terminated unexpectedly. We recommend that long-running database connections are closed and reconnected at least once per day.

## Simultaneous transaction limits

Each database has a limit on the number of simultaneous *transactions* it can process, also known as the *transaction pool*.
If you exceed the *transaction pool* setting for your database, you may encounter this error:

```
vttablet: rpc error: code = ResourceExhausted desc = transaction pool connection limit exceeded
```

This can often be mitigated by trying one of the following solutions:

A) Reduce the amount of parallelism in the requests being sent to the database. <br />
B) Shorten lengthy transactions by reducing batch sizes or making some other application-level adjustment.

If you cannot make such changes to your application, consider choosing a larger instance type with a larger transaction pool.
The exact limit varies depending on the instance type of your database.
For details, see the [plans page](/docs/plans/planetscale-skus).

## Query limits

PlanetScale has enforced some system limits to prevent long-running queries or transactions from:

* Piling up and consuming all available resources.
* Blocking other important, short-lived queries and transactions from completing.
* Overloading the database to the point where it is not recoverable without a restart.
* Blocking planned failovers and critical upgrades.

The following table details these limits:

| Type                                         | Limit  |
| :------------------------------------------- | :----- |
| Per-query rows returned, updated, or deleted | 100k   |
| Per-query result set total size              | 64 MiB |
| Per-query autocommit timeout                 | 900s   |
| Per-transaction timeout                      | 20s    |

### Recommendations for handling query limits

These limits are enforced for the safety of your database. However, we do understand you may run into a situation where the limits are a blocker. Here are some best practices for solving common issues presented by the limits:

**What should I do if I have a query that returns more than 100,000 records?**

We recommend trying to break up large queries, e.g. through [pagination](https://planetscale.com/blog/mysql-pagination).

**What should I do if I have a query that returns more than 64 MiB of data?**

If your schema currently relies on storing large amounts of variable length data within `JSON`, `BLOB`, or `TEXT` type columns that is regularly over a few MiB in size, you will want to strongly consider storing that variable length data outside of your database, such as within an object storage solution, instead.

If large values are stored within variable length data columns, it can limit the number of rows you can return.

For example, above we described a 100K limit for result sets, but if your result set's size exceeds the 64 MiB limit then you may receive an error message like the following: `resource_exhausted: grpc: received message larger than max (<RESULT_SET_SIZE_IN_BYTES> vs. 67108864)` when retrieving much less than 100K rows.

Similarly, when generating a large `INSERT` or `UPDATE` query for these types of columns you may run into the `grpc: trying to send message larger than max (<QUERY_SIZE_IN_BYTES> vs. 67108864)` error message which would require you to reduce the query's size.

**What should I do if I have a transaction that runs longer than 20 seconds?**

For transactions that are running for longer than 20 seconds, we recommend breaking these up into multiple shorter transactions. For analytics queries that are not possible to break up, see "How should I handle analytical queries?" below.

Keep your transactions small and short-lived. For transactional workloads that are inherently long-lived, consider replacing or complementing MySQL transactions with other techniques:

* For simple workloads (e.g. a `SELECT`, followed by an HTTP request, followed by an `UPDATE`), consider using optimistic locking instead of transactions.
* For complex workloads, consider using sagas to coordinate a series of small, short-lived transactions that possibly span multiple microservices.

**What should I do if I have a select query that needs to timeout faster than 900 seconds?**

For select statements only, you can provide an inline comment to kill the query if it has not returned results within the specified time in milliseconds. `select /*vt+ QUERY_TIMEOUT_MS=2000 */ sleep(3);` Please note, you can set this value shorter than 900 seconds but not greater than the 900 seconds default.

**How should I handle analytical queries?**

We recommend using [PlanetScale Connect](https://planetscale.com/blog/extract-load-and-transform-your-data-with-planetscale-connect) (via [Airbyte](/docs/vitess/integrations/airbyte) or [Stitch](/docs/vitess/integrations/stitch)) to extract your data to any compatible destination (e.g. BigQuery, Redshift, etc.). The analytics queries should then be performed at those destinations.

### OLAP mode

While it is possible to bypass these safety limits using `OLAP` mode (`SET workload = OLAP`), we do not recommend this for the reasons listed at the beginning of this doc. In OLAP mode, you may return more than 100k rows from a single query, but the 100k row limit will still apply to updates and deletes.

When the use of OLAP queries is strictly unavoidable, we recommend:

* Where possible, sending those queries to a replica. Every PlanetScale database comes with at least two replicas. Learn how to send queries to replicas using [global replica credentials](/docs/vitess/scaling/replicas#how-to-query-replicas).
* Carefully and regularly reviewing the performance of those queries with [PlanetScale Insights](/docs/vitess/monitoring/query-insights).

<Note>
  If you choose to use OLAP mode, be prepared to handle errors that may occur if the connection to PlanetScale gets interrupted.
</Note>

## Reducing table size without `OPTIMIZE TABLE`

If you delete several rows in a table, you may wish to reclaim that storage space. The MySQL [`OPTIMIZE TABLE` statement](https://dev.mysql.com/doc/refman/en/optimize-table.html) allows you to reorganize the physical storage of table data to reduce its storage requirements.

However, `OPTIMIZE TABLE` is a locking operation, so you cannot use it unless you disable [safe migrations](/docs/vitess/schema-changes/safe-migrations) in PlanetScale, which is not recommended.

Any time you create and deploy a deploy request with safe migrations on, our schema change process uses [online DDL](/docs/vitess/schema-changes/how-online-schema-change-tools-work), and in doing so, recreates the table that you are modifying — thus reclaiming the storage space.

But there are often cases where you do not need to make a schema change for a while, but you'd like to free up the space. In those cases, we suggest you do the following:

<Steps>
  <Step>
    Create a new branch.
  </Step>

  <Step>
    Run the following command:

    ```sql  theme={null}
    ALTER TABLE <TABLE_NAME> COMMENT 'Optimize table size via DR - <YYYY-MM-DD>';
    ```

    Where `<TABLE_NAME>` is the name of your table and `<YYYY-MM-DD>` is the current date.
  </Step>

  <Step>
    Create a deploy request, and merge it into production.

    If you use this option you will likely want to [throttle the deploy request](/docs/vitess/schema-changes/throttling-deploy-requests#managing-throttler-settings-per-deploy-request) running the `ALTER TABLE`.
  </Step>
</Steps>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Read-only regions
Source: https://planetscale.com/docs/vitess/scaling/read-only-regions



## Overview

Replicate your production database across the globe by creating read-only regions in any available [PlanetScale region](/docs/vitess/regions).

This feature supports globally distributed applications by enabling your database to perform low latency reads in the regions closest to your applications and users.

## How to create a read-only region

<Steps>
  <Step>
    In the [PlanetScale dashboard](https://app.planetscale.com), select the database you want to add a read-only region to.
  </Step>

  <Step>
    Navigate to the "**Branches**" page.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/read-only-regions/landing-to-branches.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=bb6f4651119b4ce2424abb724a860527" alt="landing-to-branches" data-og-width="3606" width="3606" data-og-height="1476" height="1476" data-path="docs/images/assets/docs/concepts/read-only-regions/landing-to-branches.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/read-only-regions/landing-to-branches.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=60848448f979218caf88d21c89f94f97 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/read-only-regions/landing-to-branches.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=ba3032ae40fdceb76a1f13ec6816dc32 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/read-only-regions/landing-to-branches.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=01b4933edf8ffa7366660436c22b2aff 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/read-only-regions/landing-to-branches.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=d8f394649ca2c88607d471484cdd6347 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/read-only-regions/landing-to-branches.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=467059d8b85abfc61659caf3894c3bad 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/read-only-regions/landing-to-branches.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=e8ae806f57ae343d0393c93bd0b8ac28 2500w" />
    </Frame>
  </Step>

  <Step>
    Select the current production branch.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/read-only-regions/branches-to-production.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=db67ef90e6d7ef3db80f348223fe0a94" alt="branches-to-production" data-og-width="3358" width="3358" data-og-height="1184" height="1184" data-path="docs/images/assets/docs/concepts/read-only-regions/branches-to-production.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/read-only-regions/branches-to-production.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=bfdec70d56386f2da5019cfb7e781d8b 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/read-only-regions/branches-to-production.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=a84d42bff9444cd7c2429ce4b7e240d9 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/read-only-regions/branches-to-production.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=d534cdd064c1ce0b923c8428a898c82b 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/read-only-regions/branches-to-production.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=ab9d8fe0425aede89632e0d5dabee77d 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/read-only-regions/branches-to-production.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=614ae78faf1f5005cdd72ef5ef54291c 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/read-only-regions/branches-to-production.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=162fb0982d31c57debf20dc2602c366c 2500w" />
    </Frame>
  </Step>

  <Step>
    On the right-side menu, click the "**Add region**" button.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/read-only-regions/add-region.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=a7683c2072c3833c45ecddf4440f0dda" alt="add-region" data-og-width="1296" width="1296" data-og-height="1034" height="1034" data-path="docs/images/assets/docs/concepts/read-only-regions/add-region.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/read-only-regions/add-region.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=7e7eddb10b94582eba1c1f2e6c01d503 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/read-only-regions/add-region.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=5669d6d1adbb227c0741ac92262c711a 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/read-only-regions/add-region.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=f2ec2e6e9432c741cd4479abe25ed7db 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/read-only-regions/add-region.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=95e6f69f990198668ea6addde3310621 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/read-only-regions/add-region.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=c55e829cf72366875490ad9c34ad027d 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/read-only-regions/add-region.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=53b3aa4d2ba369099a8ad508b0611fc9 2500w" />
    </Frame>
  </Step>

  <Step>
    Select the desired AWS region from the dropdown of [available regions](/docs/vitess/regions) in the modal.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/read-only-regions/modal.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=9680a8cfd4d203d17c2d175f12831389" alt="modal" data-og-width="3099" width="3099" data-og-height="1469" height="1469" data-path="docs/images/assets/docs/concepts/read-only-regions/modal.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/read-only-regions/modal.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=1ea6773f61139e7b8f66c81be3b0d182 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/read-only-regions/modal.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=9538ebe4143f4f08ba6b8a69ec3099be 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/read-only-regions/modal.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=0c8e000c36fab73dc792793af155f11d 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/read-only-regions/modal.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=e870a02d97ca4cf06911f6201ee461a9 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/read-only-regions/modal.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=e7f32026cf848b179a75eff7e7e94d19 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/read-only-regions/modal.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=6351359432a80b2983a82618210adf2d 2500w" />
    </Frame>
  </Step>

  <Step>
    Click "**Add region**" and wait for your data to finish initially replicating across regions.
  </Step>
</Steps>

## How to remove a read-only region

<Steps>
  <Step>
    Go to your database's production branch.
  </Step>

  <Step>
    Click on the "**...**" at the top right of the region that you want to delete.
  </Step>

  <Step>
    Click "**Delete region**".
  </Step>
</Steps>

Once you delete a region, you will no longer be charged for the storage or row reads associated with that region. If you were using global replica credentials, you do not need to take any additional action. Read queries will still be sent to the closest replica for any queries that are using global replica credentials.

## How to query a read-only region

Connecting to a read-only region requires using a [replica credential](/docs/vitess/scaling/replicas). You can create a global replica credential by following these steps:

<Steps>
  <Step>
    Go to your database's production branch.
  </Step>

  <Step>
    Click on the "Connect" button in the top right
  </Step>

  <Step>
    On the "Connect" page, select "Replica" as the connection type.
  </Step>

  <Step>
    Click "Create password" to generate a new username and password pair.
  </Step>
</Steps>

Alternatively, you can create a connection string by going to your database settings page > "**Passwords**" > "**New password**".

All queries made using this password will be routed to your branch's replicas or the nearest read-only region. If you want to route queries to a specific read-only region, you can go to the "**Passwords**" page within your database's settings page and select the created password. Under "**Database endpoint**", you can then select "**Direct**" and choose your desired host from the "**Host**" dropdown.

## Concepts

### Replication across regions

PlanetScale replicates your data across regions with an asynchronous strategy, first storing your changes in the primary region and then forwarding them to your read-only region(s). The time that it takes those changes to propagate to your read-only region can be defined as "replication lag" and be measured by issuing the following statement to your read-only regions:

```sql  theme={null}
SELECT max_repl_lag();
```

The `max_repl_lag()` function will return an instantaneous measurement of the maximum amount of seconds it has been since your read-only region has stored changes made to your primary region.

### Read-only connections

Connecting to a read-only region will allow you to query your data, but will not allow you to insert, update, or delete it.

## Availability and pricing

Read-only regions are available on [Scaler Pro and multi-tenant Enterprise plans](/docs/planetscale-plans). Read-only regions are priced differently depending on the selected region. You can find a full list of pricing in the [Scaler Pro cluster pricing documentation](/docs/plans/cluster-sizing).

### Storage costs

Your storage costs will increase linearly with the amount of read-only regions added.
Adding new read-only regions will always be billed as standalone storage and will not count toward your included storage.

As an example, let's say you're on our Scaler Pro plan with 10 GB of included storage and your primary contains 7 GB of data.
If you have two read-only regions, each one will be charged at our additional storage rate, for a total of 14 GB.
The read-only region storage rate is\ $0.75 per GB, and in this case would lead to an additional storage charge of\ $10.50.

For more information on storage billing costs, see our [Billing documentation](/docs/planetscale-plans).

## Pricing

Below we include the costs for read-only regions in each available region up to `400`-level instances.
Larger sizes are available in the app.

### AWS ap-northeast-1 (Tokyo)

| Read-only region | Price   |
| ---------------- | ------- |
| PS-10RR          | \ \$19  |
| PS-20RR          | \ \$28  |
| PS-40RR          | \ \$48  |
| PS-80RR          | \ \$86  |
| PS-160RR         | \ \$168 |
| PS-320RR         | \ \$336 |
| PS-400RR         | \ \$480 |

| Metal read-only region | Storage | Price   |
| ---------------------- | ------- | ------- |
| M-160RR                | 110 GB  | \ \$296 |
| M-160RR                | 460 GB  | \ \$324 |
| M-320RR                | 229 GB  | \ \$588 |
| M-320RR                | 929 GB  | \ \$640 |

### AWS ap-south-1 (Mumbai)

| Read-only region | Price   |
| ---------------- | ------- |
| PS-10RR          | \ \$10  |
| PS-20RR          | \ \$15  |
| PS-40RR          | \ \$26  |
| PS-80RR          | \ \$46  |
| PS-160RR         | \ \$91  |
| PS-320RR         | \ \$182 |
| PS-400RR         | \ \$260 |

| Metal read-only region | Storage | Price   |
| ---------------------- | ------- | ------- |
| M-160RR                | 110 GB  | \ \$256 |
| M-160RR                | 460 GB  | \ \$312 |
| M-320RR                | 229 GB  | \ \$508 |
| M-320RR                | 929 GB  | \ \$620 |

### AWS ap-southeast-1 (Singapore)

| Read-only region | Price   |
| ---------------- | ------- |
| PS-10RR          | \ \$19  |
| PS-20RR          | \ \$28  |
| PS-40RR          | \ \$48  |
| PS-80RR          | \ \$86  |
| PS-160RR         | \ \$168 |
| PS-320RR         | \ \$336 |
| PS-400RR         | \ \$480 |

| Metal read-only region | Storage | Price   |
| ---------------------- | ------- | ------- |
| M-160RR                | 110 GB  | \ \$296 |
| M-160RR                | 460 GB  | \ \$332 |
| M-320RR                | 229 GB  | \ \$588 |
| M-320RR                | 929 GB  | \ \$656 |

### AWS ap-southeast-2 (Sydney)

| Read-only region | Price   |
| ---------------- | ------- |
| PS-10RR          | \ \$19  |
| PS-20RR          | \ \$28  |
| PS-40RR          | \ \$48  |
| PS-80RR          | \ \$86  |
| PS-160RR         | \ \$168 |
| PS-320RR         | \ \$336 |
| PS-400RR         | \ \$480 |

| Metal read-only region | Storage | Price   |
| ---------------------- | ------- | ------- |
| M-160RR                | 110 GB  | \ \$280 |
| M-160RR                | 460 GB  | \ \$332 |
| M-320RR                | 229 GB  | \ \$560 |
| M-320RR                | 929 GB  | \ \$656 |

### AWS eu-central-1 (Frankfurt)

| Read-only region | Price   |
| ---------------- | ------- |
| PS-10RR          | \ \$19  |
| PS-20RR          | \ \$28  |
| PS-40RR          | \ \$48  |
| PS-80RR          | \ \$86  |
| PS-160RR         | \ \$168 |
| PS-320RR         | \ \$336 |
| PS-400RR         | \ \$480 |

| Metal read-only region | Storage | Price   |
| ---------------------- | ------- | ------- |
| M-160RR                | 110 GB  | \ \$292 |
| M-160RR                | 460 GB  | \ \$328 |
| M-320RR                | 229 GB  | \ \$584 |
| M-320RR                | 929 GB  | \ \$652 |

### AWS eu-west-1 (Dublin)

| Read-only region | Price   |
| ---------------- | ------- |
| PS-10RR          | \ \$18  |
| PS-20RR          | \ \$26  |
| PS-40RR          | \ \$44  |
| PS-80RR          | \ \$80  |
| PS-160RR         | \ \$156 |
| PS-320RR         | \ \$313 |
| PS-400RR         | \ \$448 |

| Metal read-only region | Storage | Price   |
| ---------------------- | ------- | ------- |
| M-160RR                | 110 GB  | \ \$272 |
| M-160RR                | 460 GB  | \ \$304 |
| M-320RR                | 229 GB  | \ \$540 |
| M-320RR                | 929 GB  | \ \$604 |

### AWS eu-west-2 (London)

| Read-only region | Price   |
| ---------------- | ------- |
| PS-10RR          | \ \$18  |
| PS-20RR          | \ \$28  |
| PS-40RR          | \ \$46  |
| PS-80RR          | \ \$84  |
| PS-160RR         | \ \$163 |
| PS-320RR         | \ \$327 |
| PS-400RR         | \ \$468 |

| Metal read-only region | Storage | Price   |
| ---------------------- | ------- | ------- |
| M-160RR                | 110 GB  | \ \$288 |
| M-160RR                | 460 GB  | \ \$320 |
| M-320RR                | 229 GB  | \ \$568 |
| M-320RR                | 929 GB  | \ \$632 |

### AWS sa-east-1 (Sao Paulo)

| Read-only region | Price   |
| ---------------- | ------- |
| PS-10RR          | \ \$25  |
| PS-20RR          | \ \$38  |
| PS-40RR          | \ \$63  |
| PS-80RR          | \ \$114 |
| PS-160RR         | \ \$223 |
| PS-320RR         | \ \$447 |
| PS-400RR         | \ \$639 |

### AWS us-east-1 (N. Virginia)

| Read-only region | Price   |
| ---------------- | ------- |
| PS-10RR          | \ \$16  |
| PS-20RR          | \ \$24  |
| PS-40RR          | \ \$40  |
| PS-80RR          | \ \$72  |
| PS-160RR         | \ \$140 |
| PS-320RR         | \ \$280 |
| PS-400RR         | \ \$400 |

| Metal read-only region | Storage  | Price   |
| ---------------------- | -------- | ------- |
| M-160RR                | 110 GB   | \ \$244 |
| M-160RR                | 460 GB   | \ \$276 |
| M-160RR                | 1,241 GB | \ \$404 |
| M-320RR                | 229 GB   | \ \$484 |
| M-320RR                | 929 GB   | \ \$584 |
| M-320RR                | 2,490 GB | \ \$804 |

### AWS us-east-2 (Ohio)

| Read-only region | Price   |
| ---------------- | ------- |
| PS-10RR          | \ \$16  |
| PS-20RR          | \ \$24  |
| PS-40RR          | \ \$40  |
| PS-80RR          | \ \$72  |
| PS-160RR         | \ \$140 |
| PS-320RR         | \ \$280 |
| PS-400RR         | \ \$400 |

| Metal read-only region | Storage  | Price   |
| ---------------------- | -------- | ------- |
| M-160RR                | 110 GB   | \ \$244 |
| M-160RR                | 460 GB   | \ \$276 |
| M-160RR                | 1,241 GB | \ \$404 |
| M-320RR                | 229 GB   | \ \$484 |
| M-320RR                | 929 GB   | \ \$584 |
| M-320RR                | 2,490 GB | \ \$804 |

### AWS us-west-2 (Oregon)

| Read-only region | Price   |
| ---------------- | ------- |
| PS-10RR          | \ \$16  |
| PS-20RR          | \ \$24  |
| PS-40RR          | \ \$40  |
| PS-80RR          | \ \$72  |
| PS-160RR         | \ \$140 |
| PS-320RR         | \ \$280 |
| PS-400RR         | \ \$400 |

| Metal read-only region | Storage  | Price   |
| ---------------------- | -------- | ------- |
| M-160RR                | 110 GB   | \ \$244 |
| M-160RR                | 460 GB   | \ \$276 |
| M-160RR                | 1,241 GB | \ \$404 |
| M-320RR                | 229 GB   | \ \$484 |
| M-320RR                | 929 GB   | \ \$584 |
| M-320RR                | 2,490 GB | \ \$804 |

### GCP asia-northeast3 (Seoul, South Korea)

| Read-only region | Price   |
| ---------------- | ------- |
| PS-10RR          | \ \$19  |
| PS-20RR          | \ \$28  |
| PS-40RR          | \ \$48  |
| PS-80RR          | \ \$86  |
| PS-160RR         | \ \$168 |
| PS-320RR         | \ \$336 |
| PS-400RR         | \ \$480 |

### GCP northamerica-northeast1 (Montréal, Québec)

| Read-only region | Price   |
| ---------------- | ------- |
| PS-10RR          | \ \$16  |
| PS-20RR          | \ \$24  |
| PS-40RR          | \ \$40  |
| PS-80RR          | \ \$72  |
| PS-160RR         | \ \$140 |
| PS-320RR         | \ \$280 |
| PS-400RR         | \ \$400 |

### GCP us-central1 (Council Bluffs, Iowa)

| Read-only region | Price   |
| ---------------- | ------- |
| PS-10RR          | \ \$16  |
| PS-20RR          | \ \$24  |
| PS-40RR          | \ \$40  |
| PS-80RR          | \ \$72  |
| PS-160RR         | \ \$140 |
| PS-320RR         | \ \$280 |
| PS-400RR         | \ \$400 |

### GCP us-east4 (Ashburn, Virginia)

| Read-only region | Price   |
| ---------------- | ------- |
| PS-10RR          | \ \$16  |
| PS-20RR          | \ \$24  |
| PS-40RR          | \ \$40  |
| PS-80RR          | \ \$72  |
| PS-160RR         | \ \$140 |
| PS-320RR         | \ \$280 |
| PS-400RR         | \ \$400 |

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Database replicas
Source: https://planetscale.com/docs/vitess/scaling/replicas



## Overview

A replica is a continuously updated copy of your Vitess database. Replicas serve two main purposes:

* They provide an additional source for your data to be queried.
* They increase database availability by enabling fast failovers for maintenance or unexpected failure.

<Warning>
  Before utilizing replicas for reducing load on the primary, it's important to understand the trade-offs. For more information, see the [Data consistency and replication lag](#data-consistency-and-replication-lag) section.
</Warning>

For information on replicas running in additional regions, see: [Read-only regions](/docs/vitess/scaling/read-only-regions).

## How to query replicas

PlanetScale replicas can be used to read data and reduce load on the primary. PlanetScale does not automatically route queries to replicas unless you explicitly use a replica credential or tell your application to do so.

### 1. Create a global replica credential (recommended)

With global replica credentials, you can have one credential that will automatically route queries to your branch's replicas and read-only regions. If you do not have any read-only regions, it will route to the included replicas on [your plan](/docs/planetscale-plans). If you do have any read-only regions, the queries will route to the closest replica out of your read-only region replicas and included failover replicas.

To use this, create a new credential in the PlanetScale dashboard and select "**Replica**" as the connection type. You can then use the new credential's connection string to setup a replica-only database
connection.

If you have `pscale` installed locally, you can create a replica credential with the following command:

```shell  theme={null}
pscale password create <database> <branch> <password_name> --replica
```

All queries made using this credential will be routed to the nearest replica, even as you add and remove read-only regions.

### 2. `USE @replica` (not recommended)

<Warning>
  We do not recommend using this method to query replicas. If you are currently using this method, we recommend switching to method 1.
</Warning>

If you are not using a global replica credential, then, by default, all queries are served by the primary. However you may route any `SELECT` queries to replicas by issuing the following command on a connection:

```sql  theme={null}
USE @replica
```

Once this command is run, all subsequent `SELECT` queries on that connection will be served by your branch's replicas instead of the primary node in your cluster. Please note that this does not send queries to any read-only regions, if you want to route queries all replicas, including read-only regions, you should use a global replica credential (method 1 above).

<Tip>
  For querying replicas in separate regions, see: [Read-only regions](/docs/vitess/scaling/read-only-regions).
</Tip>

We highly recommend using a global replica credential to ensure that your queries are always routed to the nearest replica.

## High availability

Replicas within PlanetScale are used to enable high availability of your database. This is a part of the reason all production branches in PlanetScale are provided at least one replica. In situations where the underlying hardware or service hosting the primary MySQL node fails, our system will automatically elect a new primary node from the available replicas and reroute traffic to that new primary. This process is know as **reparenting** and typically is all handled within milliseconds or seconds.

Querying the primary during a reparent typically goes unnoticed, other than a bit of additional query latency.
This is because Vitess [buffers queries](https://vitess.io/docs/api/reference/features/vtgate-buffering) during this time.
You can incorporate retry logic into your application to handle the rare instances where an issue arises during reparenting.

When querying a replica during a reparent, you may encounter this error:

```
no healthy tablet available for 'keyspace:"${keyspace}" shard:"${shard}" tablet_type:REPLICA'
```

Incorporating retry logic into your application can help with this scenario as well.

## Multiple availability zones

In cloud architecture, regions are further broken down into data centers known as availability zones (or AZs for short). For example, the `us-east-1` region on AWS contains 6 default AZs available to customers starting with `us-east-1a` through `us-east-1f`. The infrastructure for all Scaler Pro and Enterprise PlanetScale databases are distributed across 3 availability zones. In the instance of an AZ failure, your database will auto failover to an available AZ.

## Data consistency and replication lag

Whenever data is updated (`INSERT`, `UPDATE`, `DELETE`) on the primary node, those changes are synchronized to the replicas shortly after. The delay between when a primary is updated and the changes are applied to the replica is known as `replication lag`. Your databases replication lag is viewable on your [database dashboard](/docs/vitess/architecture#replication-lag-at-a-glance). Replication lag can also be viewed on Datadog if you set up the [PlanetScale - Datadog integration](/docs/vitess/integrations/datadog).

In Datadog, you can use the following formula to set alerts for replication lag:

```
(max:planetscale.replication_lag{ps_database:<DATABASE_NAME> ps_tablet_type:replica, ps_branch:<MAIN>})
```

Make sure you replace `<DATABASE_NAME>` with your PlanetScale database name and `<MAIN>` with the name of the branch for which you'd like to track replication lag.

It is important to be aware of replication lag whenever querying data from your replicas. For example, if you make an update and then immediately try to query for that updated data via a replica, it may not be available yet due to replication lag.

## When should you use replicas in your application?

Replicas are useful for offloading read-heavy workloads from the primary node. By using replicas, you can distribute the read load across multiple nodes, which can help improve the performance of your application. Some examples of where you might want to query a replica are scheduled jobs, analytics jobs, search features, or aggregate queries. Replicas can also be used to provide a read-only view of your data to users or applications that do not need to write data, such as when a user is logged out or writing one-off queries for debugging purposes.

## When are replicas used in PlanetScale?

We use replicas for every production database branch. The number of replicas for a given database depends on the selected plan for that database:

* **Scaler Pro** — Scaler Pro databases include 2 replicas per production branch distributed across multiple AZs in a given region.
* **Enterprise** — The Enterprise plan is customizable to fit the needs of your organization, and as such can have as many replicas as needed.

[Read-only regions](/docs/vitess/scaling/read-only-regions) use the same replica configuration as their respective database, they are just hosted in a different geographical region. It is important to note that the MySQL nodes in read-only regions are replicas intended only for reading data and are not eligible for failover if the primary node experiences an outage.

<Warning>
  Development branches DO NOT have replicas as they are intended for development only and are not designed with the same resiliency as production branches.
</Warning>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# VTGates
Source: https://planetscale.com/docs/vitess/scaling/vtgates



A VTGate, or Vitess Gateway, is the layer of Vitess that acts as a proxy between your application servers and the MySQL instances.

When an application needs to connect to a Vitess cluster, it does not make connections directly to MySQL. Instead, connections are made to our [Global Edge Network](https://planetscale.com/blog/introducing-global-replica-credentials#building-planetscale-global-network), which then routes them to a [VTGate](https://vitess.io/docs/concepts/vtgate/). The VTGate layer acts as the entry point to the cluster, proxies connections, and handles routing of incoming queries to the appropriate [keyspace](/docs/vitess/sharding/keyspaces) and/or [shard](/docs/vitess/sharding).

## Managing your VTGate layer

From the Clusters page:

* Adjust the size of your VTGates, e.g. `VTG-5` to `VTG-10`
* Adjust the number of VTGates your cluster has (only configurable for `VTG-320` and larger). By default, 1 VTGate is deployed per availability zone (for a total of 3 VTGates). You can adjust this up to 24 VTGates per AZ, for a total of 72 VTGates.
* Turn on VTGate autoscaling (for `VTG-320` and larger)
* View your VTGate CPU and memory utilization for the past hour up through the past week
* View any past changes to your VTGates

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/vtgates/vtgates-panel.png?fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=60b334894da7a4b78aeef888c3f8c177" alt="VTGate page in PlanetScale dashboard" className="block dark:hidden" data-og-width="2822" width="2822" data-og-height="1682" height="1682" data-path="docs/images/vtgates/vtgates-panel.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/vtgates/vtgates-panel.png?w=280&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=616e60d6d546d65951fe194cd38dcb3b 280w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/vtgates/vtgates-panel.png?w=560&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=d234d644573f72047e0605af8269ddba 560w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/vtgates/vtgates-panel.png?w=840&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=8f691b3326354b0738a38e91d5b2d2fe 840w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/vtgates/vtgates-panel.png?w=1100&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=89ab9dff4b82e22091055858115b43ab 1100w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/vtgates/vtgates-panel.png?w=1650&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=5ebdb765b16e8a7851969eb0e47cab3d 1650w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/vtgates/vtgates-panel.png?w=2500&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=0132e3c5068ece9a752c5cc7670220c3 2500w" />

  <img src="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/vtgates/vtgates-panel-darkmode.png?fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=69a91ce5efdb1db9e66492fcb8eb57e4" alt="VTGate autoscaling configuration" className="hidden dark:block" data-og-width="2820" width="2820" data-og-height="1686" height="1686" data-path="docs/images/vtgates/vtgates-panel-darkmode.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/vtgates/vtgates-panel-darkmode.png?w=280&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=b94c8548bae6cded2e19b09e39c5091d 280w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/vtgates/vtgates-panel-darkmode.png?w=560&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=528cb644facbf90c5d23486fb6d27a57 560w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/vtgates/vtgates-panel-darkmode.png?w=840&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=52a504e68b80f7558956550612df2643 840w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/vtgates/vtgates-panel-darkmode.png?w=1100&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=12538cc378ab9a10baf6217b83d8fab1 1100w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/vtgates/vtgates-panel-darkmode.png?w=1650&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=34dc57ec2d61f3ec0ac7611acaebb822 1650w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/vtgates/vtgates-panel-darkmode.png?w=2500&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=629fb4ad3093d51653b72bed5e29a164 2500w" />
</Frame>

## Adjusting the size and number of VTGates

To modify the size or number of VTGates, click on your database in the dashboard, "Clusters" in the sidebar, choose the branch you want to modify from the dropdown, and then click on the "VTGates" tab at the top.

From here, you can see the current VTGate utilization, adjust your VTGate size, and modify the number of VTGates per availability zone (for `VTG-320` and larger). Note the price difference on this page after you adjust. This will be added to your monthly bill going forward. When you're satisfied, click "Save changes". It will take some time to update the VTGate configuration. This is an online operation and does not cause downtime.

## VTGate default sizes

By default, every PlanetScale cluster comes with three VTGates, each in a different availability zone.

The default size of your VTGates depends on the size of your cluster's largest keyspace:

| Cluster size | Default VTGate size |
| ------------ | ------------------- |
| `PS-10`      | `VTG-5`             |
| `PS-20`      | `VTG-5`             |
| `PS-40`      | `VTG-5`             |
| `PS-80`      | `VTG-5`             |
| `PS-160`     | `VTG-10`            |
| `PS-320`     | `VTG-20`            |
| `PS-400`     | `VTG-40`            |
| `PS-640`     | `VTG-40`            |
| `PS-700`     | `VTG-80`            |
| `PS-900`     | `VTG-80`            |
| `PS-1280`    | `VTG-80`            |
| `PS-1400+`   | `VTG-320`           |

PlanetScale will automatically adjust your VTGate size whenever you resize your cluster. For example, if you resize your cluster from `PS-10` to `PS-160`, your VTGates will automatically resize from `VTG-5` to `VTG-10`.

If you have made any modifications to the default VTGate configuration, then PlanetScale will not automatically resize your VTGates during a cluster size change.

## VTGate credits and pricing

Every PlanetScale cluster includes VTGate credits that cover the cost of a VTGate configuration that will work well for most workloads. These credits automatically scale with your cluster size and complexity:

* Larger cluster sizes include more VTGate credits
* Additional keyspaces increase your VTGate credits
* Additional shards increase your VTGate credits

If your specific workload requires more powerful VTGates, you can upgrade them manually through the dashboard. Any VTGate usage that exceeds your included credits will be billed as an additional charge.

Here are some examples of how VTGate credits work:

**Example: Single keyspace**

* Cluster: One unsharded `PS-10` (1/8 vCPU, 1GB memory)
* Included credits cover: `VTG-5` VTGates (1/16 vCPU, 128MB memory each)

**Example: Multiple keyspaces**

* Cluster: Two unsharded `PS-10` clusters across two keyspaces
* The additional keyspace doubles your VTGate credit allowance

**Example: Sharded cluster**

* Cluster: Two keyspaces
  * One unsharded `PS-10` cluster
  * One sharded keyspace with two `PS-10` shards
* Credits are tripled due to the additional keyspace and shards

## Autoscaling VTGates

You have the option to automatically scale the number of your VTGates based on VTGate CPU utilization.

To turn on autoscaling, go to the Clusters page, click "VTGates", click the "Autoscaling" tab, and check the "Use horizontal autoscaling" box. You can choose from 40%, 50%, 60%, and 70% utilization.

When the average CPU utilization of your VTGates exceeds the chosen threshold, the system will automatically increase the number of VTGates to accommodate. When CPU utilization falls below this target, it will scale in — downsizing the number of VTGates.

VTGates in each availability zone will scale independently based on their individual utilization. This means that if one zone experiences higher load, it may scale up while others remain at their current number.

| VTGate size | Maximum number of VTGates per availability zone |
| ----------- | ----------------------------------------------- |
| `VTG-320`   | 16                                              |
| `VTG-640`   | 16                                              |
| `VTG-1280`  | 128                                             |

<Note>
  You will be billed for the actual VTGate usage during autoscaling periods. If your VTGates frequently scale up to handle load, this can result in significant additional costs beyond your included VTGate credits.
</Note>

## When to increase the number of VTGates

There are several indicators that your VTGates may need more resources:

### High resource utilization

Monitor your VTGate CPU and memory usage in the PlanetScale dashboard. If you consistently see:

* CPU utilization above 70%
* Memory utilization above 80%

This indicates your VTGates are under heavy load and you should consider either increasing the VTGate size or adding more VTGates to your cluster.

### Connection errors

If you encounter either of these errors, it typically means your VTGates are overwhelmed and unable to handle incoming connections:

```
ERROR HY000 (1105): unavailable: vtgate connection error: no endpoints, after 1 attempts
```

```
Error 1105 (HY000): unavailable: vtgate connection error: no healthy endpoints, after 1 attempts
```

These errors occur when VTGates are resource-constrained and cannot accept new connections. To resolve this:

1. First, check your VTGate CPU and memory metrics
2. Consider upgrading to a larger VTGate size
3. For `VTG-320` and larger, you can also increase the number of VTGates per availability zone
4. Enable autoscaling if your workload has variable load patterns

## VTGate Resources

Each VTGate size has varying levels of compute power and memory provisioned. Because an individual VTGate is run in every availability zone, your total amount of available resources is three times larger than the resources allocated to each instance.

|              | **Processor** | **Memory** |
| ------------ | ------------- | ---------- |
| **VTG-5**    | 1/16 vCPU     | 128 MB RAM |
| **VTG-10**   | 1/8 vCPU      | 256 MB RAM |
| **VTG-20**   | 1/4 vCPU      | 512 MB RAM |
| **VTG-40**   | 1/2 vCPU      | 1 GB RAM   |
| **VTG-80**   | 1 vCPU        | 2 GB RAM   |
| **VTG-320**  | 4 vCPU        | 8 GB RAM   |
| **VTG-640**  | 8 vCPU        | 16 GB RAM  |
| **VTG-1280** | 16 vCPU       | 32 GB RAM  |

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# What are workflows?
Source: https://planetscale.com/docs/vitess/scaling/workflows

PlanetScale workflows provide pre-defined recipes that make it simple to run operations on your databases.

You can currently perform the following workflows:

* [Moving tables from an unsharded to a sharded keyspace](/docs/vitess/sharding/sharding-quickstart)
* Moving tables from an existing unsharded keyspace to another unsharded keyspace

If you are familiar with [Vitess Workflows](https://vitess.io/docs/reference/vreplication/workflow/), you will see some similarities. For example, the PlanetScale workflow that allows you to move tables from an unsharded to a sharded keyspace is similar to the [Vitess `MoveTables` workflow](https://vitess.io/docs/user-guides/migration/move-tables/).

## Create a workflow

To create a new workflow, select your database, and click "Workflows" in the left nav. Next, click "New workflow". Because we currently only have one available workflow, this will drop you straight into the page to create a new workflow to move tables between keyspaces.

## View workflow history

To view the history of all completed or pending workflows, click on "Workflows" in the left nav. From here, you can see all previous workflows along with information such as status, duration, and the time it took to complete.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Non-blocking schema changes
Source: https://planetscale.com/docs/vitess/schema-changes

**Non-blocking schema changes** in PlanetScale provide a schema change workflow that allows users to update database tables without locking or causing downtime for production databases.

## Overview

<Tip>
  To make non-blocking schema changes in PlanetScale, you'll first need a basic understanding of
  [branching](/docs/vitess/schema-changes/branching), the core PlanetScale feature that provides schema changes. Our
  branching page is a great place to start.
</Tip>

PlanetScale makes it safe to deploy schema changes to production databases via *development* and *production branches with [safe migrations](/docs/vitess/schema-changes/safe-migrations) enabled*. Branches with safe migrations enabled can only be updated using deploy requests, and default branches cannot be deleted. Development branches are a separate database with a copy of the source branch's schema. Developers can make schema changes in development branches, test locally, and open a deploy request for deploying their changes to the production database.

Developers can also comment on deploy requests and request reviewers to approve a deploy request before its schema changes can deploy into base branches. Currently, requiring approval is a per-database setting that is turned off by default. With the setting turned off, developers do not need approval to merge a deploy request.

## Adding columns to large tables with PlanetScale is safe!

*Create*, *drop*, and *alter* statements, also known as Data Definition Language (DDL), are used for making schema changes in a database table.

PlanetScale enables developers to make schema changes without the fear of dropping columns, locking tables, causing downtime in their app, etc. PlanetScale also prevents schema changes with conflicts from being migrated and handles schema changes from multiple teammates. A user doesn't have to wait to find out if their changes will be rejected, they learn as they add the change to the queue.

## How do I make non-blocking schema changes with PlanetScale?

In order to make non-blocking schema changes, you **must** enable [safe migrations](/docs/vitess/schema-changes/safe-migrations) on your production branch. Without safe migrations enabled, your schema changes will run directly on your production branch, which can lead to table locking. When safe migrations are enabled on a branch, all schema changes must occur on a database branch. *(A database branch is a separate database with a copy of the production branch's schema.)*

At a high level, this is what happens during the *non-blocking schema change* process in PlanetScale:

<Steps>
  <Step>
    You create a development branch.
  </Step>

  <Step>
    You test your changes on this branch before attempting to apply the changes to the production branch. *(i.e., You made some changes to the database you wish to deploy to the production database.)*
  </Step>

  <Step>
    You open a request to deploy your changes to the base branch, the production branch.
  </Step>

  <Step>
    PlanetScale verifies that your schema changes are safe to be deployed to production. If there are any issues or schema conflicts, you'll be shown the errors.
  </Step>

  <Step>
    You click `Deploy changes`. Your deploy is added to a queue and run immediately or when existing deploys are complete.
  </Step>

  <Step>
    Your deployment makes it to the base branch, and you can now see your schema changes in production.
  </Step>
</Steps>

<Note>
  PlanetScale makes sure not to exhaust your resources; the deployment may be throttled to avoid any impact on
  production queries.
</Note>

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/nonblocking-schema-changes/diagram.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=10322a5a0d4c2bc678ed9225736f2830" alt="PlanetScale non-blocking schema changes diagram" data-og-width="1234" width="1234" data-og-height="720" height="720" data-path="docs/images/assets/docs/concepts/nonblocking-schema-changes/diagram.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/nonblocking-schema-changes/diagram.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=3ab4093ecd4752c7f801729679e5e0f3 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/nonblocking-schema-changes/diagram.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=9d52afcc838cb1a9ac3ab1ee448c563d 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/nonblocking-schema-changes/diagram.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=b6b43b59a13d507f27b512382f995fe6 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/nonblocking-schema-changes/diagram.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=9d330b7c7141e72d1a4cab3864b3bc40 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/nonblocking-schema-changes/diagram.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=b089ee493d3cd689a86c69613382314e 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/nonblocking-schema-changes/diagram.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=0f307fca63cd0c4908208300ab8da37f 2500w" />
</Frame>

## PlanetScale workflow

The PlanetScale command-line tool (CLI) runs an interactive shell equipped with many commands designed to make the database management workflow easier for developers.

A basic non-blocking schema change workflow in PlanetScale might look like this:

<Steps>
  <Step>
    Create a database:

    ```bash  theme={null}
    pscale database create <database>
    ```
  </Step>

  <Step>
    Create a development branch:

    ```bash  theme={null}
    pscale branch create <database> <branch>
    ```
  </Step>

  <Step>
    Make a schema change on this branch:

    ```bash  theme={null}
    pscale shell <database> <branch>
    ```

    <Tip>
      A schema change is any change you make to the tables in your database environment created within the PlanetScale branch. (i.e., create, drop, and alter statements)
    </Tip>

    <Warning>
      You can only apply direct schema changes to branches without safe migrations enabled.
    </Warning>

    Here is a sample CREATE table schema change you could try using:

    ```sql  theme={null}
    CREATE TABLE `reminders` (
      `id` bigint NOT NULL AUTO_INCREMENT,
      `body` varchar(1024) NOT NULL,
      `created_at` datetime(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6),
      `updated_at` datetime(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6),
      `is_done` tinyint(1) NOT NULL DEFAULT '0',
      PRIMARY KEY (`id`)
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
    ```
  </Step>

  <Step>
    Test changes on branch locally.
  </Step>

  <Step>
    Create a deployment request by running:

    ```bash  theme={null}
    pscale deploy-request create <database> <branch>
    ```
  </Step>

  <Step>
    Fix any schema conflicts.

    PlanetScale displays the difference between what is currently in the base branch and your development branch. Go back to *Step 3* of the workflow and test out new schema changes to fix the schema conflict. If you did not encounter any schema conflicts, you're ready for *Step 7*.
  </Step>

  <Step>
    Deploy the deploy request.

    * To *deploy* the **deploy request** created in *Step 5*, run the following command:

      ```bash  theme={null}
      pscale deploy-request deploy <database> <deploy-request-number>
      ```

    * To find your `deploy-request-number`, simply run:

      ```bash  theme={null}
      pscale deploy-request list <database>
      ```

    Copy the value from `NUMBER` and use that digit as your `deploy-request-number`.
  </Step>
</Steps>

## Limitations

If you want to make schema changes containing foreign key constraints, enable foreign key constraint support for your database in the database settings page.

PlanetScale doesn't support direct `RENAME` for columns and tables. Learn why and how to rename tables or columns in [this tutorial](/docs/vitess/schema-changes/handling-table-and-column-renames).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Aggressive deploy request cutover
Source: https://planetscale.com/docs/vitess/schema-changes/aggressive-cutover



## Overview

Cutover is the critical final step in an online schema migration where Vitess atomically replaces the original table with a newly created "shadow" table that contains the updated schema. This process involves acquiring metadata locks, preventing writes to the original table, ensuring complete data synchronization, and renaming tables to complete the migration.

The cutover process can fail or time out when the table is locked by long-running queries or transactions, preventing Vitess from acquiring the necessary metadata locks. When this happens, Vitess will retry the cutover operation until it succeeds.

Aggressive cutover is a setting that forces the cutover to complete immediately by killing any queries or transactions that are blocking the operation. When enabled, the system will prioritize schema migration completion over preserving running queries.

## When to enable aggressive cutover

You should consider enabling aggressive cutover in these scenarios:

1. **Migration delayed due to long-running transactions**: If you receive the "Migration delayed due to long-running transactions" notice on your deploy requests, this indicates that the cutover cannot complete because there are long-running transactions on the table. Enabling aggressive cutover will force the cutover to happen by killing those blocking queries.

2. **Application has slow queries or long-running transactions**: If your application consistently runs slow queries or long-running transactions that prevent migrations from completing, aggressive cutover may be necessary. This setting is disabled by default because well-optimized applications should not require it.

## How it works

Aggressive deploy request cutover can be enabled for a database by admins only. To enable the setting, visit the database settings page and look under "Advanced settings".

When aggressive cutover is enabled, the system immediately begins killing queries and transactions that are using or locking the migrated table on the very first cutover attempt. This aggressive approach ensures the migration completes without waiting for blocking operations to finish naturally.

**Normal cutover behavior:**

* Vitess attempts to acquire locks on the table
* If blocked by ongoing queries/transactions, it waits and retries
* This process continues until the cutover succeeds or 1 hour elapses
* After 1 hour the cutover will be forced

**Aggressive cutover behavior:**

* Vitess immediately kills any queries or transactions blocking the cutover
* The cutover proceeds without waiting for blocking operations to complete

**Important considerations:**

* Having retry logic or a strategy to handle re-running the killed queries is advised
* Once enabled, this setting applies to all future deploy requests

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Branching
Source: https://planetscale.com/docs/vitess/schema-changes/branching

PlanetScale allows you to branch database schemas the same way you branch your code.

export const YouTubeEmbed = ({id, title}) => {
  return <Frame>
      <iframe src={`https://www.youtube-nocookie.com/embed/${id}?rel=0`} title={title} className="aspect-video w-full" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" />
    </Frame>;
};

## What are branches on PlanetScale

Database branches on PlanetScale are isolated database instances that give you flexibility when developing your application. When your database is first initialized, a single production branch is created called `main` and acts as the default branch. When you create additional branches, the schema of the source branch is copied to the new branch, giving you an isolated MySQL instance to develop with. Changes made in one branch, whether to the schema or the data, do not affect any other branches for a given database.

<YouTubeEmbed id="EbG6-GUUIfc" title="Data drop: Branching and deploy requests" />

## Development and production branches

PlanetScale provides two types of database branches:

* **Development branches** — Development branches provide isolated copies of your production database schema where you can make changes, experiment, or run CI. Please note that only the schema is copied. A new development branch will not have any data stored in it unless you [restore from a backup](/docs/vitess/backups#restore-from-a-backup). To automatically create a development branch with data from another branch, see the [Data Branching® feature](/docs/vitess/schema-changes/data-branching).
* **Production branches** — Production branches are highly available databases intended for production traffic. They are automatically provided with an additional [replica](/docs/vitess/scaling/replicas) to resist outages, enabling zero-downtime failovers. Scaler Pro plans include two additional replicas, and Enterprise plans are customizable.

Both types of branches also offer [safe migrations](/docs/vitess/schema-changes/#safe-migrations) as an optional feature, which helps protect the branch from accidental schema changes and enables non-blocking schema migrations. We highly recommend that all production database branches have the safe migrations setting turned on.

If you want to set up a workflow with a ”staging” branch, see more the [staging branch documentation](/docs/vitess/schema-changes/#staging-branch) below.

## Promote a branch to production

PlanetScale provides the ability to **promote any development branch with a valid schema to production**. Promoting a branch to production will automatically set up the database replica(s) to make your database highly available. Note: Only one production branch is included in Scaler Pro plans, but you can add more production branches for an additional fee.

### Promote a development branch to production

Every new PlanetScale database is created with a production branch named `main`. This branch is intended as the starting point for building your database on PlanetScale and has increased performance and resilience by default when compared to development branches.

That said, you don't have to use the default `main` branch as your production branch. **Any development branch can be promoted to production**.

Once you are satisfied with the changes you've made to a given development branch, you can promote it to production. Going forward, you can continue to make new development branches off of this production branch to experiment with changes as needed.

A branch can be promoted from the branch overview page in the PlanetScale app or by using the [PlanetScale CLI](/docs/cli/branch), as shown below:

```
pscale branch promote <DATABASE_NAME> <BRANCH_NAME>
```

It's possible to run multiple production branches simultaneously, within [your plan's limits](/docs/planetscale-plans). Keep in mind, PlanetScale does not provide data syncing between production branches.

### Demote a production branch to development

Sometimes, you might need to demote a production branch to a development branch. This can be done from the production branch's overview page in the PlanetScale app (located in the right column of the page).

Be aware when demoting a production branch to a development branch:

* Databases must have at least one production branch at all times
* Development branches are not meant to be used with production workloads
* The branch will no longer have high-availability features
* Existing scheduled production branch backup policies will no longer run
* Any read-only regions will need to be removed

If you are on an Enterprise plan, only an administrator for your organization can request to demote a branch, and the demotion request will need to be approved by another administrator. Once the first administrator requests to demote a production branch, the second administrator can approve the demotion on the production branch's overview page. You will see the request from the first administrator and a **Demote to development branch** button to complete the demotion.

## Create a development branch

If you need to experiment with schema changes, you can create a new development branch. This will copy the schema from the base branch into a new branch where you can create and test your changes.

Development branches **will not** copy over data, just the schema. To create a development branch with data from another branch, see the [Data Branching® feature](/docs/vitess/schema-changes/data-branching) section.

**How to create a development branch**:

<Steps>
  <Step>
    On the database dashboard page, click the "**New branch**" button.
  </Step>

  <Step>
    Give your development branch a name and select the region closest to your or your application.
  </Step>

  <Step>
    Select the base branch you want to branch off of.
  </Step>

  <Step>
    Click "**Create branch**".
  </Step>

  <Step>
    (*Optional*) You can also create a new branch from the [PlanetScale CLI](/docs/cli/branch) with:

    ```
    pscale branch create <DATABASE_NAME> <BRANCH_NAME>
    ```
  </Step>
</Steps>

## Safe migrations

[Safe migrations](/docs/vitess/schema-changes/safe-migrations) is an optional, but recommended, feature that can be enabled on branches. Branches with safe migrations enabled are restricted from accepting DDL directly. This prevents accidental changes to the database schema, and also enables non-blocking schema migrations. To make changes to branches with safe migrations enabled, you must create a new branch, then merge changes using a [deploy request](/docs/vitess/schema-changes/deploy-requests). Using this method, you get to see a schema diff before merging changes, have the option to have your team review changes, and receive [additional checks and warnings](https://planetscale.com/blog/deploy-requests-now-alert-on-potential-unwanted-changes) prior to making a production schema change.

To enable safe migrations on a branch, select the branch you want to modify from the branch dropdown and click the **”cog”** in the upper right of the infrastructure card on the ”**Dashboard**” tab of the database. In the modal, toggle the option labeled **”Enable safe migrations”**, then click the **”Enable safe migrations”** button to save and close the modal.

### Staging branches

You can use a development branch with safe migrations enabled to set up a workflow with a “staging” branch. First, make sure you have safe migration enabled for your main production branch. Then, create a “staging” branch with your main production branch as the base and turn on safe migrations. All new branches created for development can use this “staging” branch as the base branch.

You can then open a deploy request against either the main production or “staging” branch. Once it is deployed to “staging,” you can open a deploy request against the main production branch. The main production branch must be set as the [default branch](/docs/vitess/schema-changes/branching#default-branches) to open a deploy request against it.

<Note>
  In this setup, the “staging” branch is still a development branch. Compared to your main production branch (additional production branches are an additional cost), it will have reduced resources, similar to other development branches.
</Note>

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/branching/branches-with-staging-branch.jpg?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=9ec1fc4e9ab40de84c941414976db50a" alt="View of the Branches tab with main < staging < dev branches" data-og-width="2666" width="2666" data-og-height="1068" height="1068" data-path="docs/images/assets/docs/concepts/branching/branches-with-staging-branch.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/branching/branches-with-staging-branch.jpg?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=2dd1f577fcd55a85aff519d12a349485 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/branching/branches-with-staging-branch.jpg?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=ef62a3c4bd0d03e73c02ba282c37105e 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/branching/branches-with-staging-branch.jpg?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=ffc2be6fe1047230ac7214a9187def24 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/branching/branches-with-staging-branch.jpg?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=d5157a411664b3cff0efc8afea61cc03 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/branching/branches-with-staging-branch.jpg?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=c617d619bd6f53fcb530985e1107bfdf 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/branching/branches-with-staging-branch.jpg?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=3e7f4a099d1dfe5ac1be9a70f0d8b42c 2500w" />
</Frame>

## How to make schema changes on a branch with safe migrations enabled

Since DDL is restricted on branches with safe migrations enabled to prevent accidental changes and enable zero-downtime migrations, you'll need to perform the following steps in order to make changes to a safe migrations enabled branch:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/branching/diagram.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=a6f802a412b4f9d7d8ecb5c8bf32ea5a" alt="PlanetScale Branching Flow Diagram" data-og-width="1234" width="1234" data-og-height="652" height="652" data-path="docs/images/assets/docs/concepts/branching/diagram.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/branching/diagram.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=28fe35a991dba28f37b632f1ee740f16 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/branching/diagram.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=abaa94da4cb1ceac4ca4b9d62e11e661 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/branching/diagram.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=ce55c5493f3a311b19076eca66e98d63 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/branching/diagram.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=cfbdbb583c576543866745c49d5fb11e 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/branching/diagram.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=8e9fb4d015052e1da44606e280e799be 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/branching/diagram.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=28a6de006d593f56540cfcc679e47765 2500w" />
</Frame>

<Tip>
  You'll see a `ERROR 1105 (HY000): direct DDL is disabled` message if you attempt to make schema changes in a branch with safe migrations enabled. Instead, create a development branch, and make your changes there.
</Tip>

### 1. Create a development branch

The first step is to create a new development branch off of the branch you want to make changes to. This will make a copy of the source branches schema into the newly created development branch where you can perform the necessary changes to the schema.

### 2. Create a deploy request

If you are working in a team, the [deploy request](/docs/vitess/schema-changes/deploy-requests) creates an opportunity for your teammates to review the changes you have made before they are deployed to production.

<Steps>
  <Step>
    To create the deploy request, go to the branch overview page for the branch you want to deploy.
  </Step>

  <Step>
    Select the base branch you want to deploy to from the "**Deploy to**" dropdown.
  </Step>

  <Step>
    (*Optional*) Add a comment describing your deploy request. . Click "**Create deploy request**".

    ![PlanetScale deploy request example](https://planetscale.com/assets/docs/concepts/branching/deploy-request-page.png)
  </Step>

  <Step>
    (*Optional*) You can also create a deploy request from the [PlanetScale CLI](/docs/cli/deploy-request) with the following command:

    ```
    pscale deploy-request create <DATABASE_NAME> <BRANCH_NAME>
    ```
  </Step>
</Steps>

### 3. Review the deploy request

The deploy request includes a schema diff so that you can review the schema changes introduced by the deploy request against the base branch.

<Steps>
  <Step>
    On the deploy request page, click the "**Schema changes**" tab.
  </Step>

  <Step>
    Schema additions are highlighted in green, and deletions are highlighted in red.
  </Step>

  <Step>
    (*Optional*) You can also run the command below to see the [schema diff in the PlanetScale CLI](/docs/cli/deploy-request):

    ```
    pscale deploy-request diff <DATABASE> <DEPLOY_REQUEST_NUMBER>
    ```
  </Step>
</Steps>

Another benefit of the deploy request review process is that **PlanetScale will determine if certain requests aren't deployable**. For example, if you try to deploy a branch with no unique key, PlanetScale will block the deployment, as the unique key is required.

### 4. Add changes to the deploy queue

Once a deploy request has been created and, optionally, approved, you need to add the changes to the deploy queue.

Schema changes are deployed to a database in the order in which they are received. PlanetScale analyzes the schema changes in a deploy request when they are added to the deploy queue to ensure that the changes do not conflict with any of the queued schema changes.

PlanetScale also provides insight on the deploy queue, listing all of the schema changes in the queue with their completion status.

<Steps>
  <Step>
    Organizations have the option to require approval before a deploy request can be added to the queue. If this is enabled, first make sure the request is approved. You can control this option in the database's "**Settings**" tab.
  </Step>

  <Step>
    To add a deploy request to the deploy queue, click “**Add changes to the deploy queue**” on the deploy request page.
  </Step>

  <Step>
    (*Optional*) You can also run the following command with the [PlanetScale CLI](/docs/cli/deploy-request):

    ```
    pscale deploy-request deploy <DATABASE_NAME> <DEPLOY_REQUEST_NUMBER>
    ```
  </Step>

  <Step>
    If successful, you'll get the message "These changes have been deployed".
  </Step>

  <Step>
    Your deploy request has now been merged into production. You can click on the "Deploy requests" tab of the database to see the list of previous deploys.
  </Step>
</Steps>

## Default branches

The `main` branch is automatically set as the default branch when the database is initialized. However, you can change the default branch if needed.

**How to change the default branch**:

There are two ways to change the default branch.

<Steps>
  <Step>
    Go to the "Branches" page in your dashboard.
  </Step>

  <Step>
    Click on the three dots "..." next to the branch whose name you'd like to change.
  </Step>

  <Step>
    Click "Set as default branch".
  </Step>
</Steps>

If the current default branch has child branches, you also have the option to move those under the new default branch from here by checking the "Replace the current default branch" checkbox.

Alternatively, you can do this from your Settings page.

<Steps>
  <Step>
    Go to your database dashboard page and click the "**Settings**" tab.
  </Step>

  <Step>
    Under "**General**" in the sidebar, you'll find the "**Default branch**" dropdown.
  </Step>

  <Step>
    Select the branch you want to be the default branch. It does not have to be a production branch.
  </Step>

  <Step>
    Scroll down and click "**Save database settings**".
  </Step>
</Steps>

<Note>
  If you change the default branch, you will also need to update your credentials in your application, or wherever else you use credentials, to reference the new default branch (if desired).
</Note>

## Renaming a branch

You can change the name of a branch from the Branches tab in your PlanetScale dashboard. Click on the three dots ("...") next to the branch name that you would like to change, type in the new name, and click "Save changes".

Renaming a branch does not affect that branch's credentials. You do not need to regenerate credentials if you rename a branch.

## Delete a branch

You can delete a branch from the Branches overview page or by running the following command in the [PlanetScale CLI](/docs/cli/branch):

```
pscale branch delete <DATABASE_NAME> <BRANCH_NAME>
```

We recommend deleting branches after a deploy request is complete or if you are no longer using the branch for testing. Scaler Pro development branches are billed only for the time that they are used to the nearest second, so it beneficial to delete them when they are no longer in use. See the [billing documentation](/docs/planetscale-plans#development-branches) for more info.

<Note>
  Only [Organization Administrators](/docs/security/access-control#organization-administrator) have permission to delete production branches.
</Note>

You cannot delete a branch that's [set as default](/docs/vitess/schema-changes/#default-branches). To delete, it set another branch as the default first.

## Resolve a schema conflict

Schema conflicts occur when your branch has conflicting changes with the base branch.

To resolve a schema conflict, create a new branch from the base branch, which will have the most up-to-date schema, and apply the necessary schema changes to the new branch before repeating the deploy process.

## Automatically copy migration data

Many frameworks and migration tools keep track of data schema changes in a migration table. When turned on, PlanetScale will automatically copy migration table metadata from your development branches to the production branch as part of our deployment process.

**Turn on automatic copying of migration data**:

<Steps>
  <Step>
    On your database dashboard page, click the "**Settings**" tab.
  </Step>

  <Step>
    Check the "**Automatically copy migration data**" box.
  </Step>

  <Step>
    Select one of the listed frameworks: Rails, Phoenix, Laravel, Django, .NET, Sequelize, or Other, which allows you to specify a custom table name.
  </Step>
</Steps>

You can see how this works in this [Rails migration tutorial](/docs/vitess/tutorials/automatic-rails-migrations).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Data branching®
Source: https://planetscale.com/docs/vitess/schema-changes/data-branching



export const VimeoEmbed = ({id, title}) => {
  return <Frame>
      <iframe src={`https://player.vimeo.com/video/${id}?dnt=true`} title={title} className="aspect-video w-full" allow="autoplay; fullscreen; picture-in-picture" />
    </Frame>;
};

## Overview

The PlanetScale Data Branching® feature allows you to create isolated copies of your database that include both the schema and data. This differs from our [regular branching feature](/docs/vitess/schema-changes/branching), which only includes the schema.

## Enable the Data Branching® feature for your database.

Before you can use the feature, you have to enable it in your database settings page.

<Steps>
  <Step>Navigate to the database that you'd like to enable.</Step>
  <Step>Click on the "**Settings**" link in the header navigation bar.</Step>
  <Step>Scroll to the option with the text "**Enable Data Branching®**"</Step>
  <Step>Enable this option and click "**Save database settings**".</Step>
</Steps>

## Create a new seeded branch

<Steps>
  <Step>After enabling the Data Branching feature, navigate to the dashboard page of the database.</Step>

  <Step>
    Clicking on "**New Branch**" should now offer an option to select "**Seed Data**". - **None** — Creates a database
    branch with only the **schema** copied to the new branch. - **From most recent backup** — Creates a database branch
    with both the **schema and data** from the latest backup of the Base branch.

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/data-branching/branch.jpg?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=bc2b83230c10dcd3abe260686370a5b1" alt="PlanetScale dashboard new branch dialog with seed option" data-og-width="991" width="991" data-og-height="955" height="955" data-path="docs/images/assets/docs/concepts/data-branching/branch.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/data-branching/branch.jpg?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=e9997bb3c6264f20988b93520922d85b 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/data-branching/branch.jpg?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=de94e1aaed988368df028e8fb8cd8cc7 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/data-branching/branch.jpg?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=b1cc602424ce92731e90f04fa420beeb 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/data-branching/branch.jpg?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=312068484874c1290b7956c0e8cd9999 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/data-branching/branch.jpg?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=8da17a5c22e21b8a0f4a75d3bfc96f1f 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/data-branching/branch.jpg?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=069b39673fecb57597d86aec0f021f74 2500w" />
    </Frame>
  </Step>
</Steps>

<Warning>
  Branching off of a production branch and seeding it with data can incur additional charges, as branches seeded from production are treated as production branches instead of development branches. See [question 5](/docs/vitess/schema-changes/#faq) in the FAQ below for more information.
</Warning>

3. Once you've selected an option, click "**Create branch**" to create a new branch.

   <VimeoEmbed id="830572488" title="Data Branching" />

### FAQ

<AccordionGroup>
  <Accordion title="Can I pick which backup is used for the new branch?">
    PlanetScale picks the latest backup so that your new branch has the latest dataset to work with.
  </Accordion>

  <Accordion title="Can I pick which tables are copied into the new branch?">
    PlanetScale seeds the new branch with **all the schema & data** from the base branch. We do not offer a way to filter out data or schema when creating a new branch.
  </Accordion>

  <Accordion title="Is data in the new branch kept up to date with changes to the base branch?">
    PlanetScale does not provide data syncing between a production branch and a development branch.
  </Accordion>

  <Accordion title="When I merge my deploy request, will any data changes be merged back to the base branch?">
    PlanetScale does not provide data syncing between a production branch and a development branch.
  </Accordion>

  <Accordion title="Will enabling the Data Branching® feature affect my billing?">
    Yes, it can. If you are branching and seeding from a *development* branch, the new branch will count towards your [development branch usage hours](/docs/planetscale-plans#development-branches) in the same way that regular branching does, but because you're seeding it with data, it will also be billed for storage.

    However, if you branch and seed from a *production* branch, the new branch will be created as a production branch at the same resource size as the source production branch, and will therefore be billed at that size. It will also be seeded with the production data and include the 2 default production replicas, so you will be charged for the storage as well for however long the branch stays running.

    For example, if you are branching off a `PS-80` production branch with 100 GB of storage, and you seed the new branch with that source production branch's data, you will be billed for the new seeded branch as follows:

    * `PS-80` — \$179
    * 100 GB — ($0.50 \* 100 GB) \* 3 replicas = $135

    We spin the new branch up using the same resource size as the production branch to ensure that large datasets initialize correctly and efficiently. After initialization, you may downgrade the branch size on the [Clusters page](/docs/vitess/cluster-configuration#adjust-your-cluster-size). All billing is prorated, so you will only be charged for the time your seeded branch is live. To avoid incurring extra charges, be sure to downsize your seeded branch, and delete it once you're done using it
  </Accordion>
</AccordionGroup>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Deploy requests
Source: https://planetscale.com/docs/vitess/schema-changes/deploy-requests

Deploy requests are an integral part of the [PlanetScale workflow](/docs/vitess/best-practices).

export const VimeoEmbed = ({id, title}) => {
  return <Frame>
      <iframe src={`https://player.vimeo.com/video/${id}?dnt=true`} title={title} className="aspect-video w-full" allow="autoplay; fullscreen; picture-in-picture" />
    </Frame>;
};

## Overview

Database branching, coupled with deploy requests, allows you to **deploy non-blocking schema changes to your production database with zero downtime**. You can also [undo deployments](#revert-a-schema-change) without losing any data that was written during that time.

<VimeoEmbed id="830571933" title="Deploy requests overview" />

## Create a deploy request

<Note>
  Before you can create a deploy request, the branch you are merging into must have [safe migrations](/docs/vitess/schema-changes/safe-migrations) enabled.
</Note>

<Steps>
  <Step>
    Click on "**Branches**".
  </Step>

  <Step>
    Select the development branch you want to deploy into the base branch.
  </Step>

  <Step>
    This page shows you a diff of the schema against its base branch.
  </Step>

  <Step>
    To the right of the page, you'll see a dropdown that says "**Deploy to**". If you don't see this option, you likely don't have [safe migrations](/docs/vitess/schema-changes/safe-migrations) enabled on the branch you're deploying into. Go to the branch page for the branch you'd like to merge into, and enable safe migrations.
  </Step>

  <Step>
    Select the branch you want to deploy to. Again, if the branch doesn't show in the dropdown, you need to enable [safe migrations](/docs/vitess/schema-changes/safe-migrations).
  </Step>

  <Step>
    Optionally, add a comment about the deploy request.
  </Step>

  <Step>
    Click "**Create deploy request**".
  </Step>
</Steps>

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/deploy-requests/deploy-request-page-2.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=a07195e9f08d13e1e7caff6de117dce4" alt="Example of deploy request on branch page" data-og-width="2970" width="2970" data-og-height="1832" height="1832" data-path="docs/images/assets/docs/concepts/deploy-requests/deploy-request-page-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/deploy-requests/deploy-request-page-2.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=a792ed08dbafe1c25bcf87741479ca14 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/deploy-requests/deploy-request-page-2.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=36696bf252d3435013663eb3b2958f5e 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/deploy-requests/deploy-request-page-2.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=5b6321ea739b3ecba6cd3bacd77ea216 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/deploy-requests/deploy-request-page-2.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=5ae481a3ef39f483b93307e35311f458 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/deploy-requests/deploy-request-page-2.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=8a693bf30c15dd27e2a73f3c57864ab9 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/deploy-requests/deploy-request-page-2.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=34bfffaebd1a94b8c0900a6de42952eb 2500w" />
</Frame>

## Review a deploy request

Once you create a deploy request, you or your team can review it and, optionally, approve it before deploying it.

PlanetScale will check if the request is deployable. This process includes checking for issues like:

* [Incompatible unique keys](/docs/vitess/schema-changes/onlineddl-change-unique-keys)
* Invalid charsets (PlanetScale supports `utf8`, `utf8mb4`, `utf8mb3`, `latin1`, and `ascii`)
* Invalid foreign key constraint names or lengths
* And other various checks to ensure successful schema changes

We will also warn you about potential data loss or inconsistencies and check if there are any known conflicts with the production schema that could prevent a clean merge. While we attempt to find all possible conflicts, it is ultimately up to you to confirm merge details.

<Steps>
  <Step>
    Click the "**Deploy requests**" tab on the database dashboard page.
  </Step>

  <Step>
    Select the open deploy request you want to review.
  </Step>

  <Step>
    Under "**Summary**", you'll see if the request is deployable.
  </Step>

  <Step>
    To review the schema changes, click the "**Schema changes**" tab.
  </Step>

  <Step>
    You'll see the proposed changes here. New additions are highlighted in green, and deletions are highlighted in red.
  </Step>

  <Step>
    If you have required deploy requests to be approved before deployment, other users in your Organization will see the option to "**Approve changes**" or "**Leave a comment**" on the "**Schema changes**" tab.
  </Step>
</Steps>

<Note>
  If you are the only administrator in your Organization and you enable the "Require administrator approval for deploy requests" setting, you can self-approve your own deploy requests. If there is more than one administrator, self-approval is not allowed.
</Note>

### Reviewing changes across shards

If your deploy request contains changes to a sharded keyspace, you can see the affected shards by clicking the arrow next to each changed table. This will show the SQL that will run, and in the next tab, each shard that will be affected.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/deploy-requests/sharded-deploy-request.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=ec1241dade76dbf362911a030687646a" alt="PlanetScale deploy request - changes on sharded keyspace" data-og-width="2960" width="2960" data-og-height="1820" height="1820" data-path="docs/images/assets/docs/concepts/deploy-requests/sharded-deploy-request.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/deploy-requests/sharded-deploy-request.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=cda95a228ca93dffa0aa2b912f2a737c 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/deploy-requests/sharded-deploy-request.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=c60cdf1adb51c08e339cb742fd889421 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/deploy-requests/sharded-deploy-request.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=a9bb7a05d2afab11a50116a64b903d53 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/deploy-requests/sharded-deploy-request.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=cc289d3309d063b13a1d626bd346393e 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/deploy-requests/sharded-deploy-request.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=a22ec6182117af12cbd74c3a91a782aa 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/deploy-requests/sharded-deploy-request.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=270a275cc4c4316846be6c467f3d1573 2500w" />
</Frame>

## Deploy a deploy request

<Steps>
  <Step>
    Once the request is approved, if required, it's ready to be added to the deploy queue. Click on the "Summary" tab, and you'll see the option to deploy.
  </Step>

  <Step>
    Here you'll have the option to choose to "Deploy changes" or to "Deploy changes instantly":

    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/deploy-requests/deploy.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=229df8fbf024c34758499843b2b89ae6" alt="PlanetScale deploy request - deploy options" data-og-width="2520" width="2520" data-og-height="1688" height="1688" data-path="docs/images/assets/docs/concepts/deploy-requests/deploy.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/deploy-requests/deploy.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=1a730eabeaedb36dcf7c3d041a5b93a5 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/deploy-requests/deploy.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=9be554f0e46d7def3d4486a48b290e38 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/deploy-requests/deploy.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=edcb0aa31be203f347d761575afb7468 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/deploy-requests/deploy.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=3653a52aba3a7c25162fdd26d63803ff 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/deploy-requests/deploy.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=102e5e9e22ea4ac2b92de0c764bf1485 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/deploy-requests/deploy.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=450baefda533eb99b0b15d66be4cd4bc 2500w" />
    </Frame>
  </Step>
</Steps>

### Deploy changes

<Steps>
  <Step>
    (Optional) You have the option to enable [**gated deployments**](#gated-deployments), which gives you the power to control exactly when the migration cuts over. You'll see an "**Auto-apply changes**" checkbox, which is checked by default. If you uncheck this, you will get the option to apply the changes once the schema changes are complete. If you leave it checked, it will auto-deploy as soon as it's ready.
  </Step>

  <Step>
    (Optional) You also have the option to run the migration using MySQL's **ALGORITHM=INSTANT**. If you would like to deploy instantly, click the arrow next to "Deploy changes" and select "Deploy changes instantly". Gated deployments are not available for this option. See the [Deploy changes instantly](#deploy-changes-instantly) section below for more information.
  </Step>

  <Step>
    When you're ready to deploy, click "**Deploy changes**". The deployment will begin or be queued if there are other pending deployments. You will still have a short window after deploying to enable [gated deployments](#gated-deployments) in this step.
  </Step>

  <Step>
    If you enabled gated deployments (step 2), you can click "**Apply changes**" to merge the deployment to production once it completes.
  </Step>

  <Step>
    After you deploy, you have **30 minutes to "undo"** it using our [schema revert feature](#revert-a-schema-change).
  </Step>

  <Step>
    If you are deploying changes to a sharded keyspace, you'll see the progress for each shard here as well.
  </Step>
</Steps>

### Deploy changes instantly

You also have the option to use MySQL's  **ALGORITHM=INSTANT** to instantly deploy the schema change. Learn more in the [**Instant Deployments** section](#instant-deployments).

1. When you're ready to deploy, click "**Deploy changes instantly**". The deployment will begin or be queued if there are other pending deployments.
   * Though the deployment many be queued, once it's at the front of the queue, it will be deployed instantly.
   * Instant deployments **cannot be reverted**.

If you would like to require an administrator's approval before a request can be deployed, go to the "**Settings**" page for your database and check the "**Require administrator approval for deploy requests**" box. You must be an Organization Administrator to enable this restriction. Please note you will not be able to approve your own deploy requests.

## Close a deploy request

If you decide you don't want to proceed with a deploy request, you can easily close it.

<Steps>
  <Step>
    Click the "**Deploy requests**" tab on the database dashboard page.
  </Step>

  <Step>
    Select the request you want to close.
  </Step>

  <Step>
    Click on the "**Close deploy request**" button on the right-hand side.
  </Step>
</Steps>

## Deploy requests and foreign key constraints

In most cases, deploy requests should work as expected when your schema changes have [foreign key constraints](/docs/vitess/foreign-key-constraints).

There are some cases where a deploy request will not be deployable.
This includes cases where there is a mismatched column type or when a foreign key constraint references a deleted column.

For example, if we open a deploy request to add a foreign key constraint `t1_id` with type `BIGINT` on a table `t2` that references a column `id` on table `t1`, where `t1.id`'s type is `BIGINT`, the following cases would produce a linting error in the deploy request because it is not deployable:

* if, while the previously mentioned deploy request is open, someone else updates `t1.id` to a different column type, i.e., `int`.
* if, while the previously mentioned deploy request is open, someone else deletes `t1.id`.
* if, while the previously mentioned deploy request is open, someone else deletes all indexes that cover `t1.id` as their prefix. (Because in a foreign key relationship, the referenced columns on the parent table must be indexed, usually by a dedicated index, but they can be the first columns in an otherwise wider index.)

These are all cases where another user changes schema, causing the initial user's definition to be invalid MySQL.

There are also two cases where a revert would cause orphaned rows that you can read about in this document's [revert section](#when-a-revert-can-result-in-orphaned-rows).

### Validating referential integrity of existing columns

Deploy requests do not validate the referential integrity of *existing* columns. `ALTER TABLE… ADD FOREIGN KEY…` does not validate existing row relations within the context of a deploy request. Unlike standard MySQL, it is possible to add the foreign key constraint to a table with orphaned rows, and they will remain orphaned. In standard MySQL, adding a foreign key is a blocking operation, and it fails if any orphaned rows are found.

## Instant deployments

Instant deployments give you the option to run schema changes using MySQL's **ALGORITHM=INSTANT**. This is different than how our [**online schema migrations**](/docs/vitess/schema-changes/how-online-schema-change-tools-work) work.

Instant deployments will apply schema changes faster, however, these schema changes must be **auto-applied** and **cannot be reverted**.

### Who should use instant deployments?

We recommend instant deployments to experienced users that are making schema changes to large tables, or users that would like their schema changes to be deployed instantly.

### Supported operations

In order for a deploy request to be instantly deployed, *all* schema changes in the deploy request must be instantly deployable. Some of those changes include:

* Adding or dropping a column (with some exceptions)
* Changing or dropping a column's default value
* Changing an `ENUM` or `SET` definition

The following changes are examples of changes that are **not** instantly deployable:

* Changing a column's data type
* Adding a column with a non-literal default value
* Adding or dropping an index
* Adding or dropping a foreign key constraint
* Extending a `VARCHAR` column size
* Updating a column to `NULL` or `NOT NULL`

To know whether or not a deploy request is instantly deployable, look for the "Instantly deployable" badge on your deploy request. This badge will only be visible on deploy requests that can be deployed instantly.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/deploy-requests/deploy-instantly.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=6eec398153b8a9a670e036e509469b0b" alt="PlanetScale deploy request - deploy instantly badge" data-og-width="2522" width="2522" data-og-height="1414" height="1414" data-path="docs/images/assets/docs/concepts/deploy-requests/deploy-instantly.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/deploy-requests/deploy-instantly.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=7f585d13b2398c3db2a411a52dfa2522 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/deploy-requests/deploy-instantly.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=81d8691687a624cb065fda9965b31c09 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/deploy-requests/deploy-instantly.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=24ade6f88b569973fef052e35d28905c 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/deploy-requests/deploy-instantly.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=f93651481b561e3b011d57f578dd2534 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/deploy-requests/deploy-instantly.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=409df0d02944756668fdc5a655cf509e 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/deploy-requests/deploy-instantly.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=1009e44e0bab07501169bcf292239183 2500w" />
</Frame>

We recommend reading [MySQL's Online DDL documentation](https://dev.mysql.com/doc/refman/8.0/en/innodb-online-ddl-operations.html) for the full list of operations that can be deployed instantly.

## Gated deployments

Gated deployments give you more control over when a migration goes live after the deployment process completes.

As part of our non-blocking schema change process, instead of directly modifying table(s) when you deploy a deploy request, we make a copy of the affected table(s) and apply changes to the copy. We get the data from the original table and the copy table in sync, and once complete, initiate a quick cutover where we swap the tables.

<Note>
  If a deploy request includes changes to multiple tables, all tables cut over at the same time — unless there is a sequential dependency.
</Note>

With gated deployments, you can initiate the deployment, but once the table syncing is complete, we'll hold off on the cutover and let you click a button to swap the tables and complete the deployment. Gated deployments can be enabled on each deploy request by unchecking the "Auto-apply changes" box before you deploy.

This feature is helpful if you have long-running migrations. For very large or complex databases, deploying a schema change can take several hours to complete. In those scenarios, you don't want the cutover to happen while you're offline. With gated deployments, you can start the deployment process by adding your deploy request to the queue, and once it's done, you'll be able to click a button to merge it in and complete the deployment while you're there to monitor it.

### Enable gated deployments in the dashboard

<Steps>
  <Step>
    When you open a deploy request, uncheck the "**Auto-apply changes**" box.
  </Step>

  <Step>
    <Frame>
      <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/deploy-requests/gated-deployments-2.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=cb1e535b9f7499c8c0158e514fc3b0a8" alt="PlanetScale deploy request - Auto-apply changes checkbox unchecked" data-og-width="2450" width="2450" data-og-height="1652" height="1652" data-path="docs/images/assets/docs/concepts/deploy-requests/gated-deployments-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/deploy-requests/gated-deployments-2.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=f7b4c397aaffcd5fcaedc1d28fe8186c 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/deploy-requests/gated-deployments-2.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=99a3013ae65886abf12ffb949301886e 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/deploy-requests/gated-deployments-2.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=076c2324e995a6d850293d70675e1360 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/deploy-requests/gated-deployments-2.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=da22f4d9230f162fae47a5f82304c64c 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/deploy-requests/gated-deployments-2.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=d1691c3f0c961800931533098143e230 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/deploy-requests/gated-deployments-2.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=ed4e84c9aa6f7385210ab6985971f349 2500w" />
    </Frame>

    Once your deploy requests begins running, you'll also have the option to uncheck the box here.
  </Step>

  <Step>
    When your deploy request has completed and is ready for cutover, the "**Apply changes**" button will appear. You can now complete the deployment at any time by clicking this button.
  </Step>
</Steps>

### Enable gated deployments via CLI

You can also manage auto-apply settings using the [PlanetScale CLI](/docs/cli/deploy-request).

To create a deploy request with auto-apply disabled:

```bash  theme={null}
pscale deploy-request create <DATABASE_NAME> <BRANCH_NAME> --disable-auto-apply
```

To disable auto-apply on an existing deploy request:

```bash  theme={null}
pscale deploy-request edit <DATABASE_NAME> <DR_NUMBER> --disable-auto-apply
```

Once your deploy request has completed and is ready for cutover, trigger the swap:

```bash  theme={null}
pscale deploy-request apply <DATABASE_NAME> <DR_NUMBER>
```

<Note>
  If neither `--enable-auto-apply` nor `--disable-auto-apply` is provided when creating a deploy request, the setting is inherited from the previous deploy request.
</Note>

### Limitations

* If you have an open gated deployment, you cannot deploy another deploy request until the current one has been merged in.
* Deploy requests that are [instantly deployed](#instant-deployments) cannot be gated.

For more information about this process and why we built it, check out the [Gated Deployments: Addressing the complexity of schema deployments at scale](https://planetscale.com/blog/gated-deployments-addressing-the-complexity-of-schema-deployments-at-scale) blog post.

## Artifact tables

During schema changes, Vitess creates artifact tables to facilitate non-blocking migrations. For detailed information about artifact table behavior, storage implications, and cleanup processes, see the [Artifact tables in schema changes](/docs/vitess/schema-changes/artifact-tables) documentation.

## Revert a schema change

If you ever merge a deploy request, only to realize you need to undo it, PlanetScale can handle that! You have the option to revert a recently deployed schema change while maintaining data that was written to the original schema during that time.

<Note>
  Deploy requests that are instantly deployed *cannot* be reverted.
</Note>

### How to revert a schema change

You can revert a deployment for **up to 30 minutes** after the deploying. After the 30 minute period is up, the deployment becomes permanent, and you will no longer have the option to revert.

<VimeoEmbed id="830571822" title="Revert a schema change" />

<Steps>
  <Step>
    Select the deploy request you want to revert.
  </Step>

  <Step>
    To revert the schema changes made with the deploy request, click "**Revert changes**" and confirm.
  </Step>

  <Step>
    We will immediately revert the base branch back to its previous schema.
  </Step>

  <Step>
    Any data that was written to the original schema in the time between deploying and reverting will remain in your database after the revert.
  </Step>

  <Step>
    The deploy request will be closed, but the branch will remain for you to continue development on if you choose.
  </Step>
</Steps>

### When is data not retained

There are some scenarios where some data is not retained when you revert your changes.

1. You add a table or column to your schema and then revert it. If any data was written to those newly introduced fields between deployment and reverting, that data will not be retained upon revert, as the fields will no longer exist.

### When a revert can result in orphaned rows

In some cases, when you are using foreign key constraints, a revert of a deploy request can result in orphaned rows. These can happen when your schema change is:

* Dropping a foreign key constraint: Once a foreign key constraint is dropped, new data written to the table is less constrained. Reverting this change may result in data that is inconsistent with the dropped foreign key constraint.
* Dropping a table with foreign key constraints: When a table with foreign key constraints is dropped, the parent table(s) will continue to be written to. If this change is reverted, data in the table that was dropped may no longer be consistent with its foreign key constraints.

<Note>
  You must enable [foreign key constraint](/docs/vitess/foreign-key-constraints) support in the database settings page before using them.
</Note>

### When are you unable to revert a schema change

There are also some edge cases where reverting a schema change is not possible. We will always attempt to revert, but if there are scenarios where your data integrity is at risk, we will not proceed with the revert. The following are some cases where a revert will fail:

1. If you deploy a schema change that expands the length of some column, such as changing from `VARCHAR(10)` to `VARCHAR(50)`, and add new data larger than 10 characters to it, a revert attempt may fail. This is to protect your data. You may have written data to the `VARCHAR(50)` field in that time that will not fit in the smaller 10 character space. If no data is added between deployment and revert, the revert process can proceed.
2. Some examples of other similar scenarios where revert won't be possible (again, only if larger sized data is added between deployment and revert) are:
   * `INT` to `BIGINT`
   * `NOT NULL` to `NULL`
   * `TIMESTAMP` to `TIMESTAMP(6)`
   * `utf8` to `utf8mb4`
   * Any other operation that expands the size of a field
3. If you deploy a schema change that removes a unique key or relaxes a unique constraint, and in the time between deployment and attempting to revert, you insert rows that would otherwise conflict with that constraint, the revert may fail.
4. Another uncommon but possible scenario: you deploy a schema change that has a `NOT NULL` column without a `DEFAULT` value, combined with an `ALTER TABLE DROP COLUMN` statement for that column. If you insert some rows between the deployment and the revert attempt, the revert will fail. We will not be able to re-add that column for the newly inserted rows and will not know how to populate it.

For an in-depth look at how this process works, check out our [Behind the scenes: how schema reverts work](https://planetscale.com/blog/behind-the-scenes-how-schema-reverts-work) blog post.

### Schema revert and migration data

If you've selected a migration framework or specified a table with migration data in the settings tab of your database, the data within the table that tracks migrations will be moved to the production branch only after the revert window has been closed. This is to ensure that if the deploy request is reverted, the production branch has the correct log of applied migrations.

### Billing considerations

You may see some temporary `_vt` tables in your database. These are called [artifact tables](/docs/vitess/schema-changes/artifact-tables) and are used to facilitate the deployment and revert process. They do not count toward your storage costs if you're using [network-attached storage](/docs/plans/planetscale-skus#network-attached-storage).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Handling table and column renames
Source: https://planetscale.com/docs/vitess/schema-changes/handling-table-and-column-renames



## Overview

In SQL, it is possible to `RENAME` either a column or a table using an [`ALTER` statement](https://dev.mysql.com/doc/refman/8.0/en/alter-table.html). This keeps the data and type of the column or entire table the same and changes only the name.

PlanetScale does not support this method of table and column renames.

If you do need to rename a column or table, the following section covers how you should do it with PlanetScale using [Deploy Requests](/docs/vitess/schema-changes/deploy-requests).

## How to rename a column on PlanetScale

<Steps>
  <Step>
    Create a new column with the new name.
  </Step>

  <Step>
    Update the application to write to both columns with new data.
  </Step>

  <Step>
    Backfill all the data in the new column for rows that are still missing that information.
  </Step>

  <Step>
    Optionally, add constraints like `NOT NULL` to the new column once all the data is backfilled.
  </Step>

  <Step>
    Update the application to only use the new column, and remove any references to the old column name.
  </Step>

  <Step>
    Drop the old column.
  </Step>
</Steps>

This means at least two deploy requests are needed (potentially more if you want to enforce `NOT NULL` without a `DEFAULT`), where you first add the newly named column and then drop the old one.

## Why not support renames?

There are two reasons why renames are not supported. The first reason is that PlanetScale uses a declarative model for determining the changes between schemas. This means that there is no way to know for sure if something is a rename or if it was an add and drop column. The second reason is that safely performing a rename requires downtime for your application.

### Declarative model for schema migrations

PlanetScale uses a declarative model to determine schema changes. This means that we look at the end state of two branches and compare them to find the difference. When you rename a column, there is no way to know whether there was a rename or if a column was added and dropped in a development branch.

It would be possible to apply heuristics to determine if something looks like a rename, but this is not 100% guaranteed to be correct. For example, if a column is renamed and a second column is dropped adjacent to the one that was renamed, it is impossible to determine which column to rename with certainty.

Because of this, we only detect simple cases that look like a rename and alert the user on that in a deploy request. While it would be possible to generate a `RENAME` in such a case, we don't know if that is correct, and do not want to execute any schema changes that are not requested.

### Downtime with renames

When using a `RENAME`, downtime for your application can only be avoided with significant and complex logic on the application side. While the application is running, suddenly a column disappears and another one appears that contains the same data. This means that the application both needs to be able to handle the state when this happens and know that the data is the same.

Usually the only way to handle a column rename is to simultaneously deploy the application, but that is very difficult as it races with the database changes and is not perfectly atomic and synchronized with the schema change.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# How Online Schema Change tools work
Source: https://planetscale.com/docs/vitess/schema-changes/how-online-schema-change-tools-work



## The schema change problem

Changing a table's schema is one of the most challenging problems in relational databases, and in MySQL in particular. In today's accelerated and rapid development cycles, engineers find that they need to make schema changes sometimes on a daily basis. Direct `ALTER TABLE` is a blocking operation, which renders the table completely inaccessible, even for reads.

MySQL's InnoDB engine supports Online DDL for most operations. However, it is only *online* on the primary. On replicas, it executes sequentially to other statements, leading to unacceptable replication lag. On the primary, it is aggressive and uncontrollable. It will compete with production traffic over resources. InnoDB also offers Instant DDL, which is available for a small subset of schema change operations. See the full breakdown on the [MySQL documentation](https://dev.mysql.com/doc/refman/8.0/en/innodb-online-ddl-operations.html).

## Schema change solutions

In today's world, the vast majority of users use one of the Online Schema Change tools. These are external, small tools or scripts, that hook onto the database and run a migration through emulation. Common external tools are [gh-ost](https://github.com/github/gh-ost), [pt-online-schema-change](https://www.percona.com/doc/percona-toolkit/3.0/pt-online-schema-change.html) and [Facebook's OSC](http://bazaar.launchpad.net/~mysqlatfacebook/mysqlatfacebook/tools/annotate/head:/osc/OnlineSchemaChange.php). This document describes the general technical operation of these tools.

It's worth noting an alternative to using Online Schema Change tools, via rolling changes on replicas. In this technique, the user runs either straight or Online DDL on replicas, typically one or a few replicas at a time. Then, once all replicas have been updated with the new schema, the user demotes the primary and promotes one of the replicas. The demoted primary, now a replica, can then be updated, as well. This technique requires no external tooling. However, it takes substantially longer to complete the overall operation and requires running a promotion, which is typically disruptive to the applications.

## The general logic behind Online Schema Change tools

The Online Schema Change tools have many differences, covered in our [Comparison of Online Schema change tools doc](/docs/vitess/schema-changes/online-schema-change-tools-comparison). However, they all share a similar fundamental logic and flow.

All tools work by *emulating* a schema change. The general flow is:

* Create a new, empty table, in the likeness of the original table. We title this the *ghost* table.
* `ALTER` the ghost table. Since the table is empty, there is no overhead to this operation.
* Validate the structural change is compatible with tooling requirements.
* Analyze the diff.
* Begin a long running process of copying existing rows from the original tables to the ghost table. Rows are copied in small batches.
* Capture or react to ongoing changes to the original table, and continuously apply them onto the ghost table.
* Monitor general database and replication metric, and throttle so as to prioritize production traffic as needed.
* When the existing data copy is complete, the migration is generally considered as ready to cut-over, give or take some small backlog or state of the replication topology.
* Final step is the cut-over: renaming away of the original table, and renaming the ghost table in its place. Up to some locking or small table outage time, the users and apps are largely ignorant that the table has been swapped under their feet.

In the following sections, we break down these steps to understand better the requirements, limitations, and correctness of the flow.

## Initializing the ghost table schema

The schema migration flow never changes or updates the original table. We only ever operate on the *ghost* table. We create a ghost table with an exact copy of the original table's schema, then we run the user's `ALTER TABLE` command on the ghost table.

First and foremost, we are able to validate that the statement is valid. For example, if the user requests an `ALTER TABLE foo DROP COLUMN bar`, executing that statement on the ghost table *successfully* validates that the column `bar` did, in fact, exist in the first place. Similarly we can catch any lower level errors such as an oversized index, invalid `DEFAULT` values, etc.

This is also a place to check whether we want to enforce a change to `AUTO_INCREMENT`.

## Validating the change

On top of validating the raw MySQL change, all Online Schema Change tools come with their own restrictions. A couple notable restrictions are:

* **Existence of relevant keys** — All tools copy data from the original table onto the ghost table by slow iteration. The iteration utilizes a `UNIQUE KEY`, ideally the `PRIMARY KEY`. The different tools have somewhat different constraints, but they all require some form of `UNIQUE KEY` or `PRIMARY KEY` to exist on the ghost table, maybe even require a *shared key* between the two tables. The existence of a key on the target table is required to satisfy the following:

  * When applying an ongoing change on the original table, for example an `UPDATE`, how do we efficiently find the row on the target table?
  * When copying a chunk of data from the original table onto the ghost table, how do we resolve conflicts? By way of example, by the time we copy the last chunk of data, we may have already captured and applied the `INSERT` change, so that the rows already exists on the ghost table. More on that logic later on, but it is the `UNIQUE KEY` that helps us resolve the conflict.

* **Existence/addition of `FOREIGN KEY` constraints** — Foreign keys do not play well with the Online Schema Change flow, and there are either severe limitations on, or rejection of, tables that have foreign key constraints.

## Analyzing the diff

The tools compare the two tables to determine what makes them similar and what makes them different. Most importantly, the tools need to understand which columns are shared between the two tables that should be copied from the original table to the ghost table.

The tools need to take into consideration a potential rename of a column, or whether columns are `GENERATED`, or turned from `GENERATED` to normal columns or vice versa.

## Copying rows

This is the single most time consuming part of the migration. Transferring all rows from one table to another can be a long running task even if running on a completely stale server. On large tables in production, the row copy could take as long as hours, days or even weeks.

Copy is done via a statement of the form:

```sql  theme={null}
INSERT [IGNORE] INTO <ghost_table> (<columns...>) SELECT <columns...> FROM <original_table> WHERE <range...>
```

It is impractical, or inefficient, to copy all rows from the original table to the ghost table in one pass. The tools work by iterating the original table start-to-end using some chosen `UNIQUE KEY` (typically the `PRIMARY KEY`). They only copy `n` rows at a time, where common values are around `100` or `1000` rows per chunk, though based on row data size and on workload, these can also be on the range `10...10000`.

We reasonably know where the table starts. But where does it "end"? Consider a table with an `AUTO_INCREMENT PRIMARY KEY` column. As we work to copy the table rows, the app writes more rows into the table, pushing the end farther and farther. This creates an illusion of a never ending flow. However, this is not the case. The tools evaluate the first and last rows (as ordered by the iteration key), and only copy rows in that range. Any excessive rows beyond that range will be captured by following the ongoing changes. It is imperative then to begin capturing changes *before* evaluating row range.

Copying, say, `100` rows at a time will very likely yield with an image inconsistent with a single snapshot of all rows. As we copy the first `100` rows, new rows can be added, removed or updated in the next `100` rows range. As we advance in our copy phase, the data gets more and more skewed from the original imaginary snapshot from the migration starting point. That is in fact not a problem, as the skew serves us to reflect an even more updated image of the data. We are interested not in the data snapshot at the *start* of the migration, but at the *end* of it. We want the ghost table to be aligned with the original table at such point when we can swap the two without data loss. Applying the ongoing changes is of course the key to achieving that. More on the interaction of row copy and ongoing changes later on.

It's simple to consider how we may iterate a table with, say, a `AUTO_INCREMENT PRIMARY KEY`. It's easy to imagine iterating rows `[1..100]`, `[101..200]`, `[201..300]` and so forth. But we must then also consider real life situations:

* We might have huge punctures in the `AUTO_INCREMENT` sequence. Rows `200..5000000` may be altogether missing, and iterating that range could be extremely wasteful.
* Can we miss out on unforseen gaps? If we copied row `100`, does it follow that the next row is `101`? Could there be some `100.5`? Not if the data type is an `INT`, but what if it's another type?
* Our iteration key may not necessarily be numeric. It may in fact contain multiple columns. How can you iterate a `PRIMARY KEY (file_name, submitted_at)` where `name` is a `VARCHAR` and `submitted_at` is a `TIMESTAMP`?

We illustrate below the *pseudo code* used by Online Schema Change tools to iterate a table using a key with any number and type of columns. For clarity, let's assume two columns `(file_name, submitted_at)`; the logic remains similar, though somewhat more detailed, the more columns we iterate by.

<Steps>
  <Step>
    Grab the minimum value via:

    ```sql  theme={null}
    SELECT file_name, submitted_at FROM original_table ORDER BY file_name, submitted_at ASC LIMIT 1 INTO @first_entry_file_name, @first_entry_submitted_at`
    ```
  </Step>

  <Step>
    Grab the maximum value via:

    ```sql  theme={null}
    SELECT file_name, submitted_at FROM original_table ORDER BY file_name DESC, submitted_at DESC LIMIT 1 INTO @last_entry_file_name, @last_entry_submitted_at`
    ```
  </Step>

  <Step>
    Assign `(@previous_iteration_end_entry_file_name, @previous_iteration_end_entry_submitted_at) := (@first_entry_file_name, @first_entry_submitted_at)` and repeat.
  </Step>

  <Step>
    For any iteration: evaluate the next range:

    ```sql  theme={null}
    SELECT file_name, submitted_at FROM (
      SELECT file_name, submitted_at FROM original_table
      WHERE
        (file_name, submitted_at) > (@previous_iteration_end_entry_file_name, @previous_iteration_end_entry_submitted_at)
        AND
        (file_name, submitted_at) <>= (@last_entry_file_name, @last_entry_submitted_at)
      ORDER BY file_name, submitted_at LIMIT 100
    ) ORDER BY file_name DESC, submitted_at DESC LIMIT 1
    INTO @this_iteration_end_entry_file_name, @this_iteration_end_entry_submitted_at
    ```

    In words, get the last `file_name, submitted_at` values counting `100` rows starting immediately after end of previous iteration, but also don't exceed our "end" position.
  </Step>

  <Step>
    Follow up with copy:

    ```sql  theme={null}
    INSERT INTO ghost_table (<columns...>)
      SELECT <columns...>
      FROM original_table
        WHERE
          (file_name, submitted_at) > (@previous_iteration_end_entry_file_name, @previous_iteration_end_entry_submitted_at)
          AND
          (file_name, submitted_at) <= (@this_iteration_end_entry_file_name, @this_iteration_end_entry_submitted_at)
      ORDER BY file_name, submitted_at
    ```

    The two queries could have been executed as one, but by first evaluating the range using only the key columns, this process is actually more efficient.
  </Step>

  <Step>
    Assign `(@previous_iteration_end_entry_file_name, @previous_iteration_end_entry_submitted_at) := (@this_iteration_end_entry_file_name, @this_iteration_end_entry_submitted_at)` and repeat.
  </Step>
</Steps>

<Note>
  The above pseudo-code is just a simplification of the process.
</Note>

There are several extra considerations that go into the full process, such as:

* Care needed to capture the very first row in iteration.
* Missing from the above is identifying the stop condition (empty row set found).
* MySQL actually performs poorly with tuple comparison. The condition `(a, b) > (x, y)` must actually be phrased as `a > x OR a = x AND b > y`. This explodes fast the more columns are at play.
* Most implementations will actually use a `INSERT IGNORE` rather than `INSERT`, to play well with ongoing updates.

## Ongoing changes

Tools will capture ongoing changes by either utilizing database triggers or by hooking into the replication stream. This way or another, the changes must be applied to the ghost table.

As with row copy, we are only interested in copying a subset of columns that are shared between the tables. Again, care needed to consider renamed or `GENERATED` columns.

The ongoing changes always represent the latest and most up-to-date image of the data. As such, they must override any prior data already captured on the ghost table. As an example, consider we have just copied our first iteration of `100` rows, and then observe an `UPDATE` on, say, row `32`. This update must override whatever value we have already copied into the ghost table for row `32`. To this effect, row copy and applying changes take lower and higher priorities, respectively, over conflicts.

Some of the tools achieve this like so:

* Row copy only issues a `INSERT IGNORE INTO <ghost_table> ... SELECT ... FROM <original_table>`
* Changes applied via `REPLACE INTO <ghost_table> ...`

Consider a counter example to our previous scenario. Imagine we're now intercepting a change to row `1024`, but row copy has not reached that row yet. We apply the change. When we finally reach and copy the range that includes row `1024`, that specific row does *not* get overwritten. Now, what if by that time of row copy the row has been updated again? Isn't the row copy now even more up-to-date than the existing `1024` row on the ghost table? It is, in fact. But if that is the case, then there must have been a change, an `UPDATE` to row `1024`, that we just haven't applied yet, and are yet to discover and apply via `REPLACE INTO`.

Different tools tackle this issue differently. A synchronous solution like `pt-online-schema-change` is *always* up-to-date with ongoing changes, while [Vitess's VReplication](https://vitess.io/docs/api/reference/vreplication/vreplication/) takes a completely different approach by marking GTID positions that have been or not have been already covered. The above should serve as a good illustration to the flow, regardless of the actual implementation.

## Monitor and throttle

Most tools run some form of monitoring over the affected server or of its replicas, or both. Either sequentially to the copying process or concurrently, the tools may choose to throttle based on one or more metrics.

The single most important metric, by far, is replication lag. How the tools discover the replicas, choose which replicas to consider, respond to topology changes, is a matter of implementation and of a different discussion. However, the tools that do observe replicas all try to maintain as low replication lag as possible, and will step back once that lag grows. What the threshold is, is configurable. On extremely busy and sensitive systems this may be as low as `1` second. Other environments may tolerate *minutes* of lag.

Not all tools can throttle completely. Specifically, trigger based tools can only throttle the row copy process, but not the ongoing changes tracking. Non-trigger based tools are able to completely halt any processing, or choose to take lower precedence over production traffic. For more information about trigger based vs non-trigger based tools, check out our doc on [Comparison of online schema change tools](/docs/vitess/schema-changes/online-schema-change-tools-comparison#trigger-based-vs-triggerless).

## Determining when cut-over is possible

Generally speaking, in a normal and successful migration flow, the migration is ready to cut-over once row-copy is complete. Different tools may still have some backlog to process. Under heavy workload, the backlog may take substantial time to process.

However, the general state of the replication topology can also be a factor. It is a good idea to only cut-over when replication lag is minimal, to avoid schema confusion between primary and replicas. Also, the current workload on the primary plays an important part, as discussed in the next section.

## Cutting over

By far the most critical point of the migration, this is where the changes take effect and the new, changed schema, tastes first production traffic.

It's first important to realize some migrations do not go as planned. Some drop the wrong column, or change the wrong index, and such is life. The cut-over point is where the user might expect trouble. When some queries suddenly fail, or others suddenly perform poorly. Some of the tools will cut-over automatically when they deem the migration is complete, and others will allow the user to choose the timing.

Replacing the table is also a point of contention. MySQL-wise, we need to acquire a table lock, also known as a metadata lock. As internal MySQL implementation goes, one cannot acquire a metadata lock while a query still runs on the table. This means a long running query may block the cut-over for a substantial time. This is why it's good to either consult with the running workload, or let the user choose the cut-over time.

Once metadata lock is acquired, application connections start noticing something goes on. Depending on the cut-over implementation, the metadata lock can be held up to a few seconds. During that time more queries will pile up, leading to increased number of connections on the server. It's not unreasonable that the number of connections will surpass the configured database connections threshold, at which time no new connections and queries are allowed. This is again obviously noticeable to the apps. Trigger based tools suffer from an even increased metadata lock, and under heavy workloads this can cause seconds or minutes long stalls.

Depending on the implementation, the tables will either swap atomically, such that the connections are suddenly unblocked and "magically" proceed to run on the new (previously *ghost*) table; or, there will be a "puncture" in time, where the table ceases to exist, which leads to query errors for a brief time.

At long last, the migration is complete with the *ghost* table assuming the role of the original table. Production traffic proceeds to run on the new table. The old table is then normally left for a while, to be dropped later on by the database engineer.

## Beyond cut-over

Most tools stop their operation and their interest at cut-over. Vitess, in particular, goes beyond that point to:

* allow the migration to be *reverted*
* clean up the left over tables

For more information about how Vitess handles schema changes, check out the [Vitess Schema Changes documentation](https://vitess.io/docs/user-guides/schema-changes/).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# How to make different types of schema changes
Source: https://planetscale.com/docs/vitess/schema-changes/how-to-make-different-types-of-schema-changes

Your database has to grow and change with your application and product.

## Overview

So schema changes are inevitable. Luckily, PlanetScale has features like [branching](/docs/vitess/schema-changes/branching), [deploy requests](/docs/vitess/schema-changes/deploy-requests), and [revert](/docs/vitess/schema-changes/deploy-requests#revert-a-schema-change) to help give you more confidence when making schema changes. But you still have to consider though how to make the schema change. Some schema changes are more complex than others. This document will help you understand how to make different types of schema changes and associated risks.

<Info>
  Most of the following applies to any relational database and not only to PlanetScale.
</Info>

## Adding a table

Adding a table is a low risk since it does not affect the existing schema. Once you add the table to your schema and deploy the database to production, if you want to backfill the data, now would be the time. Otherwise, you add it to your application code after completing your database deployment. Deploying the database first and then changing your application code to avoid downtime is best.

## Adding a column

Adding a column without a `NOT NULL` constraint is a low risk and can be handled similarly to [adding a table](#adding-a-table).

The one exception is if you want to add a column with a `NOT NULL` constraint and/or no `DEFAULT` value. The risk increases because the previously inserted rows will be expected to be `NOT NULL` or have no default values; this will cause possible errors in your application.

To work around this, you can do the following:

<Steps>
  <Step>
    Add the new column without defining a DEFAULT value, allowing NULL values.
  </Step>

  <Step>
    Write a script to backfill the new column for all missing data rows.
  </Step>

  <Step>
    Add the `NOT NULL` constraint once the data is backfilled.
  </Step>
</Steps>

## Changing a column or table

Some examples of changing a column or table include:

* Renaming an existing column or table
* Changing the data type of an existing column
* Splitting and other modifications to the data of an existing column or table

This includes modifying your application to use a different column or table in existing code. For example, if you are using `username` in the code but instead want to start using a new `user_id` column in the existing code.

Changing a column or table your current application uses is a high risk. It can cause disruptions to users and possible downtime. If you want to change a column or table, we recommend following the expand, migrate, and contract pattern described in the [backward compatible schema changes](https://planetscale.com/blog/backward-compatible-databases-changes) blog post. The blog post walks you through each step in both your database and application code and gives a detailed example.

## Removing a column or table

### In use

Removing a column or table your current application uses is a high risk. Dropping a column or table can cause disruptions to users and possible downtime. If you want to remove a column or table in use, we recommend following the expand, migrate, and contract pattern described in the [backward compatible schema changes](https://planetscale.com/blog/backward-compatible-databases-changes) blog post. The blog post walks you through each step in both your database and application code and gives a detailed example.

### Not in use

Removing a column or table that is no longer in use by your current application is a low risk. The only danger is potentially needing the removed data in the future.

## Other schema changes

This is not an exhaustive list of schema changes. Remember, making smaller incremental schema changes is always safer than larger, more complex ones. If you want to read more on making safer schema changes with PlanetScale, read this blog post on [safely making database schema changes](https://planetscale.com/blog/safely-making-database-schema-changes).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Comparison of Online Schema Change tools
Source: https://planetscale.com/docs/vitess/schema-changes/online-schema-change-tools-comparison

Online Schema Change tools for MySQL [share a similar basic design](/docs/vitess/schema-changes/how-online-schema-change-tools-work), but differ in implementation.

Their differences imply different capabilities, different impact to production, different versatility, and different user experience. In this document, we break down several Online Schema Change tools to see how their design choices lead to different behavior.

## Tools reviewed

We will compare these solutions:

* [pt-online-schema-change](https://www.percona.com/doc/percona-toolkit/3.0/pt-online-schema-change.html) as part of Percona Toolkit.
* [gh-ost](https://github.com/github/gh-ost)
* [Facebook's OSC](http://bazaar.launchpad.net/~mysqlatfacebook/mysqlatfacebook/tools/annotate/head:/osc/OnlineSchemaChange.php)
* [Vitess](https://vitess.io/docs/user-guides/schema-changes/)

Some notes before we take off:

* All reviewed solutions are free and open source
* `pt-online-schema-change`, `gh-ost`, and `Facebook's OSC` are designated (command line) tools for making schema changes.
* Vitess is a full blown sharding and infrastructure framework, where online schema changes (termed Online DDL) is one of its functions. As such, it is not an "online schema change tool" per se. In the context of Vitess's Online DDL, we assume all production traffic goes through Vitess's MySQL interface (the `VTGate` proxy), and never directly to the underlying MySQL servers.
* It's noteworthy that Vitess can manage and run `gh-ost` and `pt-online-schema-change` migrations. In this comparison we look at the native Vitess Online DDL solution, based on [VReplication](https://vitess.io/docs/user-guides/schema-changes/ddl-strategies/#onlinevreplication).
* We discuss the original Facebook online schema change tool. Facebook (now Meta) have since [redesigned and re-implemented](https://engineering.fb.com/2017/05/05/production-engineering/onlineschemachange-rebuilt-in-python/) the tool.
* The original online schema change tool, [oak-online-alter-table](https://shlomi-noach.github.io/openarkkit/oak-online-alter-table.html), as part of the OpenArk kit, is discontinued and is not reviewed here. `pt-online-schema-change` derives its design from it and the two share many properties.
* Another notable tool not covered here is [Rails LHM](https://github.com/soundcloud/lhm).

For simplicity of naming, we will refer to `pt-online-schema-change` as `pt-osc`, and to Facebook's OSC as `fb-osc`.

## Comparison by design

This document compares the online schema change solutions by key design elements, and shows how those affect capabilities and usability. We do not compare benchmarks or specific performance metrics. We will share some production experiences, while taking care not to make blanket statements. Each tool behaves differently under different workloads.

Our breakdown compares these design principles:

* Synchronous vs asynchronous
* Trigger-based vs. triggerless
* Coarse vs. fine
* Atomic vs. punctured cut-over
* Controlled vs. arbitrary cut-over schedule
* Primary vs. shared vs. flexible key iteration
* Managed vs. unmanaged
* Auditable/controllable vs. not
* Resumable after failure vs. not
* Revertible after completion vs. not
* Declarative vs. imperative

Make sure to first read [How Online Schema Change tools work](/docs/vitess/schema-changes/how-online-schema-change-tools-work).

## Synchronous vs asynchronous

In a synchronous design, the ghost table is updated within the same transaction that updates the original table. If a new row is `INSERT`ed to the original table, then within that transaction we also `INSERT` a row on the ghost table, and similarly for `UPDATE` and `DELETE`. By virtue of transactions, either both statements apply, or neither does.

This design simplifies much of the logic. Once the table copy is complete, the synchronous design ensures the tables are in perfect sync, and will continue to be so if we were to keep the process going.

The only way to achieve this design is by delegating work to the database itself, so that we're able to hook into a user's transaction. It is impossible for an external tool to apply changes to the ghost table while still operating under the user's transaction. The only means to do that in current MySQL is using table triggers (see next design comparison).

In our reviewed solutions list, `pt-online-schema-change` is the only one to use the synchronous approach.

In an asynchronous design, we apply changes to the ghost table asynchronously to the user's transaction. This means there is some *lag* between the user's `COMMIT` and the time at which the reflected change is applied to the ghost table. That lag can be small or high, and is always non-zero. Even at the moment when table copy is complete, there is no certainty that the two tables will be in complete sync. Ongoing production traffic may keep pushing the lag.

The asynchronous approach implies we need to capture the changelog. We need some buffer where we can track this `INSERT` or that `DELETE`, which can sustain a growing lag.

The two approaches to achieving asynchronous schema changes are:

* Capture data change via triggers and use a helper table for capturing the changelog — used by `fb-osc`.
* Tail the MySQL binary logs, which *are* the changelog — used by `gh-ost` and `Vitess`.

However the changelog is captured, a background process needs to iterate the incoming changes, to then apply them onto the ghost table.

## Trigger-based vs. triggerless

Triggers are MySQL constructs based on stored routines, which allow us to respond to `INSERT`, `UPDATE` and `DELETE` statements, either before or after they have been applied to the table. A trigger is invoked per row changed (created, updated or deleted) on a specific table. The invocation takes place in the same transaction space as the query which operated on the table.

Triggers make it easy to keep things in the database level. Applying an `UPDATE` to the ghost table does not leave the server's space. This means we can let MySQL handle the way data is copied. For example, if the schema change converts a column type from `INT` to `BIGINT`, or from `enum` to `VARCHAR`, we don't need to worry about the type conversion, and we let MySQL do its natural conversion.

Stored routines were introduced in MySQL `5.0` and have changed little since. A stored routine (procedure, function, event, trigger) is interpreted by the database. As opposed to other RDBMS, MySQL does not compile or pre-compile stored routines. Every time a stored routine is invoked, the code is interpreted by the database server. This means a user's transaction gets overloaded, or padded, with interpreter logic, and on high workloads this is noticeable to the app.

Another limitation of the trigger-based design is that the triggers must be active throughout the lifetime of the migration. Online schema change solutions attempt to keep migration impact to production as low as possible, and throttle based on production load. However, the triggers must keep running, since they are the means to capturing changes, or else we have data loss. They cannot be throttled.

Both `pt-osc` and `fb-osc` use triggers, but in different approaches:

* `pt-osc` uses the synchronous approach. Therefore, an `INSERT` on the original table uses the trigger to also `INSERT` into the ghost table in that same transaction. While this maintains perfect sync between the two tables, it also introduces excessive locking. While normal transactions only intend to compete over writes to the original table, with the added triggers they also have to compete over writes to the ghost table. On high workloads this can bring down application's ability to write to the table to a considerable slowdown or even to a grinding halt. Again, this is workload dependent and for some workloads this works well.
* `fb-osc` uses the asynchronous approach. The tool creates a changelog table, an append-only buffer. Each `INSERT`, `UPDATE` or `DELETE` on the original table translates to an `INSERT` on the changelog table. This reduces somewhat the locking contention since all changes map to new (and sequentially ascending) rows in the changelog table rather than existing (and random order) rows on the ghost table.

Even though `fb-osc` uses the asynchronous approach, it still keeps the data in the database itself, and again enjoys MySQL's ability to reliably copy the data from the changelog table onto the ghost table.

In the triggerless approach, the changelog cannot be acquired directly from the running transactions, and must be collected elsewhere.

Both `Vitess` and `gh-ost` use a triggerless design, and both get the changelog from MySQL's replication stream.

With binary logs enabled in MySQL, once a transaction completes its effects are written to the binary logs, and are made available to the replication stream.

Notes:

* How the transaction is written in the binary logs depends on the binlog format. Both `gh-ost` and `Vitess` require a `ROW` binlog format. `gh-ost` is more flexible in allowing a `STATEMENT` format on a primary with `ROW` format on a replica.
* Depending on MySQL's configuration, the transaction and binary log entry couple be written atomically, or, there could be lag between the `COMMIT` and the binlog entry flush.

The binary log is, in essence, the changelog of operations made on the MySQL server. It is the means for replication, where replicas replay those changes to achieve consistency with the primary MySQL server.

It is possible to read the binary logs directly from disk, but both `Vitess` and `gh-ost` choose to hook to the MySQL server imposing as replicas, and requesting the binary log events over the wire. This allows flexibility in where the schema change logic can run.

Reading data from the binary logs / stream means the schema change solution owns the interpretation or mutation of the data. It is no longer possible to delegate that work to MySQL. For example, texts are always written in UTF in the binary logs, regardless of the character set they originated with in the table. Conversions between types (again illustrating an `enum` to `varchar` change) need to be taken care of by the solution's logic. This makes a triggerless design more complex in handling of data and the numerous ways with which it can mutate.

A triggerless design is inherently asynchronous. Capturing binlog changes from MySQL's replication stream, or even from disk, is not coupled with transaction commit time. There will be some lag between the time where an `UPDATE` is in the binary log stream, and time at which it is read from the stream.

Read more on why `gh-ost` took a [triggerless approach](https://github.com/github/gh-ost/blob/master/doc/why-triggerless.md).

## Coarse vs. fine

`pt-osc`, `fb-osc` and `gh-ost` all work in a coarse approach. To some extent it's a brute-force method of copying the data:

* They iterate the original table and copy chunks of rows to the ghost table.
* They respond to ongoing changes (whether sycnhronously or asynchronously) and apply them to the ghost table.

The two operations could conflict. What if we applied a row by capturing the changelog, before we copied it via table iteration? What do we do when we then try to copy that chunk of data? In a synchronous operation, what if that actually happens concurrently in competing transactions?

The coarse approach gives the changelog a higher priority over the table copy, and lets both run independently of each other. In a normal migration, there will be many conflicts between the row-copy and the changelog. They are resolved using database queries, simplified to:

* `INSERT IGNORE INTO ghost_table (...) SELECT ... FROM original_table` for table copy, and
* `REPLACE INTO ghost_table (...) ...` for changelog capture.

Noteably `gh-ost` only ever runs table copy or applies changelog at a time, and there is never a competition between two concurrent transactions.

`Vitess` takes a fine approach where it keeps track of:

* **Executed `GTID`** — `Vitess` uses `GTID` while writing to the ghost table. It keeps a very small progress table to which it audits the `GTID` for the latest write to the ghost table. This is done atomically within the same transaction.
* **Last copied range** — `Vitess` evaluates, programmatically, which key range has already been copied. The key range is again persisted in a helper table, atomically with the write to the ghost table.

Vitess operates in a three step loop:

* **Copy** — opens a transaction with `CONSISTENT SNAPSHOT` while atomically grabbing the `GTID` at time of the transaction. Begins streaming rows from the table.
* **Catchup** — scans the changelog.
  * Ignore anything not related to the original table
  * Ignores anything that is already covered by the `GTID` captured above.
  * Ignores operations on rows which have not been iterated yet.
* **Fast forward** — prepare the next Copy phase, providing it the next unhandled row range, and, since takes non-zero time to achieve, backfill the remaining changelog events as with the Catchup change.

With this fine grained approach, Vitess is able to avoid superfluous writes to the ghost table, and only keep to the minimum necessary data copy.

## Atomic vs. punctured cut-over

The cut-over is the most critical part of the migration process. It is when the new schema takes effect, and is a source of concern:

* If we did anything wrong (dropped the wrong index? Wrong column?), that's when the application may break or degrade performance.
* There is non-zero impact time for instating the new table. Queries will either stall, break or both for a hopefully brief period.

`pt-online-schema-change` and `gh-ost` both utilize an atomic cut-over. In this approach, the app or user's queries on the original table are briefly *blocked*, but not rejected, during cut-over time. When the ghost table is renamed in place of the original table, the queries get *unblocked* and proceed to operate on the new table.

On high workloads this is typically noticeable to the app. There will be some delays to queries. Moreover, there will be some pileup of connections. This is because an open connection on MySQL can only run a single query at a time. A `1000` blocked queries will mean a `1000` open connections. The busier the workload is, the higher number of piled up connections will be. Eventually, the database may run out of open connection limit (either by its own configuration or by OS limits), at which time apps and users will experience rejected connections or queries.

`pt-osc`'s cut-over is the simplest of all. Becuase of its synchronous design, it is able to cut-over with a single two-table rename statement, such as `RENAME TABLE original TO _original_old, ghost_table TO original`. However, the metadata lock acquired also applies to the triggers, and under heavy workload the cut-over becomes a dangerous point of contention that can lock for long seconds or minutes.

`gh-ost` uses the asynchronous approach, where there is some lag between the original and ghost tables. It implements an [elaborate](https://github.com/github/gh-ost/blob/master/doc/cut-over.md) logic to block queries on the original table while still applying last backlog changes to the ghost tables. Users have shown that under very specific query sequences, in particular monitoring-like queries that happen to access the ghost table just before/at cut-over time, there are scenarios that introduce data loss. `gh-ost` also supports a two-step cut-over, as illustrated next.

`fb-osc`, also using an asynchronous approach, tackles the problem by creating a puncture. It runs a two-step flow where it first renames the original table away, and then, after processing the backlog, renames the ghost table in its place. There's a period in time where the table just does not exist. During that period in time, queries will fail, unexpectedly to the app/users. It is a simple and effective solution to the asycnhronous complexity, at the cost of user/app inconvenience.

`Vitess` utilizes the fact that traffic goes through its own `VTGate` and `VTTablet` components. Behind the scenes, it creates a puncture, much like `fb-osc`. But during that brief time, it buffers any read/write queries that operate on the migrated table, up to a threshold. The user and app will experience an atomic, blocking cut-over. Queries will see increased latency and will block until cut-over is complete. Beyond some concurrent connections threshold, Vitess will reject queries, and the apps will notice an error.

## Controlled vs. arbitrary cut-over schedule

In an arbitrary cut-over schedule design, the Online Schema Change solution decides when to cut-over, and it does so in the earliest possible opportunity.

`pt-osc` an `fb-osc` take that approach. `pt-osc` will cut-over as soon as table copy is complete, and `fb-osc` will do the same, allowing the extra brief time for processing the remaining backlog.

Since cut-over time is the most critical part of the migration, there is advantage to having people at their console and in their comfortable working hours when it takes place. Waking up at `2:00am` on a Saturday night is less than ideal for dealing with outages and incidents.

`gh-ost` and `Vitess` support arbitrary schedule, but both also allow the user to decide when to cut-over. In this controlled approach, the migration process does not terminate voluntarily by the tool/solution. After table data is copied, and even when all seems to be in order, migration simply continues running. The tools keep tracking and applying the changelog to keep the ghost table in near sync (both `gh-ost` and `Vitess` use the asynchronous approach). At some point, the user may declare they are ready. Assuming conditions allow, or as soon as possible afterwards, the tools will proceed to run the cut-over.

* `gh-ost` provides a [-postpone-cut-over-flag-file](https://github.com/github/gh-ost/blob/master/doc/command-line-flags.md#postpone-cut-over-flag-file) command line flag, and cut-over either when the file is removed or the user interactively commands the cut-over.
* `Vitess` migrations [can be postponed](https://vitess.io/docs/user-guides/schema-changes/postponed-migrations/) with `-postpone-completion` flag, and completed via `ALTER VITESS_MIGRATION <uuid> COMPLETE`.

Controlled cut-over is a means for higher confidence and peace of mind for the operational user.

## Primary vs. shared vs. flexible key iteration

Original table's data is copied to the ghost table by slowly iterating all rows. That takes place according to some order, and, in particular, according to some `UNIQUE KEY` order. Ideally, we'd always iterate by `PRIMARY KEY`, but sometimes the `PRIMARY KEY` itself is being modified by the schema change, which requires us to look for alternatives.

Similarly, when we capture, say, an `UPDATE` event in the changelog, we need to uniquely identify the relevant row in the ghost table. There must be some way to ensure we're addressing the correct row, even in face of a change in `PRIMARY KEY`.

The Online Schema Change tools differ in implementation as follows:

* `fb-osc` will only ever iterate on `PRIMARY KEY` order. It allows original and ghost tables to have different `PRIMARY KEY`s, but required the `PRIMARY KEY` on the ghost table to only cover columns which exist in the original table.
* `pt-osc` allows any type of `UNIQUE KEY`. Any change of `PRIMARY KEY` requires the user to add `--no-check-alter`. It allows the ghost table to have `UNIQUE KEY`s on columns not present in the original table, and will in such case use unindexed columns for `DELETE` operations, which is a major performance hit on large tables.
* `gh-ost` requires some [shared key](https://github.com/github/gh-ost/blob/master/doc/shared-key.md). There must be any `UNIQUE KEY` (`PRIMARY` included) on the original table, which covers exact set of columns, and in same order, as some `UNIQUE KEY` on the ghost table. The name of the key does not matter, just the identity and order of covered columns.
* `Vitess` requires the original table to have some `UNIQUE KEY` and the ghost table to have some `UNIQUE KEY`. the two do not have to be the same. Vitess requires columns covered by the original table's key to exist in the ghost table, and columns covered by the ghost table's key to exist in the original table.

## Auditable/controllable vs. not

Both `fb-osc` and `pt-osc` are non-auditable and uncontrollable. Once they start running, they're on auto-mode and will pursue the migration to completion or until it fails. It's impossible to meanwhile control the tools, change their behavior, query for information etc.

`Vitess` and `gh-ost` are both controllable and auditable.

`gh-ost` opens a Unix socket, by which the user may communicate with the running migration. It is possible to reconfigure `gh-ost` while running (by e.g. setting new throttling thresholds), manually throttling it, getting status, cutting-over and more.

`Vitess` lets the user communicate with the migration via SQL, with such commands as `SHOW VITESS_MIGRATION LIKE '...'` (get information and status), `ALTER VITESS_MIGRATION '...' CANCEL` (abort a running migration), `ALTER VITESS_MIGRATION '...' COMPLETE` (cut-over a postponed migration) and more.

Both `gh-ost` and `Vitess` provide extra information about the migration, such as estimated progress percent as well as ETA (expected time till completion).

## Managed vs. unmanaged

The tools `pt-osc`, `fb-osc` and `gh-ost` are concerned with making an `ALTER TABLE` (or emulation thereof) happen, and are not concerned about how the migration was made or scheduled, or what its consequences might be. Management of the migration flow must take place externally to the tools. The tools usually require multiple command line flags to pass information such as the server location, credentials, throttling information and thresholds, etc.

`Vitess`, as a framework, manages many aspects of the migration:

* **Scheduling** — Vitess schedules migrations to run, identifies migrations that conflict with each other, moves migration to the next state, etc.
* **Discovery** — Vitess automatically knows where the migration should execute.
* **Credentials** — Vitess can create and destroy credentials per migration (and does so for managed `gh-ost` and `pt-osc`), or use its own internal credentials for the task.
* **Throttling** — built in Vitess, and again based on internal discovery.
* **Cleanup** — safely and timely removing the migration's artifacts.
* **History** — tracking all present and past migrations.

## Resumable after failure vs. not

`gh-ost`, `fb-osc`, `pt-osc` are single process tools and do not keep track of their own progress outside the process' scope. If the tools break for any reason, or killed by the operating system, the migration is lost. They cannot be resumed from point of interruption. Someone will then also need to clean up the artifact tables. With `pt-osc` and `fb-osc`, there is furthermore the issue of dropping the leftover triggers.

Moreover, they are bound to the MySQL primary the migration was issued on. If the primary fails, the migration is lost. Even if, for example, a replica is promoted as a new primary and the MySQL cluster resumes operation, it is impossible to resume the migration from point of breakage.

[Vitess migrations survive](https://vitess.io/docs/user-guides/schema-changes/recoverable-migrations/) both failure of the Vitess process as well as failure of the MySQL server. This is the result of Vitess keeping track of the migrations' progress in helper tables, transactionally committed along with corresponding range and GTID values. Upon failure of either MySQL or Vitess itself, a new Vitess/VTTablet process is able to pick up the migration from the exact place where it left off. This works even if a new MySQL replica/server is promoted as primary. Vitess resumes the flow automatically and the user does not need to take action.

## Revertible after completion vs. not

Some migrations turn bad. Perhaps dropping the wrong column or the wrong index. This is why the cut-over is the most critical part of the migration. In such situations, the operator may want to revert the migration, i.e. restore the table to its original format.

Once a migration is complete, `pt-osc`, `fb-osc` and `gh-ost` exit. What information they had about the migration or the table's structure is lost. Since they all have no sense of migration management, there is no functionality to assist with reverting the migration. The user must contrive a new `ALTER TABLE` statement and run a new migration, or alternatively find what the original table was renamed to, and rename it back, losing some data accumulated since migration completed.

Vitess migrations are [revertible](https://vitess.io/docs/user-guides/schema-changes/revertible-migrations/). Vitess is able to restore the table onto its original schema, without losing accumulated data. Furthermore, it requires no additional information from the user. It provides a `REVERT VITESS_MIGRATION <uuid>` SQL interface, which creates a new migration. That migration is expected to run quickly, as it does not need to copy table data. It only needs to hook onto the binary log stream from the point of previous migration termination and apply data from that point on.

## Declarative vs. imperative

`pt-osc`, `fb-osc` and `gh-ost` require an `ALTER TABLE` statement. This statement is pre-evaluated by the user (or an ORM) and must be valid. The tools have no notion of the purpose nor semantics of the change.

`Vitess` likewise supports `ALTER TABLE` statements. In addition, it also supports a declarative approach. In this approach, the user issues a `CREATE TABLE` to indicate what the table *should look like*. When the migration is scheduled to run, `Vitess` compares existing schema with desired schema, computes the required `ALTER TABLE` and runs the migration. Possibly the table is already in the desired format and no changes are needed. Possibly the table does not exist in the first place and needs to be created. `Vitess` will change the schema as needed to get to the desired state. [Declarative migrations](https://vitess.io/docs/user-guides/schema-changes/declarative-migrations/) are available via the `-declarative` strategy flag.

## Conclusion

As you can see, there are several crucial differences when it comes to the implementation of Online Schema Change tools. Hopefully this guide gave you insight into the reasoning behind the design choices of the tools we reviewed.

If you're interested in learning more about Vitess schema changes, be sure to check out the [Vitess documentation](https://vitess.io/docs/user-guides/schema-changes/).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Online DDL change unique keys
Source: https://planetscale.com/docs/vitess/schema-changes/onlineddl-change-unique-keys

It is possible to modify or replace a table's `PRIMARY KEY`, or any other `UNIQUE KEY`s according to the limitation described below, followed by examples.

## Overview

To migrate data safely and [without downtime](/docs/vitess/schema-changes), PlanetScale requires that all tables have a unique, not-null key. Note that a `PRIMARY KEY` satisfies this condition, and it is generally recommended to always have a `PRIMARY KEY` on all tables.

When you modify a table, both the old and the new schema must have a unique key as described, and the columns covered by those keys must exist in both the old and the new schema.

Essentially this makes it possible for PlanetScale to unambiguously identify and correlate a row between the two schemas.

If you attempt to deploy a schema change which does not comply with the above restriction, the deploy request will fail with the error `Table ... has no shared columns covered by non-null unique keys between both branches.`.

## Examples: allowed changes

In our examples, we assume the base schema to be:

```sql  theme={null}
CREATE TABLE `users` (
	`id` int,
	`other_info` int,
	`username` varchar(128),
	`email` varchar(128),
	PRIMARY KEY (`id`)
);
```

The following are all valid changes to the schema:

### Expanding the PRIMARY KEY

```sql  theme={null}
CREATE TABLE `users` (
	`id` int,
	`other_info` int,
	`username` varchar(128),
	`email` varchar(128),
	PRIMARY KEY (`id`, `other_info`)
);
```

In the above we modified the `PRIMARY KEY` to include `other_info`. This is allowed since both `id` and `other_info` columns exist in both the old and the new schema.

### Moving PRIMARY KEY to a different column

```sql  theme={null}
CREATE TABLE `users` (
	`id` int,
	`other_info` int,
	`username` varchar(128),
	`email` varchar(128),
	PRIMARY KEY (`email`)
);
```

Since both `id` and `email` columns exist in both old and new schema, the deploy request will be allowed. The success of the operation depends on whether `email` actually contains unique values. If there's duplication in `email` values, the deployment will fail with error.

### Moving PRIMARY KEY to different columns

Likewise, there is no problem if the new `PRIMARY KEY` covers multiple columns. Again, the success of the operation depends on the actual uniqueness of the combination of columns.

```sql  theme={null}
CREATE TABLE `users` (
	`id` int,
	`other_info` int,
	`username` varchar(128),
	`email` varchar(128),
	PRIMARY KEY (`username`, `other_info`)
);
```

### Changing PRIMARY KEY and adding/removing other UNIQUE KEYs

```sql  theme={null}
CREATE TABLE `users` (
	`id` int,
	`other_info` int,
	`username` varchar(128),
	`email` varchar(128),
	PRIMARY KEY (`username`, `other_info`),
	UNIQUE KEY `email` (`email`)
);
```

## Examples: invalid changes

Consider the next scenarios and the ways to work around them:

### Changing a PRIMARY KEY to include a new column

```sql  theme={null}
CREATE TABLE `users` (
	`id` int,
	`other_info` int,
	`username` varchar(128),
	`email` varchar(128),
	`new_info` int,
	PRIMARY KEY (`username`, `new_info`)
);
```

This is an invalid change because in the new schema, the `PRIMARY KEY` covers the `new_info` column. But this column does not exist in the old schema.

Consider splitting into two distinct schema changes and deploy requests:

1. First, introduce the `new_info` column.
2. Next, change the `PRIMARY KEY`.

### Changing a PRIMARY KEY and also dropping the old covered column

```sql  theme={null}
CREATE TABLE `users` (
	`other_info` int,
	`username` varchar(128),
	`email` varchar(128),
	PRIMARY KEY (`email`)
);
```

The above is invalid because `id` column, covered by the `PRIMARY KEY` in the old schema, does not exist in the new schema.

Again, consider splitting into two distinct changes:

1. First, change the `PRIMARY KEY`.
2. Next, drop the `id` column.

## Summary

We've seen how, in many scenarios, it's straightforward to modify your table's `PRIMARY KEY` or other keys. For some scenarios, it might take two or more steps to achieve the new schema.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Safe migrations
Source: https://planetscale.com/docs/vitess/schema-changes/safe-migrations

Safe migrations is an optional but highly recommended feature for branches in PlanetScale. With safe migrations enabled on a branch, you’ll gain zero-downtime schema migrations, schema reverts, and protection against accidental schema changes. The safe migrations setting is recommended for all production database branches to prevent downtime and unintentional data loss during schema migrations.

## Zero-downtime schema migrations

Safe migrations enable the [PlanetScale workflow](/docs/vitess/best-practices) on a given branch and allow your team to create deploy requests to merge schema changes into that branch. When changes are merged using deploy requests, a ghost table will be created with the desired schema changes. Your data will be continuously synchronized with that table until you decide to apply the changes.

## Schema revert

Safe migrations and deploy requests provide the option to quickly [revert schema changes](/docs/vitess/schema-changes/deploy-requests#revert-a-schema-change) if you discover that they are not compatible with your application. With schema revert enabled for a database, the old table is retained. You’re provided a 30-minute window where data will still be synchronized as writes occur on the new table. If you decide to revert your changes, the status of the two tables is flipped, bringing the former table back.

## Protection against accidental schema changes

To prevent accidental changes to the database schema, which may cause downtime, safe migrations enforce the use of [branching](/docs/vitess/schema-changes/branching) and [deploy requests](/docs/vitess/schema-changes/deploy-requests). This requires that changes be made safely and allows all team members to check and comment on schema changes before they are applied.

With safe migrations enabled, Data Definition Language (DDL) statements issued to branches with safe migrations enabled will automatically be rejected by PlanetScale. Any `CREATE`, `ALTER`, or `DELETE` commands, whether sent using the PlanetScale built-in console, terminal, or MySQL GUI, will fail when we receive them.

## Staging branches

You can use a development branch with safe migrations enabled to set up a workflow with a “staging” branch. First, make sure you have safe migration enabled for your main production branch. Then, create a “staging” branch with your main production branch as the base and turn on safe migrations. All new branches created for development can use this “staging” branch as the base branch.

You can then open a deploy request against either the main production or “staging” branch. Once it is deployed to “staging,” you can open a deploy request against the main production branch. The main production branch must be set as the default branch (found in your database's “Settings” page) to open a deploy request against it.

<Note>
  In this setup, the “staging” branch is still a development branch. Compared to your production branch, it will have reduced resources, similar to other development branches.
</Note>

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/safe-migrations/branches-with-staging-branch.jpg?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=20e51f36f0f5bba30a6144fcb88da38b" alt="View of the Branches tab with main <- staging <- dev branches" data-og-width="2666" width="2666" data-og-height="1068" height="1068" data-path="docs/images/assets/docs/concepts/safe-migrations/branches-with-staging-branch.jpg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/safe-migrations/branches-with-staging-branch.jpg?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=fec815a5dc354f6a1f5e92d6cafffc50 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/safe-migrations/branches-with-staging-branch.jpg?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=f8bfda07b953fea00bf770d037f5bb63 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/safe-migrations/branches-with-staging-branch.jpg?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=0ddf30c62b8c83e80d42fb442977be85 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/safe-migrations/branches-with-staging-branch.jpg?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=e366cff1aea7aefe20dc3411be5d5cd2 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/safe-migrations/branches-with-staging-branch.jpg?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=2970428b8a650341f2a9a93cf8c8e0eb 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/safe-migrations/branches-with-staging-branch.jpg?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=002f8be68218bc4cbbb4bfad6dcc6b98 2500w" />
</Frame>

## How to enable safe migrations

Safe migrations can be enabled using the PlanetScale dashboard or the pscale CLI.

### Using the PlanetScale dashboard

To enable safe migrations on a branch, select the branch you want to modify from the branch dropdown and click the **”cog”** in the upper right of the infrastructure card on the ”**Dashboard**” tab of the database.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/safe-migrations/production-branch-card-with-sm-disabled-2.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=d77aff926810f9aced640b9042f2825a" alt="The production branch UI card." data-og-width="1096" width="1096" data-og-height="801" height="801" data-path="docs/images/assets/docs/concepts/safe-migrations/production-branch-card-with-sm-disabled-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/safe-migrations/production-branch-card-with-sm-disabled-2.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=a70e2e792c9be9d9910b72f60dba486e 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/safe-migrations/production-branch-card-with-sm-disabled-2.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=dc711c27836592b1c1383b81991237e9 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/safe-migrations/production-branch-card-with-sm-disabled-2.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=a378c5c5f3d96cab07b9f6d8d3574ac6 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/safe-migrations/production-branch-card-with-sm-disabled-2.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=69937667ea353b7fd6592ef34cca3f0b 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/safe-migrations/production-branch-card-with-sm-disabled-2.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=831545989868b18c8a9f7677d79d6fd9 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/safe-migrations/production-branch-card-with-sm-disabled-2.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=badcd818b18072b14bc1585c56a79054 2500w" />
</Frame>

In the modal, toggle the option labeled **”Enable safe migrations”**, then click the **”Enable safe migrations”** button to save and close the modal.

The UI card will reflect the status of the safe migrations for that branch.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/safe-migrations/production-branch-card-with-sm-enabled-2.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=e0972c621c9b01378df27a9dde76bc10" alt="Branch UI card with safe migrations enabled." data-og-width="1074" width="1074" data-og-height="792" height="792" data-path="docs/images/assets/docs/concepts/safe-migrations/production-branch-card-with-sm-enabled-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/safe-migrations/production-branch-card-with-sm-enabled-2.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=7458fc89a527d68a5f46b11de6734cb5 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/safe-migrations/production-branch-card-with-sm-enabled-2.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=08e502c38d2fdd07f8836d526c1810cb 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/safe-migrations/production-branch-card-with-sm-enabled-2.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=d45279dc3f7f2534ee71e24db6806eb9 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/safe-migrations/production-branch-card-with-sm-enabled-2.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=7e315c47cc1df73f3a291230b8e419a8 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/safe-migrations/production-branch-card-with-sm-enabled-2.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=f911c9e3abc2eeae5a115bb691759799 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/safe-migrations/production-branch-card-with-sm-enabled-2.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=5251d91ac8bcea5dede883f99806974e 2500w" />
</Frame>

You can also access the same settings from the **”cog”** on a branch overview page (from the **”Branches”** tab, then select the branch you want to view or modify).

### Using the pscale CLI

To enable safe migrations on a branch using the pscale CLI, use the following command in your terminal:

```
pscale branch safe-migrations enable <DATABASE\_NAME> <BRANCH\_NAME>
```

## How to disable safe migrations

There are two ways to disable safe migrations: the PlanetScale dashboard and the CLI.

### Using the PlanetScale dashboard

To disable safe migrations, click the **”cog”** in the upper right of the infrastructure card on the ”**Dashboard**” tab of the database.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/safe-migrations/prod-card-cog-2.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=472cb2e74a1c1a77bcf63927ee18270c" alt="Branch UI with enabled with cog highlighted." data-og-width="1074" width="1074" data-og-height="792" height="792" data-path="docs/images/assets/docs/concepts/safe-migrations/prod-card-cog-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/safe-migrations/prod-card-cog-2.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=bdda7a5ef3db38524a81aabb83b8dbf5 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/safe-migrations/prod-card-cog-2.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=4c24a90a6fa11d4a3d887eb89adf1610 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/safe-migrations/prod-card-cog-2.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=3edcf9a13deb889fc8b3627cff152989 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/safe-migrations/prod-card-cog-2.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=34c964ac1e63f836a2e01dfe1b48d10e 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/safe-migrations/prod-card-cog-2.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=68d659f197d3b3ca5d909babbaa3df5a 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/safe-migrations/prod-card-cog-2.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=70c0a8af2f4dc13f6a3183dc52229936 2500w" />
</Frame>

In the modal, toggle the option labeled **”Enable safe migrations,”** then click the **”Disable safe migrations”** button to save and close the modal.

You can also access the same settings from the **”cog”** on a branch overview page (from the **”Branches”** tab, then select the branch you want to view or modify).

### Using the pscale CLI

To disable safe migrations on a branch using the pscale CLI, use the following command in your terminal:

```
pscale branch safe-migrations disable <DATABASE\_NAME> <BRANCH\_NAME>
```

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Throttling deploy requests
Source: https://planetscale.com/docs/vitess/schema-changes/throttling-deploy-requests



## Overview

Deploy requests, though non-blocking, do of course consume some resources as the online schema change process occurs. Normally, you don't need to get involved, as the [Vitess tablet throttler](https://vitess.io/docs/api/reference/features/tablet-throttler/) automatically identifies when replication lag is high on your database and [slows down migration progress](https://vitess.io/docs/user-guides/schema-changes/audit-and-control/#controlling-throttling).

For long-running schema changes that take several minutes or hours, you may wish to increase throttling on your deploy request to mitigate load on your database.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/schema-changes/throttling-deploy-requests/deploy-request-throttling.png?fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=0ad57abb86183b8e3d13a484f83274c3" alt="Deploy request throttling settings page" data-og-width="2080" width="2080" data-og-height="1416" height="1416" data-path="docs/vitess/schema-changes/throttling-deploy-requests/deploy-request-throttling.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/schema-changes/throttling-deploy-requests/deploy-request-throttling.png?w=280&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=53aa1302878ae46d4d0c69aa45a94fba 280w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/schema-changes/throttling-deploy-requests/deploy-request-throttling.png?w=560&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=94d2fb142cf5e8754bdb1ae418f1f521 560w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/schema-changes/throttling-deploy-requests/deploy-request-throttling.png?w=840&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=94d09d57af01799b2a164606a64d7874 840w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/schema-changes/throttling-deploy-requests/deploy-request-throttling.png?w=1100&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=6528d3ce29002c1246dedc83d233aabc 1100w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/schema-changes/throttling-deploy-requests/deploy-request-throttling.png?w=1650&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=26b9344d2d254ee80bb51303953d5981 1650w, https://mintcdn.com/planetscale-cad1a68a/PORMOvRUq48-lind/docs/vitess/schema-changes/throttling-deploy-requests/deploy-request-throttling.png?w=2500&fit=max&auto=format&n=PORMOvRUq48-lind&q=85&s=5a5093c57ddcd7396c3f7abb78ff65f4 2500w" />
</Frame>

## How it works

The throttle setting allows you to adjust the underlying Vitess tablet throttler for deploy requests. The throttle setting is a value between 0 and 95, where 0 means throttling is effectively disabled, and 95 is nearly fully throttled, which will drastically slow down migrations. 95 is currently the max throttle setting available.

For example, if you set the throttle value to 25, deploy requests will run at about 75% of their normal speed. The lower the number, the less throttling.

## Managing throttler settings

There are three ways to adjust the throttler for deploy requests:

* At the database level
* At the keyspace level
* At the deploy request level

You must be an [Organization Administrator](/docs/security/access-control#organization-administrator) or [Database Administrator](/docs/security/access-control#database-administrator) for the database that you're configuring in order to change these settings at the database level.

However, any organization member who has permission to deploy a deploy request is able to adjust the configuration at the deploy request level.

### Managing throttler settings at the database or keyspace level

For the first two options, first select your database, click "Settings" in the left nav, and scroll down to "Advanced settings". Here, you'll see the options to configure deploy request throttling.

You can adjust the slider or manually type in the value you'd like to set throttling to.

If you'd like to set throttling at the [keyspace](/docs/vitess/sharding/keyspaces) level so that each keyspace is throttled differently, check "Set throttling per keyspace". This will give you the option to individually adjust the sliders for each keyspace.

Click "Save throttling settings". Going forward, these throttler settings will apply to every deploy request as configured.

### Managing throttler settings per deploy request

You also have the option to manually configure throttler settings on each deploy request.

Once you click "Deploy changes" on a deploy request, click the arrow next to "throttling" to show the settings to adjust the throttle value for that deploy request. From here, you'll see the same options to adjust the value for each keyspace or for all keyspaces.

Changes to throttler configuration on the deploy request page will only apply to that individual deploy request and will not affect the database level throttler configurations.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Connection strings
Source: https://planetscale.com/docs/vitess/security/connection-strings



## Creating a password

<Steps>
  <Step>
    To create a password, head to your database dashboard page at `https://app.planetscale.com/<ORGANIZATION>/<DATABASE_NAME>` and click on the "**Connect**" button.
  </Step>

  <Step>
    On the **Connect page**, select the branch you wanted to create a password for, pick a [password role](/docs/vitess/security/password-roles), and provide a recognizable name for the new credentials. Clicking `Create password` will then generate a **unique username and password pair** that can only be used to access the designated branch of your database. Take note of this password, as you won't be able to see it again.
  </Step>

  <Step>
    Once created, you can browse the connection string in different framework formats by selecting framework in the "Select your language or framework" section. This will also show you all of the files you need to modify to get connected with PlanetScale in your framework or language of choice.
  </Step>
</Steps>

<Note>
  There are two connection types for a password: `Primary` and `Replica`. The `Primary` connection type is used to connect to the primary region of your database, while the `Replica` connection type is used to route queries to your branch's replicas and read-only regions. You can create multiple passwords for a branch, each with a different connection type. [Read more about replicas](/docs/vitess/scaling/replicas).
</Note>

<Tip>
  Make sure you copy the credentials for your application and the "Other" format. We do not save the password in plaintext, so there will be no way to retrieve the password after you leave this page.
</Tip>

## Managing passwords

Once you've created the password, you can head over to the "**Passwords**" settings page available at `Organization > Database > Settings > Passwords` to manage them.

<Tip>
  You can also create passwords for branches other than `main` on this page.
</Tip>

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/connection-strings/manage-2.png?fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=07f620508e2a9ebd153dc80a3dcf7629" alt="Manage passwords page" data-og-width="1761" width="1761" data-og-height="604" height="604" data-path="docs/images/assets/docs/concepts/connection-strings/manage-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/connection-strings/manage-2.png?w=280&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=33d63bc6fe96ee960fac37e93b7e0853 280w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/connection-strings/manage-2.png?w=560&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=24cf5aaab28af798045f57cfbf2c7cbb 560w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/connection-strings/manage-2.png?w=840&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=6ff1f113234e375d4fc75f00839bcd04 840w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/connection-strings/manage-2.png?w=1100&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=e0279fb5a9bf953bf266f70e336d59c3 1100w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/connection-strings/manage-2.png?w=1650&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=c2cc871b1f3425d99314ecffe6d83b5a 1650w, https://mintcdn.com/planetscale-cad1a68a/1n39MWo25_njbahn/docs/images/assets/docs/concepts/connection-strings/manage-2.png?w=2500&fit=max&auto=format&n=1n39MWo25_njbahn&q=85&s=8216abe43a4fe2abffa93a3909f1a0d9 2500w" />
</Frame>

Clicking on the `...` icon on the row for your password allows you rename or delete the password.

## Renaming a password

Since the **username & password** pair is unique, the only metadata you can edit is the `display name` of the password.

## Deleting a password

Deleting a password will invalidate the username & password pair and **disconnect any active clients using this password**.

<Note>
  Any open database connections authenticated with a deleted password will be disconnected within five minutes.
</Note>

## Native MySQL authentication support

Use the tools you're familiar with to connect to PlanetScale databases.
PlanetScale supports both [MySQL native authentication](https://dev.mysql.com/doc/refman/8.0/en/native-pluggable-authentication.html), which is widely used to provide a secure connection to MySQL servers,
and [MySQL Caching SHA-2 authentication](https://dev.mysql.com/doc/refman/8.0/en/caching-sha2-pluggable-authentication.html), which is the most secure authentication mechanism to connect to MySQL.
Based on your application needs and platform support, you can switch between the authentication modes, with the same password.

For a list of tested MySQL GUI clients, review our article on [how to connect MySQL GUI applications](/docs/vitess/tutorials/connect-mysql-gui).

## Strong security model

PlanetScale Passwords are created for use with a single database branch.
This strong security model allows you to generate passwords that are tied to a branch, and cannot access data/schema from another branch.

## IP restrictions

You can restrict database connections to specific IP ranges for a single password by updating its IP restrictions. For example, if you have a database for a web application and you create a password for use in the deployed application, you can restrict usage of that specific password to the IP ranges of the deployed application. If somebody attempts to connect to the database from outside of the deployed application, the connection will be refused. IP restrictions work on a per-password basis, so if you want to use the same restriction across passwords, they must be applied to each password separately.

Some passwords are incompatible with IP restrictions, and you will need to create a new password to use IP restrictions.

Examples of when you may want to use IP restrictions:

* You want to segment database access so that the production database can only be connected to from production environments or development branches.
* You use a bastion in production and want to ensure that all database connections originate or pass through the bastion.
* You want to allow a single client to be able to access your database (e.g., for debugging) and want to provide the least amount of access for them to do so.
* You have compliance requirements that require implementing a more stringent access control list in your database.

### Updating the IP restrictions for a password

1. Go to your database's "**Settings**" tab.
2. Click "**Passwords**."
3. You can update the IP restrictions for a password in two different ways: The first way is by opening the dropdown menu to the right of any password on the Passwords page and clicking "**Manage IP restrictions**." The second way is by clicking on the password and scrolling to the bottom of its page to update IP restrictions.
4. Add the IP ranges that you want to allow to connect using the selected password.

<Note>
  If your password has no IP restrictions, it is set to **allow all traffic**. Similarly, when you add a new IP range to the restrictions, all IP addresses out of this range cannot connect to your database using that password.
</Note>

## Disconnect clients by deleting passwords

PlanetScale automatically disconnects clients that are using a deleted password.
Head on over to the `Organization > Database > Settings > Passwords` page on your database branch to delete passwords for that branch.
It may take up to five minutes for all active clients to be disconnected.

## No plain text password storage

PlanetScale only stores hashes and metadata about your database passwords.
To add an extra layer of security to your database, we do not store any passwords in plaintext.

<Note>
  In the event that you lose a password, we cannot recover it for you. We recommend creating a new password with the
  same access level.
</Note>

## GitHub Secret Scanning integration

All passwords and service tokens generated for use with PlanetScale databases are part of [GitHub's Secret Scanning](https://docs.github.com/en/code-security/secret-security/about-secret-scanning) program. If any database passwords or service tokens are committed in plaintext to any public GitHub repository, we will be notified and take corrective action to delete the access tokens and cut off their access.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Password roles
Source: https://planetscale.com/docs/vitess/security/password-roles

PlanetScale allows you to [create and manage passwords](/docs/vitess/connecting/connection-strings) for each branch of your database.

## Overview

PlanetScale passwords can be created with one of four roles:

* **Read-only** — Can query rows
* **Write-only** — Can modify rows
* **Read/Write** — Can query and modify rows
* **Admin** — All read/write permissions and can modify schema\*

\* *This does not apply to production branches with [safe migrations](/docs/vitess/schema-changes/safe-migrations) enabled, as we [do not allow direct DDL](/docs/vitess/schema-changes/how-online-schema-change-tools-work) on those branches, even if your password has the `Admin` role.*

## Create a password with custom role

<Steps>
  <Step>
    Go to your database settings page.
  </Step>

  <Step>
    Click "**Passwords**" > "**New password**".
  </Step>

  <Step>
    Give it a name, select the role from the dropdown, select the branch, and click "**Generate password**".

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/password-roles/roles.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=2baaa80270578f1e6206191646adf4bf" alt="PlanetScale password roles priority" data-og-width="970" width="970" data-og-height="756" height="756" data-path="docs/images/assets/docs/concepts/password-roles/roles.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/password-roles/roles.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=40430875893a8fc58fd82c9f13255e07 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/password-roles/roles.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=8e8750f0d041aa0e593fa68ab2b5d9f0 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/password-roles/roles.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=1f040900eb136963309a2106393373f9 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/password-roles/roles.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=77a64d0174a81f19e335cb3742a77649 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/password-roles/roles.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=68bb427aaddb8e3a1909832c657dc015 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/password-roles/roles.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=dde9a198d4570e2c5fa89f73288decf1 2500w" />
    </Frame>
  </Step>
</Steps>

Once a password is created, **its role cannot be changed**.

The access level available to these roles is shown in the table below.

| Role name  | Can create/edit schema             | Can insert/update/delete rows      | Can query rows                     |
| :--------- | :--------------------------------- | :--------------------------------- | :--------------------------------- |
| Read-only  | <Icon icon="xmark" color="red" />  | <Icon icon="xmark" color="red" />  | <Icon icon="check" color="blue" /> |
| Write-only | <Icon icon="xmark" color="red" />  | <Icon icon="check" color="blue" /> | <Icon icon="xmark" color="red" />  |
| Read/write | <Icon icon="xmark" color="red" />  | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |
| Admin      | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> | <Icon icon="check" color="blue" /> |

<Note>
  The default role for all passwords created by the **Connect** button is `Administrator`. Passwords with custom roles
  must be created from your database settings page.
</Note>

## Troubleshooting

The following errors indicate that you do not have the permissions needed to perform an action. You must create a new password with a more privileged role to proceed.

**SELECT DENIED**

`Select command denied to user ‘planetscale-writer-only for table ‘customers’ (ACL check error) (CallerID: planetscale-writer-only)`

**INSERT DENIED**

`Insert command denied to user ‘planetscale-reader’ for table ‘customers’ (ACL check error) (CallerID: planetscale-reader)`

**DELETE DENIED**

`Delete command denied to user ‘planetscale-reader’ for table ‘customers’ (ACL check error) (CallerID: planetscale-reader)`

**DDL DENIED**

`DDL command denied to user ‘planetscale-writer' for table my-new-table’ (ACL check error) (CallerID: planetscale-writer)`

<Note>
  If your pscale CLI version is less than 0.94.0, please upgrade your installation by following [this
  document](/docs/cli/planetscale-environment-setup)
</Note>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Sharding with PlanetScale
Source: https://planetscale.com/docs/vitess/sharding



<Note>
  You can create sharded keyspaces on any plan by adding a new sharded keyspace using the [Clusters page](/docs/vitess/cluster-configuration) and running an [unsharded to sharded workflow](/docs/vitess/sharding/sharding-quickstart) in your dashboard.

  If you would like additional support from our expert team, our [Enterprise plan](/docs/planetscale-plans#planetscale-enterprise-plan) may be a good fit. [Get in touch](https://planetscale.com/contact) for a quick assessment.
</Note>

## Sharding with PlanetScale

PlanetScale allows you to break up a large monolithic database and spread the data out across multiple servers.
This [reduces the load on a single database](https://planetscale.com/blog/one-million-queries-per-second-with-mysql) by distributing it across many.
PlanetScale achieves this by building our product on top of [Vitess](https://vitess.io/), which provides a transparent [sharding solution](/docs/vitess/sharding) for MySQL databases.

### Sharding without application changes

Sharding is a proven database architecture used by many organizations to help them scale up when database demand grows.
When reaching this point, many organizations choose to scale and shard their database manually.
This typically involves adding a bunch of sharding-specific logic to the application layer and/or creating a whole new proxy component to manage the shards.
In addition to this, managing a large fleet of disparate database servers is a significant operational challenge.

When using PlanetScale, customers can keep sharding logic out of their applications and let us take the burden of infrastructure management.
This is because Vitess allows you to create sharded databases and have them appear to the application layer as a single, unified database.
This means simpler application architecture, allowing your developers to worry less about the database and more on their work.

PlanetScale abstracts away all of this complexity using the **VTGate** layer of Vitess.
The VTGates act as the entryway into a Vitess database cluster.
They handle incoming connections and route queries to the appropriate MySQL instances.

When you're at the point where you've maxed out your vertical scaling efforts and you know you need to shard your database, PlanetScale allows you to do so *without* the burden of rearchitecting your entire application.

## How does our sharding process work?

When it comes time to shard your database, we recommend following our [sharding quickstart](/docs/vitess/sharding/sharding-quickstart) guide. If you need assistance identifying the best [sharding scheme](https://vitess.io/docs/api/reference/features/sharding/#sharding-scheme) for your database, or are interested in expert-level support, our [Enterprise plan](https://planetscale.com/enterprise) may be a better option for you. [Get in touch](https://planetscale.com/contact) for more information.

PlanetScale uses an explicit sharding system.
This means that, if you are going to horizontally shard your data, we have to tell Vitess which sharding strategy to use for each sharded table.
This involves some conversations to understand what your application does and the size, schema, and queries of your database.
Once we have this complete view of your application, our next objective is to identify a sharding key.

The sharding key, or [Primary Vindex](https://vitess.io/docs/api/reference/features/vindexes) is what determines how the rows of your sharded tables will be distributed across servers.
For each table you want to shard, we must choose which column to use for the Vindex, and what [type of vindex](https://vitess.io/docs/api/reference/features/vindexes/#predefined-vindexes) to use with it.
Each shard will cover a range of keyspace ID values, so this mapping is used to identify which shard a row is in.

To determine a Primary Vindex, our team will analyze your schema and query patterns. We generally will ask you for the following information:

1. A copy of your schema.
2. Some indication of the size of each table in your database. We can typically gather this information by looking at `AUTO_INCREMENT` values, but may require additional context in some cases.
3. Information about your common query patterns — typically your most frequently used 50-100 queries.

Using this holistic view of your database, we can work to determine a good candidate for the Primary Vindex. During this analysis, we also begin to determine which tables should be sharded, whether you'll require secondary Vindexes ([Lookup Vindexes](https://vitess.io/docs/api/reference/features/vindexes/#functional-and-lookup-vindex)), the strategy for tables that don't contain our chosen Primary Vindex, and more.

While no sharding strategy can ever optimize for *all* query patterns, this deep analysis makes the strategy as efficient as possible for the majority of queries, especially the most frequently used queries.

And, again, this is all done without requiring you to rearchitect your application.

## When should you consider sharding a table

We generally recommend sharding when you're running into issues with:

* Application performance due to **data size** (this can vary widely, but when your database exceeds 250 GB or a particular table becomes especially large, sharding may be something to explore)
* Hitting vertical limits with **write throughput**
* Hitting replica limits with **read throughput**

We often see customers running into other infrastructure challenges before they really see their application performance impacted by large amounts of data. As your data grows, you may find your backups are becoming unreliable and time-consuming. Or you may have no way to test those large backups.

If it feels like some things are starting to break down in your infrastructure, but you aren't sure if sharding is the solution, we can still help you identify if it's the correct strategy for scaling your database.

If you would like to explore whether or not sharding is the right solution for you, [don't hesitate to reach out](https://planetscale.com/contact), and we'll be in touch.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Avoiding cross-shard queries
Source: https://planetscale.com/docs/vitess/sharding/avoiding-cross-shard-queries



export const YouTubeEmbed = ({id, title}) => {
  return <Frame>
      <iframe src={`https://www.youtube-nocookie.com/embed/${id}?rel=0`} title={title} className="aspect-video w-full" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" />
    </Frame>;
};

When designing your database sharding scheme, it's important to think about your common query patterns with the goal of avoiding cross-shard queries.

You might start this exercise by deciding which table(s) you wish to shard. You probably already have some good candidates in mind which you know need to be sharded. Once you have these base table(s) down that you need to shard, the next step is to think about what tables you frequently `JOIN` with these sharded tables.

Let's run through an example. If you prefer video content, you can watch the video on avoiding cross-shard queries here:

<YouTubeEmbed id="4gzv5YP0D9A" title="Avoiding cross-shard queries" />

## Sharding design example

I have an unsharded database, `metal`, in a single [keyspace](/docs/vitess/sharding/keyspaces). This is the database for our gym tracker application. The database schema looks like this:

**`users` table:**

Stores the users that sign up to track their gym sessions. There are currently 200,000 users.

```
-- users
+--------------------+-----------------+------+-----+---------+----------------+
| Field              | Type            | Null | Key | Default | Extra          |
+--------------------+-----------------+------+-----+---------+----------------+
| id                 | bigint unsigned | NO	  | PRI	| NULL	  | auto\_increment |
| name	             | varchar(255)	   | NO	  |     | NULL    |                |
| email	             | varchar(255)	   | NO   |     |         |                |
| created_at         | datetime(6)	   | NO	  |     | NULL    |                |
| encrypted_password | varchar(255)	   | NO   |     |         |                |
| username	         | varchar(255)	   | NO	  |     | NULL    |                |
+--------------------+-----------------+------+-----+---------+----------------+
```

**`exercises` table:**

Stores all of the exercises that our users can track. This table is managed by the application owners. Users cannot add their own exercises. There are currently 200 exercises in the database.

```
-- exercises
+-------------+-----------------+------+------+---------+----------------+
| Field       | Type            | Null | Key  | Default | Extra          |
+-------------+-----------------+------+------+---------+----------------+
| id	        | bigint unsigned | NO   | PRI  | NULL	| auto_increment |
| name	      | varchar(255)	| NO   | NULL |         |                |
| created_at  | datetime(6)	    | NO   | NULL |         |                |
| description | varchar(1000)   | YES  | NULL |         |                |
+-------------+-----------------+------+------+---------+----------------+
```

**`exercise_logs` table:**

Stores all of the exercises that a user completes. Each `exercise_log` record holds all of the sets and reps you do for a single exercise at a specific weight. For example, if you do 5 sets of 3 reps of squats at a weight of 225 lbs, that is stored in a single `exercise_log`. If you do some warmup sets of squats at a weight of 135 lbs, that's stored in a new `exercise_log`.

There are currently 65,155,000 exercise logs.

```
-- exercise_logs
+-------------+-------------------+------+-----+---------+----------------+
| Field       | Type              | Null | Key | Default | Extra          |
+-------------+-------------------+------+-----+---------+----------------+
| id	          | bigint	          | NO	 | PRI | NULL	 | auto_increment |
| created_at  | datetime(6)       | NO	 |     | NULL    |                |
| user_id     | bigint unsigned   | NO	 | MUL | NULL    |                |
| exercise_id | bigint unsigned	  | NO	 | MUL | NULL    |                |
| reps        | smallint unsigned | YES	 |     | NULL    |                |
| sets	      | smallint unsigned | YES	 |     | NULL    |                |
| notes       | varchar(1024)	  | YES	 |     | NULL    |                |
| weight      | smallint unsigned | YES   |      | NULL    |                |
+-------------+-------------------+------+-----+---------+----------------+
```

**`programs` table:**

Stores the available pre-created gym programs that a user can optionally choose. These are created by the application owners. A user cannot create their own programs. There are currently 23 programs.

```
-- programs
+---------------+------------------+------+-----+---------+----------------+
| Field         | Type             | Null | Key | Default | Extra          |
+---------------+------------------+------+-----+---------+----------------+
| id	        | bigint unsigned  | NO	  | PRI	| NULL	  | auto_increment |
| weeks         | tinyint	       | YES  |	    | NULL    |                |
| days_per_week	| tinyint unsigned | YES  |	    | NULL    |                |
| name	        | varchar(255)	   | NO	  |	    | NULL    |                |
| description	| varchar(255)	   | YES  |     | NULL    |                |
+---------------+------------------+------+-----+---------+----------------+
```

## Selecting candidate(s) for sharding

Based on the information above, you can see that `exercise_logs` will be the first natural candidate for sharding. Each user adds several `exercise_logs` records every time they track a workout, so this table grows much faster than the other three tables.

We have decided to shard this table to spread the data across two shards/clusters.

## Common query pattern analysis

Now that we've identified the table (or in some cases, tables) we want to shard, the next step is to look at our common query patterns to see if we frequently join this table to other tables. Again, here is the schema for `exercise_logs`:

```
-- exercise_logs
+-------------+-------------------+------+-----+---------+----------------+
| Field       | Type              | Null | Key | Default | Extra          |
+-------------+-------------------+------+-----+---------+----------------+
| id	          | bigint	          | NO	 | PRI | NULL	 | auto_increment |
| created_at  | datetime(6)       | NO	 |     | NULL    |                |
| user_id     | bigint unsigned   | NO	 | MUL | NULL    |                |
| exercise_id | bigint unsigned	  | NO	 | MUL | NULL    |                |
| reps        | smallint unsigned | YES	 |     | NULL    |                |
| sets	      | smallint unsigned | YES	 |     | NULL    |                |
| notes       | varchar(1024)	  | YES	 |     | NULL    |                |
| weight      | smallint	      | YES	 |     | NULL    |                |
+-------------+-------------------+------+-----+---------+----------------+
```

Given that this table has two referential ids: `user_id` and `exercise_id`, it is likely that we often join this table to both `users` and `exercises`. In fact, some of our most executed queries involve joining all of these tables.

The `programs` table, however, is never joined with `exercise_logs`.

Given this information, we now must decide how to architect our sharding scheme such that we can avoid cross-shard queries as often as possible.

## A closer look at our sharded table

Before we dive into the other tables, let's look a little closer at `exercise_logs`. Here is an example of a common query we run to display all of the exercises a user has completed today:

```sql  theme={null}
SELECT exercise_log.*, users.name AS user_name, users.email, exercises.name AS exercise_name
FROM exercise_log
JOIN users ON exercise_log.user_id = users.id
JOIN exercises ON exercise_log.exercise_id = exercises.id
WHERE exercise_log.user_id = 5
  AND DATE(exercise_log.created_at) = CURDATE();
```

This joins both the `exercises` table and the `users` table on `exercise_logs`. For the sake of this example, let's assume this is the most commonly executed query in our application.

Now, we know we are going to shard `exercise_logs`, which means we need to choose a sharding key, or [primary Vindex](/docs/vitess/sharding/vindexes), for this table.

## Choosing a Vindex for `exercise_logs`

Using the primary key as the primary Vindex can sometimes be the most natural choice to shard on. Let's start by considering `id` as the shard key for `exercise_logs`. For every row in the table, the `id` will be hashed with [xxHash64](https://vitess.io/docs/api/reference/features/vindexes/), resulting in some hexadecimal value between `0x00000000000000000` and `0xFFFFFFFFFFFFFFFF`. These values will be evenly distributed across all shards.

With this solution, if we run the above query to get all of the exercises done by a particular user in a given day, we'll have to look across all shards because there's no guarantee that the user's exercise logs all ended up on the same shard.

Instead, we might want to consider sharding on `exercise_logs.user_id`. If we do this, every `exercise_log` record with the same `exercise_logs.user_id` will hash to the same value. So let's say the hashed `user_id` of `2` comes out to `0x30F419900AA88B20`. Every `exercise_log` for our user with `id`=`2` will have a Vindex value of `0x30F419900AA88B20`. When Vitess distributes this data across our 2 shards, the distrubtion might look like this:

**Shard 1**: Vindexes with values `0x0000000000000000` through `0x7FFFFFFFFFFFFFFF` **Shard 2**: Vindexes with values `0x8000000000000000` through `0xFFFFFFFFFFFFFFFF`

In this case, our user whose `id` hashes to `0x30F419900AA88B20` will always end up on Shard 1. This means that ultimately, with this primary Vindex, there will not be a case where a user's `exercise_logs` records live on two different shards. When a request comes in to grab all of the exercise logs for a particular user on a given day, we only have to access a single shard. This is exactly what we want.

## Handling frequently `JOIN`ed tables

With that out of the way, let's again look at how we handle joining the other relevant tables.

With our current setup, we have `exercises`, `users`, and `programs` on the unsharded keyspace and the `exercise_logs` table on the sharded keyspace, as shown below:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/sharding/cross-shard-queries/cross-shard-tables.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=ae86c816e0cb8e93264c05c7bce41b90" alt="Example of cross-shard joins" data-og-width="2598" width="2598" data-og-height="1938" height="1938" data-path="docs/images/assets/docs/sharding/cross-shard-queries/cross-shard-tables.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/sharding/cross-shard-queries/cross-shard-tables.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=57cfb6e5a019fe747eb5aa189f0ae86e 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/sharding/cross-shard-queries/cross-shard-tables.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=136749f8eb68eeb58c4775ea0f96b235 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/sharding/cross-shard-queries/cross-shard-tables.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=e933857d72a63ece1bc85fce5987f8a1 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/sharding/cross-shard-queries/cross-shard-tables.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=fb635b95d9d8740a1549f4be465f3cb1 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/sharding/cross-shard-queries/cross-shard-tables.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=feeca829099a468e12b41136f1a94371 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/sharding/cross-shard-queries/cross-shard-tables.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=f1496a13e0d80f4b155bea42c9c17250 2500w" />
</Frame>

This means that every time we run the above query, we're doing cross-keyspace `JOIN`s. In this case, we'll see a massive hit to performance, and application speed will feel slow to the end user.

Now that we have a good grasp on what we'd like to avoid, let's come up with some solutions. The main thing we need to solve is how to avoid cross-keyspace / cross-shard joins between `exercise_logs`, `users`, and `exercises`.

### The `users` table

Let's start by looking at the `users` table. We already know we're using `exercise_logs.user_id` as the primary Vindex, so all exercise logs for a particular user will end up on the same shard. However, when we join that `user_id` on the `users` table, we have to jump back over to the `metal` keyspace to access the `users` table.

To avoid this, we should move the `users` table to the `metal-sharded` keyspace and shard that as well. We'll need to choose a primary Vindex for `users` in order to shard it. Because we sharded `exercise_logs` on the `user_id`, we now have a great option for the `users` primary vindex: `users.id`. Hashing on `users.id` will guarantee that for every user, both their user record and exercise logs all end up on the same shard.

Our cluster now looks like this:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/sharding/cross-shard-queries/cross-shard-tables-2.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=d44511e15c9b04497c7b88dccf9d128c" alt="Example of cross-shard joins" data-og-width="2598" width="2598" data-og-height="1938" height="1938" data-path="docs/images/assets/docs/sharding/cross-shard-queries/cross-shard-tables-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/sharding/cross-shard-queries/cross-shard-tables-2.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=02b7ca1cfdf69565849b91b942699d17 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/sharding/cross-shard-queries/cross-shard-tables-2.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=079664802a7470a7719bb7459b9305db 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/sharding/cross-shard-queries/cross-shard-tables-2.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=51e2ce78fc4b70a6026e96f478259859 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/sharding/cross-shard-queries/cross-shard-tables-2.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=e55f92010dd7980a395d6ab7edd9cd8b 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/sharding/cross-shard-queries/cross-shard-tables-2.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=3ab38a6874ec0cdb00946fb14ec1604a 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/sharding/cross-shard-queries/cross-shard-tables-2.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=ea05048276e00122f25d149b2ab25996 2500w" />
</Frame>

### The `exercises` table

The final table we need to deal with is the `exercises` table. This is a very small table with only 200 records. Users are not allowed to modify this table, so we have a predictable and slow growth rate with this one. Let's say we expect it to never exceed 1000 records.

We could shard this table, but given that each record here could be associated with any user or any exercise log, we don't have a great path to ensure there won't be any cross-shard queries.

An alternative option in this case is to use a [reference table](https://vitess.io/docs/api/reference/vreplication/reference_tables/) to make a copy of this table on every shard. This way, any time you want to join `exercise_logs` to the `exercises` table, the entire table already exists on the same shard as the exercise log.

Reference tables can be extremely useful in scenarios like this where the table is small and not frequently updated. If, however, this table frequently modified, this could be a poor solution. Every time a record is updated in the table, it must be updated across all shards as well. This is not a problem in our scenario, but keep this tradeoff in mind when choosing to use reference tables.

## A look at our final cluster setup

Here is a recap of what we've chosen for our `metal` database cluster:

* Sharded `exercise_logs` and `users`
* Used `exercise_logs.user_id` as the primary Vindex for `exercise_logs`
* Used `users.id` as the primary Vindex for `users`
* Used a reference table to copy `exercises` to every shard in our `sharded-metal` keyspace

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/sharding/cross-shard-queries/avoiding-cross-shard-joins.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=aebe68235c1f939d6236bf51b726fc33" alt="Example of avoiding cross-shard queries" data-og-width="2598" width="2598" data-og-height="1938" height="1938" data-path="docs/images/assets/docs/sharding/cross-shard-queries/avoiding-cross-shard-joins.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/sharding/cross-shard-queries/avoiding-cross-shard-joins.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=56ce0c7653344ecdf3f81598fb7fe3c4 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/sharding/cross-shard-queries/avoiding-cross-shard-joins.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=54dcb0e52a9e966aa8fce77e25db702a 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/sharding/cross-shard-queries/avoiding-cross-shard-joins.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=ead57ed27999cd5aea876399f7917c43 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/sharding/cross-shard-queries/avoiding-cross-shard-joins.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=8ec42bd8694a214ef28d6e50e5e5c973 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/sharding/cross-shard-queries/avoiding-cross-shard-joins.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=726e7c504b6cda041e3bc3e08d1d2f0e 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/sharding/cross-shard-queries/avoiding-cross-shard-joins.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=0a5c36c47761dd730c008de9b47a2907 2500w" />
</Frame>

With this setup, running our most common query does not involve any cross-keyspace or cross-shard queries:

```sql  theme={null}
SELECT exercise_log.\*, users.name AS user_name, users.email, exercises.name AS exercise_name
FROM exercise_log
JOIN users ON exercise_log.user_id = users.id
JOIN exercises ON exercise_log.exercise_id = exercises.id
WHERE exercise_log.user_id = 5
  AND DATE(exercise_log.created_at) = CURDATE();
```

## What next?

This was a simple example meant to get you thinking about how to design your sharding scheme. You likely have several commonly executed queries. It's of course nearly impossible to optimize for every single query, so what you want to do is optimize for the **most common** queries with the goal of avoiding cross-shard and cross-keyspace queries.

If you're on the PlanetScale Enterprise Support plan, we do some of this query analysis alongside you to come up with the best sharding scheme for your database. You can learn more about that process in our [Proof of concept documentation](/docs/proof-of-concept).

If you'd like more information about our Enterprise Support, don't hesitate to [reach out](https://planetscale.com/contact).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# What is a keyspace
Source: https://planetscale.com/docs/vitess/sharding/keyspaces

In Vitess, a keyspace is a logical database that maps to one or many MySQL instances.

Keyspaces are used to group MySQL instances, typically to shard certain tables in your database cluster. While a keyspace may contain several MySQL instances across many shards, to your application, it will appear as a single database.

All PlanetScale Vitess clusters have one or more keyspaces.

## Unsharded keyspaces

If you only have a single, unsharded keyspace in your database cluster, then your keyspace maps directly to your database cluster in PlanetScale. For example, if you have an unsharded PlanetScale database named `gymtracker` with the default primary and 2 replicas, you likely only have a single keyspace. You can reference this keyspace in the same way you normally would your database:

```sql  theme={null}
use gymtracker;
show tables;
```

Unless your database cluster is sharded or you have created multiple unsharded keyspaces (uncommon), you don't necessarily need to understand the concept of a keyspace. Keyspaces become important once you start to consider sharding.

## Sharded keyspaces

As your database grows, you may wish to shard some tables in your database cluster. To do this, you will create a new keyspace and add the shards to the sharded keyspace.

The following diagram depicts a PlanetScale database with 2 keyspaces: one unsharded and one sharded. The unsharded keyspace has the default 1 primary and 2 replicas. The sharded keyspace contains two shards, each with 1 primary and 2 replicas.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/sharding/keyspace-diagram.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=2a3ec2d6ffbba8c083f30dc24611124b" alt="Keyspace diagram" data-og-width="2598" width="2598" data-og-height="1654" height="1654" data-path="docs/images/assets/docs/sharding/keyspace-diagram.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/sharding/keyspace-diagram.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=b7ef3d83cbe2712c0af9dd1fd350ee84 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/sharding/keyspace-diagram.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=eafd7c7811c4bbcd9e6259e2b620209b 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/sharding/keyspace-diagram.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=5e448bbb3f4716d4646e38e83fee4f69 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/sharding/keyspace-diagram.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=cccdae61f8fce1b859f1c08ce1980322 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/sharding/keyspace-diagram.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=a45237ad8a1b882d3314eab97102fbc7 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/sharding/keyspace-diagram.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=6957a2ca8c128de62f3c040643e51d4d 2500w" />
</Frame>

When a new request comes in, it first goes through our Global Edge Network layer. This Edge layer manages the connection and sends it to the correct Vitess cluster in PlanetScale. From here, the VTGates will parse the incoming query and determine the correct keyspace and shard to route it to.

If you want to directly target a keyspace, you can do that with the `use` syntax, but this time, specify the keyspace name. For example, if we created a sharded keyspace for `gymtracker`, we may call it `gymtracker_sharded`. You can directly target the keyspace with the following:

```sql  theme={null}
use gymtracker_sharded;
show tables;
```

## Routing queries in your application

One of the many perks of Vitess/PlanetScale is that you can distribute your data across hundreds of MySQL instances without complicating your application code. As mentioned earlier, all of these keyspaces and shards appear as a single MySQL instance to your application. Once you add multiple keyspaces, you no longer need to specify a database name in your application's database connection configuration code. Our Global Edge Network will correctly route you to the correct Vitess cluster, and the VTGates in your cluster will handle routing the request to the correct keyspace and shard.

Some frameworks and ORMs require a database name is specified. In these scenarios, you can set the database name to `@primary`, and your requests will be automatically routed to the correct keyspace/shard. Alternatively, if you have specific queries that you wish to send to replicas, you can use `@replica`.

## Modifying keyspaces in PlanetScale

Having 1 unsharded keyspace and 1 sharded is a typical setup for a database that needs sharding. On the [Clusters page](/docs/vitess/cluster-configuration), you are able to customize the number of shards in the sharded keyspace. You can also adjust the instance size for each primary and replica, and you can add additional replicas beyond the default of two if needed.

To get a better sense of this, or to configure your keyspaces, click on your [Clusters](/docs/vitess/cluster-configuration) tab in your dashboard. If you have an existing unsharded database, you'll see that database listed there as an unsharded keyspace. If you click "New keyspace", you're able to configure a brand new keyspace here.

The most common use case for creating a new keyspace is to shard one or multiple tables.

All of your keyspaces are separate databases. And, again, sharded keyspaces hold multiple databases. However, with the power of Vitess, your application views these all as a single database. It uses the VTGate load balancer to route queries to the correct keyspace, and then the correct shard, and finally the correct primary or replica, as configured.

For more information about modifying a keyspace, please see the [Cluster configuration documentation](/docs/vitess/cluster-configuration).

## Limitations

PlanetScale currently supports a maximum of 2048 tables per keyspace. If you exceed this limit, you may want to consider sharding — [distributing your tables across multiple keyspaces](/docs/vitess/sharding).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Pre-sharding checklist
Source: https://planetscale.com/docs/vitess/sharding/pre-sharding-checklist

When you begin a new [unsharded to sharded workflow](/docs/vitess/sharding/sharding-quickstart), there are a number of steps that happen behind the scenes. This document covers some of the pre-sharding work that PlanetScale handles for you.

<Note>
  If you started a workflow while following the [Sharding quickstart](/docs/vitess/sharding/sharding-quickstart) and saw a lot of incomplete steps in the validation phase, you need follow the instructions in this document, and then go back to the quickstart to continue the workflow.

  If you did not get those warnings, you're on a newer Vitess branch, and you do not need to take any action here.
</Note>

## Copy the sharded tables to the new keyspace and remove `AUTO_INCREMENT`

When you begin the workflow, we first copy the schema(s) of the table(s) you wish to shard over to the specified target keyspace. However, there is an intermediate step here: remove any existing `AUTO_INCREMENT`s on the primary key for these table(s).

When a table is spread across multiple shards, using `AUTO_INCREMENT` on your primary key can cause problems. Because each shard is its own separate MySQL instance, the shards do not have the context to know whether or not a primary key for a table entry is already in use on other shards. This means you risk two different table entries being assigned the same primary key.

To avoid this, it is a best practice to use [sequence tables](/docs/vitess/sharding/sequence-tables) instead. We will cover how to set these up shortly. First, let's remove `AUTO_INCREMENT` from the tables you're sharding:

1. Make a copy of the table schema(s) that are moving to this new keyspace. Leave off `AUTO_INCREMENT` if it previously existed. For this example, we'll create the `users` and `notifications` tables that will live on that keyspace.

<Note>
  Besides dropping `AUTO_INCREMENT`, the table schema must match exactly what you have on your original source keyspace. To quickly grab the SQL to create the table, you can go to your "Branches" tab in the dashboard, click your main branch, click the table you need to copy over, and copy the `CREATE TABLE` SQL. Again, make sure to remove `AUTO_INCREMENT`.
</Note>

```sql  theme={null}
CREATE TABLE `users` ( `id` bigint NOT NULL, `name` varchar(255) NOT NULL, `email` varchar(255) NOT NULL, `password` varchar(255) NOT NULL, PRIMARY KEY (`id`), UNIQUE KEY `index_users_on_email` (`email`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;

CREATE TABLE `notifications` ( `id` bigint NOT NULL, `content` varchar(255) NOT NULL, `user_id` bigint NOT NULL, `created_at` datetime(6) NOT NULL, `updated_at` datetime(6) NOT NULL, PRIMARY KEY (`id`), KEY `index_notifications_on_user_id` (`user_id`), KEY `index_notifications_on_user_id_and_created_at` (`user_id`,`created_at`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
```

## Add sequence tables to unsharded keyspace

As mentioned earlier, you should use [sequence tables](/docs/vitess/sharding/sequence-tables) in place of `AUTO_INCREMENT` for your sharded tables.

Your sequence tables will live in the source unsharded keyspace.

<Steps>
  <Step>
    Switch back to your original unsharded keyspace.

    ```sql  theme={null}
    use `metal`;
    ```
  </Step>

  <Step>
    Create 2 new sequence tables: one for `notifications` and one for `users`.

    ```sql  theme={null}
    CREATE TABLE `notifications_seq` ( `id` bigint NOT NULL, `next_id` bigint DEFAULT NULL, `cache` bigint DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci COMMENT='vitess_sequence';

    CREATE TABLE `users_seq` ( `id` bigint NOT NULL, `next_id` bigint DEFAULT NULL, `cache` bigint DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci COMMENT='vitess_sequence';
    ```
  </Step>
</Steps>

## Add the sequence tables to the VSchema

The following will add the sequence tables to the source keyspace VSchema (`metal`):

```sql  theme={null}
alter vschema add sequence `metal`.notifications_seq;
alter vschema add sequence `metal`.users_seq;
```

Next, add the following to specify that those sequence tables should be used as the sequence tables for the sharded tables in the new target keyspace VSchema (`metal-sharded`):

```sql  theme={null}
alter vschema on metal-sharded.notifications add auto_increment id using `metal`.notifications_seq;
alter vschema on metal-sharded.users add auto_increment id using `metal`.users_seq;
```

The resulting VSchema for `metal` will look like this:

```json  theme={null}
{
  "tables": {
    "notifications_seq": {
      "type": "sequence"
    },
    "users_seq": {
      "type": "sequence"
    }
  }
}
```

## Add the tables to the source keyspace VSchema (`metal`)

<Note>
  If you are using Vitess global routing you may have already completed this.
  If so, you can skip this step.
</Note>

You now need to add all tables to your source keyspace (`metal` for this example) VSchema. The VSchema is used to route queries to the proper keyspace. When you only had one keyspace, you didn't need to worry about this. But now that you've added a new sharded keyspace, Vitess will need to check the VSchema of each keyspace to route queries.

For more infomation, see the [VSchema documentation](/docs/vitess/sharding/vschema).

For this step, it's often easier to do from the UI instead of with an `ALTER` statement.

<Steps>
  <Step>
    On the Clusters page, click on your source unsharded keyspace (`metal`).
  </Step>

  <Step>
    Select the branch you created in the previous step.
  </Step>

  <Step>
    Click "VSchema".
  </Step>

  <Step>
    Add in **all tables** that exist in this keyspace. This is what our `metal` keyspace looks like:

    ```json  theme={null}
    {
      "tables": {
        "exercises": {},
        "exercise_log": {},
        "programs": {},
        "users": {}
      }
    }
    ```
  </Step>

  <Step>
    Click "Save changes"
  </Step>
</Steps>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Sequence Tables
Source: https://planetscale.com/docs/vitess/sharding/sequence-tables

In MySQL, it is common to have an integer primary key that uses the `AUTO_INCREMENT` feature for assigning IDs.

However, if you have a horizontally-sharded table, you will not be able to use `AUTO_INCREMENT` for your ID.
In such a setup, the rows of the table are distributed across many instances of MySQL.
The separate instances of MySQL do not have a built-in way to coordinate which IDs are in use and which are not.
Instead, you will need to use a **sequence table**.

A [sequence table](https://vitess.io/docs/api/reference/features/vitess-sequences) is a special table that contains metadata for managing the incrementing ID values for the column of a horizontally sharded table.
Each time you create a new horizontally sharded table, you should create the corresponding sequence table and update the VSchema.

## Creating a horizontally-sharded table

We recommend keeping [safe migrations](/docs/vitess/schema-changes/safe-migrations) enabled for all production databases.
Thus, the first step to make schema modifications is to create a new [branch](/docs/vitess/schema-changes/branching),
and connect to it via the [command line](/docs/cli).

Next, to create a horizontally-sharded table, switch to your desired sharded keyspace.
Create a new table in this keyspace, and do *not* use `AUTO_INCREMENT` for your ID column.
For example, to create a table in the `test_sharded` keyspace, run:

```sql  theme={null}
USE test_sharded;
CREATE TABLE test(id BIGINT UNSIGNED PRIMARY KEY, data JSON);
```

Next, switch over to the unsharded keyspace that you want to use for sequence tables.
Here, you'll create a sequence table.
It is good practice to use the same name as the sharded table with `_seq` or `_sequence` appended.
Being consistent with this naming will help maintain a clear association between your data tables and sequence tables.

```sql  theme={null}
USE test_unsharded;
CREATE TABLE test_seq(id bigint, next_id bigint, cache bigint, primary key(id)) comment 'vitess_sequence';
```

We also need to update the [VSchema](/docs/vitess/sharding/vschema) of our database.
We need to tell Vitess about this new `SEQUENCE`, let it know to use the `id` column as the shard key, and tell it to use the `test_seq` table for fetching auto incrementing IDs.

```sql  theme={null}
ALTER VSCHEMA ADD SEQUENCE `test_unsharded`.`test_seq`;

ALTER VSCHEMA ON `test_sharded`.`test` ADD VINDEX hash(id) USING hash;

ALTER VSCHEMA ON `test_sharded`.`test` ADD auto_increment id USING `test_unsharded`.`test_seq`;
```

When you are comfortable with your schema changes, create a [deploy request](/docs/vitess/schema-changes/deploy-requests) and merge.

## Sequence table values

Unlike vanilla Vitess, PlanetScale will automatically populate the single required row into any sequence table created with the above steps.
After merging your deploy request, you should be able to query the sequence table as follows:

```sql  theme={null}
SELECT * FROM test_unsharded.test_seq;
+----+---------+-------+
| id | next_id | cache |
+----+---------+-------+
|  0 |       1 |  1000 |
+----+---------+-------+
1 row in set (0.04 sec)
```

* `id` Should always be 0.
* `next_id` represents the next ID in the sequence to be fetched. You typically want this to start as 1.
* `cache` represents the number of IDs that can be fetched and cached by a VTTablet. For good performance, this should be set to a large number like 1000 or more.

We can check that the sequence table is working in assigning IDs by inserting a new row and then querying for the row with ID `1`.

```sql  theme={null}
INSERT INTO test (data) VALUES ('{"errors": [{"message": "Error message", "code": 10}]}');
Query OK, 1 row affected (0.06 sec)

SELECT data FROM test WHERE id=1;
+--------------------------------------------------------+
| data                                                   |
+--------------------------------------------------------+
| {"errors": [{"message": "Error message", "code": 10}]} |
+--------------------------------------------------------+
1 row in set (0.05 sec)
```

<Note>
  Check out the [Vitess documentation on sequences](https://vitess.io/docs/api/reference/features/vitess-sequences/) for more information.
</Note>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Workflow: Sharded to sharded keyspace
Source: https://planetscale.com/docs/vitess/sharding/sharding-a-sharded-keyspace

This tutorial covers how to adjust the number of shards in a sharded keyspace. This may become necessary if your sharded keyspace grows significantly and requires additional shards.

Adding or removing shards from a sharded keyspace requires some rebalancing, as you may be moving data from one MySQL instance to another. In PlanetScale, this is done using a sharded to sharded keyspace [workflow](/docs/vitess/scaling/workflows). This involves creating a new sharded (target) keyspace with the new desired number of shards and transferring all of the data from your current (source) keyspace to the new target. This operation is done with no downtime.

<Note>
  The steps in this documentation are similar to those in the [Sharding quickstart](/docs/vitess/sharding/sharding-quickstart), however, there is one important addition that you cannot skip. Be sure to review [Step 7 — Remove `"require_explicit_routing": true`](/docs/vitess/sharding/sharding-a-sharded-keyspace#step-7---remove-require_explicit_routing-true), as it is a crucial step in this workflow configuration that differs from the original unsharded to sharded workflow.
</Note>

* If you are sharding an existing table in an *unsharded* keyspace, follow the instructions in the [Sharding quickstart documentation](/docs/vitess/sharding/sharding-quickstart).
* If you are creating a new table that you want in your existing sharded keyspace, follow the instructions in the [Sharding new tables documentation](/docs/vitess/sharding/sharding-new-tables).
* If you simply need to adjust the size of each shard, and not the number of shards, you can do so from the [Clusters](/docs/vitess/cluster-configuration) page in the dashboard.

<Warning>
  These are advanced configuration settings that expose some of the underlying Vitess configuration of your cluster.
  Misconfiguration can cause availability issues. We recommend thoroughly reading through the documentation in the [Sharding section](/docs/vitess/sharding) of the docs prior to making any changes. If you have any questions, please [reach out to our support team](https://docs/support.planetscale.com).
</Warning>

Throughout this guide, we will refer to the source keyspace and target keyspace, which are defined as follows:

* **Source keyspace** — The original sharded keyspace from which you are moving the tables you wish to shard.
* **Target keyspace** — The new sharded keyspace that you are moving the selected tables to.

This guide also assumes that you either are already using `@primary` in your application code to [target your keyspaces](/docs/vitess/sharding/targeting-correct-keyspace) or you do not directly set a database name in your application code.

## Pre-sharding checklist

There is a small amount of upfront work that needs to happen prior to sharding your table(s) again.

### 1. Prepare to move all table(s) from source keyspace

Some common signals that a table may benefit from further sharding include:

* The table has become very large and query performance has degraded due to this
* Schema changes to the table take hours
* You expect the table to grow quickly and want to shard it further before it becomes a problem

We strongly recommend moving all table(s) from the source keyspace to the target keyspace, so that by the end of this process your database has at most two keyspaces: one unsharded keyspace, and one sharded keyspace.

### 2. Create another sharded keyspace

To set up another sharded keyspace:

<Steps>
  <Step>Go to the "**Clusters**" tab in the left nav in the PlanetScale dashboard.</Step>
  <Step>Click "**New keyspace**".</Step>
  <Step>Enter the keyspace name (for example, `metal-sharded-2`).</Step>

  <Step>
    Select the **shard count** and choose the **cluster size** for this keyspace. Keep in mind, creating a sharded
    keyspace will use the selected size for *each* shard. For example, if you are creating 4 shards and choose the
    `PS-80` cluster size, we will create 4 `PS-80`s, each with 1 primary and 2 replicas.
  </Step>

  <Step>
    Select the number of *additional* replicas, if any, that you'd like to add to each cluster. Each cluster comes with
    2 replicas by default, so any number you choose will be in addition to those 2.
  </Step>

  <Step>
    Review the new monthly cost for this keyspace below. This is in addition to your existing unsharded keyspace, as
    well as any other keyspaces you add.
  </Step>

  <Step>Once satisfied, click "**Create keyspace**".</Step>
</Steps>

### 3. Add `"require_explicit_routing": true`

If you are using [Vitess global routing](https://vitess.io/docs/api/reference/features/global-routing/) (for example, if you are using `@primary`), you will get ambiguous table errors once you add Vindexes to your new, third keyspace.

To prevent this error, you **must** temporarily add `require_explicit_routing` to your new, second keyspace's VSchema:

```
{
  "sharded": true,
  "require_explicit_routing": true,
}
```

This will instruct Vitess's global routing to exclude your second keyspace from routing until explicitly targeted. This is just temporary. You will remove this later in this tutorial *before* completing the workflow.

You can choose to do this step, and the following step, by one of the following methods:

* **Safe migrations off**: modify the target keyspace VSchema directly on the Clusters page

* **Safe migrations on**: modify the target keyspace VSchema using deploy requests

### 4. Copy Vindexes and auto-increment VSchema settings

Assuming your sharding scheme will remain the same, once you've completed the previous step of adding `"require_explicit_routing": true`, you can copy the relevant parts of your source keyspace VSchema into your target keyspace VSchema.

Since we recommend moving all tables from the source to the target keyspace, your target VSchema will look exactly the same as your source VSchema, with the addition of `"require_explicit_routing": true`. For example, if you are moving the tables `users` and `exercise_logs`, and your source keyspace VSchema looks like this:

```json expandable theme={null}
{
  "sharded": true,
  "vindexes": {
    "hash": {
      "type": "hash"
    }
  },
  "tables": {
    "exercise_logs": {
      "column_vindexes": [
        {
          "name": "hash",
          "columns": ["user_id"]
        }
      ],
      "auto_increment": {
        "column": "id",
        "sequence": "`unsharded`.`exercise_logs_seq`"
      }
    },
    "users": {
      "column_vindexes": [
        {
          "name": "hash",
          "columns": ["id"]
        }
      ],
      "auto_increment": {
        "column": "id",
        "sequence": "`unsharded`.`users_seq`"
      }
    }
  }
}
```

Your target keyspace VSchema will look like this:

```json expandable theme={null}
{
  "sharded": true,
  "require_explicit_routing": true,
  "vindexes": {
    "hash": {
      "type": "hash"
    }
  },
  "tables": {
    "exercise_logs": {
      "column_vindexes": [
        {
          "name": "hash",
          "columns": ["user_id"]
        }
      ],
      "auto_increment": {
        "column": "id",
        "sequence": "`unsharded`.`exercise_logs_seq`"
      }
    },
    "users": {
      "column_vindexes": [
        {
          "name": "hash",
          "columns": ["id"]
        }
      ],
      "auto_increment": {
        "column": "id",
        "sequence": "`unsharded`.`users_seq`"
      }
    }
  }
}
```

## Sharding with the sharded to sharded workflow

### Step 1 — Set up the workflow

<Steps>
  <Step>Click "**Workflows**" in the left nav.</Step>
  <Step>Click "**New workflow**".</Step>
  <Step>Give your workflow a name, such as "Shard users and exercise\_log".</Step>
</Steps>

We are now going to move the tables, data included, from the source keyspace to the new, target one that you created in the pre-sharding checklist.
Make sure the "**Source keyspace**" dropdown shows your original sharded keyspace and the "**Destination keyspace**" shows the new sharded keyspace you made.

<Steps>
  <Step>
    Under Source keyspace, check the tables you want to move to the new keyspace. Remember, these should match the ones
    you already prepped to move earlier.
  </Step>

  <Step>
    Once you select the tables that you want to move to the new keyspace, you'll see the destination keyspace update to
    show how the data will be replicated across the number of shards you chose for that keyspace during cluster
    configuration.
  </Step>

  <Step>Click "**Validate**".</Step>

  <Step>
    You will see a validation checklist that lets you know if all of the work from the pre-sharding checklist has been
    completed. If something is missing, you will not be able to proceed with the workflow. Please revisit the
    [pre-sharding checklist](/docs/vitess/sharding/pre-sharding-checklist) and fix any issues, and don't hesitate to
    [reach out to support](https://docs/support.planetscale.com) if you get stuck.
  </Step>

  <Step>
    Once all validations have passed, click "Create workflow" to start the process of moving the sharded tables to the
    new keyspace.
  </Step>
</Steps>

### Step 2 - Copying phase

As soon as you click "Create workflow", we begin the copying phase. During this phase, Vitess is copying rows of the table(s) you've selected from your source keyspace to your target keyspace. This uses a combination of `SELECT * FROM TABLE` and binlog-based replication.

There are no active steps for you here besides monitoring the logs at the bottom of the screen in case of errors.

### Step 3 - Verify data consistency

Once the initial data has been copied over, you'll see this message:

> The source keyspace is currently serving all traffic. Before switching traffic, we need to verify data consistency across keyspaces.

Click "Verify data" to verify the consistency of data between the keyspaces. This step may take a few minutes. Once it's complete, you should see "Data verified", meaning you can proceed to the next step.

### Step 4 - Running phase

Assuming there were no errors in the previous stage, you will have automatically entered the running phase — pure binlog-based replication. This also means replication lag was low enough for VReplication to advance into this phase. You should also see `State Changed: running` in the logs below.

During this phase, the following happens:

* Your source keyspace is still serving all primary and replica traffic for the tables you're moving over.
* All existing data that is going to the target keyspace has been copied over.
* VReplication is also replicating all new incoming writes to the tables in the target keyspace.

Again, you should check the logs below to ensure there are no errors and to better understand the ongoing workflow process. There are no active steps to take during this phase. If the logs do not show any errors, you can proceed to the next step.

### Step 5 - Switch traffic to target keyspace

You are now able to switch the traffic over so that traffic to the sharded tables is served from the target keyspace instead of the source keyspace.

You have two options here:

1. Switch both primary and replica traffic.
2. Switch just replica traffic.

If you want to test the replica traffic only first, you can select "**Switch replica traffic only**" from the dropdown, and then click the button. Otherwise, click "**Switch primary and replica traffic**".

### Step 6 - Check traffic in your application

You should now go check out your production application that uses this database to make sure everything is running as expected.

If you selected to only switch replica traffic in the previous step and data that is being served from replicas in your production application looks good, you can click "**Switch primary traffic**" when you're ready. Again, go to your production application to make sure everything is working as expected.

During this phase, you can also go to your "**Insights**" tab in the dashboard to see markers showing where your workflow started and transitioned into different states. If something looks off where you see a marker, it is worth investigating.

You might notice during this phase that you also have the option to "**Undo traffic switch**". So if you do notice an issue once you switched to serve traffic from the target keyspace, you can click this button to revert back to serving traffic from the source keyspace. Remember, both keyspaces have a copy of the same data for the targeted tables, as described in the Running phase above.

### Step 7 - Remove `"require_explicit_routing": true`

You should have added `require_explicit_routing` to your target keyspace's VSchema in step 4 of the "Pre-sharding checklist":

```
{
  "require_explicit_routing": true,
  ...
}
```

You'll need to remove it before completing the workflow.

<Warning>
  If you don't remove this, you may start to see "table not found" errors once the workflow is completed.
</Warning>

### Step 8 - Complete the workflow

Please note, up until now, you've had the option to click "Cancel workflow" in the top right corner. Once you click "Complete workflow" in this step, there is no going back. You will have the option to reinstate the routing rules if it appears your queries aren't being routed correctly, but you cannot swap the tables back to the source keyspace.

At this stage, you have the option to reverse the traffic if you need more time to test. This will switch you back to serving from the source keyspace (see step 4).

If you have removed `"require_explicit_routing": true` from your target keyspace VSchema, and you are sure you want to proceed with this operation, you can click "**Complete workflow**". This action is irreversible.

### Step 9 - Check that your production application is working as expected

Finally, check your production application to make sure everything is working as expected. You can check your [Insights](/docs/vitess/monitoring/query-insights) tab to see if queries are being properly routed to your new keyspace. Insights will also show you any errors, query performance issues, and more.

If you realize there are issues, such as queries not being correctly served to the new keyspace, you can click "My application has errors", and we will temporarily restore the routing rules. If you are facing errors, double check that you have removed `"require_explicit_routing": true` from your target keyspace VSchema. Once your application is updated, click "**I have updated my application**".

Once everything looks good, click "**My application is working**", and the workflow will complete.

That's it! The tables you selected at the beginning are now being served by the new sharded keyspace.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Sharding new tables
Source: https://planetscale.com/docs/vitess/sharding/sharding-new-tables

This tutorial shows you how to create new tables in a sharded keyspace. If you have an existing table that you want to shard, follow the [Sharding quickstart](/docs/vitess/sharding/sharding-quickstart) instead.

<Warning>
  Misconfiguration can cause availability issues. We recommend thoroughly reading through the documentation in the [Sharding section](/docs/vitess/sharding) of the docs prior to making any changes. If you have any questions, please [reach out to our support team](https://docs/support.planetscale.com).
</Warning>

Before you begin, we recommend the following reading:

<Columns cols={2}>
  <Card title="What is a keyspace?" icon="key" horizontal href="/docs/vitess/sharding" />

  <Card title="Vindexes" icon="info" horizontal href="/docs/vitess/sharding/vindexes" />

  <Card title="Sharding quickstart" icon="rocket" horizontal href="/docs/vitess/sharding/sharding-quickstart" />

  <Card title="Avoiding cross-shard queries" icon="messages-question" horizontal href="/docs/vitess/sharding/avoiding-cross-shard-queries" />

  <Card title="Sequence tables" icon="table" horizontal href="/docs/vitess/sharding/sequence-tables" />
</Columns>

<Info>
  Sharded keyspaces are not supported on databases with foreign key constraints enabled.
</Info>

When adding a new table to your database cluster, you have the option to shard it from the start. This can be a good option if you expect the table to grow incredibly quickly, or if you already have some other sharded tables that you know you will frequently join with this new table and want to avoid cross-shard queries.

Before you get started, you need a sharded keyspace. If you already have one, you can skip the next section.

## 1. Add a sharded keyspace

<Warning>
  If you are using [Vitess global routing](https://vitess.io/docs/api/reference/features/global-routing/), you must take extra care when adding a sharded keyspace to your database (for example, if you are using `@primary`).
  Before creating one, you must ensure that all tables from your first *unsharded keyspace* are added to the `VSchema` of that *unsharded keyspace*. Eg:

  ```
  {
    "tables": {
      "users": { }
      ...
    }
  }
  ```

  Otherwise, queries will fail. [Learn more about VSchema.](/docs/vitess/sharding/vschema)
</Warning>

Navigate to the [Clusters page](/docs/vitess/cluster-configuration).

<Steps>
  <Step>
    Click "New keyspace".
  </Step>

  <Step>
    Enter the keyspace name. For example, if your existing unsharded keyspace is named `metal`, you may create a sharded keyspace named `metal-sharded`.
  </Step>

  <Step>
    Select the number of shards you want to create in this keyspace.

    <Note>
      The cost of adding this additional keyspace largely depends on the number of shards you choose, the cluster size, and if you'd like to add additional replicas.
    </Note>
  </Step>

  <Step>
    Choose the cluster sizes you would like to use for this keyspace. Keep in mind, if you are creating a sharded keyspace, this will spin up multiple clusters of the selected size. For example, if you are creating 4 shards and choose the `PS-80` cluster size, we will create 4 `PS-80`s, each with 1 primary and 2 replicas.
  </Step>

  <Step>
    Select the number of *additional* replicas, if any, that you'd like to add to each cluster. Each cluster comes with 2 replicas by default, so any number you choose will be in addition to those 2.
  </Step>

  <Step>
    Review the new monthly cost for this keyspace below. This is in addition to your existing unsharded keyspace, as well as any other keyspaces you add.
  </Step>

  <Step>
    Once satisfied, click "Create keyspace".
  </Step>
</Steps>

## 2. Copy the sharded tables to the new keyspace and remove `AUTO_INCREMENT`

Let's say we're creating a new sharded table called `exercise_logs` that looks like this:

```sql  theme={null}
CREATE TABLE exercise_logs (
  id BIGINT UNSIGNED AUTO_INCREMENT,
  user_id BIGINT UNSIGNED,
  exercise_id BIGINT UNSIGNED,
  created_at DATETIME,
  reps SMALLINT UNSIGNED,
  sets SMALLINT UNSIGNED,
  weight SMALLINT UNSIGNED,
  notes VARCHAR(1024),
  PRIMARY KEY(id)
);
```

When a table is spread across multiple shards, using `AUTO_INCREMENT` on your primary key can cause problems. Because each shard is its own separate MySQL instance, the shards do not have the context to know whether or not a primary key for a table entry is already in use on other shards. This means you risk two different table entries being assigned the same primary key.

To avoid this, it is a best practice to use [sequence tables](/docs/vitess/sharding/sequence-tables) instead. We will cover how to set these up shortly. First, make sure you remove any `AUTO_INCREMENT`s from the tables you're sharding.

Switch to the new keyspace, and create the table there with no `AUTO_INCREMENT`:

```sql  theme={null}
use `metal-sharded`;

CREATE TABLE exercise_logs (
  id BIGINT UNSIGNED,
  user_id BIGINT UNSIGNED,
  exercise_id BIGINT UNSIGNED,
  created_at DATETIME,
  reps SMALLINT UNSIGNED,
  sets SMALLINT UNSIGNED,
  weight SMALLINT UNSIGNED,
  notes VARCHAR(1024),
  PRIMARY KEY(id)
);
```

## Add sequence tables to unsharded keyspace

As mentioned earlier, you should use [sequence tables](/docs/vitess/sharding/sequence-tables) in place of `AUTO_INCREMENT` for your sharded tables.

Your sequence tables will live in the source unsharded keyspace.

<Steps>
  <Step>
    Switch back to your original unsharded keyspace.

    ```sql  theme={null}
    use `metal`;
    ```
  </Step>

  <Step>
    Create the new sequence table for `exercise_logs`. If you're adding multiple new tables, create a sequence table for each:

    ```sql  theme={null}
    CREATE TABLE `exercise_logs_seq` ( `id` bigint, `user_id` bigint, `exercise_id` bigint, `created_at` datetime, `reps` smallint unsigned, `sets` smallint unsigned, `weight` smallint unsigned, notes varchar(1024), PRIMARY KEY (`id`) ) COMMENT='vitess_sequence';
    ```
  </Step>
</Steps>

Note `COMMENT='vitess_sequence'` at the end. This must be added for every sequence table you create.

## Add the sequence tables to the VSchema

The following will add the sequence tables to the source keyspace VSchema (`metal`):

```sql  theme={null}
alter vschema add sequence `metal`.exercise_logs_seq;
```

Next, add the following to specify that those sequence tables should be used as the sequence tables for the sharded tables in the new target keyspace VSchema (`metal-sharded`):

```sql  theme={null}
alter vschema on metal-sharded.exercise_logs add auto_increment id using `metal`.exercise_logs_seq;
```

The resulting VSchema for `metal` will look like this:

```json  theme={null}
{
  "tables": {
    "exercise_logs_seq": {
      "type": "sequence"
    }
  }
}
```

## 6. Add the tables to the source keyspace VSchema (`metal`)

You now need to add all tables to your source keyspace (`metal` for this example) VSchema. The VSchema is used to route queries to the proper keyspace. When you only had one keyspace, you didn't need to worry about this. But now that you've added a new sharded keyspace, Vitess will need to check the VSchema of each keyspace to route queries.

For more infomation, see the [VSchema documentation](/docs/vitess/sharding/vschema).

For this step, it's often easier to do from the UI instead of with an `ALTER` statement.

<Steps>
  <Step>
    On the Clusters page, click on your source unsharded keyspace (`metal`).
  </Step>

  <Step>
    Select the branch you created in the previous step.
  </Step>

  <Step>
    Click "VSchema".
  </Step>

  <Step>
    Add in **all tables** that exist in this keyspace. This is what our `metal` keyspace looks like:

    ```json  theme={null}
    {
      "tables": {
        "exercises": {},
        "exercise_log": {},
        "programs": {},
        "users": {}
      }
    }
    ```
  </Step>

  <Step>
    Click "Save changes"
  </Step>
</Steps>

## 7. Targeting the correct keyspace

Once you have more than one keyspace, with tables distributed across both keyspaces, your application may not know how to properly route queries to the correct keyspace.

If you originally set up your application configuration code with something like `DATABASE_NAME=your_database_name`, where `your_database_name` is the name of your original unsharded keyspace, you will need to update your configuration code so that all queries don't go straight to that keyspace.

The preferred way to do this is to just leave off the database name completely in your application configuration code. PlanetScale will be able to route traffic correctly just using the connection username and password.

While this is the preferred way, it's sometimes not possible. For example, many frameworks and ORMs require that you include a database name.

In those cases, you should use `@primary`. This will send any incoming queries first to our [Global Edge Network](https://planetscale.com/blog/introducing-global-replica-credentials#building-planetscale-global-network), which will see that you're targeting a primary. Edge will then send the request to the VTGate(s)/load balancer. We typically will use [Vitess's Global Routing](https://vitess.io/docs/api/reference/features/global-routing/) to direct the query to the correct keyspace and, optionally, correct shard.

<Note>
  If you explicitly wish to target a replica for some or all reads, using `@replica` will have the same effect as `@primary` in that it will automatically route the request to the correct keyspace.
</Note>

[Global Replica Credentials](/docs/vitess/scaling/replicas#1-create-a-global-replica-credential-recommended) are not currently supported in this context. You can still target replicas instead of your primary with `@replica`, but it will not automatically route the query to the *closest* replica.

For more information, refer to the [Targeting the correct keyspace documentation](/docs/vitess/sharding/targeting-correct-keyspace).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Workflow: Unsharded to sharded keyspace
Source: https://planetscale.com/docs/vitess/sharding/sharding-quickstart

This tutorial covers how to shard **existing tables** in your PlanetScale database. This is done using the unsharded to sharded keyspace [workflow](/docs/vitess/scaling/workflows).

export const YouTubeEmbed = ({id, title}) => {
  return <Frame>
      <iframe src={`https://www.youtube-nocookie.com/embed/${id}?rel=0`} title={title} className="aspect-video w-full" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" />
    </Frame>;
};

* If you are creating a new table that you want in a sharded keyspace, follow the instructions in the [Sharding new tables doc](/docs/vitess/sharding/sharding-new-tables).
* If you are moving tables from one sharded keyspace to another sharded keyspace, follow the intructions in the [Sharding a sharded keyspace doc](/docs/vitess/sharding/sharding-a-sharded-keyspace).

Before you begin, we recommend the following reading:

<Columns cols={2}>
  <Card title="Workflows overview" icon="repeat" horizontal href="/docs/vitess/scaling/workflows" />

  <Card title="What is a keyspace?" icon="key" horizontal href="/docs/vitess/sharding" />

  <Card title="Vindexes" icon="info" horizontal href="/docs/vitess/sharding/vindexes" />

  <Card title="Avoiding cross-shard queries" icon="question" horizontal href="/docs/vitess/sharding/avoiding-cross-shard-queries" />
</Columns>

<Note>
  Sharded keyspaces are not supported on databases with foreign key constraints enabled.
</Note>

<Warning>
  These are advanced configuration settings that expose some of the underlying Vitess configuration of your cluster. Misconfiguration can cause availability issues. We recommend thoroughly reading through the documentation in the [Sharding section](/docs/vitess/sharding) of the docs prior to making any changes. If you have any questions, please [reach out to our support team](https://docs/support.planetscale.com/).
</Warning>

Throughout this guide, we will refer to the source keyspace and target keyspace, which are defined as follows:

* **Source keyspace** — The original unsharded keyspace from which you are moving the tables you wish to shard.
* **Target keyspace** — The new sharded keyspace that you are moving the selected tables to.
* **Global keyspace** — An unsharded keyspace where sequence tables will be automatically created for workflow tables that contain `AUTO_INCREMENT` primary indexes.

If you prefer video content, you can watch the video on sharding tables with PlanetScale here:

<YouTubeEmbed id="m5eW34nfNtQ" title="Tutorial: Horizontal sharding with PlanetScale" />

## Pre-sharding checklist

There is a small amount of upfront work that needs to happen prior to sharding your table(s). Planetscale handles some of these steps for you automatically. How many steps get automatically handled depends on the Vitess version that is powering your database. Which Vitess version you are on depends on how long your database has existed and what features you have enabled. If you're curious, see the For a full list of steps, see the [pre-sharding checklist](/docs/vitess/sharding/pre-sharding-checklist).

The rest of the work that you need to do yourself is documented below.

### 1. Decide which table(s) you want to shard

First and foremost, decide which table(s) you want to shard. Some common signals that a table may benefit from sharding include:

* The table has become very large (>100 GB) and query performance has degraded due to this
* Schema changes to the table take hours
* You expect the table to grow quickly and want to shard it before it becomes a problem

### 2. Identify tables you frequently `JOIN`

Once you know the tables that you are going to move to a sharded keyspace, you also need to think about which other tables you frequently join with the tables you are going to shard. We recommend that tables you frequently join together all live in the same keyspace.

As an example, if you have a `exercise_logs` table that has become extremely large and continues to grow, you may decide to move this to a sharded keyspace. Perhaps this table is frequently joined it with the `users` table. In this scenario, we recommend moving both the `exercise_logs` and `users` tables to the new sharded keyspace and sharding both tables.

The goal here is to avoid cross-keyspace or cross-shard queries. For more information about this, see the [Avoiding cross-shard queries](/docs/vitess/sharding/avoiding-cross-shard-queries) documentation.

### 3. Create a sharded keyspace

<Warning>
  If you are using [Vitess global routing](https://vitess.io/docs/api/reference/features/global-routing/) (for example, if you are using `@primary`), you must take extra care when adding a second keyspace to a database. Before creating one, you must ensure that all tables from your *initial keyspace* are added to the `VSchema` of that *initial keyspace*. Eg:

  ```
  {
    "tables": {
      "users": { }
      ...
    }
  }
  ```

  Otherwise, queries will fail. [Learn more about VSchema.](/docs/vitess/sharding/vschema)
</Warning>

To set up the sharded keyspace:

<Steps>
  <Step>Go to the "**Clusters**" tab in the left nav in the PlanetScale dashboard.</Step>
  <Step>Click "**New keyspace**".</Step>
  <Step>Enter the keyspace name (for example, `metal-sharded`).</Step>

  <Step>
    Select the **shard count** and choose the **cluster size** for this keyspace. Keep in mind, creating a sharded
    keyspace will use the selected size for *each* shard. For example, if you are creating 4 shards and choose the
    `PS-80` cluster size, we will create 4 `PS-80`s, each with 1 primary and 2 replicas.
  </Step>

  <Step>
    Select the number of *additional* replicas, if any, that you'd like to add to each cluster. Each cluster comes with
    2 replicas by default, so any number you choose will be in addition to those 2.
  </Step>

  <Step>
    Review the new monthly cost for this keyspace below. This is in addition to your existing unsharded keyspace, as
    well as any other keyspaces you add.
  </Step>

  <Step>Once satisfied, click "**Create keyspace**".</Step>
</Steps>

### 4. Choose your Vindexes

<Warning>
  If you are using [Vitess global routing](https://vitess.io/docs/api/reference/features/global-routing/) (for example, if you are using `@primary`), you may get ambiguous table errors once you add Vindexes to your new, second keyspace, if those tables also exist in the first keyspace.

  To prevent this error, you can add `require_explicit_routing` to your new, second keyspace's VSchema:

  ```
  {
    "require\_explicit\_routing": true,
    ...
  }
  ```

  This will instruct Vitess's global routing to exclude your second keyspace from routing until explicitly targeted. Make sure to remove this field *before* completing a workflow.
</Warning>

When configuring a sharded keyspace, you must think about *how* to distribute the data across shards. This is done by selecting a [Vindex](/docs/vitess/sharding/vindexes) (Vitess index) for each table.

A Vindex provides a way to map incoming rows of data to the appropriate shard in your keyspace. Similar to how every MySQL table must have a primary key, every sharded table must additionally have a **primary Vindex**.

The primary Vindex is the Vindex that determines which shard each row of data will reside on. For more information about choosing a Vindex, see the [Vindexes documentation](/docs/vitess/sharding/vindexes). You can also see an example in the [Avoiding cross-shard queries](/docs/vitess/sharding/avoiding-cross-shard-queries) documentation.

To specify the vindex for the tables you want to shard:

<Steps>
  <Step>Create a new branch.</Step>

  <Step>
    Once on your new branch, switch to your sharded keyspace. You can do this in the PlanetScale console if preferred by
    clicking "Console" in the left nav. Continuing the previous example, we'll switch to the new `metal-sharded`
    keyspace: ``use \`metal-sharded\`;``
  </Step>

  <Step>
    Alter the VSchema (Vitess schema) of the tables you have chosen to shard to add your chosen Vindex. VSchema is
    provided to Vitess in JSON. We need to update the VSchema of both our keyspaces in order to:
  </Step>
</Steps>

* Let Vitess know that it should use the sequence tables for generating incrementing IDs
* Let Vitess know how incoming rows should be sharded using Vindexes

For example, let's say we are sharding a table called `exercise_logs`, and we determined `user_id` to be the best option. We are also using the predefined [`hash` Vindex function](https://vitess.io/docs/api/reference/features/vindexes/#predefined-vindexes), which is a common choice.

```
ALTER vschema ON exercise\_logs add vindex hash(user\_id) using hash;
ALTER vschema ON users add vindex hash(id) using hash;
```

### 5. Deploy the changes to production

Once you're finished with these pre-sharding steps, you can go ahead and deploy the changes to production.

<Steps>
  <Step>Click "Branches".</Step>
  <Step>Select the branch that has your keyspace updates.</Step>
  <Step>Select the sharded keyspace in the dropdown. You should see a diff that shows edits to your VSchema.</Step>
  <Step>Click "Create deploy request".</Step>
  <Step>If everything looks good, deploy your changes.</Step>
</Steps>

If you go back to your Clusters tab, click your sharded keyspace, and click the VSchema tab, you'll see those changes reflected there.

## Sharding with the unsharded to sharded workflow

Alright, now that the prep work is done, it's time to shard the tables you chose to move to your sharded keyspace.

You must have [Safe Migrations](/docs/vitess/schema-changes/safe-migrations) enabled on your production branch to use Workflows. If it's not enabled, go do that first.

### Step 1 — Set up the workflow

<Steps>
  <Step>Click "**Workflows**" in the left nav.</Step>
  <Step>Click "**New workflow**".</Step>
  <Step>Give your workflow a name, such as "Shard users and exercise\_log".</Step>
</Steps>

We are now going to move the tables, data included, from the original keyspace to the sharded one that you created in the pre-sharding checklist. Make sure the "**Source keyspace**" dropdown shows your original unsharded keyspace and the "**Destination keyspace**" shows the new sharded keyspace you made.

<Steps>
  <Step>
    Under Source keyspace, check the tables you want to move to the new keyspace to shard. Remember, these should match
    the ones you already prepped to move earlier.
  </Step>

  <Step>
    Once you select the tables that you want to move to the sharded keyspace, you'll see the destination keyspace update
    to show how the data will be replicated across the number of shards you chose for that keyspace during cluster
    configuration.
  </Step>

  <Step>
    Configure workflow behaviors under `Advanced options`. - Defer secondary index creation: Enable this to dramatically
    decrease the time required to copy data into the new keyspace. Indexes are created after data has been copied into
    the target keyspace. - DDL handling: Define how Vitess responds to schema changes made to the source keyspace while
    the workflow is running. - Global keyspace: Choose an unsharded keyspace where sequence tables will be automatically
    created for workflow tables that contain `AUTO_INCREMENT` primary indexes.
  </Step>

  <Step>Click "**Validate**".</Step>

  <Step>
    You will see a validation checklist that lets you know if all of the work from the pre-sharding checklist has been
    completed. If something is missing, you will not be able to proceed with the workflow. Please revisit the
    [pre-sharding checklist](/docs/vitess/sharding/pre-sharding-checklist) and fix any issues, and don't hesitate to
    [reach out to support](https://docs/support.planetscale.com/) if you get stuck.
  </Step>

  <Step>
    Once all validations have passed, click "Create workflow" to start the process of moving the sharded tables to the
    new keyspace.
  </Step>
</Steps>

### Step 2 - Copying phase

As soon as you click "Create workflow", we begin the copying phase. During this phase, Vitess is copying rows of the table(s) you've selected from your source keyspace to your target keyspace. This uses a combination of `SELECT * FROM TABLE` and binlog-based replication.

There are no active steps for you here besides monitoring the logs at the bottom of the screen in case of errors.

### Step 3 - Verify data consistency

Once the initial data has been copied over, you'll see this message:

> The source keyspace is currently serving all traffic. Before switching traffic, we need to verify data consistency across keyspaces.

Click "Verify data" to verify the consistency of data between the keyspaces. This step may take a few minutes. Once it's complete, you should see "Data verified", meaning you can proceed to the next step.

### Step 4 - Running phase

Assuming there were no errors in the previous stage, you will have automatically entered the running phase — pure binlog-based replication. This also means replication lag was low enough for VReplication to advance into this phase. You should also see `State Changed: running` in the logs below.

During this phase, the following happens:

* Your source keyspace is still serving all primary and replica traffic for the tables you're moving over.
* All existing data that is going to the target keyspace has been copied over.
* VReplication is also replicating all new incoming writes to the tables in the target keyspace.

Again, you should check the logs below to ensure there are no errors and to better understand the ongoing workflow process. There are no active steps to take during this phase. If the logs do not show any errors, you can proceed to the next step.

### Step 5 - Switch traffic to target keyspace

You are now able to switch the traffic over so that traffic to the sharded tables is served from the target keyspace instead of the source keyspace.

You have two options here:

1. Switch both primary and replica traffic.
2. Switch just replica traffic.

If you want to test the replica traffic only first, you can select "**Switch replica traffic only**" from the dropdown, and then click the button. Otherwise, click "**Switch primary and replica traffic**".

### Step 6 - Check traffic in your application

You should now go check out your production application that uses this database to make sure everything is running as expected.

If you selected to only switch replica traffic in the previous step and data that is being served from replicas in your production application looks good, you can click "**Switch primary traffic**" when you're ready. Again, go to your production application to make sure everything is working as expected.

During this phase, you can also go to your "**Insights**" tab in the dashboard to see markers showing where your workflow started and transitioned into different states. If something looks off where you see a marker, it is worth investigating.

You might notice during this phase that you also have the option to "**Undo traffic switch**". So if you do notice an issue once you switched to serve traffic from the target keyspace, you can click this button to revert back to serving traffic from the source keyspace. Remember, both keyspaces have a copy of the same data for the targeted tables, as described in the Running phase above.

### Step 7 - Update your application code to serve from `@primary`

Once you switched to have traffic serve from your target sharded keyspace in step 4, we applied [schema routing rules](https://vitess.io/docs/api/reference/features/schema-routing-rules/). Routing rules are responsible for routing traffic to the correct keyspace and/or shard.

The configuration code in your application likely says something like `database_name = your_database_name`, where `your_database_name` is your original unsharded keyspace. This was fine when you only had one keyspace, but now that you have multiple keyspaces, your application won't know that the other ones exist with this current configuration. The automatic routing rules we applied during this workflow appropriately point incoming queries from your unsharded keyspace to the sharded keyspace, where necessary.

However, when you complete the cutover in the next step, **we will remove these routing rules**. That means if your application is still configured to explicitly send traffic to your original unsharded keyspace, `database_name = your_database_name`, we will not know how to correctly route the queries that have been moved to the sharded keyspace.

The fix for this is simple: update your application to route traffic to your primary instance. You can do that by setting database name to `@primary`.

For example, in Rails, it would look like this:

```
# database.yml
production:
  <<: *default
  username: <%= Rails.application.credentials.planetscale&.fetch(:username) %>
  password: <%= Rails.application.credentials.planetscale&.fetch(:password) %>
  database: "@primary"
  host: <%= Rails.application.credentials.planetscale&.fetch(:host) %>
  ssl_mode: verify_identity
```

You can safely update and deploy this application code before completing the workflow. When you only have one keyspace, `@primary` will automatically route queries to that keyspace. Likewise, if you have queries that you specifically send to replicas or read-only regions, you can use `@replica` for those to have them automatically routed to the correct keyspace/shard.

For more framework-specific examples, see [Targeting the correct keyspace documentation](/docs/vitess/sharding/targeting-correct-keyspace).

Once you deploy this code change, double check that everything in your production application is working correctly.

### Step 8 - Complete the workflow

<Warning>
  If you added `require_explicit_routing` to your target keyspace's VSchema in step 4:

  ```
  {
    "require\_explicit\_routing": true,
    ...
  }
  ```

  You'll need to remove it before completing the workflow.
</Warning>

Again, before you proceed with this step, **it is extremely important that you complete step 6**. This requires changes to your application code.

Please note, up until now, you've had the option to click "Cancel workflow" in the top right corner. Once you click "Complete workflow" in this step, there is no going back. You will have the option to reinstate the routing rules if it appears your queries aren't being routed directly, but you cannot swap the tables back to the source keyspace.

Once you have updated your application to use `@primary` and you are sure you want to proceed with this operation, you can click "**Complete workflow**". You also have the option to Reverse the traffic again here if you need more time to test. This will switch you back to serving from the source keyspace (see step 4).

### Step 9 - Check that your production application is working as expected

Finally, check your production application to make sure everything is working as expected. You can check your [Insights](/docs/vitess/monitoring/query-insights) tab to see if queries are being properly routed to your new keyspace. Insights will also show you any errors, query performance issues, and more.

If you realize there are issues, such as queries not being correctly served to the new keyspace, you can click "My application has errors", and we will temporarily restore the routing rules. Refer to step 6 to ensure you're correctly targeting `@primary`. Once your application is updated, click "**I have updated my application**".

Once everything looks good, click "**My application is working**", and the workflow will complete.

That's it! The tables you selected at the beginning are now being served by the sharded keyspace.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Sharding workflow state reference
Source: https://planetscale.com/docs/vitess/sharding/sharding-workflow-state-reference

This document can be used as reference when going through the [Sharding quickstart](/docs/vitess/sharding/sharding-quickstart). It covers the various states you may enter during the workflow.

## `copying`

During this state, Vitess is copying rows from your source keyspace to your target keyspace using a combination of `SELECT * FROM table` and binlog-based replication.

## `running`

Once copying is complete, we switch to `running` state. Replication lag must be low enough for VReplication to advance into the `running` state. During this state, we are running pure binlog-based replication.

At this point, your original source keyspace tables are serving all primary and replica traffic. VReplication is replicating all writes to the tables in the target keyspace. Routing rules are routing all queries/writes from the target keyspace to the source keyspace.

There are some additional substates you may enter during the `running` state:

* `running` with not `verified data`: You can verify replicated data is accurate before switching any traffic
* `running` with `verified data`: You have run the verification process in the last 24 hours
* `running` with `verification stale`: You have run the verification process over 24 hours ago

## `switched_replicas`

The `switched_replicas` state is an optional intermediate step between `running` and `switched_primaries`. During the `switched_replicas` state, the new target keyspace is serving replica traffic, and the source keyspace continues serving all primary traffic.

Because writes are still doing to the source keyspace, data is still being replicated to the target keyspace.

## `switched_primaries`

During the `switched_primaries` state, the target keyspace is serving replica *and* primary traffic.

Vitess has now created a *reverse* workflow that replicates from the target keyspace back to the source keyspace.

At this point:

* The original, "forward" workflow is paused (stopped)
* Vitess has initialized the sequence tables
* The routing rules are in place, as they will be critical for `reversed_cutover` state.

If you click "reverse traffic", here's what happens:

* The workflow goes back to a `running` state
* Depending on which way you're reversing, "forward" workflow is started again or the "reverse" workflow is stopped

## `cutover`

* This is tantamount to `MoveTables Complete --keep-data=false --keep-routing-rules=false`
  * User **cannot** cancel the workflow at this point. This is because the underlying workflow is actually complete, and the source tables have been deleted
* The underlying Vitess workflow is finished.
* We reset the routing rules to what they were before the workflow started
* User **must have** updated their application code to no longer target a keyspace in order for their app to still work at this step.
  * This is because the routing rules have been cleaned up, and can no longer route queries from `source_keyspace.table` to `target_keyspace.table` and we must rely on Vitess global routing.
* If the user's app is failing at this point, have no fear! There is a `reverse cutover` button

`reversed_cutover`

* User has pressed "reverse cutover"
* We put back the routing rules we saved during `switched_primaries`
  * These rules route queries to the source keyspace table, to the target keyspace table so that queries work again
* User now has more time to update/fix their client application code

`complete`

* Does nothing except say, "I'm all done and I don't need to reverse cutover anymore".
* Allows you to do another workflow. since we limit to 1 active workflow per branch at a time

`cancelled`

* User clicked "cancel"
* We issued a `MoveTables Cancel --keep-data=false --keep-routing-rules=false`
  * target tables will be deleted
  * routing rules will be removed
* Vitess workflow deleted

`error`

* Some sort of error happened. User can click "retry" to try again.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Targeting the correct keyspace
Source: https://planetscale.com/docs/vitess/sharding/targeting-correct-keyspace

Once you have more than one keyspace, with tables distributed across both keyspaces, your application may not know how to properly route queries to the correct keyspace.

If you originally set up your application configuration code with something like `DATABASE_NAME=your_database_name`, where `your_database_name` is the name of your original unsharded keyspace, you will need to update your configuration code so that all queries don't go straight to that keyspace.

The preferred way to do this is to just leave off the database name completely in your application configuration code. PlanetScale will be able to route traffic correctly just using the connection username and password.

While this is the preferred way, it's sometimes not possible. For example, many frameworks and ORMs require that you include a database name.

In those cases, you should use `@primary`. This will send any incoming queries first to our [Global Edge Network](https://planetscale.com/blog/introducing-global-replica-credentials#building-planetscale-global-network), which will see that you're targeting a primary. Edge will then send the request to the VTGate(s)/load balancer. We typically will use [Vitess's Global Routing](https://vitess.io/docs/api/reference/features/global-routing/) to direct the query to the correct keyspace and, optionally, correct shard.

<Note>
  If you explicitly wish to target a replica for some or all reads, using `@replica` will have the same effect as `@primary` in that it will automatically route the request to the correct keyspace.
</Note>

[Global Replica Credentials](/docs/vitess/scaling/replicas#1-create-a-global-replica-credential-recommended) are not currently supported in this context. You can still target replicas instead of your primary with `@replica`, but it will not automatically route the query to the *closest* replica.

## Framework examples

Using `@primary` is simple, but there are slight variations for each framework. The following code snippets show how to target `@primary` in some of the popular languages/frameworks. If you don't see your framework on here and are unsure of how to proceed, please [reach out to support](https://docs/support.planetscale.com).

### MySQL CLI

```sql  theme={null}
mysql -h aws.connect.psdb.cloud -D @primary -u your_username -p pscale_pw_xxxxxxxxxxxxxxx --ssl-mode=VERIFY_IDENTITY --ssl-ca=/etc/ssl/cert.pem
```

### Rails

```ruby  theme={null}
# database.yml
production:
  <<: *default
  username: <%= Rails.application.credentials.planetscale&.fetch(:username) %>
  password: <%= Rails.application.credentials.planetscale&.fetch(:password) %>
  database: "@primary"
  host: <%= Rails.application.credentials.planetscale&.fetch(:host) %>
  ssl_mode: verify_identity
```

### Django

```python  theme={null}
# Connect to the database
connection = MySQLdb.connect(
  host=os.getenv("DATABASE_HOST"),
  user=os.getenv("DATABASE_USERNAME"),
  passwd=os.getenv("DATABASE_PASSWORD"),
  db="@primary",
  autocommit=True,
  ssl_mode="VERIFY_IDENTITY",
  ssl={"ca": os.getenv('SSL_CA')}
)
```

### Laravel

```php expandable theme={null}
'mysql' => [
  'driver' => 'mysql',
  'url' => env('DB_URL'),
  'host' => env('DB_HOST', '127.0.0.1'),
  'port' => env('DB_PORT', '3306'),
  'database' => '@primary',
  'username' => env('DB_USERNAME', 'root'),
  'password' => env('DB_PASSWORD', ''),
  'unix_socket' => env('DB_SOCKET', ''),
  'charset' => env('DB_CHARSET', 'utf8mb4'),
  'collation' => env('DB_COLLATION', 'utf8mb4_unicode_ci'),
  'prefix' => '',
  'prefix_indexes' => true,
  'strict' => true,
  'engine' => null,
  'options' => extension_loaded('pdo_mysql') ? array_filter([
    PDO::MYSQL_ATTR_SSL_CA => env('MYSQL_ATTR_SSL_CA'),
   ]) : [],
],
```

### PHP MySQLi

```php expandable theme={null}
# .env
DATABASE_HOST=aws.connect.psdb.cloud
DATABASE=@primary
DATABASE_USERNAME=username
DATABASE_PASSWORD=password

# index.php
// Connect to PlanetScale using credentials stored in environment variables
$mysqli = mysqli_init();
$mysqli->ssl_set(NULL, NULL, "/etc/ssl/cert.pem", NULL, NULL);
$mysqli->real_connect(
  $_ENV["DATABASE_HOST"],
  $_ENV["DATABASE_USERNAME"],
  $_ENV["DATABASE_PASSWORD"],
  "@primary",
);
```

### PHP PDO

```php  theme={null}
# .env
DATABASE_HOST=aws.connect.psdb.cloud
DATABASE=@primary
DATABASE_USERNAME=username
DATABASE_PASSWORD=password

# index.php
// Use env variables to connect to the database
$dsn = "mysql:host={$_ENV["DATABASE_HOST"]};dbname={$_ENV["DATABASE"]}";
$options = array(
  PDO::MYSQL_ATTR_SSL_CA => "/etc/ssl/cert.pem",
);
$pdo = new PDO($dsn, $_ENV["DATABASE_USERNAME"], $_ENV["DATABASE_PASSWORD"], $options);
```

### Elixir

```elixir expandable theme={null}
defmodule Connect do

  def main do

    hostname = "aws.connect.psdb.cloud"

    {:ok, pid} = MyXQL.start_link(username: System.get_env("DATABASE_USERNAME"),
      database: "@primary",
      hostname: System.get_env("DATABASE_HOST"),
      password: System.get_env("DATABASE_PASSWORD"),
      ssl: true,
      ssl_opts: [
        verify: :verify_peer,
        cacertfile: CAStore.file_path(),
        server_name_indication: String.to_charlist(hostname),
        customize_hostname_check: [
          match_fun: :public_key.pkix_verify_hostname_match_fun(:https)
        ]
      ]
    )

    {:ok, res} = MyXQL.query(pid, "select * from users limit 1")
    IO.input(res)
    IO.puts "Successfully connected to PlanetScale!"
  end
end

Connect.main
```

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Vindexes
Source: https://planetscale.com/docs/vitess/sharding/vindexes

When configuring a horizontally [sharded](/docs/vitess/sharding) keyspace, one of the most important decisions to make is _how_ to distribute the data across shards.

<Note>
  You can create sharded keyspaces on any plan by adding a new sharded keyspace using the [Clusters page](/docs/vitess/cluster-configuration) and running an [unsharded to sharded workflow](/docs/vitess/sharding/sharding-quickstart) in your dashboard.

  If you would like additional support from our expert team, our [Enterprise plan](/docs/planetscale-plans#planetscale-enterprise-plan) may be a good fit. [Get in touch](https://planetscale.com/contact) for a quick assessment.
</Note>

To do this, we must select a **Vindex** (Vitess index) for each table.

A **[Vindex](https://vitess.io/docs/api/reference/features/vindexes/)** provides a way to map incoming rows of data to the appropriate shard in your keyspace.
Similar to how every MySQL table must have a primary key, every sharded table must additionally have a **primary Vindex**.
The primary Vindex is the Vindex that determines which shard each row of data will reside on.
Additional **secondary Vindexes** can be created to help speed up queries in a sharded environment.
This is analogous to using a secondary index in MySQL.

## How do Vindexes work?

Vindexes come in two main types: **functional** and **lookup**.

### Functional Vindex

A functional Vindex is a Vindex that takes one or more column value(s) from an incoming row, executes a function, and produces a 64 bit number as output (keyspace ID).
The input column will typically be something like your `user_id` or `tenant_id` column in your sharded table.
Each time a row is inserted into the table, Vitess will extract this column value and run it through the Vindex function.
This could be the identity function (a function that returns its input), but there are also a number of [built-in functions](https://vitess.io/docs/api/reference/features/vindexes/#predefined-vindexes) to choose from.
`xxhash` is a commonly used one, as it does a good job of keeping data spread out evenly amongst shards.
The function will return a keyspace ID, which tells Vitess which shard the row should be saved to.

### Lookup Vindex

A lookup Vindex is a Vindex that uses a lookup table to store the mapping between the column value and the keyspace ID.
In this setup, you would create a dedicated lookup table with at least two columns: `from` and `to`.
The `from` column should contain the possible column values that we need to map to shards, and the `to` column contains the corresponding `keyspace ID` for that shard.
Each time a new row needs to be inserted, Vitess looks up the mapping in the table and uses the resulting `keyspace ID` to send it to the appropriate shard.
Lookup indexes tend to be slower than functional ones, therefore functional is preferred when possible.

## Keyspace IDs

In Vitess terminology, "keyspace ID" does not mean "The ID of a keyspace."
Rather, it is an identifier used to determine which shard a row should be stored on *within* a keyspace.

When a single keyspace contains multiple shards (horizontal sharding), each shard must follow the [sharding naming conventions](https://vitess.io/docs/concepts/shard/#shard-naming).
Each will be named according to the range of keyspace IDs it is responsible for, formatted like `BEGIN_ID-END_ID`.
The first shard will have an empty `BEGIN_ID` and the last one an empty `END_ID`.
For example, if we wanted to set up a keyspace with four shards, and each one is responsible for exactly one quarter of the keyspace IDs, we should name them: `-40`, `40-80`, `80-c0`, `c0-`.
You can find more information about shard naming on the [Vitess website](https://vitess.io/docs/concepts/shard/).

## Primary Vindexes

Every horizontally sharded table must have a primary Vindex.
Let's consider the following `exercise_log` table and determine which column we should use for our primary Vindex.

```sql  theme={null}
+-------------+-----------------+------+-----+---------+----------------+
| Field       | Type            | Null | Key | Default | Extra          |
+-------------+-----------------+------+-----+---------+----------------+
| log_id      | bigint unsigned | NO   | PRI | <null>  | auto_increment |
| user_id     | bigint unsigned | NO   |     | <null>  |                |
| exercise_id | bigint unsigned | NO   |     | <null>  |                |
| gym_id      | bigint unsigned | NO   |     | <null>  |                |
| reps        | smallint        | YES  |     | <null>  |                |
| created_at  | datetime        | YES  |     | <null>  |                |
| edited_at   | datetime        | YES  |     | <null>  |                |
| deleted_at  | datetime        | YES  |     | <null>  |                |
| notes       | varchar(1024)   | YES  |     | <null>  |                |
+-------------+-----------------+------+-----+---------+----------------+
```

Each `exercise_log` row has an auto incrementing `log_id`.
Each row will also have other IDs acting as foreign keys to other tables:

<Note>
  You can't use an `auto_increment` primary key in a sharded keyspace.
  Instead you'll need to use a [sequence table](/docs/vitess/sharding/sequence-tables).
</Note>

* `user_id` is a foreign key to a `user` table
* `exercise_id` is a foreign key to an `exercise` table
* `gym_id` is a foreign key to a `gym` table

In some cases, using the `primary key` as the primary Vindex can be helpful, so let's start by considering `log_id`.
For each row inserted into this table, we could take the `log_id`, run it through an `xxhash` functional Vindex, and then the result would be used by Vitess to assign the row to a shard.
This would cause all of the rows to get evenly distributed across all shards, which is good for insert performance.

However, when considering what to choose for the primary Vindex, it is important to consider the types of queries your database is expected to fulfill.
We may have queries that need to fetch sequences of logs for a given user.
Something along the lines of:

```sql  theme={null}
SELECT *
FROM exercise_log
  WHERE user_id = $SOME_USER_ID
  AND created_at BETWEEN $START_CREATED_AT AND $END_CREATED_AT;
```

If we use `log_id` as the primary Vindex, the rows will be seemingly "randomly" distributed across shards.
Each one of these queries would have to search all shards for the log messages of a single user.
This would hurt performance severely if this query is executed frequently.

Instead, we could choose `user_id` for our primary Vindex.
A given `user_id` will always produce the same output hash, meaning all of the logs for user X will end up on the same shard.
This means that when a user wants to view their exercise log history, a single shard will be able to fulfill the query, leading to improved performance.

Queries are more performant when performed on a single shard. In other words, cross-shard queries are *less* performant than single-shard queries.

## Secondary Vindexes

Secondary indexes are optional, but may be created to help speed up lookups for queries in your workload.

Consider the case where we want to look at sequences of the log that happened for all users at a specific gym.
Such a query would look something like this:

```sql  theme={null}
SELECT reps, notes, created_at
FROM exercise_log
  WHERE gym_id = $SOME_GYM_ID
  ORDER BY created_at DESC
  LIMIT 1000;
```

In this case, we are not doing any filtering by the primary Vindex, and instead using `gym_id`.
Without a secondary index, Vitess has no way of knowing which shard(s) to look at to get the rows.
To execute this, it would have to send a query to all shards, aggregate the results, and then return to the client.
We can set up a secondary lookup Vindex to help with this.

To keep the example simple, we will make an over-simplification and assume that each user goes to exactly one gym and never changes gyms.
We will need to create a lookup table in MySQL to store the mapping between gyms and logs.
The table will have two columns, `from` and `to`.

```sql  theme={null}
CREATE TABLE gym_id_exercise_log_lookup_vindex (
  from BIGINT UNSIGNED,
  to BINARY(64),
  primary key(from)
);
```

The `from` column can be populated with the `gym_id`s from the `gym` table, and the `to` colum will store the corresponding keyspace ID.
This could then be used with a Vindex function like `consistent_lookup_unique` to fulfill queries and send requests to the appropriate shards when executing the query.

## Built-in functional Vindexes

Vitess supports a [many predefined functional Vindexes](https://vitess.io/docs/api/reference/features/vindexes/#predefined-vindexes).
Here we'll cover a subset, and in what scenarios you might consider using them.

### `hash` and `xxhash`

These are popular choices, particularly for use as a primary Vindex.
They are especially useful when creating a Vindex from an incrementing ID.
This is because a sequence of numbers that are close to each other will be assigned wildy different outputs, spreading them out nicely across shards.
For example, user IDs `1`, `2`, `3`, `4`... might hash to a sequence like `3e8a`, `9b11`, `de1e`, `781b`...

`hash` should generally be avoided in favor of `xxhash`
`hash` uses the DES hashing algorithm whereas `xxhash` uses the xxhash64 algorithm.
DES was designed to be cryptographically secure and xxhash64 is not, leading to xxhash64 having better performance.
If you do not need cryptographic security for your keyspace ID, use `xxhash`.

### `numeric`

This acts as the identify function.
Since keyspace IDs need to be 64 bit numeric values, this only works for a column that are 64 bits or smaller.
A numeric Vindex function has the advantace of being the most performant.

This Vindex can be useful when you already have a good numeric key to hash on inherent in the schema of your database.
This might be a good choice if you have a database with multiple tenants and you want to have one tenant per-shard.
If you already have a `tenant_id`, you can use this value directly as your key to shard on.

### `consistent_lookup` and `consistent_lookup_unique`

These are used for lookup Vindexes, and should be used in favor of the older `lookup` and `lookup_unique` Vindexes.
These provide better performance, as they do not require two-phase commit to look up a value.
If you need to do lookup-table based Vindexing, we recommend one of these two options.

### `numeric_static_map`

This Vindex allows you to specify a static mapping between input column values and keyspace IDs via a JSON file.
This is a good option if you have a small set of mappings that can be easily managed in JSON.
If the set of mappings is large, you can opt for something like `consistent_lookup` instead.

## Next steps

This page mostly focuses on Vindexes conceptually.
If you want to see an example of how to update your Vschema to use a Vindex, check out the [sharding walkthrough](/docs/vitess/sharding/sharding-quickstart).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# VSchema
Source: https://planetscale.com/docs/vitess/sharding/vschema

PlanetScale databases are powered by Vitess.

## VSchema overview

Each Vitess cluster can have one or more [keyspaces](https://vitess.io/docs/concepts/keyspace/).
For unsharded databases, there is a 1:1 relationship between a keyspace and a database within MySQL.
For sharded databases, a single keyspace can map to multiple MySQL databases under the hood.

Each keyspace in your PlanetScale database has an associated [VSchema](https://vitess.io/docs/api/reference/features/vschema/).
The VSchema contains information about how the keyspace is sharded, sequence tables, and other Vitess schema information.

## Viewing VSchema

In order to view your VSchema, first go to the "Branches" tab in the PlanetScale app.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/enterprise/tabs.png?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=05fdd7b66a6b41b6a08808293f087986" alt="PlanetScale app tab bar" data-og-width="3660" width="3660" data-og-height="678" height="678" data-path="docs/images/assets/docs/enterprise/tabs.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/enterprise/tabs.png?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=cf847ba05c261a718af590c87460c6b4 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/enterprise/tabs.png?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=98cc0feee3b9611e2008a3b4f1b8528a 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/enterprise/tabs.png?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=1f5ac50bd86a6f3585b03c46ee974b88 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/enterprise/tabs.png?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=7063a2616744fde48d52fc75a411379a 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/enterprise/tabs.png?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=bfe9f01d0912069a7ab3a33f5882435d 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/enterprise/tabs.png?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=1b060ca9ab747db94b4f7ca6dc58673f 2500w" />
</Frame>

Click on the branch you would like to view the VSchema for.
Then, select the keyspace and expand out the "Configuration Files" drop-down.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/keyspace.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=9f5f37b5d3a1e021639ce95d428df246" alt="PlanetScale keyspace selection and configuration files drop down" data-og-width="3734" width="3734" data-og-height="1844" height="1844" data-path="docs/images/assets/docs/enterprise/keyspace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/keyspace.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=6c1af78af9c842c81974c4ac1c0fc05e 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/keyspace.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=32adee6fab4901584c60a8f8ad296241 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/keyspace.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=0f2bfb59897cc01d804149af86121e8d 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/keyspace.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=3c9b0efc073487d0fbee40edee2afbe9 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/keyspace.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=c323d1a04f276caccddbbab4292ad59c 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/keyspace.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=9997ac30601fd8f0bf572cc7eb24f074 2500w" />
</Frame>

From here, you can inspect your VSchema configuration JSON file.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/enterprise/vschema.png?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=dc4aa182fcf0fad12298ecbf5b300186" alt="VSchema JSON view" data-og-width="2358" width="2358" data-og-height="1926" height="1926" data-path="docs/images/assets/docs/enterprise/vschema.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/enterprise/vschema.png?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=e8d7e3f995e9f4d25965764a863d0362 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/enterprise/vschema.png?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=7a31e13e76332c8e4cb971be062bdce2 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/enterprise/vschema.png?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=a9000cfdf8e34804ffa194c04b2c4cc6 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/enterprise/vschema.png?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=1374c6eee8f94b9a6651eca0c6f4f67e 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/enterprise/vschema.png?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=0b55c34ce42efb1dbf25186029b6d46d 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/enterprise/vschema.png?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=ef1a5483966ed4faab7b993cf6c2ff34 2500w" />
</Frame>

## Modifying VSchema

You must have a sharded keyspace in order to make VSchema changes.

If you have a database with at least one sharded keyspace, you can modify its VSchema either in the [Clusters](/docs/vitess/cluster-configuration) tab in the dashboard, from the [pscale CLI](/docs/cli/keyspace), or using `ALTER VSCHEMA ...` commands.

### Using the Clusters page

We do not recommend modifying the VSchema directly on your production branch. In fact, it is not possible to do if you have [safe migrations](/docs/vitess/schema-changes/safe-migrations) enabled (as recommended). Instead, to modify the VSchema, you should first [create a new development branch](/docs/vitess/schema-changes/branching). Once you have your branch ready, follow these steps:

<Steps>
  <Step>
    To update on the Clusters page, select your new development branch from the dropdown at the top, and then select the
    keyspace below that has the VSchema you'd like to modify.
  </Step>

  <Step>Next, click the tab labeled "VSchema".</Step>

  <Step>
    Modify the VSchema configuration JSON file as needed. Refer to the [VSchema
    documentation](/docs/vitess/sharding/vschema) for more information about the available options.
  </Step>

  <Step>
    When finished, click "Save changes". We will validate your VSchema, and if it is valid, the changes will be saved.
    If there are errors, we will warn you here to change them before saving.
  </Step>

  <Step>
    Go back to your "Branches" tab and click on the development branch that you modified. You should see a note on the
    right that says "Updated VSchema configuration" which lets you know the VSchema(s) for this branch has been
    modified.
  </Step>

  <Step>
    From here, go through the normal [deploy request process](/docs/vitess/schema-changes/deploy-requests) to deploy
    this change to production.
  </Step>
</Steps>

Once your change is deployed to production, you can come back to the Clusters page, switch to your production branch, and view the updates to your VSchema. You can also click the "Changes" tab to see information, such as the resize event, status, and start/end time for any previous changes to the VSchema.

### Using `ALTER VSchema`

PlanetScale recommends making all such modifications in a development branch.
When ready, you can make a deploy request to get the changes into production.
Consider the following database with two keyspaces.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/sharded-keyspace.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=1f749d13c6772db5be0938b808d19ad6" alt="Sharded keyspace" data-og-width="3648" width="3648" data-og-height="2060" height="2060" data-path="docs/images/assets/docs/enterprise/sharded-keyspace.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/sharded-keyspace.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=32b3289924e54ee98b412b6aa3fd7f5b 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/sharded-keyspace.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=250b42fe84f59c901ec9b12a70e74053 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/sharded-keyspace.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=394828d9c2a9a40cd1fbe5a440ae57f4 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/sharded-keyspace.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=fdfbfd0d5d0ed50161a2d18b134a75ff 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/sharded-keyspace.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=4d0d870bddbd5a0c2b4fdd73570a60d4 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/sharded-keyspace.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=434c98012eb0664937167881aa4578d9 2500w" />
</Frame>

`sharded` is a sharded keyspace with two shards and `tweeter` is unsharded.
Also note that safe migrations are enabled.
In order to make a VSchema change for the production branch in this configuration, we first must create a new branch.
We'll call it `add-tweets`

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/new-branch.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=2e915c3aeb14e8f103969926942a6d1c" alt="New branch" data-og-width="5092" width="5092" data-og-height="1613" height="1613" data-path="docs/images/assets/docs/enterprise/new-branch.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/new-branch.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=22e928a3462687b2a9ddba83eb5209dc 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/new-branch.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=05cd7d77d98f239bfc61a769f00e2e3a 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/new-branch.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=ec977a1fcb478aa760438f86ad574f58 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/new-branch.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=2e519a77dde5c15a23f89c5572801808 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/new-branch.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=252d56fcf60a980b9000e7b82d612da8 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/new-branch.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=2ac367dfbbb17b14b289219267d2c74a 2500w" />
</Frame>

On this branch you can make your VSchema and schema changes.
In this case, we'll create a new table called `tweets` in the sharded keyspace and also update the VSchema.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/enterprise/tweets-table.png?fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=c156d09dd87eb488cd6f8c4c3a1b96d8" alt="Create the tweets table" data-og-width="5102" width="5102" data-og-height="2654" height="2654" data-path="docs/images/assets/docs/enterprise/tweets-table.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/enterprise/tweets-table.png?w=280&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=b67dcbb3c8a746426e6424b270fe518c 280w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/enterprise/tweets-table.png?w=560&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=84295a41f91fa556c01e161ca1388b09 560w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/enterprise/tweets-table.png?w=840&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=421a6fd180d0708fe69b11d129a2ca6a 840w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/enterprise/tweets-table.png?w=1100&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=f250fe6640308969c79aaa6f67b1711e 1100w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/enterprise/tweets-table.png?w=1650&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=d4ef01741c364f02747ae69f0de78601 1650w, https://mintcdn.com/planetscale-cad1a68a/Iq7wyPq-ODOtm89r/docs/images/assets/docs/enterprise/tweets-table.png?w=2500&fit=max&auto=format&n=Iq7wyPq-ODOtm89r&q=85&s=1aba21a61ae5a9f4273eedac1eaf332b 2500w" />
</Frame>

We will also create a sequence table in the unsharded keyspace, and update the VSchema accordingly:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/sequence-table.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=bcde96b109fc62b9faa8981715821ac3" alt="Create the sequence table" data-og-width="5100" width="5100" data-og-height="2650" height="2650" data-path="docs/images/assets/docs/enterprise/sequence-table.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/sequence-table.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=284cbebd84a952c0e022330534625638 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/sequence-table.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=a66cf7798c1ae22f5506cf3f7ec6c845 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/sequence-table.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=a569476cf7c2dd86b2d7f8bd78d733b0 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/sequence-table.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=187cedff80264c995b70387b7a885925 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/sequence-table.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=bac014c4d9fcf3b532d955385aa427f0 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/sequence-table.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=68893c162351632d9a0ea9b0926ebe56 2500w" />
</Frame>

We have now made updates both to our Vitess VSchema and MySQL schema.
To get these changes into production, navigate to the "Branches" page and select the `add-tweets` branch.
Here, you will be presented with a diff of both the VSchema and schema changes:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/schema-diff.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=157793f483cdc6bd563c50849c94eb4a" alt="Schema diff" data-og-width="1850" width="1850" data-og-height="2432" height="2432" data-path="docs/images/assets/docs/enterprise/schema-diff.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/schema-diff.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=3f78be609b263b4c65d3d08b170ff534 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/schema-diff.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=bdd14416d4fedbd014b611c59e51a371 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/schema-diff.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=2b9107b483159059aaf90e6aa7ff3f9c 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/schema-diff.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=dcbe388ccac476c5c8aa01e3657c57ff 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/schema-diff.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=2fcc7cce1503427287a7d2ba8860c542 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/schema-diff.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=ada16d70fb4c028403b937b531e82ae8 2500w" />
</Frame>

Click "Create deploy request."
The deploy request should indicate that it is going to apply both VSchema and Schema changes:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/deploy-request.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=1a177bd74147e4f51c1551e5aa8c8489" alt="Deploy request" data-og-width="2890" width="2890" data-og-height="1162" height="1162" data-path="docs/images/assets/docs/enterprise/deploy-request.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/deploy-request.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=fe2b6570f90d387512bc3f9867a6b9d9 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/deploy-request.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=df9d994f1a1c984b31ccf807711284a5 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/deploy-request.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=3adf240b436db7daad56b5306202ed88 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/deploy-request.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=864ae5a6ceb2267dfede973a8f212d9d 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/deploy-request.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=7404c3a8d0688ad8de974b5a72d8a5e8 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/enterprise/deploy-request.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=533c1e92271d67c79e554b9fd219534f 2500w" />
</Frame>

Click "Deploy changes."
Once complete, the Vschema and schema changes will be applied to your production branch.

<Warning>
  You can only use `ALTER VSCHEMA ...` commands in branches that have at least one sharded keyspace.
  If you do not, you will get an error message when attempting to execute `ALTER VSCHEMA ...`.
</Warning>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Strategies for maintaining referential integrity
Source: https://planetscale.com/docs/vitess/strategies-for-maintaining-referential-integrity



If you choose to not use foreign key constraints, cascading actions need to be addressed via code instead of letting the database engine handle them for you. This document outlines some recommended strategies for building a system that handles those actions for you.

<Note>
  If you want to use foreign key constraints instead for referential integrity, enable [foreign key constraint](/docs/vitess/foreign-key-constraints) support in your database settings page. If you are unsure if you should use them, the [foreign key constraints documentation](/docs/vitess/foreign-key-constraints) covers some of the advantages and disadvantages.
</Note>

The following examples will use the concept of a recipe manager with the following schema. Samples will be provided in SQL, so they are relatively universal regardless of the language or framework used.

The same scenario will be used for each suggestion below: deleting a recipe and its associated records.

## Executing actions inline

The first method you could use to address stale records is to write the necessary code to handle cascading actions in line with the main execution.

For example, if you are deleting a recipe using an API, you could extend the route handler to delete the associated ingredients and steps as well.

```sql  theme={null}
-- Main action
DELETE FROM recipes WHERE id = 123

-- Secondary actions
DELETE FROM ingredients WHERE recipe_id = 123
DELETE FROM steps WHERE recipe_id = 123
```

The benefit this approach provides is that it’s relatively straightforward using the systems that are already in place. The downside is in situations where a large number of records need to be deleted or updated, you might make your users wait for longer than needed as the cascading actions are handled.

## Scheduled jobs

The second approach is using a system that will scan the database at regular intervals for cascading actions that need to be performed.

For example, you might set up Cron with a script that will execute queries against the database to find records that are soft-deleted, and run the necessary SQL to delete the ingredients and steps, then the recipe itself.

The main action would flag the record in the `recipes` table as being soft-deleted:

```sql  theme={null}
-- Flag the record as requiring deletion.
UPDATE recipes SET is_deleted = true WHERE id = 123
```

A scheduled task could then execute the following statements to find the records requiring cleanup, and delete those records:

```sql  theme={null}
-- Find the recipes that have been deleted.
SELECT id FROM recipes WHERE is_deleted = true

-- Then for each recipe delete the necessary records.
DELETE FROM ingredients WHERE recipe_id = 123
DELETE FROM steps WHERE recipe_id = 123
DELETE FROM recipes WHERE id = 123
```

The benefit this approach provides is that the main execution point of your application (ex: an API) would return quicker as you are simply updating a single value in the database. This prevents the user from waiting for those records to be cleaned up.

The downside is that this requires an additional system with its own associated code that would require maintenance. While `DELETE` operations are relatively straightforward, addressing `UPDATE` operations may be a challenge. There is also an inherent delay where stale records would exist in the database between script executions.
You can see a full example of how to handle this in Rails in our [Ruby on Rails: 3 tips for deleting data at scale blog post](https://planetscale.com/blog/ruby-on-rails-3-tips-for-deleting-data-at-scale).

## Asynchronous cleanup using queues

The final recommended strategy would be to use a message queue like AWS SQS or RabbitMQ. The application would send messages to the queue so that a handler can perform the cascading operations asynchronously.

For example, you may set up an AWS SQS queue to receive messages from your API on what actions were performed on a particular recipe or what data was changed. When a message is dropped on the queue, a Lambda function is triggered to perform the required operations.

In this scenario, your API may start by deleting the recipe record:

```sql  theme={null}
DELETE FROM recipes WHERE id = 123
```

The API might then drop a message into the queue in the following format:

```json  theme={null}
{
  "id": 123,
  "action": "deleted"
}
```

The Lambda function that receives the message from the API could then perform the cleanup operations:

```sql  theme={null}
DELETE FROM ingredients WHERE recipe_id = 123
DELETE FROM steps WHERE recipe_id = 123
DELETE FROM recipes WHERE id = 123
```

This approach provides the greatest number of benefits. Cascading operations are asynchronous, so the caller wouldn’t need to wait. It scales well since any number of serverless functions may be started to handle the load. Cleanup operations would be executed in near real-time, as long as it takes for a message to enter the queue and the serverless function to pick it up. It is also decoupled and easily fits the serverless model very well.

The primary downside is that it increases the complexity of the overall application and requires the largest number of moving parts, as well as the knowledge to support these systems.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale terminology
Source: https://planetscale.com/docs/vitess/terminology

Here we provide definitions for technologies and concepts you will see throughout our documentation and product. Some of these are common terms in the databases space, while others are Vitess or PlanetScale specific.

## Database technologies

### MySQL

MySQL is the world's most popular relational database.
Many of the worlds largest web applications are powered by MySQL behind the scenes including Slack, X, GitHub, JD.com, and many others.
MySQL is open-source, has existed for 30+ years, and is currently maintained by Oracle.

### Vitess

[Vitess](https://vitess.io) is an open-source project created by engineers at YouTube in the early 2010's.
It was created to solve their MySQL scalability challenges.
Vitess works in tandem with MySQL and provides proxying, orchestration, monitoring, and sharding capabilities.
A Vitess-powered MySQL database can scale to petabytes of data and millions of queries per second.

### PlanetScale

PlanetScale is a complete database platform that emphasizes reliability, scalability, and developer productivity.
Every PlanetScale database is powered by Vitess and MySQL.
PlanetScale makes it easy to spin up, resize, manage, and work with Vitess-powered databases both for small organizations and large enterprises. Additionally, PlanetScale employs the majority of the Vitess core maintainers.

## PlanetScale concepts

### Database

In the PlanetScale app, users can create one or more **Databases**.
Each database is associated with an **Organization**.
A database is an individual Vitess cluster that uses MySQL under the hood.
If you've used vanilla MySQL in the past, you are probably used to being able to have a single MySQL server manage multiple distinct databases (EG: `CREATE DATABASE db1;`, `CREATE DATABASE db2;`...).
You cannot use `CREATE DATABASE` to create multiple logical databases within a single PlanetScale database instance.
However, you can achieve something similar by creating multiple **keyspaces**.

### Keyspace

A **Keyspace** represents a single, logical database.
When you create a new PlanetScale database, it contains a single, default keyspace of the same name as your database.
More keyspaces can be added using the [Clusters](/docs/vitess/cluster-configuration) page.
Each keyspace has its own primary and replicas.
The "keyspace" terminology [comes from Vitess](https://vitess.io/docs/concepts/keyspace/).
[Read more about keyspaces](/docs/vitess/sharding/keyspaces).

### Branch

A typical flow on GitHub is to work on new feature in branches.
When development is complete, the developer can create a pull request which then gets merged into `main`.
PlanetScale allows you to work with your database in a similar fashion via **branches**.
Every database has a default branch, often named `main`.
You can create branches of your database, in which you can make schema changes without affecting your production tables and data.

### Deploy request

When you are ready to bring the changes from a branch into your production database, you can create a **Deploy request** (or **DR** for short).
This is akin to a pull request on GitHub.
Deploy requests can be created, reviewed by peers, and deployed to the main database.
Deploy requests give you the ability to change the schema of your database with 0 downtime.
We also give you the ability to revert deploy requests for a short window of time after being deployed, in case you encounter an issue with your application.

## Vitess concepts

### VTGate

Every PlanetScale database comes with at least 3 **VTGates**.
A VTGate is the layer of Vitess that acts as a proxy between your application servers and the MySQL instances.
VTGates play an important role in allowing your applications to treat the database as a single server, even if your data is actually spread across many keyspaces and shards.
VTGates handle query buffering, distributing your queries across shards, and other important functions for high-availability.

### Shard

Vitess keyspaces can be unsharded or [sharded](/docs/vitess/sharding).
A sharded keyspace is one where the data of tables is spread out across multiple primaries, rather than all going to one.
The way data is sharded is specified via the [Vindex](/docs/vitess/sharding/vindexes).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale database error reference
Source: https://planetscale.com/docs/vitess/troubleshooting/errors



This documentation covers some commonly encountered errors and approaches for addressing them.

<Note>
  If you are facing issues or have questions that were not answered in the documentation, the best course of action is to [open a support ticket](https://planetscale.com/contact).

  Additionally, you can find some broader limitations in the [PlanetScale system limits documentation](/docs/vitess/troubleshooting/planetscale-system-limits).
</Note>

## MySQL and Vitess errors

All PlanetScale databases are powered by Vitess and MySQL.

Many errors you encounter will contain a substring of the form `SQL Error [XXXX]` where `XXXX` is some integer number.
Often, these are errors that are passed along back to the client from MySQL.
There are hundreds of MySQL error codes documented on the [MySQL error codes page](https://dev.mysql.com/doc/mysql-errors/8.0/en/server-error-reference.html).

For example, say you encounter this error:

```
Error synchronizing data with database Reason:
SQL Error [1364] [HY000]:
target: unigate.-.primary:
vttablet: rpc
error: code = Unknown desc = (errno 1364) (sqlstate HY000) (CallerID: unsecure_grpc_client):
Sql: ...
BindVars: {}
```

The second line says `SQL Error [1364]`, which corresponds to the [ER\_NO\_DEFAULT\_FOR\_FIELD](https://dev.mysql.com/doc/mysql-errors/8.0/en/server-error-reference.html#error_er_no_default_for_field) error in MySQL.
Knowing this, you can make the appropriate changes to your query or schema to mitigate the error.
You can also search the MySQL docs and other online sources using the error code, as there is a long history of MySQL questions and answers online.

One specific code you may come across is `SQL Error [1105]`, which represents an [unknown error](https://dev.mysql.com/doc/mysql-errors/8.0/en/server-error-reference.html#error_er_unknown_error) in MySQL.
On PlanetScale, it's likely that such an error is actually coming from Vitess, which also has an [error documentation page](https://vitess.io/docs/api/reference/errors/query-serving/).
Note that a number of older Vitess errors used the `1105` code, but there are many other errors documented there as well.
We recommend you reference this for Vitess-specific errors.

Below, we document more common errors from our customers, and provide suggestions for how to address them.

| **Error**                                                                                                                                  | **Reason**                                                                                                                                                                                                                        | **How to address**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| ------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `ResourceExhausted desc = transaction pool connection limit exceeded`                                                                      | Every PlanetScale database has limit on the number of concurrent transactions it can process. This message indicates you are hitting the limit.                                                                                   | This can be resolved in several ways. One option is to make changes to your application code that reduce the number of long-running transactions. If this is not feasible, you can also [size up your database](/docs/vitess/cluster-configuration#adjust-your-cluster-size). This can allow long-running transactions to execute quicker, and you also may get a higher concurrent transaction limit.                                                                                                                                                                                      |
| `primary is not serving, there is a reparent operation in progress`                                                                        | Your primary database server is unavailable. This often happens due to an OOM error (Out Of Memory). You also may see this on dev branches if you happen to query them during an upgrade.                                         | In the short term, if this error message persists, we recommend you [reach out to support](https://planetscale.com/contact?initial=support). If this was caused by an OOM, you should see if there are changes you can make to your queries to reduce memory pressure on your database. This could mean consolidating queries or building indexes to reduce the number of pages needing to be brought into memory during query execution. If this is not possible, you will likely need to [upgrade](/docs/vitess/cluster-configuration#adjust-your-cluster-size) to a larger cluster size. |
| `vttablet: (errno 2013) due to context deadline exceeded, elapsed time: ...`                                                               | Your query or transaction exceeded the default [20 second per-transaction timeout](/docs/vitess/troubleshooting/planetscale-system-limits#query-limits).                                                                          | Consider options to reduce the time needed for the query/transaction in question to execute. Things that could help include: adding an index to speed up the query, breaking a large multi-query transaction up into multiple shorter ones. If you need some long-running transactions for analytical purposes, consider offloading those to a tool like [Airbyte](/docs/vitess/integrations/airbyte) or [Stitch](/docs/vitess/integrations/stitch).                                                                                                                                        |
| `SQL Error [1105] [HY000]: target: platform.-.primary: vttablet: rpc error: code = Aborted desc = transaction ... (exceeded timeout: 20s)` | Max transaction time exceeded.                                                                                                                                                                                                    | See row above.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| `vttablet: rpc error: code = Aborted desc = Row count exceeded 100000`                                                                     | You have hit the [100k limit](/docs/vitess/troubleshooting/planetscale-system-limits#query-limits) for number of rows returned, updated, or deleted in a single query.                                                            | If this is coming from a `SELECT` statement, consider narrowing the search or paginating results. If it is coming from an `UPDATE` or `DELETE`, consider performing updates or deletions in smaller batches.                                                                                                                                                                                                                                                                                                                                                                                |
| `vttablet: rpc error: code = ResourceExhausted desc = Out of sort memory, consider increasing server sort buffer size (errno 1038)`        | MySQL has a [buffer specifically used for sorting](https://dev.mysql.com/doc/refman/8.4/en/server-system-variables.html#sysvar_sort_buffer_size). This error is passed up from MySQL and indicates the buffer has been exhausted. | Buffer sizes for MySQL servers on PlanetScale come pre-configured, and we do not allow you to modify the system `sort_buffer_size` parameter. You may be able to get around this error by reducing the size of the result sets being sorted, adding a covering index to avoid performing a filesort during query execution, or increasing the sort buffer size for the individual sessions the query is running in.                                                                                                                                                                         |
| `unavailable: vtgate connection error: no endpoints, after 1 attempts`                                                                     | A connection could not be made to one of your VTGates. These show up as your "load balancers" in the PlanetScale UI.                                                                                                              | This can be caused by running queries that return very large result sets, exhausting the available memory on the load balancer. Please [contact support](https://planetscale.com/contact?initial=support) to discuss options for mitigating this.                                                                                                                                                                                                                                                                                                                                           |

## PlanetScale-specific errors

You also may encounter error messages in the PlanetScale UI or on specific PlanetScale features such as [safe migrations](/docs/vitess/schema-changes/safe-migrations) or [workflows](/docs/vitess/scaling/workflows).
Here, we document a selection of those errors you may run into.

| **Error**                                                                             | **Reason**                                                                                                                                          | **How to address**                                                                                                                                                                                                                                     |
| ------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `This deploy request is not deployable`                                               | Too many schema changes in one deploy request. If you are making a large number of schema changes in a single branch, you may encounter this error. | Divide up the schema changes into multiple incremental changes, and create separate branches and deploy requests for each.                                                                                                                             |
| `Data Definition Language is not supported on branches with safe migrations enabled.` | You attempted to run DDL on a branch with [safe migrations](/docs/vitess/schema-changes/safe-migrations) enabled.                                   | We recommend you keep safe migrations enabled. If you need to change schema in such a branch, you can create a new branch, modify the schema, and then use a deploy request to bring that change into the original branch.                             |
| `not_found: branch is missing or sleeping: branch_id`                                 | The branch you are targeting is either [sleeping](/docs/plans/database-sleeping#what-is-database-sleeping) or has been deleted.                     | We recommend double checking that you are targeting the correct branch. Additionally, ensure the credentials are not tied to a deleted branch. [Reach out to support](https://planetscale.com/contact?initial=support) if you need further assistance. |

## Other errors

If you are encountering an error not listed here, we recommend you use the [MySQL](https://dev.mysql.com/doc/mysql-errors/8.0/en/server-error-reference.html) and [Vitess](https://vitess.io/docs/api/reference/errors/query-serving/) error documentation pages to narrow down your issue.
If you are unable to identify it on your own, please [reach out to support](https://planetscale.com/contact?initial=support).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# MySQL compatibility
Source: https://planetscale.com/docs/vitess/troubleshooting/mysql-compatibility

PlanetScale's Vitess product is built on top of open-source Vitess, a database clustering system for horizontal scaling of MySQL. Consequently, PlanetScale is only compatible with MySQL databases.

## Overview

PlanetScale databases run on MySQL 8. If you're [importing an existing database](/docs/vitess/imports/database-imports), PlanetScale supports MySQL database versions `5.7` through `8.0`.

New PlanetScale databases are created on MySQL 8 with character set `utf8mb4_0900_ai_ci`. PlanetScale supports `utf8`, `utf8mb4`, and `utf8mb3`, character sets. We also support `latin1` and `ascii` character sets, but do not recommend them.

## MySQL compatibility limitations

The following reference guide will cover some MySQL syntax, features, and more that PlanetScale either does not support or has limitations around. We are actively working on driving up compatibility, but it's an ongoing effort and will take some time to complete. See this [project board on GitHub](https://github.com/vitessio/docs/vitess/projects/4) to learn what the Vitess team is currently focusing on.

If you're attempting to import a database using our Import tool, there are some additional requirements that you can find in our [Database imports documentation](/docs/vitess/imports/database-imports#import-limitations).

### Queries, functions, syntax, data types, and SQL modes

<Note>
  <Icon icon="exclamation" color="orange" /> = *Limitations in support*

  <Icon icon="xmark" color="red" /> = *Not supported*
</Note>

| Statement                     | Support                           | Description                                                                                                                                                                                                                                                |
| ----------------------------- | --------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `ALTER TABLE...RENAME COLUMN` | <Icon icon="xmark" color="red" /> | Renaming columns and tables may be destructive. See our [guide for column rename recommendations](/docs/vitess/schema-changes/handling-table-and-column-renames).                                                                                          |
| `CREATE DATABASE`             | <Icon icon="xmark" color="red" /> | You cannot `CREATE` a PlanetScale database from the MySQL command line, however, this is supported in the [PlanetScale CLI](/docs/cli/database).                                                                                                           |
| `DROP DATABASE`               | <Icon icon="xmark" color="red" /> | You cannot `DROP` a PlanetScale database from the MYSQL command line, however, this is supported in the [PlanetScale CLI](/docs/cli/database).                                                                                                             |
| `JSON_TABLE`                  | <Icon icon="xmark" color="red" /> | The [`JSON_TABLE` function](https://dev.mysql.com/doc/refman/8.0/en/json-table-functions.html#function_json-table) is not yet supported. All other [JSON SQL functions](https://dev.mysql.com/doc/refman/8.0/en/json-function-reference.html) should work. |
| `PROCEDURE`                   | <Icon icon="xmark" color="red" /> | We do not support any form of [stored routines](https://dev.mysql.com/doc/refman/8.0/en/stored-routines.html).                                                                                                                                             |
| `FUNCTION`                    | <Icon icon="xmark" color="red" /> | We do not support any form of [stored routines](https://dev.mysql.com/doc/refman/8.0/en/stored-routines.html).                                                                                                                                             |
| `TRIGGER`                     | <Icon icon="xmark" color="red" /> | We do not support any form of [stored routines](https://dev.mysql.com/doc/refman/8.0/en/stored-routines.html).                                                                                                                                             |
| `EVENT`                       | <Icon icon="xmark" color="red" /> | We do not support any form of [stored routines](https://dev.mysql.com/doc/refman/8.0/en/stored-routines.html).                                                                                                                                             |
| `LOAD DATA INFILE`            | <Icon icon="xmark" color="red" /> | Loading data via [`LOAD DATA INFILE` is not supported](https://github.com/vitessio/docs/vitess/issues/2976).                                                                                                                                               |
| `KILL`                        | <Icon icon="xmark" color="red" /> | We do not support killing queries or shards from the command line.                                                                                                                                                                                         |
| `:=`                          | <Icon icon="xmark" color="red" /> | The `:=` assignment operator is not yet supported.                                                                                                                                                                                                         |
| `SET GLOBAL time_zone`        | <Icon icon="xmark" color="red" /> | The global time zone is set to UTC and can not be modified.                                                                                                                                                                                                |
| `SET GLOBAL sql_mode`         | <Icon icon="xmark" color="red" /> | The global SQL mode can not be changed permanently. Set each new session's mode instead with `SET sql_mode`.                                                                                                                                               |
| `PIPES_AS_CONCAT`             | <Icon icon="xmark" color="red" /> | Enabling this SQL mode can interfere with Vitess' evalengine parsing the SQL queries so enabling it may result in incorrect or unexpected results. Please use MySQL's standard dialect instead, e.g. `CONCAT()`.                                           |
| `ANSI_QUOTES`                 | <Icon icon="xmark" color="red" /> | Enabling this SQL mode can interfere with Vitess' evalengine parsing the SQL queries so enabling it may result in incorrect or unexpected results. Please use MySQL's standard quotation instead.                                                          |
| `WITH RECURSIVE`              | <Icon icon="xmark" color="red" /> | Experimental support for recursive common table expressions (CTEs) was introduced in Vitess 21 for `SELECT` queries.                                                                                                                                       |

## Miscellaneous

| Action                        | Support                                    | Description                                                                                                                                                                                                                               |
| ----------------------------- | ------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Empty schemas**             | <Icon icon="xmark" color="red" />          | Databases with empty schemas are invalid. You cannot deploy a schema change to production if no tables exist.                                                                                                                             |
| **Non-InnoDB Storage engine** | <Icon icon="xmark" color="red" />          | We only support [InnoDB](https://dev.mysql.com/doc/refman/8.0/en/innodb-storage-engine.html) storage engine.                                                                                                                              |
| **No applicable unique key**  | <Icon icon="xmark" color="red" />          | We require all tables have a [unique, non-null key](/docs/vitess/schema-changes/onlineddl-change-unique-keys) and that respective covered columns are shared between old and new schema.                                                  |
| **Direct DDL**                | <Icon icon="xmark" color="red" />          | We do [not allow Direct DDL](/docs/vitess/schema-changes/how-online-schema-change-tools-work) on production branches when [safe migrations](/docs/vitess/schema-changes/safe-migrations) is enabled. This includes `TRUNCATE` statements. |
| **Binary log access**         | <Icon icon="xmark" color="red" />          | PlanetScale does not currently support binlog replication to external databases.                                                                                                                                                          |
| **Large JSON documents**      | <Icon icon="exclamation" color="orange" /> | MySQL supports JSON documents up to 1 GB in size. However, we do not recommend to store more than a few MB in a JSON document for performance reasons.                                                                                    |

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Vitess system limits
Source: https://planetscale.com/docs/vitess/troubleshooting/planetscale-system-limits



<Note>
  We can sometimes manually adjust limits on a per-database level. If you are facing issues or have questions, the best course of action is to [open a support ticket](https://planetscale.com/contact).

  Additionally, you can find some solutions to specific error codes in the [Errors documentation](/docs/vitess/troubleshooting/errors).
</Note>

## Table limits

Database schemas are limited to a total of `2048` tables, including views.

Individual tables are limited to a maximum of `1017` columns each.

## Disk limits

For [**network-attached storage**](/docs/plans/planetscale-skus#network-attached-storage) databases, the disk size will auto-scale up to a maximum of 4 TB.
For [**Metal** databases](/docs/plans/planetscale-skus#metal), the disk size is dictated by what you choose when the database is created.
Disk sizes range from 110 GB to 7.4 TB.
Which sizes are available depends on the compute instance chosen.

This space is used both for user data as well as system data required to run database operations.

If your database is near this limit, some deploy request operations may not be available as they require adequate disk space to complete.

When near this limit, we recommend either [sharding your data](/docs/vitess/sharding) or distributing tables across multiple keyspaces.
For Metal databases, you can try upgrading to a larger disk size.
Please [contact us](https://planetscale.com/contact) to discuss options.

## Connection lifetime limits

Database client connections that are held open longer than `24 hours` may be terminated unexpectedly. We recommend that long-running database connections are closed and reconnected at least once per day.

## Simultaneous transaction limits

Each database has a limit on the number of simultaneous *transactions* it can process, also known as the *transaction pool*.
If you exceed the *transaction pool* setting for your database, you may encounter this error:

```
vttablet: rpc error: code = ResourceExhausted desc = transaction pool connection limit exceeded
```

This can often be mitigated by trying one of the following solutions:

A) Reduce the amount of parallelism in the requests being sent to the database. <br />
B) Shorten lengthy transactions by reducing batch sizes or making some other application-level adjustment.

If you cannot make such changes to your application, consider choosing a larger instance type with a larger transaction pool.
The exact limit varies depending on the instance type of your database.
For details, see the [plans page](/docs/plans/planetscale-skus).

## Query limits

PlanetScale has enforced some system limits to prevent long-running queries or transactions from:

* Piling up and consuming all available resources.
* Blocking other important, short-lived queries and transactions from completing.
* Overloading the database to the point where it is not recoverable without a restart.
* Blocking planned failovers and critical upgrades.

The following table details these limits:

| Type                                         | Limit  |
| :------------------------------------------- | :----- |
| Per-query rows returned, updated, or deleted | 100k   |
| Per-query result set total size              | 64 MiB |
| Per-query autocommit timeout                 | 900s   |
| Per-transaction timeout                      | 20s    |

### Recommendations for handling query limits

These limits are enforced for the safety of your database. However, we do understand you may run into a situation where the limits are a blocker. Here are some best practices for solving common issues presented by the limits:

**What should I do if I have a query that returns more than 100,000 records?**

We recommend trying to break up large queries, e.g. through [pagination](https://planetscale.com/blog/mysql-pagination).

**What should I do if I have a query that returns more than 64 MiB of data?**

If your schema currently relies on storing large amounts of variable length data within `JSON`, `BLOB`, or `TEXT` type columns that is regularly over a few MiB in size, you will want to strongly consider storing that variable length data outside of your database, such as within an object storage solution, instead.

If large values are stored within variable length data columns, it can limit the number of rows you can return.

For example, above we described a 100K limit for result sets, but if your result set's size exceeds the 64 MiB limit then you may receive an error message like the following: `resource_exhausted: grpc: received message larger than max (<RESULT_SET_SIZE_IN_BYTES> vs. 67108864)` when retrieving much less than 100K rows.

Similarly, when generating a large `INSERT` or `UPDATE` query for these types of columns you may run into the `grpc: trying to send message larger than max (<QUERY_SIZE_IN_BYTES> vs. 67108864)` error message which would require you to reduce the query's size.

**What should I do if I have a transaction that runs longer than 20 seconds?**

For transactions that are running for longer than 20 seconds, we recommend breaking these up into multiple shorter transactions. For analytics queries that are not possible to break up, see "How should I handle analytical queries?" below.

Keep your transactions small and short-lived. For transactional workloads that are inherently long-lived, consider replacing or complementing MySQL transactions with other techniques:

* For simple workloads (e.g. a `SELECT`, followed by an HTTP request, followed by an `UPDATE`), consider using optimistic locking instead of transactions.
* For complex workloads, consider using sagas to coordinate a series of small, short-lived transactions that possibly span multiple microservices.

**What should I do if I have a select query that needs to timeout faster than 900 seconds?**

For select statements only, you can provide an inline comment to kill the query if it has not returned results within the specified time in milliseconds. `select /*vt+ QUERY_TIMEOUT_MS=2000 */ sleep(3);` Please note, you can set this value shorter than 900 seconds but not greater than the 900 seconds default.

**How should I handle analytical queries?**

We recommend using [PlanetScale Connect](https://planetscale.com/blog/extract-load-and-transform-your-data-with-planetscale-connect) (via [Airbyte](/docs/vitess/integrations/airbyte) or [Stitch](/docs/vitess/integrations/stitch)) to extract your data to any compatible destination (e.g. BigQuery, Redshift, etc.). The analytics queries should then be performed at those destinations.

### OLAP mode

While it is possible to bypass these safety limits using `OLAP` mode (`SET workload = OLAP`), we do not recommend this for the reasons listed at the beginning of this doc. In OLAP mode, you may return more than 100k rows from a single query, but the 100k row limit will still apply to updates and deletes.

When the use of OLAP queries is strictly unavoidable, we recommend:

* Where possible, sending those queries to a replica. Every PlanetScale database comes with at least two replicas. Learn how to send queries to replicas using [global replica credentials](/docs/vitess/scaling/replicas#how-to-query-replicas).
* Carefully and regularly reviewing the performance of those queries with [PlanetScale Insights](/docs/vitess/monitoring/query-insights).

<Note>
  If you choose to use OLAP mode, be prepared to handle errors that may occur if the connection to PlanetScale gets interrupted.
</Note>

## Reducing table size without `OPTIMIZE TABLE`

If you delete several rows in a table, you may wish to reclaim that storage space. The MySQL [`OPTIMIZE TABLE` statement](https://dev.mysql.com/doc/refman/en/optimize-table.html) allows you to reorganize the physical storage of table data to reduce its storage requirements.

However, `OPTIMIZE TABLE` is a locking operation, so you cannot use it unless you disable [safe migrations](/docs/vitess/schema-changes/safe-migrations) in PlanetScale, which is not recommended.

Any time you create and deploy a deploy request with safe migrations on, our schema change process uses [online DDL](/docs/vitess/schema-changes/how-online-schema-change-tools-work), and in doing so, recreates the table that you are modifying — thus reclaiming the storage space.

But there are often cases where you do not need to make a schema change for a while, but you'd like to free up the space. In those cases, we suggest you do the following:

<Steps>
  <Step>
    Create a new branch.
  </Step>

  <Step>
    Run the following command:

    ```sql  theme={null}
    ALTER TABLE <TABLE_NAME> COMMENT 'Optimize table size via DR - <YYYY-MM-DD>';
    ```

    Where `<TABLE_NAME>` is the name of your table and `<YYYY-MM-DD>` is the current date.
  </Step>

  <Step>
    Create a deploy request, and merge it into production.

    If you use this option you will likely want to [throttle the deploy request](/docs/vitess/schema-changes/throttling-deploy-requests#managing-throttler-settings-per-deploy-request) running the `ALTER TABLE`.
  </Step>
</Steps>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Automatic Prisma migrations
Source: https://planetscale.com/docs/vitess/tutorials/automatic-prisma-migrations



<Note>
  This document has been updated to include the recommended Prisma and PlanetScale workflow, specifically the
  recommendation to use `prisma db push` instead of `prisma migrate dev` with shadow branches. Also, you previously
  needed to turn on the ability to automatically copy the Prisma migration metadata. You no longer need to do this. Read
  more below.
</Note>

## Introduction

In this tutorial, we're going to learn how to do Prisma migrations in PlanetScale as part of your deployment process using `prisma db push`.

### Quick introduction to Prisma's db push

From a high level, [Prisma's `db push`](https://www.prisma.io/docs/orm/reference/prisma-cli-reference#db-push) introspects your PlanetScale database to infer and execute the changes required to make your database schema reflect the state of your Prisma schema. When `prisma db push` is run, it will ensure the schema in the PlanetScale branch you are currently connected to matches your current Prisma schema.

We recommend `prisma db push` over `prisma migrate dev` for the following reasons:

PlanetScale provides [Online Schema Changes](/docs/vitess/schema-changes/how-online-schema-change-tools-work) that are deployed automatically when you merge a deploy request and prevents [blocking schema changes](/docs/vitess/schema-changes) that can lead to downtime. This is different from the typical Prisma workflow which uses `prisma migrate` in order to generate SQL migrations for you based on changes in your Prisma schema. When using PlanetScale with Prisma, the responsibility of applying the changes is on the PlanetScale side. Therefore, there is little value to using `prisma migrate` with PlanetScale.

Also, the migrations table created when `prisma migrate` runs can also be misleading since PlanetScale does the actual migration when the deploy request is merged, not when `prisma migrate` is run which only updates the schema in the development database branch. You can still see the history of your schema changes in PlanetScale.

## Prerequisites

* Add Prisma to your project using `npm install prisma --save-dev` or `yarn add prisma --dev` (depending on what package manager you prefer).
* Run `npx prisma init` inside of your project to create the initial files needed for Prisma.
* Install the [PlanetScale CLI](https://github.com/planetscale/cli).
* Authenticate the CLI with the following command:

```bash  theme={null}
pscale auth login
```

## Execute your first Prisma db push

Prisma migrations follow the PlanetScale [non-blocking schema change](/docs/vitess/schema-changes) workflow. First, the schema is applied to a *development* branch and then the development branch is merged into the `main` production database.

Let's begin with an example flow for running Prisma migrations in PlanetScale:

<Steps>
  <Step>
    Create a new *prisma-playground* database:

    ```bash  theme={null}
    pscale db create prisma-playground
    ```
  </Step>

  <Step>
    Connect to the database branch:

    ```bash  theme={null}
    pscale connect prisma-playground main --port 3309
    ```

    <Note>
      This step assumes you created a new PlanetScale database and have not yet enabled [Safe Migrations](/docs/vitess/schema-changes/safe-migrations) on the `main` branch. You will need to create a new development branch otherwise.
    </Note>
  </Step>

  <Step>
    Update your `prisma/schema.prisma` file with the following schema:

    <Note>
      In Prisma `4.5.0`, `referentialIntegrity` changed to `relationMode` and became generally available in `4.7.0`. The following schema reflects this change.

      You can learn more about Prisma's Relation mode in the
      [Prisma docs](https://www.prisma.io/docs/orm/prisma-schema/data-model/relations/relation-mode).
    </Note>

    ```js expandable theme={null}
    datasource db {
      provider = "mysql"
      url      = env("DATABASE_URL")
      relationMode = "prisma"
    }

    generator client {
      provider = "prisma-client-js"
    }

    model Post {
      id        Int      @default(autoincrement()) @id
      createdAt DateTime @default(now())
      updatedAt DateTime @updatedAt
      title     String   @db.VarChar(255)
      content   String?
      published Boolean  @default(false)
      author    User     @relation(fields: [authorId], references: [id])
      authorId  Int
    }

    model Profile {
      id     Int     @default(autoincrement()) @id
      bio    String?
      user   User    @relation(fields: [userId], references: [id])
      userId Int     @unique
    }

    model User {
      id      Int      @default(autoincrement()) @id
      email   String   @unique
      name    String?
      posts   Post[]
      profile Profile?
    }
    ```
  </Step>

  <Step>
    Update your `.env` file:

    ```shell  theme={null}
    DATABASE_URL="mysql://root@127.0.0.1:3309/prisma-playground"
    ```
  </Step>

  <Step>
    In another terminal, use the `db push` command to push the schema defined in `prisma/schema.prisma`:

    ```bash  theme={null}
    npx prisma db push
    ```

    Unlike the `prisma migrate dev` command, it will not create a migrations folder containing a SQL file with the SQL used to update the schema in your PlanetScale database. PlanetScale will be tracking your migrations in this workflow.

    <Tip>
      You can learn more about the `prisma db push` command in the
      [Prisma docs](https://www.prisma.io/docs/orm/reference/prisma-cli-reference#db-push).
    </Tip>

    After `db push` is successful, you can see the table created in your terminal. For example, to see the `Post` table:

    ```bash  theme={null}
    pscale shell prisma-playground main
    ```

    ```sql  theme={null}
    describe Post;
    ```

    <Tip>
      Use the `exit` command to exit the MySQL shell.
    </Tip>

    Or you can see it in the PlanetScale UI under the Schema tab in your `main` branch.
  </Step>

  <Step>
    Finally, turn on safe migrations on the `main` branch to enable non-blocking schema changes:

    ```bash  theme={null}
    pscale branch safe-migrations enable prisma-playground main
    ```
  </Step>
</Steps>

## Execute succeeding Prisma migrations in PlanetScale

Our first example migration flow went well, but what happens when you need to run further changes to your schema?

Let's take a look:

<Steps>
  <Step>
    Create a new *development* branch from `main` called `add-subtitle-to-posts`:

    ```bash  theme={null}
    pscale branch create prisma-playground add-subtitle-to-posts
    ```
  </Step>

  <Step>
    Close the proxy connection to your `main` branch (if still open) and connect to the new `add-subtitle-to-posts` development branch:

    ```bash  theme={null}
    pscale connect prisma-playground add-subtitle-to-posts --port 3309
    ```
  </Step>

  <Step>
    In the `prisma/schema.prisma` file, update the `Post` model:

    Add a new `subtitle` field to `Post`:

    ```
    subtitle  String   @db.VarChar(255)
    ```
  </Step>

  <Step>
    Run `db push` again to update the schema in PlanetScale:

    ```bash  theme={null}
    npx prisma db push
    ```
  </Step>

  <Step>
    Open a deploy request for your `add-subtitle-to-posts` branch, so that you can deploy these changes to `main`.

    You can complete the deploy request either in the web app or with the `pscale deploy-request` command.

    ```bash  theme={null}
    pscale deploy-request create prisma-playground add-subtitle-to-posts
    ```

    ```bash  theme={null}
    pscale deploy-request deploy prisma-playground 1
    ```
  </Step>

  <Step>
    Once the deploy request is merged, you can see the results in your main branch's `Post` table:

    ```bash  theme={null}
    pscale shell prisma-playground main
    ```

    ```sql  theme={null}
    describe Post;
    ```
  </Step>
</Steps>

## What's next?

Now that you've successfully conducted your first automatic Prisma migration in PlanetScale and know how to handle future migrations, it's time to deploy your application with a PlanetScale database! Let's learn how to [deploy an application with a PlanetScale database to Vercel](/docs/vitess/tutorials/deploy-to-vercel).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Automatic Rails migrations
Source: https://planetscale.com/docs/vitess/tutorials/automatic-rails-migrations



<Tip>
  If you are using PlanetScale with a Rails application, go to your database's Settings page in the web app and enable
  "Automatically copy migration data." Select "Rails/Phoenix" as the migration framework. When enabled, this setting
  updates the *schema\_migrations* table each time you branch with the latest migration. If disabled, running
  *rake db:migrate* will try to run all migrations every time, instead of only the latest one.
</Tip>

## Introduction

In this tutorial, you're going to learn how Rails migrations work with the PlanetScale branching and deployment workflows.

<Note>
  Migration tracking works with any migration tool, not just Rails. For other frameworks, specify the migration table name on your database's Settings page.
</Note>

## Prerequisites

Follow the [Connect a Rails app](/docs/vitess/tutorials/connect-rails-app) tutorial first. By the end, you will have:

* Installed the [PlanetScale CLI](https://github.com/planetscale/cli), Ruby, and the Rails gem
* Created a PlanetScale database named `blog`
* Started a new Rails app named `blog` with a migration creating a `Users` table
* Run the first Rails migration

### A quick introduction to Rails migrations

Rails tracks an application's migrations in an internal table called `schema_migrations`. At a high level, running `rake db:migrate` does the following:

* Rails looks at all of the migration files in your `db/migrate` directory.
* Rails queries the `schema_migrations` table to see which migrations have and haven't been run.
* Any migration that doesn’t appear in the `schema_migrations` table is considered pending and is executed by this task.

<Tip>
  When you merge a deploy request in PlanetScale, the *schema\_migrations* table in *main* is
  automatically updated with the migration data from your branch.
</Tip>

## Execute a Rails migration on PlanetScale

Rails migrations follow the PlanetScale [non-blocking schema change](/docs/vitess/schema-changes) workflow. First, the migration is applied to a *development* branch, and then the development branch is merged into the `main` production branch with [safe migrations](/docs/vitess/schema-changes/safe-migrations) enabled.

Let's add another table to your existing `blog` schema:

<Steps>
  <Step>
    Create an `add-posts-table` development branch from `main` in your database *blog*:

    ```bash  theme={null}
    pscale branch create blog add-posts-table
    ```

    When the branch is ready, you can verify that the `schema_migrations` table is up-to-date with `main` by checking for the timestamp of your `Create Users` migration file. Your migration will have a different timestamp than the one shown here.

    Check the timestamp in your codebase:

    ```bash  theme={null}
    ls db/migrate
    20211014210422_create_users.rb
    ```

    Connect to the new branch:

    ```bash  theme={null}
    pscale shell blog add-posts-table
    ```

    Query the migration table:

    ```sql  theme={null}
    blog/add-posts-table> select * from schema_migrations;
    +----------------+
    | version        |
    +----------------+
    | 20211014210422 |
    +----------------+
    ```
  </Step>

  <Step>
    Connect your development environment to the new branch:

    One way to do this is to create a new password for the `add-posts-table` branch and update `config/database.yml` with the new username, password, and host. Another is to use `pscale connect` to establish a secure connection on a local port. Since the `add-posts-table` branch won't be needed after the migration, let's use the `pscale connect` proxy.

    In a separate terminal, establish the connection:

    ```bash  theme={null}
    pscale connect blog add-posts-table --port 3309
    ```

    Then, update `config/database.yml` to connect through the proxy:

    ```yaml  theme={null}
    development:
      <<: *default
      adapter: trilogy
      database: blog
      host: 127.0.0.1
      port: 3309
    ```
  </Step>

  <Step>
    Create the second Rails migration and call it `CreatePosts`:

    ```bash  theme={null}
    rails generate migration CreatePosts
    ```

    Find the new migration file in `db/migrate` and add a few details for the new Posts table:

    ```ruby  theme={null}
    class CreatePosts < ActiveRecord::Migration[7.0]
      def change
        create_table :posts do |t|
          t.string :title
          t.text :content
          t.bool :published
          t.references :user
          t.timestamps
        end
      end
    end
    ```
  </Step>

  <Step>
    Run the CreatePosts migration:

    ```bash  theme={null}
    rake db:migrate
    ```

    This command runs the new migration against your `add-posts-table` *development* branch.

    At this point, Rails creates the `posts` table and inserts another `timestamp` into the `schema_migrations` table on your development branch.

    You can verify the change in `schema_migrations` yourself:

    ```sql  theme={null}
    blog/add-posts-table> select * from schema_migrations;
    +----------------+
    | version        |
    +----------------+
    | 20211014210422 |
    | 20220224221753 |
    +----------------+
    ```
  </Step>

  <Step>
    Open a deploy request for your `add-posts-table` branch, and deploy your changes to `main`.

    You can complete the deploy request either in the web app or with the `pscale deploy-request` command.

    ```bash  theme={null}
    pscale deploy-request create blog add-posts-table
    ```

    ```bash  theme={null}
    pscale deploy-request deploy blog 1
    ```

    To create the deploy request, PlanetScale looks at the differences between the schemas of `main` and `add-posts-table` and plans a `create table` statement to add the new table to `main`. When you deploy, PlanetScale runs that ` create table` statement and copies the second row from `schema_migrations` in `add-posts-table` to the `schema_migrations` table in `main`.\`
  </Step>

  <Step>
    Verify the changes in your `main` branch:

    In a `pscale` shell for `main` you can verify that the changes from `add-posts-table` were deployed successfully.

    ```bash  theme={null}
    pscale shell blog main
    ```

    ```sql  theme={null}
    blog/|⚠ main ⚠|> show tables;
    +----------------------+
    | Tables_in_blog       |
    +----------------------+
    | posts                |
    | schema_migrations    |
    | users                |
    +----------------------+

    blog/|⚠ main ⚠|> select * from schema_migrations;
    +----------------+
    | version        |
    +----------------+
    | 20220223232425 |
    | 20220224221753 |
    +----------------+
    ```
  </Step>
</Steps>

## Summary

In this tutorial, we learned how to use the PlanetScale deployment process with the Rails migration workflow.

## What's next?

Learn more about how PlanetScale allows you to make [schema changes](/docs/vitess/schema-changes) to your production databases without downtime or locking tables.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# AWS Lambda connection strings
Source: https://planetscale.com/docs/vitess/tutorials/aws-lambda-connection-strings

In this guide, you'll learn how to properly store and use PlanetScale MySQL connection strings for use in AWS Lambda Functions.

## Introduction

We'll use a [pre-built NodeJS](https://github.com/planetscale/aws-connection-strings-example) app for this example, but you can follow along using your own application as well.

## Prerequisites

* An AWS account
* A [PlanetScale account](https://auth.planetscale.com/sign-up)

## Set up the database

<Note>
  If you already have a database with a production branch, skip to [the next section](#configure-the-lambda-function).
</Note>

Let's start by creating the database. In the PlanetScale dashboard, click the "**New database**" button followed by "**Create new database**". Name the database **lambda-connection-strings,** or any other name that you prefer. Click "**Create database**".

Once your database has finished initializing, you'll need to enable the web console on production branches. To do this, go to the "**Settings**" tab, check "**Allow web console access to production branches**", and click "**Save database settings**".

Now, access the console of the main branch by clicking "**Console**", then "**Connect**".

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/console-3.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=32e2d97aeee544127e39ea25c03dd991" alt="The console" data-og-width="1512" width="1512" data-og-height="1105" height="1105" data-path="docs/images/assets/docs/tutorials/aws-lambda-connection-strings/console-3.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/console-3.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=f8784bafd3c7fe5452787a3fcc524360 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/console-3.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=1aad292e1280e623754c6348264f96c7 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/console-3.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=aa4ddccca932f787cfc92f2db3fba5cb 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/console-3.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=ac8ebc37ab9aada2cdfd91a27dd27a62 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/console-3.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=70bd7fadd625c6506c8a877c4a5867c0 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/console-3.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=fbd84b69c41cfe7b0a5779408aacc597 2500w" />
</Frame>

Create a simple table & insert some data using the following script:

```sql  theme={null}
CREATE TABLE Tasks(
	Id int PRIMARY KEY AUTO_INCREMENT,
	Name varchar(100),
	IsDone bit
);

INSERT INTO Tasks (Name) VALUES ('Clean the kitchen');
INSERT INTO Tasks (Name) VALUES ('Fold the laundry');
INSERT INTO Tasks (Name) VALUES ('Watch the sportsball game');
```

You may run `SELECT * FROM Tasks` to ensure the data was properly added from the console.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/select.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=bcbc6ad2ec884cb2f288977d72965f5d" alt="Records from the console" data-og-width="1262" width="1262" data-og-height="331" height="331" data-path="docs/images/assets/docs/tutorials/aws-lambda-connection-strings/select.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/select.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=acafedcd63a83be6fc2240b0527efc05 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/select.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=76b4af47d878a192d4e71a0aab75e7a9 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/select.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=c88b98c3699cd77fefddeec60183893f 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/select.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=af94a7ad7926483dbabcb7004145c256 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/select.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=26d550adfeb3f27abfcfc44907cfdbdf 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/select.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=8f938ac234303f0960da67f6184db94d 2500w" />
</Frame>

Now we need to enable [**safe migrations**](/docs/vitess/schema-changes/safe-migrations) on the **main** branch. Click the **Dashboard** tab, then click the **cog** icon in the upper right of the infrastructure card.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/production-2.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=9b84747eb949e5e638b2b77d383c5199" alt="The option to promote a branch" data-og-width="1653" width="1653" data-og-height="671" height="671" data-path="docs/images/assets/docs/tutorials/aws-lambda-connection-strings/production-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/production-2.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=7d4f46b1ddde97ba216b29c90ebc0101 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/production-2.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=76d54458ac5f71191b35874aef5a48eb 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/production-2.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=318fb175f28b1328076b956b8ea5ae36 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/production-2.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=8be5cbbd881cd798634c30cb76bfd8c9 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/production-2.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=ac85219097d32abbeb33907dff66e88c 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/production-2.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=5e46f0222e5a6fefe0def8307871faa5 2500w" />
</Frame>

Toggle on the "**Enable safe migrations**" option and click the "**Enable safe migrations**" button.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/safe-migrations-2.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=9d4f4c8d6bf945328a5d398acdf244a5" alt="Enable safe migrations" data-og-width="943" width="943" data-og-height="781" height="781" data-path="docs/images/assets/docs/tutorials/aws-lambda-connection-strings/safe-migrations-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/safe-migrations-2.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=55033b4de736fde0f6dbf9465cf121f9 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/safe-migrations-2.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=e1cb8cad14d9e5cc682d0cc8606867f4 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/safe-migrations-2.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=7181e052522173617c6e51dc1c7e0634 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/safe-migrations-2.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=2135d1fe1c78f1ab7fb6f38f7202f2ba 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/safe-migrations-2.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=262929c883a474e37c9b3b5df8c4457a 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/safe-migrations-2.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=13ba9bcdd74f0c9c22c0faf583435905 2500w" />
</Frame>

Before moving on from the PlanetScale dashboard, grab the connection details to be used in the next step. Click on the **Connect** button to go to the Connect page. Enter a name for your password, and click the **Create password** button to generate a new password.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/promoted-2.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=f9923b79a5807dc0c71084428f30aed6" alt="The dashboard after the database has been promoted" data-og-width="1627" width="1627" data-og-height="809" height="809" data-path="docs/images/assets/docs/tutorials/aws-lambda-connection-strings/promoted-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/promoted-2.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=e3869738f18adb0b375d83a913fd7835 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/promoted-2.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=8970420f55754b87718113dbb572ab64 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/promoted-2.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=d49893a3aaf475245d3f5c95c670f8bb 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/promoted-2.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=05fcd7f8446b04ed1f7b1aa9c110dca4 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/promoted-2.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=4fe2686698e387f6774ad074345bcd6d 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/promoted-2.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=f962cbc0107db8aec93e8d3958a7f55d 2500w" />
</Frame>

In the **Select your language or framework** section, select **Node.js** and note the details in the `.env` section of the guide. These details will be required to connect to the database.

## Configure the Lambda function

Secrets in AWS Lambda functions, which include database connection strings, are often stored as environment variables with the Lambda function. We’ll be uploading a sample NodeJS app that has been provided and storing the connection string from the previous section as an environment variable to test.

Start by cloning the following Git repository:

```bash  theme={null}
git clone https://github.com/planetscale/aws-connection-strings-example.git
```

Log into the AWS Console, use the universal search to search for ‘**Lambda**’, and select it from the list of services.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/aws.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=e0d641fbcd2f007f9dbdf89e34e84712" alt="Search for Lambda in the AWS Console" data-og-width="1111" width="1111" data-og-height="492" height="492" data-path="docs/images/assets/docs/tutorials/aws-lambda-connection-strings/aws.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/aws.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=0c479568bb6916940d92ecf5ada66c92 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/aws.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=d573cc912889dff8bc4f93afdd007f99 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/aws.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=94684e86d61b47a28717b8fd57cd0713 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/aws.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=81e6d4ea7b850c794a4883e5bc3aaa84 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/aws.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=b678df7902df2af63defc0524a589a08 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/aws.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=1916c057f5c31a587f0a2e75e103243b 2500w" />
</Frame>

Create a new function using the **Create function** button in the upper right of the console.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/functions.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=4d76e5576ef02650b7eefff877ae3655" alt="The default view of Lambda functions" data-og-width="1850" width="1850" data-og-height="659" height="659" data-path="docs/images/assets/docs/tutorials/aws-lambda-connection-strings/functions.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/functions.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=d2fd58373cbfad63b642826ab8ca483e 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/functions.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=2739a82e045c4b1f3b519659e5a6c46e 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/functions.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=f1d60ec76215718be80f7338e2a2445b 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/functions.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=8b2a9725aa3bbd2c854f4649eec27632 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/functions.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=19c2f6cb703d106e5ef7eae17ae90eaa 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/functions.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=2a6c9150763d1feb9ceb143fcf1a7001 2500w" />
</Frame>

Name your function **lambda-connection-strings** (or any other name that suits you) and select **NodeJS** under **Runtime**. The other fields can be left as default. Click **Create function** to finish the initial setup of your Lambda.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/create-function.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=8c94601c5c6dae6c8cd81ab3ee93579d" alt="The view to create a Lambda function" data-og-width="1286" width="1286" data-og-height="920" height="920" data-path="docs/images/assets/docs/tutorials/aws-lambda-connection-strings/create-function.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/create-function.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=713b5dc49d6db763204b156aa376db10 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/create-function.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=2ea1256c900b19e4e31526874b415f19 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/create-function.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=2d66434e81793799d654f877512d2376 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/create-function.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=27aec53074ddbfdd9c41242db4cfbda7 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/create-function.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=eb33526b70663a2ed07f94bfb10fdf2e 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/create-function.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=dae4102b4b7579b9c0f0373c78007bdd 2500w" />
</Frame>

On the next view, about halfway down the page you’ll see a section called **Code source**. Click the **Upload from** button, then **.zip file**.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/node.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=e8ebb493bcc5503172fd493dccf70799" alt="The default NodeJS Lambda function" data-og-width="1272" width="1272" data-og-height="1023" height="1023" data-path="docs/images/assets/docs/tutorials/aws-lambda-connection-strings/node.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/node.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=494e9418acf8bb6b22f39d610b8b47b0 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/node.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=8d92df49593698671c4ad9d240493acd 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/node.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=e354356192a597cfe6085193e3cb610d 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/node.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=273bc6ae88eefe4320c57545a3edc11d 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/node.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=e419fed1a6bddc10143919df0d9d80b3 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/node.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=d3b29e8af04915518252574c49c7759f 2500w" />
</Frame>

Click the **Upload** button which will display a file browser. Select the **aws-connection-strings-example.zip** file from the **dist** folder of the provided repository. Click **Save** once it’s been selected.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/upload.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=2687c7afb107aaec165086be63b1d2a4" alt="The modal to upload code" data-og-width="825" width="825" data-og-height="275" height="275" data-path="docs/images/assets/docs/tutorials/aws-lambda-connection-strings/upload.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/upload.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=b0b2b66ef0d502510e2e5e4b0c1ddcc0 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/upload.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=1996c15bd45c99245fa9fb527790f716 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/upload.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=09017097e29e997717bf048ac0baee80 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/upload.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=3e05ea641fafa113a4e011217b1fbb0c 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/upload.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=772ad58492b6b6d21d23776cdca6e5a3 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/upload.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=66cbe6e371b6288d5b72ae67c33ff40c 2500w" />
</Frame>

The contents of the code editor under **Code source** should have updated to show the code stored in the zip file.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/source.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=5b21336279bbc7f4967cfe39fb603db8" alt="The code of the Lambda function that was uploaded" data-og-width="1157" width="1157" data-og-height="761" height="761" data-path="docs/images/assets/docs/tutorials/aws-lambda-connection-strings/source.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/source.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=349f34235bb2f183b632b6dde1983099 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/source.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=63787a92fc837d801a3ed5adf410a0fc 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/source.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=bb1bdfa8829251f9a754f19ecc35d9b5 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/source.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=23ec9201f727e04888439b9d092615b8 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/source.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=6a3a0716994bd87721befe84cb77d91a 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/source.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=5ac9bb89fdbe969e2e3d7b3fcc342747 2500w" />
</Frame>

### Configure environment variables

Next, you need to set the PlanetScale `DATABASE_URL` environment variable that you copied earlier. Select the **Configuration** tab, and click **Edit**.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/configuration.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=56bdd6a0aa450a5049d228dba651bd64" alt="The configuration tab" data-og-width="1167" width="1167" data-og-height="820" height="820" data-path="docs/images/assets/docs/tutorials/aws-lambda-connection-strings/configuration.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/configuration.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=a50aa9ffa465979939ec88e2ad52233c 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/configuration.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=1b9398cd409dd4e0856e872720a9f7f6 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/configuration.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=291392c11145a663dc8f0bf374686913 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/configuration.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=f90ab0df281e1bbf9a112f98be786022 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/configuration.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=373c5b64a0e16f0c1922d6a86eecef66 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/configuration.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=d45fa4363f839910ae25c5e8aeb0bfd2 2500w" />
</Frame>

You’ll be presented with a view to add or update environment variables. Click **Add environment variable** and the view will update with a row to add an environment variable. Set the **Key** field to **DATABASE\_URL** and the **Value** to the connection string taken from the previous section. Click **Save** once finished.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/environment-variables.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=e54594c9eecf831239996c63d55fc058" alt="The view to manage environment variables" data-og-width="841" width="841" data-og-height="525" height="525" data-path="docs/images/assets/docs/tutorials/aws-lambda-connection-strings/environment-variables.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/environment-variables.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=32161f1978438d0f9a93dc25ca944d68 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/environment-variables.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=6cbda497900c200f4440e58a424ae064 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/environment-variables.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=700f496dda7c57e81b5eb47227b2ac4f 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/environment-variables.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=26c1654f54f4d5d87ecb1b9971030adc 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/environment-variables.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=2ee2f7903f7d3007d8900f5db59ba8fc 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/environment-variables.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=bd405918f62d9ff4c55b44439a7fb801 2500w" />
</Frame>

Finally, test the function by selecting the **Test** tab, and then clicking the **Test** button.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/test.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=fdeec770fd58e2678c831e128356a34f" alt="The test tab" data-og-width="1148" width="1148" data-og-height="494" height="494" data-path="docs/images/assets/docs/tutorials/aws-lambda-connection-strings/test.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/test.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=38c1e06b2be30c04c922f62dc1b507c7 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/test.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=aaf3e248a51c92c4940963070a2fb6d0 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/test.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=fd529ea977ab1ee131d0e5432d817846 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/test.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=04f3d7a4b86f18343459b8521e290f9d 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/test.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=b5a0cdca2c77c5b97b1cdd7edb5401c2 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/test.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=47f8db938bcf2c5c5286fffa1a9687e3 2500w" />
</Frame>

An **Execution results** box will display above the **Test event** section. If the box is green, it likely means everything executed as expected. Click the dropdown next to **Details** to see the results of the query. Since the results of the query were logged out to the console, they will be displayed in the **Log output** section.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/success.png?fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=7caa9b033396ad878bb7c3f7653e402d" alt="The execution results" data-og-width="1163" width="1163" data-og-height="906" height="906" data-path="docs/images/assets/docs/tutorials/aws-lambda-connection-strings/success.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/success.png?w=280&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=1df0c2700c78329e510790fdee8b0c18 280w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/success.png?w=560&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=06664ca9aaccfd9953610d6bacf4d757 560w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/success.png?w=840&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=8d81a620acc75be1a43ec24b8f9d5bbd 840w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/success.png?w=1100&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=54116be91398f050cc3b1ff3335568f1 1100w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/success.png?w=1650&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=399a8192a4f5f148316f3244a47a4b02 1650w, https://mintcdn.com/planetscale-cad1a68a/KejcMcI0BguxMNLm/docs/images/assets/docs/tutorials/aws-lambda-connection-strings/success.png?w=2500&fit=max&auto=format&n=KejcMcI0BguxMNLm&q=85&s=ce597713a77ccc23506c640be2223ce5 2500w" />
</Frame>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Cloudflare Workers database integration
Source: https://planetscale.com/docs/vitess/tutorials/cloudflare-workers



## Introduction

[Cloudflare Workers database integration](https://developers.cloudflare.com/workers/learning/integrations/databases/#planetscale) is designed to connect your Cloudflare Workers to data sources automatically by generating connection strings and storing them in the worker's secrets.

This article will utilize a sample repository that is a preconfigured Cloudflare Worker you can use to deploy to your Cloudflare account.

## Prerequisites

* [NodeJS](https://nodejs.org) installed
* A [PlanetScale account](https://auth.planetscale.com/sign-up)
* The [PlanetScale CLI](https://github.com/planetscale/cli)
* A [Cloudflare account](https://www.cloudflare.com)

## Set up the database

<Steps>
  <Step>
    Create a database in your PlanetScale account named `bookings_db`.

    ```bash  theme={null}
    pscale database create bookings_db
    ```
  </Step>

  <Step>
    Connect to the `main` branch of the new database.

    ```bash  theme={null}
    pscale shell bookings_db main
    ```
  </Step>

  <Step>
    Run the following commands to create a table in the database and populate it with some data.

    ```sql  theme={null}
    CREATE TABLE hotels (
      id INT UNSIGNED PRIMARY KEY AUTO_INCREMENT,
      name VARCHAR(50) NOT NULL,
      address VARCHAR(50) NOT NULL,
      stars FLOAT(2) UNSIGNED
    );

    INSERT INTO hotels (name, address, stars) VALUES
      ('Hotel California', '1967 Can Never Leave Ln, San Fancisco CA, 94016', 7.6),
      ('The Galt House', '140 N Fourth St, Louisville, KY 40202', 8.0);
    ```
  </Step>
</Steps>

## Deploy the Cloudflare Worker

<Steps>
  <Step>
    Clone the sample repository.

    ```bash  theme={null}
    git clone https://github.com/planetscale/cloudflare-workers-quickstart.git
    ```
  </Step>

  <Step>
    Navigate to the `worker` folder of the repository and install the dependencies.

    ```bash  theme={null}
    cd cloudflare-workers-quickstart/worker
    npm install
    ```
  </Step>

  <Step>
    Deploy the Worker to your Cloudflare account.

    ```bash  theme={null}
    npx wrangler publish
    ```
  </Step>
</Steps>

## Configure the Cloudflare PlanetScale integration

<Steps>
  <Step>
    Log into the Cloudflare dashboard and navigate to **"Workers"** > **"Overview"**. You should see a service in the list named **"planetscale-worker"**. Select it from the list.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.52.48.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=e09de5f7e8e98c67842fd0ef10c72315" alt="PlanetScale Cloudflare integration wizard - step 1" data-og-width="1760" width="1760" data-og-height="903" height="903" data-path="docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.52.48.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.52.48.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=9e82d236471d950865b0ddefa84b2ef6 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.52.48.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=0b4c709602f247d8c6f9c9c53dd4d4c7 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.52.48.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=807190b415c630258900e2f94b05407b 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.52.48.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=cba0647d673ed32e6ef960d9f31bd783 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.52.48.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=6cc676932997cf48d019c6b7edaccfc6 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.52.48.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=7eaa2a858225ca039b7afaa610fdc67c 2500w" />
    </Frame>
  </Step>

  <Step>
    Select the **"Settings"** tab, then **"Integrations"**, and finally **"Add Integration"** in the PlanetScale card.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.51.19.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=c3414a459fa818eedadd1d922c5517f2" alt="PlanetScale Cloudflare integration wizard - step 2" data-og-width="1758" width="1758" data-og-height="901" height="901" data-path="docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.51.19.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.51.19.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=7f801daa32a939f8b67674c22343ba18 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.51.19.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=016647eb7a825d1643ec98bcb4c6463d 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.51.19.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=22e0a31442f969e1c9fe1c89fb5c75df 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.51.19.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=217eecee3a12613db8e513785aa284da 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.51.19.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=2bf3f34b10a977fa0259e2ddd03583c8 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.51.19.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=d5d035320351e206ba29636c56967796 2500w" />
    </Frame>
  </Step>

  <Step>
    Click **"Accept"** under **Review and grant permissions** to allow the wizard to write the database connection details to the Worker secrets.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.55.06.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=e6c13f25e08ea22dae40c66076ba35c6" alt="PlanetScale Cloudflare integration wizard - step 3" data-og-width="1474" width="1474" data-og-height="1011" height="1011" data-path="docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.55.06.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.55.06.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=9ee1a368214784e5e6e0a12c27872c41 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.55.06.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=b52bf6e6a50c556da6b51f8d3b35044c 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.55.06.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=b07f79dcd6fa950dbc735226bcd55f75 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.55.06.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=2c1992f7cf524ebfee17c0270670d5ed 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.55.06.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=7e7d81b47a9ff64b82096beeb1cc8805 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.55.06.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=a70a2f20732738d0504962d14eddd714 2500w" />
    </Frame>
  </Step>

  <Step>
    Under **Connect to PlanetScale**, click **"Connect"** to start the process of connecting your PlanetScale and Cloudflare accounts.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.56.11.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=f2bee6ceff513f228261d8bb2bf575d0" alt="PlanetScale Cloudflare integration wizard - step 4" data-og-width="1467" width="1467" data-og-height="801" height="801" data-path="docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.56.11.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.56.11.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=90ae58d9dbbb40907c1277400b84af9b 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.56.11.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=c5d3113f80618170ab0b3533da4cbe1b 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.56.11.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=ded7904da2a3d12bd727f9f66818b2af 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.56.11.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=b51e5b3846bd7bfc582717255410dbc5 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.56.11.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=1849489e699fed6acf31ee4286bb441c 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.56.11.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=94ddd8b631a271ccc6fca41d34a02d5f 2500w" />
    </Frame>
  </Step>

  <Step>
    A modal will appear allowing you to grant access to your organization, database, and branch. Start by selecting your organization from the list. This demonstration uses an organization named “ps-deved”.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.59.44.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=945738456262d053fd1eaafe363b8bb4" alt="PlanetScale Cloudflare integration wizard - step 5" data-og-width="748" width="748" data-og-height="883" height="883" data-path="docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.59.44.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.59.44.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=a23568968d710918c8071d769d7bff40 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.59.44.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=4963680e4bdf3a780c0c8f0cc89a39f2 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.59.44.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=4375a4a56f1b1a69cfe8d75e9f7895f3 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.59.44.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=f3af118b8fb20d61b8e5304a605841a9 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.59.44.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=2b46d1c5bfa99604a8bef2811fb77cd4 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_11.59.44.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=2a748ef8a2b9c55110fda4713c1f7dac 2500w" />
    </Frame>
  </Step>

  <Step>
    Select the “bookings\_db” database from the list in the **Databases** card, and the “main” branch from the list in the **Branches** card. Finally, click **"Authorize access"**.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.00.34.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=d4008ffa166bc2f36c97360b2ee0c7be" alt="PlanetScale Cloudflare integration wizard - step 6" data-og-width="759" width="759" data-og-height="824" height="824" data-path="docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.00.34.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.00.34.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=f976c879e7f3c576bb0cb0dae6ca8f42 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.00.34.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=b770c9bd360aa8693e517c536e828371 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.00.34.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=f729e75b9db77d391687d471d7b94c0b 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.00.34.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=398fca821d8cd31958f6e6ff65a3dbfd 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.00.34.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=cb65a80d695c4831410a54d55ba02595 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.00.34.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=31bb75c37b752a6ad8ed70dea58e95d4 2500w" />
    </Frame>
  </Step>

  <Step>
    Select your organization again from the list and click **"Continue"**.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.01.32.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=e184b3f8c80b32d42421f27c59f44b82" alt="PlanetScale Cloudflare integration wizard - step 7" data-og-width="1383" width="1383" data-og-height="960" height="960" data-path="docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.01.32.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.01.32.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=1cd7184b03261d7470af344ca055c395 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.01.32.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=f699f7641313c9b626bc0695322eda03 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.01.32.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=31cac4bc09fe6ab27fd656535c2f7826 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.01.32.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=ae9ee322013959014789a517773233d4 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.01.32.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=fc70027807521177c9a2ff736d2b1a90 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.01.32.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=286310a2226098dbc616702b8b76af2f 2500w" />
    </Frame>
  </Step>

  <Step>
    Select your database and the [user role](/docs/vitess/security/password-roles) you want the integration to have.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.02.29.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=75a3f9ca4131241956cdb3fef2a1a9b3" alt="PlanetScale Cloudflare integration wizard - step 8" data-og-width="1402" width="1402" data-og-height="907" height="907" data-path="docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.02.29.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.02.29.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=b4b69fde8db00d0faddcb068da3f9604 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.02.29.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=05f21b792e07a0ecbee3f0f1d4b666e3 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.02.29.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=4136bcbe4c486ccb4604a6d6254949c1 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.02.29.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=d21b05ba311f0803bd65dbdab3f44ea3 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.02.29.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=7fcc9e4b96c6042d656a371719dd8bab 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.02.29.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=e5dab734feb249aea958762ff55638b3 2500w" />
    </Frame>
  </Step>

  <Step>
    Select the “main” branch from the list and click **"Continue"**.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.03.07.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=3953c32e12c0ca5d384c5796e3c64208" alt="PlanetScale Cloudflare integration wizard - step 9" data-og-width="1406" width="1406" data-og-height="754" height="754" data-path="docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.03.07.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.03.07.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=9ddb6c3c065cb5433c357dbcc6e740fb 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.03.07.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=15e092e97fcee20c60efac6f66ff0938 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.03.07.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=66a5ebfb214cfb09fce78d95a1e8b247 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.03.07.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=7af23600587fc9f3d9fe1c528c34b193 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.03.07.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=57f8b0a52a96538f76113d7d19581034 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_12.03.07.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=f61718a70e12c4ff50e8b1856ecf2c20 2500w" />
    </Frame>
  </Step>

  <Step>
    You’ll be given the option to rename the secrets that will be configured on your behalf. These can be left as is. Click **"Add Integration"** to complete the process.

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.33.05.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=e493ffc599cda78930738030eb5ff2d4" alt="PlanetScale Cloudflare integration wizard - step 10" data-og-width="1441" width="1441" data-og-height="681" height="681" data-path="docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.33.05.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.33.05.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=ada2f8018465e4a3d0a4f404d4f80999 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.33.05.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=55388da6408fc4baf106b989bcceb19c 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.33.05.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=cb870130ce4c96c29a5241e45a9691b4 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.33.05.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=10eb66b595928771b35967bf3595c2d3 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.33.05.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=859b47970a4b02dad289a4202d3f85af 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.33.05.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=026533cc1d16951e861b6736b770ba33 2500w" />
    </Frame>
  </Step>
</Steps>

## Test the integration

Back in the overview of the Worker, there is a preview URL that you can use to open a new tab in your browser that runs the Worker and displays the results. Once you’ve located the preview URL, click it to test the Worker.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.03.41.png?fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=7d06084c3df1da6de44710146a652871" alt="Cloudflare Worker preview URL in the dashboard" data-og-width="1436" width="1436" data-og-height="546" height="546" data-path="docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.03.41.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.03.41.png?w=280&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=49fe3df15b50f25358221e182bd2d492 280w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.03.41.png?w=560&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=746e7901b61b9346d6827e59ec2b25cb 560w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.03.41.png?w=840&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=b6ccddd8b3184f804665a4f385ad4351 840w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.03.41.png?w=1100&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=2c4e0edd7560939c1eaf7cf9eefe343f 1100w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.03.41.png?w=1650&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=fc4aae1fd497db3af8f963de47b9d306 1650w, https://mintcdn.com/planetscale-cad1a68a/52I47nER2-XBxbHF/docs/images/assets/docs/integrations/cloudflare-workers/CleanShot_2023-05-16_at_13.03.41.png?w=2500&fit=max&auto=format&n=52I47nER2-XBxbHF&q=85&s=0e5c28355077c1e17862d2de8afe11a9 2500w" />
</Frame>

Once the integration is configured, you can also run the project on your computer using:

```bash  theme={null}
npx wrangler dev
```

This will automatically use the secrets defined in Cloudflare to run the Worker on your computer.

### Test other database operations (optional)

To test other database operations that are mapped to HTTP methods, you may use the provided `tests.http` file which is designed to work with the [VSCode REST client plugin](https://marketplace.visualstudio.com/items?itemName=humao.rest-client). The file is preconfigured to work with the local environment, or you can change the `@host` variable to match the URL provided in the Cloudflare dashboard that cooresponds with your Worker project.

| Method      | Operation                 |
| :---------- | :------------------------ |
| GET /       | Get a list of all hotels. |
| POST /      | Create a hotel.           |
| PUT /:id    | Update a hotel.           |
| DELETE /:id | Delete a hotel.           |

## What's next?

Once you're done with development, it is highly recommended that [safe migrations](/docs/vitess/schema-changes/safe-migrations) be turned on for your `main` production branch to protect from accidental schema changes and enable zero-downtime deployments.

When you're ready to make more schema changes, you'll [create a new branch](/docs/vitess/schema-changes/branching) off of your production branch. Branching your database creates an isolated copy of your production schema so that you can easily test schema changes in development. Once you're happy with the changes, you'll open a [deploy request](/docs/vitess/schema-changes/deploy-requests). This will generate a diff showing the changes that will be deployed, making it easy for your team to review.

Learn more about how PlanetScale allows you to make [non-blocking schema changes](/docs/vitess/schema-changes) to your database tables without locking or causing downtime for production databases.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Connect any application to PlanetScale
Source: https://planetscale.com/docs/vitess/tutorials/connect-any-application



## Introduction

In this tutorial, you'll learn how to connect any application to your PlanetScale database.

If you're just getting started and still need to set up a database, we recommend starting with the [PlanetScale quick start guide](/docs/vitess/tutorials/planetscale-quick-start-guide) first. We also have language/framework-specific guides under "**Integration guides**" if you prefer a more detailed walk-through.

PlanetScale uses [database branches](/docs/vitess/schema-changes/branching) to create a development-friendly workflow. Your database is initially created with a default branch, `main`, which is meant to serve as a production database branch. Production branches are highly available databases intended for production traffic. They are automatically provided with an additional replica to resist outages, enabling zero-downtime failovers.

You can connect to a *production* or *development* database branch. We recommend creating and connecting to a *development* branch while in *development*, as it allows you to make schema changes without affecting production. Your production application, however, should be connected to your production database branch. Check out our [Branching guide](/docs/vitess/schema-changes/branching) for more information about the branching workflow.

<Note>
  By default, production branches have safe migrations turned off. This means that any schema changes you make will be applied immediately. Once you are ready to go to production, we recommend turning on safe migrations if you want to make non-blocking schema changes. Check out our [Safe migrations documentation](/docs/vitess/schema-changes/safe-migrations) for more information.
</Note>

There are two ways to connect your app to PlanetScale. Both are covered below.

## Option 1: Connect with username and password (Recommended)

This section will show you how to create a username and password for your branch and use those credentials to connect to your database. This is the recommended way to connect.

There are two ways to generate a new username and password for your branch:

* In the PlanetScale dashboard
* With the PlanetScale CLI

### Generate credentials in the PlanetScale dashboard

<Steps>
  <Step>
    On the database dashboard page, click the **Connect** button.
  </Step>

  <Step>
    Select the database branch that you want to create a password for.
  </Step>

  <Step>
    Click the **Create password** button.
  </Step>

  <Step>
    Select the applicable language or framework for your application or click "Other".
  </Step>

  <Step>
    Copy the credentials.
  </Step>

  <Step>
    Paste them in your application's MySQL configuration file (often just a `.env` file). The layout and name of this file will vary depending on the application language, but it may look something like this:

    ```bash  theme={null}
    DATABASE=<DATABASE_NAME>
    USERNAME=<USERNAME>
    HOST=<HOST_NAME>
    PASSWORD=<PASSWORD>
    SSL= // more information about this in next step
    ```

    Check out our language integration guides in the side navigation for more explicit instructions.
  </Step>

  <Step>
    To ensure a secure connection, you must validate the server-side certificate from PlanetScale. This configuration depends on your application, but often it just means adding a line to your `.env` file similar to this:

    ```bash  theme={null}
    MYSQL_ATTR_SSL_CA=/etc/ssl/cert.pem
    ```

    The path to the certificate depends on your system. The above example shows the path for macOS, but you can find others in our [Secure connections documentation](/docs/vitess/connecting/secure-connections#ca-root-configuration).

    Again, the variable name here, `MYSQL_ATTR_SSL_CA`, is just an example. The actual name and location for it will depend on the application.

    If you're **unsure what to put here**, we recommend selecting your application's language or framework from the 2nd section of the Connect page (see step 4 above) and copy the credentials from there. This includes the necessary SSL configuration variables and shows what files they belong in. Additionally, we show you the correct certificate path by default based on your system.
  </Step>
</Steps>

### Generate credentials in the PlanetScale CLI

If you prefer working from the CLI, you can quickly spin up new credentials there. Make sure you have the [CLI set up](/docs/cli/planetscale-environment-setup) first.

<Steps>
  <Step>
    Run the following command in the CLI to create a new username and password for your branch.

    ```bash  theme={null}
    pscale password create <DATABASE_NAME> <BRANCH_NAME> <PASSWORD_NAME>
    ```

    <Note>
      The `PASSWORD_NAME` value represents the name of the username and password being generated. You can have multiple credentials for a branch, so this gives you a way to categorize them. To manage your passwords in the dashboard, go to your database dashboard page, click "Settings", and then click "Passwords".
    </Note>
  </Step>

  <Step>
    Take note of the values returned. You won't be able to see this password again.

    ```
    Password production-password was successfully created.
    Please save the values below, as they will not be shown again.

      NAME                  USERNAME       ACCESS HOST URL                     ROLE               PASSWORD
     --------------------- ------------- --------------------------------- ------------------ --------------------------------
      production-password   xxxxxxxxxx   xxxxxxxxxx.us-east-2.psdb.cloud   Can Read & Write   pscale_pw_xxxxxx_xxxxxxxxxxxxx
    ```
  </Step>

  <Step>
    Paste the values from the console output into your application's MySQL configuration file. The layout and name of this file will vary depending on the application language, but it may look something like this:

    ```bash  theme={null}
    DATABASE=<DATABASE_NAME>
    USERNAME=<USERNAME>
    HOST=<ACCESS HOST URL>
    PASSWORD=<PASSWORD>
    SSL= // This is covered in the next step
    ```
  </Step>

  <Step>
    To ensure a secure connection, you must validate the server-side certificate from PlanetScale. This configuration depends on your application, but often it just means adding a line to your `.env` file similar to this:

    ```bash  theme={null}
    MYSQL_ATTR_SSL_CA=/etc/ssl/cert.pem
    ```

    The path to the certificate depends on your system. The above example shows the path for macOS, but you can find others in our [Secure connections documentation](/docs/vitess/connecting/secure-connections#ca-root-configuration).

    Again, the variable name here, `MYSQL_ATTR_SSL_CA`, is just an example. The actual name and location for it will depend on the application.

    If you're **unsure what to put here**, we recommend selecting your application's language from the dropdown in the PlanetScale dashboard (see step 3 from the previous section) and copying the credentials from there. This includes the necessary SSL configuration variables and shows what files they belong in. Additionally, we show you the correct certificate path by default based on your system.
  </Step>
</Steps>

## Option 2: Connect using the PlanetScale proxy

Another way to connect your application to your PlanetScale database *during development* is using the PlanetScale proxy. You won't have to fiddle with configuring any credential details, as that's handled by PlanetScale. It's as simple as a single CLI command.

You'll use the CLI to establish a secure connection to PlanetScale. It will listen on a local port that your application can connect to. The main benefit of this method is you won't have to generate and remember multiple passwords every time you're creating or switching to a new branch.

<Steps>
  <Step>
    Make sure you have [the CLI set up](/docs/cli/planetscale-environment-setup), and then run the following command:

    ```bash  theme={null}
    pscale connect <DATABASE_NAME> <BRANCH_NAME>
    ```

    This establishes a secure connection and opens a port on your local machine that you can use to connect to any MySQL client.
  </Step>

  <Step>
    Take note of the address it returns to you. By default, it is `127.0.0.1:3306`. The CLI will use a different port if `3306` is unavailable.
  </Step>

  <Step>
    In your application's MySQL configuration file, use the following to connect:

    ```bash  theme={null}
    DATABASE=<DATABASE_NAME>
    HOST=127.0.0.1
    PORT=3306 // use the value that was returned in the console
    ```
  </Step>
</Steps>

Your application should now be connected to the specified PlanetScale database branch!

## What's next?

Once your application is connected to a development database branch, you can make schema changes in an isolated development environment without worrying about affecting production. Additionally, we recommend that [safe migrations](/docs/vitess/schema-changes/safe-migrations) be enabled on your production database branch, which allows you to make [non-blocking schema changes](/docs/vitess/schema-changes) without locking or causing downtime.

### PlanetScale workflow

Here's the general workflow that you'll go through to get schema changes from development to production:

<Steps>
  <Step>
    Follow this guide to connect to a development branch.
  </Step>

  <Step>
    Modify your schema as needed.
  </Step>

  <Step>
    Test them locally or in your staging environment.
  </Step>

  <Step>
    Once satisfied and ready to deploy your changes to production, [create a deploy request](/docs/vitess/schema-changes/deploy-requests).
  </Step>

  <Step>
    You or your team can review and approve the schema changes.
  </Step>

  <Step>
    Deploy your deploy request to production.
  </Step>

  <Step>
    Bonus: If you realize you made a mistake, you can click "Revert changes" to [undo a schema change](/docs/vitess/schema-changes/deploy-requests#revert-a-schema-change).
  </Step>
</Steps>

<Info>
  Note: you must already have [a production branch](/docs/vitess/schema-changes/branching#promote-a-branch-to-production) in place to create a deploy request.
</Info>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Connect a Django application to PlanetScale
Source: https://planetscale.com/docs/vitess/tutorials/connect-django-app



## Introduction

In this tutorial, you'll learn how to connect a Django application to a PlanetScale MySQL database using a pre-built Django application.

<Tip>
  Already have a Django application and just want to connect to PlanetScale? Check out the [Django quick connect repo](https://github.com/planetscale/connection-examples/tree/main/python).
</Tip>

## Prerequisites

* Python — This tutorial uses `v3.6`
* A [PlanetScale account](https://auth.planetscale.com/sign-up)
* (Optional) [PlanetScale CLI](https://github.com/planetscale/cli) — This isn't required, but it can make setup much faster

## Set up the Django application

This guide integrates a simple Django application with PlanetScale. The application has one endpoint that displays a list of products and categories pulled from the database. If you have an existing application, you can also use that.

<Steps>
  <Step>
    Clone the starter Django application and switch into the project folder:

    ```bash  theme={null}
    git clone https://github.com/planetscale/django-example.git
    cd django-example
    ```
  </Step>

  <Step>
    Start the virtual environment:

    ```bash  theme={null}
    python3 -m venv env
    source env/bin/activate
    ```

    For Windows, use `env/Scripts/activate`.
  </Step>

  <Step>
    Install the required packages:

    ```bash  theme={null}
    pip install -r ./requirements.txt
    ```
  </Step>
</Steps>

## Set up the database

Next, you need to set up your PlanetScale database and connect to it in the Django application.

You can create a database either in the [PlanetScale dashboard](https://app.planetscale.com) or from the PlanetScale CLI. This guide will use the CLI, but you can follow the database setup instructions in the [PlanetScale quickstart guide](/docs/vitess/tutorials/planetscale-quick-start-guide#getting-started-planetscale-dashboard) if you prefer the dashboard.

Authenticate the CLI with the following command:

```bash  theme={null}
pscale auth login
```

Create a new database with a default `main` branch with the following command:

```bash  theme={null}
pscale database create <DATABASE_NAME> --region <REGION_SLUG>
```

This tutorial uses `django_example` for `DATABASE_NAME`, but you can use any name with lowercase, alphanumeric characters, or underscores. You can also use dashes, but we don't recommend them, as they may need to be escaped in some instances.

For `REGION_SLUG`, choose a region closest to you from the [available regions](/docs/vitess/regions#available-regions) or leave it blank.

That's it! Your database is ready to use. Next, let's connect it to the Django application and then add some data.

## Connect to the Django application

There are **two ways to connect** your Django application to PlanetScale:

* With an auto-generated username and password
* Using the PlanetScale proxy with the CLI

Both options are covered below.

### Option 1: Connect with username and password (Recommended)

<Steps>
  <Step>
    Create a username and password with the PlanetScale CLI by running:

    ```bash  theme={null}
    pscale password create <DATABASE_NAME> <BRANCH_NAME> <PASSWORD_NAME>
    ```

    <Note>
      The `PASSWORD_NAME` value represents the name of the username and password being generated. You can have multiple credentials for a branch, so this gives you a way to categorize them. To manage your passwords in the dashboard, go to your database dashboard page, click "Settings", and then click "Passwords".
    </Note>

    You can also get these exact values to copy/paste from your [PlanetScale dashboard](https://app.planetscale.com). In the dashboard, click on the database > "**Connect**" > "**Connect with**" language dropdown > "**Django**". If the password is blurred, click "**New password**".

    Take note of the values returned to you, as you won't be able to see this password again.
  </Step>

  <Step>
    Open the .env file in your Django app, find the database connection section, and fill it in as follows:

    ```bash  theme={null}
    DB_HOST=<ACCESS HOST URL>
    DB_PORT=3306
    DB_NAME=<DATABASE_NAME>
    DB_USER=<USERNAME>
    DB_PASSWORD=<PLAIN TEXT>
    MYSQL_ATTR_SSL_CA=/etc/ssl/cert.pem
    ```

    The value for `MYSQL_ATTR_SSL_CA` may differ [depending on your operating system](/docs/vitess/connecting/secure-connections#ca-root-configuration).
  </Step>

  <Step>
    Next, in the `mysite/settings.py` file, scroll down and look for the `DATABASES` object. Replace it with the following:

    ```python  theme={null}
    DATABASES = {
      'default': {
        'ENGINE': 'django_psdb_engine',
        'NAME': os.environ.get('DB_NAME'),
        'HOST': os.environ.get('DB_HOST'),
        'PORT': os.environ.get('DB_PORT'),
        'USER': os.environ.get('DB_USER'),
        'PASSWORD': os.environ.get('DB_PASSWORD'),
        'OPTIONS': {'ssl': {'ca': os.environ.get('MYSQL_ATTR_SSL_CA')}}
      }
    }
    ```
  </Step>
</Steps>

### Option 2: Connect with PlanetScale proxy

To connect with the PlanetScale proxy, you'll need the [PlanetScale CLI](https://github.com/planetscale/cli).

<Steps>
  <Step>
    Open a connection by running the following:

    ```bash  theme={null}
    pscale connect <DATABASE_NAME> <BRANCH_NAME>
    ```

    If you're following this guide exactly and haven't created any new branches, you'll use the default branch, `main`, for `BRANCH_NAME`.
  </Step>

  <Step>
    A secure connection to your database will be established and you'll see a local address you can use to connect to your application.
  </Step>

  <Step>
    Open the `.env` file in your Django app and update it as follows:

    ```bash  theme={null}
    DB_HOST=127.0.0.1
    DB_PORT=3306
    DB_NAME=<DATABASE_NAME>
    ```

    The connection uses port `3306` by default, but if that's being used, it will pick a random port. Make sure you paste in whatever port is returned in the terminal.
  </Step>
</Steps>

## Optional — Bring in PlanetScale custom database wrapper

This next step is only necessary if you're using your own application to go through this guide **and** [do not want to use foreign key constraints](/docs/vitess/operating-without-foreign-key-constraints). If you cloned the sample app, this already exists in the repo.

Foreign key constraint support is not enabled by default in PlanetScale. If you'd like to use foreign key constraints in your Django application with PlanetScale, you must first enable foreign key constraint support in your database settings page.

Django uses foreign key constraint syntax by default. So, if you want to proceed without foreign key constraints, you need to pull in the PlanetScale database wrapper for Django to disable foreign key syntax in the Django migrations.

<Steps>
  <Step>
    Run the following to pull it in:

    ```bash  theme={null}
    git clone https://github.com/planetscale/django_psdb_engine.git
    ```
  </Step>

  <Step>
    In your `settings.py` file, add `django_psdb_engine` as the database engine.

    ```python  theme={null}
    DATABASES = {
        'default': {
            'ENGINE': 'django_psdb_engine',
        }
    }
    ```
  </Step>
</Steps>

## Run migrations and seeder

Now that you're connected, let's add some data to see everything in action.

You can find the migrations file in `mysite/store/migrations/0002_auto_20220919_0058.py` that references the `Category` and `Product` models to create the schema. It also contains seed data to create two products and two categories. Run this migration with:

```bash  theme={null}
python manage.py migrate
```

This will also run the default Django migrations.

## Display the data

Finally, let's display the data to confirm that everything worked correctly.

This Django starter application has a pre-built endpoint, `/products`, that will grab and display all of the product data.

To view the data, start the server:

```bash  theme={null}
python manage.py runserver
```

Then go to [`localhost:8000/products`](http://localhost:8000/products) and you'll see a list of the data from the products table.

## Add data manually

If you want to continue playing around with adding data on the fly, you have a few options:

* PlanetScale CLI shell
* PlanetScale dashboard console
* Your favorite MySQL client (for a list of tested MySQL clients, review our article on [how to connect MySQL GUI applications](/docs/vitess/tutorials/connect-mysql-gui))

The first two options are covered below.

### Add data with PlanetScale CLI

You can use the PlanetScale CLI to open a MySQL shell to interact with your database.

You may need to [install the MySQL command line client](/docs/cli/planetscale-environment-setup) if you haven't already.

Run the following command in your terminal:

```bash  theme={null}
pscale shell <DATABASE_NAME> <BRANCH_NAME>
```

This will open up a MySQL shell connected to the specified database and branch.

<Note>
  A branch, `main`, was automatically created when you created your database, so you can use that for `BRANCH_NAME`.
</Note>

Add a record to the `store_product` table:

```sql  theme={null}
INSERT INTO `store_product` (name, description, image, category_id)
VALUES  ('Product 3', 'Product 3 description', 'https://via.placeholder.com/300.png?text=Product1', 1);
```

The value `id` will be filled with a default value.

Type `exit` to exit the shell.

Refresh the Django homepage to see the new record. You can also verify it was added in the PlanetScale CLI MySQL shell with:

```sql  theme={null}
select * from store_product;
```

### Add data with PlanetScale dashboard console

If you don't care to install MySQL client or the PlanetScale CLI, another quick option using the MySQL console built into the PlanetScale dashboard.

<Steps>
  <Step>
    Go to your [PlanetScale dashboard](https://app.planetscale.com) and select your Django database.
  </Step>

  <Step>
    Click on "**Console**".
  </Step>

  <Step>
    Select the `main` branch and click "**Connect**".
  </Step>

  <Step>
    Add a new record to the `store_product` table with:

    ```sql  theme={null}
    INSERT INTO `store_product` (name, description, image, category_id)
    VALUES  ('Product 3', 'Product 3 description', 'https://via.placeholder.com/300.png?text=Product1', 1);
    ```
  </Step>

  <Step>
    You can confirm that it was added by running:

    ```sql  theme={null}
    select * from store_product;
    ```
  </Step>
</Steps>

You can also head to the [`/products`](http://localhost:8000/products) endpoint in your Django application to see the new data.

## Foreign key constraints with PlanetScale

If you'd like to use foreign key constraints in your Django application with PlanetScale, you must first enable foreign key constraint support in your database settings page.

If you prefer to enforce referential integrity at the application level, you will have to do some extra configuration with Django. You can disable foreign key constraint checks at the model level in Django, but if you're running the default migrations, you'll need to turn them off globally.

The [PlanetScale custom database backend](https://github.com/planetscale/django_psdb_engine) manages this, but if you want to do it manually in each model. For example, in the `models.py` file for the example in this document, we define the foreign key on the `category` table with the following:

```python  theme={null}
category = models.ForeignKey(Category, on_delete=models.DO_NOTHING, db_constraint=False)
```

This isn't necessary to do in every model if you're pulling in the `django_psdb_engine` because it overrides the setting globally anyway, but this will work if you want to do it per model.

## What's next?

Once you're done with initial development, you can enable [safe migrations](/docs/vitess/schema-changes/safe-migrations) to protect from accidental schema changes and enable zero-downtime deployments.

Learn more about how PlanetScale allows you to make [non-blocking schema changes](/docs/vitess/schema-changes) to your database tables without locking or causing downtime for production databases.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Connect a Go application to PlanetScale
Source: https://planetscale.com/docs/vitess/tutorials/connect-go-app



export const VimeoEmbed = ({id, title}) => {
  return <Frame>
      <iframe src={`https://player.vimeo.com/video/${id}?dnt=true`} title={title} className="aspect-video w-full" allow="autoplay; fullscreen; picture-in-picture" />
    </Frame>;
};

<VimeoEmbed id="759188218" title="Connect to PlanetScale with Go" />

## Introduction

In this guide, you’ll learn how to connect to a PlanetScale MySQL database with Go by exploring a sample API built using the Gin routing framework.

**Prerequisites:**

* [Go](https://go.dev/doc/install)
* [A PlanetScale account](https://auth.planetscale.com/sign-up)
* [VS Code](https://code.visualstudio.com/download) (optional)
* The [VS Code Rest Client plugin](https://marketplace.visualstudio.com/items?itemName=humao.rest-client) (optional)

<Tip>
  Already have a Go application and just want to connect to PlanetScale? Check out the [Go quick connect repo](https://github.com/planetscale/connection-examples/tree/main/go).
</Tip>

## Create the database

Start in PlanetScale by creating a new database. From the dashboard, click "**New Database**", then "**Create new database**". Name the database `products_db`, select the desired [Plan type](/docs/planetscale-plans), and click "**Create database**".

By default, web console access to production branches is disabled to prevent accidental deletion. From your database's dashboard page, click on the "**Settings**" tab, check the box labelled "**Allow web console access to production branches**", and click "**Save database settings**".

Then, click on the **"Console"** tab, then "**Connect**".

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-app/console-2.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=e929a8164f551599c3e017eaa84822a2" alt="The Console tab" data-og-width="1529" width="1529" data-og-height="1108" height="1108" data-path="docs/images/assets/docs/tutorials/connect-go-app/console-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-app/console-2.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=d5a32e5945bfb91095bd6efb432f2885 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-app/console-2.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=5d752f103cb8719e90ca64917dad9d07 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-app/console-2.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=b1d6e198141a4ee71403472eaed9ec29 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-app/console-2.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=ceeb905aad3b2a1dd0c86911ecabed84 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-app/console-2.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=0e417a25b629e3b588b5eda5ef045e3e 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-app/console-2.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=8e57adb68a39cb5e8656a548e9131152 2500w" />
</Frame>

Run the following two commands to create a sample table and insert some data:

```sql  theme={null}
CREATE TABLE `products` (
	`id` int PRIMARY KEY AUTO_INCREMENT,
	`name` varchar(100) NOT NULL,
	`price` int NOT NULL
);

INSERT INTO `products` (name, price) VALUES
  ('Cyberfreak 2076', 40),
  ('Destination 2: Shining Decline', 20),
  ('Edge Properties 3', 15);
```

Finally, head to the **"Dashboard"** tab and click **"Connect"**.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-app/connect-2.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=53c1edb9e16bac9c8119103690844757" alt="The location of the Connect button" data-og-width="1598" width="1598" data-og-height="806" height="806" data-path="docs/images/assets/docs/tutorials/connect-go-app/connect-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-app/connect-2.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=db431b276de850eee09743d33005c5ca 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-app/connect-2.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=f0941b0b42cd57c7bb26884cc5a42bd5 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-app/connect-2.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=808691765296267e54988ad0dc44555b 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-app/connect-2.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=f5ec8b34d6726a3e0cedfb402ff9193b 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-app/connect-2.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=c979d16c8cc363ea2d789478ea934125 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-app/connect-2.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=3a7ddec18decaf35febd384199dbba96 2500w" />
</Frame>

On the following page, click **"Create password"** to generate a new password for your database. Then click **Go** in the **Select your language or framework** section, and copy the contents of the `.env` file. You'll need it for the next section.

## Run the demo project

Start by opening a terminal on your workstation and clone the sample repository provided.

```bash  theme={null}
git clone https://github.com/planetscale/golang-example-gin.git
```

Open the project in VS Code and add a new file in the root of the project named `.env`, Populate the file with the contents taken from the Connect modal in the previous section.

```sql  theme={null}
DSN=****************:************@tcp(us-east.connect.psdb.cloud)/products_db?tls=true&interpolateParams=true
```

Now open an integrated terminal in VS Code and run the project using the following commands:

```bash  theme={null}
go mod tidy
go run .
```

The terminal should update with the following output.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-app/go-run-output.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=f66347a6a0754688762e69a9ffcc229b" alt="The output of the GET test" data-og-width="3140" width="3140" data-og-height="1322" height="1322" data-path="docs/images/assets/docs/tutorials/connect-go-app/go-run-output.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-app/go-run-output.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=9b8d8a2b4423b2d0e642ef372ecc682e 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-app/go-run-output.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=63c6d8ecf725017eac32667b4df60d5f 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-app/go-run-output.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=3c07de26b2faff0d0dc7c2b4c235027f 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-app/go-run-output.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=6fc1d6918f14bb4e91ee9f12d79a2c44 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-app/go-run-output.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=d092e57eb20014556af4ad0e26e76449 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-app/go-run-output.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=9f74af27aecbd69b5832266383eaf2cf 2500w" />
</Frame>

## Exploring the code

Now that the project is running, let’s explore the code to see how everything works. All of the code is stored in `main.go`, with each of the core SQL operations mapped by HTTP method in the `main` function:

| HTTP Method Name | Query Type |
| :--------------- | :--------- |
| get              | SELECT     |
| post             | INSERT     |
| put              | UPDATE     |
| delete           | DELETE     |

```go  theme={null}
func main() {
	// Load in the `.env` file
	err := godotenv.Load()
	if err != nil {
		log.Fatal("failed to load env", err)
	}

	// Open a connection to the database
	db, err = sql.Open("mysql", os.Getenv("DSN"))
	if err != nil {
		log.Fatal("failed to open db connection", err)
	}

	// Build router & define routes
	router := gin.Default()
	router.GET("/products", GetProducts)
	router.GET("/products/:productId", GetSingleProduct)
	router.POST("/products", CreateProduct)
	router.PUT("/products/:productId", UpdateProduct)
	router.DELETE("/products/:productId", DeleteProduct)

	// Run the router
	router.Run()
}
```

Open the `tests.http` file, which contains HTTP requests that can be sent to test the API. Running the `get {{hostname}}/products` test is the equivalent of running `SELECT * FROM products` in SQL and returning the results as JSON.

<Warning>
  If you do not wish to use VS Code with the Rest Client plugin, you may use `tests.http` as a reference for your preferred IDE and API testing software.
</Warning>

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-app/go-run-output.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=f66347a6a0754688762e69a9ffcc229b" alt="The terminal output of the go run command" data-og-width="3140" width="3140" data-og-height="1322" height="1322" data-path="docs/images/assets/docs/tutorials/connect-go-app/go-run-output.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-app/go-run-output.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=9b8d8a2b4423b2d0e642ef372ecc682e 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-app/go-run-output.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=63c6d8ecf725017eac32667b4df60d5f 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-app/go-run-output.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=3c07de26b2faff0d0dc7c2b4c235027f 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-app/go-run-output.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=6fc1d6918f14bb4e91ee9f12d79a2c44 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-app/go-run-output.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=d092e57eb20014556af4ad0e26e76449 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-app/go-run-output.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=9f74af27aecbd69b5832266383eaf2cf 2500w" />
</Frame>

This is the `GetProducts` function defined in `main.go`. Notice how the `query` variable is the `SELECT` statement, which is passed into `db.Query` before being scanned into a slice of `Product` structs.

```go expandable theme={null}
func GetProducts(c *gin.Context) {
	query := "SELECT * FROM products"
	res, err := db.Query(query)
	defer res.Close()
	if err != nil {
		log.Fatal("(GetProducts) db.Query", err)
	}

	products := []Product{}
	for res.Next() {
		var product Product
		err := res.Scan(&product.Id, &product.Name, &product.Price)
		if err != nil {
			log.Fatal("(GetProducts) res.Scan", err)
		}
		products = append(products, product)
	}

	c.JSON(http.StatusOK, products)
}
```

To pass parameters into queries, you may use a `?` as a placeholder for the parameter. For example, `GetSingleProduct` uses a query with a `WHERE` clause that is passed into the `db.QueryRow` function along with the query string.

```go expandable theme={null}
func GetSingleProduct(c *gin.Context) {
	productId := c.Param("productId")
	productId = strings.ReplaceAll(productId, "/", "")
	productIdInt, err := strconv.Atoi(productId)
	if err != nil {
		log.Fatal("(GetSingleProduct) strconv.Atoi", err)
	}

	var product Product
	// `?` is a placeholder for the parameter
	query := `SELECT * FROM products WHERE id = ?`
	// `productIdInt` is passed in with the query
	err = db.QueryRow(query, productIdInt).Scan(&product.Id, &product.Name, &product.Price)
	if err != nil {
		log.Fatal("(GetSingleProduct) db.Exec", err)
	}

	c.JSON(http.StatusOK, product)
}
```

Parameters in queries are populated in the order they are passed into the respective `db` function, as demonstrated in `CreateProduct`.

```go expandable theme={null}
func CreateProduct(c *gin.Context) {
	var newProduct Product
	err := c.BindJSON(&newProduct)
	if err != nil {
		log.Fatal("(CreateProduct) c.BindJSON", err)
	}

	// This query has multiple `?` parameter placeholders
	query := `INSERT INTO products (name, price) VALUES (?, ?)`
	// The `Exec` function takes in a query, as well as the values for
	//     the parameters in the order they are defined
	res, err := db.Exec(query, newProduct.Name, newProduct.Price)
	if err != nil {
		log.Fatal("(CreateProduct) db.Exec", err)
	}
	newProduct.Id, err = res.LastInsertId()
	if err != nil {
		log.Fatal("(CreateProduct) res.LastInsertId", err)
	}

	c.JSON(http.StatusOK, newProduct)
}
```

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Connect a Go application using GORM to PlanetScale
Source: https://planetscale.com/docs/vitess/tutorials/connect-go-gorm-app



## Introduction

In this tutorial, you'll learn how to connect a Go application to a PlanetScale MySQL database using a sample Go starter app with GORM.

<Tip>
  Already have a Go application and just want to connect to PlanetScale? Check out the [Go quick connect
  repo](https://github.com/planetscale/connection-examples/tree/main/go).
</Tip>

## Prerequisites

* [Go](https://go.dev/doc/install)
* A [PlanetScale account](https://auth.planetscale.com/sign-up)
* [PlanetScale CLI](https://github.com/planetscale/cli) — You can also follow this tutorial in the PlanetScale admin dashboard, but the CLI will make setup quicker.

## Set up the Go app

This guide will integrate [a simple Go (Golang) app](https://github.com/planetscale/golang-example) with PlanetScale that will display a list of products stored in the database. If you have an existing application, you can also use that.

<Steps>
  <Step>
    Clone the starter Go application:

    ```bash  theme={null}
    git clone https://github.com/planetscale/golang-example.git
    ```
  </Step>

  <Step>
    Enter into the folder:

    ```bash  theme={null}
    cd golang-example
    ```
  </Step>

  <Step>
    Copy the `.env.example` file into `.env`:

    ```bash  theme={null}
    cp .env.example .env
    ```
  </Step>
</Steps>

## Set up the database

Next, you need to set up your PlanetScale database and connect to it in the Go application.

You can create a database in the [PlanetScale dashboard](https://app.planetscale.com) or from the PlanetScale CLI. This guide will use the CLI, but you can follow the database setup instructions in the [PlanetScale quickstart guide](/docs/vitess/tutorials/planetscale-quick-start-guide) if you prefer the dashboard.

<Steps>
  <Step>
    Authenticate the CLI with the following command:

    ```bash  theme={null}
    pscale auth login
    ```
  </Step>

  <Step>
    Create a new database with a default `main` branch with the following command:

    ```bash  theme={null}
    pscale database create <DATABASE_NAME> --region <REGION_SLUG>
    ```

    For `DATABASE_NAME`, you can use any name with lowercase, alphanumeric characters, or underscores. You can also use dashes, but we don't recommend them, as they may need to be escaped in some instances.

    For `REGION_SLUG`, choose a region closest to you from the [available regions](/docs/vitess/regions#available-regions) or leave it blank.
  </Step>
</Steps>

That's it! Your database is ready to use. Next, let's connect it to the Go application and then add some data.

## Connect to the Go app

There are **two ways to connect** your Go app to PlanetScale:

* With an auto-generated username and password
* Using the PlanetScale proxy with the CLI

Both options are covered below.

### Option 1: Connect with username and password (Recommended)

<Steps>
  <Step>
    Create a username and password with the PlanetScale CLI by running:

    ```bash  theme={null}
    pscale password create <DATABASE_NAME> <BRANCH_NAME> <PASSWORD_NAME>
    ```

    A default branch, `main`, is created when you create the database, so you can use that for `BRANCH_NAME`.

    <Note>
      The `PASSWORD_NAME` value represents the name of the username and password being generated. You can have multiple
      credentials for a branch, so this gives you a way to categorize them. To manage your passwords in the dashboard, go to
      your database dashboard page, click "Settings", and then click "Passwords".
    </Note>

    Take note of the values returned to you, as you won't be able to see this password again.
  </Step>

  <Step>
    Open the `.env` file in your Go app and update `DSN` as follows:

    ```bash  theme={null}
    DSN="<USERNAME>:<PASSWORD>@tcp(<ACCESS HOST URL>)/<DATABASE_NAME>?tls=true&interpolateParams=true"
    ```

    Fill in `USERNAME`, `PASSWORD`, `ACCESS HOST URL`, and `DATABASE_NAME` with the appropriate values from the CLI output above. Do not remove the parentheses around the access host URL.

    You can also get these exact values to copy/paste from your PlanetScale dashboard. In the dashboard, click on the database > "**Connect**" > "**Connect with**" language dropdown > "**Go**".
  </Step>
</Steps>

### Option 2: Connect with the PlanetScale proxy

To connect with the PlanetScale proxy, you need the [PlanetScale CLI](https://github.com/planetscale/cli).

<Steps>
  <Step>
    Open a connection by running the following:

    ```bash  theme={null}
    pscale connect <DATABASE_NAME> <BRANCH_NAME>
    ```

    If you're following this guide exactly and haven't created any branches, you can use the default branch, `main`.
  </Step>

  <Step>
    A secure connection to your database will be established, and you'll see a local address you can use to connect to your application.
  </Step>

  <Step>
    Open the `.env` file in your Go app and update it as follows:

    ```bash  theme={null}
    DSN="mysql://root@tcp(127.0.0.1:<PORT>)/<DATABASE_NAME>?interpolateParams=true"
    ```

    The connection uses port `3306` by default, but if that's being used, it will pick a random port. Make sure you paste in whatever port is returned in the terminal. Fill in the database name as well.
  </Step>
</Steps>

## Run migrations and seeder

Now that you're connected let's add some data to see it in action. The sample application has an endpoint that you can use to run migrations to create your `categories` and `products` tables. It will seed your database with sample product and category data. You can find this in `main.go`.

Let's run those now.

<Steps>
  <Step>
    First, start your Go app with:

    ```bash  theme={null}
    go run .
    ```
  </Step>

  <Step>
    Next, navigate to [`localhost:8080/seed`](http://localhost:8080/seed) to run the migrations and the seeder.
  </Step>

  <Step>
    You can now see the products and categories:

    * Get all products — [`localhost:8080/products`](http://localhost:8080/products)
    * Get all categories — [`localhost:8080/categories`](http://localhost:8080/categories)
    * Get a single product — [`localhost:8080/product/{id}`](http://localhost:8080/products/1)
    * Get a single category — [`localhost:8080/category/{id}`](http://localhost:8080/categories/1)
  </Step>
</Steps>

### Foreign key constraints

If you're using GORM in your Go application and [do not want to use foreign key constraints](/docs/vitess/operating-without-foreign-key-constraints), you can turn them off with this line in the `main.go` file of the Go starter application:

```go  theme={null}
// ...
DisableForeignKeyConstraintWhenMigrating: true,
// ...
```

If you prefer to use foreign key constraints in your Go application, you can skip the previous step. However, you need to first enable [foreign key constraint](/docs/vitess/foreign-key-constraints) support in your database settings page.

## Add data manually

If you want to continue to play around with adding data on the fly, you have a few options:

* PlanetScale CLI shell
* PlanetScale dashboard console
* Your favorite MySQL client (for a list of tested MySQL clients, review our article on [how to connect MySQL GUI applications](/docs/vitess/tutorials/connect-mysql-gui))

The first two options are covered below.

### Add data with PlanetScale CLI

You can use the PlanetScale CLI to open a MySQL shell to interact with your database.

You may need to install the MySQL command line client if you haven't already.

<Steps>
  <Step>
    Run the following command in your terminal:

    ```bash  theme={null}
    pscale shell <DATABASE_NAME> <BRANCH_NAME>
    ```

    This will open up a MySQL shell connected to the specified database and branch.

    <Note>
      A branch, `main`, was automatically created when you created your database, so you can use that for `BRANCH_NAME`.
    </Note>
  </Step>

  <Step>
    Add a record to the `products` table:

    ```sql  theme={null}
    INSERT INTO `products` (name, description, image, category_id)
    VALUES  ('Spaceship', 'Get ready for the trip of a lifetime', 'https://via.placeholder.com/300.png', 2);
    ```

    The value `id` will be filled with a default value.
  </Step>

  <Step>
    You can verify it was added in the PlanetScale CLI MySQL shell with:

    ```sql  theme={null}
    SELECT * FROM products;
    ```
  </Step>

  <Step>
    Type `exit` to exit the shell.

    You can now navigated the [Go products page](http://localhost:8080/products) to see the new record.
  </Step>
</Steps>

### Add data with PlanetScale dashboard console

If you don't care to install MySQL client or the PlanetScale CLI, another quick option is using the MySQL console built into the PlanetScale dashboard.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-gorm-app/console-2.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=a00e4fc53cc45987bc4558fee82a44de" alt="PlanetScale console insert and select example" data-og-width="1842" width="1842" data-og-height="1187" height="1187" data-path="docs/images/assets/docs/tutorials/connect-go-gorm-app/console-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-gorm-app/console-2.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=1f278c743487d389eba5928cb427a8b6 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-gorm-app/console-2.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=b69e99444ad36c675ee312f935aff94e 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-gorm-app/console-2.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=e7014d42f8627415a2e61afa4a44c728 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-gorm-app/console-2.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=8de7475162022d1b0db215587f008118 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-gorm-app/console-2.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=3668fe02052f60ef9848ea068d6dfe4b 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-go-gorm-app/console-2.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=d79607d812890b6c053392c6a91439d2 2500w" />
</Frame>

<Steps>
  <Step>
    Go to your [PlanetScale dashboard](https://app.planetscale.com) and select your Go database.
  </Step>

  <Step>
    Click on "**Console**".
  </Step>

  <Step>
    Select the `main` branch and click "**Connect**".
  </Step>

  <Step>
    Add a new record to the `product` table with:

    ```sql  theme={null}
    INSERT INTO `products` (name, description, image, category_id)
    VALUES  ('Spaceship', 'Get ready for the trip of a lifetime', 'https://via.placeholder.com/300.png', 2);
    ```
  </Step>

  <Step>
    You can confirm that it was added by running:

    ```sql  theme={null}
    SELECT * FROM products;
    ```
  </Step>
</Steps>

You can now refresh the [Go products page](http://localhost:8080/products) to see the new record.

## What's next?

Once you're done with initial development, you can enable [safe migrations](/docs/vitess/schema-changes/safe-migrations) on your `main` production branch to protect it against direct schema changes and enable zero-downtime schema migrations.

When you're reading to make more schema changes, you'll [create a new branch](/docs/vitess/schema-changes/branching) off of your production branch. Branching your database creates an isolated copy of your production schema so that you can easily test schema changes in development. Once you're happy with the changes, you'll open a [deploy request](/docs/vitess/schema-changes/deploy-requests). This will generate a diff showing the changes that will be deployed, making it easy for your team to review.

Learn more about how PlanetScale allows you to make [non-blocking schema changes](/docs/vitess/schema-changes) to your database tables without locking or causing downtime for production databases.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Connect a Laravel application to PlanetScale
Source: https://planetscale.com/docs/vitess/tutorials/connect-laravel-app

In this tutorial, you'll learn how to connect a Laravel 12 application to a PlanetScale MySQL database using a sample Laravel starter app.

## Prerequisites

* [PHP](https://www.php.net/manual/en/install.php) — This tutorial uses `v8.2`
* [Composer](https://getcomposer.org/)
* A [PlanetScale account](https://auth.planetscale.com/sign-up)

## Set up the Laravel app

This guide will integrate [a simple Laravel 12 app](https://github.com/planetscale/planetscale-laravel-mysql) with PlanetScale. The application displays a list of users from your PlanetScale database. The sample repo contains migrations and seed data to create and populate the `users` table. If you have an existing application, you can also use that.

<Steps>
  <Step>
    Clone the starter Laravel application:

    ```bash  theme={null}
    git clone https://github.com/planetscale/planetscale-laravel-mysql.git
    ```
  </Step>

  <Step>
    Enter into the folder and install the dependencies:

    ```bash  theme={null}
    cd planetscale-laravel-mysql
    composer install
    ```

    You may need to run `composer update` if you haven't updated in a while.
  </Step>

  <Step>
    Copy the `.env.example` file into `.env` and generate the app key:

    ```bash  theme={null}
    cp .env.example .env
    php artisan key:generate
    ```
  </Step>
</Steps>

## Set up the database

Next, you need to set up your PlanetScale database and connect to it in the Laravel application.

<Note>
  If you have an existing cloud-hosted database, you can choose the "**Import**" option to import your database to PlanetScale using our Import tool. If you go this route, we recommend using our [Database Imports documentation](/docs/vitess/imports/database-imports).
</Note>

If this is your first time in the dashboard, you'll be prompted to create an organization and go through the database creation walkthrough. Otherwise, click "**New database**" > "**Create new database**".

* **Name** — You can use any name with lowercase, alphanumeric characters, or underscores. We also permit dashes, but don't recommend them, as they may need to be escaped in some instances.
* **Region** — Choose the [region](/docs/vitess/regions#available-regions) closest to you or your application. It's important to note if you intend to make this branch a production branch, you will not be able to change the region later, so choose the region with this in mind.
* **Storage option** — Choose a storage option. You can choose between network-attached storage or [Metal](/docs/metal) for storage. For more information, see the [plans documentation](/docs/vitess/tutorials/connect-laravel-app).
* **Cluster size** — Select the [desired cluster size](/docs/planetscale-plans) for your database.

Finally, click "**Create database**".

A [production branch](/docs/vitess/schema-changes/branching), `main`, is automatically created when you create your database. [Safe migrations](/docs/vitess/schema-changes/safe-migrations) are turned off by default, so you can make schema changes directly to this branch. Once you're ready for production, you can turn on safe migrations to protect from accidental schema changes and enable zero-downtime deployments.

That's it! Your database is ready to use. Next, let's connect it to the Laravel application and then add some data.

## Connect to the Laravel app

There are **two ways to connect** to PlanetScale:

* With an auto-generated username and password
* Using the PlanetScale proxy with the CLI

Both options are covered below.

### Option 1: Connect with username and password (Recommended)

First, you need to generate a database username and password so that you can use it to connect to your application.

You'll be presented with this option after creating your database. You can also access the password creation page by clicking "**Connect**" -> "**Create password**".

As long as you're an organization administrator, this will generate a username and password that has administrator privileges to the database.

<Tip>
  If the password value is blurred, you need to click "**New password**" to generate a new one.
</Tip>

Click "Laravel" as the framework, then copy the contents of the `.env` tab and paste them into your own `.env` file in your Laravel application. The structure will look like this:

```bash  theme={null}
DB_CONNECTION=mysql
DB_HOST=<ACCESS HOST URL>
DB_PORT=3306
DB_DATABASE=<DATABASE_NAME>
DB_USERNAME=<USERNAME>
DB_PASSWORD=<PASSWORD>
MYSQL_ATTR_SSL_CA=/etc/ssl/cert.pem
```

For `DB_DATABASE`, you can use your PlanetScale database name directly if you have a *single unsharded keyspace*. If you have a sharded keyspace, you'll need to use `@primary`. This will automatically direct incoming queries to the correct keyspace/shard. For more information, see the [Targeting the correct keyspace documentation](/docs/vitess/sharding/targeting-correct-keyspace).

The `MYSQL_ATTR_SSL_CA` value is platform-dependent. Please refer to our documentation around [how to connect to PlanetScale securely](/docs/vitess/connecting/secure-connections#ca-root-configuration) for the platform you're using.

### Option 2: Connect with the PlanetScale proxy

To connect with the PlanetScale proxy, you need to install and use the [PlanetScale CLI](https://github.com/planetscale/cli).

<Steps>
  <Step>
    Open a connection by running the following:

    ```bash  theme={null}
    pscale connect <DATABASE_NAME> <BRANCH_NAME>
    ```

    If you're following this guide exactly and haven't created any branches, you can use the default branch, `main`.
  </Step>

  <Step>
    A secure connection to your database will be established and you'll see a local address you can use to connect to your application.
  </Step>

  <Step>
    Open the `.env` file in your Laravel app and update it as follows:

    ```bash  theme={null}
    DB_CONNECTION=mysql
    DB_HOST=127.0.0.1
    DB_PORT=3306 # Get this from the output of the previous step
    DB_DATABASE=<DATABASE_NAME>
    DB_USERNAME=
    DB_PASSWORD=
    ```

    The connection uses port `3306` by default, but if that's being used, it will pick a random port. Make sure you paste in whatever port is returned in the terminal. You can leave `DB_USERNAME` and `DB_PASSWORD` blank.
  </Step>
</Steps>

## Run migrations and seeder

Now that you're connected, let's add some data to see it in action. The sample application comes with some default Laravel migration files, `database/migrations/`, to create the database schema. It also contains a user seeder to seed some mock user data.

<Note>
  Laravel uses foreign key constraints by default. PlanetScale, however, has foreign key constraint support turned off by default. For this tutorial, we're keeping the Laravel defaults, so you need to enable [foreign key constraint](/docs/vitess/foreign-key-constraints) support in your database settings page. Click the checkbox next to "Allow foreign key constraints" and press "Save database settings".
</Note>

Let's migrate and seed the database now.

<Steps>
  <Step>
    In the root of the Laravel project, run the following to migrate and seed the database:

    ```bash  theme={null}
    php artisan migrate --seed
    ```
  </Step>

  <Step>
    Start the application:

    ```bash  theme={null}
    php artisan serve
    ```
  </Step>
</Steps>

You can view the application at [http://localhost:8000](http://localhost:8000).

1. Refresh your Laravel homepage and you'll see a list of users.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/laravel-users.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=5430150ba4ac9f7e1fbef8adf04f1820" alt="Laravel PlanetScale starter app homepage" data-og-width="2956" width="2956" data-og-height="1726" height="1726" data-path="docs/images/laravel-users.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/laravel-users.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=5518d3da4cf8aa17e3502d7ed3c5aead 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/laravel-users.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=8f4d31ac6091c921ae28f5353fcc199a 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/laravel-users.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=77a92b0460cd947aa5fd1ab5d72efef4 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/laravel-users.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=80f5542a3c9108f59cd7c7ea8b8c0436 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/laravel-users.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=9d334cea4af6626d2e0b06c85bdcabc3 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/laravel-users.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=aca493c4bcd5948322e5a9a21ba6211f 2500w" />
</Frame>

## Add data manually

If you want to continue to play around with adding data on the fly, you have a few options:

* PlanetScale [dashboard console](/docs/vitess/web-console)
* [Laravel Tinker](hhttps://laravel.com/docs/12.x/artisan#tinker)
* [PlanetScale CLI shell](/docs/cli/shell)
* Your favorite MySQL client (for a list of tested MySQL clients, review our article on [how to connect MySQL GUI applications](/docs/vitess/tutorials/connect-mysql-gui))

The first option is covered below.

### Add data in PlanetScale dashboard console

PlanetScale has a [built-in console](/docs/vitess/web-console) where you can run MySQL commands against your branches.

By default, web console access to production branches is disabled to prevent accidental deletion. From your database's dashboard page, click on the "**Settings**" tab, check the box labelled "**Allow web console access to production branches**", and click "**Save database settings**".

To access it, click "**Console**" > select your branch > "**Connect**".

From here, you can run MySQL queries and DDL against your database branch.

<Steps>
  <Step>
    Add a record to the `users` table:

    ```sql  theme={null}
    UPDATE users
    SET email = 'cyrus@planetscale.com'
    WHERE id=1;
    ```
  </Step>

  <Step>
    Refresh the Laravel homepage to see the new record. You can also verify it was added in the console with:

    ```sql  theme={null}
    SELECT * FROM users;
    ```

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/laravel-web-console.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=05af89bb7e3461c8d275b78e8fe9797d" alt="PlanetScale web console" data-og-width="2974" width="2974" data-og-height="1836" height="1836" data-path="docs/images/laravel-web-console.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/laravel-web-console.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=af415642ecc7e9746c02507d26aa27e0 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/laravel-web-console.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=32b90fd9157d0060f815d1de717c6262 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/laravel-web-console.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=359c73e3748273530b00e6b27a04d114 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/laravel-web-console.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=826dde1eae6516114b6f8dddad1d639d 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/laravel-web-console.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=4ade12ca71d1fced2c2830c3ae6d73bf 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/laravel-web-console.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=be2d156b6bcad346a93721eb91fb8ded 2500w" />
    </Frame>
  </Step>
</Steps>

## What's next?

Once you're done with initial development, you can enable [safe migrations](/docs/vitess/schema-changes/safe-migrations) to protect from accidental schema changes and enable zero-downtime deployments.

To learn more about PlanetScale, take a look at the following resources:

* [PlanetScale workflow](/docs/vitess/best-practices) — Quick overview of the PlanetScale workflow: branching, non-blocking schema changes, deploy requests, and reverting a schema change.
* [PlanetScale branching](/docs/vitess/schema-changes/branching) — Learn how to utilize branching to ship schema changes with no locking or downtime.
* [PlanetScale CLI](/docs/cli) — Power up your workflow with the PlanetScale CLI. Every single action you just performed in this quickstart (and much more) can also be done with the CLI.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Connect a MySQL GUI to PlanetScale
Source: https://planetscale.com/docs/vitess/tutorials/connect-mysql-gui



## Introduction

In this tutorial, you'll learn how to connect to a PlanetScale database using a MySQL GUI. While this tutorial uses Sequel Ace as a demonstration, many applications that connect to MySQL databases will support connecting to and querying a PlanetScale database as long as the applicaton supports connecting over SSL.

## Gather the credentials

To connect to a PlanetScale database, you'll need four pieces of information:

* The database name
* Host name
* Username
* Password

The easiest way to gather this information is by selecting the **Connect** button from the **Dashboard** tab. Then, on the **Connect** page, select the branch that you wish to connect to and click the **Create password** button. Within the **Select a language or framework** section, select "Other" to display the connection details as a list instead of a language or framework-specific connection string.

<Note>
  As a security best practice, passwords are only displayed when they are created.
</Note>

## Connect to the database

In the application you are using, enter the access information you gathered in the previous step into the appropriate fields. Make sure to check **"Require SSL"** as SSL is required to connect to a PlanetScale database. Click **"Connect"** once you are finished.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-mysql-gui/ace-connect.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=71ebdc9541dfc89bdc6bce8be56096d9" alt="The new connection window in Sequel Ace." data-og-width="1390" width="1390" data-og-height="953" height="953" data-path="docs/images/assets/docs/tutorials/connect-mysql-gui/ace-connect.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-mysql-gui/ace-connect.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=f469e702d200c1b9510d6b64b5a01424 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-mysql-gui/ace-connect.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=4f11670130c96ae8eab1fc6c2c8c23eb 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-mysql-gui/ace-connect.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=5a82118faac825e0d8d5d1402c032b44 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-mysql-gui/ace-connect.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=6e564e809d739026303d4883b2abd11f 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-mysql-gui/ace-connect.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=79e2f3ac9133d7c07327373ec7f5cfc0 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-mysql-gui/ace-connect.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=0b46a93847eee2ecf508630ad08419dd 2500w" />
</Frame>

If the connection is successful, you should be able to query your database and perform other [supported operations](/docs/vitess/troubleshooting/mysql-compatibility).

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-mysql-gui/ace-query.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=c09b6d81290dc195807777bd69c03c5e" alt="A sample query in Sequel Ace." data-og-width="1389" width="1389" data-og-height="909" height="909" data-path="docs/images/assets/docs/tutorials/connect-mysql-gui/ace-query.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-mysql-gui/ace-query.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=fbad5b50c91eb34868a9cc5ad1472fc4 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-mysql-gui/ace-query.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=cbbc20d09a9d798f73e9461083be6387 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-mysql-gui/ace-query.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=f0c2846c07edfb8df59f88d04fc9bd94 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-mysql-gui/ace-query.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=50f35996899c6d4c8b2f81359f867a22 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-mysql-gui/ace-query.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=838489b7ea951b2c94123f08b86d4305 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-mysql-gui/ace-query.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=eb1fa75bb857be2e1654f297479e1907 2500w" />
</Frame>

## Caveats

While many standard MySQL statements are supported, there are a few caveats worth calling out:

<Steps>
  <Step>
    Each branch of a PlanetScale database is considered an isolated MySQL database. You'll need separate connection details per branch.
  </Step>

  <Step>
    Production branches with [safe migrations](/docs/vitess/schema-changes/safe-migrations) enforce the use of [branching](/docs/vitess/schema-changes/branching) and [deploy requests](/docs/vitess/schema-changes/deploy-requests) to safely make schema changes and do not support direct DDL as a result. However, DDL is supported on development branches and production branches without safe migrations enabled (not recommended).
  </Step>

  <Step>
    Creating new databases is not supported using any GUI tool.
  </Step>
</Steps>

## Tested GUIs

The following MySQL GUI applications have been tested and confirmed to work with PlanetScale databases:

<Columns cols={2}>
  <Card title="Sequel Ace" icon="rocket-launch" href="https://sequel-ace.com/" horizontal />

  <Card title="TablePlus" icon="table" href="https://tableplus.com/" horizontal />

  <Card title="MySQL Workbench" icon="desktop" href="https://www.mysql.com/products/workbench/" horizontal />

  <Card title="JetBrains DataGrip" icon="code" href="https://planetscale.com/blog/blog/using-planetscale-with-jetbrains-datagrip-mysql-gui" horizontal />
</Columns>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Connect a Next.js application to PlanetScale
Source: https://planetscale.com/docs/vitess/tutorials/connect-nextjs-app



In this tutorial, you'll create a [Next.js](https://nextjs.org/) application that uses [Tailwind CSS](https://tailwindcss.com/) for styling and [Prisma](https://www.prisma.io/) to connect to a [PlanetScale](/) database.

## Prerequisites

* [Node.js](https://nodejs.org/en/download/)
* A [PlanetScale account](https://auth.planetscale.com/sign-up)

## Set up the database

If this is your first time in the dashboard, you'll be prompted to go through a database creation walkthrough where you'll create a new database. Otherwise, click "**New database**" > "**Create new database**".

* **Name** — You can use any name with lowercase, alphanumeric characters, or underscores. We also permit dashes, but don't recommend them, as they may need to be escaped in some instances.
* **Plan type** — Select the [desired plan](/docs/planetscale-plans) for your database.
* **Region** — Choose the [region](/docs/vitess/regions#available-regions) closest to you or your application. It's important to note if you intend to make this branch a production branch, you will not be able to change the region later, so choose the region with this in mind.

Finally, click "**Create database**".

<Note>
  If you have an existing cloud-hosted database, you can also choose the "Import" option to import your database to PlanetScale using our Import tool. If you go this route, we recommend using our [Database Imports documentation](/docs/vitess/imports/database-imports).
</Note>

A [production branch](/docs/vitess/schema-changes/branching), `main`, is automatically created when you create your database. Production branches are highly available, protected database that you can connect your production application to. Once you are satisfied with your initial development, you may enable [safe migrations](/docs/vitess/schema-changes/safe-migrations) to enable zero-downtime migrations and protect the branch from accidental data deletion.

## Set up the starter Next.js app

Now that you have your database, clone the [Next.js starter repository](https://github.com/planetscale/nextjs-starter), or grab your own project.

```sh  theme={null}
git clone https://github.com/planetscale/nextjs-starter
```

Install the dependencies with:

```sh  theme={null}
cd nextjs-starter
npm install
```

Create your `.env` file by renaming the `.env.example` file to `.env`:

```sh  theme={null}
mv .env.example .env
```

## Generate a connection string

Next, you need to generate a database username and password so that you can use it to connect to your application.

In your PlanetScale dashboard, select your database, click "**Connect**", and select "**Prisma**" from the "**Connect with**" dropdown.

As long as you're an organization administrator, this will generate a username and password that has administrator privileges to the database.

Copy the `DATABASE_URL` string from the `.env` tab and paste it into your own `.env` file. The structure will look like this:

```sh  theme={null}
DATABASE_URL='mysql://<USERNAME>:<PASSWORD>@<HOST>/<DATABASE_NAME>?sslaccept=strict'
```

For `DATABASE_NAME`, you can use your PlanetScale database name directly if you have a *single unsharded keyspace*. If you have a sharded keyspace, you'll need to use `@primary`. This will automatically direct incoming queries to the correct keyspace/shard. For more information, see the [Targeting the correct keyspace documentation](/docs/vitess/sharding/targeting-correct-keyspace).

Your PlanetScale database should now be connected to your application.

## Add the schema and data

With the database connected, you're now ready to add a schema, and read/write data.

The sample repo we shared above includes a pre-built schema and some seed data built with Prisma. Push the database schema to your PlanetScale database with:

```sh  theme={null}
npx prisma db push
```

Run the seed script. This will populate your database with `Product` and `Category` data:

```sh  theme={null}
npm run seed
```

## Run the app

Run the app with following command:

```sh  theme={null}
npm run dev
```

Open your browser at [localhost:3000](http://localhost:3000) to see the running application.

## Deploy your app

After you have your application running locally, you may want to deploy it to production. Your database branch (`main` by default) is already a production database branch. You should also enable [safe migrations](/docs/vitess/schema-changes/safe-migrations), which protects your production branch from accidental schema changes. This can be done from the PlanetScale dashboard by clicking the same **"cog"** once the branch has been promoted to production.

In the modal that will appear, toggle the option labeled **"Enable safe migrations"**, then click the **"Enable safe migrations"**. This will close the modal and save the setting.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-nextjs-app/prod-branch-options-modal.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=188775a86603816ad0f0c900cdf1fe85" alt="Enable safe migrations" data-og-width="1368" width="1368" data-og-height="1134" height="1134" data-path="docs/images/assets/docs/tutorials/connect-nextjs-app/prod-branch-options-modal.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-nextjs-app/prod-branch-options-modal.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=5f5acc74e56d8aedcaa5d5e5b347f948 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-nextjs-app/prod-branch-options-modal.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=a217c533e52e021ff824e56d6a7ee91a 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-nextjs-app/prod-branch-options-modal.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=f103551ae71cafee6b6c16ae0c5a2211 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-nextjs-app/prod-branch-options-modal.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=9651667ab473245471968d8c4746b038 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-nextjs-app/prod-branch-options-modal.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=76f01afb085141b8708705321c0c28f3 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-nextjs-app/prod-branch-options-modal.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=8361c28a1fcb824d955b8e9f70e64891 2500w" />
</Frame>

### Deploy to Vercel

If you'd like to deploy to Vercel, check out our [Deploy to Vercel documentation](/docs/vitess/tutorials/deploy-to-vercel).

### Deploy to Netlify

If you'd like to deploy to Netlify, check out our [Deploy to Netlify documentation](/docs/vitess/tutorials/deploy-to-netlify).

<Note>
  If you are deploying the `nextjs-starter` repo, the `Netlify.toml` file in this repository includes the configuration for you to customize the `PLANETSCALE_PRISMA_DATABASE_URL` property on the initial deploy.
</Note>

## What's next?

To learn more about PlanetScale, take a look at the following resources:

* [PlanetScale workflow](/docs/vitess/best-practices) — Quick overview of the PlanetScale workflow: branching, non-blocking schema changes, deploy requests, and reverting a schema change.
* [PlanetScale branching](/docs/vitess/schema-changes/branching) — Learn how to utilize branching to ship schema changes with no locking or downtime.
* [PlanetScale CLI](/docs/cli) — Power up your workflow with the PlanetScale CLI. Every single action you just performed in this quickstart (and much more) can also be done with the CLI.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Connect a Node.js application to PlanetScale
Source: https://planetscale.com/docs/vitess/tutorials/connect-nodejs-app

In this tutorial, you'll create a simple Node.js and Express.js application and connect it to a PlanetScale database.

<Tip>
  Already have a Node.js application and just want to connect to PlanetScale? Check out the [Node.js quick connect repo](https://github.com/planetscale/connection-examples/tree/main/nodejs).
</Tip>

## Prerequisites

<Card title="Node.js" icon="node" horizontal href="https://nodejs.org/en/download/" />

## Set up the database

First, create a new database with the following command:

```bash  theme={null}
pscale database create <DATABASE_NAME>
```

Next, let's add some data to the database. You'll create a new table called `users` and add one record to it.

To do this, use the PlanetScale CLI shell to open a MySQL shell where you can manipulate your database. You may need to [install the MySQL command line client](/docs/cli/planetscale-environment-setup) if you haven't already.

```bash  theme={null}
pscale shell <DATABASE_NAME> <BRANCH_NAME>
```

<Note>
  A branch, `main`, was automatically created when you created your database, so you can use that for `BRANCH_NAME`.
</Note>

Create the `users` table:

```sql  theme={null}
CREATE TABLE `users` (
  `id` int NOT NULL AUTO_INCREMENT PRIMARY KEY,
  `email` varchar(255) NOT NULL,
  `first_name` varchar(255),
  `last_name` varchar(255)
);
```

Then, add a record to it with:

```sql  theme={null}
INSERT INTO `users` (email, first_name, last_name)
VALUES  ('jp@example.com', 'Jean', 'Pixy');
```

You can verify it was added with:

```sql  theme={null}
select * from users;
```

```
+----+----------------+------------+-----------+
| id | email          | first_name | last_name |
+----+----------------+------------+-----------+
|  1 | jp@example.com | Jean       | Pixy      |
+----+----------------+------------+-----------+
```

Next, you'll set up the Express starter application.

## Set up the starter Node.js app

Clone the starter repository:

```bash  theme={null}
git clone https://github.com/planetscale/express-example.git
```

Enter into the folder and install the dependencies with:

```bash  theme={null}
cd express-example
npm install
```

Now that your application is set up and the database is ready to be used, let's connect them.

## Connect to PlanetScale with Express.js

There are **two ways to connect** to PlanetScale:

* With an auto-generated username and password
* Using the PlanetScale proxy with the CLI

Both options are covered below.

### Option 1: Connect with username and password (Recommended)

These instructions show you how to generate a set of credentials with [the PlanetScale CLI](/docs/cli/planetscale-environment-setup).

You can also get these exact values to copy/paste from your [PlanetScale dashboard](https://app.planetscale.com). In the dashboard, click on the database > "**Connect**" > "**Connect with**" language dropdown > "**Node.js**". If the password is blurred, click "**New password**". Skip to step 3 once you have these credentials.

1. Authenticate the CLI with the following command:

   ```bash  theme={null}
   pscale auth login
   ```

2. Using the PlanetScale CLI, create a new username and password for the branch of your database:

   ```bash  theme={null}
   pscale password create <DATABASE_NAME> <BRANCH_NAME> <PASSWORD_NAME>
   ```

   <Note>
     The `PASSWORD_NAME` value represents the name of the username and password being generated. You can have multiple credentials for a branch, so this gives you a way to categorize them. To manage your passwords in the dashboard, go to your database dashboard page, click "Settings", and then click "Passwords".
   </Note>

   Take note of the values returned to you, as you won't be able to see this password again.

   ```
   Password production-password was successfully created.
   Please save the values below as they will not be shown again

     NAME                  USERNAME       ACCESS HOST URL                     ROLE               PASSWORD
    --------------------- -------------- ----------------------------------- ------------------ -------------------------------------------------------
     production-password   xxxxxxxxxxxxx   xxxxxx.us-east-2.psdb.cloud   Can Read & Write   pscale_pw_xxxxxxx
   ```

3. Next, create your `.env` file by renaming the `.env.example` file to `.env`:

   ```bash  theme={null}
   mv .env.example .env
   ```

4. Use the values from the CLI output in step 1 to construct your connection string that will be used to connect your Node app to your PlanetScale database. Create your connection string in the following format:

   ```
   mysql://<USERNAME>:<PLAIN_TEXT_PASSWORD>@<ACCESS_HOST_URL>/<DATABASE_NAME>?ssl={"rejectUnauthorized":true}
   ```

5. In the `.env` file, fill in the `DATABASE_URL` variable with the value you constructed above. It should look something like this:

   ```bash  theme={null}
   DATABASE_URL=mysql://xxxxxxxxxxxxx:pscale_pw_xxxxxxx@xxxxxx.us-east-2.psdb.cloud/express_database?ssl={"rejectUnauthorized":true}
   ```

6. Finally, run your Express application with:

   ```bash  theme={null}
   node app.js
   ```

Navigate to [http://localhost:3000](http://localhost:3000) and you'll see the data from your `users` table!

### Option 2: Using the PlanetScale proxy with the CLI

Use the following command to create a connection to your database and start the application:

```bash  theme={null}
pscale connect <DATABASE_NAME> <BRANCH_NAME> --execute 'node app.js'
```

<Note>
  Running `pscale connect` with the execute flag will pass a `DATABASE_URL` to the Node application, enabling it to connect to PlanetScale. Don't forget to look in `app.js` to see how the DATABASE\_URL is used.
</Note>

Navigate to [http://localhost:3000](http://localhost:3000) and you'll see the data from your `users` table!

## What's next?

Learn more about how PlanetScale allows you to make [non-blocking schema changes](/docs/vitess/schema-changes) to your database tables without locking or causing downtime for production databases. If you're interested in learning how to secure your application when connecting to PlanetScale,
please read [Connecting to PlanetScale securely](/docs/vitess/connecting/secure-connections).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Connect a PHP application to PlanetScale
Source: https://planetscale.com/docs/vitess/tutorials/connect-php-app

In this tutorial, you'll learn how to connect a PHP application to a PlanetScale MySQL database with a sample PHP starter app using [MySQLi](https://www.php.net/manual/en/book.mysqli.php).

<Tip>
  Already have a PHP application and just want to connect to PlanetScale? Check out the [PHP quick connect repo](https://github.com/planetscale/connection-examples/tree/main/php).
</Tip>

## Prerequisites

* [PHP](https://www.php.net/manual/en/install.php) — This tutorial uses `v8.1`
* [Composer](https://getcomposer.org/)
* A [PlanetScale account](https://auth.planetscale.com/sign-up)
* [PlanetScale CLI](https://github.com/planetscale/cli) (Optional) — You can also follow this tutorial using just the PlanetScale admin dashboard, but the CLI will make setup quicker.

## Set up the PHP app

This guide uses [a simple PHP app](https://github.com/planetscale/php-example) that displays a list of products stored in a PlanetScale database. If you have an existing application, you can also use that.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-php-app/example.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=01938ff12f9975f684c6861a99524fa7" alt="PHP sample application homepage priority" data-og-width="1500" width="1500" data-og-height="990" height="990" data-path="docs/images/assets/docs/tutorials/connect-php-app/example.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-php-app/example.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=336c29c2c02992599f6e4a323595cdc5 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-php-app/example.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=bdd437690f4b5af41e426cbcf429c811 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-php-app/example.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=7eef53276615af1002e04cf08ea3d309 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-php-app/example.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=2847fda7092814dfe7a59650d8604d66 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-php-app/example.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=51c46dee5d5569961f215ac10639f737 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-php-app/example.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=ab15dac78852b9800781dfd272f125a3 2500w" />
</Frame>

<Steps>
  <Step>
    Clone the starter PHP application:

    ```bash  theme={null}
    git clone https://github.com/planetscale/php-example.git
    ```
  </Step>

  <Step>
    Enter into the folder and install the dependencies:

    ```bash  theme={null}
    cd php-example
    composer install
    ```
  </Step>

  <Step>
    Rename the `.env.example` file to `.env`:

    ```bash  theme={null}
    mv .env.example .env
    ```
  </Step>

  <Step>
    Start the application:

    ```bash  theme={null}
    php -S localhost:8000
    ```
  </Step>
</Steps>

You can view the application at [http://localhost:8000](http://localhost:8000).

## Set up the database

Next, you need to set up your PlanetScale database and connect it to the PHP application.

You can create a database either in the [PlanetScale dashboard](https://app.planetscale.com) or from the PlanetScale CLI.

This guide will use the CLI, but you can follow the database setup instructions in the [PlanetScale quickstart guide](/docs/vitess/tutorials/planetscale-quick-start-guide#create-a-database) if you prefer the dashboard. Just create the database and then come back here to continue.

<Steps>
  <Step>
    Install the [PlanetScale CLI](/docs/cli/planetscale-environment-setup).
  </Step>

  <Step>
    Authenticate in the CLI with the following command:

    ```bash  theme={null}
    pscale auth login
    ```
  </Step>

  <Step>
    Create a new database with the following command:

    ```bash  theme={null}
    pscale database create <DATABASE_NAME> --region <REGION_SLUG>
    ```
  </Step>
</Steps>

You can use any name with lowercase, alphanumeric characters, or underscores. You can also use dashes, but we don't recommend them, as they may need to be escaped in some instances.

For `REGION_SLUG`, choose a region closest to you from the [available regions](/docs/vitess/regions#available-regions) or leave it blank.

Your database is created with a default branch, `main`, which is meant to serve as your production database branch.

That's it! Your database is ready to use. Next, let's connect it to the PHP application and then add some data.

## Connect to the PHP app

There are **two ways to connect** to PlanetScale:

* With an auto-generated username and password
* Using the PlanetScale proxy with the CLI

Both options are covered below.

The environment variables you fill in next will be used in the [`db.php` file of the sample application](https://github.com/planetscale/php-example/blob/main/db.php):

```php  theme={null}
<?php
$hostname = $_ENV['HOST'];
$dbName = $_ENV['DATABASE'];
$username = $_ENV['USERNAME'];
$password = $_ENV['PASSWORD'];
$ssl = $_ENV['MYSQL_ATTR_SSL_CA'];
$port = 3306;

$mysqli = mysqli_init();
$mysqli->ssl_set(NULL, NULL, $ssl, NULL, NULL);
$mysqli->real_connect($hostname, $username, $password, $dbName, $port);

if ($mysqli->connect_error) {
    echo 'not connected to the database';
} else {
    echo "Connected successfully";
}
```

For `dbName`, you can use your PlanetScale database name directly if you have a *single unsharded keyspace*. If you have a sharded keyspace, you'll need to use `@primary`. This will automatically direct incoming queries to the correct keyspace/shard. For more information, see the [Targeting the correct keyspace documentation](/docs/vitess/sharding/targeting-correct-keyspace).

### Option 1: Connect with username and password (Recommended)

If you're not using the CLI, you can get the exact values to copy/paste from your PlanetScale dashboard. In the dashboard, select the branch you want to connect to from the infrastructure card (we're using `main`), click "**Connect**", and select "**PHP**" from the language dropdown. Copy these credentials, and then skip to step 2 to fill them in.

<Steps>
  <Step>
    Create a username and password with the PlanetScale CLI by running:

    ```bash  theme={null}
    pscale password create <DATABASE_NAME> <BRANCH_NAME> <PASSWORD_NAME>
    ```

    A default branch, `main`, was created when you created the database, so you can use that for `BRANCH_NAME`.

    <Note>
      The `PASSWORD_NAME` value represents the name of the username and password being generated. You can have multiple
      credentials for a branch, so this gives you a way to categorize them. To manage your passwords in the dashboard, go to
      your database dashboard page, click "Settings", and then click "Passwords".
    </Note>

    Take note of the values returned to you, as you won't be able to see this password again.
  </Step>

  <Step>
    Open the `.env` file in your PHP app:

    ```bash  theme={null}
    HOST=<ACCESS_HOST_URL>
    DATABASE=<DATABASE_NAME>
    USERNAME=<USERNAME>
    PASSWORD=<PASSWORD>
    MYSQL_ATTR_SSL_CA=
    ```

    Fill in your database name. For `USERNAME`, `PASSWORD`, and `HOST`, use the corresponding values from the CLI output.
  </Step>

  <Step>
    For `MYSQL_ATTR_SSL_CA`, use our [CA root configuration doc](/docs/vitess/connecting/secure-connections#ca-root-configuration) to find the correct value for your system. For example, if you're on MacOS, it would be:

    ```bash  theme={null}
    MYSQL_ATTR_SSL_CA=/etc/ssl/cert.pem
    ```
  </Step>

  <Step>
    Refresh your PHP homepage, and you should see the message that you're connected to your database!
  </Step>
</Steps>

### Option 2: Connect with the PlanetScale proxy

We recommend connecting with a username and password, but you can also open a quick connection with the PlanetScale proxy. You'll need the [PlanetScale CLI](https://github.com/planetscale/cli) for this option.

<Steps>
  <Step>
    Open a connection by running the following:

    ```bash  theme={null}
    pscale connect <DATABASE_NAME> <BRANCH_NAME>
    ```

    If you're following this guide exactly and haven't created any branches, you can use the default branch, `main`.
  </Step>

  <Step>
    A secure connection to your database will be established, and you'll see a local address you can use to connect to your application.
  </Step>

  <Step>
    Open the `.env` file in your PHP app and update it as follows:

    ```bash  theme={null}
    HOST=127.0.0.1
    PORT=3306 # Get this from the output of the previous step
    DATABASE=<DATABASE_NAME>
    ```

    The connection uses port `3306` by default, but we'll assign a random port if `3306` is in use. Make sure you paste in whatever port is returned in the terminal. Fill in the database name as well.
  </Step>

  <Step>
    Open `db.php` and replace it with the following:

    ```php expandable theme={null}
    <?php
    $hostname = $_ENV['HOST'];
    $dbName = $_ENV['DATABASE'];
    $port = $_ENV['PORT'];
    // $ssl = $_ENV['MYSQL_ATTR_SSL_CA'];

    $mysqli = mysqli_init();
    // $mysqli->ssl_set(NULL, NULL, $ssl, NULL, NULL);
    $mysqli->real_connect($hostname, '', '', $dbName, $port);

    if ($mysqli->connect_error) {
        echo 'not connected to the database';
    } else {
        echo "Connected successfully";
    }
    ```

    This removes all references to `username`, `password`, and `ssl`.

    <Note>
      It's important to make sure that you add the SSL check back if you switch back to username and password credentials.
      We're intentionally commenting it out instead of deleting it in case you switch back.
    </Note>
  </Step>

  <Step>
    Refresh your PHP homepage, and you should see the message that you're connected to your database!
  </Step>
</Steps>

## Add the schema and data

Now that you're connected to the database let's create the `products` and `categories` tables and add some data. There are a few ways to do this:

* PlanetScale CLI shell
* PlanetScale dashboard console
* Your favorite MySQL client (for a list of tested MySQL clients, review our article on [how to connect MySQL GUI applications](/docs/vitess/tutorials/connect-mysql-gui))

The first two options are covered below.

### Option 1: Add data with PlanetScale dashboard console

If you don't care to install the MySQL client or the PlanetScale CLI, another quick option is using the MySQL console built into the PlanetScale dashboard.

<Steps>
  <Step>
    Go to your [PlanetScale dashboard](https://app.planetscale.com) and select your PHP database.
  </Step>

  <Step>
    On the "**Dashboard**" page, you will need to select the gear icon and demote your `main` branch by toggling the "Promote to production" option. This is so you can create tables directly on your `main` branch.
  </Step>

  <Step>
    Click on the "**Console**" and select the `main` branch (or whatever development branch you used).
  </Step>

  <Step>
    Create the `categories` table:

    ```sql  theme={null}
    CREATE TABLE categories (
    id INT AUTO_INCREMENT NOT NULL,
    name VARCHAR(255) NOT NULL,
    description VARCHAR(255) NOT NULL,
    PRIMARY KEY(id)) DEFAULT CHARACTER SET utf8mb4 COLLATE `utf8mb4_unicode_ci` ENGINE = InnoDB;
    ```
  </Step>

  <Step>
    Create the `products` table:

    ```sql  theme={null}
    CREATE TABLE products (
    id INT AUTO_INCREMENT NOT NULL,
    name VARCHAR(255) NOT NULL,
    description VARCHAR(255) NOT NULL,
    image VARCHAR(255) NOT NULL,
    category_id INT NOT NULL,
    PRIMARY KEY(id)) DEFAULT CHARACTER SET utf8mb4 COLLATE `utf8mb4_unicode_ci` ENGINE = InnoDB;
    ```

    <Note>
      If you are using foreign key constraints, you must first enable [foreign key constraints](/docs/vitess/foreign-key-constraints) support in your database settings page.
    </Note>
  </Step>

  <Step>
    Add data to the `products` table with:

    ```sql  theme={null}
    INSERT INTO `products` (name, description, image, category_id) VALUES
    ('Shoes', 'Description for Shoes', 'https://via.placeholder.com/150.png', '1'),
    ('Hat', 'Description for Hats', 'https://via.placeholder.com/150.png', '1'),
    ('Bicycle', 'Description for Bicycle', 'https://via.placeholder.com/150.png', '4');
    ```
  </Step>

  <Step>
    Add data to the `categories` table with:

    ```sql  theme={null}
    INSERT INTO `categories` (name, description) VALUES
    ('Clothing', 'Description for Clothing'),
    ('Electronics', 'Description for Electronics'),
    ('Appliances', 'Description for Appliances'),
    ('Health', 'Description for Health');
    ```
  </Step>

  <Step>
    You can confirm that it was added by running:

    ```sql  theme={null}
    SELECT * FROM products;
    SELECT * FROM categories;
    ```
  </Step>
</Steps>

You can now refresh the [PHP homepage](http://localhost:8000) to see the new record.

### Option 2: Add data with PlanetScale CLI

You can use the PlanetScale CLI to open a MySQL shell to interact with your database.

You may need to [install the MySQL command line client](/docs/cli/planetscale-environment-setup) if you haven't already.

<Steps>
  <Step>
    Run the following command in your terminal:

    ```bash  theme={null}
    pscale shell <DATABASE_NAME> <BRANCH_NAME>
    ```

    This will open up a MySQL shell connected to the specified database and branch.

    <Note>
      A branch, `main`, was automatically created when you created your database, so you can use that for `BRANCH_NAME`.
    </Note>
  </Step>

  <Step>
    Create the `categories` table:

    ```sql  theme={null}
    CREATE TABLE categories (
      id INT AUTO_INCREMENT NOT NULL,
      name VARCHAR(255) NOT NULL,
      description VARCHAR(255) NOT NULL
    );
    ```
  </Step>

  <Step>
    Create the `products` table:

    ```sql  theme={null}
    CREATE TABLE products (
      id int NOT NULL AUTO_INCREMENT PRIMARY KEY,
      name VARCHAR(255) NOT NULL,
      description VARCHAR(255) NOT NULL,
      image VARCHAR(255) NOT NULL,
      category_id INT NOT NULL,
      KEY category_id_idx (category_id)
    );
    ```
  </Step>

  <Step>
    Add some records to the `products` table:

    ```sql  theme={null}
    INSERT INTO `products` (name, description, image, category_id) VALUES
    ('Shoes', 'Description for Shoes', 'https://via.placeholder.com/150.png', '1'),
    ('Hat', 'Description for Hats', 'https://via.placeholder.com/150.png', '1'),
    ('Bicycle', 'Description for Bicycle', 'https://via.placeholder.com/150.png', '4');
    ```

    The value `id` will be filled with a default value.
  </Step>

  <Step>
    Add some data to the `categories` table:

    ```sql  theme={null}
    INSERT INTO `categories` (name, description) VALUES
    ('Clothing', 'Description for Clothing'),
    ('Electronics', 'Description for Electronics'),
    ('Appliances', 'Description for Appliances'),
    ('Health', 'Description for Health');
    ```
  </Step>

  <Step>
    You can verify everything was added in the PlanetScale CLI MySQL shell with:

    ```sql  theme={null}
    SELECT * FROM products;
    SELECT * FROM categories;
    ```
  </Step>

  <Step>
    Type `exit` to exit the shell.
  </Step>
</Steps>

You can now refresh the [PHP homepage](http://localhost:8000) to see the new records.

## What's next?

Once you're done with initial development, you can promote your branch to production and enable [safe migrations](/docs/vitess/schema-changes/safe-migrations) on your `main` production branch to protect it against direct schema changes and enable zero-downtime schema migraions.

When you're reading to make more schema changes, you'll [create a new branch](/docs/vitess/schema-changes/branching) off of your production branch. Branching your database creates an isolated copy of your production schema so that you can easily test schema changes in development. Once you're happy with the changes, you'll [open a deploy request](/docs/vitess/schema-changes/deploy-requests). This will generate a diff showing the changes that will be deployed, making it easy for your team to review.

Learn more about how PlanetScale allows you to make [non-blocking schema changes](/docs/vitess/schema-changes) to your database tables without locking or causing downtime for production databases.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Connect a Rails application to PlanetScale
Source: https://planetscale.com/docs/vitess/tutorials/connect-rails-app



## Introduction

In this tutorial, you’re going to create a simple Rails application named *blog* and connect it to a PlanetScale database. You’ll perform the initial migration from your local Rails application, and set up the database for future development.

<Tip>
  Already have a Rails application and just want to connect to PlanetScale? Check out the [Rails quick connect repo](https://github.com/planetscale/connection-examples/tree/main/ruby).
</Tip>

## Prerequisites

* Install [Ruby and the Rails gem](https://guides.rubyonrails.org/getting_started.html#creating-a-new-rails-project-installing-rails).
* Install the [PlanetScale CLI](https://github.com/planetscale/cli).
* Authenticate the CLI with the following command:

```bash  theme={null}
pscale auth login
```

## Create a Rails project

To connect a Rails application to a PlanetScale database, you'll first create a sample Rails project named *blog* and install the libraries needed to connect to your PlanetScale database.

Open the command line and follow these steps:

<Steps>
  <Step>
    Create a Rails app named *blog* by running the following command:

    ```bash  theme={null}
    rails new blog
    ```
  </Step>

  <Step>
    Change into the directory you just created, the `blog` Rails app:

    ```bash  theme={null}
    cd blog
    ```
  </Step>

  <Step>
    Next, add the `trilogy` gem to your *Gemfile*:

    ```ruby  theme={null}
    gem "trilogy"
    ```
  </Step>

  <Step>
    Then run `bundle install`

    At this point, you have accomplished two things: you've created a Rails project called *blog* and installed the libraries that you'll need to connect to your PlanetScale database. Now, it’s time to create a PlanetScale database.
  </Step>
</Steps>

## Create a PlanetScale database and password

Now you'll need to create credentials for your Rails application to use.

### Using the CLI to create a connection string

<Steps>
  <Step>
    Using the `pscale` CLI, create a new database also named *blog*:

    ```bash  theme={null}
    pscale database create blog
    ```
  </Step>

  <Step>
    Using the `pscale` CLI, create a new database password for the `main` branch of your database named *blog*:

    ```bash  theme={null}
    pscale password create blog main <PASSWORD_NAME>
    ```

    <Note>
      The `PASSWORD_NAME` value represents the name of the username and password being generated. You can have multiple credentials for a branch, so this gives you a way to categorize them. To manage your passwords in the dashboard, go to your database dashboard page, click "Settings", and then click "Passwords".
    </Note>
  </Step>

  <Step>
    Take note of the values returned to you, as they will not be shown again.

    ```
      NAME                  BRANCH   USERNAME       ACCESS HOST URL                     ROLE     ROLE DESCRIPTION   PASSWORD
     --------------------- -------- -------------- ----------------------------------- -------- ------------------ -------------------------------------------------------
      development-password  main     xxxxxxxx   xxxxxxxxxx.us-east-3.psdb.cloud   writer   Can Read & Write   pscale_pw_xxxxxxxxxxxxxxxxxxxxx
    ```
  </Step>
</Steps>

<Note>
  You can also create passwords in the PlanetScale dashboard, as documented [in our Creating a password documentation](/docs/vitess/connecting/connection-strings#creating-a-password).
</Note>

## Configure Rails and PlanetScale

Let's set up the Rails application to talk to the new database.

Open `config/database.yml` and configure the `development` database settings with your new credentials from the output in the previous step:

```yaml  theme={null}
development:
  <<: *default
  adapter: trilogy
  database: blog
  username: <USERNAME>
  host: <ACCESS HOST URL>
  password: <PASSWORD>
  ssl_mode: <%= Trilogy::SSL_VERIFY_IDENTITY %>
```

For `database` (database name), you can use your PlanetScale database name directly if you have a *single unsharded keyspace*. If you have a sharded keyspace, you'll need to use `@primary`. This will automatically direct incoming queries to the correct keyspace/shard. For more information, see the [Targeting the correct keyspace documentation](/docs/vitess/sharding/targeting-correct-keyspace).

The correct `sslca` path depends on your operating system and distribution. See [CA root configuration](/docs/vitess/connecting/secure-connections#ca-root-configuration) for more information.

<Note>
  You're configuring the **development** Rails environment here for the sake of expedience. In actual use, the **main** database branch would typically serve the **production** environment.
</Note>

Because this is a Rails app, you can also enable [Automatic Rails migrations](/docs/vitess/tutorials/automatic-rails-migrations) from the database's settings page. Select your database, click on the `main` branch, click "**Settings**", check the "**Automatically copy migration data**" box, and select "**Rails**" from the dropdown.

## Migrate your database

Here comes the fun stuff! Now that your application is configured to talk to PlanetScale, you can create your first migration.

<Steps>
  <Step>
    Create a Rails migration and call it `CreateUsers`:

    ```bash  theme={null}
    rails generate migration CreateUsers
    ```

    This rails command begins the migration for your table that is currently empty and generates a Ruby file that’ll be named something like this:
    `db/migrate/20211014210422_create_users.rb`

    Fill in the body of this skeleton file with a few more relevant details, such as a user's **name** and **email**.

    ```ruby  theme={null}
    class CreateUsers < ActiveRecord::Migration[6.1]
      def change
        create_table :users do |t|
          t.string :name
          t.string :email
          t.timestamps
        end
      end
    end
    ```
  </Step>

  <Step>
    Run your migration:

    ```bash  theme={null}
    bin/rails db:migrate
    ```
  </Step>

  <Step>
    Now, give it a whirl to make sure you can query the new table with the `pscale` CLI:

    ```bash  theme={null}
    pscale shell blog main
    ```

    ```sql expandable theme={null}
    blog/main> show tables;
    +----------------------+
    | Tables_in_blog       |
    +----------------------+
    | ar_internal_metadata |
    | schema_migrations    |
    | users                |
    +----------------------+
    blog/main> describe users;
    +------------+--------------+------+-----+---------+----------------+
    | Field      | Type         | Null | Key | Default | Extra          |
    +------------+--------------+------+-----+---------+----------------+
    | id         | bigint       | NO   | PRI | NULL    | auto_increment |
    | name       | varchar(255) | YES  |     | NULL    |                |
    | email      | varchar(255) | YES  |     | NULL    |                |
    | created_at | datetime(6)  | NO   |     | NULL    |                |
    | updated_at | datetime(6)  | NO   |     | NULL    |                |
    +------------+--------------+------+-----+---------+----------------+
    ```
  </Step>
</Steps>

## Enable safe migrations

[Safe migrations](/docs/vitess/schema-changes/safe-migrations) is an optional but highly recommended feature for branches on PlanetScale. With safe migrations enabled, direct schema changes (`CREATE`, `ALTER`, and `DELETE`) are not allowed on production branches to prevent accidental data loss and must be applied via [deploy requests](/docs/vitess/best-practices).

```bash  theme={null}
pscale branch safe-migrations enable blog main
```

Congratulations! You're ready to develop your Rails application against PlanetScale.

## Summary

In this tutorial, you created a simple Rails application named *blog* and connected it to a PlanetScale database.

## Further reading

If you're interested in learning how to secure your application's connection to PlanetScale, please read [Connecting to PlanetScale securely](/docs/vitess/connecting/secure-connections).

## What's next?

Now that you've successfully connected your Rails app to PlanetScale, it's time to make more schema changes to your tables! Learn more about how PlanetScale allows you to make [non-blocking schema changes](/docs/vitess/schema-changes) to your database, or how to keep your **schema\_migrations** table up-to-date between development and production branches with [automatic schema migrations](/docs/vitess/tutorials/automatic-rails-migrations).

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Connect a Symfony application to PlanetScale
Source: https://planetscale.com/docs/vitess/tutorials/connect-symfony-app

In this tutorial, you'll learn how to connect a Symfony application to a PlanetScale MySQL database using a sample Symfony starter app.

## Prerequisites

* [PHP](https://www.php.net/manual/en/install.php) — This tutorial uses `v8.1`
* [Composer](https://getcomposer.org/)
* A [PlanetScale account](https://auth.planetscale.com/sign-up)
* [PlanetScale CLI](https://github.com/planetscale/cli) — You can also follow this tutorial using just the PlanetScale admin dashboard, but the CLI will make setup quicker.

## Set up the Symfony app

This guide will integrate [a simple Symfony app](https://github.com/planetscale/symfony-example) with PlanetScale that will display a list of products stored in the database. If you have an existing application, you can also use that.

<Steps>
  <Step>
    Clone the starter Symfony application:

    ```bash  theme={null}
    git clone https://github.com/planetscale/symfony-example.git
    ```
  </Step>

  <Step>
    Enter into the folder and install the dependencies:

    ```bash  theme={null}
    cd symfony-example
    composer install
    ```
  </Step>

  <Step>
    Rename the `.env.example` file to `.env.local`:

    ```bash  theme={null}
    mv .env.example .env.local
    ```

    Once you deploy to production, don't forget to update `.env.local` to `.env`.
  </Step>

  <Step>
    Start the application:

    ```bash  theme={null}
    symfony serve
    ```
  </Step>
</Steps>

You can view the application at [http://localhost:8000](http://localhost:8000).

## Set up the database

Next, you need to set up your PlanetScale database and connect to it in the Symfony application.

You can create a database either in the [PlanetScale dashboard](https://app.planetscale.com) or from the PlanetScale CLI. This guide will use the CLI, but you can follow the database setup instructions in the [PlanetScale quickstart guide](/docs/vitess/tutorials/planetscale-quick-start-guide) if you prefer the dashboard.

<Steps>
  <Step>
    Authenticate the CLI with the following command:

    ```bash  theme={null}
    pscale auth login
    ```
  </Step>

  <Step>
    Create a new database with a default `main` branch with the following command:

    ```bash  theme={null}
    pscale database create <DATABASE_NAME> --region <REGION_SLUG>
    ```
  </Step>
</Steps>

This tutorial uses `symfony_example` for `DATABASE_NAME`, but you can use any name with lowercase, alphanumeric characters, or underscores. You can also use dashes, but we don't recommend them, as they may need to be escaped in some instances.

For `REGION_SLUG`, choose a region closest to you from the [available regions](/docs/vitess/regions#available-regions) or leave it blank.

That's it! Your database is ready to use. Next, let's connect it to the Symfony application and then add some data.

## Connect to the Symfony app

There are **two ways to connect** to PlanetScale:

* With an auto-generated username and password
* Using the PlanetScale proxy with the CLI

Both options are covered below.

### Option 1: Connect with username and password (Recommended)

These instructions show you how to use the [PlanetScale CLI](/docs/cli/planetscale-environment-setup) to generate a set of credentials.

You can also get these exact values to to copy/paste from your [PlanetScale dashboard](https://app.planetscale.com). In the dashboard, click on the database > "**Connect**" > "**Generate new password**" > "**General**" dropdown > "**Symfony**". If the password is blurred, click "**New password**". Skip to step 3 once you have these credentials.

<Steps>
  <Step>
    Create a username and password with the PlanetScale CLI by running:

    ```bash  theme={null}
    pscale password create <DATABASE_NAME> <BRANCH_NAME> <PASSWORD_NAME>
    ```

    A default branch, `main`, is created when you create the database, so you can use that for `BRANCH_NAME`.

    <Note>
      The `PASSWORD_NAME` value represents the name of the username and password being generated. You can have multiple
      credentials for a branch, so this gives you a way to categorize them. To manage your passwords in the dashboard, go to
      your database dashboard page, click "Settings", and then click "Passwords".
    </Note>

    Take note of the values returned to you, as you won't be able to see this password again.
  </Step>

  <Step>
    Open the `.env.local` file in your Symfony app, find the database connection section, and replace the existing `DATABASE_URL` value with:

    ```bash  theme={null}
    DATABASE_URL="mysql://<USERNAME>:<PASSWORD>@<ACCESS_HOST_URL>:3306/<DATABASE_NAME>?serverVersion=8.0"
    ```

    Fill in `USERNAME`, `PASSWORD`, `ACCESS HOST URL` and `DATABASE_NAME` with the appropriate values from the CLI output above.
  </Step>
</Steps>

Refresh your Symfony homepage and you should see the message that you're connected to your database!

### Option 2: Connect with the PlanetScale proxy

To connect with the PlanetScale proxy, you'll need the [PlanetScale CLI](https://github.com/planetscale/cli).

<Steps>
  <Step>
    Open a connection by running the following:

    ```bash  theme={null}
    pscale connect <DATABASE_NAME> <BRANCH_NAME>
    ```

    If you're following this guide exactly and haven't created any branches, you can use the default branch, `main`.
  </Step>

  <Step>
    A secure connection to your database will be established and you'll see a local address you can use to connect to your application.
  </Step>

  <Step>
    Open the `.env.local` file in your Symfony app and update it as follows:

    ```bash  theme={null}
    DB_HOST=127.0.0.1
    DB_PORT=3306 # Get this from the output of the previous step
    DB_NAME=
    DATABASE_URL=mysql://${DB_HOST}:${DB_PORT}/${DB_NAME}?serverVersion=5.7
    ```

    The connection uses port `3306` by default, but if that's being used, it will pick a random port. Make sure you paste in whatever port is returned in the terminal. Fill in the database name as well.
  </Step>

  <Step>
    Open up `config/packages/doctrine.yaml`. Under `option`, you'll see a line for the SSL certificate that was used to connect with username and password:

    ```php  theme={null}
    !php/const:PDO::MYSQL_ATTR_SSL_CA: /etc/ssl/cert.pem
    ```
  </Step>
</Steps>

Delete that line and save.

Refresh your Symfony homepage and you should see the message that you're connected to your database!

## Run migrations and seeder

Now that you're connected, let's add some data to see it in action. The sample application comes with a migration file at `migrations/Version20220120102247.php` that will create `category` and `product` tables in the database.

There are also two seeder files, `src/DataFixtures/CategoryFixtures.php` and `src/DataFixtures/ProductFixtures.php`, that will add ten random categories and products to the `category` and `product` tables, respectively. Let's run those now.

<Steps>
  <Step>
    Make sure your database connection has been established. You'll see the message "You are connected to your-database-name" on the [Symfony app homepage](http://localhost:8000/) if everything is configured properly.
  </Step>

  <Step>
    In your terminal in the root of the Symfony project, run the following to run the migrations:

    ```bash  theme={null}
    symfony console doctrine:migrations:migrate
    ```

    You will get a message asking you to confirm. Type "yes" and hit enter to proceed.
  </Step>

  <Step>
    Next, seed the database by running:

    ```bash  theme={null}
    symfony console doctrine:fixtures:load
    ```

    This will purge your database and load the placeholder data into it.
  </Step>

  <Step>
    Refresh your Symfony homepage and you'll see a list of products and their category printed out.
  </Step>
</Steps>

The `templates/product/index.html.twig` file pulls this data from the `product` table with the help of the `src/Controller/ProductController.php` file.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-symfony-app/example.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=4679f2f4f9d9c6af56e192bf4cfd7a02" alt="Symfony PlanetScale starter app homepage" data-og-width="2896" width="2896" data-og-height="2400" height="2400" data-path="docs/images/assets/docs/tutorials/connect-symfony-app/example.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-symfony-app/example.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=4675897a4ffc14f46d5972fa9fcc353d 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-symfony-app/example.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=23fff82f386f197856cd88b39e6550fa 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-symfony-app/example.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=4daea7eead3c3ce9dbbe6ec31064a034 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-symfony-app/example.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=13eb631e6a0df30ecc837d649baea1e1 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-symfony-app/example.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=2f475a06f927318962ed66b599634306 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/connect-symfony-app/example.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=c3e500abb8c8c3fddb2a04670a300e9e 2500w" />
</Frame>

## Add data manually

If you want to continue to play around with adding data on the fly, you have a few options:

* PlanetScale CLI shell
* PlanetScale dashboard console
* Your favorite MySQL client (for a list of tested MySQL clients, review our article on [how to connect MySQL GUI applications](/docs/vitess/tutorials/connect-mysql-gui))

The first two options are covered below.

### Add data with PlanetScale CLI

You can use the PlanetScale CLI to open a MySQL shell to interact with your database.

You may need to [install the MySQL command line client](/docs/cli/planetscale-environment-setup) if you haven't already.

Run the following command in your terminal:

```bash  theme={null}
pscale shell <DATABASE_NAME> <BRANCH_NAME>
```

This will open up a MySQL shell connected to the specified database and branch.

<Note>
  A branch, `main`, was automatically created when you created your database, so you can use that for `BRANCH_NAME`.
</Note>

Add a record to the `product` table:

```sql  theme={null}
INSERT INTO `store_product` (name, description, image, category_id)
VALUES  ('Spaceship', 'Get ready for the trip of a lifetime', 'https://via.placeholder.com/150.png', 2);
```

The value `id` will be filled with a default value.

You can verify it was added in the PlanetScale CLI MySQL shell with:

```sql  theme={null}
select * from product;
```

Type `exit` to exit the shell.

You can now refresh the [Symfony homepage](http://localhost:8000) to see the new record.

### Add data with PlanetScale dashboard console

If you don't care to install MySQL client or the PlanetScale CLI, another quick option using the MySQL console built into the PlanetScale dashboard.

By default, web console access to production branches is disabled to prevent accidental deletion. From your database's dashboard page, click on the "**Settings**" tab, check the box labelled "**Allow web console access to production branches**", and click "**Save database settings**".

<Steps>
  <Step>
    Go to your [PlanetScale dashboard](https://app.planetscale.com) and select your Symfony database.
  </Step>

  <Step>
    Click on the "**Branches** and select the `main` branch.
  </Step>

  <Step>
    Click on "**Console**"
  </Step>

  <Step>
    Add a new record to the `product` table with:

    ```sql  theme={null}
    INSERT INTO `store_product` (name, description, image, category_id)
    VALUES  ('Spaceship', 'Get ready for the trip of a lifetime', 'https://via.placeholder.com/150.png', 2);
    ```
  </Step>

  <Step>
    You can confirm that it was added by running:

    ```sql  theme={null}
    select * from product;
    ```
  </Step>
</Steps>

You can now refresh the [Symfony homepage](http://localhost:8000) to see the new record.

## What's next?

Once you're done with initial development, you can enable [safe migrations](/docs/vitess/schema-changes/safe-migrations) on your `main` production branch to protect it against direct schema changes and enable zero-downtime schema migrations.

Learn more about how PlanetScale allows you to make [non-blocking schema changes](/docs/vitess/schema-changes) to your database tables without locking or causing downtime for production databases.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Deploy a Django app to Heroku
Source: https://planetscale.com/docs/vitess/tutorials/deploy-a-django-app-to-heroku



## Overview

This article will describe how to deploy a Django app to Heroku, which includes the necessary setup in Heroku’s dashboard.

## Prerequisites

* A PlanetScale database — If you haven't created a database, refer to our [PlanetScale quickstart guide](/docs/vitess/tutorials/planetscale-quick-start-guide) to get started.

* A Heroku account.

* A project deployed to Heroku — If you're just poking around and don't already have an application to deploy, you can use our [Django sample](https://github.com/planetscale/django-example).

## Set up the project for Heroku

There are a few requirements for running a Django application in Heroku:

* The `gunicorn` and `django-heroku` packages as requirements.

* A properly setup [Procfile](https://devcenter.heroku.com/articles/procfile).

* Proper Config Var setup in Heroku.

<Note>
  This article will make use of the [django-example GitHub repository](https://github.com/planetscale/django-example) that is built for the [Connect a Django application to PlanetScale document](/docs/vitess/tutorials/connect-django-app)
</Note>

### Set up the Heroku Config Vars

It’s important to store the connection details for the PlanetScale database in **Config Vars** in Heroku so they are properly secured. These details can be obtained from the PlanetScale dashboard by clicking the "**Connect**" button.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/database-2.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=0b4894c8e34286d128feb2387541999b" alt="The location of the “Connect” button in the PlanetScale dashboard. priority" data-og-width="1864" width="1864" data-og-height="754" height="754" data-path="docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/database-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/database-2.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=8288ad545d227e85740c52d52a4f352e 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/database-2.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=aa136d9a40d471ca75795a4166ce2fb6 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/database-2.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=33e931b771b359e426bb99813fe5b437 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/database-2.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=c8cf2e0d26aed2085bc75ef050db4bef 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/database-2.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=1e5002335180ea99a9ede3b4ba42637e 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/database-2.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=3abafcbc67aaf6a910ce567bc2518e51 2500w" />
</Frame>

In the following modal, choose Django from the “Connect with” dropdown. The .env tab will show all of the Config vars that need to be set up in Heroku. Take note of these and head to the Heroku dashboard.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/connect-2.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=0e98ff1f01ab2a00cc56bf8c173d6d65" alt="The connection details for the project." data-og-width="1528" width="1528" data-og-height="1124" height="1124" data-path="docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/connect-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/connect-2.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=e23af061118e7f8155838190344aad71 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/connect-2.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=986efce8184015ee468b4ede8df2aeb2 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/connect-2.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=d082bc162d033c1553163f7ab614bb05 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/connect-2.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=e027f602562e061acc4ce51407f2f197 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/connect-2.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=cbcc3f52cb266802247f442e3a4f3348 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/connect-2.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=e5953ab88142d64855f855f4054d03eb 2500w" />
</Frame>

Select the **Settings** tab of your Heroku project and then “**Reveal Config Vars”** from the Config **Vars** section. You should see your current Config Vars or an empty set of inputs if there are none configured yet.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/heroku.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=6a61a50fe442d66e9b44a638ea19a75c" alt="The Settings tab of the Heroku dashboard." data-og-width="1229" width="1229" data-og-height="700" height="700" data-path="docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/heroku.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/heroku.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=107b6cbc6f5340c44dfcb3caefdc2631 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/heroku.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=b0613154f4c3868940525004c4fc71b8 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/heroku.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=14fb3fe8bd6719531c4434c538f13a4a 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/heroku.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=e1478e88a2832f51c2ed9397031754fd 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/heroku.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=c2456a24b569a41bb2cd72e8514ed56f 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/heroku.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=fe09bee918d5d45d4ffd7f5b46b62e47 2500w" />
</Frame>

Set up a separate **Config Var** for each line you captured from the PlanetScale dashboard. The one exception is the `MYSQL_ATTR_SSL_CA`, which should be set to `/etc/ssl/certs/ca-certificates.crt`

<Note>
  Heroku uses Ubuntu by default to run applications deployed to their systems, which is why the `MYSQL_ATTR_SSL_CA` value needs to be different than the default values provided by PlanetScale
</Note>

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/ssl.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=3e1816d000f0ee63d5e1316bd191e40e" alt="The Config Vars setup for the project." data-og-width="811" width="811" data-og-height="410" height="410" data-path="docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/ssl.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/ssl.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=8b10568b9c80c0b0d10cbb607f9a72ce 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/ssl.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=7552c338c9570d2e8a89772934e65bf1 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/ssl.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=91e7461b213a1d761cd29e33e51a07fc 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/ssl.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=75301b698ee8e49211caec1696eb1b8a 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/ssl.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=93b4e6703a6e32147327524f94f0ce01 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-a-django-app-to-heroku/ssl.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=e5dc04689e8e94a4726337479fbf8d6e 2500w" />
</Frame>

### Update the requirements

Add `gunicorn` and `django-heroku` to your `requirements.txt` file. This will install the necessary packages when deploying to Heroku. If you are following along using the example provided, here is the updated `requirements.txt` file:

```
asgiref==3.4.1
Django==4.0.1
djangorestframework==3.13.1
mysqlclient==2.1.0
python-dotenv==0.19.2
pytz==2021.3
sqlparse==0.4.2
gunicorn
django-heroku
```

### Add a Procfile

The **Procfile** in your project tells Heroku how it should start up the project. The file must be in the root of the project and not in a subdirectory. Here is the **Procfile** used to deploy the **django-example** project to Heroku:

```
web: gunicorn --chdir ./mysite mysite.wsgi --log-file -
```

After these steps have been completed, you may redeploy your application to Heroku. To view a complete example, please refer to the [heroku-deployment branch](https://github.com/planetscale/django-example/tree/heroku-deployment) of the sample repository. This concludes the guide on deploying a Django application to Heroku.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Deploy to Netlify
Source: https://planetscale.com/docs/vitess/tutorials/deploy-to-netlify

This guide will walk you through setting up and deploying your PlanetScale database on Netlify.

<Note>
  This doc is intended for users that are manually storing a connection string in an environment variable in Netlify. If you want to use the Netlify integration, which handles this for you, see the [PlanetScale integration in the Netlify docs](https://docs.netlify.com/integrations/planetscale-integration).
</Note>

## Prerequisites

* A PlanetScale database — If you haven't created a database, refer to our [PlanetScale quickstart guide](/docs/vitess/tutorials/planetscale-quick-start-guide) to get started
* A [Netlify account](https://netlify.com/)
* A project deployed to Netlify — If you're just poking around and don't already have an application to deploy, you can use our [Next.js + PlanetScale sample](/docs/vitess/tutorials/connect-nextjs-app)

## Connecting your PlanetScale database to your Netlify application

### Get your connection string from PlanetScale

<Steps>
  <Step>
    In your [PlanetScale dashboard](https://app.planetscale.com), click on the database you want to connect to.
  </Step>

  <Step>
    Click "**Connect**".
  </Step>

  <Step>
    Create a new password. Make sure to copy the password, as you'll only be shown it once.
  </Step>

  <Step>
    Select the framework you're using from the "**Select your language or framework**" section. This will give you the exact environment variable names you need for your selected framework. If your framework is not listed, choose "Other".

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-netlify/prisma.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=5223643d334fa7b6d57b3f49abfbaeed" alt="PlanetScale dashboard connect modal priority" data-og-width="3058" width="3058" data-og-height="1644" height="1644" data-path="docs/images/assets/docs/tutorials/deploy-to-netlify/prisma.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-netlify/prisma.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=e6f12bcde37364d056fb4aafb82ff652 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-netlify/prisma.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=7340c10cec9455bac5a646ad31accd7d 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-netlify/prisma.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=2a2e334d70a193c95d6e8e97d2327d68 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-netlify/prisma.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=f23e9869ad71758206a3066091462b4b 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-netlify/prisma.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=ca11cdcede078f634ea54985cc979fbe 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-netlify/prisma.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=3886ae06e04d5a2075cb58a56c363ba1 2500w" />
    </Frame>
  </Step>

  <Step>
    Keep this page open, as you'll need to copy these to Netlify momentarily.
  </Step>

  <Step>
    If you navigate away from the page, and can no longer access the password, create a new password by repeating steps 1-5.
  </Step>
</Steps>

### Copy environment variables to Netlify

<Steps>
  <Step>
    Go to your Netlify dashboard.
  </Step>

  <Step>
    Click on your Netlify project.
  </Step>

  <Step>
    Click "**Site settings**".
  </Step>

  <Step>
    Click "**Build & deploy**," then "**Environment**".
  </Step>

  <Step>
    Click "**Edit variable**".
  </Step>

  <Step>
    Click "**New variable**" and copy each value from your PlanetScale dashboard into a new environment variable in Netlify. Once you're done with one, click "**Add**" and continue to the next, if applicable.
  </Step>
</Steps>

For example, if you're using Prisma, your connection string will look similar to this:

```bash  theme={null}
DATABASE_URL='mysql://xxxxxxxxx:************@xxxxxxxxxx.us-east-3.psdb.cloud/my-database?sslaccept=strict'
```

<Note>
  Your environment variable name will be the same in your application's code. We used `DATABASE_URL` as an example, but this can be given a different name.
</Note>

In Netlify, you'll set it as follows:

* **Key** = `DATABASE_URL`
* **Value** = `mysql://xxxxxxxxx:************@xxxxxxxxxx.us-east-3.psdb.cloud/my-database?sslaccept=strict`

<Info>
  The credentials are blurred for the example, but when you paste them in, use the actual values.
</Info>

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-netlify/environment-variables.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=63adc643664622d94dd39f71b2a3022f" alt="Netlify dashboard - Environment variables" data-og-width="1890" width="1890" data-og-height="1066" height="1066" data-path="docs/images/assets/docs/tutorials/deploy-to-netlify/environment-variables.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-netlify/environment-variables.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=ac023209b24fc8c2a5ee8942f53b4a24 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-netlify/environment-variables.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=7f0e43c3f32c2b950843e790cf99985d 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-netlify/environment-variables.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=73c82080fe7654b3b3ee66aa3ae765e9 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-netlify/environment-variables.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=c72efaa9df1d71e03e2333626f87e062 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-netlify/environment-variables.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=c8da466829a7349d59cbc7ebc3d19b2b 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-netlify/environment-variables.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=6902e1352cac203b2157de3f7afbbecc 2500w" />
</Frame>

After you have saved, you will need to rebuild the site with the new environment variable.

## What's next?

Learn more about how PlanetScale allows you to make [non-blocking schema changes](/docs/vitess/schema-changes) to your database tables without locking or causing downtime for production databases.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Deploy to Vercel
Source: https://planetscale.com/docs/vitess/tutorials/deploy-to-vercel

This guide will walk you through setting up and deploying your PlanetScale database on Vercel.

To use a PlanetScale database with Vercel, there are a few prerequisites:

* A PlanetScale database — If you haven't created a database, refer to our [PlanetScale quickstart guide](/docs/vitess/tutorials/planetscale-quick-start-guide) to get started
* A [Vercel account](https://vercel.com/)
* A project deployed to Vercel — If you're just poking around and don't already have an application to deploy, you can use our [Next.js + PlanetScale sample](/docs/vitess/tutorials/connect-nextjs-app)

## Get your connection string from PlanetScale

<Steps>
  <Step>
    In your [PlanetScale dashboard](https://app.planetscale.com), click on the database you want to connect to.
  </Step>

  <Step>
    Click "**Connect**".
  </Step>

  <Step>
    Create a new password. Make sure to copy the password, as you'll only be shown it once.
  </Step>

  <Step>
    Select the framework you're using from the "**Select your language or framework**" section. This will give you the exact environment variable names you need for your selected framework. If your framework is not listed, choose "Other".

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/prisma.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=fac97227b034edf22a1c3656ed5d2fc7" alt="PlanetScale dashboard connect modal {priority}" data-og-width="3058" width="3058" data-og-height="1644" height="1644" data-path="docs/images/assets/docs/tutorials/deploy-to-vercel/prisma.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/prisma.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=04521f523c423e9c2b2f5fe91c5f233a 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/prisma.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=3b932dc84efe572119fa65a2fe5c0fb2 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/prisma.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=5d66cb5b2649222a6e7386e5624ceb7c 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/prisma.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=394acb2d72332a7dc1c97902cf0f38b9 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/prisma.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=ba86e785c3f4b65b34194d8a1d968da9 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/prisma.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=b02f3870e62d865bd5f263255c9c60d4 2500w" />
    </Frame>
  </Step>

  <Step>
    Keep this page open, as you'll need to copy these to Vercel momentarily.
  </Step>

  <Step>
    If you navigate away from the page, and can no longer access the password, create a new password by repeating steps 1-5.
  </Step>
</Steps>

## Copy environment variables to Vercel

<Steps>
  <Step>
    Go to your Vercel dashboard.
  </Step>

  <Step>
    Click on your Vercel project.
  </Step>

  <Step>
    Click "**Settings**".
  </Step>

  <Step>
    Click "**Environment variables**".
  </Step>

  <Step>
    Copy each value from your PlanetScale dashboard into a new environment variable in Vercel. Once you're done with one, click "**Add**" and continue to the next, if applicable.
  </Step>
</Steps>

For example, if you're using Prisma, your connection string will look similar to this:

```bash  theme={null}
DATABASE_URL='mysql://xxxxxxxxx:************@xxxxxxxxxx.us-east-3.psdb.cloud/my_database?sslaccept=strict'
```

In Vercel, you'll set it as follows:

* **NAME** = `DATABASE_URL`
* **VALUE** = `mysql://xxxxxxxxx:************@xxxxxxxxxx.us-east-3.psdb.cloud/my_database?sslaccept=strict`

<Note>
  The credentials are blurred for the example, but when you paste them in, use the actual values.
</Note>

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/environment-variables.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=67a6c07441bc0ae4d0f321643211a2a0" alt="Vercel dashboard - Environment variables" data-og-width="1400" width="1400" data-og-height="843" height="843" data-path="docs/images/assets/docs/tutorials/deploy-to-vercel/environment-variables.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/environment-variables.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=f85c3cd707e970253ebb67939123df2b 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/environment-variables.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=c2e65af344a33fe86250e698b01d26e0 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/environment-variables.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=d883079f64d02f3dd6dbec13cab3025b 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/environment-variables.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=1b77728c5301f34b406ad1bc9943f82f 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/environment-variables.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=4ea56380aba4e8e8c83b10c9996864b6 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/environment-variables.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=ae5ba57fa910c64a2c202f317ca8884b 2500w" />
</Frame>

## What's next?

Learn more about how PlanetScale allows you to make [non-blocking schema changes](/docs/vitess/schema-changes) to your database tables without locking or causing downtime for production databases.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Deploy to Netlify
Source: https://planetscale.com/docs/vitess/tutorials/deployments/deploy-to-netlify

This guide will walk you through setting up and deploying your PlanetScale database on Netlify.

<Note>
  This doc is intended for users that are manually storing a connection string in an environment variable in Netlify. If you want to use the Netlify integration, which handles this for you, see the [PlanetScale integration in the Netlify docs](https://docs.netlify.com/integrations/planetscale-integration).
</Note>

## Prerequisites

* A PlanetScale database — If you haven't created a database, refer to our [PlanetScale quickstart guide](/docs/vitess/tutorials/planetscale-quick-start-guide) to get started
* A [Netlify account](https://netlify.com/)
* A project deployed to Netlify — If you're just poking around and don't already have an application to deploy, you can use our [Next.js + PlanetScale sample](/docs/vitess/tutorials/connect-nextjs-app)

## Connecting your PlanetScale database to your Netlify application

### Get your connection string from PlanetScale

<Steps>
  <Step>
    In your [PlanetScale dashboard](https://app.planetscale.com), click on the database you want to connect to.
  </Step>

  <Step>
    Click "**Connect**".
  </Step>

  <Step>
    Create a new password. Make sure to copy the password, as you'll only be shown it once.
  </Step>

  <Step>
    Select the framework you're using from the "**Select your language or framework**" section. This will give you the exact environment variable names you need for your selected framework. If your framework is not listed, choose "Other".

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-netlify/prisma.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=5223643d334fa7b6d57b3f49abfbaeed" alt="PlanetScale dashboard connect modal priority" data-og-width="3058" width="3058" data-og-height="1644" height="1644" data-path="docs/images/assets/docs/tutorials/deploy-to-netlify/prisma.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-netlify/prisma.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=e6f12bcde37364d056fb4aafb82ff652 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-netlify/prisma.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=7340c10cec9455bac5a646ad31accd7d 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-netlify/prisma.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=2a2e334d70a193c95d6e8e97d2327d68 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-netlify/prisma.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=f23e9869ad71758206a3066091462b4b 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-netlify/prisma.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=ca11cdcede078f634ea54985cc979fbe 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-netlify/prisma.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=3886ae06e04d5a2075cb58a56c363ba1 2500w" />
    </Frame>
  </Step>

  <Step>
    Keep this page open, as you'll need to copy these to Netlify momentarily.
  </Step>

  <Step>
    If you navigate away from the page, and can no longer access the password, create a new password by repeating steps 1-5.
  </Step>
</Steps>

### Copy environment variables to Netlify

<Steps>
  <Step>
    Go to your Netlify dashboard.
  </Step>

  <Step>
    Click on your Netlify project.
  </Step>

  <Step>
    Click "**Site settings**".
  </Step>

  <Step>
    Click "**Build & deploy**," then "**Environment**".
  </Step>

  <Step>
    Click "**Edit variable**".
  </Step>

  <Step>
    Click "**New variable**" and copy each value from your PlanetScale dashboard into a new environment variable in Netlify. Once you're done with one, click "**Add**" and continue to the next, if applicable.
  </Step>
</Steps>

For example, if you're using Prisma, your connection string will look similar to this:

```bash  theme={null}
DATABASE_URL='mysql://xxxxxxxxx:************@xxxxxxxxxx.us-east-3.psdb.cloud/my-database?sslaccept=strict'
```

<Note>
  Your environment variable name will be the same in your application's code. We used `DATABASE_URL` as an example, but this can be given a different name.
</Note>

In Netlify, you'll set it as follows:

* **Key** = `DATABASE_URL`
* **Value** = `mysql://xxxxxxxxx:************@xxxxxxxxxx.us-east-3.psdb.cloud/my-database?sslaccept=strict`

<Info>
  The credentials are blurred for the example, but when you paste them in, use the actual values.
</Info>

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-netlify/environment-variables.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=63adc643664622d94dd39f71b2a3022f" alt="Netlify dashboard - Environment variables" data-og-width="1890" width="1890" data-og-height="1066" height="1066" data-path="docs/images/assets/docs/tutorials/deploy-to-netlify/environment-variables.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-netlify/environment-variables.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=ac023209b24fc8c2a5ee8942f53b4a24 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-netlify/environment-variables.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=7f0e43c3f32c2b950843e790cf99985d 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-netlify/environment-variables.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=73c82080fe7654b3b3ee66aa3ae765e9 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-netlify/environment-variables.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=c72efaa9df1d71e03e2333626f87e062 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-netlify/environment-variables.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=c8da466829a7349d59cbc7ebc3d19b2b 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-netlify/environment-variables.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=6902e1352cac203b2157de3f7afbbecc 2500w" />
</Frame>

After you have saved, you will need to rebuild the site with the new environment variable.

## What's next?

Learn more about how PlanetScale allows you to make [non-blocking schema changes](/docs/vitess/schema-changes) to your database tables without locking or causing downtime for production databases.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Deploy to Vercel
Source: https://planetscale.com/docs/vitess/tutorials/deployments/deploy-to-vercel

This guide will walk you through setting up and deploying your PlanetScale database on Vercel.

It will cover two options:

<CardGroup>
  <Card title="Manually adding PlanetScale environment variables to your Vercel project" href="#manually-connect-to-vercel" icon="angles-right" />

  <Card title="Connecting your PlanetScale database to your application using the Vercel integration" href="#deploy-with-the-planetscale-vercel-integration" icon="angles-right" />
</CardGroup>

## Prerequisites

* A PlanetScale database — If you haven't created a database, refer to our [PlanetScale quickstart guide](/docs/vitess/tutorials/planetscale-quick-start-guide) to get started
* A [Vercel account](https://vercel.com/)
* A project deployed to Vercel — If you're just poking around and don't already have an application to deploy, you can use our [Next.js + PlanetScale sample](/docs/vitess/tutorials/connect-nextjs-app)

## Manually connect to Vercel

### Get your connection string from PlanetScale

<Steps>
  <Step>
    In your [PlanetScale dashboard](https://app.planetscale.com), click on the database you want to connect to.
  </Step>

  <Step>
    Click "**Connect**".
  </Step>

  <Step>
    Create a new password. Make sure to copy the password, as you'll only be shown it once.
  </Step>

  <Step>
    Select the framework you're using from the "**Select your language or framework**" section. This will give you the exact environment variable names you need for your selected framework. If your framework is not listed, choose "Other".

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/prisma.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=fac97227b034edf22a1c3656ed5d2fc7" alt="PlanetScale dashboard connect modal {priority}" data-og-width="3058" width="3058" data-og-height="1644" height="1644" data-path="docs/images/assets/docs/tutorials/deploy-to-vercel/prisma.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/prisma.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=04521f523c423e9c2b2f5fe91c5f233a 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/prisma.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=3b932dc84efe572119fa65a2fe5c0fb2 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/prisma.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=5d66cb5b2649222a6e7386e5624ceb7c 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/prisma.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=394acb2d72332a7dc1c97902cf0f38b9 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/prisma.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=ba86e785c3f4b65b34194d8a1d968da9 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/prisma.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=b02f3870e62d865bd5f263255c9c60d4 2500w" />
    </Frame>
  </Step>

  <Step>
    Keep this page open, as you'll need to copy these to Vercel momentarily.
  </Step>

  <Step>
    If you navigate away from the page, and can no longer access the password, create a new password by repeating steps 1-5.
  </Step>
</Steps>

### Copy environment variables to Vercel

<Steps>
  <Step>
    Go to your Vercel dashboard.
  </Step>

  <Step>
    Click on your Vercel project.
  </Step>

  <Step>
    Click "**Settings**".
  </Step>

  <Step>
    Click "**Environment variables**".
  </Step>

  <Step>
    Copy each value from your PlanetScale dashboard into a new environment variable in Vercel. Once you're done with one, click "**Add**" and continue to the next, if applicable.
  </Step>
</Steps>

For example, if you're using Prisma, your connection string will look similar to this:

```bash  theme={null}
DATABASE_URL='mysql://xxxxxxxxx:************@xxxxxxxxxx.us-east-3.psdb.cloud/my_database?sslaccept=strict'
```

In Vercel, you'll set it as follows:

* **NAME** = `DATABASE_URL`
* **VALUE** = `mysql://xxxxxxxxx:************@xxxxxxxxxx.us-east-3.psdb.cloud/my_database?sslaccept=strict`

<Note>
  The credentials are blurred for the example, but when you paste them in, use the actual values.
</Note>

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/environment-variables.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=67a6c07441bc0ae4d0f321643211a2a0" alt="Vercel dashboard - Environment variables" data-og-width="1400" width="1400" data-og-height="843" height="843" data-path="docs/images/assets/docs/tutorials/deploy-to-vercel/environment-variables.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/environment-variables.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=f85c3cd707e970253ebb67939123df2b 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/environment-variables.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=c2e65af344a33fe86250e698b01d26e0 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/environment-variables.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=d883079f64d02f3dd6dbec13cab3025b 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/environment-variables.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=1b77728c5301f34b406ad1bc9943f82f 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/environment-variables.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=4ea56380aba4e8e8c83b10c9996864b6 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/environment-variables.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=ae5ba57fa910c64a2c202f317ca8884b 2500w" />
</Frame>

## Deploy with the PlanetScale Vercel integration

If you don't want to copy and paste the environment variables over to Vercel, you can use the [PlanetScale integration from the Vercel marketplace](https://vercel.com/integrations/planetscale). You can choose which database you want to connect to, and we'll automatically pull the necessary environment variables into your Vercel project.

<Steps>
  <Step>
    You must have an existing PlanetScale database to use the integration. You can create a database in the [PlanetScale dashboard](https://app.planetscale.com).
  </Step>

  <Step>
    Click "**Add integration**" on the [Vercel integrations page](https://vercel.com/integrations/planetscale).
  </Step>

  <Step>
    Select the Vercel account you want to connect with.
  </Step>

  <Step>
    On the left, you'll see the Vercel options, and on the right, the PlanetScale options.
  </Step>

  <Step>
    Select the Vercel project you want to connect to, and beneath that, select the framework you're using. If the framework isn't listed, select "**General**". This selection is what determines the names of the environment variables.
  </Step>

  <Step>
    On the right side, choose the [PlanetScale Organization](/docs/security/access-control) that the database is in. The integration will remain tied to this Organization and cannot be changed.
  </Step>

  <Step>
    Beneath that, select the database you want to connect to.
  </Step>

  <Step>
    Click "**Connect database**".
  </Step>

  <Step>
    Back in your Vercel dashboard, confirm the environment variables were added by going to your Vercel project > "**Settings**" > "**Environment variables**"
  </Step>
</Steps>

### Configure your connection

After you set up the initial connection, you also have the option to configure the PlanetScale connection, add more databases to the project, or remove databases from the project.

To access the configuration page:

<Steps>
  <Step>
    In your Vercel project dashboard, click "**Integrations**".
  </Step>

  <Step>
    Click the "**PlanetScale**" integration.
  </Step>

  <Step>
    Click the "**Configure**" button.
  </Step>
</Steps>

<Note>
  If you're modifying an existing connection on a Vercel project, these environment variable values will be regenerated and overwritten.
</Note>

<Note>
  * Environment variables are removed from all projects tied to the integration.
  * Your application will no longer be able to connect to your PlanetScale database.
</Note>

## What's next?

Learn more about how PlanetScale allows you to make [non-blocking schema changes](/docs/vitess/schema-changes) to your database tables without locking or causing downtime for production databases.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Next.js and PlanetScale Netlify template tutorial
Source: https://planetscale.com/docs/vitess/tutorials/deployments/nextjs-planetscale-netlify-template



## Overview

This guide will show you how to get up and running with the [Netlify, Next.js, and PlanetScale starter template](https://templates.netlify.com/template/nextjs-planetscale-starter/). The template includes the following features:

* Simple user admin dashboard
* [PlanetScale](/) database
* [Prisma ORM](https://www.prisma.io/) integration
* [Next.js authentication](https://nextjs.org/docs/authentication)
* One-click [deploy to Netlify](https://netlify.com)
* [Tailwind CSS](https://tailwindcss.com/) styling

You can see a [live demo of the starter application here {priority}](https://nextjs-planetscale-starter.netlify.app/).

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/example.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=e31ec138b463cde47fcaa0c9c4800082" alt="Example of the dashboard application" data-og-width="2360" width="2360" data-og-height="1465" height="1465" data-path="docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/example.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/example.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=03095af4c4619b0e3b2884205491db7e 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/example.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=eaa3814f5503714f2d5e50cdd6290dc4 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/example.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=296062f814a76bd0530908e291e1a64a 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/example.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=f52f4a33bffa31c79f6827ecfcae7751 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/example.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=70de583d37751f43d3ab5e1e72ea4df7 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/example.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=467fe053bade00c089e78a1934b986ca 2500w" />
</Frame>

<Note>
  If you're coming from the **[Netlify Template](https://templates.netlify.com/template/nextjs-planetscale-starter/)** and you already clicked deploy, you're in the right place! This tutorial will walk you through how to set up your PlanetScale database so that you can fill in the environment variables in the Netlify dashboard. You'll also learn how to set up your local environment so you can continue to develop and extend this starter template. Just read through the prerequisites and then skip the first section to [go straight to the local setup](#set-up-the-project-locally).

  If you're **starting fresh and haven't deployed yet** (or don't want to deploy), you can start from the beginning of this tutorial.
</Note>

### Prerequisites

To follow along with this guide, you'll need the following:

* A [free PlanetScale account](https://auth.planetscale.com/sign-up)
* The [PlanetScale CLI](https://github.com/planetscale/cli#installation)
* [Yarn](https://yarnpkg.com/getting-started/install)
* [Node (LTS)](https://nodejs.org/en/)
* A [free Netlify account](https://app.netlify.com/signup)

## One-click deploy to Netlify

The one-click deploy button allows you to connect Netlify to your GitHub account to clone the `nextjs-planetscale-starter` repository and automatically deploy it. Be sure to [sign up for a Netlify account](https://app.netlify.com/signup) before clicking the deploy button.

[![Deploy to Netlify button](https://www.netlify.com/img/deploy/button.svg)](https://app.netlify.com/start/deploy?repository=https://github.com/planetscale/nextjs-planetscale-starter)

Once you click the button, you'll be taken to Netlify’s direct deploy page with the pre-built project’s repository passed as a parameter in the URL. Click the "**Connect to GitHub**" button to authorize access.

Next, you'll be asked to configure your site variables. For the `Secret` value, navigate to [`https://generate-secret.now.sh/32`](https://generate-secret.now.sh/32) to generate a secret and then paste that in. You can leave the `Database URL` and `NextAuth URL` values blank for now. Click "Save & Deploy".

Your site will take about a minute to build and then you'll be taken to a settings page. A unique Netlify URL will be generated for the project. You can click that now to see your live site! The next section will show you how to set the project up locally and create your PlanetScale database to connect to your live site.

## Set up the project locally

If you already went through the [Netlify deployment](https://templates.netlify.com/template/nextjs-planetscale-starter/), find the repository that was created for you in your GitHub account and clone it.

If you didn't deploy and just want to run the template locally, you can clone the [`nextjs-planetscale-starter` repository](https://github.com/planetscale/nextjs-planetscale-starter).

Enter into the folder and install the dependencies:

```bash  theme={null}
yarn install
```

Run the application with:

```bash  theme={null}
yarn next
```

Navigate to [http://localhost:3000/](http://localhost:3000/) in your browser to view the PlanetScale Next.js Starter app.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/starter.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=45c94ed90bc60e0a3dcf351a5f6db93e" alt="Next.js PlanetScale Starter application homepage" data-og-width="3522" width="3522" data-og-height="1722" height="1722" data-path="docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/starter.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/starter.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=fc89c6bfa0ec880fb7b9a3148bba718f 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/starter.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=aa9c43490ff3f261da22d00e45fbb993 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/starter.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=71eb1a2fefec7e140d94fd742e47ad5d 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/starter.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=996115fa53fde5fa6ad8f676ad93e9c1 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/starter.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=5a44661d5b9f816a0832f616daab953a 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/starter.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=91b65399a578a077cb60899f32cdf008 2500w" />
</Frame>

## Database setup

Next, you need to set up your PlanetScale database. If you don't already have a [PlanetScale account](/docs/planetscale-plans), you can [sign up for a free one here](https://auth.planetscale.com/sign-up).

### Create your database

To begin, create a new database. You can either do this in the dashboard or using the [PlanetScale CLI](/docs/cli).

In the dashboard, click on the "**Create a database**" button. Name your database "`netlify-starter`", or whatever name you wish. Select the region closest to you, and click "**Create database**".

Alternatively, [sign in and create a database with the CLI](/docs/vitess/tutorials/planetscale-quick-start-guide#getting-started-planetscale-cli) by running the following:

```bash  theme={null}
pscale database create <database-name> --region <region-slug>
```

The list of region slugs can be found in our [Regions documentation](/docs/vitess/regions#available-regions).

### Connect to your database branches locally

To connect locally, make sure you've authenticated in the CLI. If not, follow the directions in the [sign-in section of our quickstart guide](/docs/vitess/tutorials/planetscale-quick-start-guide#sign-in-to-your-account).

Next, connect to your `main` branch locally by running the following in your terminal:

```bash  theme={null}
pscale connect <database-name> main --port <port>
```

Choose any unused port. This tutorial uses `3309`. You'll see the response "Secure connection to database `netlify-starter` and branch `main` is established!" along with the local proxy address for your database. Take note of that address for the next step.

## Set up local environment variables

For the last part of setup, you need to fill in your environment variables.

Make a copy of the `.env.example` file at the root of your project and rename it `.env`:

```bash  theme={null}
cp .env.example .env
```

The `DATABASE_URL` value comes from your PlanetScale database and will be in the following form:

`mysql://<user>@<address>:<port>/<database>`

* `user` — the database user
* `address` — the local address returned in the previous step
* `port` — the port you specified in the previous step
* `database` — the name of your database

Below is an example of the `.env` file based on this tutorial:

```js  theme={null}
DATABASE_URL="mysql://root@127.0.0.1:3309/netlify-starter"
NEXTAUTH_URL=http://localhost:3000
# Navigate to https://generate-secret.now.sh/32 to generate a secret for the variable below
NEXTAUTH_SECRET=
```

Remember, these are the values for your **local environment**. The values needed for the Netlify environment variables will be covered shortly.

## Push your database schema to PlanetScale

Now that your PlanetScale database is connected to your application, it's time to [push your database schema](https://www.prisma.io/docs/orm/reference/prisma-cli-reference#db-push).

In your terminal, run:

```bash  theme={null}
yarn db:push
```

You can view the schema in your PlanetScale dashboard by clicking the `netlify-starter` database > "**Branches**" > `main` > "**Schema**" > "**Refresh schema**".

You'll now see the following tables:

* `Account`
* `Session`
* `User`
* `VerificationToken`

You can also verify it worked using the PlanetScale CLI. Run the following to start a MySQL shell (where `netlify-starter` is your database):

```bash  theme={null}
pscale shell netlify-starter main
```

Once in the shell, view the tables with:

```bash  theme={null}
SHOW tables;
```

Type `exit` to exit the MySQL shell.

The database schema, `db/schema.prisma`, has been loaded and your PlanetScale database is now ready for data.

## Seed the database

To seed the database with users, run the following:

```bash  theme={null}
yarn db:seed
```

This will add three mock users to the `User` database, as described in `db/seed.ts`. To verify that they were added, click on the "**Console**" tab in your PlanetScale dashboard on the `main` branch of your `netlify-starter` database.

Run the following:

```bash  theme={null}
SELECT * from User;
```

You can also run this command from the CLI using the same `pscale shell` command mentioned above.

## Netlify environment variables

Now that you have your site running locally, let's shift back to the live Netlify site.

The final step in the site deployment is configuring your production environment variables. In the Netlify dashboard under your site's settings page, click on "**Build & deploy**" > "**Environment**" in the left nav.

Click on the "**Edit variables**" button and enter in the following key/value pairs:

* `DATABASE_URL` — To find this value, go back to your PlanetScale dashboard, click on the `netlify-starter` database, click "**Connect**". If the password is masked and you don't have one, you may click the "**Generate new password**" button to create a new one. Next, click on the "**General**" dropdown in that modal and select "**Prisma**". Copy the value for `url` and paste that back in the Netlify dashboard as the value for `DATABASE_URL`. Be sure to save your PlanetScale password somewhere as you won't be able to access it again after closing the modal.
* `NEXTAUTH_SECRET` — You may have already filled this out, but if not generate a new secret at [https://generate-secret.now.sh/32](https://generate-secret.now.sh/32) and paste in the value that's returned.
* `NEXTAUTH_URL` — Paste in the Netlify site name that was generated for you. For example, `https://stoic-lumiere-6df10.netlify.app`

Click "**Save**". Now, redeploy the site with these new variables by going to "Deploys" in the top nav and clicking the "**Trigger deploy**" > "**Deploy site**" button.

## Set up your admin account

Now that your site is live and connected to your PlanetScale database, you need to set up your admin account for your application. Go to `/admin/setup` on your live Netlify site (or locally if you didn't deploy), and fill in the form to set up your account. This will be automatically saved to your PlanetScale database.

Creating an admin account gives you access to the `/admin` route in your application, which is the dashboard to manage your users.

Once you're signed in as an admin, navigate to `/admin` to see a list of your users.

## Customize and extend

And that's it! If you followed this tutorial completely, you now have a local and production version of your **Next.js + PlanetScale + Prisma admin dashboard**. So what's next?

Now, it's time to extend it! Next.js Authentication is baked into this starter, so you can explore the [Next.js docs to manage authentication](https://nextjs.org/docs/authentication) in your application.

You also have a fully functional PlanetScale MySQL database built for scale using the power of [Vitess](https://vitess.io/). You might have noticed that this tutorial uses the same database locally as it is in production. This was just for simplicity, but with PlanetScale, you can take advantage of our [powerful branching feature](/docs/vitess/schema-changes/branching) to create development branches of your database specifically for testing locally. All you have to do is create a new branch and swap out the `DATABASE_URL` environment variable in your local `.env` file.

Finally, you have [Prisma ORM](https://www.prisma.io/docs/) already configured in your application. If you want to add any more fields to your `User` table or create any new tables, Prisma makes it easy with the `schema.prisma` file. If you want to make any schema changes, it's a perfect time to try out the [PlanetScale branching feature](/docs/vitess/schema-changes/branching) feature. If you mess up, those changes won't touch production. And once you're satisfied with the changes, you can deploy to production without causing downtime thanks to PlanetScale [non-blocking schema changes](/docs/vitess/schema-changes).

Hopefully this tutorial has been helpful. We'd love to hear how you're extending your starter application. [Tweet us @PlanetScale](https://twitter.com/planetscale) and share what you built!

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale GitHub Actions
Source: https://planetscale.com/docs/vitess/tutorials/github-actions



See our [tech talk on Databases + CI/CD](https://planetscale.com/blog/databases-ci-cd-pipeline) to see pscale + GitHub Actions
used in a real application.

With GitHub Actions, you can automate the creation of branches and deploy requests all within your CI workflow.

<Columns cols={2}>
  <Card title="Getting Started" href="#getting-started" icon="rocket-launch" horizontal />

  <Card title="Authentication" href="#authentication" icon="key" horizontal />

  <Card title="Convert GitHub branch name to PlanetScale branch name" href="#convert-github-branch-name-to-planetscale-branch-name" icon="code-branch" horizontal />

  <Card title="Create a PlanetScale branch" href="#create-a-planetscale-branch" icon="code-branch" horizontal />

  <Card title="Create a password for a branch" href="#create-a-password-for-a-branch" icon="key" horizontal />

  <Card title="Open a deploy request" href="#open-a-deploy-request" icon="code-branch" horizontal />

  <Card title="Get deploy request by branch name" href="#get-deploy-request-by-branch-name" icon="code-branch" horizontal />

  <Card title="Get deploy request diff and comment on pull request" href="#get-deploy-request-diff-and-comment-on-pull-request" icon="code-branch" horizontal />

  <Card title="Check for dropped columns" href="#check-for-dropped-columns" icon="code-branch" horizontal />

  <Card title="Submit a deploy request by branch name" href="#submit-a-deploy-request-by-branch-name" icon="code-branch" horizontal />
</Columns>

## Getting started

The best way to use PlanetScale within GitHub Actions is via the `pscale` CLI.

Use [`planetscale/setup-pscale-action`](https://github.com/planetscale/setup-pscale-action) to make pscale available within your GitHub Actions.

```yaml  theme={null}
- name: Setup pscale
  uses: planetscale/setup-pscale-action@v1
```

The action works with Linux, Windows, and Mac runners. Once installed it will be added to your tool cache for subsequent runs.

## Authentication

Authentication for pscale is via service token environment variables.

You will need to [create a service token](/docs/cli/service-token). Make sure to give your service token the proper permissions to the database you'll be using in your workflow.

Add your `PLANETSCALE_SERVICE_TOKEN_ID` and `PLANETSCALE_SERVICE_TOKEN` to your [Actions secrets](https://docs.github.com/en/actions/security-guides/using-secrets-in-github-actions#creating-secrets-for-a-repository).

In your Actions workflow, you will need to make the secrets available as environment variables.

```yaml  theme={null}
- name: Run pscale command
  env:
    PLANETSCALE_SERVICE_TOKEN_ID: ${{ secrets.PLANETSCALE_SERVICE_TOKEN_ID }}
    PLANETSCALE_SERVICE_TOKEN: ${{ secrets.PLANETSCALE_SERVICE_TOKEN }}
  run: pscale database list
```

## Examples

The following examples show how to accomplish common Actions workflows with PlanetScale. In each example, notice that we use secrets for the service token, database and organization names.
You will need to set them in your GitHub repository to target your own database.

```
PLANETSCALE_ORG_NAME
PLANETSCALE_DATABASE_NAME
PLANETSCALE_SERVICE_TOKEN_ID
PLANETSCALE_SERVICE_TOKEN
```

### Convert GitHub branch name to PlanetScale branch name

PlanetScale branch names must be lowercase, alphanumeric characters and hyphens are allowed.

Since git branch names allow more possibilities, you can use the following code to transform a git branch name into an acceptable PlanetScale branch name.

```yaml  theme={null}
- name: Rename branch name
  run: echo "PSCALE_BRANCH_NAME=$(echo ${{ github.head_ref }} | tr -cd '[:alnum:]-'| tr '[:upper:]' '[:lower:]')" >> $GITHUB_ENV
```

This makes `${{ env.PSCALE_BRANCH_NAME }}` available for use in the rest of the workflow. This is useful to run in any scenario where you are creating
a PlanetScale branch to correspond with a git branch.

### Create a PlanetScale branch

You can use `pscale branch create` to create a branch that matches your GitHub branch name.

```yaml  theme={null}
- name: Create branch
  env:
    PLANETSCALE_SERVICE_TOKEN_ID: ${{ secrets.PLANETSCALE_SERVICE_TOKEN_ID }}
    PLANETSCALE_SERVICE_TOKEN: ${{ secrets.PLANETSCALE_SERVICE_TOKEN }}
  run: |
    set +e
    pscale branch show ${{ secrets.PLANETSCALE_DATABASE_NAME }} ${{ env.PSCALE_BRANCH_NAME }} --org ${{ secrets.PLANETSCALE_ORG_NAME }}
    exit_code=$?
    set -e

    if [ $exit_code -eq 0 ]; then
      echo "Branch exists. Skipping branch creation."
    else
      echo "Branch does not exist. Creating."
      pscale branch create ${{ secrets.PLANETSCALE_DATABASE_NAME }} ${{ env.PSCALE_BRANCH_NAME }} --wait --org ${{ secrets.PLANETSCALE_ORG_NAME }}
    fi
```

Notice that we first check if the branch exists. If it does, we do nothing. Otherwise we create it and pass the `--wait` flag.

This is useful when running in CI, as the workflow may run multiple times and you'll want the branch ready if you are running schema migrations immediately after creating the branch.

### Create a password for a branch

You can use `pscale password create` to generate credentials for your database branch.

```yaml expandable theme={null}
- name: Generate password for branch
  env:
    PLANETSCALE_SERVICE_TOKEN_ID: ${{ secrets.PLANETSCALE_SERVICE_TOKEN_ID }}
    PLANETSCALE_SERVICE_TOKEN: ${{ secrets.PLANETSCALE_SERVICE_TOKEN }}
  run: |
    response=$(pscale password create ${{ secrets.PLANETSCALE_DATABASE_NAME }} ${{ env.PSCALE_BRANCH_NAME }} "" -f json --org ${{ secrets.PLANETSCALE_ORG_NAME }})

    id=$(echo "$response" | jq -r '.id')
    host=$(echo "$response" | jq -r '.access_host_url')
    username=$(echo "$response" | jq -r '.username')
    password=$(echo "$response" | jq -r '.plain_text')
    ssl_mode="verify_identity"  # Assuming a default value for ssl_mode
    ssl_ca="/etc/ssl/certs/ca-certificates.crt"  # Assuming a default value for ssl_ca

    # Set the password ID, allows us to later delete it if wanted.
    echo "PASSWORD_ID=$id" >> $GITHUB_ENV

    # Create the DATABASE_URL
    database_url="mysql://$username:$password@$host/${{ secrets.PLANETSCALE_DATABASE_NAME }}?sslmode=$ssl_mode&sslca=$ssl_ca"
    echo "DATABASE_URL=$database_url" >> $GITHUB_ENV
    echo "::add-mask::$DATABASE_URL"
- name: Use the DATABASE_URL in a subsequent step
  run: |
    echo "Using DATABASE_URL: $DATABASE_URL"
```

This example shows creating the password and getting back a response in json. The json is then parsed to create a `DATABASE_URL` which can be used in later steps.

<Note>
  `pscale password create` can also accept a `ttl` flag which lets you limit the number of minutes the password is valid for.
</Note>

### Open a deploy request

You can use `pscale deploy-request create` to open a new deploy request from GitHub Actions.
This can be useful after running migrations against a branch.

```yaml  theme={null}
- name: Open DR if migrations
  env:
    PLANETSCALE_SERVICE_TOKEN_ID: ${{ secrets.PLANETSCALE_SERVICE_TOKEN_ID }}
    PLANETSCALE_SERVICE_TOKEN: ${{ secrets.PLANETSCALE_SERVICE_TOKEN }}
  run: pscale deploy-request create ${{ secrets.PLANETSCALE_DATABASE_NAME }} ${{ env.PSCALE_BRANCH_NAME }}
```

### Get deploy request by branch name

You can use `pscale deploy-requests show` to grab the latest deploy request by branch name.

This can be useful when deploying your application. You can first check if there are any deploy requests open for the branch being deployed. If there are, you can
trigger the deploy request to run before you deploy your application.

```yaml  theme={null}
- name: Get Deploy Requests
  env:
    PLANETSCALE_SERVICE_TOKEN_ID: ${{ secrets.PLANETSCALE_SERVICE_TOKEN_ID }}
    PLANETSCALE_SERVICE_TOKEN: ${{ secrets.PLANETSCALE_SERVICE_TOKEN }}
  run: |
    deploy_request_number=$(pscale deploy-request show ${{ secrets.PLANETSCALE_DATABASE_NAME }} ${{ env.PSCALE_BRANCH_NAME }} --org ${{ secrets.PLANETSCALE_ORG_NAME }} -f json | jq -r '.number')
    echo "DEPLOY_REQUEST_NUMBER=$deploy_request_number" >> $GITHUB_ENV
```

This example also makes the deploy request number available as an `env` var so that it can be used in later steps.

### Get deploy request diff and comment on pull request

We can use `pscale deploy-request diff` to see the full schema diff of a deploy request.

This example is useful when combined with opening a deploy request for a git branch. You can then automatically comment the diff back to the GitHub pull request.

```yaml  theme={null}
- name: Comment on PR
  env:
    PLANETSCALE_SERVICE_TOKEN_ID: ${{ secrets.PLANETSCALE_SERVICE_TOKEN_ID }}
    PLANETSCALE_SERVICE_TOKEN: ${{ secrets.PLANETSCALE_SERVICE_TOKEN }}
  run: |
    echo "Deploy request opened: https://app.planetscale.com/${{ secrets.PLANETSCALE_ORG_NAME }}/${{ secrets.PLANETSCALE_DATABASE_NAME }}/deploy-requests/${{ env.DEPLOY_REQUEST_NUMBER }}" >> migration-message.txt
    echo "" >> migration-message.txt
    echo "\`\`\`diff" >> migration-message.txt
    pscale deploy-request diff ${{ secrets.PLANETSCALE_DATABASE_NAME }} ${{ env.DEPLOY_REQUEST_NUMBER }} --org ${{ secrets.PLANETSCALE_ORG_NAME }} -f json | jq -r '.[].raw' >> migration-message.txt
    echo "\`\`\`" >> migration-message.txt
- name: Comment PR - db migrated
  uses: thollander/actions-comment-pull-request@v2
  with:
    filePath: migration-message.txt
```

This writes the diff to the `migration-message.txt` file. And then creates a comment on the pull request that triggered the workflow.

### Check for dropped columns

PlanetScale sets a `can_drop_data` boolean for any schema change that drop a column or table. We can make use of this to emit a warning into our pull requests.

In this example, we first wait for the deployment check to be `ready`. During this time, PlanetScale is examining the schema change, verifying that it is safe and
generating the DDL statements to make the change. Once it's done, we then use this information to put a comment on the deploy request with tips on how to deploy it safely.

```yaml expandable theme={null}
- name: Check deployment state
    id: check-state
    env:
      PLANETSCALE_SERVICE_TOKEN_ID: ${{ secrets.PLANETSCALE_SERVICE_TOKEN_ID }}
      PLANETSCALE_SERVICE_TOKEN: ${{ secrets.PLANETSCALE_SERVICE_TOKEN }}
    run: |
      for i in {1..10}; do
        deployment_state=$(pscale deploy-request show ${{ secrets.PLANETSCALE_ORG_NAME }} ${{ env.PSCALE_BRANCH_NAME }} --org ${{ secrets.PLANETSCALE_ORG_NAME }} --format json | jq -r '.deployment_state')
        echo "Deployment State: $deployment_state"

        if [ "$deployment_state" = "ready" ]; then
          echo "Deployment state is ready. Continuing."
          break
        fi

        echo "Deployment state is not ready. Waiting 2 seconds before checking again."
        sleep 2
      done
  - name: Comment PR - db migrated
    if: ${{ env.DR_OPENED }}
    env:
      PLANETSCALE_SERVICE_TOKEN_ID: ${{ secrets.PLANETSCALE_SERVICE_TOKEN_ID }}
      PLANETSCALE_SERVICE_TOKEN: ${{ secrets.PLANETSCALE_SERVICE_TOKEN }}
    run: |
      deploy_data=$(pscale api organizations/${{ secrets.PLANETSCALE_ORG_NAME }}/databases/${{ secrets.PLANETSCALE_DATABASE_NAME }}/deploy-requests/${{ env.DEPLOY_REQUEST_NUMBER }}/deployment --org planetscale)
      can_drop_data=$(echo "$deploy_data" | jq -r '.deploy_operations[] | select(.can_drop_data == true) | .can_drop_data')

      echo "Deploy request opened: https://app.planetscale.com/${{ secrets.PLANETSCALE_ORG_NAME }}/${{ secrets.PLANETSCALE_DATABASE_NAME }}/deploy-requests/${{ env.DEPLOY_REQUEST_NUMBER }}" >> migration-message.txt
      echo "" >> migration-message.txt

      if [ "$can_drop_data" = "true" ]; then
        echo ":rotating_light: You are dropping a column. Before running the migration make sure to do the following:" >> migration-message.txt
        echo "" >> migration-message.txt

        echo "1. [ ] Deploy app changes to ensure the column is no longer being used." >> migration-message.txt
        echo "2. [ ] Once you've verified it's no used, run the deploy request." >> migration-message.txt
        echo "" >> migration-message.txt
      else
        echo "When adding to the schema, the Deploy Request must be run **before** the code is deployed." >> migration-message.txt
        echo "Please ensure your schema changes are compatible with the application code currently running in production." >> migration-message.txt
        echo "" >> migration-message.txt

        echo "1. [ ] Successfully run the Deploy Request" >> migration-message.txt
        echo "2. [ ] Deploy this PR" >> migration-message.txt
        echo "" >> migration-message.txt
      fi

      echo "\`\`\`diff" >> migration-message.txt
      pscale deploy-request diff ${{ secrets.PLANETSCALE_ORG_NAME }} ${{ env.DEPLOY_REQUEST_NUMBER }} -f json | jq -r '.[].raw' >> migration-message.txt
      echo "\`\`\`" >> migration-message.txt
  - name: Comment PR - db migrated
    uses: thollander/actions-comment-pull-request@v2
    if: ${{ env.DR_OPENED }}
    with:
      filePath: migration-message.txt
```

### Submit a deploy request by branch name

To trigger a deploy, we can use `pscale deploy-request deploy`. This command will accept either the deploy request number, or the name of the branch that the deploy request was created from.

When using with GitHub Actions, it's often easier to use the branch name.

The `--wait` flag will let the command run until the deployment is complete. This is important if you want your schema change to run before the next step in your workflow.

```yaml  theme={null}
- name: Deploy schema migrations
  env:
    PLANETSCALE_SERVICE_TOKEN_ID: ${{ secrets.PLANETSCALE_SERVICE_TOKEN_ID }}
    PLANETSCALE_SERVICE_TOKEN: ${{ secrets.PLANETSCALE_SERVICE_TOKEN }}
  run: |
    pscale deploy-request deploy ${{ secrets.PLANETSCALE_DATABASE_NAME }} ${{ env.PSCALE_BRANCH_NAME }} --org ${{ secrets.PLANETSCALE_ORG_NAME }} --wait
```

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Deploy to Vercel
Source: https://planetscale.com/docs/vitess/tutorials/nextjs-deploy-to-vercel

This guide will walk you through setting up and deploying your PlanetScale database on Vercel.

It will cover two options:

<CardGroup>
  <Card title="Manually adding PlanetScale environment variables to your Vercel project" href="#manually-connect-to-vercel" icon="angles-right" />

  <Card title="Connecting your PlanetScale database to your application using the Vercel integration" href="#deploy-with-the-planetscale-vercel-integration" icon="angles-right" />
</CardGroup>

## Prerequisites

* A PlanetScale database — If you haven't created a database, refer to our [PlanetScale quickstart guide](/docs/vitess/tutorials/planetscale-quick-start-guide) to get started
* A [Vercel account](https://vercel.com/)
* A project deployed to Vercel — If you're just poking around and don't already have an application to deploy, you can use our [Next.js + PlanetScale sample](/docs/vitess/tutorials/connect-nextjs-app)

## Manually connect to Vercel

### Get your connection string from PlanetScale

<Steps>
  <Step>
    In your [PlanetScale dashboard](https://app.planetscale.com), click on the database you want to connect to.
  </Step>

  <Step>
    Click "**Connect**".
  </Step>

  <Step>
    Create a new password. Make sure to copy the password, as you'll only be shown it once.
  </Step>

  <Step>
    Select the framework you're using from the "**Select your language or framework**" section. This will give you the exact environment variable names you need for your selected framework. If your framework is not listed, choose "Other".

    <Frame>
            <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/prisma.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=fac97227b034edf22a1c3656ed5d2fc7" alt="PlanetScale dashboard connect modal {priority}" data-og-width="3058" width="3058" data-og-height="1644" height="1644" data-path="docs/images/assets/docs/tutorials/deploy-to-vercel/prisma.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/prisma.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=04521f523c423e9c2b2f5fe91c5f233a 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/prisma.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=3b932dc84efe572119fa65a2fe5c0fb2 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/prisma.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=5d66cb5b2649222a6e7386e5624ceb7c 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/prisma.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=394acb2d72332a7dc1c97902cf0f38b9 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/prisma.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=ba86e785c3f4b65b34194d8a1d968da9 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/prisma.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=b02f3870e62d865bd5f263255c9c60d4 2500w" />
    </Frame>
  </Step>

  <Step>
    Keep this page open, as you'll need to copy these to Vercel momentarily.
  </Step>

  <Step>
    If you navigate away from the page, and can no longer access the password, create a new password by repeating steps 1-5.
  </Step>
</Steps>

### Copy environment variables to Vercel

<Steps>
  <Step>
    Go to your Vercel dashboard.
  </Step>

  <Step>
    Click on your Vercel project.
  </Step>

  <Step>
    Click "**Settings**".
  </Step>

  <Step>
    Click "**Environment variables**".
  </Step>

  <Step>
    Copy each value from your PlanetScale dashboard into a new environment variable in Vercel. Once you're done with one, click "**Add**" and continue to the next, if applicable.
  </Step>
</Steps>

For example, if you're using Prisma, your connection string will look similar to this:

```bash  theme={null}
DATABASE_URL='mysql://xxxxxxxxx:************@xxxxxxxxxx.us-east-3.psdb.cloud/my_database?sslaccept=strict'
```

In Vercel, you'll set it as follows:

* **NAME** = `DATABASE_URL`
* **VALUE** = `mysql://xxxxxxxxx:************@xxxxxxxxxx.us-east-3.psdb.cloud/my_database?sslaccept=strict`

<Note>
  The credentials are blurred for the example, but when you paste them in, use the actual values.
</Note>

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/environment-variables.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=67a6c07441bc0ae4d0f321643211a2a0" alt="Vercel dashboard - Environment variables" data-og-width="1400" width="1400" data-og-height="843" height="843" data-path="docs/images/assets/docs/tutorials/deploy-to-vercel/environment-variables.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/environment-variables.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=f85c3cd707e970253ebb67939123df2b 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/environment-variables.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=c2e65af344a33fe86250e698b01d26e0 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/environment-variables.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=d883079f64d02f3dd6dbec13cab3025b 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/environment-variables.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=1b77728c5301f34b406ad1bc9943f82f 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/environment-variables.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=4ea56380aba4e8e8c83b10c9996864b6 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/deploy-to-vercel/environment-variables.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=ae5ba57fa910c64a2c202f317ca8884b 2500w" />
</Frame>

## Deploy with the PlanetScale Vercel integration

If you don't want to copy and paste the environment variables over to Vercel, you can use the [PlanetScale integration from the Vercel marketplace](https://vercel.com/integrations/planetscale). You can choose which database you want to connect to, and we'll automatically pull the necessary environment variables into your Vercel project.

<Steps>
  <Step>
    You must have an existing PlanetScale database to use the integration. You can create a database in the [PlanetScale dashboard](https://app.planetscale.com).
  </Step>

  <Step>
    Click "**Add integration**" on the [Vercel integrations page](https://vercel.com/integrations/planetscale).
  </Step>

  <Step>
    Select the Vercel account you want to connect with.
  </Step>

  <Step>
    On the left, you'll see the Vercel options, and on the right, the PlanetScale options.
  </Step>

  <Step>
    Select the Vercel project you want to connect to, and beneath that, select the framework you're using. If the framework isn't listed, select "**General**". This selection is what determines the names of the environment variables.
  </Step>

  <Step>
    On the right side, choose the [PlanetScale Organization](/docs/security/access-control) that the database is in. The integration will remain tied to this Organization and cannot be changed.
  </Step>

  <Step>
    Beneath that, select the database you want to connect to.
  </Step>

  <Step>
    Click "**Connect database**".
  </Step>

  <Step>
    Back in your Vercel dashboard, confirm the environment variables were added by going to your Vercel project > "**Settings**" > "**Environment variables**"
  </Step>
</Steps>

### Configure your connection

After you set up the initial connection, you also have the option to configure the PlanetScale connection, add more databases to the project, or remove databases from the project.

To access the configuration page:

<Steps>
  <Step>
    In your Vercel project dashboard, click "**Integrations**".
  </Step>

  <Step>
    Click the "**PlanetScale**" integration.
  </Step>

  <Step>
    Click the "**Configure**" button.
  </Step>
</Steps>

<Note>
  If you're modifying an existing connection on a Vercel project, these environment variable values will be regenerated and overwritten.
</Note>

<Note>
  * Environment variables are removed from all projects tied to the integration.
  * Your application will no longer be able to connect to your PlanetScale database.
</Note>

## What's next?

Learn more about how PlanetScale allows you to make [non-blocking schema changes](/docs/vitess/schema-changes) to your database tables without locking or causing downtime for production databases.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Next.js and PlanetScale Netlify template tutorial
Source: https://planetscale.com/docs/vitess/tutorials/nextjs-planetscale-netlify-template



## Overview

This guide will show you how to get up and running with the [Netlify, Next.js, and PlanetScale starter template](https://templates.netlify.com/template/nextjs-planetscale-starter/). The template includes the following features:

* Simple user admin dashboard
* [PlanetScale](/) database
* [Prisma ORM](https://www.prisma.io/) integration
* [Next.js authentication](https://nextjs.org/docs/authentication)
* One-click [deploy to Netlify](https://netlify.com)
* [Tailwind CSS](https://tailwindcss.com/) styling

You can see a [live demo of the starter application here {priority}](https://nextjs-planetscale-starter.netlify.app/).

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/example.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=e31ec138b463cde47fcaa0c9c4800082" alt="Example of the dashboard application" data-og-width="2360" width="2360" data-og-height="1465" height="1465" data-path="docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/example.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/example.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=03095af4c4619b0e3b2884205491db7e 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/example.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=eaa3814f5503714f2d5e50cdd6290dc4 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/example.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=296062f814a76bd0530908e291e1a64a 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/example.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=f52f4a33bffa31c79f6827ecfcae7751 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/example.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=70de583d37751f43d3ab5e1e72ea4df7 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/example.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=467fe053bade00c089e78a1934b986ca 2500w" />
</Frame>

<Note>
  If you're coming from the **[Netlify Template](https://templates.netlify.com/template/nextjs-planetscale-starter/)** and you already clicked deploy, you're in the right place! This tutorial will walk you through how to set up your PlanetScale database so that you can fill in the environment variables in the Netlify dashboard. You'll also learn how to set up your local environment so you can continue to develop and extend this starter template. Just read through the prerequisites and then skip the first section to [go straight to the local setup](#set-up-the-project-locally).

  If you're **starting fresh and haven't deployed yet** (or don't want to deploy), you can start from the beginning of this tutorial.
</Note>

### Prerequisites

To follow along with this guide, you'll need the following:

* A [free PlanetScale account](https://auth.planetscale.com/sign-up)
* The [PlanetScale CLI](https://github.com/planetscale/cli#installation)
* [Yarn](https://yarnpkg.com/getting-started/install)
* [Node (LTS)](https://nodejs.org/en/)
* A [free Netlify account](https://app.netlify.com/signup)

## One-click deploy to Netlify

The one-click deploy button allows you to connect Netlify to your GitHub account to clone the `nextjs-planetscale-starter` repository and automatically deploy it. Be sure to [sign up for a Netlify account](https://app.netlify.com/signup) before clicking the deploy button.

[![Deploy to Netlify button](https://www.netlify.com/img/deploy/button.svg)](https://app.netlify.com/start/deploy?repository=https://github.com/planetscale/nextjs-planetscale-starter)

Once you click the button, you'll be taken to Netlify’s direct deploy page with the pre-built project’s repository passed as a parameter in the URL. Click the "**Connect to GitHub**" button to authorize access.

Next, you'll be asked to configure your site variables. For the `Secret` value, navigate to [`https://generate-secret.now.sh/32`](https://generate-secret.now.sh/32) to generate a secret and then paste that in. You can leave the `Database URL` and `NextAuth URL` values blank for now. Click "Save & Deploy".

Your site will take about a minute to build and then you'll be taken to a settings page. A unique Netlify URL will be generated for the project. You can click that now to see your live site! The next section will show you how to set the project up locally and create your PlanetScale database to connect to your live site.

## Set up the project locally

If you already went through the [Netlify deployment](https://templates.netlify.com/template/nextjs-planetscale-starter/), find the repository that was created for you in your GitHub account and clone it.

If you didn't deploy and just want to run the template locally, you can clone the [`nextjs-planetscale-starter` repository](https://github.com/planetscale/nextjs-planetscale-starter).

Enter into the folder and install the dependencies:

```bash  theme={null}
yarn install
```

Run the application with:

```bash  theme={null}
yarn next
```

Navigate to [http://localhost:3000/](http://localhost:3000/) in your browser to view the PlanetScale Next.js Starter app.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/starter.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=45c94ed90bc60e0a3dcf351a5f6db93e" alt="Next.js PlanetScale Starter application homepage" data-og-width="3522" width="3522" data-og-height="1722" height="1722" data-path="docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/starter.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/starter.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=fc89c6bfa0ec880fb7b9a3148bba718f 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/starter.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=aa9c43490ff3f261da22d00e45fbb993 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/starter.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=71eb1a2fefec7e140d94fd742e47ad5d 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/starter.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=996115fa53fde5fa6ad8f676ad93e9c1 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/starter.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=5a44661d5b9f816a0832f616daab953a 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/nextjs-planetscale-netlify-template/starter.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=91b65399a578a077cb60899f32cdf008 2500w" />
</Frame>

## Database setup

Next, you need to set up your PlanetScale database. If you don't already have a [PlanetScale account](/docs/planetscale-plans), you can [sign up for a free one here](https://auth.planetscale.com/sign-up).

### Create your database

To begin, create a new database. You can either do this in the dashboard or using the [PlanetScale CLI](/docs/cli).

In the dashboard, click on the "**Create a database**" button. Name your database "`netlify-starter`", or whatever name you wish. Select the region closest to you, and click "**Create database**".

Alternatively, [sign in and create a database with the CLI](/docs/vitess/tutorials/planetscale-quick-start-guide#getting-started-planetscale-cli) by running the following:

```bash  theme={null}
pscale database create <database-name> --region <region-slug>
```

The list of region slugs can be found in our [Regions documentation](/docs/vitess/regions#available-regions).

### Connect to your database branches locally

To connect locally, make sure you've authenticated in the CLI. If not, follow the directions in the [sign-in section of our quickstart guide](/docs/vitess/tutorials/planetscale-quick-start-guide#sign-in-to-your-account).

Next, connect to your `main` branch locally by running the following in your terminal:

```bash  theme={null}
pscale connect <database-name> main --port <port>
```

Choose any unused port. This tutorial uses `3309`. You'll see the response "Secure connection to database `netlify-starter` and branch `main` is established!" along with the local proxy address for your database. Take note of that address for the next step.

## Set up local environment variables

For the last part of setup, you need to fill in your environment variables.

Make a copy of the `.env.example` file at the root of your project and rename it `.env`:

```bash  theme={null}
cp .env.example .env
```

The `DATABASE_URL` value comes from your PlanetScale database and will be in the following form:

`mysql://<user>@<address>:<port>/<database>`

* `user` — the database user
* `address` — the local address returned in the previous step
* `port` — the port you specified in the previous step
* `database` — the name of your database

Below is an example of the `.env` file based on this tutorial:

```js  theme={null}
DATABASE_URL="mysql://root@127.0.0.1:3309/netlify-starter"
NEXTAUTH_URL=http://localhost:3000
# Navigate to https://generate-secret.now.sh/32 to generate a secret for the variable below
NEXTAUTH_SECRET=
```

Remember, these are the values for your **local environment**. The values needed for the Netlify environment variables will be covered shortly.

## Push your database schema to PlanetScale

Now that your PlanetScale database is connected to your application, it's time to [push your database schema](https://www.prisma.io/docs/orm/reference/prisma-cli-reference#db-push).

In your terminal, run:

```bash  theme={null}
yarn db:push
```

You can view the schema in your PlanetScale dashboard by clicking the `netlify-starter` database > "**Branches**" > `main` > "**Schema**" > "**Refresh schema**".

You'll now see the following tables:

* `Account`
* `Session`
* `User`
* `VerificationToken`

You can also verify it worked using the PlanetScale CLI. Run the following to start a MySQL shell (where `netlify-starter` is your database):

```bash  theme={null}
pscale shell netlify-starter main
```

Once in the shell, view the tables with:

```bash  theme={null}
SHOW tables;
```

Type `exit` to exit the MySQL shell.

The database schema, `db/schema.prisma`, has been loaded and your PlanetScale database is now ready for data.

## Seed the database

To seed the database with users, run the following:

```bash  theme={null}
yarn db:seed
```

This will add three mock users to the `User` database, as described in `db/seed.ts`. To verify that they were added, click on the "**Console**" tab in your PlanetScale dashboard on the `main` branch of your `netlify-starter` database.

Run the following:

```bash  theme={null}
SELECT * from User;
```

You can also run this command from the CLI using the same `pscale shell` command mentioned above.

## Netlify environment variables

Now that you have your site running locally, let's shift back to the live Netlify site.

The final step in the site deployment is configuring your production environment variables. In the Netlify dashboard under your site's settings page, click on "**Build & deploy**" > "**Environment**" in the left nav.

Click on the "**Edit variables**" button and enter in the following key/value pairs:

* `DATABASE_URL` — To find this value, go back to your PlanetScale dashboard, click on the `netlify-starter` database, click "**Connect**". If the password is masked and you don't have one, you may click the "**Generate new password**" button to create a new one. Next, click on the "**General**" dropdown in that modal and select "**Prisma**". Copy the value for `url` and paste that back in the Netlify dashboard as the value for `DATABASE_URL`. Be sure to save your PlanetScale password somewhere as you won't be able to access it again after closing the modal.
* `NEXTAUTH_SECRET` — You may have already filled this out, but if not generate a new secret at [https://generate-secret.now.sh/32](https://generate-secret.now.sh/32) and paste in the value that's returned.
* `NEXTAUTH_URL` — Paste in the Netlify site name that was generated for you. For example, `https://stoic-lumiere-6df10.netlify.app`

Click "**Save**". Now, redeploy the site with these new variables by going to "Deploys" in the top nav and clicking the "**Trigger deploy**" > "**Deploy site**" button.

## Set up your admin account

Now that your site is live and connected to your PlanetScale database, you need to set up your admin account for your application. Go to `/admin/setup` on your live Netlify site (or locally if you didn't deploy), and fill in the form to set up your account. This will be automatically saved to your PlanetScale database.

Creating an admin account gives you access to the `/admin` route in your application, which is the dashboard to manage your users.

Once you're signed in as an admin, navigate to `/admin` to see a list of your users.

## Customize and extend

And that's it! If you followed this tutorial completely, you now have a local and production version of your **Next.js + PlanetScale + Prisma admin dashboard**. So what's next?

Now, it's time to extend it! Next.js Authentication is baked into this starter, so you can explore the [Next.js docs to manage authentication](https://nextjs.org/docs/authentication) in your application.

You also have a fully functional PlanetScale MySQL database built for scale using the power of [Vitess](https://vitess.io/). You might have noticed that this tutorial uses the same database locally as it is in production. This was just for simplicity, but with PlanetScale, you can take advantage of our [powerful branching feature](/docs/vitess/schema-changes/branching) to create development branches of your database specifically for testing locally. All you have to do is create a new branch and swap out the `DATABASE_URL` environment variable in your local `.env` file.

Finally, you have [Prisma ORM](https://www.prisma.io/docs/) already configured in your application. If you want to add any more fields to your `User` table or create any new tables, Prisma makes it easy with the `schema.prisma` file. If you want to make any schema changes, it's a perfect time to try out the [PlanetScale branching feature](/docs/vitess/schema-changes/branching) feature. If you mess up, those changes won't touch production. And once you're satisfied with the changes, you can deploy to production without causing downtime thanks to PlanetScale [non-blocking schema changes](/docs/vitess/schema-changes).

Hopefully this tutorial has been helpful. We'd love to hear how you're extending your starter application. [Tweet us @PlanetScale](https://twitter.com/planetscale) and share what you built!

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale quickstart guide
Source: https://planetscale.com/docs/vitess/tutorials/planetscale-quick-start-guide



export const VimeoEmbed = ({id, title}) => {
  return <Frame>
      <iframe src={`https://player.vimeo.com/video/${id}?dnt=true`} title={title} className="aspect-video w-full" allow="autoplay; fullscreen; picture-in-picture" />
    </Frame>;
};

## Overview

The following guide will show you how to:

* Create a database with PlanetScale
* Make a schema change
* Insert data
* Promote your database branch to production

If you already have your PlanetScale database set up, you may find the [Connecting your application](/docs/vitess/tutorials/connect-any-application) or [Branching](/docs/vitess/schema-changes/branching) guides more helpful.

This guide is split up so that you can either follow it in the [PlanetScale dashboard](#getting-started--planetscale-dashboard) or using the [PlanetScale CLI](#getting-started--planetscale-cli).

<VimeoEmbed id="830571983" title="PlanetScale quickstart guide" />

## Getting started — PlanetScale dashboard

You'll need [a PlanetScale account](https://auth.planetscale.com/sign-up) to complete this guide.

### Create a database

Follow these steps to create a database:

<Steps>
  <Step>
    Click "**New database**" > "**Create a database**" on your organization's overview page.
  </Step>

  <Step>
    Name your database.
  </Step>

  <Step>
    Select a [region](/docs/vitess/regions). For the lowest latency, select a region near you or your application's hosting location.
  </Step>

  <Step>
    Select the desired [cluster and storage size](/docs/planetscale-plans#base) for your database.
  </Step>

  <Step>
    Enter a valid credit or debit card.
  </Step>

  <Step>
    Finally, click the "**Create database**" button to deploy your database.
  </Step>
</Steps>

Your database is created with a [**default production branch**](/docs/vitess/schema-changes/branching), `main`, which you will use to apply a schema change and insert data. While this is just the first branch we create for you, you can always create new development branches (isolated copies of the production schema) off of production to use for development.

### Add a schema to your database

This quickstart demonstrates how to create and use two relational tables: `categories` and `products`.

<Steps>
  <Step>
    By default, web console access to production branches is disabled to prevent accidental deletion. From your database's dashboard page, click on the "**Settings**" tab, check the box labelled "**Allow web console access to production branches**", and click "**Save database settings**".
  </Step>

  <Step>
    Click on the "**Console**" tab in the database navigation. This will open up a [web console](/docs/vitess/web-console) connected to your database branch.
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-quick-start-guide/the-console-tab.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=a4d3520f08cfedc57295ff6894b73e4a" alt="Branches" data-og-width="1634" width="1634" data-og-height="1084" height="1084" data-path="docs/images/assets/docs/tutorials/planetscale-quick-start-guide/the-console-tab.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-quick-start-guide/the-console-tab.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=4b45fdd728ea28a3888fa80ee4706836 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-quick-start-guide/the-console-tab.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=0733b0ebaa9cc1fcb38d25b10f124b68 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-quick-start-guide/the-console-tab.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=92dd8b845c8b6a409eb2907598209cc4 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-quick-start-guide/the-console-tab.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=58f498acbe3f6bcac0458e35ea03faef 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-quick-start-guide/the-console-tab.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=2c307dcba327b1b227d39e7429115e48 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-quick-start-guide/the-console-tab.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=c6921139d3c8048a69a36881fa2c8be6 2500w" />
  </Step>

  <Step>
    By default the `main` branch is preselected. Click **"Connect"**.
  </Step>

  <Step>
    Create the `categories` and `products` tables by running the following commands in the web console:

    ```sql  theme={null}
    CREATE TABLE categories (
      id int NOT NULL AUTO_INCREMENT PRIMARY KEY,
      name varchar(255) NOT NULL
    );
    ```

    ```sql  theme={null}
    CREATE TABLE products (
      id int NOT NULL AUTO_INCREMENT PRIMARY KEY,
      name varchar(255) NOT NULL,
      image_url varchar(255),
      category_id INT,
      KEY category_id_idx (category_id)
    );
    ```

    <Note>
      If you want to make schema changes containing foreign key constraints, enable [foreign key constraints](/docs/vitess/foreign-key-constraints) support in your database settings page.
    </Note>
  </Step>

  <Step>
    You can confirm that the tables have been added by running:

    ```sql  theme={null}
    SHOW TABLES;
    ```
  </Step>
</Steps>

### Insert data into your database

Now that you have created your tables, let's insert some data. Run the following commands to add a product and category to your tables:

```sql  theme={null}
INSERT INTO categories (name)
VALUES  ('Office supplies');
```

```sql  theme={null}
INSERT INTO products (name, image_url, category_id)
VALUES  ('Ballpoint pen', 'https://example.com/500x500', '1');
```

You can confirm the data has been added with:

```sql  theme={null}
SELECT * FROM products;
```

```sql  theme={null}
SELECT * FROM categories;
```

You can view the schema of your database by navigating to the "**Branches**" tab and selecting the database you want to view. For now, select the `main` database, and it will display the names of the two tables you just created. Click on the name of each table to see further schema details.

### Enable safe migrations on your `main` branch

All of the work you've done so far has been on a default production branch, `main`, that was automatically created when you created the database.

A production branch is a highly available database branch that includes an additional replica. It also has the option to enable [safe migrations](/docs/vitess/schema-changes/safe-migrations), which enables non-blocking schema changes and can protect your database from accidental schema changes.

[Safe migrations](/docs/vitess/schema-changes/safe-migrations) is an optional, but highly recommended, feature that adds an additional layer of protection to your branch by preventing accidental schema modifications and enabling no-downtime schema changes. With safe migrations enabled, any DDL issued directly to the branch will not be accepted. Instead, changes must be made using the PlanetScale flow, where deploy requests are used to safely merge changes in a collaborative environment.

### To enable safe migrations:

<Steps>
  <Step>
    Click "Dashboard" in the navigation, and click the "**cog**" in the upper right of the infrastructure card to open a modal.
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-quick-start-guide/production-branch-card-with-sm-disabled-2.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=847b9711b99d7eb2b8a6979fef715096" alt="Production UI card" data-og-width="1904" width="1904" data-og-height="819" height="819" data-path="docs/images/assets/docs/tutorials/planetscale-quick-start-guide/production-branch-card-with-sm-disabled-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-quick-start-guide/production-branch-card-with-sm-disabled-2.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=af62627e3b9b3aa9b3c6b26753fc8920 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-quick-start-guide/production-branch-card-with-sm-disabled-2.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=31bc516f819eba4c8fa8182cba59ea97 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-quick-start-guide/production-branch-card-with-sm-disabled-2.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=85114b57e4b57e12599f215dbe22e201 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-quick-start-guide/production-branch-card-with-sm-disabled-2.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=7b91a17d0e01136ba3197a2740ae113d 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-quick-start-guide/production-branch-card-with-sm-disabled-2.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=b0865022d0d152331122ba5a34ee7ccf 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-quick-start-guide/production-branch-card-with-sm-disabled-2.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=f3095e7a4862f24b8957cdcfc2b34736 2500w" />
  </Step>

  <Step>
    Toggle "**Enable safe migrations**", then click the "**Enable safe migrations**" button.
  </Step>
</Steps>

The `main` branch now contains the `categories` and `products` tables you created, along with the data you inserted. In addition, it is highly available with an additional replica, and is enabled for zero-downtime migrations with [safe migrations](/docs/vitess/schema-changes/safe-migrations).

### What's next?

Now that you've created a database, applied schema changes, added data, and enable safe migrations, it's time to connect to your application.

You can use our [Connect Any Application tutorial](/docs/vitess/tutorials/connect-any-application) for a general step-by-step approach, one of our language-specific guides, or head straight to our [Connection Strings documentation](/docs/vitess/connecting/connection-strings) for more information about creating connection strings.

When you want to continue development on your database:

<Steps>
  <Step>
    [Create a new branch](/docs/vitess/schema-changes/branching) off of your production branch
  </Step>

  <Step>
    Go through the same process described in this doc to make schema changes
  </Step>

  <Step>
    [Create a deploy request](/docs/vitess/schema-changes/deploy-requests) to merge the changes into your production branch
  </Step>
</Steps>

<Note>
  When you branch off of a production branch, your development branch will have the same schema as production, but it
  **will not** copy over any data from the production database. We suggest seeding development branches with mock
  data.
</Note>

## Getting started — PlanetScale CLI

Make sure you first have [downloaded and installed the PlanetScale CLI](https://github.com/planetscale/cli#installation).

You will also need a PlanetScale account. You can [sign up for a PlanetScale account here](https://auth.planetscale.com/sign-up) or run `pscale signup` to create an account straight from the CLI.

### Sign in to your account

To authenticate with the PlanetScale CLI, enter the following:

```bash  theme={null}
pscale auth login
```

You'll be taken to a screen in the browser where you'll be asked to confirm the code displayed in your terminal. If the confirmation codes match, click the "**Confirm code**" button in your browser.

You should receive the message "Successfully logged in" in your terminal. You can now close the confirmation page in the browser and proceed in the terminal.

### Create a database

Run the following command to create a database:

```bash  theme={null}
pscale database create <DATABASE_NAME> --region <REGION_SLUG>
```

* **DATABASE\_NAME** — Your database name can contain lowercase, alphanumeric characters, or underscores. We allow dashes, but don't recommend them, as they may need to be escaped in some instances.
* **REGION\_SLUG** — For the lowest latency, choose the region closest to you or your application's hosting location. You can find our regions and their slugs on the [Regions page](/docs/vitess/regions#available-regions).

<Note>
  If you do not specify a region, your database will automatically be deployed to **US East — Northern Virginia**.
</Note>

Your database is created with an [**initial branch**](/docs/vitess/schema-changes/branching), `main`, which you will use to apply a schema change and insert data. While this is just the first branch we create for you, you can always create new branches (isolated copies of the production schema) off of production to use for development.

### Add a schema to your database

To add a schema to your database, you will need to connect to MySQL, so [make sure you `mysql-client` installed](/docs/cli/planetscale-environment-setup#setup-overview).

<Steps>
  <Step>
    Run the following command:

    ```bash  theme={null}
    pscale shell <DATABASE_NAME> main
    ```

    You are now connected to your `main` branch and can run MySQL queries against it.
  </Step>

  <Step>
    Create the `categories` and `products` tables by running the following:

    ```sql  theme={null}
    CREATE TABLE categories (
      id int NOT NULL AUTO_INCREMENT PRIMARY KEY,
      name varchar(255) NOT NULL
    );
    ```

    ```sql  theme={null}
    CREATE TABLE products (
      id int NOT NULL AUTO_INCREMENT PRIMARY KEY,
      name varchar(255) NOT NULL,
      image_url varchar(255),
      category_id INT,
      KEY category_id_idx (category_id)
    );
    ```

    <Note>
      If you want to make schema changes containing foreign key constraints, enable [foreign key constraints](/docs/vitess/foreign-key-constraints) support in your database settings page.
    </Note>
  </Step>

  <Step>
    You can confirm that the table has been added by running:

    ```sql  theme={null}
    SHOW TABLES;
    ```
  </Step>

  <Step>
    To see the table schemas, run:

    ```sql  theme={null}
    DESCRIBE categories;
    ```

    ```sql  theme={null}
    DESCRIBE products;
    ```
  </Step>
</Steps>

### Insert data into your database

Now that you have your schema set up, let's insert some data.

<Steps>
  <Step>
    Run the following commands to add one entry to each table:

    ```sql  theme={null}
    INSERT INTO `categories` (name)
    VALUES  ('Office supplies');
    ```

    ```sql  theme={null}
    INSERT INTO `products` (name, image_url, category_id)
    VALUES  ('Ballpoint pen', 'https://example.com/500x500', '1');
    ```
  </Step>

  <Step>
    You can confirm the data has been added with:

    ```sql  theme={null}
    SELECT * FROM products;
    ```

    ```sql  theme={null}
    SELECT * FROM categories;
    ```
  </Step>

  <Step>
    Exit the shell by typing `exit`.
  </Step>
</Steps>

### Enable safe migrations

All of the work you've done so far has been on a default production branch, `main`, that was automatically created when you created the database.

A production branch is a highly available database branch that includes an additional replica. It also has the option to enable [safe migrations](/docs/vitess/schema-changes/safe-migrations), which enables non-blocking schema changes and can protect your database from accidental schema changes.

[Safe migrations](/docs/vitess/schema-changes/safe-migrations) is an optional, but highly recommended, feature that adds an additional layer of protection to your branch by preventing accidental schema modifications and enabling no-downtime schema changes. With safe migrations enabled, any DDL issued directly to the branch will not be accepted. Instead, changes must be made using the PlanetScale flow, where deploy requests are used to safely merge changes in a collaborative environment.

To enable safe migrations on your branch, run:

```bash  theme={null}
pscale branch safe-migrations enable <DATABASE_NAME> main
```

The `main` branch now contains the `categories` and `products` tables you created, along with the data you inserted. In addition, it is highly available with an additional replica, and is enabled for zero-downtime migrations with [safe migrations](/docs/vitess/schema-changes/safe-migrations).

### What's next?

Now that you've created a database, applied schema changes, added data, and enabled safe migrations, it's time to connect to your application.

You can use our [Connect Any Application tutorial](/docs/vitess/tutorials/connect-any-application) for a general step-by-step approach, one of our language-specific guides, or head straight to our [Connection Strings documentation](/docs/vitess/connecting/connection-strings) for more information about creating connection strings.

When you want to continue development on your database:

<Steps>
  <Step>
    [Create a new branch](/docs/vitess/schema-changes/branching) off of your production branch
  </Step>

  <Step>
    Go through the same process described in this doc to make schema changes
  </Step>

  <Step>
    [Create a deploy request](/docs/vitess/schema-changes/deploy-requests) to merge the changes into your production branch
  </Step>
</Steps>

<Note>
  When you branch off of a production branch, your development branch will have the same schema as production, but it
  **will not** copy over any data from the production database. We suggest seeding development branches with mock
  data.
</Note>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# PlanetScale serverless driver for JavaScript
Source: https://planetscale.com/docs/vitess/tutorials/planetscale-serverless-driver



## Why use the PlanetScale serverless driver

Before learning how to use the PlanetScale serverless driver for JavaScript, it’s worth understanding why you should use this over other MySQL packages available in the directory. Some serverless and edge function hosts do not permit arbitrary outbound TCP connections, which is how many MySQL clients operate.

Using the PlanetScale serverless driver for JavaScript provides a means of accessing your database and executing queries over an HTTP connection, which is generally not blocked by cloud providers. If you encounter issues using MySQL packages with PlanetScale, use the serverless driver instead.

<Note>
  Be sure to check out our [F1 Championship Stats demo application](https://github.com/planetscale/f1-championship-stats) to find examples for use with Cloudflare Workers, Vercel Edge Functions, and Netlify Edge Functions.
</Note>

## Add and use the PlanetScale serverless driver for JavaScript to your project

To install the package in your project, run the following install command:

```bash  theme={null}
npm install @planetscale/database
```

### Connect to the database

The first step to using the PlanetScale serverless driver for JavaScript is to connect to your database.

You can get your connection string in the PlanetScale dashboard by clicking on your database, clicking "**Connect**", and selecting `database-js` from the "Select your language or framework" section.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver/connect-serverless-credentials-database-js.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=90c1d10b58839a8ae02f43ea55a4cec1" alt="Database-js selection priority" data-og-width="3512" width="3512" data-og-height="1632" height="1632" data-path="docs/images/assets/docs/tutorials/planetscale-serverless-driver/connect-serverless-credentials-database-js.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver/connect-serverless-credentials-database-js.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=ac68aacadd62607e3e6a2b327029527c 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver/connect-serverless-credentials-database-js.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=5b57ec3a6d666ef6c43388464d2fd061 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver/connect-serverless-credentials-database-js.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=58bd08177ed713cedcc761f99b34a595 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver/connect-serverless-credentials-database-js.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=a4b54aa055fe7442c9b3be9850f706d7 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver/connect-serverless-credentials-database-js.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=7d181469927efbc41ece4368c33ebe01 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver/connect-serverless-credentials-database-js.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=2df54bdf74427117e4c266aedb62ef51 2500w" />
</Frame>

Scroll down to the env variables. You'll need this to connect to your database.

Use the `connect` function to create the connection and return it to an object.

```js  theme={null}
const config = {
  host: 'aws.connect.psdb.cloud',
  username: '<PS_USERNAME>',
  password: '<PS_PASSWORD>'
}
const conn = await connect(config)
```

### Executing queries

To execute a query, use the `execute` function of the connection object, with the query passed as the first parameter.

```js  theme={null}
const results = await conn.execute('SELECT * FROM hotels')
```

Here is the content of the `results` object from the `SELECT` statement:

```js expandable theme={null}
{
  headers: [ 'id', 'name', 'address', 'stars' ],
  types: {
    id: 'UINT32',
    name: 'VARCHAR',
    address: 'VARCHAR',
    stars: 'FLOAT32'
  },
  rows: [
    {
      id: 1,
      name: 'Four Seasons Resort Jackson hole',
      address: '7680 Granite Loop Rd, Teton Village, WY 83025',
      stars: 4.7
    },
    {
      id: 2,
      name: 'The Galt House',
      address: '140 N Fourth St, Louisville, KY 40202',
      stars: 4
    },
    // ...results removed for brevity
  ],
  rowsAffected: null,
  insertId: null,
  error: null,
  size: 5,
  statement: 'SELECT * FROM hotels',
  time: 136
}
```

For parameterized queries, there are two ways in which to pass data to the query. The first is by the order in which they appear in the query. The first step is to add a `?` in the specific locations you want the parameters passed into.

```js  theme={null}
const query = 'INSERT INTO hotels (`name`, `address`, `stars`) VALUES (?, ?, ?)'
```

Then you can pass your parameters as an array of values. The driver package will replace the `?` entries in the query with the values passed in the array, in the order in which they were placed.

```js  theme={null}
const params = ['The Galt House', '140 N Fourth St, Louisville, KY 40202', 4.2]
const results = await conn.execute(query, params)
```

Here is the content of the `results` object for the `INSERT` statement:

```js  theme={null}
{
  headers: [],
  types: {},
  rows: [],
  rowsAffected: 1,
  insertId: '6',
  error: null,
  size: 0,
  statement: "INSERT INTO hotels (`name`, `address`, `stars`) VALUES ('Montage Kapalua Bay 2', '1 Bay Dr, Lahaina, HI 96761', 4)",
  time: 102
}
```

Alternately, you can name your parameters using the `:param_name` format.

```js  theme={null}
const query = 'INSERT INTO hotels (`name`, `address`, `stars`) VALUES (:name, :address, :stars)'
const params = {
  name: 'The Galt House',
  address: '140 N Fourth St, Louisville, KY 40202',
  stars: 4.2
}
const results = await conn.execute(query, params)
```

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Node.js example using the PlanetScale serverless driver
Source: https://planetscale.com/docs/vitess/tutorials/planetscale-serverless-driver-node-example

This guide will cover how to use the provided Node.js sample application using the PlanetScale serverless driver for JavaScript.

## Overview

<Note>
  This guide will be using VS Code as the IDE, but you may use your preferred IDE.
</Note>

## Use the sample repository

We offer a sample repository that can be used as an educational resource. It is an Express API that can be run locally with sample `SELECT`, `INSERT`, `UPDATE`, and `DELETE` statements mapped to the proper API endpoints.

To follow along, you’ll need the following:

* A PlanetScale account, as well as knowing how to create a database.
* The PlanetScale CLI is installed on your computer, which will be used to seed data.

Start by creating a database in PlanetScale by clicking **"New database"** > **"Create new database"**.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/how-to-create-a-new-database-2.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=c498f865bb460d39374b40aa0d0c9f5e" alt="How to create a new database. priority" data-og-width="1604" width="1604" data-og-height="825" height="825" data-path="docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/how-to-create-a-new-database-2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/how-to-create-a-new-database-2.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=6ba0a2413fa5c5b8d8a6eab6695c7ed3 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/how-to-create-a-new-database-2.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=072bebd5e88951b1a9c40876d3256b21 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/how-to-create-a-new-database-2.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=6971bb21d99e5993a6b8495f27335440 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/how-to-create-a-new-database-2.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=5f9290f26dd8697d532adc60c11033e8 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/how-to-create-a-new-database-2.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=fdf571502d7224b8dbd096aa06e0f5fc 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/how-to-create-a-new-database-2.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=1e6ed9155a19c7e37cecd9459c60b9fa 2500w" />
</Frame>

Name the database `travel_db`. Click **"Create database"**. Wait for the database to finish initializing before moving on.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/initializing.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=8581222d3e5be6da8671460a14a4dd31" alt="The travel_db initializing." data-og-width="3490" width="3490" data-og-height="1144" height="1144" data-path="docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/initializing.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/initializing.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=0f584bc7a8f23915f2a74bff0e42156c 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/initializing.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=d3db45498496e194383b146d895e7b6d 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/initializing.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=d16fc1daa20742f695b053849365d6d6 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/initializing.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=a38bf28ff2741360175e8a5aaa900f37 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/initializing.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=af6f7566a850750db1175ef0c42ee839 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/initializing.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=fa407b86be8240b3489d7e29db36b748 2500w" />
</Frame>

Generate a set of credentials by clicking the **"Connect"** button.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/the-connect-button-in-the-planetscale-dashboard.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=03865ddef1ce2c082d0988a0ae14f1a7" alt="The Connect button in the PlanetScale dashboard." data-og-width="3494" width="3494" data-og-height="1078" height="1078" data-path="docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/the-connect-button-in-the-planetscale-dashboard.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/the-connect-button-in-the-planetscale-dashboard.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=ec43ffb42a8c8ca24bf755ee89c24ec9 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/the-connect-button-in-the-planetscale-dashboard.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=bc2c4e32abf80f10808d1a7733b7c2eb 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/the-connect-button-in-the-planetscale-dashboard.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=6f99c1a695581319b6ad8aecbb1014d0 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/the-connect-button-in-the-planetscale-dashboard.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=a8c772b9413aef4dc952b21a5cda167e 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/the-connect-button-in-the-planetscale-dashboard.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=8c7252745f5e5a1dba80447221207042 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/the-connect-button-in-the-planetscale-dashboard.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=59b71b28a3bc48a1e72489ba0ea245a3 2500w" />
</Frame>

Copy your password credentials first:

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/the-serverlessjs-connect-modal.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=7891a38fffcd855a7f8b4361bae2e6b3" alt="The password details." data-og-width="3498" width="3498" data-og-height="1118" height="1118" data-path="docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/the-serverlessjs-connect-modal.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/the-serverlessjs-connect-modal.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=487986ba44c9e925e50505ea6239eb7c 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/the-serverlessjs-connect-modal.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=6b7d27c21a9b792d8ade9db1bcd24180 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/the-serverlessjs-connect-modal.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=fa598f2f16d18b462ea5266a65b36141 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/the-serverlessjs-connect-modal.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=1936b2e9146bb188adbd0282f39c8e46 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/the-serverlessjs-connect-modal.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=c2ddbead3b61cdb65401a51198458ed8 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/the-serverlessjs-connect-modal.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=9c69974c751ed28d91c7995c1025a4c1 2500w" />
</Frame>

Scroll down and select **"database-js"** from the "Select your language or framework" options. Copy the text from the **".env"** section, as we'll be putting this in the project after it's pulled down from GitHub.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/the-serverlessjs-connect-modal-env.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=622d7b1deca3f77fafec63775742caad" alt="The password env details." data-og-width="3496" width="3496" data-og-height="618" height="618" data-path="docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/the-serverlessjs-connect-modal-env.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/the-serverlessjs-connect-modal-env.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=9f9983feac3311480ce6b167292223cb 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/the-serverlessjs-connect-modal-env.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=7020db0bd4442ea094d25ea789f2d18b 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/the-serverlessjs-connect-modal-env.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=c11cede2439a0acab38ead59bf1c6092 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/the-serverlessjs-connect-modal-env.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=47f1ab272b2ab8e76d9cb73a7b02e967 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/the-serverlessjs-connect-modal-env.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=46d4352255fac2e718d05787c0ae9ed1 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/the-serverlessjs-connect-modal-env.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=f28174b61ece42508546aee7e3454559 2500w" />
</Frame>

On your workstation, open a terminal and clone the repository to your computer by running the following command:

```bash  theme={null}
git clone https://github.com/planetscale/database-js-starter
```

Navigate to the `scripts` folder and run the `seed_database.sh` script to populate a small database simulating a travel agency.

```bash  theme={null}
cd database-js-starter/scripts
./seed-database.sh
```

<Note>
  If you are using Windows, run this command through the [Windows Subsystem for Linux (WSL)](https://docs.microsoft.com/en-us/windows/wsl/)
</Note>

Create a new file named `.env` in the root of the project and paste in the sample provided from PlanetScale that you copied earlier.

To run the project, run the following commands from the root of the project.

```bash  theme={null}
npm install
npm start
```

If the project is running properly, you should receive a message stating that the API is running.

The `tests.http` file is designed to work with the [VS Code Rest Client plugin](https://marketplace.visualstudio.com/items?itemName=humao.rest-client), but can be used as a reference when testing with the tool of your choosing. If you are using the plugin, you may click the **"Send request"** button that appears above each request to see the API in action.

<Frame>
    <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/an-example-of-a-post-request-to-the-sample-project.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=278a59dab18c19f67f075a275fcb0691" alt="An example of a POST request to the sample project." data-og-width="860" width="860" data-og-height="310" height="310" data-path="docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/an-example-of-a-post-request-to-the-sample-project.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/an-example-of-a-post-request-to-the-sample-project.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=49f55ad87b30ef8ddddd2cac501e0e60 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/an-example-of-a-post-request-to-the-sample-project.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=7049d0e1d996f3ddf5e54234a28b3889 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/an-example-of-a-post-request-to-the-sample-project.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=ea3d665a9d45e25e6736a2f9a6aab1b2 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/an-example-of-a-post-request-to-the-sample-project.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=39e4d13a9d9635aa72ab0a48650bf7db 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/an-example-of-a-post-request-to-the-sample-project.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=91b173e646538f38c8ea0964ebfb40fc 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/assets/docs/tutorials/planetscale-serverless-driver-node-example/an-example-of-a-post-request-to-the-sample-project.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=74f69c690c3aa96f20b12836ea0240d5 2500w" />
</Frame>

If you check the terminal where the API was started, the response from the `execute` function is logged out for review.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Using the PlanetScale serverless driver with Prisma
Source: https://planetscale.com/docs/vitess/tutorials/planetscale-serverless-driver-prisma-example

This document outlines how you can use the [PlanetScale serverless driver](/docs/vitess/tutorials/planetscale-serverless-driver) along with Prisma in your application.

## Set up

To get started:

<Steps>
  <Step>
    Install the Prisma driver adapter for PlanetScale (`@prisma/adapter-planetscale`), PlanetScale serverless driver (`@planetscale/database`), and `undici` packages:

    ```shell  theme={null}
    npm install @prisma/adapter-planetscale @planetscale/database undici
    ```

    <Note>
      When using an older version of Node.js, you can provide a custom fetch function implementation. We recommend the `undici` package on which Node's built-in fetch is based. Node.js version 18 includes a built-in global `fetch` function.
    </Note>
  </Step>

  <Step>
    Enable the `driverAdapters` Preview feature flag:

    ```javascript  theme={null}
    // schema.prisma
    generator client {
      provider        = "prisma-client-js"
      previewFeatures = ["driverAdapters"]
    }

    datasource db {
      provider     = "mysql"
      url          = env("DATABASE_URL")
      relationMode = "prisma"
    }
    ```

    <Note>
      Ensure you update the host value in your connection string to `aws.connect.psdb.cloud`. You can learn more about this [here](/docs/vitess/tutorials/planetscale-serverless-driver#add-and-use-the-planetscale-serverless-driver-for-javascript-to-your-project).
    </Note>
  </Step>

  <Step>
    Generate Prisma Client:

    ```shell  theme={null}
    npx prisma generate
    ```
  </Step>

  <Step>
    Update your Prisma Client instance to use the PlanetScale serverless driver:

    ```javascript expandable theme={null}
    import { Client } from '@planetscale/database'
    import { PrismaPlanetScale } from '@prisma/adapter-planetscale'
    import { PrismaClient } from '@prisma/client'
    import dotenv from 'dotenv'
    import { fetch as undiciFetch } from 'undici'

    dotenv.config()
    const connectionString = `${process.env.DATABASE_URL}`

    const client = new Client({ url: connectionString, fetch: undiciFetch })
    const adapter = new PrismaPlanetScale(client)
    const prisma = new PrismaClient({ adapter })

    async function main() {
      const posts = await prisma.post.findMany()
      console.log(posts)
    }
    ```
  </Step>
</Steps>

You can then use Prisma Client as you usually would with auto-completion and full type-safety.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Prisma best practices
Source: https://planetscale.com/docs/vitess/tutorials/prisma-best-practices

This document provides various best practices for getting the most out of Prisma, a next-generation ORM for Node.js and TypeScript, and PlanetScale. It also includes relevant links to Prisma's documentation.

## Referential actions and integrity with Prisma and PlanetScale

When using Prisma with PlanetScale, you need to make sure to set `relationMode` to `prisma` in your Prisma schema:

```js  theme={null}
datasource db {
  provider     = "mysql"
  url          = env("DATABASE_URL")
  relationMode = "prisma"
}
```

<Note>
  In Prisma `4.5.0`, `referentialIntegrity` changed to `relationMode` and generally became available in `4.7.0`.
</Note>

The `prisma` relation mode emulates some foreign key constraints and referential actions for each Prisma Client query to maintain referential integrity, using some additional database queries and logic.

Read more about [Relation mode in Prisma's documentation](https://www.prisma.io/docs/orm/prisma-schema/data-model/relations/relation-mode).

### Creating an index for relation scalar fields

When a Prisma client uses the `foreignKeys` relation mode, which does not work with PlanetScale, the database implicitly creates an index for the foreign key columns. Therefore, it is recommended that you create an index for your relation scalar fields with the `@@index` attribute (or the `@unique`, `@@unique` or `@@id` attributes, if applicable) when using `prisma` relation mode with PlanetScale.

```js  theme={null}
model Post {
  id     Int  @id
  userId Int
  user   User @relation(fields: [userId], references: [id])

  @@index([userId])
}
```

If you do not add the index, you might notice that some of your queries are slow or are performing full table scans and reading a lot of data. When you run `prisma format` or `prisma validate`, it will warn you about a missing index where you are using foreign key constraints.

If you want to learn more about MySQL indexes, check out the [MySQL for Developers course section on indexes](https://planetscale.com/learn/courses/mysql-for-developers/indexes/introduction-to-indexes).

## Migration workflows using `prisma db push`

With Prisma, there are two ways to apply schema changes to your database: `prisma migrate` and `prisma db push`. We recommend `prisma db push` over `prisma migrate dev` for the following reasons:

PlanetScale automatically provides built-in [Online Schema Change tools](/docs/vitess/schema-changes/how-online-schema-change-tools-work) when you merge a deploy request and prevents [blocking schema changes](/docs/vitess/schema-changes) that can lead to downtime. This differs from the typical Prisma workflow, which uses `prisma migrate` to generate SQL migrations for you based on changes in your Prisma schema. When using PlanetScale with Prisma, the responsibility of applying the changes is on the PlanetScale side. Therefore, there is little value to using `prisma migrate` with PlanetScale.

Also, the migrations table created when `prisma migrate` runs can be misleading since PlanetScale does the actual migration when the deploy request is merged, not when `prisma migrate` is run, which only updates the schema in the development database branch. You can still see the history of your schema changes in PlanetScale.

If you want to read more about `prisma db push`, see the [Prisma documentation on making schema changes with `prisma db push`](https://www.prisma.io/docs/orm/overview/databases/planetscale#how-to-make-schema-changes-with-db-push).

## Connection management

When using Prisma with PlanetScale, you might encounter some specific error messages. It might seem like your PlanetScale database is down, but there are other reasons why it might appear this way.

For both errors, if your serverless function or application servers and database are not in the same region, this can contribute to latency problems. If possible, try to move them closer together to decrease physical latency.

Here are some of the common error messages with their possible causes and solutions:

### Prisma error P1001

```bash  theme={null}
"Can't reach database server at {database_host}:{database_port} Please make sure your database server is running at {database_host}:{database_port}."
```

**Possible cause:**

* The Prisma Client did not establish the connection within the `connect_timeout`. There are many possible reasons for this to occur, two of the common examples we see are DNS resolution issues and network latency.

**Possible solutions:**

* Increase the `connect_timeout` in your `DATABASE_URL`.

Example `DATABASE_URL`: `mysql://USER:PASSWORD@HOST:PORT/DATABASE?connect_timeout=30`

The default is `5`. The `connect_timeout` is the maximum number of seconds to wait for a new connection to be opened, `0` means no timeout. We suggest trying a higher number around `30` seconds if you have this issue. See the [Prisma connection URL argument documentation](https://www.prisma.io/docs/orm/overview/databases/mysql#arguments) for more information.

### Prisma error P2024

```
"Timed out fetching a new connection from the connection pool. (More info: http://pris.ly/d/connection-pool (Current connection pool timeout: {timeout}, connection limit: {connection_limit})"
```

**Possible cause:**

* The connection is established, but the Prisma query engine is not able to process a query in the queue before the time limit.

**Possible solutions:**

* Increase the pool size by increasing the `connection_limit`.

Example `DATABASE_URL`: `mysql://USER:PASSWORD@HOST:PORT/DATABASE?connection_limit=10`

The default is `num_cpus * 2 + 1`. The `connection_limit` is the maximum size of the connection pool for each instance of the Prisma Client. `connection_limit` is not the total for all of your application servers or serverless functions. While PlanetScale can handle hundreds of thousands of connections at a time, we recommend incrementally increasing this number to tune your Prisma Client. See the [Prisma documentation on optimizing the connection pool size](https://www.prisma.io/docs/orm/prisma-client/setup-and-configuration/databases-connections#optimizing-the-connection-pool) for more info.

* Increase the `pool_timeout`.

Example `DATABASE_URL`: `mysql://USER:PASSWORD@HOST:PORT/DATABASE?pool_timeout=30`

The default is `10`. The `pool_timeout` is the maximum number of seconds to wait for a new connection from the pool, `0` means no timeout. We recommend increasing this only after you've tuned the `connection_limit`. See the [Prisma documentation on optimizing the connection pool timeout](https://www.prisma.io/docs/orm/prisma-client/setup-and-configuration/databases-connections#optimizing-the-connection-pool) for more info.

* Decrease query duration, which might include asking for less data in the query or improving performance with an index or other methods. Start by looking at your [Insights](/docs/vitess/monitoring/query-insights) page within your PlanetScale database to see which queries might be performing poorly.

Lastly, this [discussion topic response on Prisma connection pooling](https://github.com/planetscale/discussion/discussions/188#discussioncomment-3808093) can be helpful if you are experiencing these errors.

## Other resources:

<Columns cols={2}>
  <Card title="Using PlanetScale with Prisma" icon="file-lines" horizontal href="/docs/vitess/tutorials/using-planetscale-with-prisma" />

  <Card title="Prisma quickstart for adding Prisma to an existing project" icon="rocket" horizontal href="https://www.prisma.io/docs/getting-started/setup-prisma/add-to-existing-project/relational-databases-typescript-planetscale" />

  <Card title="Prisma document on connection management" icon="gear" horizontal href="https://www.prisma.io/docs/orm/prisma-client/setup-and-configuration/databases-connections/connection-management" />

  <Card title="Prisma document on how the connection pool works in Prisma" icon="code-merge" horizontal href="https://www.prisma.io/docs/orm/prisma-client/setup-and-configuration/databases-connections/connection-pool#how-the-connection-pool-works" />

  <Card title="Video: Prisma & PlanetScale Best Practices" icon="video" horizontal href="https://youtu.be/iaHt5_hg44c" />
</Columns>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Using the PlanetScale serverless driver with Prisma
Source: https://planetscale.com/docs/vitess/tutorials/prisma-example

This document outlines how you can use the [PlanetScale serverless driver](/docs/vitess/tutorials/planetscale-serverless-driver) along with Prisma in your application.

## Set up

To get started:

<Steps>
  <Step>
    Install the Prisma driver adapter for PlanetScale (`@prisma/adapter-planetscale`), PlanetScale serverless driver (`@planetscale/database`), and `undici` packages:

    ```shell  theme={null}
    npm install @prisma/adapter-planetscale @planetscale/database undici
    ```

    <Note>
      When using an older version of Node.js, you can provide a custom fetch function implementation. We recommend the `undici` package on which Node's built-in fetch is based. Node.js version 18 includes a built-in global `fetch` function.
    </Note>
  </Step>

  <Step>
    Enable the `driverAdapters` Preview feature flag:

    ```javascript  theme={null}
    // schema.prisma
    generator client {
      provider        = "prisma-client-js"
      previewFeatures = ["driverAdapters"]
    }

    datasource db {
      provider     = "mysql"
      url          = env("DATABASE_URL")
      relationMode = "prisma"
    }
    ```

    <Note>
      Ensure you update the host value in your connection string to `aws.connect.psdb.cloud`. You can learn more about this [here](/docs/vitess/tutorials/planetscale-serverless-driver#add-and-use-the-planetscale-serverless-driver-for-javascript-to-your-project).
    </Note>
  </Step>

  <Step>
    Generate Prisma Client:

    ```shell  theme={null}
    npx prisma generate
    ```
  </Step>

  <Step>
    Update your Prisma Client instance to use the PlanetScale serverless driver:

    ```javascript expandable theme={null}
    import { Client } from '@planetscale/database'
    import { PrismaPlanetScale } from '@prisma/adapter-planetscale'
    import { PrismaClient } from '@prisma/client'
    import dotenv from 'dotenv'
    import { fetch as undiciFetch } from 'undici'

    dotenv.config()
    const connectionString = `${process.env.DATABASE_URL}`

    const client = new Client({ url: connectionString, fetch: undiciFetch })
    const adapter = new PrismaPlanetScale(client)
    const prisma = new PrismaClient({ adapter })

    async function main() {
      const posts = await prisma.post.findMany()
      console.log(posts)
    }
    ```
  </Step>
</Steps>

You can then use Prisma Client as you usually would with auto-completion and full type-safety.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Prometheus
Source: https://planetscale.com/docs/vitess/tutorials/prometheus

PlanetScale exposes Prometheus-compatible metrics endpoints for scraping metrics about your database branches. This, along with our API-driven service discovery, allow you to automatically get in-depth information about all of the databases in your organization.

In order to collect and store these, you will need to use Prometheus or a Prometheus-compatible metrics engine (such as VictoriaMetrics) that is capable of using the [HTTP SD](https://prometheus.io/docs/prometheus/latest/http_sd/) protocol.

## Prerequisites

This document assumes we'll be configuring a Prometheus 3.x instance via a configuration file running on our local machine.

If you are using managed Prometheus via AWS, GCP or another provider, you will have to deploy Prometheus to scrape and forward metrics via `remote_write`, as these services do not support scraping metrics.

## Getting Started

First, provision a new PlanetScale [Service token](/docs/api/reference/service-tokens) in your Organization settings. Make sure to save the ID and token, as they will not be visible after they've been generated.

When that's created, grant the token `read_metrics_endpoints` permissions and click "Save permissions". Your token should look like the following:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-service-token-configuration.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=6788de4f7c33b53bae655708c4baff9f" alt="Service Token configuration for Metrics Exporting" data-og-width="1348" width="1348" data-og-height="1136" height="1136" data-path="docs/images/metrics-service-token-configuration.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-service-token-configuration.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=64c0aa40bda297dd71fcdc604c7593c8 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-service-token-configuration.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=0f82feb485f8046096d6dc4c9fde8cde 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-service-token-configuration.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=a507b57eef83b96e43ef16ad9c29afcd 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-service-token-configuration.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=2f04f5ebb7f7f148dc809654a8b192da 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-service-token-configuration.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=aa009d73fa64ae6d3fd60d37ba0bd94c 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-service-token-configuration.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=45f8237641306c452d6e3293fec320c1 2500w" />
</Frame>

## Configuring Prometheus

Now that we have a Service Token, we can add a scrape configuration for your PlanetScale organization. A minimal Prometheus configuration should look like the following:

```yaml  theme={null}
scrape_configs:
  - job_name: "${ORG}"
    http_sd_configs:
    - url: https://api.planetscale.com/v1/organizations/${ORG}/metrics
      authorization:
        type: "token"
        credentials: "${TOKEN_ID}:${TOKEN}"
      refresh_interval: 10m
```

Fill in your organization name in the `job_name` and `url`, and place the Service Token and ID that we created in the previous step for the credentials.

Save this file to `prometheus.yml` in your working directory.

## Start Prometheus

Run Prometheus pointed at this configuration file:

```bash  theme={null}
$ prometheus --config.file=prometheus.yml
```

By default, Prometheus will listen at `0.0.0.0:9090`, which means you can access it in your browser at [http://127.0.0.1:9090](http://127.0.0.1:9090).

### Validating Service Discovery

First, let's make sure that Prometheus is properly querying the PlanetScale API for the right branches. If you go to `http://127.0.0.1:9090/service-discovery` you should see the job that we created earlier, with all of your branches listed under `Discovered labels`. In this example, our organization is called `nick`, so it looks like the following:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-targets.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=eb5b15d46086cd686d2e3cdc27a81655" alt="Prometheus Target List" data-og-width="2886" width="2886" data-og-height="1456" height="1456" data-path="docs/images/metrics-prometheus-targets.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-targets.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=e6cb52a5d4cfccd6692995ee06e6c17d 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-targets.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=91f2153cfefce2e2517e04e47459e658 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-targets.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=4d22ffa22fa8569ff0fae3be6c848ab0 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-targets.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=4e43bea221193dd4858ea72417cdd4b0 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-targets.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=3ea68d9b55697ad22d7dd93c4aa58190 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-targets.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=0fa878b0e86c25df0a3b99ea55d34ba4 2500w" />
</Frame>

Here, I have two branches that have been discovered. I can confirm that this matches what's in my organization:

```bash  theme={null}
$ pscale branch list test --org nick
  ID             NAME         PARENT BRANCH   REGION    PRODUCTION   SAFE MIGRATIONS   READY   CREATED AT    UPDATED AT
 -------------- ------------ --------------- --------- ------------ ----------------- ------- ------------- ---------------
  7wxuxewx4l0p   main         n/a             us-east   Yes          Yes               Yes     2 years ago   7 minutes ago
  6o0rr27785fl   partitions   main            us-east   No           No                Yes     1 month ago   9 minutes ago
```

Now, if I go to my list of targets I should see each branch as an Endpoint:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-endpoints.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=e0c332509b19a9bb2ed2b63f14e1020d" alt="Prometheus Endpoint List" data-og-width="2384" width="2384" data-og-height="660" height="660" data-path="docs/images/metrics-prometheus-endpoints.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-endpoints.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=ecde7c2e1f790d7eb6730781b3b07224 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-endpoints.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=c5886bea0a0f432f7ac971d6e1e8de2b 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-endpoints.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=408a725b71e31d934868b64a2919a8c7 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-endpoints.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=88a517da96ac66ad8f2662d9dd1fb502 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-endpoints.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=cd421d5b722f15fa176793ec2f0143b9 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-endpoints.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=a0717a3d71c7bd84bafa55f801912964 2500w" />
</Frame>

This screenshot shows that they're being correctly scraped, and I can start to query my Prometheus instance.

## Querying Prometheus

Now that we're collecting metrics for my branches, our [reference guide](/docs/vitess/integrations/prometheus-metrics) has a list of everything that we export. If I want to see how many `vtgate` pods are running per AZ for my branch, I can query:

```
planetscale_vtgate_total_pods{planetscale_database_branch_id="7wxuxewx4l0p"}
```

Make sure the graph is set to stacked, and it should look like this:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-querying.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=bd02102ef5e2bbc4893da279353807cb" alt="Querying Prometheus for VTGate Count" data-og-width="2710" width="2710" data-og-height="2422" height="2422" data-path="docs/images/metrics-prometheus-querying.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-querying.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=150f556daa4de597267520394eadf38e 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-querying.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=f5e4451baa8d463ad094d56ca63e45bf 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-querying.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=532753037cda6f5f7dcc62ffe07e456e 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-querying.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=e69c7becc71cd28b7403b518ad44df0d 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-querying.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=0be7dd37cb8970f9c3d16cef92f8ae77 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-querying.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=ec9705a66b171b308f34f32c008836ff 2500w" />
</Frame>

## Next Steps

If you keep this Prometheus instance running, it will collect metrics every 30 seconds, and refresh the list of branches every 10 minutes.

For more information, see:

* [Metrics reference](/docs/vitess/integrations/prometheus-metrics) for a list of metrics we expose
* [Grafana and Prometheus](/docs/vitess/tutorials/prometheus-metrics-grafana) tutorial for using PlanetScale's provided dashboard to visualize these metrics in Grafana.
* [Sending metrics to New Relic](/docs/vitess/tutorials/prometheus-metrics-newrelic) tutorial for using Prometheus to forward metrics to New Relic.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Sending Prometheus Metrics to Datadog
Source: https://planetscale.com/docs/vitess/tutorials/prometheus-metrics-datadog

If you're looking for more metrics than PlanetScale's native Datadog integration provides, this tutorial will show how to configure your [Datadog agent](https://docs.datadoghq.com/agent/) to scrape PlanetScale's [Prometheus infrastructure](/docs/vitess/integrations/prometheus) automatically, allowing you to collect detailed metrics for all of your PlanetScale branches.

## Overview

In this tutorial, we'll assume that you have a Datadog Agent Version 7 running. For more information on what the Datadog Agent is and how to install it, start with the [Datadog Agent documentation](https://docs.datadoghq.com/getting_started/agent/).

For the purposes of this guide, we'll be using a Datadog agent running with the recommended installation steps on a Linux system.

## Prerequisites

You'll need a working Datadog agent and access to add a [Custom Agent Check](https://docs.datadoghq.com/developers/custom_checks/) to that instance. This may require `root` or `sudo` access on the machine running the Datadog agent.

You'll also need a [Service token](/docs/api/reference/service-tokens) in your Organization, with the `read_metrics_endpoints` permission granted.

## Adding the Plugin to the Datadog Agent

Go to [https://github.com/planetscale/planetscale-datadog](https://github.com/planetscale/planetscale-datadog), which is the repository that has our custom OpenMetrics Check.

Place the unedited `planetscale.py` in the `checks.d` directory of your Datadog Agent.

* On Linux, that is `/etc/datadog-agent/checks.d/`
* On macOS, that is `/opt/datadog-agent/etc/checks.d/`

Make sure that it belongs to the appropriate user. If you're using the recommended Linux installation steps, it will have created a `dd-agent` user:

```
$ pwd
/etc/datadog-agent/checks.d
$ ls -al planetscale.py
-rw-r--r-- 1 dd-agent dd-agent 9261 Apr  2 22:54 planetscale.py
```

This file is owned by the `dd-agent` user and group in the `/etc/datadog-agent/checks.d` directory.

If you're on macOS, it will depend on whether you installed the agent as a 'Single User Agent' or a 'Systemwide Agent'. If you picked Single User, there should be no additional permission changes needed. If you installed it as a Systemwide agent, make sure the user and group you installed the agent with as ownership of the file.

## Configuring the Datadog Agent

Now that we have the plugin installed, we need to configure it. In the `conf.d` directory of the Datadog agent take the `conf.d/planetscale.yaml.example` file and edit it with your organization name and Service Token information. It should look like this:

```bash expandable theme={null}
instances:
  - planetscale_organization: 'nick' # Required: Your PlanetScale organization ID
    ps_service_token_id: '${TOKEN_ID}' # Required: Your PlanetScale Service Token ID
    ps_service_token_secret: '${TOKEN}' # Required: Your PlanetScale Service Token Secret. Consider using Datadog secrets management: https://docs.datadoghq.com/agent/guide/secrets-management/

    namespace: 'planetscale' # Required: Namespace for the metrics
    metrics: # Required: List of metrics to collect. Use mapping for renaming/type overrides.
      - planetscale_vtgate_queries_duration: vtgate_query_duration

    min_collection_interval: 60
    send_distribution_buckets: true
    collect_counters_with_distributions: true
```

This configures the integration to look for all of the branches in the `"nick"` PlanetScale organization, only collect the `planetscale_vtgate_queries_duration` metric, which it will rename `vtgate_query_duration` and put it inside of the `planetscale` namespace.

Save the file at `planetscale.yaml`, making sure to double check permissions:

```
$ pwd
/etc/datadog-agent/conf.d
$ ls -al planetscale.yaml
-rw-r--r-- 1 root root 1518 Apr  2 22:57 planetscale.yaml
```

## Restart the Datadog Agent

Now that this is configured and installed, restart the Agent:

```
$ sudo systemctl restart datadog-agent
```

## Validating the PlanetScale Plugin

Now that the Datadog Agent is running the PlanetScale plugin, metrics should start flowing into Datadog within a couple of minutes. To validate, we can ask the Datadog Agent:

```
sudo -u dd-agent -- datadog-agent check planetscale
```

If the plugin is installed successfuly, this should output the scrape targets for your branches, as well as metadata about when it was last run and how many metrics were emitted:

```bash expandable theme={null}
$ sudo -u dd-agent -- datadog-agent check planetscale
=== Service Checks ===
[
  {
    "check": "planetscale.api.can_connect",
    "host_name": "ubuntu",
    "timestamp": 1743638192,
    "status": 0,
    "message": "",
    "tags": [
      "planetscale_org:nick"
    ]
  },
  {
    "check": "planetscale.prometheus.health",
    "host_name": "ubuntu",
    "timestamp": 1743638192,
    "status": 0,
    "message": "",
    "tags": [
      "endpoint:https://metrics.psdb.cloud/metrics/branch/7wxuxewx4l0p?..."
    ]
  },
  {
    "check": "planetscale.prometheus.health",
    "host_name": "ubuntu",
    "timestamp": 1743638192,
    "status": 0,
    "message": "",
    "tags": [
      "endpoint:https://metrics.psdb.cloud/metrics/branch/6o0rr27785fl?..."
    ]
  }
]


  Running Checks
  ==============

    planetscale (unversioned)
    -------------------------
      Instance ID: planetscale:planetscale:8d4d64f696d967be [OK]
      Configuration Source: file:/etc/datadog-agent/conf.d/planetscale.yaml
      Total Runs: 1
      Metric Samples: Last Run: 0, Total: 0
      Events: Last Run: 0, Total: 0
      Service Checks: Last Run: 3, Total: 3
      Histogram Buckets: Last Run: 77, Total: 77
      Average Execution Time : 809ms
      Last Execution Date : 2025-04-02 23:56:32 UTC (1743638192000)
      Last Successful Execution Date : 2025-04-02 23:56:32 UTC (1743638192000)


  Metadata
  ========
    config.hash: planetscale:planetscale:8d4d64f696d967be
    config.provider: file
```

The Service Checks show that it has successfully connected to the PlanetScale API to request information about how to scrape for the branches in my organization, and it has successfully scraped both of what it discovered.

We can also see that it successfully executed at `2025-04-02 23:56:32 UTC` and produced 77 Histogram Buckets.

## Adding Metrics

In our earlier configuration, we only added one metric. For a complete list of what PlanetScale exposes, please take a look at our [Metrics Reference Documentation](/docs/vitess/integrations/prometheus-metrics).

Note that the Datadog agent [normalizes metrics with certain suffixes starting in v7.32.0](https://github.com/DataDog/integrations-core/blob/master/openmetrics/README.md):

> Starting in Datadog Agent v7.32.0, in adherence to the OpenMetrics specification standard, counter names ending in \_total must be specified without the \_total suffix. For example, to collect promhttp\_metric\_handler\_requests\_total, specify the metric name promhttp\_metric\_handler\_requests. This submits to Datadog the metric name appended with .count, promhttp\_metric\_handler\_requests.count.

This means that to scrape a metric such as `planetscale_mysql_bytes_received_total`, you would configure the Datadog agent for `planetscale_mysql_bytes_received`.

If I want to collect additional metrics, I can add them to the list:

```
    metrics: # Required: List of metrics to collect. Use mapping for renaming/type overrides.
      - planetscale_vtgate_queries_duration: vtgate_query_duration
      - planetscale_edge_active_connections: active_connections
```

Then, restart the Datadog Agent:

```
$ sudo systemctl restart datadog-agent
```

If I check the status of the PlanetScale Plugin, I can see our last run added a Metric Sample:

```bash  theme={null}
  Running Checks
  ==============

    planetscale (unversioned)
    -------------------------
      Instance ID: planetscale:planetscale:fde586b60a54a38f [OK]
      Configuration Source: file:/etc/datadog-agent/conf.d/planetscale.yaml
      Total Runs: 1
      Metric Samples: Last Run: 1, Total: 1
      Events: Last Run: 0, Total: 0
      Service Checks: Last Run: 3, Total: 3
      Histogram Buckets: Last Run: 77, Total: 77
      Average Execution Time : 826ms
      Last Execution Date : 2025-04-03 00:01:45 UTC (1743638505000)
      Last Successful Execution Date : 2025-04-03 00:01:45 UTC (1743638505000)
```

In the Datadog UI, I can see data for the `planetscale.active_connections` metric:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/prometheus-datadog-graph.png?fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=781999f47c7ed676a7f28ddbf354cc09" alt="Datadog Connections Metric" data-og-width="2770" width="2770" data-og-height="1206" height="1206" data-path="docs/images/prometheus-datadog-graph.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/prometheus-datadog-graph.png?w=280&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=1d8ae95b45ead1d2e94f625ecfb2837d 280w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/prometheus-datadog-graph.png?w=560&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=7bd483e4da6d5115ae865a55d415a476 560w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/prometheus-datadog-graph.png?w=840&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=01966f97ca700bff63d4b0d74f5b7db6 840w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/prometheus-datadog-graph.png?w=1100&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=2b2f08a497661ec60658b6b7f5524171 1100w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/prometheus-datadog-graph.png?w=1650&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=8e64b937f30dfbf9bea33c476df99140 1650w, https://mintcdn.com/planetscale-cad1a68a/bmpntbyCIjsL6T87/docs/images/prometheus-datadog-graph.png?w=2500&fit=max&auto=format&n=bmpntbyCIjsL6T87&q=85&s=5fad67405ddd88425c59fefb966fb952 2500w" />
</Frame>

## What's Next?

Now that you're sending a couple of metrics from PlanetScale to Datadog, take a look at our [full list](/docs/vitess/integrations/prometheus-metrics) and start building dashboards!

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Grafana Dashboard for PlanetScale Branches
Source: https://planetscale.com/docs/vitess/tutorials/prometheus-metrics-grafana

In this tutorial, you'll learn how to set up Grafana and connect it to a Prometheus instance to see metrics about your PlanetScale database.

## Introduction

This guide requires that you've set up a Prometheus instance from our documentation.

If you're already running Grafana in production and you're just looking for our standard dashboard template, you can find it [on GitHub](https://github.com/planetscale/grafana-dashboard).

## Install Grafana

Grafana's [installation documentation](https://grafana.com/docs/grafana/latest/setup-grafana/installation/) contains information for their supported platforms. For this guide, we'll be setting this up locally on a macOS machine.

If you're using a hosted Grafana option such as [Grafana Cloud](https://grafana.com/products/cloud/) or [AWS Managed Grafana](https://aws.amazon.com/grafana/) you can skip this step.

On macOS, Grafana is availabile via [homebrew](https://brew.sh/), and I can install it with:

```bash  theme={null}
$ brew install grafana
```

This will download and install Grafana, and I can start it with:

```bash  theme={null}
$ brew services start grafana
```

When that succeeds, I can go to `http://localhost:3000/` and I should see the Grafana welcome page:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-grafana-welcome.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=08bc56eebe2e6eddedc68118017e2604" alt="Grafana Welcome Page" data-og-width="3008" width="3008" data-og-height="2326" height="2326" data-path="docs/images/metrics-grafana-welcome.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-grafana-welcome.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=f0da2994135c10ee2b6ff0c29a5acb4a 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-grafana-welcome.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=ac21de7565a035fac9555c3bd28c6dac 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-grafana-welcome.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=087b5a456c38a3335459f5bf889d80f4 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-grafana-welcome.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=76e3b99231f26e8c91b86e57fc682aa9 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-grafana-welcome.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=cf6e5e907b95280bfa35bf80f6388d46 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-grafana-welcome.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=5cc12cd28d2fc162c0cb0a30fcd650d3 2500w" />
</Frame>

The default username and password for a new install is `admin` and `admin`. Grafana will ask you to change the password the first time you log in, please pick something more secure than `admin`.

### Adding a Prometheus Endpoint

You can skip this step as well if you're already running a managed Prometheus or have added your datasource to Grafana already.

If you're running Prometheus locally, you'll need to add that as a datasource. To do this:

<Steps>
  <Step>
    Open the menu in the top left and click "Connections"
  </Step>

  <Step>
    Search for "Prometheus" and pick the plain "Prometheus" option
  </Step>

  <Step>
    Click "Add new data source" in the top right of the page
  </Step>
</Steps>

Now, you should look see a page that looks like this:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-add-prometheus-connection.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=3e2ccdd3e5dfbe91bdbd9789ac3656d9" alt="Grafana Add Datasource" data-og-width="3024" width="3024" data-og-height="3010" height="3010" data-path="docs/images/metrics-add-prometheus-connection.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-add-prometheus-connection.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=28bb2ebb10caa61b3e3398749ff351b5 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-add-prometheus-connection.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=921f003dc70f3a780e9dc0a39808c56f 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-add-prometheus-connection.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=2a4e71d2bf72a51915bbb22d38dfcd4c 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-add-prometheus-connection.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=43a79211e0e110aa4fa88ea2e77cf3ad 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-add-prometheus-connection.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=df7e792607ac28d2602a3d76bd7a13e0 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-add-prometheus-connection.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=0b56a962d616ff3e870f9bd87710f512 2500w" />
</Frame>

You can call this whatever you want, we'll use the following:

* Name: "PlanetScale"
* Prometheus server URL: `http://localhost:9090/`

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-configuration.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=b9bbba72a0a2ed20cfcfd5f25c34d232" alt="Grafana Prometheus Configuration" data-og-width="2936" width="2936" data-og-height="2922" height="2922" data-path="docs/images/metrics-prometheus-configuration.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-configuration.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=c6ede1957735b8d9697df728ebb2da40 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-configuration.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=a319774363f508a784410f6ce3945231 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-configuration.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=cb966b69267429b143b6cfd2dcc65ad4 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-configuration.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=e761dcaa6df2b51b38ee1982e528cb53 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-configuration.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=9b9e55cc23ea2e10dcec729a76dd4eb4 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-configuration.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=9f8a444053006ebf3c282b78be8bfcd8 2500w" />
</Frame>

Because this is running on your local machine, we do not need to use any Authentication or TLS

Scroll down to the "Interval behaviour" section and set the "Scrape interval" to `1m`.

Finally, scroll to the bottom and click "Save & test".

## Import the PlanetScale Dashboard

Now that we have our datasource added, let's import the PlanetScale Dashboard. This is a starter dashboard that PlanetScale has produced which shows an overview of your branch with the metrics that we expose.

From the Grafana homepage, go to the top left menu and pick "Dashboards".

In the top right, click "New" and then Import":

PlanetScale maintains the latest version of the dashboard located here:

[https://github.com/planetscale/grafana-dashboard/blob/main/overview.json](https://github.com/planetscale/grafana-dashboard/blob/main/overview.json)

Download this file to your computer, and then click "Upload dashboard JSON file".

Find the JSON file you downloaded in the previous step, and configure it with the Prometheus datasource that we added in an earlier:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-configuration.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=b9bbba72a0a2ed20cfcfd5f25c34d232" alt="Grafana Prometheus Configuration" data-og-width="2936" width="2936" data-og-height="2922" height="2922" data-path="docs/images/metrics-prometheus-configuration.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-configuration.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=c6ede1957735b8d9697df728ebb2da40 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-configuration.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=a319774363f508a784410f6ce3945231 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-configuration.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=cb966b69267429b143b6cfd2dcc65ad4 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-configuration.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=e761dcaa6df2b51b38ee1982e528cb53 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-configuration.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=9b9e55cc23ea2e10dcec729a76dd4eb4 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-prometheus-configuration.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=9f8a444053006ebf3c282b78be8bfcd8 2500w" />
</Frame>

Click 'Import' and you should be directed to the dashboard, configured to query your local Prometheus with the data it's been scraping!

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Sending Prometheus Metrics to New Relic
Source: https://planetscale.com/docs/vitess/tutorials/prometheus-metrics-newrelic

If you're looking for your PlanetScale database metrics in your New Relic account, this tutorial will show how to configure a [Prometheus](https://prometheus.io/) instance to scrape PlanetScale's [Prometheus infrastructure](/docs/vitess/integrations/prometheus) automatically, allowing you to collect detailed metrics for all of your PlanetScale branches.

While this tutorial is written for New Relic, using Prometheus' remote write is a common pattern for sending metrics to [AWS Managed Prometheus](https://aws.amazon.com/prometheus/), [Google Cloud Managed Service for Prometheus](https://cloud.google.com/stackdriver/docs/managed-prometheus), [Grafana hosted Prometheus](https://grafana.com/products/cloud/metrics/) and many other tools.

For more information on Prometheus Remote Write and New Relic, see the [New Relic documentation on sending Prometheus metric data](https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/).

## Overview

In this tutorial, we will be using an instance of [Prometheus](https://prometheus.io/) running on a Linux VM to scrape metrics from PlanetScale and then forward them to New Relic using [Remote Write](https://prometheus.io/docs/specs/prw/remote_write_spec/). We will make sure that Prometheus stays running by creating a [Systemd Unit File](https://www.digitalocean.com/community/tutorials/understanding-systemd-units-and-unit-files).

The default configuration we will create will send all PlanetScale metrics to New Relic, and we will cover how to filter to drop certain metrics that may not be desired.

In order to proceed, you'll need:

* A [New Relic API Key](https://docs.newrelic.com/docs/apis/intro-apis/new-relic-api-keys), make sure it is the `Ingest - License` type.
* PlanetScale [Service token](/docs/api/reference/service-tokens) with `read_metrics_endpoints` permissions.

## Prometheus Installation

First, let's download the latest release of Prometheus and create our user that is going to run it. We'll be using the latest 3.x release from the [GitHub Releases Page](https://github.com/prometheus/prometheus/releases).

Create a `prometheus` user:

```bash  theme={null}
$ sudo useradd -M -U prometheus
```

```bash  theme={null}
$ wget https://github.com/prometheus/prometheus/releases/download/v3.2.1/prometheus-3.2.1.linux-amd64.tar.gz
$ tar xf prometheus-3.2.1.linux-amd64.tar.gz
$ sudo mv prometheus-3.2.1.linux-amd64/ /opt/prometheus
$ sudo chown prometheus:prometheus -R /opt/prometheus
```

This has put the Prometheus binary in `/opt/prometheus` along with the example configuration file that we can use.

## Create our Systemd Unit File

Now that we have the binary in place, let's setup Systemd to run Prometheus by creating a Unit File in `/etc/systemd/system/prometheus.service` with the following contents:

```ini expandable theme={null}
[Unit]
Description=Prometheus Agent
Documentation=https://prometheus.io/
After=network-online.target

[Service]
User=prometheus
Group=prometheus
Restart=on-failure
ExecStart=/opt/prometheus/prometheus \
  --config.file=/opt/prometheus/prometheus.yml \
  --storage.agent.path=/opt/prometheus/data \
  --web.listen-address=0.0.0.0:9091 \
  --agent

[Install]
WantedBy=multi-user.target
```

## Configure Prometheus

Now that we've got Prometheus installed and a unit file present, let's configure Prometheus. We will be borrowing some of our configuration from the [Prometheus Guide](/docs/vitess/integrations/prometheus), and adding some New Relic specific configuration. Edit `/opt/prometheus/prometheus.yml` in your editor of choice so that it contains this, making sure to replace your org name, service token information, and New Relic API key:

```yaml  theme={null}
global:
  scrape_interval: 1m
scrape_configs:
  - job_name: "${ORG}"
    http_sd_configs:
    - url: https://api.planetscale.com/v1/organizations/${ORG}/metrics
      authorization:
        type: "token"
        credentials: "${TOKEN_ID}:${TOKEN}"
      refresh_interval: 10m
remote_write:
  - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=planetscale
    authorization:
      credentials: ${NEW_RELIC_API_TOKEN}
```

This configuration file does the following:

* Configures Prometheus to discover scraping endpoints from the PlanetScale API using a service token
* Points Prometheus to write the metrics it scrapes from PlanetScale to the New Relic API

## Starting Prometheus

Now that we have a Systemd unit file and a configured Prometheus, let's run it!

```bash  theme={null}
$ sudo systemctl daemon-reload
$ sudo systemctl start prometheus.service
```

We can also tell Systemd to run Prometheus when my VM boots:

```bash  theme={null}
$ sudo systemctl enable prometheus.service
```

Now, let's check to make sure everything is running properly:

```bash expandable theme={null}
$ sudo systemctl status prometheus.service
● prometheus.service - Prometheus Agent
     Loaded: loaded (/etc/systemd/system/prometheus.service; enabled; preset: enabled)
     Active: active (running) since Fri 2025-04-04 17:36:57 UTC; 38s ago
       Docs: https://prometheus.io/
   Main PID: 745542 (prometheus)
      Tasks: 9 (limit: 9486)
     Memory: 21.2M (peak: 21.9M)
        CPU: 264ms
     CGroup: /system.slice/prometheus.service
             └─745542 /opt/prometheus/prometheus --config.file=/opt/prometheus/prometheus.yml --storage.agent.path=/opt/prometheus/data --web.listen-address=0.0.0.0:9091 --agent

Apr 04 17:36:57 ubuntu prometheus[745542]: time=2025-04-04T17:36:57.804Z level=INFO source=main.go:1305 msg=EXT4_SUPER_MAGIC
Apr 04 17:36:57 ubuntu prometheus[745542]: time=2025-04-04T17:36:57.804Z level=INFO source=main.go:1308 msg="Agent WAL storage started"
Apr 04 17:36:57 ubuntu prometheus[745542]: time=2025-04-04T17:36:57.804Z level=INFO source=main.go:1437 msg="Loading configuration file" filename=/opt/prometheus/prometheus.yml
Apr 04 17:36:57 ubuntu prometheus[745542]: time=2025-04-04T17:36:57.805Z level=INFO source=watcher.go:225 msg="Starting WAL watcher" component=remote remote_name=fbe64a url="https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=planetscal>
Apr 04 17:36:57 ubuntu prometheus[745542]: time=2025-04-04T17:36:57.805Z level=INFO source=metadata_watcher.go:90 msg="Starting scraped metadata watcher" component=remote remote_name=fbe64a url="https://metric-api.newrelic.com/prometheus/v1/write?prometh>
Apr 04 17:36:57 ubuntu prometheus[745542]: time=2025-04-04T17:36:57.805Z level=INFO source=watcher.go:277 msg="Replaying WAL" component=remote remote_name=fbe64a url="https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=planetscale" queu>
Apr 04 17:36:57 ubuntu prometheus[745542]: time=2025-04-04T17:36:57.806Z level=INFO source=main.go:1476 msg="updated GOGC" old=100 new=75
Apr 04 17:36:57 ubuntu prometheus[745542]: time=2025-04-04T17:36:57.806Z level=INFO source=main.go:1486 msg="Completed loading of configuration file" db_storage=791ns remote_storage=610.171µs web_handler=897ns query_engine=301ns scrape=517.175µs scrape_s>
Apr 04 17:36:57 ubuntu prometheus[745542]: time=2025-04-04T17:36:57.806Z level=INFO source=main.go:1213 msg="Server is ready to receive web requests."
```

This reports that prometheus is `active (running)`, and I can see the logs showing that it started successfully. Great!

## Querying on New Relic

After a couple of minutes, head over to your New Relic dashboard and we can query for your database metrics. First, let's get a list of the database branches in the `nick` organization that I'm using to test:

```bash  theme={null}
$ pscale branch list test --org nick
  ID             NAME         PARENT BRANCH   REGION    PRODUCTION   SAFE MIGRATIONS   READY   CREATED AT     UPDATED AT
 -------------- ------------ --------------- --------- ------------ ----------------- ------- -------------- ----------------
  7wxuxewx4l0p   main         n/a             us-east   Yes          No                Yes     2 years ago    50 minutes ago
  6o0rr27785fl   partitions   main            us-east   No           No                Yes     2 months ago   7 minutes ago
```

For this, we'll use the `7wxuxewx4l0p` branch.

Using New Relic's NRQL, we can visualize the memory usage of my VTTablet instances with the following query:

```sql  theme={null}
FROM Metric SELECT average(planetscale_pods_cpu_util_percentages) WHERE planetscale_database_branch_id = '7wxuxewx4l0p' AND planetscale_component='vttablet' SINCE 30 minutes AGO TIMESERIES FACET planetscale_pod
```

Because my `main` branch is production, we will see the memory usage for my primary and both my replicas over the last 30 minutes:

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-new-relic-dashboard.png?fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=350baedb8e4bb0303c4c15d1be15105e" alt="New Relic Memory Query" data-og-width="2758" width="2758" data-og-height="1450" height="1450" data-path="docs/images/metrics-new-relic-dashboard.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-new-relic-dashboard.png?w=280&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=d8f24a0171ed8581e89f2f693adbbd35 280w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-new-relic-dashboard.png?w=560&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=b9c54e24aa73b1fe357bd0b2c372553d 560w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-new-relic-dashboard.png?w=840&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=bb280f54bebf3bc4cf03a491dc5309cf 840w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-new-relic-dashboard.png?w=1100&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=4163e89e0a1bbe0adc7654f1dddeed69 1100w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-new-relic-dashboard.png?w=1650&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=6ee6c8a59ae4432790cc5eb4eedbd6bc 1650w, https://mintcdn.com/planetscale-cad1a68a/iWRuaKlb_EL2Lj7R/docs/images/metrics-new-relic-dashboard.png?w=2500&fit=max&auto=format&n=iWRuaKlb_EL2Lj7R&q=85&s=dab70925dcd1d0fa7dbf1fcfb57f49ef 2500w" />
</Frame>

## Filtering Metrics

If you don't want to ingest every metric into New Relic, you can tell Prometheus to drop certain metrics. For more information, see the [New Relic Documentation](https://docs.newrelic.com/docs/infrastructure/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/#allow-deny).

If we adjust our Prometheus configuration that we have in `/opt/prometheus/prometheus.yml` we can instruct Prometheus to drop all metrics unless they match a certain naming convention:

```yaml  theme={null}
remote_write:
  - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=planetscale
    authorization:
      credentials: ${NEW_RELIC_API_TOKEN}
  - source_labels: [__name__]
    regex: "planetscale_pods_(.*)"
    action: keep
```

If you replace your `remote_write` block with what's above, Prometheus will only forward the timeseries that match the `planetscale_pods_*` name. For a full list of metrics, see our [Metric List](/docs/vitess/integrations/prometheus-metrics).

## What's Next?

Now that you have your branch metrics in New Relic, you can create dashboards and alerts for conditions such as high CPU or replication delay.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Using PlanetScale with Prisma
Source: https://planetscale.com/docs/vitess/tutorials/using-planetscale-with-prisma

[Prisma](https://www.prisma.io/) ORM provides type-safe database access through an intuitive API, eliminating the need to write SQL queries manually.

Prisma and PlanetScale together offer a powerful workflow. Prisma handles the object-relational mapping with full type safety and intuitive data modeling, while PlanetScale provides horizontal scaling, branching workflows, and zero-downtime schema changes, making it an ideal stack for building modern, scalable applications.

This guide covers how to use Prisma and PlanetScale together, including how to:

* Create a database with PlanetScale
* Integrate it with Prisma
* Use additional features like sharding and the serverless driver

If you don't already have an application set up with Prisma, but want to test how it works with PlanetScale, we recommend grabbing a sample application from [Prisma's examples repository](https://github.com/prisma/prisma-examples/tree/latest/orm).

## Set up your PlanetScale database

First, set up your PlanetScale database.

<Steps>
  <Step>
    Sign in to your PlanetScale account
  </Step>

  <Step>
    Click "Create a database"
  </Step>

  <Step>
    Give your database a name
  </Step>

  <Step>
    Select your preferred region
  </Step>

  <Step>
    Choose cluster and storage size
  </Step>

  <Step>
    Click "Create database"
  </Step>
</Steps>

Your database will deploy with an initial production branch, `main`.

### Create a password

After the database is created, you'll be prompted to generate credentials for it. You can come back to this later if needed.

<Steps>
  <Step>
    Give your password a name or leave the default
  </Step>

  <Step>
    Select a [password role](/docs/vitess/security/password-roles). We recommend `Admin` for your default password.
  </Step>

  <Step>
    Click "Create password"
  </Step>

  <Step>
    Select "Prisma" for "Select your language or framework"
  </Step>

  <Step>
    Do not navigate away from this page. The next section covers where to paste these credentials in your Prisma app. If you close without copying, you can regenerate new credentials.
  </Step>
</Steps>

## Connect to PlanetScale in your application

Next, add your database credentials to your Prisma application. If you are migrating an existing database to PlanetScale, you can test the PlanetScale/Prisma integration locally or in staging first. Once you're ready to migrate the database from your existing provider, refer to our no downtime [migration guides](/docs/vitess/imports/database-imports).

To connect PlanetScale to your Prisma application:

<Steps>
  <Step>
    Follow the steps in the above section if you navigated away from the page previously
  </Step>

  <Step>
    Update your `prisma/schema.prisma` file with the following:

    ```js  theme={null}
    datasource db {
      provider     = "mysql"
      url          = env("DATABASE_URL")
      relationMode = "prisma"
    }
    ```
  </Step>

  <Step>
    Update the `DATABASE_URL` value in your `.env` file with the value provided to you when you created a new password in the PlanetScale dashboard. It should look like this:

    ```
    DATABASE_URL='mysql://<USERNAME>:<PASSWORD>@aws.connect.psdb.cloud/<DATABASE>?sslaccept=strict'
    ```
  </Step>
</Steps>

## Foreign key constraints

PlanetScale does not enable foreign key constraints by default. If you are not using foreign key constraints to enforce referential integrity at the database level, integrating with Prisma will still work, but it requires a couple additional steps (detailed below).

If you do plan to use foreign key constraints, enable them on your PlanetScale database settings page.

<Steps>
  <Step>
    In your database dashboard, go to "Settings"
  </Step>

  <Step>
    Enable "Foreign key constraints"
  </Step>

  <Step>
    Save your changes
  </Step>
</Steps>

### Using Prisma without foreign key constraints

If you are not using foreign key constraints, you can use Prisma's [`relationMode`](https://www.prisma.io/docs/orm/prisma-schema/data-model/relations/relation-mode) to emulate relations.

You need to update `datasource db` in your `schema.prisma` file to include `relationMode = "prisma"`:

```
datasource db {
  provider     = "mysql"
  url          = env("DATABASE_URL")
  relationMode = "prisma"
}
```

When emulating relations, you also need to manually create dedicated indexes on foreign keys to avoid performance issues. For example, if you have `Post` and `Comment` tables, where the `Comment` table references a `post`, you need to add an index to your `Post` model:

```js  theme={null}
model Post {
  id       Int       @id @default(autoincrement())
  title    String
  content  String
  likes    Int       @default(0)
  comments Comment[]
}

model Comment {
  id      Int    @id @default(autoincrement())
  comment String
  postId  Int
  post    Post   @relation(fields: [postId], references: [id], onDelete: Cascade)

  @@index([postId]) // manually created index
}
```

You can learn more about how to do this in Prisma in their [Emulating relations documentation](https://www.prisma.io/docs/orm/overview/databases/planetscale#option-1-emulate-relations-in-prisma-client).

## Push your Prisma schema to PlanetScale

Push your schema to your PlanetScale branch with:

```bash  theme={null}
npx prisma db push
```

The recommended workflow with using Prisma alongside PlanetScale is to use `prisma db push` instead of `prisma migrate`. You can read more about [`prisma db push` here](https://www.prisma.io/docs/orm/reference/prisma-cli-reference#db-push).

Your PlanetScale database schema now matches the Prisma schema you configured in `prisma/schema.prisma`. To confirm this, go to your PlanetScale dashboard, click "Branches", and select the branch you generated credentials for. You should see your schema.

## Using the PlanetScale serverless driver with Prisma

The [PlanetScale serverless driver](/docs/vitess/tutorials/planetscale-serverless-driver) allows you to execute queries over HTTP. You can use it with Prisma ORM via the @prisma/adapter-planetscale driver adapter. This adapter is available in Preview from Prisma ORM versions 5.4.2 and later.

<Steps>
  <Step>
    Enable the `driverAdapters` Preview feature in your Prisma schema:

    ```js  theme={null}
    generator client {
      provider        = "prisma-client-js"
      previewFeatures = ["driverAdapters"]
    }
    ```
  </Step>

  <Step>
    Generate Prisma Client:

    ```bash  theme={null}
    npx prisma generate
    ```
  </Step>

  <Step>
    If you're not already using a direct connection string, update your host to `aws.connect.psdb.cloud` or `gcp.connect.psdb.cloud`, depending on your chosen region.
  </Step>

  <Step>
    Install the Prisma `adapter-planetscale` and PlanetScale serverless driver packages:

    ```bash  theme={null}
    npm install @prisma/adapter-planetscale @planetscale/database undici
    ```
  </Step>

  <Step>
    Configure your Prisma Client with the adapter:

    ```js  theme={null}
    import { PrismaPlanetScale } from '@prisma/adapter-planetscale'
    import { PrismaClient } from '@prisma/client'
    import dotenv from 'dotenv'
    import { fetch as undiciFetch } from 'undici'

    dotenv.config()
    const connectionString = `${process.env.DATABASE_URL}`

    const adapter = new PrismaPlanetScale({ url: connectionString, fetch: undiciFetch })
    const prisma = new PrismaClient({ adapter })
    ```
  </Step>
</Steps>

For more information, see the [Prisma documentation](https://www.prisma.io/docs/orm/overview/databases/planetscale#how-to-use-the-planetscale-serverless-driver-with-prisma-orm-preview).

## Sharding with PlanetScale and Prisma

If you are using a [sharded database](/docs/vitess/sharding) with PlanetScale, Prisma supports defining shard keys in your Prisma schema. This is currently available as a [Preview](https://www.prisma.io/docs/orm/more/releases#preview) feature in Prisma as of [v6.10.0](https://github.com/prisma/prisma/releases/tag/6.10.0).

To use shard key attributes, specify the `shardKeys` Preview feature on your Prisma generator in `schema.prisma`:

```prisma  theme={null}
generator client {
  provider        = "prisma-client-js"
  previewFeatures = ["shardKeys"]
}
```

This allows you to use `@shardKey` and `@@shardKey` attributes.

* `@shardKey` — Used to define a single-column shard key
* `@@shardKey` — Used to define a multi-column shard key

For example, if you have a shard key on your `region` column in your `User` database, you can define that in your Prisma model with:

```js  theme={null}
model User {
  id     String @default(uuid())
  region String @shardKey
}
```

## Next steps

* Make safe schema changes with [branching and deploy requests](/docs/vitess/schema-changes/branching)
* Explore [PlanetScale Insights](/docs/vitess/monitoring/query-insights) for performance monitoring
* Learn how to [target your replicas](/docs/vitess/scaling/replicas)
* Explore our [migration guides](/docs/vitess/imports/database-imports)
* Enable [PlanetScale vectors](/docs/vitess/vectors)
* Implement [read replicas](/docs/vitess/scaling/replicas) for scale
* Join the [PlanetScale Discord](https://discord.gg/planetscale) community

## Additional resources

<Columns cols={2}>
  <Card title="Prisma Documentation" icon="file-lines" horizontal href="https://www.prisma.io/docs" />

  <Card title="Prisma + PlanetScale Best Practices" icon="file-lines" horizontal href="https://www.prisma.io/docs/orm/overview/databases/planetscale" />

  <Card title="PlanetScale Support" icon="envelope" horizontal href="https://docs/support.planetscale.com" />

  <Card title="Foreign key constraints support" icon="file-lines" horizontal href="/docs/vitess/foreign-key-constraints" />
</Columns>

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Vector search and storage
Source: https://planetscale.com/docs/vitess/vectors

Vectors are a data structure that captures opaque semantic meaning about something and allows a database to search for resources by similarity based on this opaque meaning.

As a data type, a vector is just an array of floating-point numbers. Those numbers are generated by submitting some resource — a word, a string, a document, an image, audio, etc. — to an *embedding model*,¹ which converts the resource to a vector.

A vector database stores those vector embeddings alongside other relational data. In practice, that might look like a table with columns for ID (a primary key), content (as a BLOB or VARCHAR), and a vector. Then it becomes possible to perform queries that find content similar to a search vector, like so:

```sql  theme={null}
SELECT id
  FROM t1
  ORDER BY DISTANCE(TO_VECTOR('[1.2, 3.4, 5.6]'), embedding, 'L2_squared')
  LIMIT 10;
```

Possible applications include recommendation engines that show products similar to a user's purchase history, or search engines that find documents or other resources based on natural-language queries. Read our [applications of vector databases](/docs/vitess/vectors/use-cases) docs to learn more about how vector databases can be applied in the real world.

PlanetScale has added support for vector columns, vector distance functions, and vector indexes, as described below.

If at any point you experience issues with vectors, we highly encourage you to get in touch. Your feedback is extremely valuable as we continue to refine vectors support, so don’t hesitate to reach out. You can [submit a support ticket](https://planetscale.com/contact) to relay any feedback or issues. We also have a vectors channel in our [Discord](https://discord.com/invite/pDUGAAFEJx) where you can ask questions and share feedback.

\[¹]: PlanetScale does not currently provide an embedding service. You can find several good cloud-based options like OpenAI or AWS Titan, or local options like Python `sentence_transformers`.

## Enabling PlanetScale vectors

PlanetScale has a custom version of MySQL that has been extended with vector support. Vector support can be enabled on a per-branch basis. The branch will be updated to the vectors-capable version of MySQL. Branches with vectors enabled [will repurpose half of the memory](#resource-requirements) in the InnoDB buffer pool for use with vectors.

To enable the vector support on a branch:

<Steps>
  <Step>
    In the database where you want to use vectors, navigate to the “Branches” tab, and click on the branch where you would like to enable vector support.
  </Step>

  <Step>
    Click on the small gear icon underneath the “Connect” button on the right.
  </Step>

  <Step>
    Click the toggle next to “Enable vectors”.
  </Step>

  <Step>
    Click “Save branch settings”.
  </Step>

  <Step>
    The branch will upgrade asynchronously to the correct version of MySQL, which may take 30-60 minutes. While this happens, the database dashboard will show an "Enabling vectors" badge, which changes to a "Vector-enabled" badge when the upgrade is complete.
  </Step>
</Steps>

## Adding vector columns

As a first step, create one or more columns with the VECTOR type and then insert some vectors. Here’s an example:

```sql  theme={null}
CREATE TABLE t1(id INT PRIMARY KEY auto_increment, embedding VECTOR(4));
INSERT INTO t1(embedding) VALUES (TO_VECTOR('[1, 2, 3, 4]')),
                                 (TO_VECTOR('[5, 6, 7, 8]'));
```

At some point, you’ll want to build an index on this vector column to facilitate fast similarity searches. Here’s how to create one:

```sql  theme={null}
CREATE /*vt+ QUERY_TIMEOUT_MS=0 */
  VECTOR INDEX embedding_index ON t1(embedding);
```

The `QUERY_TIMEOUT_MS` comment is optional, but important for tables over 100,000 rows. Vector indexes take a significant amount of time to build, and the comment allows the `CREATE` statement to bypass Vitess’s usual timeout.

Then, you can perform similarity searches like so:

```sql  theme={null}
SELECT id, DISTANCE(TO_VECTOR('[3, 3, 3, 3]'), embedding, 'L2_SQUARED') AS d
  FROM t1
  ORDER BY d
  LIMIT 10;
```

Use an `EXPLAIN` query to confirm that the query uses the new index. This query actually won’t use the index until the table has around 50 rows in it.

Vector indexes provide approximate results. An unindexed query with LIMIT 100 returns exactly the 100 rows closest to the reference vector, after performing a full table scan and a sort. An indexed query returns, on average, about 100 of the top 105 (around 95%) of the rows closest to the reference vector, but much faster than a full table scan. This is expected, because all efficient vector indexes, including PlanetScale’s vector indexes, perform approximate nearest neighbor (ANN) searches.

If you are adding vectors to your database from an application, you may want to use prepared statements, although we do not recommend it. `TO_VECTOR` works in that setting, but serializing the vectors on the client side and uploading them as binary is faster. The serialized format is IEEE-754 32-bit floats, which you can serialize with code like this:

* Python: `struct.pack(f'{len(float_array)}f', *float_array)`
* Ruby: `float_array.pack(“f*”)`
* Rust: `float_array.map(|f| f.to_ne_bytes()).flatten().collect()`

You can use the resulting blob (which will be 4 bytes times the number of dimensions in the vector) in an `INSERT` statement like this:

```sql  theme={null}
INSERT INTO t1(embedding) VALUES
  (CAST(? AS CHAR CHARACTER SET binary));
```

## Vector index parameters

Statements that create a vector index take optional parameters, which can be specified as JSON key-value pairs, like so:

```sql  theme={null}
CREATE VECTOR INDEX embedding_index ON t1(embedding)
  SECONDARY_ENGINE_ATTRIBUTE='{"type":"spann", "distance":"cosine"}';
```

The `type` attribute specifies the algorithm used to build and query the vector index. It is optional, and the only valid value is `spann`, which is the default. The `distance` attribute specifies the distance metric that queries will use, and can be any of the following:

* `dot` for the dot product
* `cosine` for the cosine of the angle between the two vectors.
* `l2` or `euclidean` for the length of a line between the ends of the vectors
* `l2_squared` or `euclidean_squared` for the square of the Euclidean distance. This is the default.

The `distance` metric specified at index creation time must match the distance metric used at query time, or the index cannot be used, and MySQL will perform a full-table scan instead.

Other possible options include `fixed_quantization` and `product_quantization`, which configure [vector compression](/docs/vitess/vectors/terminology-and-concepts#quantization) in the index, resulting in lower storage requirements and faster query times. See the [Quantization](/docs/vitess/vectors/reference#quantization-options) section for more information.

## Filtered vector queries

Vector data is stored in tables alongside any other relational data, and sometimes applications need to query, join, or filter based on that relational data. In particular, queries with `WHERE` clauses work, and MySQL chooses the index or indexes that allow the query to complete the fastest. For example, in a table of products for sale containing indexed columns for price and seller ID, here are how two queries might be executed:

```sql  theme={null}
SELECT id,price,seller_id
  FROM products
  WHERE price < 20.0
  ORDER BY DISTANCE(TO_VECTOR('[1.2, 3.4, 5.6]'), embedding, 'L2_squared')
  LIMIT 10;
```

This query selects the ten products with a price under $20 closest to some reference vector. If a non-trivial fraction of products cost less than $20, then MySQL will use the vector index to produce more than ten results, and filter them down to exactly ten results that meet the price constraint.

```sql  theme={null}
SELECT id,price,seller_id
  FROM products
  WHERE seller_id=789
  ORDER BY DISTANCE(TO_VECTOR('[1.2, 3.4, 5.6]'), embedding, 'L2_squared')
  LIMIT 10;
```

This query selects the ten products from a given seller that are closest to some reference vector. If that seller’s products are a small enough fraction of the rows in the table, then MySQL will use the index on seller\_id to find all products from that seller, and will sort those products by vector distance and return the top ten.

The MySQL query planner chooses whether to use the vector index or some other index automatically based on the query and based on the contents of the table, to maximize query performance. Use `EXPLAIN` on any given query to see how it will execute.

As we continue to refine vectors support, we’re looking for feedback on how well MySQL plans vector queries. If you believe you’ve hit an edge case or something looks wrong, please [open a support ticket](https://planetscale.com/contact) and let us know.

## Resource requirements

A vector index with default parameters requires 3-4x as much disk space as the underlying data in the vector column, and it requires around 15% as much memory as the underlying data. For example, a vector column with 1536 dimensions and a million rows takes up 1536 \* 1000000 \* 4 = 5.7GiB of space in the table. The corresponding index will be 18-24GiB, and it will require around 800MiB of memory to build and query. Using [quantization settings](/docs/vitess/vectors/reference#quantization-options) other than the default will increase or decrease the required disk space and memory proportional to how much less or more each vector gets compressed relative to the 16-bit default.

PlanetScale scales storage space automatically for any branch backed by EBS. On Metal, make sure the instance size you choose has enough storage for the vector indexes you plan to create.

By default, vector indexes reserve half of the buffer-pool memory allocated to each instance, equal to about one third of overall instance memory. This tradeoff can be changed in the [cluster configuration](/docs/vitess/cluster-configuration) for any branch with vectors enabled.

## Usage recommendations

* **Incremental indexes** (indexes that begin empty and update as new vectors are added) function correctly, but are significantly slower to build compared to a one-shot index (an index built on an existing set of vectors). Disk usage is much higher due to potentially very high InnoDB blob fragmentation issues, so it's much easier to run out of disk space.
* Once you opt a branch into the vectors feature, that branch must continue to run a vectors-enabled version of MySQL. You can remove your vector columns/tables, but you cannot downgrade that branch to its prior version of MySQL.

## Feedback

We want to make our vectors offering as reliable, fast, and feature-rich as possible. Feedback from our early users will help make this possible. If you encounter any issues, crashes, unexpected errors or poor performance, please [submit a support ticket](https://planetscale.com/contact). You are also welcome to reach out with general feedback and suggestions.

We also have a [Discord](https://discord.com/invite/pDUGAAFEJx) channel for the vectors feature where you can ask questions, share feedback, and discuss what you’re working on.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Vector type and index reference
Source: https://planetscale.com/docs/vitess/vectors/reference



## Vector type

PlanetScale MySQL provides a `VECTOR(X)` type that can be used to store vectors.
To add a vector column to a table, set it to type `VECTOR(X)` where `X` is the dimension of the vectors to be stored in this column.

### Example

```sql  theme={null}
CREATE TABLE t1 (
  id INT PRIMARY KEY auto_increment,
  embedding VECTOR(4)
);
```

## Vector index

PlanetScale MySQL provides a new `VECTOR INDEX` to facilitate fast and scalable approximate nearest neighbor (ANN) search on vector data.

Statements that create a vector index may take optional parameters, which can be specified as JSON key-value pairs, via the `SECONDARY_ENGINE_ATTRIBUTE` variable.
There are four options that can be specified in the JSON. The first two are the following:

* `type`: specifies the algorithm used to build and query the vector index.
  * Supported values: `spann` (more info on the [SPANN algorithm](/docs/vitess/vectors/terminology-and-concepts#space-partitioned-approximate-nearest-neighbors-spann))
* `distance` specifies the distance metric that queries will use.
  * Supported values:
    * `dot` for the dot product
    * `cosine` for the cosine of the angle between the two vectors.
    * `l2` or `euclidean` for the length of a line between the ends of the vectors
    * `l2_squared` or `euclidean_squared` for the square of the Euclidean distance. This is the default.

The distance metric specified at index creation time must match the distance metric used at query time, or the index cannot be used, and MySQL will perform a full-table scan instead.

### Quantization options

Vector indexes in PlanetScale MySQL can be configured to use quantization to reduce the size of the vectors stored in the index. Quantization is a compression technique
that reduces the number of bits used to store each vector. This compression is only applied to the vectors as they're stored in the index: the vectors that you insert in
your MySQL table are always stored losslessly.

The default mode of quantization is `bfloat16`, which reduces the memory and disk space used by an index by half with little or no effect on recall. If you need full, 32-bit floating point numbers in a vector index, you must explicitly request `"fixed_quantization":"none"` when creating the index.

The quantization options can be specified alongside the other index options in the `SECONDARY_ENGINE_ATTRIBUTE` variable. The following quantization algorithms are currently supported:

* `product_quantization` (PQ) specifies that product quantization should be used for vector compression.
  * A value of the form `{"dimensions":X}` must be provided, where `X` is the number of dimensions to use for the quantization.
    `X` must be a divisor of the vector's dimension. As an example, vectors with 1536 dimensions can be quantized to 192 dimensions with minimal losses to recall, but overly small values of `X` may end up affecting
    the recall of the index.
  * Product quantization currently only works with the `l2` distance metric.

<Note>
  Product Quantization is a learned quantization algorithm, which means that an existing vector dataset is required to train the quantization model.
  To use Product Quantization on your indexes, begin by inserting a large number of vectors into your table. The more vectors you insert, the better the resulting quantization will be. Then, create the index with the Product Quantization options you want.
  Once the index has been created, you can continuously insert, update, and delete vectors from your table, but beware that if the distribution of the new vectors changes significantly (e.g. because they come from a different source), the
  recall on the index may be affected.
  If you create a Product Quantized index on an empty table, you will need to insert at least 1000 vectors in a single transaction before the index is usable.
</Note>

* `"fixed_quantization" : "float16"` specifies that fixed quantization into [Float16](https://en.wikipedia.org/wiki/Half-precision_floating-point_format) should be used for vector compression. No extra arguments are required.
* `"fixed_quantization" : "bfloat16"` specifies that fixed quantization into [Brain Float 16](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format) should be used for vector compression. No extra arguments are required. This setting is the default.
* `"fixed_quantization" : "none"` specifies that no quantization should occur; the index will use full, 32-bit floats. No extra arguments are required.
* `"fixed_quantization" : "onebit"` specifies that fixed quantization into 1 bit should be used for vector compression. No extra arguments are required.
  * This is a lossy compression method that can be used to store vectors in a very compact form. It is not suitable for all use cases, as it can significantly affect the recall of the index.
  * This quantization method currently only works with the `l2` distance metric, which effectively calculates the Hamming Distance between the vectors.

### Examples

```sql  theme={null}
CREATE /*vt+ QUERY_TIMEOUT_MS=0 */
  VECTOR INDEX embedding_index ON t1(embedding);
```

```sql  theme={null}
CREATE /*vt+ QUERY_TIMEOUT_MS=0 */
  VECTOR INDEX embedding_index ON t1(embedding)
  SECONDARY_ENGINE_ATTRIBUTE='{"type":"spann", "distance":"cosine"}';
```

```sql  theme={null}
CREATE  /*vt+ QUERY_TIMEOUT_MS=0 */
  TABLE t1(
    title VARCHAR(250),
    vec VECTOR(1536),
    VECTOR KEY k(vec) SECONDARY_ENGINE_ATTRIBUTE='{"type":"spann", "distance":"l2", "product_quantization":{"dimensions":96}}'
  );
```

## Vector functions

PlanetScale MySQL includes several new functions for working with vectors.

## `TO_VECTOR(string)` or `STRING_TO_VECTOR(string)`

Converts a text string to a binary vector value. The text string is an array of floating point numbers in JSON format.

### Example

```sql  theme={null}
SELECT TO_VECTOR('[1, 2.78, 3.14]');
  -> 0x0000803F85EB3140C3F54840
```

## `FROM_VECTOR(string)` or `VECTOR_TO_STRING(vector)`

Converts a binary vector to a human-readable string.

### Example

```sql  theme={null}
SELECT FROM_VECTOR(0x0000803F85EB3140C3F54840);
  -> [1.00000e+00,2.78000e+00,3.14000e+00]
```

## `VECTOR_DIM(string)`

Calculates the dimension of a vector.

### Example

```sql  theme={null}
SELECT VECTOR_DIM(TO_VECTOR('[1,2,3]'));
  -> 3
```

## `DISTANCE(vector1, vector2, [metric])`

Calculates the distance between `vector1` and `vector2`.
The optional third parameter specifies which distance metric is to be used: `dot`, `cosine`, `l2` (`euclidean`), or `l2_squared` (`euclidean_squared`).

* `dot` means the dot product.
* `cosine` means the cosine of the angle between the two vectors.
  This is mathematically defined as the dot product divided by the magnitude of the two vectors, which yields a value between `-1` and `1`.
  Some vector database vendors do additional math on the result to normalize the value to be between `0` and `1` or between `0` and `2`.
  Our implementation normalizes the output, and returns a value between `0` and `2`.
  A result of `0` means the vectors are proportional (point in the same direction) and `2` means the vectors are opposite.
* `l2` (or `euclidean`) means the length of a line between the ends of the vectors.
* `l2_squared` (or `euclidean_squared`) is the square of the Euclidean distance

If the distance metric is omitted, it defaults to `dot`.

### Examples

```sql  theme={null}
SELECT DISTANCE(TO_VECTOR('[1,2]'), TO_VECTOR('[5,4]'), 'dot');
  -> 13
```

```sql  theme={null}
SELECT DISTANCE(TO_VECTOR('[1,2]'), TO_VECTOR('[5,4]'), 'cosine');
  -> 0.9079593845004517
```

```sql  theme={null}
SELECT DISTANCE(TO_VECTOR('[1,2]'), TO_VECTOR('[5,4]'), 'l2');
  -> 4.47213595499958
```

```sql  theme={null}
SELECT DISTANCE(TO_VECTOR('[1,2]'), TO_VECTOR('[5,4]'), 'l2_squared');
  -> 20
```

```sql  theme={null}
SELECT id, price, seller_id
  FROM products
  WHERE price < 20.0
  ORDER BY DISTANCE(TO_VECTOR('[1.2, 3.4, 5.6]'), embedding, 'l2_squared')
  LIMIT 10;
```

## `DISTANCE_DOT(vector1, vector2)`

Is the same as `DISTANCE(vector1, vector2, 'dot')`

## `DISTANCE_COSINE(vector1, vector2)`

Is the same as `DISTANCE(vector1, vector2, 'cosine')`

## `DISTANCE_L2(vector1, vector2)`

Is the same as `DISTANCE(vector1, vector2, 'l2')`

## `DISTANCE_EUCLIDEAN(vector1, vector2)`

Is the same as `DISTANCE(vector1, vector2, 'l2')`

## `DISTANCE_L2_SQUARED(vector1, vector2)`

Is the same as `DISTANCE(vector1, vector2, 'l2_squared')`

## `DISTANCE_EUCLIDEAN_SQUARED(vector1, vector2)`

Is the same as `DISTANCE(vector1, vector2, 'l2_squared')`

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Vector database terminology and concepts
Source: https://planetscale.com/docs/vitess/vectors/terminology-and-concepts



There are many concepts, algorithms and data structures that are used when discussing vector databases.
Here, we provide an overview of many of these concepts and describe the indexing technique that PlanetScale MySQL uses (SPANN).

## General terminology

Common terms used when discussing vector databases and indexes.

#### Vector (Embedding)

Vectors (more specifically, Embeddings) are a data structure that captures opaque semantic meaning about something and allows a database to search for resources by similarity based on this opaque meaning.
As a data type, a vector is just an array of floating-point numbers.
Those numbers are generated by submitting some resource — a word, a string, a document, an image, audio, etc — to an *embedding model*, which converts the resource to a vector.

#### K-Nearest Neighbors (KNN)

KNN refers to the K-Nearest Neighbors to some point P in a high-dimensional space.
With vector databases, we sometimes want to store millions or billions of high-dimensional vectors.
A common query on a vector database is to find the other vectors in our data set that are similar to some input search vector.
If the vectors are embeddings generated from an AI model, two vectors being similar (or near each other) in the high-dimensional space means that they have similar opaque meaning.
We can ask the vector database to give us the 100 closest vectors, which is a form of KNN search, where K = 100.

#### Approximate Nearest Neighbors (ANN)

ANN It is similar to KNN, but instead of finding the *exact* K closest neighbors to some vector, we instead look for the *approximately* K closest neighbors in the high-dimensional space.
This means that we might miss some of the *actual* closest neighbors.
However, if we relax our requirement and use ANN instead of KNN, we can often get significant performance improvement using specialized vector search indexes, which are further discussed later on this page.

We measure how good a job an algorithm does at producing nearest neighbors with ANN via a measure known as *recall*.

#### Recall

In the context of ANN search, the *recall* is the percentage of the actual nearest neighbors found in an ANN search.
For example, say that we are performing a search on a large set of vectors and want to get the nearest 100 neighbors.

If we perform KNN search, this will give us the exact 100 top searches.
Instead, we may perform ANN search.
If that ANN search returns 95 of the actual top 100 and then 5 results that are outside of the top 100, we would say the recall is 95%.
If instead it returned 80 of the actual top 100 and then 20 results outside, we say the recall was 80%.

Most vector indexes provide ANN search.
Ideally, we'd like to perform these searches with high recall.
Often, there is a trade-off between speed and recall %.

#### Quantization

Quantization is a technique for compressing a vector into a more compact representation.
Many vector indexes use quantization to compress the vector data stored in the index to reduce its memory and disk footprint.
Sometimes, the full vector representation will still be stored somewhere on disk, and sometimes only the quantized vector will be stored.
With PlanetScale, the SQL table always stores the full, original vectors, and the index stores half-precision `bfloat16` values by default. You can use the different quantization options when creating a vector index to use other quantized formats or disable quantization entirely.

#### Fixed Quantization

Fixed Quantization is one of the simplest ways to quantize a vector, because it is stateless and does not require any training data: all the elements of the vector are compressed into a smaller datatype (e.g. from float32 to float16, or all the way down to a single bit).

#### Product Quantization (PQ)

PQ is a learned algorithm for compressing a vector into a more compact representation. Unlike fixed quantization methods, it requires an existing vector dataset to "train" before it can be used.

## Types of search algorithms and indexes

There are many algorithms and indexes that have been developed over the years, each with their strengths and weaknesses.

#### Brute-force search

Brute-force search on an vector data set for nearest neighbors is one that does not use any index.
Instead, it compares the search vector to all other vectors to find the nearest neighbors.
The advantage of this is that is can produce exact KNN results.
The disadvantage is that it does not scale well beyond a few hundred or a few thousand vectors.
It is impractical for any large-scale data set.

#### Inverted File Index (IVF)

IVF is a type of index used to speed up ANN search.
In an IVF index, all of the vectors are partitioned into chunks of similar indexes.
The number of chunks to partition the data into is typically configurable.
In the image below, we have our vectors (blue dots) partitioned into 5 chunks (colored regions).
This shows a representation in 2D space, but is applicable to N-dimensional space as well.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/vector-indexes/ivf.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=08140f5995a4595e7bd18dd6b4870129" alt="IVF Visual" data-og-width="2000" width="2000" data-og-height="800" height="800" data-path="docs/images/assets/docs/concepts/vector-indexes/ivf.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/vector-indexes/ivf.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=096fa5927582375ddc4ed9624ef9169b 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/vector-indexes/ivf.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=3ed42dc0145fe054c7034b967a03a349 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/vector-indexes/ivf.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=c2df113afd2ec66378295cce0b36dc6a 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/vector-indexes/ivf.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=aad1df273ec3daebb818a7461ee6f022 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/vector-indexes/ivf.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=125dc0688bbc7dabe04ff03cc4f04b04 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/vector-indexes/ivf.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=de8cb959330bac2cd876c32e6fec4363 2500w" />
</Frame>

When a vector search is performed, the index finds the partition(s) that are most likely to contain similar vectors, and only performs similarity search on vectors in those.

This technique allows for much faster search compared to a full scan of all vectors, as we eliminate much of the vector data from the search early in the process.
However, it might miss some similar vectors in the chunks that it skips, and therefore is only capable of performant ANN searches.

* **Pros:** Speeds up searches, can be combined with other methods.
* **Cons:** Large data sets either cause slowdown or reduced recall, or both.

#### Hierarchical Navigable Small World (HNSW)

HNSW is one of the most commonly implemented vector indexes in modern vector databases.
This is because it provides efficient ANN search for small and medium-sized data sets, and is a widely-adopted algorithm.

HNSW indexes map every vector in the data set onto a graph, with nodes representing each vector and edges between nodes that are near each-other in the high-dimensional space.

HNSW builds multiple graph layers.
The bottom layer is the full similarity graph, and each level above is a sparser version of the graph below (in other words, some nodes and edges get pruned).

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/vector-indexes/hnsw.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=3a3f856230a9666bec47cf83039ae936" alt="HNSW Visual" data-og-width="3000" width="3000" data-og-height="1515" height="1515" data-path="docs/images/assets/docs/concepts/vector-indexes/hnsw.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/vector-indexes/hnsw.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=92de3908cec9bd27fe735677a049ad38 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/vector-indexes/hnsw.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=9b5b81dc1b9cdbaabb245ab3456e3b0a 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/vector-indexes/hnsw.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=f29e1fdc42688cc8da296b13c9953db7 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/vector-indexes/hnsw.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=74bc5566535682f22c954af2f6307740 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/vector-indexes/hnsw.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=47464b60595058e6b01fff10c5e60daf 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/vector-indexes/hnsw.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=bcde3f23893457543f52b641644bd191 2500w" />
</Frame>

HNSW begins its search for a vector at the top layer.
Since the graph is sparse, it can quickly navigate through and find the nearest similar vector.
It then drops down to the next level, and continues to search for the closest vector match in that region of the graph.
This process continues until it finds the closest match(es) in the lowest level.

HNSW works really well for small and medium sized vector data sets when the index can all fit in RAM.
However, once the index no longer fits in RAM, performance takes a drastic hit.
This is because the search on the graph produces a lot of non-sequential I/O.
In memory this is fine, but this leads to poorly-optimized disk access patterns, even for SSDs.

Another downside of HNSW indexes is that they cannot be updated incrementally, so they require periodically re-building the index with the underlying vector data.

* **Pros:** Popular, easy to implement, fast searches.
* **Cons:** Does not scale well for large data sets, requires periodically re-building the index

#### DiskANN

DiskANN is another graph-based ANN search algorithm akin to HNSW.
However, DiskANN does not use the multi-layer graph approach like HNSW does.
Instead, it builds a single large graph of the vector data in several phases.

Initially, a graph node is created for each vector in the data set, and random edges are added, leading to a very "messy" graph.
Then, two phases of optimization and pruning occur.
The first optimization phase does significant adjustment and pruning to optimize similarity search with short edges.
The second optimization phase build longer edges into the graph, allowing for faster graph traversals.
This graph construction algorithm is known as *Vamana*.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/vector-indexes/diskann.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=f1b4d9622d043ae9edd98dc378e02052" alt="DiskANN Visual" data-og-width="3235" width="3235" data-og-height="1494" height="1494" data-path="docs/images/assets/docs/concepts/vector-indexes/diskann.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/vector-indexes/diskann.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=724605674699f89f52286ef485684432 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/vector-indexes/diskann.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=f2922adecc45c7227ae84252f366314a 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/vector-indexes/diskann.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=d8e7457500698d861d217f88599f26a0 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/vector-indexes/diskann.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=c80cd26d079a7501c0d7d19ffd5e74c2 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/vector-indexes/diskann.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=09818725c7ee77eab1397099682a7d07 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/vector-indexes/diskann.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=8b2074e182de5b88c711286f111dccdd 2500w" />
</Frame>

As the name suggests, DiskANN has better performance than HNSW when the index does not all fit into RAM.
DiskANN uses a different technique for building the graph compared to HNSW.
This technique leads to different neighbors for each node and a different memory / disk layout.
When the index cannot fit into memory, this graph allows for a significant reduction in the number of disk read operations needed to fulfill a search compared to HNSW.

DiskANN scales well, but suffers from worse query performance.
While it can be modified to allow incremental updates, these are not particularly efficient and are hard to map to transactional SQL semantics.

* **Pros:** Fast searches, scales better than HNSW or IVF
* **Cons:** Relies on basic graph traversal, incremental index updates are expensive

#### Space-Partitioned Approximate Nearest Neighbors (SPANN)

SPANN is a hybrid vector indexing and search algorithm that uses both graph and tree structures, and was specifically designed to work well for indexes requiring SSD usage.

A graph is created for the vector data, with edges representing nearby neighbors.
The graph is partitioned into many small clusters called *posting lists*.
In SPANN, nodes that are near the boundary between two posting lists may reside in multiple posting lists to help improve recall.

The full set of posting lists are stored on disk.
The center-most node of each posting list (known as the *centroid*) is stored in a special SPTAG index, which is designed to fit in memory.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/vector-indexes/spann.png?fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=982127bceb1d9af249aea7bf94c7846a" alt="SPANN Visual" data-og-width="2167" width="2167" data-og-height="1243" height="1243" data-path="docs/images/assets/docs/concepts/vector-indexes/spann.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/vector-indexes/spann.png?w=280&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=3876cb0c08c58e65432425b24737a549 280w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/vector-indexes/spann.png?w=560&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=2aa9c36930d46da02fbdd75be2afdcd6 560w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/vector-indexes/spann.png?w=840&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=8f61e27b2ed3b2ea56ba38319ae929ba 840w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/vector-indexes/spann.png?w=1100&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=5bc9f8208978c50cb646b0cc5e96cc56 1100w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/vector-indexes/spann.png?w=1650&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=e7177c9e87ac14c141c31a3af0633eed 1650w, https://mintcdn.com/planetscale-cad1a68a/dVvNdxOWlcjFg3oF/docs/images/assets/docs/concepts/vector-indexes/spann.png?w=2500&fit=max&auto=format&n=dVvNdxOWlcjFg3oF&q=85&s=999e0e29900e2081dbeebf1b509594dd 2500w" />
</Frame>

When an ANN search is performed, the search algorithm can quickly identify a small set of the nearest centroids to the search vector using the in-memory index.
Then, a relatively small number of disk reads can take place to load only the relevant parts of the graph into memory, and then the search can be completed.
According to the [SPANN research paper](https://www.microsoft.com/en-us/research/uploads/prod/2021/11/SPANN_finalversion1.pdf), this leads to a 2x performance improvement over DiskANN.

* **Pros:** Fast search, scales to huge data sets, the SPFresh variant allows for efficient incremental updates
* **Cons:** High complexity, high disk usage because of replicated data on-disk

**PlanetScale MySQL uses a variant of the SPANN algorithm for vector indexes.**
Though challenging to implement, we wanted to provide the best solution for our users to scale their vector data sets.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Common use cases for Vector search
Source: https://planetscale.com/docs/vitess/vectors/use-cases



The ability to store and search vector data in your MySQL database, right alongside the rest of your relational data, is a powerful tool when applied correctly.
Some are already keenly familiar with the types of features and products that you can build with this.
However, many are not.
Here, we provide some examples of the types of features you can build with vector search when it is built directly in to your MySQL database.

## Semantic search

We use search all the time, from getting answers to questions via Google, looking for products on Amazon, finding a video on YouTube, or even searching for a document in our local file system.
However, not all search systems are created equal, and they can be implemented in a variety of ways.

Let's say we have a corpus of blog posts that we want to be able to search through to find relevant articles to read.
The table to store the blog posts in looks like this:

```sql  theme={null}
CREATE TABLE blog (
  id BIGINT NOT NULL AUTO_INCREMENT,
  url VARCHAR(2048) NOT NULL,
  title VARCHAR(1024),
  subtitle VARCHAR(1024),
  author VARCHAR(128),
  PRIMARY KEY(id)
);
```

One way we could accomplish this is using plain-text matching.
For our search feature, a user would type in a search string, and then the text of each blog post's title and subtitle is searched for exact string (or substring) matches of our search term.

```sql  theme={null}
SELECT title, url
  FROM blog
  WHERE title LIKE '%$SEARCH_TERM%'
  OR subtitle LIKE '%$SEARCH_TERM%'
  LIMIT 10;
```

(Note: we could also use a MySQL `FULLTEXT` index to perform faster text-matching search).

This could give some good results, however there would be some instances where it would be problematic.
Searching only for exact matches may miss some of the relevant results.
For example, a user might search for the term "dogs" and end up with some posts about dogs.
However, it would miss results that do not use the term dog in favor of words like "puppy" or "hound."
It also might miss documents that are about "wolves" or "coyotes."
This search knows nothing about the *meaning* of the word "dog."

This is where **vector similarity search** comes into play.
With this type of search, we would generate an [embedding](/docs/vitess/vectors/terminology-and-concepts#vector-embedding) for each blog post in our data set.
An embedding is an N-dimensional vector that captures opaque meaning about some piece of data — in this case, the title + subtitle of a blog post.
This vector would then be stored right along with the corresponding blog post row in the database.

```sql  theme={null}
CREATE TABLE blog (
  id BIGINT NOT NULL AUTO_INCREMENT,
  url VARCHAR(2048) NOT NULL,
  title VARCHAR(1024),
  subtitle VARCHAR(1024),
  author VARCHAR(128),
  embedding VECTOR(384), /* <-- New column for the embedding */
  PRIMARY KEY(id)
);
```

Whenever a search occurs on our database, we will also generate an embedding for that search term.
Then, we can use vector similarity search to find the top 10 results that have the most similar *meaning* to the search term.

```sql  theme={null}
SELECT title, url
  FROM blog
  ORDER BY DISTANCE($SEARCH_TERM_VECTOR, embedding, 'L2_squared')
  LIMIT 10;
```

This type of search will be able to include those other posts, since we are now searching by meaning rather than text matches.

## Recommendation systems

Recommendation systems are also common in many products.
Amazon may recommend purchases similar to ones you view, and streaming services may recommend shows to you based on your watch history.
Similar types of systems can be built using vector similarity search.

Perhaps we have an e-commerce platform.
In our database, we have a `product` table, a `user` table, and ` purchase` table to track which items each user purchases.

```sql expandable theme={null}
CREATE TABLE product (
  product_id BIGINT NOT NULL AUTO_INCREMENT PRIMARY KEY,
  title VARCHAR(256) NOT NULL,
  description VARCHAR(1024) NOT NULL,
  price INT NOT NULL,
  ...
);

CREATE TABLE user (
  user_id BIGINT NOT NULL AUTO_INCREMENT PRIMARY KEY,
  username VARCHAR(256) NOT NULL,
  email VARCHAR(256) NOT NULL,
  ...
);

CREATE TABLE purchase (
  purchase_id BIGINT NOT NULL AUTO_INCREMENT PRIMARY KEY,
  product_id BIGINT NOT NULL,
  user_id BIGINT NOT NULL,
  purchased_at DATETIME NOT NULL,
  ...
);
```

A great way to add a purchase recommendation feature to our application would be to use vectors.
The first step would be to add a `VECTOR` column to the `product` table:

```sql  theme={null}
CREATE TABLE product (
  product_id bigint not null auto_increment primary key,
  title varchar(256) not null,
  description varchar(1024) not null,
  price INT NOT NULL,
  embedding VECTOR(384), /* <-- New column for the embedding */
  ...
);
```

We could generate an embedding for each row by feeding the title and description into an embedding model and storing the results in this new column.
Whenever a user logs in to our platform, we want to show them a list of 5 recommended purchases, based on similarity to their most recently purchased item.
To do this, we need a query that finds a user's most recent purchase, and then performs a KNN search for the 5 most similar products in the product table, based on vector similarity.
This query would look something like this:

```sql expandable theme={null}
SET @uid = 12345;

SET @recentEmbedding =
  (SELECT product.embedding
     FROM product
     JOIN purchase ON product.product_id = purchase.product_id
     JOIN user ON purchase.user_id = user.user_id
     WHERE user.user_id = @uid
     ORDER BY purchase.purchased_at DESC
     LIMIT 1);

SET @recommendationIDs =
  (SELECT product_id
    FROM product
    ORDER BY DISTANCE(@recentEmbedding, embedding, 'L2_squared')
    LIMIT 5);
```

We can then use the products in `@recommendationIDs` and display those to the user.

## Retrieval-Augmented Generation (RAG)

RAG is a popular technique for augmenting and enhancing results produced by an LLM.
LLMs such as GPT-4.0 or Sonnet-3.5 are extremely powerful, as they have been trained on immense data sets.
However, these LLMs are not trained on the entire universe of data, and it is often useful to pass them additional context to help answer a query.

Suppose we have a private question/answer platform, internal to our organization.
None of the information on this platform is on the public internet and was not used to train any public LLMs.
This platform stores questions and answers like so:

```sql  theme={null}
CREATE TABLE question (
  question_id BIGINT NOT NULL AUTO_INCREMENT PRIMARY KEY,
  title VARCHAR(256) NOT NULL,
  text VARCHAR(2048) NOT NULL
);

CREATE TABLE answer(
  answer_id BIGINT NOT NULL AUTO_INCREMENT PRIMARY KEY,
  question_id BIGINT NOT NULL,
  text VARCHAR(2048) NOT NULL
);
```

We would like to add a feature to this internal Q/A platform to allow a user to ask an LLM-powered chatbot questions instead of posting a question for other humans to answer.

One way we could do this is allow the user to enter a question, send the prompt directly to the OpenAI API (or similar APIs), fetch the result, and display to the user.
This may not work well for questions that are specific to our organization, as OpenAI may not have sufficient knowledge in this area.
What we can do instead is leverage similarity search to augment the user's question with additional context before sending the prompt to OpenAI, which will allow it to produce better answers.

To do this, we will yet again need to add a vector column to the `answer` table and populate them with embeddings:

```sql  theme={null}
CREATE TABLE answer(
  answer_id BIGINT NOT NULL AUTO_INCREMENT PRIMARY KEY,
  question_id BIGINT NOT NULL,
  text VARCHAR(2048) NOT NULL,
  embedding VECTOR(384) /* <-- New column for the embedding */
);
```

Whenever a user enters a query for the chatbot, we will first generate an embedding for that question.
Then, we will perform similarity search to find existing answers on our platform that are related to this:

```sql  theme={null}
SELECT answer.text
  FROM answer
  JOIN question ON question.question_id = answer.question_id
  ORDER BY DISTANCE($QUERY_EMBEDDING, question.embedding, 'L2_squared')
  LIMIT 5;
```

Now we have the user's initial question and several related answers.
We will construct a string formatted something like this:

```
Here is some information that is related to a question that a user has:

[RELATED ANSWERS]

Using this information, please answer the following user question:

[THE USER'S ORIGINAL QUERY]
```

This new larger string will get passed on to our AI API for a response.
Since we have added context to the prompt, it will be able to do a better job at giving the user a satisfactory answer.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Vectors with an ORM
Source: https://planetscale.com/docs/vitess/vectors/using-with-an-orm



Many modern web development frameworks provide ORMs to make mapping data between your database and the objects in your application a seamless experience.
Here, we show several examples of how you can use the `VECTOR` MySQL type with ORMs.
These docs to not provide a comprehensive list of how to use vectors with all ORMs.
Rather, small examples for several popular choices are provided (Drizzle, Prisma, Rails).
If you use a different ORM and are having trouble getting it to work with vectors, please [reach out](https://planetscale.com/contact).

Also note that for now, using PlanetScale MySQL vectors with these ORMs requires either a custom type or running raw MySQL queries.

## Drizzle

Here, we'll show how you can create and insert rows into a table that has a `VECTOR` column using Drizzle.
First, modify your `schema.ts` file to import the appropriate items and define two new types.
The first type is for the distance functions, and the second is to represent a vector.

```typescript expandable theme={null}
import { sql } from 'drizzle-orm'
import { mysqlTable, customType, text, MySqlColumn, bigint } from 'drizzle-orm/mysql-core'

export type DistanceFunction = 'COSINE' | 'DOT' | 'L2' | 'L2_SQUARED'

export const vector = customType<{
  data: ArrayBuffer
  config: { length: number }
  configRequired: true
  driverData: Buffer
}>({
  dataType(config) {
    return `VECTOR(${config.length})`
  },
  fromDriver(value) {
    return value.buffer as ArrayBuffer
  },
  toDriver(value) {
    return Buffer.from(value)
  }
})
```

With these in place, we can go ahead and declare our table, also in `schema.ts`.

```typescript  theme={null}
export const product = mysqlTable('product', {
  id: bigint('id', { mode: 'number', unsigned: true }).autoincrement().primaryKey(),
  name: text('name'),
  description: text('description'),
  embedding: vector('embedding', { length: 384 })
})
```

This would be in addition to other schema declarations you need for your application.

We now have an object to model a table with a vector embedding column.
Elsewhere in our application, we can perform typical operations like inserts, deletes, searches, etc.
For example, to insert a row into this table, do the following:

```typescript  theme={null}
const embedding = // generate an embedding with your preferred API
const serializedEmbedding = Buffer.from((new Float32Array(embedding)).buffer)
await db.insert(product).values({
  description: 'hi there',
  embedding: serializedEmbedding
})
```

We can also perform searches on the data:

```typescript  theme={null}
const rankedItems = await db
  .select({ description: product.description })
  .from(product)
  .orderBy(`DISTANCE(TO_VECTOR(${embedding}), ${product.embedding}, L2_SQUARED)`)
  .limit(10)
```

If you have a large data set, you'll want to make sure you create an index on this vector column.

## Prisma

Next, let's look at what it takes to get a table with a `VECTOR` column working with Prisma.
As of this writing, [Prisma does not support custom types](https://github.com/prisma/prisma/issues/5039).

Until Prisma provides support, you can still use vectors in a Prisma-powered application by using the `Unsupported` function in your `schema.prisma` and then use raw queries to perform vector searches.
We can add a new table with a `VECTOR(4)` column by adding the following to our `schema.prisma` file.

```typescript  theme={null}
model Product {
  id          BigInt     @id @default(autoincrement()) @db.UnsignedBigInt
  name        String?    @db.Text
  description String?    @db.Text
  embedding   Unsupported("vector(4)")?
}
```

When running `prisma db push`, it will create the table with the `embedding` column having type `VECTOR(4)`, even though Prisma does not technically support vectors.

From here, you can use Prisma's `queryRaw` feature to run raw SQL queries.
For example, to insert a row into this table.

```typescript  theme={null}
const name = 'Pots'
const description = 'For cooking'
const embedding = '[0.5, 0.4, 0.3, 0.2]'
await prisma.$queryRaw(
  Prisma.sql`
    INSERT INTO
      Product (name, description, embedding)
      VALUES(${name}, ${description}, TO_VECTOR(${embedding}))`
)
```

And to retrieve results from this table:

```typescript  theme={null}
const result = await prisma.$queryRaw(
  Prisma.sql`SELECT * FROM Product ORDER BY DISTANCE(TO_VECTOR(${query_vector}), embedding, 'l2_squared')`
)
console.log(result)
```

If you have a large data set, you'll want to make sure you create an index on this vector column.

## Ruby on Rails

Lets look at how you can work with a vector column in a Ruby on Rails application.
Say you have an existing object that models rows in a table, and you are using ActiveRecord to manage the mapping between your objects and your database.
For example, an object representing a tweet in `app/models/tweet.rb`.

```ruby  theme={null}
class Tweet < ApplicationRecord
  belongs_to :user
  has_one_attached :image
  default_scope -> { order(created_at: :desc) }
  validates :content, presence: true, length: { maximum: 140 }
  validates :user_id, presence: true
end
```

Since vectors are a new type for MySQL, we will add a custom migration to handle adding and dropping a `VECTOR` column to this table in the database.
To do this, add a new migration in `db/migrations` like so:

```ruby expandable theme={null}
class AddTweetEmbeddings < ActiveRecord::Migration[7.0]
  def up
    execute <<-SQL
      ALTER TABLE tweets ADD COLUMN embedding VECTOR(1536);
    SQL
    execute <<-SQL
      CREATE VECTOR INDEX idx_tweet_embeddings ON tweets(embedding) '{"type":"spann","distance":"cosine"}';
    SQL
  end

  def down
    execute <<-SQL
      ALTER TABLE tweets DROP COLUMN embedding;
    SQL
  end
end
```

You'll also need to make the appropriate adjustments in your project to ensure that this gets executed when you run `rails db:migrate`.
With the schema updated appropriately, you can run raw SQL queries via ActiveRecord to insert rows with vectors and perform search.
For example, to insert a new row with a vector, you can do something like this:

```ruby expandable theme={null}
embedding = [1.0, 0.5, 0.25, 0.125].to_json
created_at = Time.now.to_s
updated_at = Time.now.to_s
content = 'A tweet!'
user_id =  100

sql = <<-SQL
  INSERT INTO tweets (user_id, content, content_embeddings, created_at, updated_at)
    VALUES (
      #{ActiveRecord::Base.connection.quote(user_id)},
      #{ActiveRecord::Base.connection.quote(content)},
      TO_VECTOR(#{ActiveRecord::Base.connection.quote(embedding)}),
      #{ActiveRecord::Base.connection.quote(created_at)},
      #{ActiveRecord::Base.connection.quote(updated_at)}
    )
SQL

ActiveRecord::Base.connection.execute(sql)
```

To perform a similarity search based on an input embedding, you can execute the following:

```ruby  theme={null}
embedding = [1.0, 0.5, 0.25, 0.125]
sql = <<-SQL
  SELECT id, content, distance(content_embeddings, TO_VECTOR('#{embedding}'), 'COSINE') as d FROM tweets ORDER BY d LIMIT 10
SQL

results = ActiveRecord::Base.connection.execute(sql)
```

## Other ORMs

If you have requests for other ORMs you'd like to see documented for using with vectors, please [reach out](https://planetscale.com/contact) with your questions.
On the other hand, if you get PlanetScale MySQL vectors working with an ORM not listed here and would like to share your technique, please do as well.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# Web console
Source: https://planetscale.com/docs/vitess/web-console

The PlanetScale web console is an interactive interface for running MySQL queries and DDL (Create, Alter, and Delete) against your PlanetScale database branches.

## Get started

The PlanetScale web console can be used to query to any database branch; however, it is [disabled for production branches](/docs/vitess/web-console#enable-for-production-branches) by default to prevent accidental data loss.

To access the web console, navigate to a database, and click on the "Console" tab in the page navigation. From here, you can select which branch you'd like to connect to by selecting it in the dropdown and clicking "Connect".

You can also access the web console directly by adding `/console` to the URL from any database branch page,
`app.planetscale.com/<org>/<database>/<branch>/console`.

Once you have accessed the web console, you can run queries against your database branch, or apply DDL to branches without [safe migrations](/docs/vitess/schema-changes/safe-migrations) enabled.

The following are examples of MySQL statements you may find useful within the web console:

Use `SHOW TABLES;` to see a list of the tables in your database branch.

Use `DESCRIBE table_name;` to obtain information about a given table's structure.

Use `EXPLAIN` in front of `SELECT`, `DELETE`, `INSERT`, `REPLACE` and `UPDATES` statements to learn how the database is executing a query. This can be useful for optimizing slow queries.

## Supported console commands

| Command   | Description                       |
| :-------- | :-------------------------------- |
| ?, \\?    | Synonym for `help`                |
| clear, \c | Clear the current input statement |
| help, \h  | Display list of commands          |
| ego, \G   | Send command to server            |
| go, \g    | Send command to server            |

## Enable for production branches

By default, the web console is disabled for production branches to prevent accidental data loss.

You can enable the web console for production branches on the "Settings" page for the given database,
`app.planetscale.com/<org>/<database>/settings`.

Select the checkbox for "Allow web console access to production branches", then scroll down and click the "Save database settings" button to save your changes.

This will enable the ability to use the web console to run queries against production branches for the given database.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


# What is PlanetScale?
Source: https://planetscale.com/docs/what-is-planetscale



export const VimeoEmbed = ({id, title}) => {
  return <Frame>
      <iframe src={`https://player.vimeo.com/video/${id}?dnt=true`} title={title} className="aspect-video w-full" allow="autoplay; fullscreen; picture-in-picture" />
    </Frame>;
};

PlanetScale is a fully managed relational database platform for [Vitess](/docs/vitess) and [Postgres](/docs/postgres), bringing you scale, performance, and reliability — without sacrificing developer experience.

We don't call ourselves the world's fastest database for nothing. Benchmarks for both [Vitess](https://planetscale.com/benchmarks/docs/vitess) and [Postgres](https://planetscale.com/blog/benchmarking-postgres) show PlanetScale in the lead over and over again for both queries per second and latency.

With PlanetScale, you get blazing fast performance with our [locally-attached NVMe drives](/docs/metal), high availability 3 node clusters (1 primary and 2 replicas by default), automated failovers, connection pooling, online fully-managed version upgrades, and more.

Our Vitess product offers unlimited scalability through explicit horizontal sharding. Vitess was [created at YouTube in 2010](https://docs/vitess.io/docs/overview/history/#:~:text=Vitess%20was%20created%20in%202010,exceed%20the%20database's%20serving%20capacity.) to solve the scaling issues they faced with their massive MySQL database. Vitess was later donated to the CNCF and continues to scale massive companies like [Slack](https://slack.engineering/scaling-datastores-at-slack-with-vitess/), [GitHub](https://github.blog/2021-09-27-partitioning-githubs-relational-databases-scale/), and more.

The team building PlanetScale is made up of passionate industry experts who have spent decades working on databases for some of the web's largest companies. Our team has directly felt the pain of overly-complicated, unintuitive database tools and came to PlanetScale to build the future of databases — the database they wished they had at their previous companies.

## PlanetScale features

The fastest way to understand how PlanetScale is changing the database landscape is to take a peek inside the product. The following features create a powerful developer experience that enables teams to develop quickly and confidently.

### PlanetScale Metal

When you deploy a PlanetScale database, you can choose from network-attached storage or Metal — locally-attached NVMe SSD drives. These blazing fast NVMe drives unlock unlimited IOPS, ultra-low latencies, and the [highest throughput](https://planetscale.com/blog/benchmarking-postgres) for your workloads while still running in the cloud (AWS or GCP).

### Non-blocking schema changes

With most businesses now operating online, downtime and maintenance windows are no longer acceptable. Not only does downtime hurt customer experience and trust, but even a small amount of downtime can result in thousands to millions of dollars lost for companies.

Our [non-blocking schema change workflow](/docs/vitess/schema-changes) for Vitess means that you'll never experience costly table locking or downtime when running schema changes. This is a fundamental piece of PlanetScale and something that we think everyone should have access to, so there's no additional configuration required. Zero-downtime schema changes are baked into the product.

### Branching workflow

Our [branching workflow](/docs/vitess/schema-changes/branching) paired with [safe migrations](/docs/vitess/schema-changes/safe-migrations) is what enables non-blocking schema changes on your production Vitess database. Instead of applying schema changes directly to your production database, we let you create branches, which are essentially copies of your database. When you create a new branch off of production, you have an isolated copy of your database that you can use for development to make schema changes.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/docs/image.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=49b807586d6d33d2210e66dbe1daf182" alt="Branching workflow diagram - Create dev branch off of main, make schema changes, make deploy request, resolve schema conflicts, test, deploy to main" data-og-width="1234" width="1234" data-og-height="652" height="652" data-path="docs/images/docs/image.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/docs/image.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=8bb7884c19eb2f9caeb37acfaa5d0d22 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/docs/image.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=c710b2e7ddc4466be252a3483fb8eae9 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/docs/image.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=e5804510c080fa7c8813475a14ea3edc 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/docs/image.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=7e084d3a5be0bf3e3c0cc9e863737d37 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/docs/image.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=6a4a24b0fdbce399e64f6dc50cb811c3 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/docs/image.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=610ecfa13ad42d15ce797aaa76d53484 2500w" />
</Frame>

Development branches can serve as your staging environment, so you don't have to worry about spinning up a new testing database and constantly syncing it with production. We handle all of that for you.

Once you're ready to deploy schema changes from your development branch to production, you [open a deploy request](/docs/vitess/schema-changes/deploy-requests). The deploy request allows your team to view a diff of the schema changes being made, comment, and approve before deploying the change to production.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/docs/image2.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=c9e61f3a01ef25682750a81c789cbeee" alt="Example of a deploy request showing comments, approval, and deployment" data-og-width="3430" width="3430" data-og-height="2776" height="2776" data-path="docs/images/docs/image2.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/docs/image2.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=67350bb7211a9b35856f6eb71c30a9d4 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/docs/image2.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=9d4548b4aa5f1222b2d9e6d65ac79057 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/docs/image2.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=e94a5ff5c2e05241c39b412fe7acccf6 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/docs/image2.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=c8d731c391afb4e23606e650489f2da9 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/docs/image2.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=1400d08b4e40073068a026357dbee72a 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/docs/image2.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=ff573714b6921f5f52125b8ca0b45c6c 2500w" />
</Frame>

### Revert a schema change

The final piece of the non-blocking schema change workflow is the ability to [revert a recently deployed schema change](/docs/vitess/schema-changes/deploy-requests#revert-a-schema-change) without losing any data that was written since deploying.

<VimeoEmbed id="830571822" title="Revert a schema change" />

Despite all the safeguards we put in place, accidents can happen. If someone on your team deploys a schema change, only to realize afterward that it adversely affected the application, you can simply revert it in the PlanetScale dashboard with the click of a button. Perhaps the most impressive part is that when you revert, you won't lose any data that was written to your database during the time the updated schema was live. We keep track of it and apply it back to the original schema once you revert.

No more fumbling around with snapshots or backups and restores. Just revert.

### Scale with sharding + unlimited connections

With Vitess under the hood, we're able to offer horizontal scaling via sharding with minimal application changes.

PlanetScale allows you to break up a monolithic database and partition the data across several databases. This [reduces the load on a single database](https://planetscale.com/blog/one-million-queries-per-second-with-mysql) by distributing it across several. Sharding can easily become a convoluted and hard-to-manage scenario, but because of our underlying architecture, we're able to keep this sharding logic largely out of the application. So, from the application's perspective, there only exists one database.

Another scenario that companies with massive databases often run into is connection limits due to MySQL. With PlanetScale, we can support [nearly infinite connections](https://planetscale.com/blog/one-million-connections). Vitess offers built-in [connection pooling](https://docs/vitess.io/docs/reference/features/connection-pools/), and we've built our own [edge infrastructure](https://planetscale.com/blog/introducing-the-planetscale-serverless-driver-for-javascript) into PlanetScale to ensure connection limits are never an issue.

We generally recommend exploring horizontal sharding when your database exceeds 250 GB of data and you are beginning to feel some of the [pains associated with large scale](https://planetscale.com/blog/how-to-scale-your-database-and-when-to-shard-mysql). [Sharding](/docs/vitess/sharding/sharding-quickstart) is offered on our Scaler Pro plan. If you need assistance with setting up horizontal sharding, migrating to PlanetScale, or want enterprise-level SLAs, we offer this through our [Enterprise plan](https://planetscale.com/enterprise) option. [Please reach out](https://https://planetscale.com/contact) for more information.

### Insights

[PlanetScale Insights](/docs/vitess/monitoring/query-insights) is our in-dashboard query performance analytics tool. What's unique about Insights is that you can track performance down to the individual query level.

<VimeoEmbed id="830571854" title="PlanetScale Insights" />

At a glance, the interactive graph shows you query latency, queries per second, rows read, and rows written charted against time. You'll also see any deploy requests on the graph, so you can quickly see the impact of those changes.

If you notice your application is running slower than it should or you want to do a deep dive on your bill, you can come to the Insights dashboard and drill down at the individual query level to see:

* number of times a query has run
* total time the query has run
* time per query
* rows read, affected, and returned

### No downtime import tool

We understand changing database providers can be a pain, from dealing with downtime to complicated dumps and restores and endless compatibility issues.

We built a [database import tool](/docs/vitess/imports/database-imports) to make importing to Vitess as pain-free as possible.

With our import tool, you can connect your internet-accessible database to PlanetScale and begin the import process. During the import, your production database remains live, and both your PlanetScale and production databases are continuously synced. This means that as new or updated data hits your production database, PlanetScale will pull it in as long as the connection remains open. Once you're ready to do the swap, the cutover happens in an instant. No downtime and no data loss.

<Frame>
  <img src="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/docs/image3.png?fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=70dc5a8668cc78b9f834983911164b0c" alt="Step 3 of database import - Primary mode" data-og-width="1395" width="1395" data-og-height="415" height="415" data-path="docs/images/docs/image3.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/docs/image3.png?w=280&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=b56f778ee2dbaa94b23e3bc03f5635fd 280w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/docs/image3.png?w=560&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=4cb5e2007b9e51c282f30768c33f53f9 560w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/docs/image3.png?w=840&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=1ca1ba1878172037878f554d7d63252c 840w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/docs/image3.png?w=1100&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=97dffef6591dcea787b6db42764ae7e2 1100w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/docs/image3.png?w=1650&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=182d850abf9ca8831d5a0aff72d08295 1650w, https://mintcdn.com/planetscale-cad1a68a/FWWWKZleEvIA3gmW/docs/images/docs/image3.png?w=2500&fit=max&auto=format&n=FWWWKZleEvIA3gmW&q=85&s=2d7538816982168d587eae85dec8c0db 2500w" />
</Frame>

### Connect

A common task companies need to handle is extracting data out of their database for transformation and analysis.

[PlanetScale Connect](/docs/vitess/integrations/airbyte) provides you with an easy way to perform ELT. You can connect your PlanetScale database to Airbyte or Stitch, and select the destination from there. Both platforms support several data storage destinations, such as Snowflake, Google Big Query, and more.

### CLI

Nearly every action you can take in the PlanetScale dashboard can also be done with our [`pscale` CLI](/docs/cli).

With commands for branching, deploy requests, backups, service tokens, and more, the CLI allows teams to work quickly and efficiently. You can use the CLI to extend PlanetScale into your own DevOps workflow with [GitHub Actions](https://planetscale.com/blog/using-the-planetscale-cli-with-github-actions-workflows), [AWS CodeBuild](https://planetscale.com/blog/build-a-multi-stage-pipeline-with-planetscale-and-aws), and more.

### API

Like the CLI, you can programmatically interact with PlanetScale using our [API](/docs/api/planetscale-api-oauth-applications).

The API is useful for building PlanetScale into other developer tooling for faster development workflows. For example, you can programmatically create and delete database branches, open and merge deploy requests, and more.

See the [PlanetScale API reference](https://planetscale.com/docs/api/reference/getting-started-with-planetscale-api) for more information.

### Replicas

Every production PlanetScale branch comes with two [replicas](/docs/vitess/scaling/replicas). Replicas are read-only copies of your database that can be used to offload read traffic from your primary. With global replica credentials, you can have one credential that will automatically route queries to your branch's replicas and read-only regions.

### Read-only regions

Spin up [read-only regions](/docs/vitess/scaling/read-only-regions) for Vitess with the click of a button. For globally distributed applications, read-only regions allow you to place a copy of your data close to your users.

To query your read-only region, create [a replica credential](/docs/vitess/scaling/replicas) for your database. Replica queries will be automatically routed to the nearest read-only region or one of the branch's replicas, whichever has the lowest latency available.

## Get in touch

Want to learn more about PlanetScale and how it can help your business prevent downtime and improve development speed?

[Reach out to learn more or schedule a demo](https://planetscale.com/contact), and we'll be in touch shortly.

## Need help?

Get help from [the PlanetScale Support team](https://support.planetscale.com/), or join our [GitHub discussion board](https://github.com/planetscale/discussion/discussions) to see how others are using PlanetScale.


