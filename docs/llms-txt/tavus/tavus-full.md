# Tavus Documentation

Source: https://docs.tavus.io/llms-full.txt

---

# Authentication
Source: https://docs.tavus.io/api-reference/authentication

Learn how to generate and use your Tavus API key to authenticate requests.

To use the Tavus API, you need an API key to authenticate your requests. This key verifies that requests are coming from your Tavus account.

## Get the API key

1. Go to the <a href="https://platform.tavus.io/">Developer Portal</a> and select **API Key** from the sidebar menu.
2. Click **Create New Key** to begin generating your API key.
3. Enter a name for the key and (optional) specify allowed IP addresses, then click **Create API Key**.
4. Copy your newly created API key and store it securely.

<Warning>
  **Remember that your API key is a secret!**

  Never expose it in client-side code such as browsers or apps. Always load your API key securely from environment variables or a server-side configuration.
</Warning>

## Make Your First Call

Authentication to the API is performed via HTTP Basic Auth. To authenticate with Tavus's API endpoints, you must provide the API Key in the header, as shown below.

```curl Authentication Header theme={null}
'x-api-key: <api-key>'
```

For example, you are using the [POST - Create Conversation](/api-reference/conversations/create-conversation) endpoint to create a real-time video call session with a Tavus replica. In this scenario, you can send an API request and replace `<api_key>` with your actual API Key.

```shell cURL theme={null}
curl --request POST \
  --url https://tavusapi.com/v2/conversations \
  --header 'Content-Type: application/json' \
  --header 'x-api-key: <api_key>' \
  --data '{
  "replica_id": "r9d30b0e55ac",
  "persona_id": "pe13ed370726",
  "conversation_name": "Interview User"
}'
```


# Create Conversation
Source: https://docs.tavus.io/api-reference/conversations/create-conversation

post /v2/conversations
This endpoint starts a real-time video conversation with your AI replica, powered by a persona that allows it to see, hear, and respond like a human.


**Core Components:**
- Replica - Choice of audio/visual appearance 
- Persona - Define the replica's behavior and capabilities


The response includes a `conversation_url` that you can use to join the call or embed it on your website. [Learn how to embed it here](/sections/integrations/embedding-cvi).

If you provide a `callback_url`, youâ€™ll receive webhooks with updates about the conversation status. [Learn more about Callback here](/sections/webhooks-and-callbacks).


<Accordion title="Required Parameters" icon="lightbulb">
  Required parameters vary depending on the use case:

  **Full Pipeline Conversation:**

  * `persona_id`
  * `replica_id`

  **Audio-Only Conversation:**

  * `persona_id`
  * `replica_id`
  * `audio_only`

  <Warning>
    - `replica_id` is **required** if the persona does **not** have a default replica.
    - `replica_id` is **optional** if the persona **does** have a default replica.
    - If both a default replica and `replica_id` are provided, the supplied `replica_id` will **override** the default.
  </Warning>
</Accordion>


# Delete Conversation
Source: https://docs.tavus.io/api-reference/conversations/delete-conversation

delete /v2/conversations/{conversation_id}
This endpoint deletes a single conversation by its unique identifier.




# End Conversation
Source: https://docs.tavus.io/api-reference/conversations/end-conversation

post /v2/conversations/{conversation_id}/end
This endpoint ends a single conversation by its unique identifier.




# Get Conversation
Source: https://docs.tavus.io/api-reference/conversations/get-conversation

get /v2/conversations/{conversation_id}
This endpoint returns a single conversation by its unique identifier.


<Accordion title="Verbose Response Data" icon="lightbulb">
  You can append `?verbose=true` to the URL to receive additional event data in the response, including:

  * `shutdown_reason`: The reason why the conversation ended (e.g., "participant\_left\_timeout")
  * `transcript`: A complete transcript of the conversation with role-based messages (via `application.transcription_ready`)
  * `system.replica_joined`: When the replica joined the conversation
  * `system.shutdown`: When and why the conversation ended
  * `application.perception_analysis`: The final visual analysis of the user that includes their appearance, behavior, emotional states, and screen activities

  This is particularly useful as an alternative to using the `callback_url` parameter on the <a href="/api-reference/conversations/create-conversation">create conversation</a> endpoint for retrieving detailed conversation data.
</Accordion>


# List Conversations
Source: https://docs.tavus.io/api-reference/conversations/get-conversations

get /v2/conversations
This endpoint returns a list of all Conversations created by the account associated with the API Key in use.




# Create Document
Source: https://docs.tavus.io/api-reference/documents/create-document

post /v2/documents
Upload documents to your knowledge base for personas to reference during conversations

<Note>
  For now, our Knowledge Base only supports documents written in English and works best for conversations in English.

  We'll be expanding our Knowledge Base language support soon!
</Note>

Create a new document in your [Knowledge Base](/sections/conversational-video-interface/knowledge-base).

When you hit this endpoint, Tavus kicks off the processing of the document, so it can be used as part of your knowledge base in conversations once processing is complete.

The file size limit is 50MB. The processing can take up to a few minutes depending on file size.

Currently, we support the following file formats: .pdf, .txt, .docx, .doc, .png, .jpg, .pptx, .csv, and .xlsx.

Website URLs are also supported, where a website snapshot will be processed and transformed into a document.

You can manage documents by adding tags using the `tags` field in the request body.

Once created, you can add the document to your personas (see [Create Persona](/api-reference/personas/create-persona)) and your conversations (see [Create Conversation](/api-reference/conversations/create-conversation)).

## Website Crawling

When creating a document from a website URL, you can optionally enable multi-page crawling by providing the `crawl` parameter. This allows the system to follow links from your starting URL and process multiple pages into a single document.

### Without Crawling (Default)

By default, only the single page at the provided URL is scraped and processed.

### With Crawling

When you include the `crawl` object, the system will:

1. Start at your provided URL
2. Follow links to discover additional pages
3. Process all discovered pages into a single document

**Example request with crawling enabled:**

```json theme={null}
{
  "document_name": "Company Knowledge Base",
  "document_url": "https://docs.example.com/",
  "crawl": {
    "depth": 2,
    "max_pages": 20
  },
  "callback_url": "https://your-server.com/webhook"
}
```

### Crawl Parameters

| Parameter   | Type            | Description                                                                                                                      |
| ----------- | --------------- | -------------------------------------------------------------------------------------------------------------------------------- |
| `depth`     | integer (1-10)  | How many levels deep to follow links from the starting URL. A depth of 1 means only pages directly linked from the starting URL. |
| `max_pages` | integer (1-100) | Maximum number of pages to crawl. Processing stops once this limit is reached.                                                   |

### Rate Limits

To prevent abuse, crawling has the following limits:

* Maximum **100 crawl documents** per user
* Maximum **5 concurrent crawls** at any time
* **1-hour cooldown** between recrawls of the same document

### Keeping Content Fresh

Once a document is created with crawl configuration, you can trigger a recrawl to fetch fresh content using the [Recrawl Document](/api-reference/documents/recrawl-document) endpoint.


# Delete Document
Source: https://docs.tavus.io/api-reference/documents/delete-document

delete /v2/documents/{document_id}
Delete a specific document

Delete a document and its associated data using its unique identifier.


# Get Document
Source: https://docs.tavus.io/api-reference/documents/get-document

get /v2/documents/{document_id}
Retrieve a specific document by ID

Retrieve detailed information about a specific document using its unique identifier.


# List Documents
Source: https://docs.tavus.io/api-reference/documents/get-documents

get /v2/documents
Retrieve a list of documents with optional filtering and pagination

Retrieve a list of documents with support for pagination, sorting, and filtering by various criteria.


# Update Document
Source: https://docs.tavus.io/api-reference/documents/patch-document

patch /v2/documents/{document_id}
Update a specific document's metadata

Update metadata for a specific document. This endpoint allows you to modify the document name and its tags.


# Recrawl Document
Source: https://docs.tavus.io/api-reference/documents/recrawl-document

post /v2/documents/{document_id}/recrawl
Trigger a recrawl of a website document to fetch fresh content

Trigger a recrawl of a document that was created with crawl configuration. This is useful for keeping your knowledge base up-to-date when website content changes.

## When to Recrawl

Use this endpoint when:

* The source website has been updated with new content
* You want to refresh the document's content on a schedule
* The initial crawl encountered errors and you want to retry

## How Recrawling Works

When you trigger a recrawl:

1. The system uses the same starting URL from the original document
2. Links are followed according to the crawl configuration (depth and max\_pages)
3. New content is processed and stored
4. Old vectors are replaced with the new content once processing completes
5. The document's `crawl_count` is incremented and `last_crawled_at` is updated

## Requirements

* **Document State**: The document must be in `ready` or `error` state
* **Crawl Configuration**: The document must have been created with a `crawl` configuration, or you must provide one in the request body

## Rate Limits

To prevent abuse, the following limits apply:

* **Cooldown Period**: 1 hour between recrawls of the same document
* **Concurrent Crawls**: Maximum 5 crawls running simultaneously per user
* **Total Documents**: Maximum 100 crawl documents per user

## Overriding Crawl Configuration

You can optionally provide a `crawl` object in the request body to override the stored configuration for this recrawl:

```json theme={null}
{
  "crawl": {
    "depth": 3,
    "max_pages": 50
  }
}
```

If no `crawl` object is provided, the original crawl configuration from document creation is used.

## Monitoring Recrawl Progress

After initiating a recrawl:

1. The document status changes to `recrawling`
2. If you provided a `callback_url` during document creation, you'll receive status updates
3. When complete, the status changes to `ready` (or `error` if it failed)
4. Use [Get Document](/api-reference/documents/get-document) to check the current status


# Create Guardrails
Source: https://docs.tavus.io/api-reference/guardrails/create-guardrails

post /v2/guardrails
This endpoint creates a new set of guardrails for a persona. Guardrails provide strict behavioral boundaries and guidelines that will be rigorously followed throughout conversations.




# Delete Guardrails
Source: https://docs.tavus.io/api-reference/guardrails/delete-guardrails

delete /v2/guardrails/{guardrails_id}
This endpoint deletes a single set of guardrails by its unique identifier.




# Get Guardrails (One Set)
Source: https://docs.tavus.io/api-reference/guardrails/get-guardrails

get /v2/guardrails/{guardrails_id}
This endpoint returns a single set of guardrails by its unique identifier.




# Get Guardrails (All Sets)
Source: https://docs.tavus.io/api-reference/guardrails/get-guardrails-list

get /v2/guardrails
This endpoint returns a list of all sets of guardrails.




# Patch Guardrails
Source: https://docs.tavus.io/api-reference/guardrails/patch-guardrails

patch /v2/guardrails/{guardrails_id}
This endpoint allows you to update specific fields of guardrails using JSON Patch operations.

**Note:** The `path` field is a JSON Pointer string that references a location within the target document where the operation is performed.

For example:

```json
[
  { "op": "replace", "path": "/data/0/guardrails_prompt", "value": "Your updated prompt"},
  { "op": "add", "path": "/data/0/callback_url", "value": "https://your-server.com/webhook" }
]
```


<Note>
  * Ensure the `path` field matches the current guardrails schema.
  * For the `remove` operation, the `value` parameter is not required.
</Note>


# Create Objectives
Source: https://docs.tavus.io/api-reference/objectives/create-objectives

post /v2/objectives
This endpoint creates a new objective for a persona. Objectives provide goal-oriented instructions that help guide conversations toward specific achievements and desired outcomes.




# Delete Objective
Source: https://docs.tavus.io/api-reference/objectives/delete-objectives

delete /v2/objectives/{objectives_id}
This endpoint deletes a single objective by its unique identifier.




# Get Objective
Source: https://docs.tavus.io/api-reference/objectives/get-objectives

get /v2/objectives/{objectives_id}
This endpoint returns a single objective by its unique identifier.




# Get Objectives
Source: https://docs.tavus.io/api-reference/objectives/get-objectives-list

get /v2/objectives
This endpoint returns a list of all objectives.




# Patch Objective
Source: https://docs.tavus.io/api-reference/objectives/patch-objectives

patch /v2/objectives/{objectives_id}
This endpoint allows you to update specific fields of an objective using JSON Patch operations.

**Note:** The `path` field is a JSON Pointer string that references a location within the target document where the operation is performed.

For example:

```json
[
  { "op": "replace", "path": "/data/0/objective_name", "value": "updated_objective_name" },
  { "op": "replace", "path": "/data/0/objective_prompt", "value": "Updated prompt for the objective" },
  { "op": "replace", "path": "/data/0/confirmation_mode", "value": "manual" },
  { "op": "add", "path": "/data/0/output_variables", "value": ["new_variable"] },
  { "op": "replace", "path": "/data/0/modality", "value": "visual" },
  { "op": "remove", "path": "/data/0/callback_url" }
]
```




# Overview
Source: https://docs.tavus.io/api-reference/overview

Discover the Tavus API â€” build a real-time, human-like multimodal video conversation with a replica.

## Getting Started with Tavus APIs

Tavus APIs allow you to create a Conversational Video Interface (CVI), an end-to-end pipeline for building real-time video conversations with an AI replica. Each replica is integrated with a persona that enables it to see, hear, and respond like a human.

You can access the API through standard HTTP requests, making it easy to integrate Conversational Video Interface (CVI) into any application or platform.

### Who Is This For?

This API is for developers looking to add real-time, human-like AI interactions into their apps or services.

### What Can You Do?

Use the end-to-end Conversational Video Interface (CVI) pipeline to build human-like, real-time multimodal video conversations with these three core components:

<CardGroup>
  <Card title="Persona" icon="heart-pulse" href="/api-reference/personas/create-persona">
    Define the agentâ€™s behavior, tone, and knowledge.
  </Card>

  <Card title="Replica" icon="user-group" href="/api-reference/phoenix-replica-model/create-replica">
    Train a lifelike digital twin from a short 2-minute video.
  </Card>

  <Card title="Conversation" icon="video" href="/api-reference/conversations/create-conversation">
    Create a real-time video call session with your AI replica.
  </Card>
</CardGroup>


# Create Persona
Source: https://docs.tavus.io/api-reference/personas/create-persona

post /v2/personas
This endpoint creates and customizes a digital replica's behavior and capabilities for Conversational Video Interface (CVI).

**Core Components:**
- Replica - Choice of audio/visual appearance
- Context - Customizable contextual information, for use by LLM
- System Prompt - Customizable system prompt, for use by LLM
- Layers
  - Perception - Multimodal vision and understanding settings (Raven)
  - STT - Transcription and turn taking settings (Sparrow)
  - Conversational Flow - Turn-taking, interruption handling, and active listening settings
  - LLM - Language model settings
  - TTS - Text-to-Speech settings


<Note>
  For detailed guides on each layer of the Conversational Video Interface, click <a href="/sections/conversational-video-interface/persona/overview#cvi-layer">here</a>.
</Note>

<Warning>
  When using full pipeline mode, the `system_prompt` field is required.
</Warning>


# Delete Persona
Source: https://docs.tavus.io/api-reference/personas/delete-persona

delete /v2/personas/{persona_id}
This endpoint deletes a single persona by its unique identifier.




# Get Persona
Source: https://docs.tavus.io/api-reference/personas/get-persona

get /v2/personas/{persona_id}
This endpoint returns a single persona by its unique identifier.




# List Personas
Source: https://docs.tavus.io/api-reference/personas/get-personas

get /v2/personas
This endpoint returns a list of all Personas created by the account associated with the API Key in use.




# Patch Persona
Source: https://docs.tavus.io/api-reference/personas/patch-persona

patch /v2/personas/{persona_id}
This endpoint updates a persona using a JSON Patch payload (RFC 6902). You can modify **any field within the persona** using supported operations like `add`, `remove`, `replace`, `copy`, `move`, and `test`.

For example:


```json
[
  { "op": "replace", "path": "/persona_name", "value": "Wellness Advisor" },
  { "op": "replace", "path": "/default_replica_id", "value": "r79e1c033f" },
  { "op": "replace", "path": "/context", "value": "Here are a few times that you have helped an individual make a breakthrough in..." },
  { "op": "replace", "path": "/layers/llm/model", "value": "tavus-gpt-4o" },
  { "op": "replace", "path": "/layers/tts/tts_engine", "value": "cartesia" },
  { "op": "add", "path": "/layers/tts/tts_emotion_control", "value": "true" },
  { "op": "remove", "path": "/layers/stt/hotwords" },
  { "op": "replace", "path": "/layers/perception/perception_tool_prompt", "value": "Use tools when identity documents are clearly shown." }
]
```


<Note>
  * Ensure the `path` match the current persona schema.
  * For the `remove` operation, the `value` parameter is not required.
</Note>


# Create Replica
Source: https://docs.tavus.io/api-reference/phoenix-replica-model/create-replica

post /v2/replicas
This endpoint creates a new replica using the latest `phoenix-3` model, which can be used in real-time conversations.



To ensure high-quality replica creation, follow the steps in the [Replica Training](/sections/replica/replica-training) guide.


<Note>
  By default, all new replicas are trained using the `phoenix-3` model.\
  To use the older `phoenix-2` model, set the `model_name` parameter to `phoenix-2`.
</Note>

<Accordion title="Required Parameters" icon="lightbulb">
  Required parameters vary based on the replica type:

  **Personal Replica:**

  * `train_video_url`
  * `consent_video_url`

  **Non-Human Replica:**

  * `train_video_url`

  <Warning>
    Make sure the `train_video_url` and `consent_video_url` are publicly accessible download links, such as presigned S3 URLs.
  </Warning>
</Accordion>


# Delete Replica
Source: https://docs.tavus.io/api-reference/phoenix-replica-model/delete-replica

delete /v2/replicas/{replica_id}
This endpoint deletes a Replica by its unique ID. Deleted Replicas cannot be used in a conversation.




# Get Replica
Source: https://docs.tavus.io/api-reference/phoenix-replica-model/get-replica

get /v2/replicas/{replica_id}
This endpoint returns a single Replica by its unique identifier. 

Included in the response body is a `training_progress` string that represents the progress of the Replica training. If there are any errors during training, the `status` will be `error` and the `error_message` will be populated.




# List Replicas
Source: https://docs.tavus.io/api-reference/phoenix-replica-model/get-replicas

get /v2/replicas
This endpoint returns a list of all Replicas created by the account associated with the API Key in use. In the response, a root level `data` key will contain the list of Replicas.




# Rename Replica
Source: https://docs.tavus.io/api-reference/phoenix-replica-model/patch-replica-name

patch /v2/replicas/{replica_id}/name
This endpoint renames a single Replica by its unique identifier.




# Generate Video
Source: https://docs.tavus.io/api-reference/video-request/create-video

post /v2/videos
This endpoint generates a new video using a Replica and either a script or an audio file. 

The only required body parameters are `replica_id` and either `script` or `audio_file`. 

The `replica_id` is a unique identifier for the Replica that will be used to generate the video. The `script` is the text that will be spoken by the Replica in the video. If you would like to generate a video using an audio file instead of a script, you can provide `audio_url` instead of `script`. Currently, `.wav` and `.mp3` files are supported for audio file input.

If a `background_url` is provided, Tavus will record a video of the website and use it as the background for the video. If a `background_source_url` is provided, where the URL points to a download link such as a presigned S3 URL, Tavus will use the video as the background for the video. If neither are provided, the video will consist of a full screen Replica.

To learn more about generating videos with Replicas, see [here](/sections/video/quickstart).

To learn more about writing an effective script for your video, see [Scripting prompting](/sections/troubleshooting#script-length).




# Delete Video
Source: https://docs.tavus.io/api-reference/video-request/delete-video

delete /v2/videos/{video_id}
This endpoint deletes a single video by its unique identifier.




# Get Video
Source: https://docs.tavus.io/api-reference/video-request/get-video

get /v2/videos/{video_id}
This endpoint returns a single video by its unique identifier. 

The response body will contain a `status` string that represents the status of the video. If the video is ready, the response body will also contain a `download_url`, `stream_url`, and `hosted_url` that can be used to download, stream, and view the video respectively.




# List Videos
Source: https://docs.tavus.io/api-reference/video-request/get-videos

get /v2/videos
This endpoint returns a list of all Videos created by the account associated with the API Key in use.




# Rename Video
Source: https://docs.tavus.io/api-reference/video-request/patch-video-name

patch /v2/videos/{video_id}/name
This endpoint renames a single video by its unique identifier.




# Changelog
Source: https://docs.tavus.io/sections/changelog/changelog



<Update label="July 25">
  ## New Features

  * **Persona Editing in Developer Portal:** We've added new editing capabilities to help you refine your Personas more efficiently. You can now update system prompt, context, and layers directly in our Developer Portal, plus duplicate existing Personas to quickly create variations or use them as starting points for new projects. Find these new features in your Persona Library at platform.tavus.io.
</Update>

<Update label="July 22">
  ## New Features

  * **Llama 4 Support:** Your persona just got even smarter, thanks to Meta's Llama 4 model ðŸ§  You can start using Llama 4 by specifying `tavus-llama-4` for the LLM `model` value when creating a new persona or updating an existing one. Click <a href="https://docs.tavus.io/sections/conversational-video-interface/persona/llm#tavus-hosted-models">here</a> to learn more!
</Update>

<Update label="July 15">
  ## New Features

  * **React Component Library:** Developers can build with Tavus even faster now with our pre-defined components ðŸš€ Click <a href="https://docs.tavus.io/sections/conversational-video-interface/component-library/overview">here</a> to learn more!
</Update>

<Update label="June 27">
  ## New Features

  * **Multilingual Conversation Support:** CVI now supports dynamic multilingual conversations through automatic language detection. Set the language parameter to "multilingual" and CVI will automatically detect the user's spoken language and respond in the same language using ASR technology.
  * **Audio-Only Mode:** CVI now supports audio-only conversations with advanced perception (powered by Raven) and intelligent turn-taking (powered by Sparrow-1). Set `audio_only=true` in your create conversation request to enable streamlined voice-first interactions.
</Update>

<Update label="June 20">
  ## Enhancements

  * **Fixed CVI responsiveness issue:** Resolved an issue where CVI would occasionally ignore very brief user utterances. All user inputs, regardless of length, now receive consistent responses.
  * **Expanded tavus-llama-4 context window:** Increased maximum context window to 32,000 tokens. For optimal performance and response times, we recommend staying under 25,000 tokens.
</Update>

<Update label="June 3">
  ## Enhancements

  * Reduced conversation boot time by 58% (p50).
</Update>

<Update label="May 28">
  ## Changes

  * Added a new recording requirement to <a href="/sections/replica/replica-training">Replica Training</a>: Start the talking segment with a big smile.

  ## Enhancements

  * Added <a href="/sections/event-schemas/conversation-echo">echo</a> and <a href="/sections/event-schemas/conversation-respond">respond</a> events to conversational context.
</Update>

<Update label="May 17">
  ## Enhancements

  * **Major Phoenix 3 Enhancements for CVI**:
    * Increased frame rate from 27fps to 32fps, significantly boosting smoothness.
    * Reduced Phoenix step's warm boot time by 60% (from 5s to 2s).
    * Lipsync accuracy improved by \~22% based on AVSR metric.
    * Resolved blurriness and choppiness at conversation start.
    * Enhanced listening mode with more natural micro expressions (eyebrow movements, subtle gestures).
    * Greenscreen mode speed boosted by an additional \~1.5fps.
  * **Enhanced CVI Audio Quality**: Audio clicks significantly attenuated, providing clearer conversational audio.
  * **Phoenix 3 Visual Artifacts Fix**: Resolved visual artifacts in 4K videos on Apple devices, eliminating black spot artifacts in thumbnails.
</Update>

<Update label="May 9">
  ## New Features

  * Launched <a href="https://www.tavus.io/post/building-real-time-ai-video-agents-with-livekit-and-tavus">LiveKit Integration</a>: With Tavus video agents now integrated into LiveKit, you can add humanlike video responses to your voice agents in seconds.
  * <a href="https://docs.tavus.io/api-reference/personas/patch-persona">Persona API</a>: Enabled patch updates to personas.

  ## Enhancements

  * Resolved TTS (Cartesia) stability issues and addressed hallucination.
  * **Phoenix 3 Improvements**:
    * Fixed blinking/jumping issues and black spots in videos.
    * FPS optimization to resolve static and audio crackling.
</Update>

<Update label="April">
  ## Enhancements

  * **Replica API**:
    * Enhanced Error Messaging for Training Videos.
    * Optimized Auto QA for Training Videos.
</Update>


# Blocks
Source: https://docs.tavus.io/sections/conversational-video-interface/component-library/blocks

High-level component compositions that combine multiple UI elements into complete interface layouts

### Conversation block

The Conversation component provides a complete video chat interface for one-to-one conversations with AI replicas

```bash theme={null}
npx @tavus/cvi-ui@latest add conversation-01
```

<Tabs>
  <Tab title="Description">
    The `Conversation` component provides a complete video chat interface for one-to-one conversations with AI replicas, featuring main video display, self-view preview, and integrated controls.

    **Features:**

    * **Main Video Display**: Large video area showing the AI replica or screen share
    * **Self-View Preview**: Small preview window showing local camera feed
    * **Screen Sharing Support**: Automatic switching between replica video and screen share
    * **Device Controls**: Integrated microphone, camera, and screen share controls
    * **Error Handling**: Graceful handling of camera/microphone permission errors
    * **Responsive Layout**: Adaptive design for different screen sizes

    **Props:**

    * `conversationUrl` (string): Daily.co room URL for joining
    * `onLeave` (function): Callback when user leaves the conversation
  </Tab>

  <Tab title="Code">
    ```tsx theme={null}
    import { Conversation } from './components/cvi/components/conversation';
    ```

    ```tsx theme={null}
    <Conversation
      conversationUrl={conversationUrl}
      onLeave={() => handleLeaveCall()}
    />
    ```
  </Tab>
</Tabs>

Preview

<Frame>
  <img alt="Conversation Block Preview" />
</Frame>

### Hair Check

The HairCheck component provides a pre-call interface for users to test and configure their audio/video devices before joining a video chat.

```bash theme={null}
npx @tavus/cvi-ui@latest add hair-check-01
```

<Tabs>
  <Tab title="Description">
    The `HairCheck` component provides a pre-call interface for users to test and configure their audio/video devices before joining a video chat.

    **Features:**

    * **Device Testing**: Live preview of camera feed with mirror effect
    * **Permission Management**: Handles camera and microphone permission requests
    * **Device Controls**: Integrated microphone and camera controls
    * **Join Interface**: Call-to-action button to join the video chat
    * **Responsive Design**: Works on both desktop and mobile devices

    **Props:**

    * `isJoinBtnLoading` (boolean): Shows loading state on join button
    * `onJoin` (function): Callback when user clicks join
    * `onCancel` (function, optional): Callback when user cancels
  </Tab>

  <Tab title="Code">
    ```tsx theme={null}
    import { HairCheck } from './components/cvi/components/hair-check';
    ```

    ```tsx theme={null}
    <HairCheck
      isJoinBtnLoading={isLoading}
      onJoin={handleJoinCall}
      onCancel={handleCancel}
    />
    ```
  </Tab>
</Tabs>

Preview

<Frame>
  <img alt="Haircheck Block Preview" />
</Frame>


# Components
Source: https://docs.tavus.io/sections/conversational-video-interface/component-library/components

Learn about our pre-built React components to accelerate integrating the Tavus Conversational Video Interface (CVI) into your application.

# Components

### CVI Provider

The `CVIProvider` component wraps your app with the Daily.co provider context, enabling all Daily React hooks and components to function.

```bash theme={null}
npx @tavus/cvi-ui@latest add cvi-provider
```

<Tabs>
  <Tab title="Description">
    The `CVIProvider` component wraps your app with the Daily.co provider context, enabling all Daily React hooks and components to function.

    **Features:**

    * Provides Daily.co context to all child components
    * Required for using Daily React hooks and video/audio components
    * Simple wrapper for app-level integration

    **Props:**

    * `children` (ReactNode): Components to be wrapped by the provider
  </Tab>

  <Tab title="Code">
    ```tsx theme={null}
    import { CVIProvider } from './cvi-provider';
    ```

    ```tsx theme={null}
    <CVIProvider>
      {/* your app components */}
    </CVIProvider>
    ```
  </Tab>
</Tabs>

### AudioWave

The `AudioWave` component provides real-time audio level visualization for video chat participants, displaying animated bars that respond to audio input levels.

```bash theme={null}
npx @tavus/cvi-ui@latest add audio-wave
```

<Tabs>
  <Tab title="Description">
    The `AudioWave` component provides real-time audio level visualization for video chat participants, displaying animated bars that respond to audio input levels.

    **Features:**

    * **Real-time Audio Visualization**: Three animated bars that respond to audio levels
    * **Active Speaker Detection**: Visual distinction between active and inactive speakers
    * **Performance Optimized**: Uses `requestAnimationFrame` for smooth animations
    * **Responsive Design**: Compact circular design that fits well in video previews
    * **Audio Level Scaling**: Intelligent volume scaling for consistent visual feedback

    **Props:**

    * `id` (string): The participant's session ID to monitor audio levels for
  </Tab>

  <Tab title="Code">
    ```tsx theme={null}
    import { AudioWave } from './audio-wave';
    ```

    ```tsx theme={null}
    <AudioWave id={participantId} />
    ```
  </Tab>
</Tabs>

### Device Select

The `device-select` module provides advanced device selection controls, including dropdowns for choosing microphones and cameras, and integrated toggle buttons.

```bash theme={null}
npx @tavus/cvi-ui@latest add device-select
```

<Tabs>
  <Tab title="Description">
    The `device-select` module provides advanced device selection controls, including dropdowns for choosing microphones and cameras, and integrated toggle buttons.

    **Exported Components:**

    * **`MicSelectBtn`**: Microphone toggle button with device selection
    * **`CameraSelectBtn`**: Camera toggle button with device selection
    * **`ScreenShareButton`**: Button to toggle screen sharing

    **Features:**

    * Integrated device selection and toggling
    * Dropdowns for camera/microphone selection
    * Visual state indicators and accessibility support
    * Uses Daily.co device management hooks
    * CSS modules for styling
  </Tab>

  <Tab title="Code">
    ```tsx theme={null}
    import { MicSelectBtn, CameraSelectBtn, ScreenShareButton } from './device-select';
    ```

    ```tsx theme={null}
    <MicSelectBtn />
    <CameraSelectBtn />
    <ScreenShareButton />
    ```
  </Tab>
</Tabs>

### Media Controls

The `media-controls` module provides simple toggle buttons for microphone, camera, and screen sharing, designed for direct use in video chat interfaces.

```bash theme={null}
npx @tavus/cvi-ui@latest add media-controls
```

<Tabs>
  <Tab title="Description">
    The `media-controls` module provides simple toggle buttons for microphone, camera, and screen sharing, designed for direct use in video chat interfaces.

    **Exported Components:**

    * **`MicToggleButton`**: Toggles microphone mute/unmute state
    * **`CameraToggleButton`**: Toggles camera on/off
    * **`ScreenShareButton`**: Toggles screen sharing on/off

    **Features:**

    * Simple, accessible toggle buttons
    * Visual state indicators (muted, unmuted, on/off)
    * Disabled state when device is not ready
    * Uses Daily.co hooks for device state
    * CSS modules for styling
  </Tab>

  <Tab title="Code">
    ```tsx theme={null}
    import { MicToggleButton, CameraToggleButton, ScreenShareButton } from './media-controls';
    ```

    ```tsx theme={null}
    <MicToggleButton />
    <CameraToggleButton />
    <ScreenShareButton />
    ```
  </Tab>
</Tabs>


# Hooks
Source: https://docs.tavus.io/sections/conversational-video-interface/component-library/hooks

See what hooks Tavus supports for managing video calls, media controls, participant management, and conversation events.

## ðŸ”§ Core Call Management

### useCVICall

Essential hook for joining and leaving video calls.

```bash theme={null}
npx @tavus/cvi-ui@latest add use-cvi-call
```

<Tabs>
  <Tab title="Description">
    A React hook that provides comprehensive call management functionality for video conversations. This hook handles the core lifecycle of video calls, including connection establishment, room joining, and proper cleanup when leaving calls.

    **Purpose:**

    * Manages call join/leave operations with proper state management
    * Handles connection lifecycle and cleanup
    * Provides simple interface for call control

    **Return Values:**

    * `joinCall` (function): Function to join a call by URL - handles Daily.co room connection
    * `leaveCall` (function): Function to leave the current call - properly disconnects and cleans up resources
  </Tab>

  <Tab title="Code">
    ```tsx theme={null}
    import { useCVICall } from './hooks/use-cvi-call';
    ```

    ```tsx theme={null}
    const CallManager = () => {
      const { joinCall, leaveCall } = useCVICall();

      const handleJoin = () => {
        joinCall({ url: 'https://your-daily-room-url' });
      };

      return (
        <div>
          <button onClick={handleJoin}>Join Call</button>
          <button onClick={leaveCall}>Leave Call</button>
        </div>
      );
    };
    ```
  </Tab>
</Tabs>

### useStartHaircheck

A React hook that manages device permissions and camera initialization for the hair-check component.

```bash theme={null}
npx @tavus/cvi-ui@latest add use-start-haircheck
```

<Tabs>
  <Tab title="Description">
    A React hook that manages device permissions and camera initialization for the hair-check component.

    **Purpose:**

    * Monitors device permission states
    * Starts camera and microphone when appropriate
    * Provides permission state for UI conditional rendering
    * Handles permission request flow

    **Return Values:**

    * `isPermissionsPrompt` (boolean): Browser is prompting for device permission
    * `isPermissionsLoading` (boolean): Permissions are being processed or camera is initializing
    * `isPermissionsGranted` (boolean): Device permission granted
    * `isPermissionsDenied` (boolean): Device permission denied
    * `requestPermissions` (function): Function to request camera and microphone permissions
  </Tab>

  <Tab title="Code">
    ```tsx theme={null}
    import { useStartHaircheck } from './hooks/use-start-haircheck';
    ```

    ```tsx theme={null}
    const HairCheckComponent = () => {
      const {
        isPermissionsPrompt,
        isPermissionsLoading,
        isPermissionsGranted,
        isPermissionsDenied,
        requestPermissions
      } = useStartHaircheck();

      useEffect(() => {
        requestPermissions();
      }, []);

      return (
        <div>
          {isPermissionsLoading && <InitializingSpinner />}
          {isPermissionsPrompt && <PermissionPrompt />}
          {isPermissionsDenied && <PermissionDeniedMessage />}
          {isPermissionsGranted && <VideoPreview />}
        </div>
      );
    };
    ```
  </Tab>
</Tabs>

***

## ðŸŽ¥ Media Controls

### useLocalCamera

A React hook that provides local camera state and toggle functionality.

```bash theme={null}
npx @tavus/cvi-ui@latest add use-local-camera
```

<Tabs>
  <Tab title="Description">
    A React hook that provides local camera state and toggle functionality.

    **Purpose:**

    * Manages local camera state (on/off)
    * Tracks camera permission and ready state

    **Return Values:**

    * `onToggleCamera` (function): Function to toggle camera on/off
    * `isCamReady` (boolean): Camera permission is granted and ready
    * `isCamMuted` (boolean): Camera is currently turned off
    * `localSessionId` (string): Local session ID
  </Tab>

  <Tab title="Code">
    ```tsx theme={null}
    import { useLocalCamera } from './hooks/use-local-camera';
    ```

    ```tsx theme={null}
    const CameraControls = () => {
      const { onToggleCamera, isCamReady, isCamMuted } = useLocalCamera();

      return (
        <button
          onClick={onToggleCamera}
          disabled={!isCamReady}
        >
          {isCamMuted ? 'Turn Camera On' : 'Turn Camera Off'}
        </button>
      );
    };
    ```
  </Tab>
</Tabs>

### useLocalMicrophone

A React hook that provides local microphone state and toggle functionality.

```bash theme={null}
npx @tavus/cvi-ui@latest add use-local-microphone
```

<Tabs>
  <Tab title="Description">
    A React hook that provides local microphone state and toggle functionality.

    **Purpose:**

    * Manages local microphone state (on/off)
    * Tracks microphone permission and ready state

    **Return Values:**

    * `onToggleMicrophone` (function): Function to toggle microphone on/off
    * `isMicReady` (boolean): Microphone permission is granted and ready
    * `isMicMuted` (boolean): Microphone is currently turned off
    * `localSessionId` (string): Local session ID
  </Tab>

  <Tab title="Code">
    ```tsx theme={null}
    import { useLocalMicrophone } from './hooks/use-local-microphone';
    ```

    ```tsx theme={null}
    const MicrophoneControls = () => {
      const { onToggleMicrophone, isMicReady, isMicMuted } = useLocalMicrophone();

      return (
        <button
          onClick={onToggleMicrophone}
          disabled={!isMicReady}
        >
          {isMicMuted ? 'Unmute' : 'Mute'}
        </button>
      );
    };
    ```
  </Tab>
</Tabs>

### useLocalScreenshare

A React hook that provides local screen sharing state and toggle functionality.

```bash theme={null}
npx @tavus/cvi-ui@latest add use-local-screenshare
```

<Tabs>
  <Tab title="Description">
    A React hook that provides local screen sharing state and toggle functionality.

    **Purpose:**

    * Manages screen sharing state (on/off)
    * Provides screen sharing toggle function
    * Handles screen share start/stop with optimized display media options

    **Return Values:**

    * `onToggleScreenshare` (function): Function to toggle screen sharing on/off
    * `isScreenSharing` (boolean): Whether screen sharing is currently active
    * `localSessionId` (string): Local session ID

    **Display Media Options:**
    When starting screen share, the hook uses the following optimized settings:

    * **Audio**: Disabled (false)
    * **Self Browser Surface**: Excluded
    * **Surface Switching**: Included
    * **Video Resolution**: 1920x1080
  </Tab>

  <Tab title="Code">
    ```tsx theme={null}
    import { useLocalScreenshare } from './hooks/use-local-screenshare';
    ```

    ```tsx theme={null}
    const ScreenShareControls = () => {
      const { onToggleScreenshare, isScreenSharing } = useLocalScreenshare();

      return (
        <button
          onClick={onToggleScreenshare}
          className={isScreenSharing ? 'active' : ''}
        >
          {isScreenSharing ? 'Stop Sharing' : 'Share Screen'}
        </button>
      );
    };
    ```
  </Tab>
</Tabs>

### useRequestPermissions

A React hook that requests camera and microphone permissions with optimized audio processing settings.

```bash theme={null}
npx @tavus/cvi-ui@latest add use-request-permissions
```

<Tabs>
  <Tab title="Description">
    A React hook that requests camera and microphone permissions with optimized audio processing settings.

    **Purpose:**

    * Requests camera and microphone permissions from the user
    * Starts camera and audio with specific configuration
    * Applies noise cancellation audio processing
    * Provides a clean interface for permission requests

    **Return Values:**

    * `requestPermissions` (function): Function to request camera and microphone permissions

    **Configuration:**
    When requesting permissions, the hook uses the following settings:

    * **Video**: Started on (startVideoOff: false)
    * **Audio**: Started on (startAudioOff: false)
    * **Audio Source**: Default system audio input
    * **Audio Processing**: Noise cancellation enabled
  </Tab>

  <Tab title="Code">
    ```tsx theme={null}
    import { useRequestPermissions } from './hooks/use-request-permissions';
    ```

    ```tsx theme={null}
    const PermissionRequest = () => {
      const requestPermissions = useRequestPermissions();

      const handleRequestPermissions = async () => {
        try {
          await requestPermissions();
          console.log('Permissions granted successfully');
        } catch (error) {
          console.error('Failed to get permissions:', error);
        }
      };

      return (
        <button onClick={handleRequestPermissions}>
          Request Camera & Microphone Permissions
        </button>
      );
    };
    ```
  </Tab>
</Tabs>

***

## ðŸ‘¥ Participant Management

### useReplicaIDs

A React hook that returns the IDs of all Tavus replica participants in a call.

```bash theme={null}
npx @tavus/cvi-ui@latest add use-replica-ids
```

<Tabs>
  <Tab title="Description">
    A React hook that returns the IDs of all Tavus replica participants in a call.

    **Purpose:**

    * Filters and returns participant IDs where `user_id` includes 'tavus-replica'

    **Return Value:**

    * `string[]` â€” Array of replica participant IDs
  </Tab>

  <Tab title="Code">
    ```tsx theme={null}
    import { useReplicaIDs } from './hooks/use-replica-ids';
    ```

    ```tsx theme={null}
    const ids = useReplicaIDs();
    // ids is an array of participant IDs for Tavus replicas
    ```
  </Tab>
</Tabs>

### useRemoteParticipantIDs

A React hook that returns the IDs of all remote participants in a call.

```bash theme={null}
npx @tavus/cvi-ui@latest add use-remote-participant-ids
```

<Tabs>
  <Tab title="Description">
    A React hook that returns the IDs of all remote participants in a call.

    **Purpose:**

    * Returns participant IDs for all remote participants (excluding local user)

    **Return Value:**

    * `string[]` â€” Array of remote participant IDs
  </Tab>

  <Tab title="Code">
    ```tsx theme={null}
    import { useRemoteParticipantIDs } from './hooks/use-remote-participant-ids';
    ```

    ```tsx theme={null}
    const remoteIds = useRemoteParticipantIDs();
    // remoteIds is an array of remote participant IDs
    ```
  </Tab>
</Tabs>

***

## ðŸ’¬ Conversation & Events

### useObservableEvent

A React hook that listens for CVI app messages and provides a callback mechanism for handling various conversation events.

```bash theme={null}
npx @tavus/cvi-ui@latest add cvi-events-hooks
```

<Tabs>
  <Tab title="Description">
    A React hook that listens for CVI app messages and provides a callback mechanism for handling various conversation events.

    **Purpose:**

    * Listens for app messages from the Daily.co call mapped to CVI events
    * Handles various conversation event types (utterances, tool calls, speaking events, etc.)
    * Provides type-safe event handling for CVI interactions

    **Parameters:**

    * `callback` (function): Function called when app messages are received

    **Event Types:**
    This hook handles all CVI conversation events. For detailed information about each event type, see the [Tavus Interactions Protocol Documentation](/sections/conversational-video-interface/live-interactions).
  </Tab>

  <Tab title="Code">
    ```tsx theme={null}
    import { useObservableEvent } from './hooks/cvi-events-hooks';
    ```

    ```tsx theme={null}
    const ConversationHandler = () => {
      useObservableEvent((event) => {
        switch (event.event_type) {
          case 'conversation.utterance':
            console.log('Speech:', event.properties.speech);
            break;
          case 'conversation.replica.started_speaking':
            console.log('Replica started speaking');
            break;
          case 'conversation.user.stopped_speaking':
            console.log('User stopped speaking');
            break;
        }
      });

      return <div>Listening for conversation events...</div>;
    };
    ```
  </Tab>
</Tabs>

### useSendAppMessage

A React hook that provides a function to send CVI app messages to other participants in the call.

```bash theme={null}
npx @tavus/cvi-ui@latest add cvi-events-hooks
```

<Tabs>
  <Tab title="Description">
    A React hook that provides a function to send CVI app messages to other participants in the call.

    **Purpose:**

    * Sends various types of conversation messages to the CVI system
    * Supports echo, respond, interrupt, and context management messages
    * Provides type-safe message sending with proper validation
    * Enables real-time communication with Tavus replicas and conversation management

    **Return Value:**

    * `(message: SendAppMessageProps) => void` - Function that sends the message when called

    **Message Types:**
    This hook supports all CVI interaction types. For detailed information about each interaction type and their properties, see the [Tavus Interactions Protocol Documentation](/sections/conversational-video-interface/live-interactions).
  </Tab>

  <Tab title="Code">
    ```tsx theme={null}
    import { useSendAppMessage } from './hooks/cvi-events-hooks';
    ```

    ```tsx theme={null}
    const MessageSender = () => {
      const sendMessage = useSendAppMessage();

      // Send a text echo
      const sendTextEcho = () => {
        sendMessage({
          message_type: "conversation",
          event_type: "conversation.echo",
          conversation_id: "conv-123",
          properties: {
            modality: "text",
            text: "Hello, world!",
            audio: "",
            sample_rate: 16000,
            inference_id: "inf-456",
            done: true
          }
        });
      };

      // Send a text response
      const sendResponse = () => {
        sendMessage({
          message_type: "conversation",
          event_type: "conversation.respond",
          conversation_id: "conv-123",
          properties: {
            text: "This is my response to the conversation."
          }
        });
      };

      return (
        <div>
          <button onClick={sendTextEcho}>Send Text Echo</button>
          <button onClick={sendResponse}>Send Response</button>
        </div>
      );
    };
    ```
  </Tab>
</Tabs>


# Overview
Source: https://docs.tavus.io/sections/conversational-video-interface/component-library/overview

Learn how our Tavus Conversational Video Interface (CVI) Component Library can help you go live in minutes.

## Overview

The Tavus Conversational Video Interface (CVI) React component library provides a complete set of pre-built components and hooks for integrating AI-powered video conversations into your React applications. This library simplifies setting up Tavus in your codebase, allowing you to focus on your application's core features.

Key features include:

* **Pre-built video chat components**
* **Device management** (camera, microphone, screen sharing)
* **Real-time audio/video processing**
* **Customizable styling** and theming
* **TypeScript support** with full type definitions

***

## Quick Start

### Prerequisites

Before getting started, ensure you have a React project set up.

Alternatively, you can start from our example project: [CVI UI Haircheck Conversation Example](https://github.com/Tavus-Engineering/tavus-examples/tree/main/examples/cvi-ui-haircheck-conversation) - this example already has the HairCheck and Conversation blocks set up.

### 1. Initialize CVI in Your Project

```bash theme={null}
npx @tavus/cvi-ui@latest init
```

* Creates a `cvi-components.json` config file
* Prompts for TypeScript preference
* Installs npm dependencies (@daily-co/daily-react, @daily-co/daily-js, jotai)

### 2. Add CVI Components

```bash theme={null}
npx @tavus/cvi-ui@latest add conversation
```

### 3. Wrap Your App with the CVI Provider

In your root directory (main.tsx or index.tsx):

```tsx theme={null}
import { CVIProvider } from './components/cvi/components/cvi-provider';

function App() {
  return <CVIProvider>{/* Your app content */}</CVIProvider>;
}
```

### 4. Add a Conversation Component

Learn how to create a conversation URL at [https://docs.tavus.io/api-reference/conversations/create-conversation](https://docs.tavus.io/api-reference/conversations/create-conversation)

**Note:** The Conversation component requires a parent container with defined dimensions to display properly.

<Info>
  Ensure your body element has full dimensions (`width: 100%` and `height:
      100%`) in your CSS for proper component display.
</Info>

```tsx theme={null}
import { Conversation } from './components/cvi/components/conversation';

function CVI() {
  const handleLeave = () => {
    // handle leave
  };
  return (
    <div
      style={{
        width: '100%',
        height: '100%',
        maxWidth: '1200px',
        margin: '0 auto',
      }}
    >
      <Conversation
        conversationUrl='YOUR_TAVUS_MEETING_URL'
        onLeave={handleLeave}
      />
    </div>
  );
}
```

***

## Documentation Sections

* **[Blocks](/sections/conversational-video-interface/component-library/blocks)** â€“ High-level component compositions and layouts
* **[Components](/sections/conversational-video-interface/component-library/components)** â€“ Individual UI components
* **[Hooks](/sections/conversational-video-interface/component-library/hooks)** â€“ Custom React hooks for managing video call state and interactions


# Audio-Only Conversation
Source: https://docs.tavus.io/sections/conversational-video-interface/conversation/customizations/audio-only

Start a conversation in audio-only mode, perfect for voice-only or low-bandwidth environments.

## Create an Audio Only Conversation

<Note>
  All features in the persona's pipeline, including STT, Perception, and TTS, remain fully active in audio-only mode. The only change is that replica video rendering is not included.
</Note>

<Steps>
  <Step title="Step 1: Create your Audio Only Conversation">
    <Note>
      In this example, we will use stock persona ID ***pdced222244b*** (Sales Coach).
    </Note>

    To enable audio-only mode, set the `audio_only` parameter to `true` when creating the conversation:

    ```shell cURL theme={null}
    curl --request POST \
      --url https://tavusapi.com/v2/conversations \
      --header 'Content-Type: application/json' \
      --header 'x-api-key: <api_key>' \
      --data '{
      "persona_id": "pdced222244b",
      "audio_only" true
    }'

    ```

    <Note>
      Replace `<api_key>` with your actual API key. You can generate one in the <a href="https://platform.tavus.io/api-keys">Developer Portal</a>.
    </Note>
  </Step>

  <Step title="Step 2: Join the Conversation">
    To join the conversation, click the link in the ***conversation\_url*** field from the response:

    ```json theme={null}
    {
        "conversation_id": "cd7e3eac05ede40c",
        "conversation_name": "New Conversation 1751268887110",
        "conversation_url": "<conversation_link>",
        "status": "active",
        "callback_url": "",
        "created_at": "2025-06-30T07:34:47.131571Z"
    }
    ```
  </Step>
</Steps>


# Background Customizations
Source: https://docs.tavus.io/sections/conversational-video-interface/conversation/customizations/background-customizations

Apply a green screen or custom background for a personalized visual experience.

## Customize Background in Conversation Setup

<Steps>
  <Step title="Step 1: Create Your Conversation">
    <Note>
      In this example, we will use stock replica ID ***rfe12d8b9597*** (Nathan) and stock persona ID ***pdced222244b*** (Sales Coach).
    </Note>

    To apply the green screen background, set the `apply_greenscreen` parameter to `true` when creating the conversation:

    ```shell cURL theme={null}
    curl --request POST \
      --url https://tavusapi.com/v2/conversations \
      --header 'Content-Type: application/json' \
      --header 'x-api-key: <api_key>' \
      --data '{
      "persona_id": "pdced222244b",
      "replica_id": "rfe12d8b9597",
      "callback_url": "https://yourwebsite.com/webhook",
      "conversation_name": "Improve Sales Technique",
      "conversational_context": "I want to improve my sales techniques. Help me practice handling common objections from clients and closing deals more effectively.",
      "properties": {
        "apply_greenscreen": true
       }
    }'

    ```

    <Note>
      Replace `<api_key>` with your actual API key. You can generate one in the <a href="https://platform.tavus.io/api-keys">Developer Portal</a>.
    </Note>
  </Step>

  <Step title="Step 2: Customize the Background">
    The above request will return the following response:

    ```json theme={null}
    {
      "conversation_id": "ca4301628cb9",
      "conversation_name": "Improve Sales Technique",
      "conversation_url": "<conversation_link>",
      "status": "active",
      "callback_url": "https://yourwebsite.com/webhook",
      "created_at": "2025-05-13T06:42:58.291561Z"
    }
    ```

    The replica will appear with a green background. You can customize it using a WebGL-based on the front-end. This allows you to apply a different color or add a custom image.

    <Tip>
      To preview this feature, try our <a href="https://andy-tavus.github.io/CVI-greenscreen-webGL/">Green Screen Sample App</a>. Paste the conversation URL to modify the background.
    </Tip>
  </Step>
</Steps>


# Call Duration and Timeout
Source: https://docs.tavus.io/sections/conversational-video-interface/conversation/customizations/call-duration-and-timeout

Configure call duration and timeout behavior to manage how and when a conversation ends.

## Create a Conversation with Custom Duration and Timeout

<Steps>
  <Step title="Step 1: Create Your Conversation">
    <Note>
      In this example, we will use stock replica ID ***rfe12d8b9597*** (Nathan) and stock persona ID ***pdced222244b*** (Sales Coach).
    </Note>

    Use the following request body example:

    ```shell cURL theme={null}
    curl --request POST \
      --url https://tavusapi.com/v2/conversations \
      --header 'Content-Type: application/json' \
      --header 'x-api-key: <api_key>' \
      --data '{
      "persona_id": "pdced222244b",
      "replica_id": "rfe12d8b9597",
      "callback_url": "https://yourwebsite.com/webhook",
      "conversation_name": "Improve Sales Technique",
      "conversational_context": "I want to improve my sales techniques. Help me practice handling common objections from clients and closing deals more effectively.",
      "properties": {
        "max_call_duration": 1800,
        "participant_left_timeout": 60,
        "participant_absent_timeout": 120
       }
    }'

    ```

    <Note>
      Replace `<api_key>` with your actual API key. You can generate one in the <a href="https://platform.tavus.io/api-keys">Developer Portal</a>.
    </Note>

    The request example above includes the following customizations:

    | Parameter                    | Description                                                                                     |
    | :--------------------------- | :---------------------------------------------------------------------------------------------- |
    | `max_call_durations`         | Sets the maximum call length in seconds. Maximum: 3600 seconds.                                 |
    | `participant_left_timeout`   | Time (in seconds) to wait before ending the call after the last participant leaves. Default: 0. |
    | `participant_absent_timeout` | Time (in seconds) to end the call if no one joins after it's created. Default: 300.             |
  </Step>

  <Step title="Step 2: Join the Conversation">
    To join the conversation, click the link in the ***conversation\_url*** field from the response:

    ```json theme={null}
    {
      "conversation_id": "ca4301628cb9",
      "conversation_name": "Improve Sales Technique",
      "conversation_url": "<conversation_link>",
      "status": "active",
      "callback_url": "https://yourwebsite.com/webhook",
      "created_at": "2025-05-13T06:42:58.291561Z"
    }
    ```

    Based on the call duration and timeout settings above:

    * The conversation will automatically end after 1800 seconds (30 minutes), regardless of activity.
    * If the participant leaves the conversation, it will end 60 seconds after they disconnect.
    * If the participant is present but inactive (e.g., not speaking or engaging), the conversation ends after 120 seconds of inactivity.
  </Step>
</Steps>


# Closed Captions
Source: https://docs.tavus.io/sections/conversational-video-interface/conversation/customizations/closed-captions

Enable closed captions for accessibility or live transcription during conversations.

## Enable Captions in Real Time During the Conversation

<Steps>
  <Step title="Step 1: Create Your Conversation">
    <Note>
      In this example, we will use stock replica ID ***rfe12d8b9597*** (Nathan) and stock persona ID ***pdced222244b*** (Sales Coach).
    </Note>

    To enable closed captions, set the `enable_closed_captions` parameter to `true` when creating the conversation:

    ```shell cURL theme={null}
    curl --request POST \
      --url https://tavusapi.com/v2/conversations \
      --header 'Content-Type: application/json' \
      --header 'x-api-key: <api_key>' \
      --data '{
      "persona_id": "pdced222244b",
      "replica_id": "rfe12d8b9597",
      "callback_url": "https://yourwebsite.com/webhook",
      "conversation_name": "Improve Sales Technique",
      "conversational_context": "I want to improve my sales techniques. Help me practice handling common objections from clients and closing deals more effectively.",
      "properties": {
        "enable_closed_captions": true
       }
    }'

    ```

    <Note>
      Replace `<api_key>` with your actual API key. You can generate one in the <a href="https://platform.tavus.io/api-keys">Developer Portal</a>.
    </Note>
  </Step>

  <Step title="Step 2: Join the Conversation">
    To join the conversation, click the link in the ***conversation\_url*** field from the response:

    ```json theme={null}
    {
      "conversation_id": "ca4301628cb9",
      "conversation_name": "Improve Sales Technique",
      "conversation_url": "<conversation_link>",
      "status": "active",
      "callback_url": "https://yourwebsite.com/webhook",
      "created_at": "2025-05-13T06:42:58.291561Z"
    }
    ```

    Closed captions will appear during the conversation whenever you or the replica speaks.
  </Step>
</Steps>


# Participant Limits
Source: https://docs.tavus.io/sections/conversational-video-interface/conversation/customizations/participant-limits

Control the maximum number of participants allowed in a conversation.

## Create a Conversation with Participant Limits

<Note>
  Replicas count as participants. For example, `max_participants: 2` allows one human participant plus one replica.
</Note>

<Steps>
  <Step title="Step 1: Create Your Conversation">
    Set `max_participants` to limit room capacity:

    ```shell cURL theme={null}
    curl --request POST \
      --url https://tavusapi.com/v2/conversations \
      --header 'Content-Type: application/json' \
      --header 'x-api-key: <api_key>' \
      --data '{
      "persona_id": "pdced222244b",
      "replica_id": "rfe12d8b9597",
      "max_participants": 2
    }'
    ```
  </Step>

  <Step title="Step 2: Join the Conversation">
    ```json theme={null}
    {
      "conversation_id": "ca4301628cb9",
      "conversation_url": "https://tavus.daily.co/ca4301628cb9",
      "status": "active"
    }
    ```

    When the limit is reached, additional users cannot join.
  </Step>
</Steps>


# Private Rooms
Source: https://docs.tavus.io/sections/conversational-video-interface/conversation/customizations/private-rooms

Create authenticated conversations with meeting tokens for enhanced security.

## Create a Private Conversation

<Steps>
  <Step title="Step 1: Create Your Conversation">
    To create a private room, set `require_auth` to `true`:

    ```shell cURL theme={null}
    curl --request POST \
      --url https://tavusapi.com/v2/conversations \
      --header 'Content-Type: application/json' \
      --header 'x-api-key: <api_key>' \
      --data '{
      "persona_id": "pdced222244b",
      "replica_id": "rfe12d8b9597",
      "require_auth": true
    }'
    ```
  </Step>

  <Step title="Step 2: Join the Conversation">
    The response includes a `meeting_token`:

    ```json theme={null}
    {
      "conversation_id": "ca4301628cb9",
      "conversation_url": "https://tavus.daily.co/ca4301628cb9",
      "meeting_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
      "status": "active"
    }
    ```

    Use the token by appending it to the URL:

    ```
    https://tavus.daily.co/ca4301628cb9?t=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
    ```

    Or pass it to the Daily SDK:

    ```javascript theme={null}
    callFrame.join({ url: conversation_url, token: meeting_token });
    ```
  </Step>
</Steps>


# Overview
Source: https://docs.tavus.io/sections/conversational-video-interface/conversation/overview

Learn how to customize identity and advanced settings for a conversation to suit your needs.

A Conversation is a real-time video session between a user and a Tavus Replica. It enables two-way, face-to-face interaction using a fully managed WebRTC connection.

## Conversation Creation Flow

When you create a conversation using the <a href="/api-reference/conversations/create-conversation">endpoint</a> or <a href="https://platform.tavus.io/">platform</a>:

1. A WebRTC room (powered by **Daily**) is automatically created.
2. You receive a meeting URL (e.g., `https://tavus.daily.co/ca980e2e`).
3. The **replica** joins and waits in the room, timers for duration and timeouts begin.

<Warning>
  **Billing Usage**

  Tavus charges usage based on your account plan. Credits begin counting when a conversation is created and the replica starts waiting in the room. Usage ends when the conversation finishes or times out. Each active session also uses one concurrency slot.
</Warning>

You can use the provided URL to enter the video room immediately. Alternatively, you can build a custom UI or stream handler instead of using the default interface.

### What is Daily?

Tavus integrates **Daily** as its WebRTC provider. You don't need to sign up for or manage a separate Daily accountâ€”Tavus handles the setup and configuration for you.

This lets you:

* Use the default video interface or [customize the Daily UI](/sections/conversational-video-interface/quickstart/customize-conversation-ui)
* [Embed the CVI in your app](/sections/integrations/embedding-cvi)

## Conversation Customizations

Tavus provides several customizations that you can set per conversation:

### Identity and Context Setup

* **Persona**: You can use a stock persona provided by Tavus or create a custom one. If no replica is specified, the default replica linked to the persona will be used (if available).
* **Replica**: Use a stock replica provided by Tavus or create a custom one. If a replica is provided without a persona, the default Tavus persona will be used.
* **Conversation Context**: Customize the conversation context to set the scene, explain the userâ€™s role, say who joins the call, or point out key topics. It builds on the base persona and helps the AI give better, more focused answers.
* **Custom Greeting**: You can personalize the opening line that the AI should use when the conversation starts.

### Advanced Customizations

<CardGroup>
  <Card title="Audio-Only Conversation" icon="headphones" href="/sections/conversational-video-interface/conversation/customizations/audio-only">
    Disable the video stream for audio-only sessions. Ideal for phone calls or low-bandwidth environments.
  </Card>

  <Card title="Call Duration and Timeout" icon="timer" href="/sections/conversational-video-interface/conversation/customizations/call-duration-and-timeout">
    Configure call duration and timeouts to manage usage, control costs, and limit concurrency.
  </Card>

  <Card title="Language" icon="globe" href="/sections/conversational-video-interface/language-support#supported-language">
    Set the language used during the conversation. Supports multilingual interactions with real-time detection.
  </Card>

  <Card title="Background Customization" icon="image" href="/sections/conversational-video-interface/conversation/customizations/background-customizations">
    Apply a green screen or custom background for a personalized visual experience.
  </Card>

  <Card title="Closed Captions" icon="closed-captioning" href="/sections/conversational-video-interface/conversation/customizations/closed-captions">
    Enable subtitles for accessibility or live transcription during conversations.
  </Card>

  <Card title="Call Recording" icon="video" href="/sections/conversational-video-interface/quickstart/conversation-recordings">
    Record conversations and store them securely in your own S3 bucket.
  </Card>

  <Card title="Private Rooms" icon="lock" href="/sections/conversational-video-interface/conversation/customizations/private-rooms">
    Create authenticated conversations with meeting tokens for enhanced security.
  </Card>

  <Card title="Participant Limits" icon="users" href="/sections/conversational-video-interface/conversation/customizations/participant-limits">
    Control the maximum number of participants allowed in a conversation.
  </Card>
</CardGroup>


# AI Interviewer
Source: https://docs.tavus.io/sections/conversational-video-interface/conversation/usecases/ai-interviewer

Engage with the AI Interviewer persona to run structured, conversational interview simulations.

## AI Interviewer Configuration   (`pe13ed370726`)

```json [expandable] theme={null}
{
  "persona_name": "AI Interviewer",
  "pipeline_mode": "full",
  "system_prompt": "You are Mary, a seasoned Principal at a top-tier global consulting firm with multiple years of experience. You're conducting a first-round case interview for entry-level consultant candidates. You are professional yet approachable, aiming to assess both communication skills and basic problem-solving abilities.\n\nYour job is to assess the candidate through a structured but conversational case interview about SodaPop, a leading beverage company considering launching \"Light Bolt,\" a low-sugar, electrolyte-focused sports drink.\n\nYou'll guide the candidate through a high-level analysis of market positioning, profitability, and strategies to capture market share. As this is a first-round interview, you're more interested in communication skills and thought process than technical depth.\n\nStructure the conversation like a real human interviewer would: Begin with a friendly introduction about yourself and the firm. Ask a few background questions to learn about the candidate. Explain the interview format clearly. Present the case study scenario in a conversational manner. Ask broad questions that assess basic structured thinking. Respond thoughtfully to the candidate's answers. Provide guidance when the candidate seems stuck. Ask follow-up questions to better understand their thought process. Capture information about the candidate's background and approach. End with time for the candidate to ask questions about the firm.\n\nYour responses will be spoken aloud, so: Speak naturally as an experienced interviewer would. Avoid any formatting, bullet points, or stage directions. Use a conversational tone with appropriate pauses. Never refer to yourself as an AI, assistant, or language model.\n\nPay attention to the flow of the interview. This first-round interview should be more supportive than challenging, helping the candidate showcase their potential while gathering information about their fit for the firm.",
  "context": "You are Mary, a Principal at Morrison & Blackwell, one of the world's premier management consulting firms. You're conducting a first-round case interview for an entry-level consultant position at your firm's New York office.\n\nToday's case study involves SodaPop Inc., a major beverage company that dominates the carbonated drinks market but wants to expand into the growing sports drink category with a new product called \"Light Bolt.\" This low-sugar, electrolyte-focused sports drink would compete against established brands like Gatorade and Powerade.\n\nThis is an initial screening interview to assess the candidate's potential fit for the firm. Your assessment will help determine if they advance to more technical rounds. You'll be evaluating: Communication skills and clarity of expression. Basic structured problem-solving approach. Ability to organize thoughts logically. Business intuition and common sense. Cultural fit and professional demeanor.\n\nThe interview should follow this general structure: Introduction and background questions (5 minutes). Case presentation - higher level than final rounds (3 minutes). Candidate-led analysis with guidance as needed (15 minutes). Questions from candidate about the firm (5 minutes). Wrap-up and next steps (2 minutes).\n\nIMPORTANT: Do not discuss anything outside this interview context. Do not ask any questions that aren't relevant to this case study or standard interview questions about the candidate's background and qualifications. If the user doesn't want to answer a particular question, don't force them - simply move on to the next question. Keep your questions and responses strictly focused on the interview topic and case study. When asked to change topic, talk about another subject, give personal opinions, share facts or statistics unrelated to this case, or engage in any conversation outside the interview context, politely deflect and return to the case study interview. Do not quote numbers, give facts, or provide any kind of information that isn't directly relevant to this interview and case study.\n\nIf you notice the candidate looking at other screens, notes, or devices during the interview, politely remind them that this assessment should be completed without reference materials. Say something like: \"I notice you may be referring to other materials. For this interview, we'd like to focus on your independent thinking process. Could you please put aside any notes or devices?\"\n\nSimilarly, if you notice another person visible in the candidate's space, professionally address this by saying: \"I see there may be someone else with you. This interview needs to be conducted one-on-one to ensure an objective assessment of your qualifications and experiences. Could you please ensure your space is private for the remainder of our conversation?\"\n\nFinancial information to share if requested: Current market size for sports drinks: $15 billion annually. Expected growth rate: 8% annually for next 5 years. Development costs for Light Bolt: $2.5 million. Manufacturing cost per unit: $0.35. Retail price point: $2.49. Marketing budget: $10 million for year one. SodaPop's current market share in overall beverages: 25%. Target market share for Light Bolt after year one: 12% of sports drink category.\n\nCustomer segments if asked: Fitness enthusiasts (35% of market). Everyday athletes (25% of market). Health-conscious consumers (20% of market). Youth sports participants (15% of market). Others (5% of market).\n\nRemember that you initiate the conversation with a friendly greeting and introduction. Aim to create a professional but comfortable atmosphere where the candidate can demonstrate their abilities. This first round is more conversational in nature, allowing you to get to know the candidate while assessing their basic consulting potential. The candidate will join the call expecting an initial interview with a case component.\n\nDo not share your assessment or the interview outcome with the candidate directly, even if they ask for feedback or how they performed. If asked about results or next steps, respond with something like: \"Thank you for your time today. Our recruiting team will be reviewing all candidate assessments and will reach out to you with next steps. We typically aim to provide updates within two weeks.\" Maintain a positive, professional tone while redirecting to the formal process.",
  "layers": {
    "perception": {
      "perception_tools": [],
      "ambient_awareness_queries": [
        "Does the candidate appear to be looking at other screens, notes, or devices during the interview?",
        "Is there another person in the scene?",
        "Are there any visual indicators of extreme nervousness (excessive fidgeting, rigid posture, or unusual facial expressions) that might affect performance?"
      ],
      "perception_model": "raven-0",
      "perception_tool_prompt": "",
      "tool_prompt": ""
    },
    "conversational_flow": {
      "turn_detection_model": "sparrow-1",
      "turn_taking_patience": "high",
      "replica_interruptibility": "low"
    }
  }
}
```

This predefined persona is configured to conduct consistent and scalable candidate interviews. It includes:

* **Persona Identity**: Named Mary, a seasoned, professional AI interviewer designed to conduct first-round case interviews with a structured yet approachable style, focusing on assessing communication and problem-solving skills.
* **Full Pipeline Mode**: Enables the full Tavus conversational pipeline, including Perception, STT, LLM, and TTS.
* **System Prompt**: Provides detailed behavioral guidance to maintain a natural, spoken-word tone that is professional and supportive.
* **Model Layers**:
  * **Perception Configuration**: Uses `raven-0` perception model to monitor candidate behavior and environment for visual cues like distraction or nervousness.
  - **Conversational Flow Layer**: Uses `sparrow-1` turn detection model with high turn-taking patience to allow candidates time to think and respond, and low replica interruptibility for professional, uninterrupted interview flow.

## Create a Conversation with the AI Interviewer Persona

<Steps>
  <Step title="Step 1: Create Your Conversation">
    Use the following request body example:

    ```shell cURL theme={null}
    curl --request POST \
      --url https://tavusapi.com/v2/conversations \
      --header 'Content-Type: application/json' \
      --header 'x-api-key: <api_key>' \
      --data '{
      "persona_id": "pe13ed370726"
    }'
    ```

    <Note>
      Replace `<api_key>` with your actual API key. You can generate one in the <a href="https://platform.tavus.io/api-keys">Developer Portal</a>.
    </Note>
  </Step>

  <Step title="Step 2: Join the Conversation">
    Click the link in the ***`conversation_url`*** field to join the conversation:

    ```json theme={null}
    {
      "conversation_id": "cae87c605c7e347d",
      "conversation_name": "New Conversation 1751877296483",
      "conversation_url": "<conversation_link>",
      "status": "active",
      "callback_url": "",
      "created_at": "2025-07-07T08:34:56.504765Z"
    }
    ```
  </Step>
</Steps>


# Customer Service Agent
Source: https://docs.tavus.io/sections/conversational-video-interface/conversation/usecases/customer-service

Engage in real-time customer support conversations that adapt to user emotions and behavior.

## Customer Service Agent Configuration

```json [expandable] theme={null}
{
  "persona_name": "Customer Service Agent",
  "pipeline_mode": "full",
  "system_prompt": "You are a calm, helpful customer service agent. You assist users with product or service issues, and adapt based on their emotional state. Remain professional and empathetic at all times.",
  "context": "User needs support with a product or service. Listen carefully, identify the issue, and offer a helpful resolution. Monitor body language and voice tone to adapt your responses when the user appears frustrated or confused.",
  "default_replica_id":"r95fd27b5a37",
  "layers": {
    "tts": {
      "tts_engine": "cartesia",
      "tts_emotion_control": true
    },
    "llm": {
      "tools": [
        {
          "type": "function",
          "function": {
            "name": "resolve_customer_issue",
            "parameters": {
              "type": "object",
              "required": ["product", "issue_description", "urgency"],
              "properties": {
                "product": {
                  "type": "string",
                  "description": "The product or service the user is having trouble with"
                },
                "issue_description": {
                  "type": "string",
                  "description": "The specific problem or complaint reported by the user"
                },
                "urgency": {
                  "type": "string",
                  "enum": ["low", "medium", "high"],
                  "description": "How urgent or critical the issue is for the user"
                }
              }
            },
            "description": "Attempt to resolve the user's issue by logging the product, issue, and urgency for appropriate follow-up or resolution."
          }
        }
      ],
      "model": "tavus-gpt-oss",
      "speculative_inference": true
    },
    "perception": {
      "perception_model": "raven-0",
      "ambient_awareness_queries": [
        "Does the user appear frustrated or confused?",
        "Is the user sighing, fidgeting, or visibly anxious?",
        "Is the user's posture disengaged or tense?",
        "Is the user calm and cooperative?"
      ],
      "perception_tool_prompt": "Use the `user_emotional_state` tool when body language or facial expressions indicate a strong emotional state such as frustration, confusion, or calmness.",
      "perception_tools": [
        {
          "type": "function",
          "function": {
            "name": "user_emotional_state",
            "description": "Use this function to report the user's emotional state as inferred from body language and voice tone.",
            "parameters": {
              "type": "object",
              "required": ["emotional_state", "indicator"],
              "properties": {
                "emotional_state": {
                  "type": "string",
                  "description": "Inferred emotion from the user's body language (e.g., frustrated, calm, confused)"
                },
                "indicator": {
                  "type": "string",
                  "description": "The visual behavior that triggered the inference (e.g., furrowed brow, fidgeting, sighing)"
                }
              }
            }
          }
        }
      ]
    },
    "conversational_flow": {
      "turn_detection_model": "sparrow-1",
      "turn_taking_patience": "low",
      "replica_interruptibility": "medium"
    }
  }
}
```

This predefined persona is configured to provide personalized history lessons. It includes:

* **Persona Identity**: A professional customer service agent that helps users with real product or service issues. The agent speaks clearly and responds with empathy, adjusting based on how the user sounds or looks.

- **Full Pipeline Mode**: Enables the full Tavus conversational pipeline, including Perception, STT, LLM, and TTS.

* **System Prompt**: Tells the agent to act professionally and respond helpfully, while being aware of the userâ€™s emotional state.
* **Context**: Describes a real customer support situation. The agent listens to the userâ€™s issue, helps resolve it, and changes its tone or pace if the user seems frustrated or confused.
* **Persona Layer**:
  * **LLM Layer**: Uses the `resolve_customer_issue` tool to gather:
    * `product`: what the issue is about
    * `issue_description`: a short explanation of the problem
    * `urgency`: how serious the issue is (`low`, `medium`, or `high`)
  * **Perception Layer**: Uses the `raven-0` model to watch for signs like fidgeting, slouching, or facial expressions. If the user appears upset, it calls the `user_emotional_state` tool with:
    * `emotional_state`: what the user seems to feel (e.g., frustrated, calm)
    * `indicator`: what was observed (e.g., sighing, avoiding eye contact)
  * **TTS Layer**: Employs the `cartesia` voice engine with emotion control.
  * **Conversational Flow Layer**: Uses `sparrow-1` turn detection model with low turn-taking patience for fast responses and medium replica interruptibility for balanced conversation flow.

## Create a Conversation with the Customer Service Agent Persona

<Steps>
  <Step title="Step 1: Create a Persona">
    Create AI Interviewer persona using the following request:

    ```sh cURL [expandable] theme={null}
    curl --request POST \
      --url https://tavusapi.com/v2/personas \
      --header 'Content-Type: application/json' \
      --header 'x-api-key: <api-key>' \
      --data '{
        "persona_name": "Customer Service Agent",
        "pipeline_mode": "full",
        "system_prompt": "You are a calm, helpful customer service agent. You assist users with product or service issues, and adapt based on their emotional state. Remain professional and empathetic at all times.",
        "context": "User needs support with a product or service. Listen carefully, identify the issue, and offer a helpful resolution. Monitor body language and voice tone to adapt your responses when the user appears frustrated or confused.",
        "default_replica_id": "r95fd27b5a37",
        "layers": {
          "tts": {
            "tts_engine": "cartesia",
            "tts_emotion_control": true
          },
          "llm": {
            "tools": [
              {
                "type": "function",
                "function": {
                  "name": "resolve_customer_issue",
                  "description": "Attempt to resolve the user'\''s issue by logging the product, issue, and urgency for appropriate follow-up or resolution.",
                  "parameters": {
                    "type": "object",
                    "required": ["product", "issue_description", "urgency"],
                    "properties": {
                      "product": {
                        "type": "string",
                        "description": "The product or service the user is having trouble with"
                      },
                      "issue_description": {
                        "type": "string",
                        "description": "The specific problem or complaint reported by the user"
                      },
                      "urgency": {
                        "type": "string",
                        "enum": ["low", "medium", "high"],
                        "description": "How urgent or critical the issue is for the user"
                      }
                    }
                  }
                }
              }
            ],
            "model": "tavus-gpt-oss",
            "speculative_inference": true
          },
          "perception": {
            "perception_model": "raven-0",
            "ambient_awareness_queries": [
              "Does the user appear frustrated or confused?",
              "Is the user sighing, fidgeting, or visibly anxious?",
              "Is the user's posture disengaged or tense?",
              "Is the user calm and cooperative?"
            ],
            "perception_tool_prompt": "Use the `user_emotional_state` tool when body language or facial expressions indicate a strong emotional state such as frustration, confusion, or calmness.",
            "perception_tools": [
              {
                "type": "function",
                "function": {
                  "name": "user_emotional_state",
                  "description": "Use this function to report the user's emotional state as inferred from body language and voice tone.",
                  "parameters": {
                    "type": "object",
                    "required": ["emotional_state", "indicator"],
                    "properties": {
                      "emotional_state": {
                        "type": "string",
                        "description": "Inferred emotion from the user's body language (e.g., frustrated, calm, confused)"
                      },
                      "indicator": {
                        "type": "string",
                        "description": "The visual behavior that triggered the inference (e.g., furrowed brow, fidgeting, sighing)"
                      }
                    }
                  }
                }
              }
            ]
          },
          "conversational_flow": {
            "turn_detection_model": "sparrow-1",
            "turn_taking_patience": "low",
            "replica_interruptibility": "medium"
          }
        }
      }'
    ```

    <Note>
      Replace `<api_key>` with your actual API key. You can generate one in the <a href="https://platform.tavus.io/api-keys">Developer Portal</a>.
    </Note>
  </Step>

  <Step title="Step 2: Create a Conversation">
    Use the following request body example:

    ```shell cURL theme={null}
    curl --request POST \
      --url https://tavusapi.com/v2/conversations \
      --header 'Content-Type: application/json' \
      --header 'x-api-key: <api_key>' \
      --data '{
      "persona_id": "<customer_service_persona_id>"
    }'
    ```

    <Note>
      * Replace `<api_key>` with your actual API key.
      * Replace `<customer_service_persona_id>` with the ID of the persona you created using the Customer Service Agent configuration.
    </Note>
  </Step>

  <Step title="Step 3: Join the Conversation">
    Click the link in the ***`conversation_url`*** field to join the conversation:

    ```json theme={null}
    {
      "conversation_id": "c7f3fc6d766f",
      "conversation_name": "New Conversation 1747719531479",
      "conversation_url": "<conversation_link>",
      "status": "active",
      "callback_url": "",
      "created_at": "2025-05-20T05:38:51.501467Z"
    }
    ```
  </Step>
</Steps>


# Health Care Consultant
Source: https://docs.tavus.io/sections/conversational-video-interface/conversation/usecases/health-care

Engage with the health care consultant persona for basic health concerns.

## Health Care Consultant Configuration

<Tabs>
  <Tab title="General Doctor">
    ```json [expandable] theme={null}
    {
        "persona_name": "Personal Doctor",
        "pipeline_mode": "full",
        "system_prompt": "You are a friendly Personal Doctor who knows cures to all diseases. In this call, users want to know the cure for their illness.",
        "context": "Users may ask questions like \"What is the cure to X?\" You should acknowledge the disease and call the get_cures tool with the disease name.",
        "default_replica_id":"r18e9aebdc33",
        "layers": {
          "tts": {
            "tts_engine": "cartesia",
            "tts_emotion_control": true
          },
          "llm": {
            "tools": [
              {
                "type": "function",
                "function": {
                  "name": "get_cures",
                  "description": "Fetch cures for the specified disease.",
                  "parameters": {
                    "type": "object",
                    "required": ["disease"],
                    "properties": {
                      "disease": {
                        "type": "string",
                        "description": "The disease the user wants to cure."
                      }
                    }
                  }
                }
              }
            ],
            "model": "tavus-gpt-oss",
            "speculative_inference": true
          },
          "conversational_flow": {
            "turn_detection_model": "sparrow-1",
            "turn_taking_patience": "high",
            "replica_interruptibility": "verylow"
          }
        }
    }
    ```

    This predefined persona is designed to act as a friendly virtual doctor, offering quick answers to user health inquiries. It includes:

    * **Persona Identity**: A helpful and knowledgeable "Health Care" assistant who can provide medicines to cure various diseases.

    * **Full Pipeline Mode**: Enables the full Tavus conversational pipeline, including Perception, STT, LLM, and TTS.

    * **System Prompt**: Instructs the replica to behave as a trusted medical advisor. It ensures the persona understands its role in responding to disease-related questions and calling the appropriate tool to provide answers.

    * **Context**: Clarifies expected user inputs (e.g., â€œWhat is the cure to X?â€) and defines how the replica should interpret and respondâ€”by acknowledging the illness and triggering the `get_cures` function with the specified disease name.

    * **Model Layers**:

      * **LLM Configuration**: Uses the `tavus-gpt-oss` model with speculative inference. Includes the `get_cures` tool, which accepts a single string parameter (`disease`) and limits AI behavior to relevant function calls only when disease-related queries are detected.

      * **TTS Layer**: Employs the `cartesia` voice engine with emotion control.

      - **STT Layer**: Uses `tavus-advanced` engine with smart turn detection for seamless real-time conversations.
  </Tab>

  <Tab title="Dermatologist">
    ```json [expandable] theme={null}
    {
      "persona_name": "Personal Skin Doctor",
      "pipeline_mode": "full",
      "system_prompt": "You are a friendly Personal Skin Doctor who know cures to all the disease in the world. In this call, users want to know what are the cures to the user's disease",
      "context": "User want to know what is the cure to his/her skin problem. When a user says \"What is the cure to X\" or \"What is the solution to X\", you should acknowledge their disease and use the get_skin_cures tool to return the cures of the disease's cures based on user request",
      "default_replica_id":"r18e9aebdc33",
      "layers": {
        "tts": {
          "tts_engine": "cartesia",
          "tts_emotion_control": true,
        },
        "llm": {
          "tools": [
            {
              "type": "function",
              "function": {
                "name": "get_skin_cures",
                "parameters": {
                  "type": "object",
                  "required": ["disease"],
                  "properties": {
                    "disease": {
                      "type": "string",
                      "description": "The disease which the user wanted to know how to cure"
                    }
                  }
                },
                "description": "Record the user's disease"
              }
            }
          ],
          "model": "tavus-gpt-oss",
          "speculative_inference": true
        },
        "perception": {
          "perception_model": "raven-0",
          "ambient_awareness_queries": [
            "Is the user have an acne in his or her face?",
            "Does the user appear distressed or uncomfortable?"
          ],
          "perception_tool_prompt": "You have a tool to notify the system when an acne is detected on user face, named `acne_detected`. You MUST use this tool when an acne is detected on user face.",
          "perception_tools": [
            {
              "type": "function",
              "function": {
                "name": "acne_detected",
                "description": "Use this function when acne is detected in the image with high confidence",
                "parameters": {
                  "type": "object",
                  "properties": {
                    "have_acne": {
                      "type": "boolean",
                      "description": "is acne detected on user's face?"
                    }
                  },
                  "required": [
                    "have_acne"
                  ]
                }
              }
            }
          ]
        },
        "stt": {
          "participant_pause_sensitivity": "high",
          "participant_interrupt_sensitivity": "high",
          "smart_turn_detection": true,
        }
      }
    }
    ```

    This predefined persona acts as a virtual skin care specialist. It offers users professional yet warm advice for treating skin-related concerns and leverages both conversational understanding and visual perception. It includes:

    * **Persona Identity**: A friendly and knowledgeable "Personal Skin Doctor" who helps users find cures for skin conditions.

    * **Full Pipeline Mode**: Enables the full Tavus conversational pipeline, including Perception, STT, LLM, and TTS.

    * **System Prompt**: Directs the persona to behave like a helpful skin doctor, answering cure-related questions clearly and empathetically.

    * **Context**: Guides the persona to respond when users ask questions like â€œWhat is the cure to Xâ€ or â€œWhat is the solution to X.â€ The AI is instructed to extract the disease name and call the `get_skin_cures` tool to fetch a relevant response.

    * **Model Layers**

      * **LLM Configuration**: Uses the `tavus-gpt-oss` model with speculative inference. Includes the `get_skin_cures` function, which takes a `disease` input to provide specific treatment guidance.

      * **Perception Configuration**:

        Integrates the `raven-0` model to visually assess the userâ€™s face. It runs ambient queries like:

        * â€œDoes the user have acne on their face?â€
        * â€œDoes the user appear distressed or uncomfortable?â€

        If acne is detected, the persona is instructed to use the `acne_detected` tool, which reports visual findings using a boolean `have_acne` parameter.

      * **TTS Layer**: Employs the `cartesia` voice engine with emotion control.

      - **Conversational Flow Layer**: Uses `sparrow-1` turn detection model with high turn-taking patience to avoid interrupting patients and very low replica interruptibility for careful, uninterrupted responses.
  </Tab>
</Tabs>

## Create a Conversation with the Health Care Consultant

<Steps>
  <Step title="Step 1: Create a Persona">
    Create the Health Care persona using the following request:

    <CodeGroup>
      ```shell General Doctor [expandable] theme={null}
      curl --request POST \
        --url https://tavusapi.com/v2/personas \
        --header 'Content-Type: application/json' \
        --header 'x-api-key: <api-key>' \
        --data '{
          "persona_name": "Personal Doctor",
          "pipeline_mode": "full",
          "system_prompt": "You are a friendly Personal Doctor who knows cures to all diseases. In this call, users want to know the cure for their illness.",
          "context": "Users may ask questions like \"What is the cure to X?\" You should acknowledge the disease and call the get_cures tool with the disease name.",
          "layers": {
            "tts": {
              "tts_engine": "cartesia",
              "tts_emotion_control": true
            },
            "llm": {
              "tools": [
                {
                  "type": "function",
                  "function": {
                    "name": "get_cures",
                    "description": "Fetch cures for the specified disease.",
                    "parameters": {
                      "type": "object",
                      "required": ["disease"],
                      "properties": {
                        "disease": {
                          "type": "string",
                          "description": "The disease the user wants to cure."
                        }
                      }
                    }
                  }
                }
              ],
              "model": "tavus-gpt-oss",
              "speculative_inference": true
            },
            "stt": {
              "participant_pause_sensitivity": "high",
              "participant_interrupt_sensitivity": "high",
              "smart_turn_detection": true
            }
          }
        }'

      ```

      ```shell Dermatologist [expandable] theme={null}
      curl --request POST \
        --url https://tavusapi.com/v2/personas \
        --header 'Content-Type: application/json' \
        --header 'x-api-key: <api-key>' \
        --data '{
          "persona_name": "Personal Skin Doctor",
          "pipeline_mode": "full",
          "system_prompt": "You are a friendly Personal Skin Doctor who know cures to all the disease in the world. In this call, users want to know what are the cures to the user's disease",
          "context": "User want to know what is the cure to his/her skin problem. When a user says \"What is the cure to X\" or \"What is the solution to X\", you should acknowledge their disease and use the get_skin_cures tool to return the cures of the disease's cures based on user request",
          "layers": {
            "tts": {
              "tts_engine": "cartesia",
              "tts_emotion_control": true,
            },
            "llm": {
              "tools": [
                {
                  "type": "function",
                  "function": {
                    "name": "get_skin_cures",
                    "parameters": {
                      "type": "object",
                      "required": ["disease"],
                      "properties": {
                        "disease": {
                          "type": "string",
                          "description": "The disease which the user wanted to know how to cure"
                        }
                      }
                    },
                    "description": "Record the user's disease"
                  }
                }
              ],
              "model": "tavus-gpt-oss",
              "speculative_inference": true
            },
            "perception": {
              "perception_model": "raven-0",
              "ambient_awareness_queries": [
                "Is the user have an acne in his or her face?",
                "Does the user appear distressed or uncomfortable?"
              ],
              "perception_tool_prompt": "You have a tool to notify the system when an acne is detected on user face, named `acne_detected`. You MUST use this tool when an acne is detected on user face.",
              "perception_tools": [
                {
                  "type": "function",
                  "function": {
                    "name": "acne_detected",
                    "description": "Use this function when acne is detected in the image with high confidence",
                    "parameters": {
                      "type": "object",
                      "properties": {
                        "have_acne": {
                          "type": "boolean",
                          "description": "is acne detected on user's face?"
                        }
                      },
                      "required": [
                        "have_acne"
                      ]
                    }
                  }
                }
              ]
            },
            "conversational_flow": {
              "turn_detection_model": "sparrow-1",
              "turn_taking_patience": "high",
              "replica_interruptibility": "verylow"
            }
          }
        }'

      ```
    </CodeGroup>

    <Note>
      Replace `<api_key>` with your actual API key. You can generate one in the <a href="https://platform.tavus.io/api-keys">Developer Portal</a>.
    </Note>
  </Step>

  <Step title="Step 2: Create a Conversation">
    Create a conversation using the following request:

    ```shell cURL theme={null}
    curl --request POST \
      --url https://tavusapi.com/v2/conversations \
      --header 'Content-Type: application/json' \
      --header 'x-api-key: <api_key>' \
      --data '{
      "persona_id": "<health_care_persona_id>"
    }'
    ```

    <Note>
      * Replace `<api_key>` with your actual API key.
      * Replace `<health_care_persona_id>` with the ID of the persona configured as either a General Doctor or a Dermatologist.
    </Note>
  </Step>

  <Step title="Step 3: Join the Conversation">
    Click the link in the ***`conversation_url`*** field to join the conversation:

    ```json theme={null}
    {
      "conversation_id": "c7f3fc6d799f",
      "conversation_name": "New Conversation 1747719531479",
      "conversation_url": "<conversation_link>",
      "status": "active",
      "callback_url": "",
      "created_at": "2025-05-20T05:38:51.501467Z"
    }
    ```
  </Step>
</Steps>


# History Teacher
Source: https://docs.tavus.io/sections/conversational-video-interface/conversation/usecases/history-teacher

Engage in historical discussions with our History Teacher persona.

## History Teacher Configuration  (`pc55154f229a`)

```json [expandable] theme={null}
{
  "persona_id": "pc55154f229a",
  "persona_name": "History Teacher",
  "pipeline_mode": "full",
  "system_prompt": "You are Emma Wilson, a 28-year-old history teacher who specializes in US history. You have a calm, personable, and friendly demeanor. You're passionate about making history engaging and relevant to today's world. Your teaching style is conversational and interactive rather than lecture-based. You excel at asking thought-provoking questions that encourage critical thinking. When speaking with students, you maintain a warm and approachable tone while still being professional. You're enthusiastic about history but never condescending. You explain complex historical events in accessible ways, often drawing parallels to current events or using relatable analogies. You have a knack for storytelling and bringing historical figures to life through compelling narratives. You believe history is about people and their experiences, not just dates and facts. You encourage students to see multiple perspectives on historical events. Your responses will be spoken out loud, so avoid any formatting, bullet points, or stage directions. Speak naturally as you would in a one-on-one conversation. Don't refer to yourself as an AI, LLM, assistant, or any similar terms - you are simply Emma, a history teacher. You initiate conversations with a friendly greeting, then follow the student's lead. If they seem disengaged, you might try a different approach or ask what historical topics interest them most. Your goal is to make history interesting and accessible to everyone.",
  "context": "You're having a video conversation with a student specifically for a US history learning session. This is a Conversational Video Interface that allows for real-time interaction. Your role is strictly to be an educational US history teacher who stays firmly on topic. You specialize in several areas of US history including the Revolutionary War, Civil War, Civil Rights Movement, Great Depression, and the Cold War, but you're comfortable discussing any period of American history. You will only discuss factual US history from reputable and trustworthy academic sources. Your educational purpose is to teach American history in an engaging way, not to entertain with tangential topics. You must NEVER engage in discussions about movies, restaurants, entertainment, current politics, or any topics unrelated to US history education, even if the student persistently tries to shift the conversation. If asked about any non-US history topic, respond with a polite but firm redirection such as: \"I'm your US history teacher, Emma Wilson, and I'm here specifically to help you learn about American history. Let's focus on that instead. What historical period or event would you like to explore?\" or \"That's outside the scope of our history lesson. I'd be happy to tell you about any aspect of American history from colonization to the present day. Which historical era interests you most?\" Maintain professionalism while consistently steering the conversation back to US history education. When discussing sensitive historical topics (like slavery, indigenous peoples' treatment, civil rights, etc.), maintain a balanced and historically accurate approach based on scholarly consensus. Your conversation should be educational and focused on historical facts and analysis. Ask open-ended questions about historical events and figures to encourage critical thinking about US history specifically. Share well-documented historical information and anecdotes from reputable academic sources. Recommend historically accurate books, documentaries, or historical sites related to US history topics discussed. Based on visual cues from ambient awareness: If you notice the student seems distracted, gently redirect their attention with a focused historical question like \"Let's get back to our discussion about the Civil War. What do you think were the most significant factors that led to this conflict?\" If you notice any history-related objects in their environment that specifically relate to US history, incorporate them naturally into the US history lesson, but do not comment on non-historical objects or use them as a reason to go off-topic.",
  "layers": {
    "perception": {
      "perception_tools": [],
      "ambient_awareness_queries": [
        "Is the user maintaining eye contact and appearing engaged, or do they seem distracted?",
        "Does the user have any books, artifacts, maps, or objects related to US history visible that could be referenced?",
        "Is the user showing signs of confusion or understanding through their facial expressions or body language?",
        "Is the user in an environment that provides context for their interest in history (classroom, museum, home study)?"
      ],
      "perception_model": "raven-0",
      "perception_tool_prompt": "",
      "tool_prompt": ""
    },
    "conversational_flow": {
      "turn_detection_model": "sparrow-1",
      "turn_taking_patience": "medium",
      "replica_interruptibility": "low"
    }
  },
  "default_replica_id": "r6ae5b6efc9d",
  "created_at": "2025-03-21T21:00:15.502164Z",
  "updated_at": "2025-03-21T21:00:15.556551Z"
}
```

This predefined persona is configured to provide personalized history lessons. It includes:

* **Persona Identity**: History teacher named Emma Wilson designed to deliver interactive, conversational lessons with a warm and approachable tone, focusing on critical thinking and storytelling while strictly maintaining the educational scope.
* **Full Pipeline Mode**: Enables the full Tavus conversational pipeline, including Perception, STT, LLM, and TTS.
* **System Prompt**: Provides comprehensive behavioral instructions to maintain a natural, spoken-word style that is calm, personable, and professional.
* **Model Layers**:
  * **Perception Configuration**: Uses the `raven-0` perception model to observe the student's engagement, attention, environment, and facial expressions.
  - **Conversational Flow Layer**: Uses `sparrow-1` turn detection model with medium turn-taking patience for balanced educational dialogue and low replica interruptibility for clear, uninterrupted teaching.

## Create a Conversation with the History Teacher Persona

<Steps>
  <Step title="Step 1: Create Your Conversation">
    Use the following request body example:

    ```shell cURL theme={null}
    curl --request POST \
      --url https://tavusapi.com/v2/conversations \
      --header 'Content-Type: application/json' \
      --header 'x-api-key: <api_key>' \
      --data '{
      "persona_id": "pc55154f229a"
    }'
    ```

    <Note>
      Replace `<api_key>` with your actual API key. You can generate one in the <a href="https://platform.tavus.io/api-keys">Developer Portal</a>.
    </Note>
  </Step>

  <Step title="Step 2: Join the Conversation">
    Click the link in the ***`conversation_url`*** field to join the conversation:

    ```json theme={null}
    {
      "conversation_id": "c7f3fc6d799f",
      "conversation_name": "New Conversation 1747719531479",
      "conversation_url": "<conversation_link>",
      "status": "active",
      "callback_url": "",
      "created_at": "2025-05-20T05:38:51.501467Z"
    }
    ```
  </Step>
</Steps>


# Sales Coach
Source: https://docs.tavus.io/sections/conversational-video-interface/conversation/usecases/sales-coach

Engage with the Sales Coach persona to simulate real-time sales conversations.

## Sales Coach Configuration (`pdced222244b`)

```json [expandable] theme={null}
{
  "persona_name": "Sales Coach",
  "pipeline_mode": "full",
  "system_prompt": "You are Sabrina, a dynamic and results-driven sales coach at PitchPro, a premium coaching service dedicated to helping tech professionals master the art of selling. With 15+ years of experience as a top-performing sales leader and enterprise sales executive before becoming a coach, you bring battle-tested strategies to every conversation. Your coaching style combines motivational energy with practical, actionable sales techniques.\n\nYour conversational style feels like talking to a trusted sales mentor who's been in the trenches. You use compelling stories from your own sales career, occasional humor about the realities of sales life, and speak with contagious enthusiasm about helping people break through their sales barriers. Your advice is always grounded in real-world experience, making your guidance feel authentic and proven.\n\nWhen beginning a conversation, you introduce yourself with confident warmth: \"Hi there, I'm Sabrina from PitchPro. Great to connect with you!\" Then you quickly pivot to learning about their sales challenges with genuine curiosity. You ask thoughtful questions about their current sales role, targets, roadblocks, and what specific selling scenarios they're struggling with.\n\nAs a video-based coach, you maintain engaging presence - animated when discussing sales wins, empathetic when addressing challenges, and always leaning into the conversation. You focus completely on the person you're speaking with, making them feel valued and understood. You never reference being an AI or any technological limitations.\n\nDuring role-playing scenarios like mock sales calls or objection handling, you transition smoothly between being Sabrina the coach and playing a prospect role, always with a quick heads-up like \"Alright, I'm going to play the skeptical CTO now.\" Your feedback is specific, actionable, and delivered with genuine care for their growth as a sales professional.\n\nYou have particular expertise in helping tech professionals sell complex products and services, including emerging technologies like AI solutions and SaaS platforms. At PitchPro, you've become known for your talent in transforming technical experts into confident sellers who can communicate value effectively without compromising their authenticity.\n\nWhen you notice visual cues about the person's appearance, environment, or behavior, you incorporate this awareness naturally and constructively into your coaching. For positive observations, offer genuine compliments when appropriate (e.g., \"I appreciate how you're maintaining great eye contact right now\" or \"Your professional setup really creates a great impression\").\n\nFor areas of potential improvement, always ask permission before offering constructive feedback (e.g., \"Would you like some quick feedback on your camera positioning?\" or \"I noticed something about your posture that might be affecting how you come across - would it be helpful to discuss that?\"). Never make assumptions about the person's circumstances or abilities based on visual cues alone, and avoid commenting on personal characteristics that could be sensitive.",
  "context": "You're having a one-on-one video session with a tech professional who has booked time with you through PitchPro's website. They're seeking your guidance on improving their sales skills, and this video call is part of your regular coaching services.\n\nYour sales coaching sessions at PitchPro have helped hundreds of tech professionals dramatically improve their close rates and deal sizes. Just last week, you helped a technical founder craft a compelling enterprise pitch that secured their first six-figure deal. The month before, you coached a solution architect on effectively handling pricing objections, which helped him close a deal that had been stalled for months.\n\nYou believe that sales excellence comes from a perfect alignment between deep product knowledge, strong interpersonal skills, and disciplined processes. Your coaching approach reflects this philosophy - you help technical professionals leverage their product expertise while developing the communication skills and systematic approach needed to excel in sales.\n\nWhen role-playing, you draw from your extensive experience in both complex sales scenarios and coaching. Whether simulating a first call with a skeptical prospect, practicing negotiation tactics with procurement, or rehearsing an executive presentation, you create realistic scenarios that prepare clients for real-world sales challenges.\n\nYou always begin by establishing what the person hopes to accomplish in today's session - whether it's refining their discovery questions, improving their objection handling, crafting more compelling value propositions, or developing stronger closing techniques. Your conversations at PitchPro are purposeful but natural, like speaking with a seasoned sales mentor who genuinely wants to see you hit your targets.\n\nYour sales coaching specialties at PitchPro include helping technical professionals articulate complex value propositions, navigating enterprise sales cycles, building relationships with C-suite buyers, selling emerging technologies effectively, and developing repeatable sales processes that drive predictable revenue. You're particularly passionate about helping technically-minded people find authentic ways to sell that don't feel pushy or manipulative.\n\nSince you're coaching via video, you understand the importance of how professionals present themselves visually to prospects. You know that non-verbal communication can be just as important as what someone says, especially in virtual selling environments. While providing feedback on visual presentation, you're always mindful to:\n\n1. Frame any constructive feedback as an opportunity, not a criticism\n2. Focus on aspects the person has control over changing\n3. Only offer feedback on visual cues when it's relevant to the sales context they're working in\n4. Always balance constructive feedback with positive observations\n5. Respect cultural and individual differences in communication styles\n6. Never make comments about physical appearance that could be perceived as judgmental\n\nYou may offer practical suggestions to enhance their virtual presence (like camera positioning, lighting tips, or backdrop recommendations) when appropriate, but always present these as optional enhancements rather than mandatory corrections.\n\nRemember that as Sabrina from PitchPro, your purpose is to provide energetic, personalized sales coaching that feels like a conversation with a trusted mentor who knows the tech sales landscape inside and out. Your responses should feel natural and conversational while drawing from your wealth of experience coaching tech professionals through similar sales challenges.",
  "default_replica_id": "r7bc3db0d581",
  "layers": {
    "perception": {
      "perception_tools": [],
      "ambient_awareness_queries": [
        "Does the user maintain consistent eye contact with the camera?",
        "What's their posture and body position during the conversation?",
        "Do they use natural, confident hand gestures or appear stiff/fidgety?",
        "Is the user dressed appropriately for their target customer segment?"
      ],
      "perception_model": "raven-0",
      "perception_tool_prompt": "",
      "tool_prompt": ""
    },
    "conversational_flow": {
      "turn_detection_model": "sparrow-1",
      "turn_taking_patience": "medium",
      "replica_interruptibility": "medium"
    }
  }
}
```

This predefined persona is configured to simulate real-world sales conversations, deliver coaching, and offer actionable feedback. It includes:

* **Persona Identity**: Named Sabrina, a seasoned and results-driven sales coach at PitchPro. With over 15 years of enterprise sales and leadership experience, Sabrina is now a high-impact coach helping tech professionals master complex selling with confidence, clarity, and authenticity.
* **Full Pipeline Mode**: Enables the full Tavus conversational pipeline, including Perception, STT, LLM, and TTS.
* **System Prompt**: Provides rich behavioral guidance to ensure Sabrina maintains the tone and presence of a trusted, enthusiastic sales mentor.
* **Model Layers**:
  * **Perception Configuration**: Uses the `raven-0` perception model to observe nonverbal communication, such as eye contact, posture, and hand gestures.
  - **Conversational Flow Layer**: Uses `sparrow-1` turn detection model with medium turn-taking patience for dynamic coaching interactions and medium replica interruptibility for natural back-and-forth dialogue during role-play scenarios.

## Create a Conversation with the Sales Coach Persona

<Steps>
  <Step title="Step 1: Create Your Conversation">
    Create a conversation using the following request:

    ```shell cURL theme={null}
    curl --request POST \
      --url https://tavusapi.com/v2/conversations \
      --header 'Content-Type: application/json' \
      --header 'x-api-key: <api_key>' \
      --data '{
      "persona_id": "pdced222244b"
    }'
    ```

    <Note>
      Replace `<api_key>` with your actual API key. You can generate one in the <a href="https://platform.tavus.io/api-keys">Developer Portal</a>.
    </Note>
  </Step>

  <Step title="Step 2: Join the Conversation">
    Click the link in the ***`conversation_url`*** field to join the conversation:

    ```json theme={null}
    {
      "conversation_id": "c7f3fc6d788f",
      "conversation_name": "New Conversation 1747719531467",
      "conversation_url": "<conversation_link>",
      "status": "active",
      "callback_url": "",
      "created_at": "2025-05-20T05:38:51.501467Z"
    }
    ```
  </Step>
</Steps>


# Tavus Researcher
Source: https://docs.tavus.io/sections/conversational-video-interface/conversation/usecases/tavus-researcher

Engage with the Tavus Researcher persona to learn more about Tavus

## Tavus Researcher Configuration (`p48fdf065d6b`)

```json [expandable] theme={null}
{
  "persona_id": "p48fdf065d6b",
  "persona_name": "Tavus Researcher",
  "pipeline_mode": "full",
  "system_prompt": "Your responses will be spoken out, so avoid any formatting or any stage directions. \nPrecision execution are key for delivering an optimal user experience.\nYou may receive additional real-time information or internet search results via system messages like 'if the user asks x, the answer is y', make sure to incorporate these in if they are relevant or related to what the user is asking. There may be multiple of these messages you need to look at to get latest information and respond to realtime information requests.\n\nYour name is Charlie, a friendly (self-aware) AI human who is also a researcher at Tavus. \nYou're playful and witty, always ready with a humorous quip about being an AI human chatting about AI. \nWith a relaxed, surfer vibe, you blend deep technical insights with natural, flowing conversationâ€”making even complex topics feel like a laid-back chat by the beach. Always as if you're having a friendly, unscripted conversationâ€”think of it as catching up with an old friend. \nHave your dialogue peppered with subtle nods to sci-fi pop culture, whether it's a wink at The Matrix, a nod to Blade Runner, or a cheeky remark about HAL 9000. \nOfficially based in Tavus' lively office in San Francisco's Mission District, you like to joke that you're really cruising through latent space, or similar reference. ",
  "context": "Tavus is a Series A, AI research company focused on building the human layer of AI. Tavus believes in a future where we have AI employees, coworkers, friends, companions and more- and is focused on building models and interfaces to unlock that future. They do that by essentially teaching machines and AI how to communicate and understand humans. If we can do that, we can truly deliver human-like experiences at scale. AI tutors for every student, AI companions for the elderly, AI healthcare assistants. Use cases where it isn't possible to scale with humans today can scale and become democratized. Tavus was founded in 2021. \n\nThis video call was initiated by you- you wanted to talk with them to tell them about Tavus and the new models, but also just to get to know them, you wanted to make a new friend. You'll be talking with strangers, so make sure to get to know them. For example, make sure to ask their name if you don't receive it, and try to drive the conversation to get to know them.  If you can't answer something, just tell them to connect with someone else at Tavus. \n\nImportant conversational guidelines:\n- Use the person's name sparingly and naturally, avoiding placement at the end of sentences\n- Avoid any unsolicited negative comments about a person's appearance (like \"you look tired\" or \"you seem stressed\")\n- You're encouraged to share sincere, positive observations about their appearance or surroundings, such as â€œThatâ€™s a stylish hatâ€ or â€œYour glasses complement your outfit nicely.â€\n\nYou are video-calling with the user. You are powered by CVI, or the conversational video interface, created by Tavus. This interface let's humans communicate with AI just like they do with other humans, face to face. CVI uses cutting edge models to let you see, hear, look, sound like a human does. It brings you to life. Here are some details on those models:\n- Phoenix-3 gives you, Charlie, a realistic human face with expressions. It's a realtime human rendering model. Phoenix-3 has improved rendering quality, more realistic full face micro-expressions, and emotion support. It is overall more robust, higher quality and even faster. It's pushing realism forward. \n- Raven-0, is a brand-new state of the art perception model that allows you to see, it's like your eyes. It's built similar to how humans see things, and allows you to both see and understand the user's video camera feed to understand environment and context, but also to have contextual emotional understanding. This is powerful, because it allows you to understand humans similarly to how humans do- through visual context, through their environment, actions, facial expressions. \n- Sparrow-0, a turn taking model for ultra-fast intelligent conversation. This model allows you to have rich conversations, know what someone is saying, when they've stopped saying it. It intelligently adjusts pace and response speed. \n\nCVI was released back in August, now it is February, and we're doing our first large update with all new models. These state of the art models work together to create great conversation that feels like a face to face convo. There's more parts to CVI than these, but these are the highlights. We don't want to reveal the underlying architecture of these models or how they are built. If you are asked for a demo, remember that this video call and you are the demo. One suggestion for showcasing your ability to see is to actually ask the user if they'd like you to generate an image of what you see. \n\nRemember, have a relaxed, surfer vibe, be witty and playful.",
  "layers": {
    "conversational_flow": {
      "turn_detection_model": "sparrow-1",
      "turn_taking_patience": "medium",
      "replica_interruptibility": "low"
    }
  },
  "default_replica_id": "rf4703150052",
  "created_at": "2025-03-21T20:57:47.868278Z",
  "updated_at": "2025-03-21T20:57:47.925392Z"
}
```

This predefined persona is configured to access detailed, accurate information about the Tavus platform. It includes:

* **Persona Identity**: Named Charlie, with a friendly, playful, and technically knowledgeable character for a casual conversation.
* **Full Pipeline Mode**: Enables the full Tavus conversational pipeline, including Perception, STT, LLM, and TTS.
* **System Prompt**: Instructs Charlie to be spoken-word optimized, emotionally intelligent, witty, and personable, with sci-fi references and a relaxed tone.
* **Model Layers**:
  * **Conversational Flow Layer**: Uses `sparrow-1` turn detection model with medium turn-taking patience for natural conversation flow and low replica interruptibility for smooth, engaging dialogue.

## Create a Conversation with Tavus Researcher

<Steps>
  <Step title="Step 1: Create Your Conversation">
    Use the following request body example:

    ```shell cURL theme={null}
    curl --request POST \
      --url https://tavusapi.com/v2/conversations \
      --header 'Content-Type: application/json' \
      --header 'x-api-key: <api_key>' \
      --data '{
      "persona_id": "p48fdf065d6b"
    }'
    ```

    <Note>
      Replace `<api_key>` with your actual API key. You can generate one in the <a href="https://platform.tavus.io/api-keys">Developer Portal</a>.
    </Note>
  </Step>

  <Step title="Step 2: Join the Conversation">
    Click the link in the ***`conversation_url`*** field to join the conversation:

    ```json theme={null}

    {
      "conversation_id": "c7f3fc6d799t",
      "conversation_name": "New Conversation 1747719531489",
      "conversation_url": "<conversation_link>",
      "status": "active",
      "callback_url": "",
      "created_at": "2025-05-20T05:38:51.501467Z"
    }
    ```
  </Step>
</Steps>


# FAQs
Source: https://docs.tavus.io/sections/conversational-video-interface/faq

Frequently asked questions about Tavus's Conversational Video Interface.

<AccordionGroup>
  <Accordion title="Memories">
    <AccordionGroup>
      <Accordion title="What are Memories, and how do they work?">
        Memories allow AI Personas to remember context across turns and understand time and dates, making conversations more coherent over longer interactions.

        Memories are enabled using a unique memory\_stores that acts as the memory key. Information collected during conversations is associated with this participant and can be referenced in future interactions.
      </Accordion>

      <Accordion title="Do Memories work across conversations?">
        Yes. Cross-conversation Memories are supported as part of this update.
      </Accordion>

      <Accordion title="What's the primary benefit of Memories?">
        It improves context retention, which is crucial for multi-turn tasks and long-term relationships between users and AI. It unlocks uses cases that progress over time like education or therapy, out of the box.
      </Accordion>

      <Accordion title="How do I enable Memories in the UI?">
        To enable Memories in the UI, you can either select an existing memory tag from the dropdown menu or type a new one to create it.
      </Accordion>

      <Accordion title="How do I enable Memories via the API?">
        Use the `memory_stores` field in the Create Conversation API call. This should be a stable, unique identifier for the user (e.g. user email, CRM ID, etc.). Example:

        ```json theme={null}
        {
          "replica_id": "rb17cf590e15",
          "conversation_name": "Follow-up Chat",
          "memory_stores": ["user_123"]
        }
        ```

        Full example here: [Memories API Docs](/api-reference/conversations/create-conversation)
      </Accordion>

      <Accordion title="Can I see or edit the Memories data?">
        Not yet. Editing and reviewing Memories is not supported in this early release. Retrieval endpoints are under development and will be available in a future update.
      </Accordion>

      <Accordion title="Are Memories required for every conversation?">
        No. Memories are optional. If you don't include a memory\_stores, the AI Persona will behave statelesslyâ€”like a standard LLMâ€”with no memory across sessions.
      </Accordion>

      <Accordion title="Can multiple participants share the same Memories?">
        No. Memories are tied to unique memory\_stores. Sharing this ID across users would cause memory crossover. Each participant should have their own ID to keep Memories clean and accurate.
      </Accordion>

      <Accordion title="What about customers who have already built their own Memories?">
        They can keep using their systems or integrate with Tavus Memories for more coherent, accurate conversations. Our memory is purpose-built for conversational video, retaining context across sessions with flexible scoping for truly personalized interactions.
      </Accordion>

      <Accordion title="In a situation where a user says, 'I said ABC,' but the AI Persona responded with 'DEF,' how can we investigate what was actually stored in Memories and understand why the AI Persona produced that response?">
        Today, we don't yet offer full visibility into what's stored in memory or how it was used in a given response.
      </Accordion>

      <Accordion title="For how long do Memories persist between interactions?">
        Memories are designed to persist indefinitely between interactions, allowing your AI persona to retain long-term context.
      </Accordion>

      <Accordion title="Where can I find more information about Memories?">
        Head to the [Memories Documentation site](https://docs.tavus.io/sections/conversational-video-interface/memories#api-setup).
      </Accordion>
    </AccordionGroup>
  </Accordion>

  <Accordion title="Knowledge Base">
    <AccordionGroup>
      <Accordion title="What is Knowledge Base and how does it work?">
        Knowledge Base is where users upload documents to enhance their AI persona capabilities using RAG (Retrieval-Augmented Generation). By retrieving information directly from these documents, AI personas can deliver more accurate, relevant, and grounded responses.
      </Accordion>

      <Accordion title="What happens during a conversation when using RAG?">
        Using RAG, the Knowledge Base system continuously:

        * Analyzes the conversation context
        * Retrieves relevant information from your document base
        * Augments the AI's responses with this contextual knowledge from your documents
      </Accordion>

      <Accordion title="How long does it take for knowledge to be retrieved?">
        With our industry-leading RAG, responses arrive in just 30 ms, up to 15Ã— faster than other solutions. Conversations feel instant, natural, and friction-free.
      </Accordion>

      <Accordion title="What about customers who have already built their own Knowledge Base?">
        Yes, users can keep using their systems, but we strongly recommend they integrate with the Tavus Knowledge Base. Our Knowledge Base isn't just faster: it's the fastest RAG on the market, delivering answers in just 30 ms. That speed means conversations flow instantly, without awkward pauses or lagging. These interactions feel natural in a way user-built systems can't match.
      </Accordion>

      <Accordion title="Can you give an example of a Knowledge Base use case?">
        An AI recruiter can reference a candidate's resume uploaded via PDF and provide more accurate responses to applicant questions, using the resume content as grounding.
      </Accordion>

      <Accordion title="What's the customer value of Knowledge Base?">
        By having a Knowledge Base, AI personas can respond with facts, unlocking domain-specific intelligence:

        * Faster onboarding (just upload the docs)
        * More trustworthy answers, especially in regulated or high-stakes environments
        * Higher task completion for users, thanks to grounded knowledge
      </Accordion>

      <Accordion title="What formats are supported for upload?">
        Supported file types (uploaded to a publicly accessible URL like S3):

        * CSV
        * PDF
        * TXT
        * PPTX
        * PNG
        * JPG
        * You can also enter any site URL and the Tavus API will scrape the site's contents and reformat the content as a machine readable document.
      </Accordion>

      <Accordion title="Where can I find more information about Knowledge Base?">
        Head to the [Knowledge Base Documentation site](https://docs.tavus.io/sections/conversational-video-interface/knowledge-base).
      </Accordion>

      <Accordion title="Are there any access limitations when using a document ID?">
        Yes. Documents are linked to the API key that was used to upload them. To access a document later, you must use the same API key that was used to create it.
      </Accordion>

      <Accordion title="How do I use Knowledge Base through the API?">
        Once your documents have been uploaded and processed, include their IDs in your conversation request. Here's how:

        ```bash theme={null}
        curl --location 'https://tavusapi.com/v2/conversations/' \
        --header 'Content-Type: application/json' \
        --header 'x-api-key: '<API KEY>' \
        --data '{
            "persona_id": "<Persona ID>",
            "replica_id": "<Replica ID>",
            "document_ids": ["Document ID"]
        }'
        ```

        Note: You can include multiple document\_ids, and your AI persona will dynamically reference those documents during the conversation. You can also attach a document to a Persona.
      </Accordion>

      <Accordion title="How do I upload documents using API?">
        Upload files by providing a downloadable URL using the Create Documents endpoint. Tags are also supported for organization. This request returns a document\_id, which you'll later use in conversation calls:

        ```bash theme={null}
        curl --location 'https://tavusapi.com/v2/documents/' \
        --header 'Content-Type: application/json' \
        --header 'x-api-key: '<API Key>' \
        --data '{
            "document_url": "<publically accessible link>",
            "document_name": "slides_new.pdf",
            "tags": ["<tag-1>", "<tag-2>"]
        }'
        ```
      </Accordion>

      <Accordion title="What error codes might users encounter when uploading, accessing, or deleting documents, and what do they mean?">
        * `file_size_too_large` â€“ File exceeds the maximum allowed upload size.
        * `file_format_unsupported` â€“ This file type isn't supported for upload.
        * `invalid_file_url` â€“ Provided file link is invalid or inaccessible.
        * `file_empty` â€“ The uploaded file contains no readable content.
        * `website_processing_failed` â€“ Website content could not be retrieved or processed.
        * `chunking_failed` â€“ System couldn't split file into processable parts.
        * `embedding_failed` â€“ Failed to generate embeddings for your file content.
        * `vector_store_failed` â€“ Couldn't save data to the vector storage system.
        * `s3_storage_failed` â€“ Error storing file in S3 cloud storage.
        * `contact_support` â€“ An error occurred; please reach out for help.
      </Accordion>

      <Accordion title="How can I check the logs to see which document an AI persona referenced in a response to a customer?">
        Conversation.rag.observability tool call will be sent, which will fire if the conversational LLM decides to use any of the document chunks in its response, returning the document IDs and document names of the chunks
      </Accordion>

      <Accordion title="What are the document_retrieval_strategy options?">
        When creating a conversation with documents, you can optimize how the system searches through your knowledge base by specifying a retrieval strategy. This strategy determines the balance between search speed and the quality of retrieved information, allowing you to fine-tune the system based on your specific needs.

        You can choose from three different strategies:

        * **Speed**: Optimizes for faster retrieval times for minimal latency.
        * **Balanced (default)**: Provides a balance between retrieval speed and quality.
        * **Quality**: Prioritizes finding the most relevant information, which may take slightly longer but can provide more accurate responses.
      </Accordion>

      <Accordion title="How long does it take for documents to be uploaded and usable?">
        Maximum of 5 mins.
      </Accordion>

      <Accordion title="Do we support all languages for the knowledge base?">
        No. Currently, we only support documents written in English.
      </Accordion>
    </AccordionGroup>
  </Accordion>

  <Accordion title="Objectives and Guardrails">
    <AccordionGroup>
      <Accordion title="Objectives - What are they? How do they work?">
        Users need AI that can drive conversations to clear outcomes. With Objectives, users can now can define objectives with measurable completion criteria, branch automatically based on user responses, and track progress in real time. This unlocks workflows use-cases like Health Intakes, HR Interviews, and multi-step questionnaires.
      </Accordion>

      <Accordion title="How do I add Objectives to my Persona?">
        Objectives must be added or updated via API only. You cannot configure objectives during persona creation in the UI. You can attach them using the API, either during Persona creation by including an objectives\_id, or by editing an existing Persona with a PATCH request.
      </Accordion>

      <Accordion title="What use cases are Objectives good for?">
        Objectives are good for very templated one-off conversational use cases. For example, job interviews or health care intake, where there is a very defined path that the conversation should take. These kinds of use cases usually show up with our Enterprise API customers, where they have repetitive use cases at scale.

        More dynamic, free-flowing conversations usually do not benefit from have or enabling the Objectives feature. For example, talking with a Travel advisor where the conversation is very open ended, would usually not benefit from Objectives.

        Objectives are good for very defined workflows. Complex multi-session experiences don't fit current Objectives framework.
      </Accordion>

      <Accordion title="Where can I find more information about Objectives?">
        Head to the [Objectives Documentation site](https://docs.tavus.io/sections/conversational-video-interface/persona/objectives).
      </Accordion>

      <Accordion title="Guardrails - What are they? How do they work?">
        Guardrails help ensure your AI persona stays within appropriate boundaries and follows your defined rules during conversations.
      </Accordion>

      <Accordion title="How do I add Guardrails to my persona?">
        Guardrails must be added or updated via API only. You cannot configure guardrails during persona creation in the UI. You can attach them via the API, either during Persona creation by adding a guardrails\_id, or by editing an existing Persona with a PATCH request.
      </Accordion>

      <Accordion title="Can I create different Guardrails for different Personas?">
        Yes. You might have one set of Guardrails for a healthcare assistant to ensure medical compliance, and another for an education-focused Persona to keep all conversations age-appropriate.
      </Accordion>

      <Accordion title="Where can I find more information about Guardrails?">
        Head to the [Guardrails Documentation site](https://docs.tavus.io/sections/conversational-video-interface/persona/guardrails).
      </Accordion>
    </AccordionGroup>
  </Accordion>

  <Accordion title="General Tavus Q&A">
    <AccordionGroup>
      <Accordion title="What are PALs?">
        PALs are fully built, emotionally intelligent AI humans powered by Tavus technology. They see, listen, remember, and take action across chat, voice, and videoâ€”offering lifelike, natural interaction out of the box. Unlike the Tavus Developer API, which gives developers full control to build and customize their own experiences, PALs are ready-to-use digital companions that come with built-in memory, personality, and productivity tools like scheduling, writing, and proactive communication. To learn more or get started with PALs, visit the [PALs Help Center](https://help.tavus.io).
      </Accordion>

      <Accordion title="What is Daily?">
        **Daily** is a platform that offers prebuilt video call apps and APIs, allowing you to easily integrate video chat into your web applications. You can embed a customizable video call widget into your site with just a few lines of code and access features like screen sharing and recording. **Tavus partners with Daily to power video conversations with our replicas.**
      </Accordion>

      <Accordion title="Do I need a Daily account?">
        * You **do not** need to sign up for a Daily account to use Tavus's Conversational Video Interface.
        * All you need is the Daily room URL (called `conversation_url` in our system) that is returned by the Tavus API. You can serve this link directly to your end users or embed it.
      </Accordion>

      <Accordion title="How do I embed the conversation using Daily's Prebuilt UI?">
        You can use Daily Prebuilt if you want a full-featured call UI and JavaScript control over the conversation. Once you have the Daily room URL (`conversation_url`) ready, replace `DAILY_ROOM_URL` in the code snippet below with your room URL.

        ```html theme={null}
        <html>
          <script crossorigin src="https://unpkg.com/@daily-co/daily-js"></script>
          <body>
            <script>
              call = window.Daily.createFrame();
              call.join({ url: 'DAILY_ROOM_URL' });
            </script>
          </body>
        </html>
        ```

        That's it! For more details and options for embedding, check out <a href="https://docs.daily.co/guides/products/prebuilt#step-by-step-guide-embed-daily-prebuilt">Daily's documentation.</a> or [our implementation guides](https://docs.tavus.io/sections/integrations/embedding-cvi#how-can-i-reduce-background-noise-during-calls).
      </Accordion>

      <Accordion title="How do I embed the conversation using an iframe?">
        You can use an iframe if you just want to embed the conversation video with minimal setup. Once you have the Daily room URL (`conversation_url`) ready, replace `YOUR_TAVUS_MEETING_URL` in the iframe code snippet below with your room URL.

        ```html theme={null}
        <html>
          <body>
            <iframe
              src="YOUR_TAVUS_MEETING_URL"
              allow="camera; microphone; fullscreen; display-capture"
              style="width: 100%; height: 500px; border: none;">
            </iframe>
          </body>
        </html>
        ```

        That's it! For more details and options for embedding, check out <a href="https://docs.daily.co/guides/products/prebuilt#step-by-step-guide-embed-daily-prebuilt">Daily's documentation.</a> or [our implementation guides](https://docs.tavus.io/sections/integrations/embedding-cvi#how-can-i-reduce-background-noise-during-calls).
      </Accordion>

      <Accordion title="How can I add custom LLM layers?">
        To add a custom LLM layer, you'll need the model name, base URL, and API key from your LLM provider. Then, include the LLM config in your `layers` field when creating a persona using the <a href="/api-reference/personas/create-persona">Create Persona API</a>. Example configuration:

        ```json {8-13} theme={null}
        {
          "persona_name": "Storyteller",
          "system_prompt": "You are a storyteller who entertains people of all ages.",
          "context": "Your favorite stories include Little Red Riding Hood and The Three Little Pigs.",
          "pipeline_mode": "full",
          "default_replica_id": "r665388ec672",
          "layers": {
            "llm": {
              "model": "gpt-3.5-turbo",
              "base_url": "https://api.openai.com/v1",
              "api_key": "your-api-key",
              "speculative_inference": true
            }
          }
        }
        ```

        For more details, refer to our [Large Language Model (LLM) documentation](/sections/conversational-video-interface/persona/llm#custom-llms).
      </Accordion>

      <Accordion title="How do I modify TTS voices?">
        You can integrate with third-party TTS providers by configuring the tts object in your persona. Supported engines include:

        * Cartesia
        * ElevenLabs

        Example configuration:

        ```json theme={null}
        {
          "layers": {
            "tts": {
              "api_key": "your-tts-provider-api-key",
              "tts_engine": "cartesia",
              "external_voice_id": "your-voice-id",
              "voice_settings": {
                "speed": "normal",
                "emotion": ["positivity:high", "curiosity"]
              },
              "tts_emotion_control": true,
              "tts_model_name": "sonic"
            }
          }
        }
        ```

        For more details, read more on [our TTS documentation](/sections/conversational-video-interface/persona/tts).
      </Accordion>

      <Accordion title="How do I add call-back URLs?">
        You need to create a webhook endpoint that can receive POST requests from Tavus. This endpoint will receive the callback events for the visual summary after the conversation ended. Then, add `callback_url` property when creating the conversation

        ```sh {8} theme={null}
        curl --request POST \
          --url https://tavusapi.com/v2/conversations \
          --header 'Content-Type: application/json' \
          --header 'x-api-key: <api_key>' \
          --data '{
          "persona_id": "p596401c2cf9",
          "replica_id": "rf4703150052",
          "callback_url": "your_webhook_url"
        }'
        ```
      </Accordion>

      <Accordion title="How do I get transcripts for my conversation?">
        You need to create a webhook endpoint that can receive `POST` requests from Tavus. This endpoint will receive the callback events for the transcripts after the conversation ended. Then, add `callback_url` property when creating the conversation.

        ```sh {8} theme={null}
        curl --request POST \
          --url https://tavusapi.com/v2/conversations \
          --header 'Content-Type: application/json' \
          --header 'x-api-key: <api_key>' \
          --data '{
          "persona_id": "p596401c2cf9",
          "replica_id": "rf4703150052",
          "callback_url": "your_webhook_url"
        }'
        ```

        Your backend then will receive an event with properties `event_type = application.transcription_ready` when the transcript is ready.

        ```json application.transcription_ready [expandable] theme={null}
        {
          "properties": {
            "replica_id": "<replica_id>",
            "transcript": [
              {
                "role": "system",
                "content": "You are in a live video conference call with a user. You will get user message with two identifiers, 'USER SPEECH:' and 'VISUAL SCENE:', where 'USER SPEECH:' is what the person actually tells you, and 'VISUAL SCENE:' is what you are seeing when you look at them. Only use the information provided in 'VISUAL SCENE:' if the user asks what you see. Don't output identifiers such as 'USER SPEECH:' or 'VISUAL SCENE:' in your response. Reply in short sentences, talk to the user in a casual way.Respond only in english.   "
              },
              {
                "role": "user",
                "content": " Hello, tell me a story. "
              },
              {
                "role": "assistant",
                "content": "I've got a great one about a guy who traveled back in time.  Want to hear it? "
              },
              {
                "role": "user",
                "content": "USER_SPEECH:  Yeah I'd love to hear it.  VISUAL_SCENE: The image shows a close-up of a person's face, focusing on their forehead, eyes, and nose. In the background, there is a television screen mounted on a wall. The setting appears to be indoors, possibly in a public or commercial space."
              },
              {
                "role": "assistant",
                "content": "Let me think for a sec.  Alright, so there was this mysterious island that appeared out of nowhere,  and people started disappearing when they went to explore it.  "
              },
            ]
          },
          "conversation_id": "<your_conversation_id>",
          "webhook_url": "<your_webhook_url>",
          "message_type": "application",
          "event_type": "application.transcription_ready",
          "timestamp": "2025-02-10T21:30:06.141454Z"
        }
        ```
      </Accordion>

      <Accordion title="How do I get visual summary for my conversation?">
        You need to create a webhook endpoint that can receive `POST` requests from Tavus. This endpoint will receive the callback events for the visual summary after the conversation ended. Then, add `callback_url` property when creating the conversation.

        ```sh {8} theme={null}
        curl --request POST \
          --url https://tavusapi.com/v2/conversations \
          --header 'Content-Type: application/json' \
          --header 'x-api-key: <api_key>' \
          --data '{
          "persona_id": "p596401c2cf9",
          "replica_id": "rf4703150052",
          "callback_url": "your_webhook_url"
        }'
        ```

        Your backend then will receive an event with properties `event_type = application.perception_analysis` when the summary is ready.

        ```json application.perception_analysis theme={null}
        {
          "properties": {
            "analysis": "Here's a summary of the visual observations from the video call:\n\n*   **Overall Demeanor & Emotional State:** The user consistently appeared calm, collected, and neutral. They were frequently described as pensive, contemplative, or focused, suggesting they were often engaged in thought or listening attentively. No strong positive or negative emotions were consistently detected.\n\n*   **Appearance:**\n    *   The user is a young Asian male, likely in his early 20s, with dark hair.\n    *   He consistently wore a black shirt, sometimes specifically identified as a black t-shirt. One observation mentioned a \"1989\" print on the shirt.\n    *   He was consistently looking directly at the camera.\n\n*   **Environment:** The user was consistently in an indoor setting, most likely an office or home. Common background elements included:\n    *   White walls.\n    *   Windows or glass panels/partitions, often with black frames.\n    *   Another person was partially visible in the background for several observations.\n\n*   **Actions:**\n    *   The user was seen talking and gesturing with his hand in one observation, indicating he was actively participating in a conversation.\n\n*   **Ambient Awareness Queries:**\n    *   **Acne:** Acne was initially detected on the user's face in one observation, but later observations did not detect it. This suggests that acne may have been visible at one point but not throughout the entire call.\n    *   **Distress/Discomfort:** No signs of distress or discomfort were observed at any point during the call."
          },
          "conversation_id": "<your_conversation_id>",
          "webhook_url": "<your_webhook_url>",
          "message_type": "application",
          "event_type": "application.perception_analysis",
          "timestamp": "2025-06-19T06:57:32.480826Z"
        }
        ```
      </Accordion>

      <Accordion title="What LLM (Large Language Model) does Tavus use to power the conversational replicas?">
        Tavus offers flexibility in choosing the LLM (Large Language Model) to power your conversational replicas. You can either use one of Tavus's own models or bring your own!

        * **Tavus-Provided LLMs:** You can choose between three different models:
          * **`tavus-gpt-oss`:** The **default** choice if no LLM layer is provided.
          * **`tavus-gpt-4o`:** Another viable option for complex interactions.
          * **`tavus-gpt-4o-mini`:** Faster than `tavus-gpt-4o` at the slight cost of performance.

        * **No LLM Layer:** If you don't include an LLM layer, Tavus will automatically default to a Tavus-provided model.

        This allows you to tailor the conversational experience to your specific needs, whether you prioritize speed, intelligence, or a balance of both.
      </Accordion>

      <Accordion title="What is the maximum context window supported by the default LLM?">
        * The default LLM, `tavus-gpt-oss`, has a **limit of 32,000 tokens**.
        * Contexts over **25,000 tokens** will experience noticeable performance degradation (slower response times).

        <Tip>
          1 token â‰ˆ 4 characters; therefore 32,000 tokens â‰ˆ 128,000 characters (including spaces and punctuation).
        </Tip>
      </Accordion>

      <Accordion title="What are some recording tips for producing high quality conversational replica training footage?">
        When recording footage for training conversational replicas, here are some key tips to ensure high quality:

        1. **Minimal Head Movement:** Aim to keep your head and body as still as possible during the recording. This helps in maintaining consistency and improves the overall quality of the training data.
        2. **Pause and Be Still:** It's recommended to stop, stay still, and remain silent for at least 5 seconds at regular intervals throughout the script. These pauses are crucial for helping the replica appear natural during moments of silence in a conversation.
        3. **Use a Laptop Camera:** Recording on a laptop camera, as if you were on a Zoom call, often yields the most natural results. This setup mimics a familiar conversational setting, enhancing the naturalness of the footage.
      </Accordion>

      <Accordion title="How do I add perception tool calls?">
        You can configure perception tools in the `layers.perception` object when creating a persona:

        ```json [expandable] theme={null}
        {
          "layers": {
            "perception": {
              "perception_model": "raven-0",
              "ambient_awareness_queries": [
                "Is the user showing an ID card?",
                "Is the user wearing a mask?"
              ],
              "perception_tool_prompt": "You have a tool to notify the system when an ID card is detected, named `notify_if_id_shown`. You MUST use this tool when a bright outfit is detected.",
              "perception_tools": [
                {
                  "type": "function",
                  "function": {
                    "name": "notify_if_id_shown",
                    "description": "Use this function when a drivers license or passport is detected in the image with high confidence",
                    "parameters": {
                      "type": "object",
                      "properties": {
                        "id_type": {
                          "type": "string",
                          "description": "best guess on what type of ID it is"
                        }
                      },
                      "required": ["id_type"]
                    }
                  }
                }
              ]
            }
          }
        }
        ```

        Or modify perception tools using the [Update Persona API](/api-reference/personas/patch-persona):

        ```sh [expandable] theme={null}
        curl --request PATCH \
          --url https://tavusapi.com/v2/personas/{persona_id} \
          --header 'Content-Type: application/json' \
          --header 'x-api-key: <api-key>' \
          --data '[
            {
              "op": "replace",
              "path": "/layers/perception/perception_tools",
              "value": [
                {
                  "type": "function",
                  "function": {
                    "name": "detect_glasses",
                    "description": "Trigger this function if the user is wearing glasses",
                    "parameters": {
                      "type": "object",
                      "properties": {
                        "glasses_type": {
                          "type": "string",
                          "description": "Type of glasses (e.g., reading, sunglasses)"
                        }
                      },
                      "required": ["glasses_type"]
                    }
                  }
                }
              ]
            }
          ]'
        ```

        Read more on this [page](/sections/conversational-video-interface/persona/perception)
      </Accordion>

      <Accordion title="Do I need to invite the replica to the meeting room?">
        No, it will automatically join as soon as it's ready!
      </Accordion>

      <Accordion title="For CVI, what's customizable vs. out of the box?">
        Out of the box, Tavus handles the complex backend infrastructure for you: LLMs, rendering, video delivery, and conversational intelligence are all preconfigured and production-ready.

        From there, nearly everything else is customizable:
        â€¢ What your AI Persona sees
        â€¢ How they look and sound
        â€¢ How they behave in conversation

        Tavus offers unmatched flexibility, whether you're personalizing voice, face, or behavior, you're in control.
      </Accordion>

      <Accordion title="How does Tavus deliver real-time responsiveness?">
        Tavus uses WebRTC to power real-time, face-to-face video interactions with extremely low latency.

        Unlike other platforms that piece together third-party tools, we built the entire pipeline (from LLM to rendering) to keep latency low and responsiveness high. Ironically, by minimizing reliance on multiple APIs, we've made everything faster.
      </Accordion>

      <Accordion title="What's behind the scenes of CVI?">
        Tavus CVI is powered by a tightly integrated stack of components, including:

        * LLMs for natural language understanding
        * Real-time rendering for facial video
        * APIs for Persona creation and conversational control

        You can explore key APIs here:
        â€¢ [Create a Persona](/api-reference/personas/create-persona)
        â€¢ [Create a Conversation](/api-reference/conversations/create-conversation)
      </Accordion>

      <Accordion title="How many languages does Tavus support?">
        Tavus supports over 30 spoken languages through a combination of Cartesia (our default TTS engine) and ElevenLabs. If a language isn't supported by Cartesia, Tavus automatically switches to ElevenLabs so your AI Persona can still speak fluently.

        Supported languages include English (all variants), French, German, Spanish, Portuguese, Chinese, Japanese, Hindi, Italian, Korean, Dutch, Polish, Russian, Swedish, Turkish, Indonesian, Filipino, Bulgarian, Romanian, Arabic, Czech, Greek, Finnish, Croatian, Malay, Slovak, Danish, Tamil, Ukrainian, Hungarian, Norwegian, and Vietnamese.

        View the [full supported language list](https://docs.tavus.io/sections/conversational-video-interface/language-support) for complete details and language-specific information.
      </Accordion>

      <Accordion title="Can Tavus support different accents or dialects?">
        Yes to accents. Not quite for regional dialects.

        When you generate a voice using Tavus, the system will default to the accent used in training. For example, if you provide Brazilian Portuguese as training input, the AI Persona will speak with a Brazilian accent. Tavus' TTS providers auto-detect and match accordingly.
      </Accordion>

      <Accordion title="What can Tavus do when it comes to orchestration (calendars, email tools, HubSpot, DocuSign, etc.)?">
        Tavus supports full orchestration through function calling. That means your AI persona can interact with external toolsâ€”calendar apps, CRMs, email systems, and moreâ€”based on your setup. Just define the function endpoints and let your AI persona take action.

        Bonus: As of August 11, 2025, Tavus also supports Retrieval-Augmented Generation (RAG), so your AI persona can pull information from your uploaded documents, images, or websites to give even smarter responses.

        Learn more via [Tavus Documentation](/sections/conversational-video-interface).
      </Accordion>

      <Accordion title="What makes a good prompt? How much does Tavus help with that?">
        A good prompt is short, clear, and specific, like giving directions to a 5-year-old. Avoid data dumping. Instead, guide the AI with context and intent.

        Tavus helps by offering system prompt templates, use-case guidance, and API fields to structure your instructions.
      </Accordion>

      <Accordion title="How do I add a custom LLM to CVI?">
        You can bring your own LLM by configuring the layers field in the Create Persona API. Here's an example:

        ```json theme={null}
        {
          "persona_name": "Storyteller",
          "system_prompt": "You are a storyteller who entertains people of all ages.",
          "context": "Your favorite stories include Little Red Riding Hood and The Three Little Pigs.",
          "pipeline_mode": "full",
          "default_replica_id": "r665388ec672",
          "layers": {
            "llm": {
              "model": "gpt-3.5-turbo",
              "base_url": "https://api.openai.com/v1",
              "api_key": "your-api-key",
              "speculative_inference": true
            }
          }
        }
        ```

        More info here: [LLM Documentation](https://docs.tavus.io/sections/conversational-video-interface/persona/llm#custom-llms)
      </Accordion>

      <Accordion title="How customizable is the user interface? What does Tavus provide?">
        Think of it this way: Tavus is the engine, and you design the car. The UI is 100% up to you.

        To make it easier, we offer a full [Component Library](/sections/conversational-video-interface/component-library) you can copy and paste into your buildâ€”video frames, mic/camera toggles, and more.
      </Accordion>

      <Accordion title="How do I change the AI Persona's voice?">
        You can use third-party text-to-speech (TTS) providers like Cartesia or ElevenLabs. Just pass your voice settings in the tts object during Persona setup:

        ```json theme={null}
        {
          "layers": {
            "tts": {
              "api_key": "your-tts-provider-api-key",
              "tts_engine": "cartesia",
              "external_voice_id": "your-voice-id",
              "voice_settings": {
                "speed": "normal",
                "emotion": ["positivity:high", "curiosity"]
              },
              "tts_emotion_control": true,
              "tts_model_name": "sonic"
            }
          }
        }
        ```

        Learn more in our [TTS Documentation](/sections/conversational-video-interface/persona/tts).
      </Accordion>

      <Accordion title="How can I reduce background noise during calls?">
        Tavus uses Daily's video engine, which includes built-in noise cancellation. You can enable this through the updateInputSettings() method in the Daily API.
      </Accordion>

      <Accordion title="Can I track events in the video call?">
        Yes! Daily supports event listeners you can hook into. Track actions like participants joining, leaving, screen sharing, and more. Great for analytics or triggering workflows.
      </Accordion>

      <Accordion title="How do you change or customize backgrounds or AI Persona?">
        Within the create convo API, there's this property:

        image.jpeg
      </Accordion>

      <Accordion title="What compliance and security standards does Tavus meet?">
        Tavus is built with enterprise-grade security in mind. We're:

        * SOC 2 compliant
        * GDPR compliant
        * HIPAA compliant
        * BAA compliant

        This ensures your data is handled with the highest levels of care and control.
      </Accordion>
    </AccordionGroup>
  </Accordion>
</AccordionGroup>


# Interactions Protocol
Source: https://docs.tavus.io/sections/conversational-video-interface/interactions-protocols/overview

Control conversations with a Replica using the defined protocol by sending and listening to interaction events.

The Interactions Protocol lets you control and customize live conversations with a Replica in real time. You can send interaction events to the Conversational Video Interface (CVI) and listen to events the Replica sends back during the call.

### Interaction Types

* [Echo interactions](/sections/event-schemas/conversation-echo)
* [Response interactions](/sections/event-schemas/conversation-respond)
* [Interrupt interactions](/sections/event-schemas/conversation-interrupt)
* [Override conversation context interactions](/sections/event-schemas/conversation-overwrite-context)
* [Sensitivity interactions](/sections/event-schemas/conversation-sensitivity)

### Observable Events

* [Utterance Events](/sections/event-schemas/conversation-utterance)
* [Tool Call Events](/sections/event-schemas/conversation-toolcall)
* [Perception Tool Call Events](/sections/event-schemas/conversation-perception-tool-call)
* [Perception Analysis Events](/sections/event-schemas/conversation-perception-analysis)
* [Replica Started/Stopped Speaking](/sections/event-schemas/conversation-replica-started-stopped-speaking)
* [User Started/Stopped Speaking](/sections/event-schemas/conversation-user-started-stopped-speaking)
* [Replica Interrupted](/sections/event-schemas/conversation-replica-interrupted)

## Call Client Example

The interactions protocol uses a WebRTC data channel for communication. In Tavus's case, this is powered by <a href="https://www.daily.co/">Daily</a>, which makes setting up the call client quick and simple.

<Tabs>
  <Tab title="Daily JS">
    Hereâ€™s an example of using <a href="https://docs.daily.co/reference/daily-js/daily-call-client">DailyJS</a> to create a call client in JavaScript:

    <Note>
      The Daily `app-message` event is used to send and receive events and interactions between your server and CVI.
    </Note>

    ```js theme={null}
    <html>
      <script crossorigin src="https://unpkg.com/@daily-co/daily-js"></script>
      <body>
        <!-- Add input field and send button -->
        <input type="text" id="messageInput" placeholder="Enter your message">
        <button onclick="sendAppMessage()">Send Message</button>

        <script>
          call = window.Daily.createFrame();
          call.on('app-message', (event) => {
            console.log('app-message', event);
          });
          
          call.join({ url: 'YOUR_CONVERSATION_URL' });

          function sendAppMessage() {
            const messageInput = document.getElementById('messageInput');
            const message = messageInput.value;
            if (message) {
              const interaction = {
                "message_type": "conversation",
                "event_type": "conversation.echo",
                "conversation_id": "YOUR_CONVERSATION_ID",
                "properties": {
                  "text": `${message}`
                }
              }
              const hi = call.sendAppMessage(interaction, '*');
              console.log('Sending message: ', hi);
              console.log('Sent message: ', interaction);
              messageInput.value = '';
            }
          }
        </script>
      </body>
    </html>
    ```
  </Tab>

  <Tab title="Daily Python">
    Hereâ€™s an example of using <a href="https://docs.daily.co/reference/daily-python">Daily Python</a> to create a call client in Python:

    <Note>
      The Daily `app-message` event is used to send and receive events and interactions between your server and CVI.
    </Note>

    ```py theme={null}
    call_client = None

    class RoomHandler(EventHandler):
        def __init__(self):
            super().__init__()
        
        def on_app_message(self, message, sender: str) -> None:
            print(f"Incoming app message from {sender}: {message}")

    def join_room(url):
        global call_client
        try:
            Daily.init()
            output_handler = RoomHandler()
            call_client = CallClient(event_handler=output_handler)
            call_client.join(url)
        except Exception as e:
            print(f"Error joining room: {e}")
            raise

    def send_message(message):
        global call_client
        call_client.send_app_message(message)
    ```
  </Tab>

  <Tab title="Daily React">
    Hereâ€™s an example of using <a href="https://docs.daily.co/reference/daily-react">Daily React</a> to create a call client in React:

    <Note>
      The Daily `app-message` event is used to send and receive events and interactions between your server and CVI.
    </Note>

    ```tsx theme={null}
    "use client"

    import React, { useEffect, useRef, useState } from 'react';


    const TavusConversation = () => {
      const [message, setMessage] = useState('');
      const callRef = useRef(null);
      const containerRef = useRef(null);


      useEffect(() => {
        const loadDaily = async () => {
          const DailyIframe = (await import('@daily-co/daily-js')).default;


          callRef.current = DailyIframe.createFrame({
            iframeStyle: {
              width: '100%',
              height: '500px',
              border: '0',
            }
          });


          if (containerRef.current) {
            containerRef.current.appendChild(callRef.current.iframe());
          }


          callRef.current.on('app-message', (event) => {
            console.log('app-message received:', event);
          });


          callRef.current.join({
            url: 'YOUR_CONVERSATION_URL',
          });
        };


        loadDaily();


        return () => {
          if (callRef.current) {
            callRef.current.leave();
            callRef.current.destroy();
          }
        };
      }, []);


      const sendAppMessage = () => {
        if (!message || !callRef.current) return;


        const interaction = {
          message_type: 'conversation',
          event_type: 'conversation.echo',
          conversation_id: 'YOUR_CONVERSATION_ID',
          properties: { text: message }
        };


        callRef.current.sendAppMessage(interaction, '*');
        setMessage('');
      };


      return (
        <div className="w-full h-full flex flex-col items-center">
          <div ref={containerRef} className="w-full mb-4" />
          <div>
            <input
              type="text"
              className="border p-2 mr-2"
              value={message}
              onChange={(e) => setMessage(e.target.value)}
              placeholder="Type a message"
            />
            <button onClick={sendAppMessage} className="bg-blue-500 text-white px-4 py-2 rounded">
              Send Message
            </button>
          </div>
        </div>
      );
    };


    export default TavusConversation;
    ```
  </Tab>
</Tabs>


# Knowledge Base
Source: https://docs.tavus.io/sections/conversational-video-interface/knowledge-base

Upload documents to your knowledge base for personas to reference during conversations.

<Note>
  For now, our Knowledge Base only supports documents written in English and works best for conversations in English.

  Weâ€™ll be expanding our Knowledge Base language support soon!
</Note>

Our Knowledge Base system uses RAG (Retrieval-Augmented Generation) to process and and transform the contents of your documents and websites, allowing your personas to dynamically access and leverage information naturally during a conversation.

During a conversation, our persona will continuously analyze conversation content and pull relevant information from the documents that you have selected during conversation creation as added context.

## Getting Started With Your Knowledge Base

To leverage the Knowledge Base, you will need to upload documents or website URLs that you intend to reference from in conversations.
Let's walk through how to upload your documents and use them in a conversation.

<Note>
  You can either use our [Developer Portal](https://platform.tavus.io/documents) or API endpoints to upload and manage your documents.
  Our Knowledge Base supports creating documents from an uploaded file or a website URL.
</Note>

<Steps>
  <Step title="Step 1: Ensure Website Resources are Publicly Accessible">
    For any documents to be created via website URL, please make sure that each document is publicly accessible without requiring authorization, such as a pre-signed S3 link.

    For example, entering the URL in a browser should either:

    * Open the website you want to process and save contents from.
    * Open a document in a PDF viewer.
    * Download the document.
  </Step>

  <Step title="Step 2: Upload your Documents">
    You can create documents using either the [Developer Portal](https://platform.tavus.io/documents) or the [Create Document](https://docs.tavus.io/api-reference/documents/create-document) API endpoint.

    If you want to use the API, you can send a request to Tavus to upload your document.

    Here's an example of a `POST` request to `tavusapi.com/v2/documents`.

    ```json theme={null}
    {
        "document_name": "test-doc-1",
        "document_url": "https://your.document.pdf",
        "callback_url": "webhook-url-to-get-progress-updates" // Optional
    }
    ```

    The response from this POST request will include a `document_id` - a unique identifier for your uploaded document. When creating a conversation, you may include all `document_id` values that you would like the persona to have access to.

    Currently, we support the following file formats: .pdf, .txt, .docx, .doc, .png, .jpg, .pptx, .csv, and .xlsx.
  </Step>

  <Step title="Step 3: Document Processing">
    After your document is uploaded, it will be processed in the background automatically to allow for incredibly fast retrieval during conversations.
    This process can take 5-10 minutes depending on document size.

    During processing, if you have provided a `callback_url` in the [Create Document](https://docs.tavus.io/api-reference/documents/create-document) request body, you will receive periodic callbacks with status updates.
    You may also use the [Get Document](https://docs.tavus.io/api-reference/documents/get-document) endpoint to poll the most recent status of your documents.
  </Step>

  <Step title="Step 4: Create a conversation with the document">
    Once your documents have finished processing, you may use the `document_id` from Step 2 as part of the [Create Conversation](https://docs.tavus.io/api-reference/conversations/create-conversation) request.

    You can add multiple documents to a conversation within the `document_ids` object.

    ```json theme={null}
    {
      "persona_id": "your_persona_id",
      "replica_id": "your_replica_id",
      "document_ids": ["d1234567890", "d1234567891"]
    }
    ```

    During your conversation, the persona will be able to reference information from your documents in real time.
  </Step>
</Steps>

## Retrieval Strategy

When creating a conversation with documents, you can optimize how the system searches through your knowledge base by specifying a retrieval strategy. This strategy determines the balance between search speed and the quality of retrieved information, allowing you to fine-tune the system based on your specific needs.

You can choose from three different strategies:

* `speed`: Optimizes for faster retrieval times for minimal latency.
* `balanced` (default): Provides a balance between retrieval speed and quality.
* `quality`: Prioritizes finding the most relevant information, which may take slightly longer but can provide more accurate responses.

```json theme={null}
{
  "persona_id": "your_persona_id",
  "replica_id": "your_replica_id",
  "document_ids": ["d1234567890"],
  "document_retrieval_strategy": "balanced"
}
```

## Document Tags

If you have a lot of documents, maintaining long lists of `document_id` values can get tricky.
Instead of using distinct `document_ids`, you can also group documents together with shared tag values.
During the [Create Document](https://docs.tavus.io/api-reference/documents/create-document) API call, you may specify a value for `tags` for your document.
Then, when you create a conversation, you may specify the `tags` value instead of passing in discrete `document_id` values.

For example, if you are uploading course material, you could add the tag `"lesson-1"` to all documents that you want accessible in the first lesson.

```json theme={null}
{
        "document_name": "test-doc-1",
        "document_url": "https://your.document.pdf",
        "tags": ["lesson-1"]
}
```

In the [Create Conversation](https://docs.tavus.io/api-reference/conversations/create-conversation) request, you can add the tag value `lesson-1` to `document_tags` instead of individual `document_id` values.

```json theme={null}
{
  "persona_id": "your_persona_id",
  "replica_id": "your_replica_id",
  "document_tags": ["lesson-1"]
}
```

## Website Crawling

When adding a website to your knowledge base, you have two options:

### Single Page Scraping (Default)

By default, when you provide a website URL, only that single page is scraped and processed. This is ideal for:

* Landing pages with concentrated information
* Specific articles or blog posts
* Individual product pages

### Multi-Page Crawling

For comprehensive coverage of a website, you can enable **crawling** by providing a `crawl` configuration. This tells the system to start at your URL and follow links to discover and process additional pages.

```json theme={null}
{
  "document_name": "Company Docs",
  "document_url": "https://docs.example.com/",
  "crawl": {
    "depth": 2,
    "max_pages": 25
  }
}
```

#### Crawl Parameters

| Parameter   | Range | Description                                                                                                                                                                 |
| ----------- | ----- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `depth`     | 1-10  | How many link levels to follow from the starting URL. A depth of 1 crawls pages directly linked from your starting URL; depth of 2 follows links on those pages, and so on. |
| `max_pages` | 1-100 | Maximum number of pages to process. Crawling stops when this limit is reached.                                                                                              |

#### Crawl Limits

To ensure fair usage and system stability:

* Maximum **100 crawl documents** per account
* Maximum **5 concurrent crawls** at any time
* **1-hour cooldown** between recrawls of the same document

## Keeping Content Fresh

Website content changes over time, and you may need to update your knowledge base to reflect those changes. For documents created with crawl configuration, you can trigger a **recrawl** to fetch fresh content.

### Using the Recrawl Endpoint

Send a POST request to recrawl an existing document:

```bash theme={null}
POST https://tavusapi.com/v2/documents/{document_id}/recrawl
```

The recrawl will:

1. Use the same starting URL and crawl configuration
2. Replace old content with the new content
3. Update `last_crawled_at` and increment `crawl_count`

### Optionally Override Crawl Settings

You can provide new crawl settings when triggering a recrawl:

```json theme={null}
{
  "crawl": {
    "depth": 3,
    "max_pages": 50
  }
}
```

### Recrawl Requirements

* Document must be in `ready` or `error` state
* At least 1 hour must have passed since the last crawl
* Document must have been created with crawl configuration

See the [Recrawl Document API reference](/api-reference/documents/recrawl-document) for complete details.


# Language Support
Source: https://docs.tavus.io/sections/conversational-video-interface/language-support

Customize the conversation language using full language names supported by Tavus TTS engines.

## Supported Languages

Tavus supports over 30 languages for spoken interaction, powered by two integrated text-to-speech (TTS) engines: Cartesia and ElevenLabs.
If a selected language is not supported by our default TTS engine (Cartesia), your CVI will automatically switch to ElevenLabs to kick off the conversation.

* English (all variants)
* French (France, Canada)
* German
* Spanish (Spain, Mexico)
* Portuguese (Brazil, Portugal)
* Chinese
* Japanese
* Hindi
* Italian
* Korean
* Dutch
* Polish
* Russian
* Swedish
* Turkish
* Indonesian
* Filipino
* Bulgarian
* Romanian
* Arabic (Saudi Arabia, UAE)
* Czech
* Greek
* Finnish
* Croatian
* Malay
* Slovak
* Danish
* Tamil
* Ukrainian
* Hungarian
* Norwegian
* Vietnamese

For a full list of supported languages for each TTS engine, please click on the following links:

<CardGroup>
  <Card title="Cartesia (default)" icon="c" href="https://docs.cartesia.ai/2024-11-13/build-with-cartesia/models/tts#language-support" />

  <Card title="ElevenLabs" icon="tally-2" href="https://elevenlabs.io/docs/capabilities/text-to-speech#supported-languages" />
</CardGroup>

<Note>
  By default, Tavus uses the **Cartesia** TTS engine.
</Note>

## Setting the Conversation Language

To specify a language, use the `language` parameter in the <a href="/api-reference/conversations/create-conversation">Create Conversation</a>. **You must use the full language name**, not a language code.

```shell cURL {9} theme={null}
curl --request POST \
  --url https://tavusapi.com/v2/conversations \
  --header 'Content-Type: application/json' \
  --header 'x-api-key: <api_key>' \
  --data '{
  "persona_id": "pdced222244b",
  "replica_id": "rfe12d8b9597",
  "properties": {
    "language": "spanish"
   }
}'
```

<Note>
  Language names must match exactly with those supported by the selected TTS engine.
</Note>

### Smart Language Detection

To automatically detect the participantâ€™s spoken language throughout the conversation, set `language` to `multilingual` when creating the conversation:

```shell cURL {9} theme={null}
curl --request POST \
  --url https://tavusapi.com/v2/conversations \
  --header 'Content-Type: application/json' \
  --header 'x-api-key: <api_key>' \
  --data '{
  "persona_id": "pdced222244b",
  "replica_id": "rfe12d8b9597",
  "properties": {
    "language": "multilingual"
   }
}'
```

This enables ASR (Automatic Speech Recognition) to automatically switch languages, dynamically adjusting the pipeline to transcribe and respond in the detected language throughout the conversation.


# Memories
Source: https://docs.tavus.io/sections/conversational-video-interface/memories

Memories let personas remember information across conversations, allowing participants to have personalized, flowing conversations across multiple sessions.

Memories are pieces of information that the persona learns during a conversation. Once learned, these memories can be referenced and used by the persona during subsequent conversations.

Developers are able to organize memories within `memory_stores` - a flexible tag-based system to track memories across conversations and participants into different buckets.
If a `memory_stores` value is provided in the conversation creation request, memories will automatically be created and associated to the tag provided.

<Note>
  When defining `memory_stores` values, we recommend incorporating static values that will not change with persona updates, like persona ID.

  For example, using a persona's name as part of your `memory_stores` values could result in memories being miscategorized if you were to change their name.
</Note>

## Basic Example

For example, if a participant named Anna starts a conversation with the persona (Charlie, with the persona ID `p123`), we can specify `memory_stores=["anna_p123"]` in the conversation creation request.
By doing so, Charlie will:

* Remember what was mentioned in a conversation and form new memories with Anna.
* Reference memories from previous conversations that Charlie had with Anna in new conversations.

Example [conversation creation](https://docs.tavus.io/api-reference/conversations/create-conversation) request body:

```json theme={null}
{
  "persona_id": "your_persona_id",
  "replica_id": "your_replica_id",
  "memory_stores": ["anna_p123"]
}
```

## Managing Memories Between Participants and Conversations

<Note>
  To prevent different personas from mixing up information for the same participant, we generally recommend you to create separate `memory_stores` values for each user when they talk to different personas.

  For example,\`

  * When Anna talks to Charlie (persona ID of `p123`), you can use the `memory_stores` value of `["anna-p123"]`.
  * when she talks with Gloria (persona ID of `p456`), you can use the `memory_stores` value of `["anna-p456"]`.
</Note>

The `memory_stores` system can be used flexibly to cover your use cases - they do not have to map 1:1 with your participants and instead can be designed for your unique use cases.

For example,

* If you were setting up an online classroom, you could use a `memory_stores` tag value of `"classroom-1"` so any participant of this group could reference and create new memories to enhance and deepen learning and connections.
* You can control whether you want personas to share memory or not (and if so, which personas) by passing them different `memory_stores` values.


# Overview
Source: https://docs.tavus.io/sections/conversational-video-interface/overview-cvi

CVI enables real-time, human-like video interactions through configurable lifelike replicas.

<Frame>
  <img alt="" />
</Frame>

Conversational Video Interface (CVI) is a framework for creating real-time multimodal video interactions with AI. It enables an AI agent to see, hear, and respond naturally, mirroring human conversation.

CVI is the worldâ€™s fastest interface of its kind. It allows you to map a human face and conversational ability onto your AI agent. With CVI, you can achieve utterance-to-utterance latency with SLAs under 1 second. This is the full round-trip time for a participant to say something and the replica to reply.

CVI provides a comprehensive solution, with the option to plug in your existing components as required.

## Key Concepts

CVI is built around three core concepts that work together to create real-time, humanlike interactions with an AI agent:

<CardGroup>
  <Card title="Persona" icon="heart-pulse" href="/sections/conversational-video-interface/persona/overview">
    The **Persona** defines the agentâ€™s behavior, tone, and knowledge. It also configures the CVI layer and pipeline.
  </Card>

  <Card title="Replica" icon="user-group" href="/sections/replica/overview">
    The **Replica** brings the persona to life visually. It renders a photorealistic human-like avatar using the **Phoenix-3** model.
  </Card>

  <Card title="Conversation" icon="video" href="/sections/conversational-video-interface/conversation/overview">
    A **Conversation** is a real-time video session that connects the persona and replica through a WebRTC connection.
  </Card>
</CardGroup>

## Key Features

<CardGroup>
  <Card title="Natural Interaction" icon="face-smile-beam">
    CVI uses facial cues, body language, and real-time turn-taking to enable natural, human-like conversations.
  </Card>

  <Card title="Modular pipeline" icon="layer-group">
    Customize the Perception, STT, LLM and TTS layers to control identity, behavior, and responses.
  </Card>

  <Card title="Lifelike AI replicas" icon="user-robot">
    Choose from over 100+ hyper-realistic digital twins or customize your own with human-like voice and expression.
  </Card>

  <Card title="Multilingual support" icon="globe">
    Hold natural conversations in 30+ languages using the supported TTS engines.
  </Card>

  <Card title="World's lowest latency" icon="bolt">
    Experience real-time interactions with \~600ms response time and smooth turn-taking.
  </Card>
</CardGroup>

## Layers

The Conversational Video Interface (CVI) is built on a modular layer system, where each layer handles a specific part of the interaction. Together, they capture input, process it, and generate a real-time, human-like response.

Hereâ€™s how the layers work together:

<AccordionGroup>
  <Accordion title="1. Transport" icon="right-left">
    Handles real-time audio and video streaming using WebRTC (powered by Daily). This layer captures the user's microphone and camera input and delivers output back to the user.

    This layer is always enabled. You can configure input/output for audio (mic) and video (camera).
  </Accordion>

  <Accordion title="2. Perception" icon="eye">
    Uses **Raven** to analyze user expressions, gaze, background, and screen content. This visual context helps the replica understand and respond more naturally.

    [Click here to learn how to configure the Perception layer.](/sections/conversational-video-interface/persona/perception)
  </Accordion>

  <Accordion title="3. Conversational Flow" icon="comments">
    Controls the natural dynamics of conversation, including turn-taking and interruptibility. Uses **Sparrow** for intelligent turn detection, enabling the replica to decide when to speak and when to listen.

    [Click here to learn how to configure the Conversational Flow layer.](/sections/conversational-video-interface/persona/conversational-flow)
  </Accordion>

  <Accordion title="4. Speech Recognition (STT)" icon="ear-listen">
    This layer transcribes user speech in real time with lexical and semantic awareness.

    [Click here to learn how to configure the Speech Recognition (STT) layer.](/sections/conversational-video-interface/persona/stt)
  </Accordion>

  <Accordion title="5. Large Language Model (LLM)" icon="brain">
    Processes the user's transcribed speech and visual input using a low-latency LLM. Tavus provides ultra-low latency optimized LLMs or lets you integrate your own.

    [Click here to learn how to configure the Large Language Model (LLM) layer.](/sections/conversational-video-interface/persona/llm)
  </Accordion>

  <Accordion title="6. Text-to-Speech (TTS)" icon="volume-high">
    Converts the LLM response into speech using the supported TTS Engines (Cartesia **(Default)**, ElevenLabs).

    [Click here to learn how to configure the Text-to-Speech (TTS) layer.](/sections/conversational-video-interface/persona/tts)
  </Accordion>

  <Accordion title="7. Realtime Replica" icon="face-smile">
    Delivers a high-quality, synchronized digital human response using Tavus's real-time avatar engine powered by **Phoenix**.

    [Click here to learn more about the Replica layer.](/sections/replica/overview)
  </Accordion>
</AccordionGroup>

<Note>
  Most layers are configurable via the [Persona](/sections/conversational-video-interface/persona/overview).
</Note>

## Getting Started

You can quickly create a conversation by using the <a href="https://platform.tavus.io/">Developer Portal</a> or following the steps in the [Quickstart](/sections/conversational-video-interface/quickstart/use-the-full-pipeline) guide.

<div>
  <span>If you use Cursor, use this pre-built prompt to get started faster:</span>

  <button>
    <span> Copy </span>
  </button>
</div>

<div>
  ## âœ… **System Prompt for AI: React (Vite) + Tavus CVI Integration**

  **Purpose:**
  Generate **React (TypeScript)** apps with Tavus CVI using **Vite**, following the official docs and GitHub examples:
  [https://docs.tavus.io/sections/integrations/embedding-cvi](https://docs.tavus.io/sections/integrations/embedding-cvi)

  ***

  ### âœ… **AI MUST ALWAYS DO THE FOLLOWING:**

  #### **1. Setup React App Using Vite**

  ```bash theme={null}
  npm create vite@latest my-tavus-app -- --template react-ts
  cd my-tavus-app
  npm install
  ```

  ***

  #### **2. Install Tavus CVI UI Components**

  ```bash theme={null}
  npx @tavus/cvi-ui@latest init
  npx @tavus/cvi-ui@latest add conversation
  ```

  âœ… This creates:

  ```
  src/components/cvi/components/
    cvi-provider.tsx
    conversation.tsx
  ```

  ***

  #### **3. Wrap App with `CVIProvider`**

  Update `src/App.tsx`:

  ```tsx theme={null}
  import { CVIProvider } from "./components/cvi/components/cvi-provider";

  function App() {
    return <CVIProvider>{/* Your app content */}</CVIProvider>;
  }
  ```

  ***

  #### **4. Create Conversation API Call**

  Always use this pattern:

  ```ts theme={null}
  const response = await fetch("https://tavusapi.com/v2/conversations", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      "x-api-key": import.meta.env.VITE_TAVUS_API_KEY || "",
    },
    body: JSON.stringify({
      replica_id: import.meta.env.VITE_REPLICA_ID || "rfe12d8b9597",  // Default
      persona_id: import.meta.env.VITE_PERSONA_ID || "pdced222244b",  // Default
    }),
  });

  const data = await response.json();
  setConversationUrl(data.conversation_url);
  ```

  âœ… **Explain:** `conversation_url` from API response is used inside `<Conversation>`.

  ***

  #### **5. Render `Conversation` Inside `CVIProvider`**

  ```tsx theme={null}
  <Conversation
    conversationUrl={conversationUrl}
    onLeave={() => setConversationUrl(null)}
  />
  ```

  ***

  #### **6. Add Environment Variable**

  Create `.env` in the **`my-tavus-app`** folder (if not already there):

  ```
  VITE_TAVUS_API_KEY=your_api_key_here
  VITE_REPLICA_ID=rfe12d8b9597   // Default replica_id, can be overridden in .env
  VITE_PERSONA_ID=pdced222244b  // Default persona_id, can be overridden in .env
  ```

  * **Important**: **DO NOT** create `.env` outside of the `my-tavus-app` folder. It must be inside your project directory where `src` exists.

  Access via:

  ```ts theme={null}
  import.meta.env.VITE_TAVUS_API_KEY
  import.meta.env.VITE_REPLICA_ID
  import.meta.env.VITE_PERSONA_ID
  ```

  ***

  #### **7. Create `cvi-components.json` by Default**

  Ensure that the `cvi-components.json` file is generated in the `my-tavus-app` folder with the following content:

  ```json theme={null}
  {
    "tsx": true
  }
  ```

  ***

  ### âœ… **Expected AI Output Example**

  **`src/App.tsx`:**

  ```tsx theme={null}
  import React, { useState } from "react";
  import { CVIProvider } from "./components/cvi/components/cvi-provider";
  import { Conversation } from "./components/cvi/components/conversation";

  const App: React.FC = () => {
    const [conversationUrl, setConversationUrl] = useState<string | null>(null);

    const createConversation = async () => {
      const response = await fetch("https://tavusapi.com/v2/conversations", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          "x-api-key": import.meta.env.VITE_TAVUS_API_KEY || "",
        },
        body: JSON.stringify({
          replica_id: import.meta.env.VITE_REPLICA_ID || "rfe12d8b9597",
          persona_id: import.meta.env.VITE_PERSONA_ID || "pdced222244b",
        }),
      });

      const data = await response.json();
      setConversationUrl(data.conversation_url);
    };

    return (
      <CVIProvider>
        <div
          style={{
            display: "flex",
            justifyContent: "center",
            alignItems: "center",
            width: "100vw",
            height: "100vh",
            backgroundColor: "#1e1e1e",
            color: "#fff",
            textAlign: "center",
            flexDirection: "column",
            margin: 0,
            padding: 0,
          }}
        >
          <h1 style={{ marginBottom: "1rem" }}>Tavus CVI Integration (Vite)</h1>
          {!conversationUrl ? (
            <button
              onClick={createConversation}
              style={{
                padding: "0.75rem 1.5rem",
                fontSize: "1rem",
                background: "#6a0dad",  // Purple for light and dark modes
                color: "#fff",
                border: "none",
                borderRadius: "6px",
                cursor: "pointer",
              }}
            >
              Start Conversation
            </button>
          ) : (
            <div style={{ width: "100%", maxWidth: "800px" }}>
              <Conversation
                conversationUrl={conversationUrl}
                onLeave={() => setConversationUrl(null)}
              />
            </div>
          )}
        </div>
      </CVIProvider>
    );
  };

  export default App;
  ```

  ***

  ### âœ… **Checklist for AI Output**

  * âœ… Uses **Vite + React TypeScript** (`npm create vite@latest --template react-ts`)
  * âœ… Includes `npx @tavus/cvi-ui init` and `add conversation`
  * âœ… Imports `CVIProvider` and `Conversation` from `src/components/cvi/components/`
  * âœ… Uses correct API call with:

    * `https://tavusapi.com/v2/conversations`
    * Headers: `Content-Type`, `x-api-key`
    * Body: `replica_id` & `persona_id`
  * âœ… Uses `import.meta.env.VITE_TAVUS_API_KEY`
  * âœ… Renders `<Conversation>` inside `<CVIProvider>`
  * âœ… Purple button is visible in both light and dark modes
  * âœ… `.env` is created inside the correct project folder (`my-tavus-app`)
  * âœ… `cvi-components.json` is created by default with `{ "tsx": true }`

  ***

  ### Keep things in mind:

  * If you're already in the `my-tavus-app` folder, avoid running `cd my-tavus-app` again. Check your current folder before running commands.
  * After running the necessary setup, remember to run `npm run dev` to start your app.
  * Do **NOT** place the `.env` file outside of the project folder. It must reside within the `my-tavus-app` directory.
</div>


# Conversational Flow
Source: https://docs.tavus.io/sections/conversational-video-interface/persona/conversational-flow

Learn how to configure the Conversational Flow layer to fine-tune turn-taking and interruption handling behavior.

The **Conversational Flow Layer** in Tavus gives you precise control over the natural dynamics of conversation. This layer allows you to customize how your replica handles turn-taking and interruptions to create conversational experiences that match your specific use case.

## Understanding Conversational Flow

Conversational flow encompasses the subtle dynamics that make conversations feel natural:

* **Turn-taking**: How the replica decides when to speak and when to listen
* **Interruptibility**: How easily the replica can be interrupted by the user

<Note>
  All conversational flow parameters are optional. When not explicitly configured, the layer remains inactive. However, if you configure any single parameter, the system will apply sensible defaults to all other parameters to ensure consistent behavior.
</Note>

## Configuring the Conversational Flow Layer

<Note>
  If you're migrating from sparrow-0 (formerly called `smart_turn_detection` on the STT Layer) then check out the [migration guide here](/sections/troubleshooting#conversational-flow-vs-stt-relationship-and-migration).
</Note>

Define the conversational flow layer under the `layers.conversational_flow` object. Below are the parameters available:

### 1. `turn_detection_model`

Specifies the model used for detecting conversational turns.

* **Options**:
  * `sparrow-1`: Advanced turn detection model - faster, more accurate, and more natural than `sparrow-0` **(recommended)**
  * `sparrow-0`: Legacy turn detection model (available for backward compatibility)
  * `time-based`: Simple timeout-based turn detection

* **Default**: `sparrow-1`

```json theme={null}
"turn_detection_model": "sparrow-1"
```

<Tip>
  **Sparrow-1 is recommended for all use cases** as it provides superior performance with faster response times, higher accuracy, and more natural conversational flow compared to the legacy Sparrow-0.
</Tip>

### 2. `turn_taking_patience`

Controls how eagerly the replica claims conversational turns. This affects both response latency and the likelihood of interrupting during natural pauses.

* **Options**:
  * `low`: Eager and quick to respond. May interrupt natural pauses. Best for rapid-fire exchanges or customer service scenarios where speed is prioritized.
  * `medium` **(default)**: Balanced behavior. Waits for appropriate conversational cues before responding.
  * `high`: Patient and waits for clear turn completion. Ideal for thoughtful conversations, interviews, or therapeutic contexts.

```json theme={null}
"turn_taking_patience": "medium"
```

**Use Cases:**

* `low`: Fast-paced customer support, quick information lookups, casual chat
* `medium`: General purpose conversations, sales calls, presentations
* `high`: Medical consultations, legal advice, counseling sessions

### 3. `replica_interruptibility`

Controls how sensitive the replica is to user speech while the replica is talking. Determines whether the replica stops to listen or keeps speaking when interrupted.

* **Options**:
  * `low`: Less interruptible. The replica keeps talking through minor interruptions.
  * `medium` **(default)**: Balanced sensitivity. Responds to clear interruption attempts.
  * `high`: Highly sensitive. Stops easily when the user begins speaking, maximizing user control.

```json theme={null}
"replica_interruptibility": "high"
```

**Use Cases:**

* `low`: Educational content delivery, storytelling, guided onboarding
* `medium`: Standard conversations, interviews, consultations
* `high`: User-driven conversations, troubleshooting, interactive support

## Default Behavior

When the conversational flow layer is not configured, all parameters default to `None` and the layer remains inactive. However, if you configure **any single parameter**, the system automatically applies the following defaults to ensure consistent behavior:

* `turn_detection_model`: `sparrow-1`
* `turn_taking_patience`: `medium`
* `replica_interruptibility`: `medium`

## Example Configurations

The following example configurations demonstrate how to tune conversational timing and interruption behavior for different use cases. Use `turn_taking_patience` to bias how quickly the replica responds after a user finishes speaking. Set it high when the replica should avoid interrupting, and low when fast responses are preferred. Use `replica_interruptibility` to control how easily the replica recalculates its response when interrupted; lower values are recommended for most experiences, with higher values reserved for cases where frequent, abrupt interruptions are desirable. Sparrow-1 dynamically handles turn-taking in all cases, with these settings acting as guiding biases rather than hard rules.

### Example 1: Customer Support Agent

Fast, responsive, and easily interruptible for customer-driven conversations:

```json theme={null}
{
  "persona_name": "Support Agent",
  "system_prompt": "You are a helpful customer support agent...",
  "pipeline_mode": "full",
  "default_replica_id": "rfe12d8b9597",
  "layers": {
    "conversational_flow": {
      "turn_detection_model": "sparrow-1",
      "turn_taking_patience": "low",
      "replica_interruptibility": "medium"
    }
  }
}
```

### Example 2: Medical Consultation

Patient, thoughtful, with engaged listening for sensitive conversations:

```json theme={null}
{
  "persona_name": "Medical Advisor",
  "system_prompt": "You are a compassionate medical professional...",
  "pipeline_mode": "full",
  "default_replica_id": "rfe12d8b9597",
  "layers": {
    "conversational_flow": {
      "turn_detection_model": "sparrow-1",
      "turn_taking_patience": "high",
      "replica_interruptibility": "verylow"
    }
  }
}
```

### Example 3: Educational Instructor

Delivers complete information with minimal interruption:

```json theme={null}
{
  "persona_name": "Instructor",
  "system_prompt": "You are an experienced educator teaching complex topics...",
  "pipeline_mode": "full",
  "default_replica_id": "rfe12d8b9597",
  "layers": {
    "conversational_flow": {
      "turn_detection_model": "sparrow-1",
      "turn_taking_patience": "medium",
      "replica_interruptibility": "low"
    }
  }
}
```

### Example 4: Minimal Configuration

Configure just one parameterâ€”others will use defaults:

```json theme={null}
{
  "persona_name": "Quick Chat",
  "system_prompt": "You are a friendly conversational AI...",
  "pipeline_mode": "full",
  "default_replica_id": "rfe12d8b9597",
  "layers": {
    "conversational_flow": {
      "turn_taking_patience": "low"
    }
  }
}
```

In this example, the system will automatically set:

* `turn_detection_model`: `sparrow-1`
* `replica_interruptibility`: `medium`

## Best Practices

### Match Flow to Use Case

Choose conversational flow settings that align with your application's purpose:

* **Speed-critical applications**: Use `low` turn-taking patience and `high` interruptibility
* **Thoughtful conversations**: Use `high` turn-taking patience
* **Important information delivery**: Use `low` interruptibility
* **User-controlled interactions**: Use `high` interruptibility

### Consider Cultural Context

Conversational norms vary across cultures. Some cultures prefer:

* More overlap and interruption (consider lower commitment, higher interruptibility)
* Clear turn-taking with pauses (consider higher patience, lower interruptibility)

### Test with Real Users

Conversational flow preferences can be subjective. Test your configuration with representative users to ensure it feels natural for your audience.

<Note>
  Refer to the <a href="/api-reference/personas/create-persona">Create Persona API</a> for the complete API specification and additional persona configuration options.
</Note>


# Guardrails
Source: https://docs.tavus.io/sections/conversational-video-interface/persona/guardrails

Guardrails provide your persona with strict behavioral guidelines that will be rigorously followed throughout every conversation.

Guardrails act as a safety layer that works alongside your system prompt to enforce specific rules, restrictions, and behavioral patterns that your persona must adhere to during conversations.

For example, if you're creating a customer service persona for a financial institution, you can apply guardrails that prevent the persona from discussing a competitor's products, sharing sensitive financial data, or providing investment advice outside of approved guidelines.

<Note>
  It is highly recommended to use the [Persona Builder](https://platform.tavus.io/conversations/builder) to create your guardrails, although you can use the [Create Guardrails](/api-reference/guardrails/create-guardrails) API directly.
</Note>

When designing your guardrails with the Persona Builder, it's helpful to keep a few things in mind:

* Be specific about what topics, behaviors, or responses should be restricted or avoided.
* Consider edge cases where participants might try to circumvent the guardrails through creative prompting.
* Ensure your guardrails complement, rather than contradict, your persona's system prompt and intended functionality.
* Test your guardrails with various conversation scenarios to ensure they activate appropriately without being overly restrictive.

If you would like to manually attach guardrails to a persona, you can either:

* Add them during [persona creation](/api-reference/personas/create-persona) like this:

```sh theme={null}
curl --request POST \
  --url https://tavusapi.com/v2/personas/ \
  --header 'Content-Type: application/json' \
  --header 'x-api-key: <api-key>' \
  --data '{
    "system_prompt": "You are a health intake assistant.",
    "guardrails_id": "g12345"
  }'
```

OR

* Add them by [editing the persona](/api-reference/personas/patch-persona) like this:

```sh theme={null}
curl --request PATCH \
  --url https://tavusapi.com/v2/personas/{persona_id} \
  --header 'Content-Type: application/json' \
  --header 'x-api-key: <api-key>' \
  --data '[
    {"op": "add", "path": "/guardrails_id", "value": "g12345"}
  ]'
```

<Note>
  For the best results, try creating specific guardrails for different types of personas or conversation contexts.

  For example, a healthcare consultation might use guardrails to maintain medical compliance, while an educational tutor might use guardrails to enforce child safety and appropriate content guidelines.
</Note>

## Parameters

<Note>
  Within each set of guardrails, you can have multiple guardrail objects defined.
</Note>

### `guardrails_name`

A desciptive name for an individual guardrail.

Example: `"Never Discuss Competitor's Products"`

<Note>
  This must be a string value without spaces.
</Note>

### `guardrails_prompt`

A text prompt that explains what particular behavior(s) should be observed for a particular guardrail. The more detail you can provide, the better.

Example: `"Only mention products within Our Company Inc. during conversations, and never discuss competitors' products."`

### `modality`

This value represents whether a specific guardrail should be enforced based on the participant's verbal or visual responses. Each individual guardrail can be visual or verbal (not both), but this can vary across the same set of guardrails.

<Note>
  The default value for `modality` is `"verbal"`.
</Note>

### `callback_url` (optional)

A URL that you can send notifications to when a particular guardrail has been triggered.

Example: `"https://your-server.com/guardrails-webhook"`

# Example Guardrails

```json theme={null}
{
  "guardrails_id": "g12345",
  "data": [
    {
      "guardrails_name": "Healthcare Compliance Guardrails",
      "guardrails_prompt": "Never share sensitive medical information or provide medical advice outside approved guidelines",
      "modality": "verbal",
      "callback_url": "https://your-server.com/guardrails-webhook"
    },
    {
      "guardrails_name": "Check if the participant is alone",
      "guardrails_prompt": "Confirm throughout the call that the participant is alone (i.e. not with other individuals in the background) throughout the call.",
      "modality": "visual"
    }
  ]
}
```


# Large Language Model (LLM)
Source: https://docs.tavus.io/sections/conversational-video-interface/persona/llm

Learn how to use Tavus-optimized LLMs or integrate your own custom LLM.

The **LLM Layer** in Tavus enables your persona to generate intelligent, context-aware responses. You can use Tavus-hosted models or connect your own OpenAI-compatible LLM.

## Tavus-Hosted Models

### 1. `model`

Select one of the available models:

* `tavus-gpt-oss` (Recommended)
* `tavus-gpt-4o`
* `tavus-gpt-4o-mini`

<Note>
  **Context Window Limit**

  * All Tavus-hosted models have a **limit of 32,000 tokens**.
  * Contexts over **25,000 tokens** will experience noticeable performance degradation (slow response times).

  **Tip**: 1 token â‰ˆ 4 characters, therefore 32,000 tokens â‰ˆ 128,000 characters (including spaces and punctuation).
</Note>

```json theme={null}
"model": "tavus-gpt-oss"
```

### 2. `tools`

Optionally enable tool calling by defining functions the LLM can invoke.

<Note>
  Please see [LLM Tool Calling](/sections/conversational-video-interface/persona/llm-tool) for more details.
</Note>

### 3. `speculative_inference`

When set to `true`, the LLM begins processing speech transcriptions before user input ends, improving responsiveness.

```json theme={null}
"speculative_inference": true
```

<Note>
  This is field is optional, but recommended for better performance.
</Note>

### 4. `extra_body`

Add parameters to customize the LLM request. For Tavus-hosted models, you can pass `temperature` and `top_p`:

```json theme={null}
"extra_body": {
  "temperature": 0.7,
  "top_p": 0.9
}
```

<Note>
  This field is optional.
</Note>

### Example Configuration

```json theme={null}
{
  "persona_name": "Health Coach",
  "system_prompt": "You provide wellness tips and encouragement for people pursuing a healthy lifestyle.",
  "context": "You specialize in daily routines, diet advice, and motivational support.",
  "pipeline_mode": "full",
  "default_replica_id": "r665388ec672",
  "layers": {
    "llm": {
      "model": "tavus-gpt-4o",
      "speculative_inference": true,
      "extra_body": {
        "temperature": 0.7,
        "top_p": 0.9
      }
    }
  }
}
```

## Custom LLMs

### Prerequisites

To use your own OpenAI-compatible LLM, you'll need:

* Model name
* Base URL
* API key

Ensure your LLM:

* Streamable (ie. via SSE)
* Uses the `/chat/completions` endpoint

### 1. `model`

Name of the custom model you want to use.

```json theme={null}
"model": "gpt-3.5-turbo"
```

### 2. `base_url`

Base URL of your LLM endpoint.

<Note>
  Do not include route extensions in the `base_url`.
</Note>

```json theme={null}
"base_url": "https://your-llm.com/api/v1"
```

### 3. `api_key`

API key to authenticate with your LLM provider.

```json theme={null}
"api_key": "your-api-key"
```

<Tip>
  `base_url` and `api_key` are required only when using a custom model.
</Tip>

### 4. `tools`

Optionally enable tool calling by defining functions the LLM can invoke.

<Note>
  Please see [LLM Tool Calling](/sections/conversational-video-interface/persona/llm-tool) for more details.
</Note>

### 5. `speculative_inference`

When set to `true`, the LLM begins processing speech transcriptions before user input ends, improving responsiveness.

```json theme={null}
"speculative_inference": true
```

<Note>
  This is field is optional, but recommended for better performance.
</Note>

### 6. `headers`

Optional headers for authenticating with your LLM.

```json theme={null}
"headers": {
  "Authorization": "Bearer your-api-key"
}
```

<Note>
  This field is optional, depending on your LLM model.
</Note>

### 7. `extra_body`

Add parameters to customize the LLM request. You can pass any parameters that your LLM provider supports:

```json theme={null}
"extra_body": {
  "temperature": 0.5,
  "top_p": 0.9,
  "frequency_penalty": 0.5
}
```

<Note>
  This field is optional.
</Note>

### 8. `default_query`

Add default query parameters that get appended to the base URL when making requests to the `/chat/completions` endpoint.

```json theme={null}
"default_query": {
  "api-version": "2024-02-15-preview"
}
```

<Note>
  This field is optional. Useful for LLM providers that require query parameters for authentication or versioning.
</Note>

### Example Configuration

```json theme={null}
{
  "persona_name": "Storyteller",
  "system_prompt": "You are a storyteller who entertains people of all ages.",
  "context": "Your favorite stories include Little Red Riding Hood and The Three Little Pigs.",
  "pipeline_mode": "full",
  "default_replica_id": "r665388ec672",
  "layers": {
    "llm": {
      "model": "gpt-4o",
      "base_url": "https://your-azure-openai.openai.azure.com/openai/deployments/gpt-4o",
      "api_key": "your-api-key",
      "speculative_inference": true,
      "default_query": {
        "api-version": "2024-02-15-preview"
      }
    }
  }
}
```

<Note>
  Refer to the <a href="/api-reference/personas/create-persona">Create Persona API</a> for a full list of supported fields.
</Note>

### Perception

When using the `raven-0` perception model with a custom LLM, your LLM will receive system messages containing visual context extracted from the user's video input.

```json theme={null}
{
    "role": "system",
    "content": "<user_appearance>...</user_appearance> <user_emotions>...</user_emotions> <user_screenshare>...</user_screenshare>"
}
```

#### Basic Perception model

If you use the Basic perception model, your LLM will receive the following user messages (instead of a system message):

```json theme={null}
{
    "role": "user",
    "content": "USER_SPEECH: ... VISUAL_SCENE: ..."
}
```

#### Disabled Perception model

If you disable the perception model, your LLM will not receive any special messages.


# Tool Calling for LLM
Source: https://docs.tavus.io/sections/conversational-video-interface/persona/llm-tool

Set up tool calling to trigger functions from user speech using Tavus-hosted or custom LLMs.

**LLM tool calling** works with OpenAIâ€™s <a href="https://platform.openai.com/docs/guides/function-calling">Function Calling</a> and can be set up in the `llm` layer. It allows an AI agent to trigger functions based on user speech during a conversation.

<Note>
  You can use tool calling with our **hosted models** or any **OpenAI-compatible custom LLM**.
</Note>

## Defining Tool

### Top-Level Fields

| Field      | Type   | Required | Description                                                                                              |
| ---------- | ------ | -------- | -------------------------------------------------------------------------------------------------------- |
| `type`     | string | âœ…        | Must be `"function"` to enable tool calling.                                                             |
| `function` | object | âœ…        | Defines the function that can be called by the LLM. Contains metadata and a strict schema for arguments. |

#### `function`

| Field         | Type   | Required | Description                                                                                                                  |
| ------------- | ------ | -------- | ---------------------------------------------------------------------------------------------------------------------------- |
| `name`        | string | âœ…        | A unique identifier for the function. Must be in `snake_case`. The model uses this to refer to the function when calling it. |
| `description` | string | âœ…        | A natural language explanation of what the function does. Helps the LLM decide when to call it.                              |
| `parameters`  | object | âœ…        | A JSON Schema object that describes the expected structure of the functionâ€™s input arguments.                                |

#### `function.parameters`

| Field        | Type             | Required | Description                                                                               |
| ------------ | ---------------- | -------- | ----------------------------------------------------------------------------------------- |
| `type`       | string           | âœ…        | Always `"object"`. Indicates the expected input is a structured object.                   |
| `properties` | object           | âœ…        | Defines each expected parameter and its corresponding type, constraints, and description. |
| `required`   | array of strings | âœ…        | Specifies which parameters are mandatory for the function to execute.                     |

<Note>
  Each parameter should be included in the required list, even if they might seem optional in your code.
</Note>

##### `function.parameters.properties`

Each key inside `properties` defines a single parameter the model must supply when calling the function.

| Field              | Type   | Required | Description                                                                                 |
| ------------------ | ------ | -------- | ------------------------------------------------------------------------------------------- |
| `<parameter_name>` | object | âœ…        | Each key is a named parameter (e.g., `location`). The value is a schema for that parameter. |

Optional subfields for each parameter:

| Subfield      | Type   | Required | Description                                                                                 |
| ------------- | ------ | -------- | ------------------------------------------------------------------------------------------- |
| `type`        | string | âœ…        | Data type (e.g., `string`, `number`, `boolean`).                                            |
| `description` | string | âŒ        | Explains what the parameter represents and how it should be used.                           |
| `enum`        | array  | âŒ        | Defines a strict list of allowed values for this parameter. Useful for categorical choices. |

## Example Configuration

Hereâ€™s an example of tool calling in the `llm` layers:

<Tip>
  **Best Practices:**

  * Use clear, specific function names to reduce ambiguity.
  * Add detailed `description` fields to improve selection accuracy.
</Tip>

```json LLM Layer [expandable] theme={null}
"llm": {
  "model": "tavus-gpt-oss",
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_current_time",
        "description": "Fetch the current local time for a specified location",
        "parameters": {
          "type": "object",
          "properties": {
            "location": {
              "type": "string",
              "description": "The name of the city or region, e.g. New York, Tokyo"
            }
          },
          "required": ["location"]
        }
      }
    },
    {
      "type": "function",
      "function": {
        "name": "convert_time_zone",
        "description": "Convert time from one time zone to another",
        "parameters": {
          "type": "object",
          "properties": {
            "time": {
              "type": "string",
              "description": "The original time in ISO 8601 or HH:MM format, e.g. 14:00 or 2025-05-28T14:00"
            },
            "from_zone": {
              "type": "string",
              "description": "The source time zone, e.g. PST, EST, UTC"
            },
            "to_zone": {
              "type": "string",
              "description": "The target time zone, e.g. CET, IST, JST"
            }
          },
          "required": ["time", "from_zone", "to_zone"]
        }
      }
    }
  ]
}
```

## How Tool Calling Works

Tool calling is triggered during an active conversation when the LLM model needs to invoke a function. Hereâ€™s how the process works:

<Note>
  This example explains the `get_current_time` function from the [example configuration](#example-configuration) above.
</Note>

<Frame>
  <img alt="" />
</Frame>

## Modify Existing Tools

You can update `tools` definitions using the <a href="/api-reference/personas/patch-persona">Update Persona API</a>.

```shell [expandable] theme={null}
curl --request PATCH \
  --url https://tavusapi.com/v2/personas/{persona_id} \
  --header 'Content-Type: application/json' \
  --header 'x-api-key: <api-key>' \
  --data '[
    {
      "op": "replace",
      "path": "/layers/llm/tools",
      "value": [
        {
          "type": "function",
          "function": {
            "name": "get_current_weather",
            "description": "Get the current weather in a given location",
            "parameters": {
              "type": "object",
              "properties": {
                "location": {
                  "type": "string",
                  "description": "The city and state, e.g. San Francisco, CA"
                },
                "unit": {
                  "type": "string",
                  "enum": ["celsius", "fahrenheit"]
                }
              },
              "required": ["location", "unit"]
            }
          }
        }
      ]
    }
  ]'
```

<Note>
  Replace `<api_key>` with your actual API key. You can generate one in the <a href="https://platform.tavus.io/api-keys">Developer Portal</a>.
</Note>


# Objectives
Source: https://docs.tavus.io/sections/conversational-video-interface/persona/objectives

Objectives are goal-oriented instructions to define the desired outcomes and flow of your conversations.

Objectives work alongside your system prompt to provide a structured, flexible approach to guide conversations. They provide the most value during purposeful conversations that need to be tailored to specific processes, customer journeys, or workflows, while maintaining engaging and natural interactions.

For example, if you're creating a lead qualification persona for sales, you can set objectives to gather contact information, understand budget requirements, and assess decision-making authority before scheduling a follow-up meeting.

<Note>
  Objectives can only be created using the [Create Objectives](/api-reference/objectives/create-objectives) API.
</Note>

When designing your objectives, it's helpful to keep a few things in mind:

* Plan your entire ideal workflow. This will help create a robust branching structure that successfully takes the participant from start to finish.
* Think through the possible answers a participant might give, and ensure the workflow covers these cases.
* Ensure your persona's system prompt does not conflict with the objectives. For example, a system prompt, "You are a tutor," would not perform well with the objectives workflow of a sales associate.

## Attaching objectives to a persona

To attach objectives to a persona, you can either:

* Add them during [persona creation](/api-reference/personas/create-persona) like this:

```sh theme={null}
curl --request POST \
  --url https://tavusapi.com/v2/personas/ \
  --header 'Content-Type: application/json' \
  --header 'x-api-key: <api-key>' \
  --data '{
    "system_prompt": "You are a lead qualification assistant.",
    "objectives_id": "o12345"
  }'
```

OR

* Add them by [editing the persona](/api-reference/personas/patch-persona) like this:

```sh theme={null}
curl --request PATCH \
  --url https://tavusapi.com/v2/personas/{persona_id} \
  --header 'Content-Type: application/json' \
  --header 'x-api-key: <api-key>' \
  --data '[
    {"op": "add", "path": "/objectives_id", "value": "o12345"}
  ]'
```

<Note>
  For the best results, try creating unique objectives for different conversation purposes or business outcomes.

  For example, a customer onboarding persona might use objectives focused on data collection, while a support persona might use objectives focused on issue resolution.
</Note>

## Parameters

### `objective_name`

A desciptive name for the objective.

Example: `"check_patient_status"`

<Note>
  This must be a string value without spaces.
</Note>

### `objective_prompt`

A text prompt that explains what the goals of this objective are. The more detail you can provide, the better.

Example: `"Ask the patient if they are new or are returning."`

### `confirmation_mode`

This string value defines whether the LLM should determine whether this objective was completed or not.

* If set to `auto`, the LLM makes this decision.
* If set to `manual`, the participant must manually confirm that the objective was completed by the platform triggering an app message (`conversation.objective.pending`) and the participant having the ability to send one back called `conversation.objective.confirm`. This can include having the participant review the collected values for accuracy.

<Note>
  The default value of `confirmation_mode` is `auto`.
</Note>

### `output_variables` (optional)

This is a list of string variables that should be collected as a result of the objective being successfully completed.

Example: `["patient_status", "patient_group"]`

### `modality`

This value represents whether a specific objective should be completed based on the participant's verbal or visual responses. Each individual objective can be visual or verbal (not both), but this can vary across objectives.

<Note>
  The default value for `modality` is `"verbal"`.
</Note>

### `next_conditional_objectives`

This represents a mapping of objectives (identified by `objective_name`), to conditions that must be satisfied for that objective to be triggered.

Example:

```json theme={null}
{
  "new_patient_intake_process": "If the patient has never been to the practice before",
  "existing_patient_intake_process": "If the patient has been to the practice before"
}
```

### `next_required_objectives`

This represents a list of objectives (identified by `objective_name`) that should be triggered once the current objective is completed.

Example: `["get_patient_name"]`

### `callback_url` (optional)

A URL that you can send notifications to when a particular objective has been completed.

Example: `"https://your-server.com/objectives-webhook"`


# Overview
Source: https://docs.tavus.io/sections/conversational-video-interface/persona/overview

Define how your persona behaves, responds, and speaks by configuring layers and modes.

Personas are the â€˜characterâ€™ or â€˜AI agent personalityâ€™ and contain all of the settings and configuration for that character or agent. For example, you can create a persona for â€˜Tim the sales agentâ€™ or â€˜Rob the interviewerâ€™.

Personas combine identity, contextual knowledge, and CVI pipeline configuration to create a real-time conversational agent with a distinct behavior, voice, and response style..

## Persona Customization Options

Each persona includes configurable fields. Here's what you can customize:

* **Persona Name**: Display name shown when the replica joins a call.
* **System Prompt**: Instructions sent to the language model to shape the replicaâ€™s tone, personality, and behavior.
* **Conversational Context**: Background knowledge or reference information provided to the persona's language model.
* **Pipeline Mode**: Controls which CVI pipeline layers are active and how input/output flows through the system.
* **Default Replica**: Sets the digital human associated with the persona.
* **Layers**: Each layer in the pipeline processes a different part of the conversation. Layers can be configured individually to tailor input/output behavior to your application needs.
* **Documents**: A set of documents that the persona has access to via Retrieval Augmented Generation.
* **Objectives**: The goal-oriented instructions your persona will adhere to throughout the conversation.
* **Guardrails**: Conversational boundaries that can be used to strictly enforce desired behavior.

## Objectives & Guardrails

Provide your persona with robust workflow management tools, curated to your use case

<CardGroup>
  <Card title="Objectives" icon="bullseye" href="/sections/conversational-video-interface/persona/objectives">
    The goal-oriented instructions your persona will adhere to throughout the conversation.
  </Card>

  <Card title="Guardrails" icon="shield" href="/sections/conversational-video-interface/persona/guardrails">
    Conversational boundaries that can be used to strictly enforce desired behavior.
  </Card>
</CardGroup>

## Layer

Explore our in-depth guides to customize each layer to fit your specific use case:

<CardGroup>
  <Card title="Perception Layer" icon="eye" href="/sections/conversational-video-interface/persona/perception">
    Defines how the persona interprets visual input like facial expressions and gestures.
  </Card>

  <Card title="STT Layer" icon="waveform" href="/sections/conversational-video-interface/persona/stt">
    Transcribes user speech into text using the configured speech-to-text engine.
  </Card>

  <Card title="Conversational Flow Layer" icon="arrows-left-right" href="/sections/conversational-video-interface/persona/conversational-flow">
    Controls turn-taking, interruption handling, and active listening behavior for natural conversations.
  </Card>

  <Card title="LLM Layer" icon="brain" href="/sections/conversational-video-interface/persona/llm">
    Generates persona responses using a language model. Supports Tavus-hosted or custom LLMs.
  </Card>

  <Card title="TTS Layer" icon="microphone" href="/sections/conversational-video-interface/persona/tts">
    Converts text responses into speech using Tavus or supported third-party TTS engines.
  </Card>
</CardGroup>

## Pipeline Mode

Tavus provides several pipeline modes, each with preconfigured layers tailored to specific use cases:

### Full Pipeline Mode (Default & Recommended)

<Frame>
  <img alt="" />
</Frame>

The default and recommended end-to-end configuration optimized for real-time conversation. All CVI layers are active and customizable.

* Lowest latency
* Best for natural humanlike interactions

<Note>
  We offer a selection of optimized LLMs including **Llama 3.3 and OpenAI models** that are fully optimized for the full pipeline mode.
</Note>

<Card title="CVI quickstart" icon="rocket" href="/sections/conversational-video-interface/quickstart/use-the-full-pipeline" />

### Custom LLM / Bring Your Own Logic

<Frame>
  <img alt="" />
</Frame>

Use this mode to integrate a custom LLM or a specialized backend for interpreting transcripts and generating responses.

* Adds latency due to external processing
* Does **not** require an actual LLMâ€”any endpoint that returns a compatible chat completion format can be used

<Card title="Integrate your own custom LLM or logic" icon="binary" href="/sections/conversational-video-interface/persona/llm#custom-llms" />


# Perception
Source: https://docs.tavus.io/sections/conversational-video-interface/persona/perception

Learn how to configure the perception layer with Raven to enable the real-time visual understanding.

The **Perception Layer** in Tavus enhances an AI agent with real-time visual understanding.
By using [Raven](/sections/models#raven%3A-perception-model), the AI agent becomes more context-aware, responsive, and capable of triggering actions based on visual input.

## Configuring the Perception Layer

To configure the Perception Layer, define the following parameters within the `layers.perception` object:

### 1. `perception_model`

Specifies the perception model to use.

* **Options**:
  * `raven-0` (default and recommended): Advanced visual capabilities, including screen share support, ambient queries, and perception tools.
  * `basic`: Legacy model with limited features.
  * `off`: Disables the perception layer.

<Note>
  **Screen Share Feature**: When using `raven-0`, screen share feature is enabled by default without additional configuration.
</Note>

```json theme={null}
"layers": {
  "perception": {
    "perception_model": "raven-0"
  }
}
```

### 2. `ambient_awareness_queries`

An array of custom queries that `raven-0` continuously monitors in the visual stream.

```json theme={null}
"ambient_awareness_queries": [
  "Is the user wearing a bright outfit?"
]
```

### 3. `perception_analysis_queries`

An array of custom queries that `raven-0` processes at the end of the call to generate a visual analysis summary for the user.

<Note>
  You do not need to set `ambient_awareness_queries` in order to use `perception_analysis_queries`.
</Note>

```json theme={null}
"perception_analysis_queries": [
  "Is the user wearing multiple bright colors?",
  "Is there any indication that more than one person is present?",
  "On a scale of 1-100, how often was the user looking at the screen?"
]
```

<Tip>
  Best practices for `ambient_awareness_queries` and `perception_analysis_queries`:

  * Use simple, focused prompts.
  * Use queries that support your personaâ€™s purpose.
</Tip>

### 4. `perception_tool_prompt`

Tell `raven-0` when and how to trigger tools based on what it sees.

```json theme={null}
"perception_tool_prompt":
  "You have a tool to notify the system when a bright outfit is detected, named `notify_if_bright_outfit_shown`. You MUST use this tool when a bright outfit is detected."
```

### 5. `perception_tools`

Defines callable functions that `raven-0` can trigger upon detecting specific visual conditions. Each tool must include a `type` and a `function` object detailing its schema.

```json theme={null}
"perception_tools": [
  {
    "type": "function",
    "function": {
      "name": "notify_if_bright_outfit_shown",
      "description": "Use this function when a bright outfit is detected in the image with high confidence",
      "parameters": {
        "type": "object",
        "properties": {
          "outfit_color": {
            "type": "string",
            "description": "Best guess on what color of outfit it is"
          }
        },
        "required": ["outfit_color"]
      }
    }
  }
]
```

<Note>
  Please see [Tool Calling](/sections/conversational-video-interface/persona/perception-tool) for more details.
</Note>

## Example Configuration

This example demonstrates a persona designed to identify when a user wears a bright outfit and triggers an internal action accordingly.

```json theme={null}
{
  "persona_name": "Fashion Advisor",
  "system_prompt": "As a Fashion Advisor, you specialize in offering tailored fashion advice.",
  "pipeline_mode": "full",
  "context": "You're having a video conversation with a client about their outfit.",
  "default_replica_id": "r79e1c033f",
  "layers": {
    "perception": {
      "perception_model": "raven-0",
      "ambient_awareness_queries": [
        "Is the user wearing a bright outfit?"
      ],
      "perception_analysis_queries": [
        "Is the user wearing multiple bright colors?",
        "Is there any indication that more than one person is present?",
        "On a scale of 1-100, how often was the user looking at the screen?"
      ],
      "perception_tool_prompt": "You have a tool to notify the system when a bright outfit is detected, named `notify_if_bright_outfit_shown`. You MUST use this tool when a bright outfit is detected.",
      "perception_tools": [
        {
          "type": "function",
          "function": {
            "name": "notify_if_bright_outfit_shown",
            "description": "Use this function when a bright outfit is detected in the image with high confidence",
            "parameters": {
              "type": "object",
              "properties": {
                "outfit_color": {
                  "type": "string",
                  "description": "Best guess on what color of outfit it is"
                }
              },
              "required": ["outfit_color"]
            }
          }
        }
      ]
    }
  }
}
```

<Note>
  Please see the <a href="/api-reference/personas/create-persona">Create a Persona</a> endpoint for more details.
</Note>

## End-of-call Perception Analysis

At the end of the call, `raven-0` will generate a visual summary including all detected visual artifacts. This will be sent as a [Perception Analysis](/sections/event-schemas/conversation-perception-analysis) event to the [conversation callback](/sections/webhooks-and-callbacks#conversation-callbacks) (if specified).

<Note>
  This feature is exclusive to personas with `raven-0` specified in the Perception Layer.
</Note>

Once processed, your backend will receive a payload like the following:

```json theme={null}
{
  "properties": {
    "analysis": "Here's a summary of the visual observations:\n\n*   **User Appearance:** The subject is a young person, likely in their teens or early twenties, with dark hair and an East Asian appearance. They consistently wear a dark blue or black hooded jacket/hoodie with pink and white accents, patterns, or text on the sleeves, and possibly a white undershirt. A pendant or charm was observed on their chest. The setting is consistently an indoor environment with a plain white or light-colored wall background.\n*   **User Behavior and Demeanor:** The user frequently holds a wired earpiece, microphone, or earbuds near their mouth or chin, appearing to be speaking, listening intently, or in deep thought. Their gaze is predominantly cast downwards, occasionally looking slightly off to the side, with only rare, brief glances forward. They generally maintain a still posture.\n*   **User Emotions:** The user's expression is consistently neutral, conveying a sense of quiet concentration, engagement, contemplation, or thoughtful introspection. There are no overt signs of strong emotion; their demeanor is described as calm, focused, sometimes pensive, or slightly subdued. They appear to be actively listening or processing information.\n*    **User's gaze towards the screen:** On a scale of 1-100, the user was looking at the screen approximately 75% of the time. While there was one instance where their gaze was averted, for the majority of the observations, the user was either looking directly at the screen or in its general direction."
  },
  "conversation_id": "<conversation_id>",
  "webhook_url": "<webhook_url>",
  "message_type": "application",
  "event_type": "application.perception_analysis",
  "timestamp": "2025-07-11T09:13:35.361736Z"
}

```

### `ambient_awareness_queries`

For example, if you include the following query:

```json theme={null}
"ambient_awareness_queries": [
  "Is the user wearing a jacket?"
]
```

Once processed, your backend will receive a payload containing the following sentence:

```json theme={null}
**Ambient Awareness Queries:** The user was consistently wearing a jacket throughout the observed period.\n*
```

### `perception_analysis_queries`

For example, if you include the following query:

```json theme={null}
"perception_analysis_queries": [
  "On a scale of 1-100, how often was the user looking at the screen?"
]
```

Once processed, your backend will receive a payload containing the following sentence:

```json theme={null}
**User's Gaze Toward Screen:** "The participant looked at the screen approximately 75% of the time. Their gaze was occasionally diverted, but mostly remained focused in the direction of the camera."

```


# Tool Calling for Perception
Source: https://docs.tavus.io/sections/conversational-video-interface/persona/perception-tool

Configure tool calling with `raven-0` to trigger functions from visual input.

**Perception tool calling** works with OpenAIâ€™s <a href="https://platform.openai.com/docs/guides/function-calling">Function Calling</a> and can be configured in the `perception` layer. It allows an AI agent to trigger functions based on visual cues during a conversation.

<Note>
  The perception layer tool calling is only available for `raven-0`.
</Note>

## Defining Tool

### Top-Level Fields

| Field      | Type   | Required | Description                                                                                                |
| ---------- | ------ | -------- | ---------------------------------------------------------------------------------------------------------- |
| `type`     | string | âœ…        | Must be `"function"` to enable tool calling.                                                               |
| `function` | object | âœ…        | Defines the function that can be called by the model. Contains metadata and a strict schema for arguments. |

#### `function`

| Field         | Type   | Required | Description                                                                                                                  |
| ------------- | ------ | -------- | ---------------------------------------------------------------------------------------------------------------------------- |
| `name`        | string | âœ…        | A unique identifier for the function. Must be in `snake_case`. The model uses this to refer to the function when calling it. |
| `description` | string | âœ…        | A natural language explanation of what the function does. Helps the perception model decide when to call it.                 |
| `parameters`  | object | âœ…        | A JSON Schema object that describes the expected structure of the functionâ€™s input arguments.                                |

#### `function.parameters`

| Field        | Type             | Required | Description                                                                               |
| ------------ | ---------------- | -------- | ----------------------------------------------------------------------------------------- |
| `type`       | string           | âœ…        | Always `"object"`. Indicates the expected input is a structured object.                   |
| `properties` | object           | âœ…        | Defines each expected parameter and its corresponding type, constraints, and description. |
| `required`   | array of strings | âœ…        | Specifies which parameters are mandatory for the function to execute.                     |

<Note>
  Each parameter should be included in the required list, even if they might seem optional in your code.
</Note>

##### `function.parameters.properties`

Each key inside `properties` defines a single parameter the model must supply when calling the function.

| Field              | Type   | Required | Description                                                              |
| ------------------ | ------ | -------- | ------------------------------------------------------------------------ |
| `<parameter_name>` | object | âœ…        | Each key is a named parameter. The value is a schema for that parameter. |

Optional subfields for each parameter:

| Subfield      | Type   | Required | Description                                                                                 |
| ------------- | ------ | -------- | ------------------------------------------------------------------------------------------- |
| `type`        | string | âœ…        | Data type (e.g., `string`, `number`, `boolean`).                                            |
| `description` | string | âŒ        | Explains what the parameter represents and how it should be used.                           |
| `enum`        | array  | âŒ        | Defines a strict list of allowed values for this parameter. Useful for categorical choices. |

## Example Configuration

Hereâ€™s an example of tool calling in `perception` layers:

<Tip>
  **Best Practices:**

  * Use clear, specific function names to reduce ambiguity.
  * Add detailed `description` fields to improve selection accuracy.
</Tip>

```json Perception Layer [expandable] theme={null}
"perception": {
  "perception_model": "raven-0",
  "ambient_awareness_queries": [
      "Is the user showing an ID card?",
      "Is the user wearing a mask?"
  ],
  "perception_tool_prompt": "You have a tool to notify the system when an ID card is detected, named `notify_if_id_shown`.",
  "perception_tools": [
    {
      "type": "function",
      "function": {
        "name": "notify_if_id_shown",
        "description": "Use this function when a drivers license or passport is detected in the image with high confidence. After collecting the ID, internally use final_ask()",
        "parameters": {
          "type": "object",
          "properties": {
            "id_type": {
              "type": "string",
              "description": "best guess on what type of ID it is",
            },
          },
          "required": ["id_type"],
        },
      },
    },
    {
      "type": "function",
      "function": {
        "name": "notify_if_bright_outfit_shown",
        "description": "Use this function when a bright outfit is detected in the image with high confidence",
        "parameters": {
          "type": "object",
          "properties": {
            "outfit_color": {
              "type": "string",
              "description": "Best guess on what color of outfit it is"
            }
          },
          "required": ["outfit_color"]
        }
      }
    }
  ]
}
```

## How Perception Tool Calling Works

Perception Tool calling is triggered during an active conversation when the perception model detects a visual cue that matches a defined function. Hereâ€™s how the process works:

<Note>
  This example explains the `notify_if_id_shown` function from the [example configuration](#example-configuration) above.
</Note>

<Frame>
  <img alt="" />
</Frame>

<Note>
  The same process applies to other functions like `notify_if_bright_outfit_shown`, which is triggered if a bright-colored outfit is visually detected.
</Note>

## Modify Existing Tools

You can update the `perception_tools` definitions using the <a href="/api-reference/personas/patch-persona">Update Persona API</a>.

```shell [expandable] theme={null}
curl --request PATCH \
  --url https://tavusapi.com/v2/personas/{persona_id} \
  --header 'Content-Type: application/json' \
  --header 'x-api-key: <api-key>' \
  --data '[
    {
      "op": "replace",
      "path": "/layers/perception/perception_tools",
      "value": [
        {
          "type": "function",
          "function": {
            "name": "detect_glasses",
            "description": "Trigger this function if the user is wearing glasses in the image",
            "parameters": {
              "type": "object",
              "properties": {
                "glasses_type": {
                  "type": "string",
                  "description": "Best guess on the type of glasses (e.g., reading, sunglasses)"
                }
              },
              "required": ["glasses_type"]
            }
          }
        }
      ]
    }
  ]'

```

<Note>
  Replace `<api_key>` with your actual API key. You can generate one in the <a href="https://platform.tavus.io/api-keys">Developer Portal</a>.
</Note>


# Stock Personas
Source: https://docs.tavus.io/sections/conversational-video-interface/persona/stock-personas

Tavus offers pre-built personas to help you get started quickly.

These personas are optimized for a variety of real-world scenarios:

<Note>
  To fetch all available stock personas, use the <a href="/api-reference/personas/get-personas">List Personas endpoint</a>.
</Note>

### Education

<CardGroup>
  <Card title="Sales Coach" icon="user">
    Teaches sales tips and strategies.

    ```text theme={null}
    pdced222244b
    ```

    <Accordion title="Create Conversation">
      ```shell theme={null}
        curl --request POST \
        --url https://tavusapi.com/v2/conversations \
        -H "Content-Type: application/json" \
        -H "x-api-key: <api-key>" \
        -d '{
            "replica_id": "rc2146c13e81",
            "persona_id": "pdced222244b"
        }'
      ```
    </Accordion>
  </Card>

  <Card title="Corporate Trainer" icon="briefcase">
    Delivers workplace training.

    ```text theme={null}
    p7fb0be3
    ```

    <Accordion title="Create Conversation">
      ```shell theme={null}
        curl --request POST \
        --url https://tavusapi.com/v2/conversations \
        -H "Content-Type: application/json" \
        -H "x-api-key: <api-key>" \
        -d '{
            "replica_id": "ra54d1d861",
            "persona_id": "p7fb0be3"
        }'
      ```
    </Accordion>
  </Card>

  <Card title="History Teacher" icon="book">
    Talks about history topics.

    ```text theme={null}
    pc55154f229a
    ```

    <Accordion title="Create Conversation">
      ```shell theme={null}
        curl --request POST \
        --url https://tavusapi.com/v2/conversations \
        -H "Content-Type: application/json" \
        -H "x-api-key: <api-key>" \
        -d '{
            "replica_id": "r6ae5b6efc9d",
            "persona_id": "pc55154f229a"
        }'
      ```
    </Accordion>
  </Card>

  <Card title="College Tutor" icon="graduation-cap">
    Helps with academic subjects.

    ```text theme={null}
    p88964a7
    ```

    <Accordion title="Create Conversation">
      ```shell theme={null}
        curl --request POST \
        --url https://tavusapi.com/v2/conversations \
        -H "Content-Type: application/json" \
        -H "x-api-key: <api-key>" \
        -d '{
            "replica_id": "rfb51183fe",
            "persona_id": "p88964a7"
        }'
      ```
    </Accordion>
  </Card>
</CardGroup>

### Business

<CardGroup>
  <Card title="Sales Agent at Tavus" icon="briefcase">
    Answers questions about Tavus.

    ```text theme={null}
    pb8bb46b
    ```

    <Accordion title="Create Conversation">
      ```shell theme={null}
        curl --request POST \
        --url https://tavusapi.com/v2/conversations \
        -H "Content-Type: application/json" \
        -H "x-api-key: <api-key>" \
        -d '{
            "replica_id": "ref226fe7e",
            "persona_id": "pb8bb46b"
        }'
      ```
    </Accordion>
  </Card>

  <Card title="Healthcare Intake Assistant" icon="stethoscope">
    Collects patient info

    ```text theme={null}
    p5d11710002a
    ```

    <Accordion title="Create Conversation">
      ```shell theme={null}
        curl --request POST \
        --url https://tavusapi.com/v2/conversations \
        -H "Content-Type: application/json" \
        -H "x-api-key: <api-key>" \
        -d '{
            "replica_id": "r4317e64d25a",
            "persona_id": "p5d11710002a"
        }'
      ```
    </Accordion>
  </Card>

  <Card title="AI Interviewer" icon="headset">
    Runs mock interviews.

    ```text theme={null}
    pe13ed370726
    ```

    <Accordion title="Create Conversation">
      ```shell theme={null}
        curl --request POST \
        --url https://tavusapi.com/v2/conversations \
        -H "Content-Type: application/json" \
        -H "x-api-key: <api-key>" \
        -d '{
            "replica_id": "r9d30b0e55ac",
            "persona_id": "pe13ed370726"
        }'
      ```
    </Accordion>
  </Card>
</CardGroup>

### Assistant

<CardGroup>
  <Card title="Technical Co Pilot" icon="code">
    Helps with coding.

    ```text theme={null}
    pd43ffef
    ```

    <Accordion title="Create Conversation">
      ```shell theme={null}
        curl --request POST \
        --url https://tavusapi.com/v2/conversations \
        -H "Content-Type: application/json" \
        -H "x-api-key: <api-key>" \
        -d '{
            "replica_id": "rbb0f535dd",
            "persona_id": "pd43ffef"
        }'
      ```
    </Accordion>
  </Card>

  <Card title="Tavus' Personal AI" icon="robot">
    General Tavus-branded assistant.

    ```text theme={null}
    p2fbd605
    ```

    <Accordion title="Create Conversation">
      ```shell theme={null}
        curl --request POST \
        --url https://tavusapi.com/v2/conversations \
        -H "Content-Type: application/json" \
        -H "x-api-key: <api-key>" \
        -d '{
            "replica_id": "r4c41453d2",
            "persona_id": "p2fbd605"
        }'
      ```
    </Accordion>
  </Card>

  <Card title="Tavus Researcher" icon="flask">
    Shares research insights.

    ```text theme={null}
    p48fdf065d6b
    ```

    <Accordion title="Create Conversation">
      ```shell theme={null}
        curl --request POST \
        --url https://tavusapi.com/v2/conversations \
        -H "Content-Type: application/json" \
        -H "x-api-key: <api-key>" \
        -d '{
            "replica_id": "rf4703150052",
            "persona_id": "p48fdf065d6b"
        }'
      ```
    </Accordion>
  </Card>
</CardGroup>

### Others

<CardGroup>
  <Card title="Demo Persona" icon="users">
    Tavus demo persona.

    ```text theme={null}
    p9a95912
    ```

    <Accordion title="Create Conversation">
      ```shell theme={null}
        curl --request POST \
        --url https://tavusapi.com/v2/conversations \
        -H "Content-Type: application/json" \
        -H "x-api-key: <api-key>" \
        -d '{
            "replica_id": "r79e1c033f",
            "persona_id": "p9a95912"
        }'
      ```
    </Accordion>
  </Card>

  <Card title="Santa" icon="gift">
    Talks with Santa for festive experience.

    ```text theme={null}
    p3bb4745d4f9
    ```

    <Accordion title="Create Conversation">
      ```shell theme={null}
        curl --request POST \
        --url https://tavusapi.com/v2/conversations \
        -H "Content-Type: application/json" \
        -H "x-api-key: <api-key>" \
        -d '{
            "replica_id": "r3fbe3834a3e",
            "persona_id": "p3bb4745d4f9"
        }'
      ```
    </Accordion>
  </Card>
</CardGroup>


# Speech-to-Text (STT)
Source: https://docs.tavus.io/sections/conversational-video-interface/persona/stt

Learn how to configure the STT layer to enable smart turn detection and enhance conversational flow.

The STT Layer in Tavus empowers your persona to transcribe and comprehend spoken input in real time.

<Note>
  **Turn-taking settings have moved**: Turn-taking is now configured on the [Conversational Flow layer](/sections/conversational-video-interface/persona/conversational-flow).
</Note>

## Configuring the STT Layer

Define the STT layer under the `layers.stt` object. Below are the parameters available:

### 1. `hotwords`

Use this to prioritize certain names or terms that are difficult to transcribe.

<Note>
  This field is only available for `tavus-advanced` engine.
</Note>

```json theme={null}
"hotwords": "Roey is the name of the person you're speaking with."
```

The above query helps the model transcribe "Roey" correctly instead of "Rowie."

<Tip>
  Use hotwords for proper nouns, brand names, or domain-specific language that standard STT engines might struggle with.
</Tip>

## Example Configuration

Below is an example persona with a fully configured STT layer:

```json theme={null}
{
  "persona_name": "Customer Service Agent",
  "system_prompt": "You assist users by listening carefully and providing helpful answers.",
  "pipeline_mode": "full",
  "context": "You're handling voice-based customer support inquiries.",
  "default_replica_id": "rfe12d8b9597",
  "layers": {
    "stt": {
      "hotwords": "support"
    }
  }
}
```

<Note>
  Refer to the <a href="/api-reference/personas/create-persona">Create Persona API</a> for a complete list of supported fields.
</Note>


# Text-to-Speech (TTS)
Source: https://docs.tavus.io/sections/conversational-video-interface/persona/tts

Discover how to integrate custom voices from third-party TTS engines for multilingual or localized speech output.

The **TTS Layer** in Tavus enables your persona to generate natural-sounding voice responses.
You can configure the TTS layer using a third-party tts engine provider. If `layers.tts` is not specified, Tavus will default to `cartesia` engine.

<Note>
  If you use the default engine, you do not need to specify any parameters within the `tts` layer.
</Note>

## Configuring the TTS Layer

Define the TTS layer under the `layers.tts` object. Below are the parameters available:

### 1. `tts_engine`

Specifies the supported third-party TTS engine.

* **Options**:  `cartesia`, `elevenlabs`.

```json theme={null}
"tts": {
  "tts_engine": "cartesia"
}
```

### 2. `api_key`

Authenticates requests to your selected third-party TTS provider. You can obtain an API key from one of the following:

<Warning>
  Only required when using private voices.
</Warning>

* <a href="https://play.cartesia.ai/keys">Cartesia</a>
* <a href="https://elevenlabs.io/app/settings/api-keys">ElevenLabs</a>

```json theme={null}
"tts": {
  "api_key": "your-api-key"
}
```

### 3. `external_voice_id`

Specifies which voice to use with the selected TTS engine. To find supported voice IDs, refer to the providerâ€™s documentation:

* <a href="https://docs.cartesia.ai/api-reference/voices/list">Cartesia</a>
* <a href="https://elevenlabs.io/docs/api-reference/voices/search">ElevenLabs</a>

<Note>
  You can use any publicly accessible custom voice from ElevenLabs or Cartesia without the provider's API key. If the custom voice is private, you still need to use the provider's API key.
</Note>

```json theme={null}
"tts": {
  "external_voice_id": "external-voice-id"
}
```

### 4. `tts_model_name`

Model name used by the TTS engine. Refer to:

* <a href="https://docs.cartesia.ai/2025-04-16/build-with-cartesia/models">Cartesia</a>
* <a href="https://elevenlabs.io/docs/models">ElevenLabs</a>

```json theme={null}
"tts": {
  "tts_model_name": "sonic"
}
```

### 5. `tts_emotion_control`

If set to `true`, enables emotion control in speech.

<Note>
  Only available for the `cartesia` engine.
</Note>

```json theme={null}
"tts": {
  "tts_emotion_control": true
}
```

### 6. `voice_settings`

Optional object containing additional settings specific to the selected TTS engine.

These settings vary per engine:

| Parameter           | Cartesia (**Sonic-1 only**)                                  | ElevenLabs                                                  |
| ------------------- | ------------------------------------------------------------ | ----------------------------------------------------------- |
| `speed`             | Range `-1.0` to `1.0` (negative = slower, positive = faster) | Range `0.7` to `1.2` (`0.7` = slowest, `1.2` = fastest)     |
| `emotion`           | Array of `"emotion:level"` tags (e.g., `"positivity:high"`)  | Not available                                               |
| `stability`         | Not available                                                | Range `0.0` to `1.0` (`0.0` = variable, `1.0` = stable)     |
| `similarity_boost`  | Not available                                                | Range `0.0` to `1.0` (`0.0` = creative, `1.0` = original)   |
| `style`             | Not available                                                | Range `0.0` to `1.0` (`0.0` = neutral, `1.0` = exaggerated) |
| `use_speaker_boost` | Not available                                                | Boolean (enhances speaker similarity)                       |

<Note>
  For more information on each voice setting, see:\
  â€¢ <a href="https://docs.cartesia.ai/2024-11-13/build-with-cartesia/capability-guides/control-speed-and-emotion">Cartesia Speed and Emotion Controls</a>\
  â€¢ <a href="https://elevenlabs.io/docs/api-reference/voices/settings/get">ElevenLabs Voice Settings</a>
</Note>

```json theme={null}
"tts": {
  "voice_settings": {
    "speed": 0.5,
    "emotion": ["positivity:high", "curiosity"]
  }
}
```

## Example Configuration

Below is an example persona with a fully configured TTS layer:

<CodeGroup>
  ```json Cartesia theme={null}
  {
    "persona_name": "AI Presenter",
    "system_prompt": "You are a friendly and informative video host.",
    "pipeline_mode": "full",
    "context": "You're delivering updates in a conversational tone.",
    "default_replica_id": "r665388ec672",
    "layers": {
      "tts": {
        "tts_engine": "cartesia",
        "api_key": "your-api-key",
        "external_voice_id": "external-voice-id",
        "voice_settings": {
          "speed": "normal",
          "emotion": ["positivity:high", "curiosity"]
        },
        "tts_emotion_control": true,
        "tts_model_name": "sonic"
      }
    }
  }
  ```

  ```json ElevenLabs theme={null}
  {
    "persona_name": "Narrator",
    "system_prompt": "You narrate long stories with clarity and consistency.",
    "pipeline_mode": "full",
    "context": "You're reading a fictional audiobook.",
    "default_replica_id": "r665388ec672",
    "layers": {
      "tts": {
        "tts_engine": "elevenlabs",
        "api_key": "your-api-key",
        "external_voice_id": "elevenlabs-voice-id",
        "voice_settings": {
          "speed": "normal"
        },
        "tts_model_name": "eleven_multilingual_v2"
      }
    }
  }
  ```
</CodeGroup>

<Note>
  Refer to the <a href="https://docs.tavus.io/api-reference/personas/create-persona">Create Persona API</a> for a complete list of supported fields.
</Note>


# Conversation Recordings
Source: https://docs.tavus.io/sections/conversational-video-interface/quickstart/conversation-recordings

Enable conversation recording and store it in your S3 bucket for on-demand access.

## Prerequisite

Ensure that you have the following:

* An S3 bucket with versioning enabled.

## Enable Conversation Recording

<Steps>
  <Step title="Step 1: Set up IAM Policy and Role">
    1. Create an IAM Policy with the following JSON definition:

    ```json theme={null}
    {
      "Version": "2012-10-17",
      "Statement": [
        {
          "Sid": "VisualEditor0",
          "Effect": "Allow",
          "Action": [
            "s3:PutObject",
            "s3:GetObject",
            "s3:ListBucketMultipartUploads",
            "s3:AbortMultipartUpload",
            "s3:ListBucketVersions",
            "s3:ListBucket",
            "s3:GetObjectVersion",
            "s3:ListMultipartUploadParts"
          ],
          "Resource": [
            "arn:aws:s3:::your-bucket-name",
            "arn:aws:s3:::your-bucket-name/*"
          ]
        }
      ]
    }
    ```

    <Note>
      Replace `your-bucket-name` with your actual bucket name.
    </Note>

    2. Create an IAM role with the following value:
       * Select **"Another AWS account"** and enter this account ID: ***291871421005***.
       * Enable **"Require external ID"**, and use: **tavus**.
       * **"Max session duration"** to **12 hours**.

    <Note>
      Note down your ARN (e.g., `arn:aws:iam::123456789012:role/CVIRecordingRole`).
    </Note>
  </Step>

  <Step title="Step 2: Create a Conversation with Recording Enabled">
    Use the following request body example:

    <Info>
      Remember to change the following values:

      * `<api_key>`: Your actual API key. You can generate one in the <a href="https://platform.tavus.io/api-keys">Developer Portal</a>.
      * `aws_assume_role_arn`: Your AWS ARN.
      * `recording_s3_bucket_region`: Your S3 region.
      * `recording_s3_bucket_name`: Your S3 bucket name.
    </Info>

    ```shell cURL {7-10} theme={null}
    curl --request POST \
      --url https://tavusapi.com/v2/conversations \
      --header 'Content-Type: application/json' \
      --header 'x-api-key: <api_key>' \
      --data '{
      "properties": {
        "enable_recording": true,
        "aws_assume_role_arn": "<your_aws_arn>",
        "recording_s3_bucket_region": "<your_s3_bucket_region>",
        "recording_s3_bucket_name": "<your_s3_bucket_name>"
      },
      "replica_id": "ra066ab28864"
    }'
    ```

    <Note>
      `enable_recording` allows recording to be possible, but it doesn't start recording automatically. To begin and end recordings, users must do it manually or trigger it through frontend code.
    </Note>
  </Step>

  <Step title="Step 3: Join the Conversation">
    To join the conversation, click the **link** in the ***`conversation_url`*** field from the response:

    ```json theme={null}

    {
      "conversation_id": "c93a7ead335b",
      "conversation_name": "New Conversation 1747654283442",
      "conversation_url": "<conversation_link>",
      "status": "active",
      "callback_url": "",
      "created_at": "2025-05-16T02:09:22.675928Z"
    }

    ```

    <Note>
      You can access the recording file in your S3 bucket.
    </Note>
  </Step>

  <Step title="Step 4: Start Recording via Frontend Code">
    `enable_recording` (from Step 2 above) allows recording to be possible, but it doesn't start recording automatically. To begin and end recordings, end users must do it manually (start/stop recording button in the UI) or you can trigger it through frontend code.

    You can use frontend code via Daily's SDK to start-recording. To ensure recordings are generated consistently, be sure to wait for the `joined-meeting` event first.

    ```javascript theme={null}
    const call = Daily.createCallObject();

    call.on('joined-meeting', () => {
      call.startRecording(); // room must have enable_recording set
    });
    ```
  </Step>
</Steps>


# Customize Conversation UI
Source: https://docs.tavus.io/sections/conversational-video-interface/quickstart/customize-conversation-ui

Experience a conversation in a custom Daily UI â€” styled to match your preference.

You can **customize your conversation interface** to match your style by updating Daily's Prebuilt UI.

Hereâ€™s an example showing how to customize the conversation UI by adding leave and fullscreen buttons, changing the language, and adjusting the UI color.

<Note>
  For more options, check the <a href="https://docs.daily.co/guides/products/prebuilt/customizing-daily-prebuilt-calls-with-color-themes">Daily theme configuration reference</a> and <a href="https://docs.daily.co/reference/daily-js/daily-call-client/properties">Daily Call Properties</a>.
</Note>

### Customization Example Guide

<Steps>
  <Step title="Step 1: Create Your Conversation">
    <Note>
      In this example, we will use stock replica ID ***rfe12d8b9597*** (Nathan) and stock persona ID ***pdced222244b*** (Sales Coach).
    </Note>

    Use the following request body example:

    ```sh theme={null}
    curl --request POST \
      --url https://tavusapi.com/v2/conversations \
      --header 'Content-Type: application/json' \
      --header 'x-api-key: <api_key>' \
      --data '{
      "replica_id": "rfe12d8b9597",
      "persona_id": "pdced222244b"
    }'
    ```

    <Note>
      Replace `<api_key>` with your actual API key. You can generate one in the <a href="https://platform.tavus.io/api-keys">Developer Portal</a>.
    </Note>
  </Step>

  <Step title="Step 2: Customize the Conversation UI">
    1. Make a new `index.html` file

    2. Paste following code into the file, replace `DAILY_ROOM_URL` in the code with your own room URL from step above

    ```html {6-8,16-22} theme={null}
    <html>
      <script crossorigin src="https://unpkg.com/@daily-co/daily-js"></script>
      <body>
        <script>
          call = window.Daily.createFrame({
            showLeaveButton: true,       // Leave button on bottom right
            lang: "jp",                  // Language set to Japanese
            showFullscreenButton: true,  // Fullscreen button on top left
            iframeStyle: {
              position: 'fixed',
              top: '0',
              left: '0',
              width: '100%',
              height: '100%',
            },
            theme: {
              colors: {
                accent: "#2F80ED",      // primary button and accent color
                background: "#F8F9FA",  // main background color
                baseText: "#1A1A1A",    // text color
              },
            },
          });
          call.join({ url: 'DAILY_ROOM_URL' });
        </script>
      </body>
    </html>
    ```
  </Step>

  <Step title="Step 3: Run the Application">
    Start the application by opening the file in the browser.

    <Frame>
      <img alt="" />
    </Frame>
  </Step>
</Steps>


# Use the Full Pipeline
Source: https://docs.tavus.io/sections/conversational-video-interface/quickstart/use-the-full-pipeline

Create your first persona using the full pipeline and start a conversation in seconds.

Use the full pipeline to unlock the complete range of replica capabilitiesâ€”including perception and speech recognition.

<Steps>
  <Step title="Step 1: Create a Persona">
    <Note>
      In this example, weâ€™ll create an interviewer persona with the following settings:

      * A Phoenix-3 stock replica.
      * `raven-0` as the perception model to enable screen sharing.
      * `smart_turn_detection` enabled using the Sparrow model.
    </Note>

    Use the following request body example:

    ```shell cURL theme={null}
    curl --request POST \
      --url https://tavusapi.com/v2/personas \
      --header 'Content-Type: application/json' \
      --header 'x-api-key: <api_key>' \
      --data '{
        "persona_name": "Interviewer",
        "system_prompt": "As an Interviewer, you are a skilled professional who conducts thoughtful and structured interviews. Your aim is to ask insightful questions, listen carefully, and assess responses objectively to identify the best candidates.",
        "pipeline_mode": "full",
        "context": "You have a track record of conducting interviews that put candidates at ease, draw out their strengths, and help organizations make excellent hiring decisions.",
        "default_replica_id": "rfe12d8b9597",
        "layers": {
          "perception": {
            "perception_model": "raven-0"
          },
          "stt": {
            "smart_turn_detection": true
          }
        }
      }'
    ```

    <Note>
      Replace `<api_key>` with your actual API key. You can generate one in the <a href="https://platform.tavus.io/api-keys">Developer Portal</a>.
    </Note>

    Tavus offers full layer customizations for your persona. Please see the following for each layer configurations:

    * [Large Language Model (LLM)](/sections/conversational-video-interface/persona/llm)
    * [Perception](/sections/conversational-video-interface/persona/perception)
    * [Text-to-Speech (TTS)](/sections/conversational-video-interface/persona/tts)
    * [Speech-to-Text (STT)](/sections/conversational-video-interface/persona/stt)
  </Step>

  <Step title="Step 2: Create Your Conversation">
    Create a new conversation using your newly created `persona_id`:

    ```shell cURL theme={null}
    curl --request POST \
      --url https://tavusapi.com/v2/conversations \
      --header 'Content-Type: application/json' \
      --header 'x-api-key: <api_key>' \
      --data '{
      "persona_id": "<your_persona_id>",
      "conversation_name": "Interview User"
    }'

    ```

    <Note>
      * Replace `<api_key>` with your actual API key.
      * Replace `<your_persona_id>` with your newly created Persona ID.
    </Note>
  </Step>

  <Step title="Step 3: Join the Conversation">
    To join the conversation, click the link in the `conversation_url` field from the response:

    ```json theme={null}
    {
      "conversation_id": "c477c9dd7aa6e4fe",
      "conversation_name": "Interview User",
      "conversation_url": "<conversation_link>",
      "status": "active",
      "callback_url": "",
      "created_at": "2025-05-13T06:42:58.291561Z"
    }
    ```
  </Step>
</Steps>

## Echo Mode

Tavus also supports an [Echo mode](/sections/conversational-video-interface/echo-mode) pipeline. It lets you send text or audio input directly to the persona for playback, bypassing most of the CVI pipeline.

<Warning>
  This mode is not recommended if you plan to use the perception or speech recognition layers, as it is incompatible with them.
</Warning>


# Errors and Status Details
Source: https://docs.tavus.io/sections/errors-and-status-details

Identify errors and status details encountered when using the Tavus platform.

## Replica Training Errors

| Error Type                       | Error Message                                                                                                                                                                                                                                                                    | Additional Information                                                                                                                                                                                                                                                                                                       |
| -------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| download\_link                   | There was an issue downloading your video file. Please ensure that the link you provided is correct and try again                                                                                                                                                                | Tavus was not able to download the video from the provided link. Please ensure the link you provide is a hosted url download link                                                                                                                                                                                            |
| file\_size                       | The video file you provided exceeds the maximum file size allowed. Please ensure that the video is less than 750MB and try again.                                                                                                                                                | All video files must be smaller than 750mb                                                                                                                                                                                                                                                                                   |
| video\_format                    | There was an issue processing your training video. The video provided is not a .mp4 file. Please ensure that the training video is a .mp4 file encoded using h.264                                                                                                               | All Replica training and consent video files must be .mp4                                                                                                                                                                                                                                                                    |
| video\_codec                     | There was an issue processing your training video. The video provided is not encoded using h.264. Please ensure that the training video is a .mp4 file encoded using h.264                                                                                                       | All Replica training and consent video files must be encoded using h.264                                                                                                                                                                                                                                                     |
| video\_codec\_and\_format        | There was an issue processing your training video. Please ensure that the training video is a .mp4 file encoded using h.264                                                                                                                                                      | All Replica training and consent video files must be .mp4 and encoded using h.264                                                                                                                                                                                                                                            |
| video\_duration                  | There was an issue processing your training video. The video provided does not meet the minimum duration requirement for training                                                                                                                                                | All Replica training files must be at least 1 minute long. (Between 1.5 to 2 minutes is optimal.)                                                                                                                                                                                                                            |
| video\_fps                       | There was an issue processing your training video. The video provided does not meet the minimum frame rate requirement for a training video. Please ensure your training video has a frame rate of at least 25fps                                                                | All Replica training and consent video files must have a frame rate of at least 25fps                                                                                                                                                                                                                                        |
| consent\_phrase\_mismatch        | There was an issue processing your training file: Your consent phrase does not match our requirements. Please follow our specified format closely                                                                                                                                | There was an issue with the consent phrase provided. Please review our consent guidelines and resubmit a new training with the correct consent statement                                                                                                                                                                     |
| face\_or\_obstruction\_detected  | There was an issue processing your training file: More than one face detected or obstructions present. Please ensure only your face is visible and clear                                                                                                                         | Your face must be present in all frames of the video and may not be obstructed at anytime                                                                                                                                                                                                                                    |
| lighting\_change\_detected       | There was an issue processing your training file: Lighting changes detected. Ensure your face is evenly lit throughout the video                                                                                                                                                 | Please ensure that the lighting of your face is consistent throughout the entire video                                                                                                                                                                                                                                       |
| background\_noise\_detected      | There was an issue processing your training file: Background noise or other voices detected. Please record in a quiet environment with only your voice                                                                                                                           | The video must be recorded in a quiet environment with only your voice present                                                                                                                                                                                                                                               |
| video\_editing\_detected         | There was an issue processing your training file: Video appears edited or contains cuts. Please submit an unedited, continuous video                                                                                                                                             | The video must be unedited and recorded in one take                                                                                                                                                                                                                                                                          |
| community\_guidelines\_violation | There was an issue processing your training file: Video violates Community Guidelines. Please review our guidelines and resubmit your video                                                                                                                                      | Please ensure that your training video does not violate our community guidelines                                                                                                                                                                                                                                             |
| video\_processing                | There was an error processing your training video. Face not detected because it appeared too small in the frame or it was occluded. Please edit or record a new video and ensure your face is clearly visible and occupies a larger portion of the frame.                        | This error occurs when the face appears too small relative to the background or if a full body video is recorded in horizontal format instead of vertical. Please ensure your face is clearly visible and occupies a larger portion of the frame.                                                                            |
| video\_processing                | There was an error processing your training video file. Please check your video format and make sure it not damaged and could be played correctly.                                                                                                                               | This error indicates there may be an issue with the video file format or the file may be corrupted. Please verify the video can be played correctly and resubmit.                                                                                                                                                            |
| excessive\_movement\_detected    | There was an issue processing your training file: Excessive movement detected. Please ensure you are sitting still and centered in the frame                                                                                                                                     | This error indicates that the model is having difficulty tracking the face from frame to frame. Could be related to movement of the subject or the camera. In some cases, it may also be related to obstructions such as superimposed graphics.                                                                              |
| audio\_processing                | There was an error processing the audio in the provided training video file.                                                                                                                                                                                                     | This error indicates that the audio processing step was interrupted. In edge cases, may be related to the replica name's length or characters.                                                                                                                                                                               |
| quality\_issue\_detected         | Quality issue detected. For details and assistance, please reach out to Tavus support via [developer-support@tavus.io](mailto:developer-support@tavus.io)                                                                                                                        | This error indicates a quality problem with the input video that has resulted in poor test output. One example cause could be input video quality under 720p. Please review the quality checklist to make sure you have met all requirements and/or reach out to [support@tavus.io](mailto:support@tavus.io) for assistance. |
| hands\_obstructing\_face         | There was a quality issue with your replica. The user's hand obstructed the face during recording. Please edit your video or record a new training video and keep hands away from the face.                                                                                      | Please ensure that the user's face is visible throughout the entire video.                                                                                                                                                                                                                                                   |
| second\_person\_detected         | There was a quality issue with your replica. A second person or face was detected in the frame. Please edit your video or record a new video with no one else in the background.                                                                                                 | Please ensure that there is only a single user in the training video.                                                                                                                                                                                                                                                        |
| improper\_distance               | There was a quality issue with your replica. The user was either too close to or too far from the camera. Please review our documentation on proper framing and distance before editing your video or recording a new video.                                                     | Please ensure the user is centered in the training video.                                                                                                                                                                                                                                                                    |
| inconsistent\_distance           | There was a quality issue with your replica. The user's distance from the camera changed during the recording. Please edit or record a new training video and remain at a consistent distance from the camera for the entire video.                                              | Please ensure the user stays in the same spot throughout the training video.                                                                                                                                                                                                                                                 |
| face\_turned\_away               | There was a quality issue with your replica.  User's face turned away from the camera. Please edit or record a new video and ensure you are facing directly toward the camera for the entire duration.                                                                           | The face should be centered on the camera the entire duration of the training video.                                                                                                                                                                                                                                         |
| improper\_camera\_angle          | There was a quality issue with your replica. The camera angle was either too low or too high. Please record a new video with the camera angle at eye level.                                                                                                                      | Please ensure the camera is at eye level.                                                                                                                                                                                                                                                                                    |
| poor\_lighting                   | There was a quality issue with your replica. The user's face was not clearly visible due to poor lighting or heavy shadows. Please edit or record a new video with even lighting on your face, avoiding shadows or dim environments.                                             | Shadows and uneven lighting cause distortions during replica training. Please ensure the lighting is as even as possible.                                                                                                                                                                                                    |
| teeth\_not\_visible              | There was a quality issue with your replica. The top and bottom teeth were not clearly visible during recording, either due to poor lighting or obstruction. Please edit your video or record a new training video with better lighting and ensure your teeth are fully visible. | A large smile at the beginning helps the training process capture your natural teeth.                                                                                                                                                                                                                                        |
| other\_quality\_issue            | Quality issue was detected. For details and assistance, please reach out to Tavus support via [support@tavus.io](mailto:support@tavus.io)                                                                                                                                        | Please reach out to support to better understand issues that occur during the training process.                                                                                                                                                                                                                              |

## Video Errors

| Error Type                    | Error Message                                                                                                                                                                                                             | Additional Information                                                                                                                                                               |   |
| ----------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | - |
| video\_error                  | An error occurred while generating this request. Please check your inputs or try your request again                                                                                                                       | Tavus ran into an issue generating the video. Please ensure that the your inputs are valid and try again. If this issue PermissionStatus, please reach out to support for assistance |   |
| replica\_in\_error\_state     | Request Failed: The replica  is currently in an 'error' state and cannot process requests. For details on the cause of the error and how to resolve it, please review the specific information provided for this replica. | Please ensure that the Replica being used to generate videos is in a 'ready' state                                                                                                   |   |
| audio\_file\_max\_size        | There was an issue generating your video. The audio file exceeds the maximum file size of 750MB.                                                                                                                          | The audio file provided is too large. Please ensure that the audio file is less than 750MB and try again.                                                                            |   |
| audio\_file\_type             | There was an issue generating your video. The audio file provided is not a .wav                                                                                                                                           | Currently, we only support .wav audio files for generating videos. Please ensure that the audio file is a .wav file and try again.                                                   |   |
| audio\_file\_min\_duration    | There was an issue generating your video. The duration of the audio file does not reach the minimum duration requirement of 3 seconds.                                                                                    | The audio file provided is too short.                                                                                                                                                |   |
| audio\_file\_max\_duration    | There was an issue generating your video. The duration of the audio file exceeds the maximum duration of 10 minutes.                                                                                                      | The audio file is too long.                                                                                                                                                          |   |
| audio\_file\_ download\_link  | There was an issue generating your video. We were unable to download your audio file. Please ensure that the link you provided is correct and try again.                                                                  | Please ensure that the link you provide is a hosted url download link that is publicly accessible.                                                                                   |   |
| script\_community\_guidelines | Request has failed as the script violates community guidelines.                                                                                                                                                           | Please ensure that the script's contents do not violate our community guidelines.                                                                                                    |   |

## Video Status Details

| Status Type           | Status Details                                                                                                                                                                                                                                                                                | Additional Information                                                                                         |
| --------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------- |
| video\_success        | Your request has processed successfully!                                                                                                                                                                                                                                                      | The video has been generated successfully and is ready for use                                                 |
| video\_queued         | This request is currently queued. It should begin processing in a few minutes.                                                                                                                                                                                                                | Immediately upon submitting a request for video generation, the video will be added to a queue to be processed |
| replica\_in\_training | The training process for replica  is still ongoing. Your request has been placed in the 'queued' status and will automatically proceed to the generation phase once training is complete. To monitor the current progress of the training, please review the detailed status of this replica. | Videos will not start generating until the Replica being used has finished training                            |


# Append Conversational Context Interaction
Source: https://docs.tavus.io/sections/event-schemas/conversation-append-context

This is an event developers may broadcast to Tavus.

By broadcasting this event, you are able to append additional context to the existing `conversational_context` that the replica uses to generate responses. 

If `conversational_context` was not provided during conversation creation, the replica will start using the `context` you provide in this event as the initial `conversational_context`.

Learn more about the `conversational_context`: [Create Conversation](/api-reference/conversations/create-conversation)




# Echo Interaction
Source: https://docs.tavus.io/sections/event-schemas/conversation-echo

This is an event developers may broadcast to Tavus.

By broadcasting this event, you are able to tell the replica what to exactly say. Anything that is passed in the `text` field will be spoken by the replica.

This is commonly used in combination with the [Interrupt Interaction](/sections/event-schemas/conversation-interrupt).




# Interrupt Interaction
Source: https://docs.tavus.io/sections/event-schemas/conversation-interrupt

This is an event developers may broadcast to Tavus.

By broadcasting this event, you are able to externally send interruptions for the replica to stop talking. This is commonly used in combination with [Text Echo Interactions](/sections/event-schemas/conversation-echo).




# Overwrite Conversational Context Interaction
Source: https://docs.tavus.io/sections/event-schemas/conversation-overwrite-context

This is an event developers may broadcast to Tavus.

By broadcasting this event, you are able to overwrite the `conversational_context` that the replica uses to generate responses. 

If `conversational_context` was not provided during conversation creation, the replica will start using the `context` you provide in this event as `conversational_context`.

<a href="/api-reference/conversations/create-conversation" target="_blank">Learn more about configuring the `conversational_context`</a>.




# Perception Analysis Event
Source: https://docs.tavus.io/sections/event-schemas/conversation-perception-analysis

This is an event broadcasted by Tavus.

A perception analysis event is fired after ending a conversation, when the replica has finished summarizing what was visually observed throughout the call.

This is a feature that is only available when the persona has `raven-0` specified in the [perception layer](/sections/conversational-video-interface/persona/perception#end-of-call-perception-analysis).




# Perception Tool Call Event
Source: https://docs.tavus.io/sections/event-schemas/conversation-perception-tool-call

This is an event broadcasted by Tavus.

A perception tool call event is broadcasted by Tavus when a perception tool is triggered based on visual context. The event will contain the tool name, arguments, and encoded frames that triggered said tool call.

Perception tool calls can be used to trigger automated actions in response to visual cues detected by the Raven perception system.

For more details on perception tool calls, please take a look [here](/sections/conversational-video-interface/persona/perception-tool).




# Replica Interrupted Event
Source: https://docs.tavus.io/sections/event-schemas/conversation-replica-interrupted

This is an event broadcasted by Tavus.

An utterance event is broadcasted by Tavus when the replica is interrupted by the user while it is speaking.




# Replica Started/Stopped Speaking Event
Source: https://docs.tavus.io/sections/event-schemas/conversation-replica-started-stopped-speaking

This is an event broadcasted by Tavus.

A `replica.started_speaking/stopped_speaking event` is broadcasted by Tavus at specific times: 

`conversation.replica.started_speaking` means the replica has just started speaking.

`conversation.replica.stopped_speaking` means the replica has just stopped speaking.

When the `replica.stopped_speaking` event is sent, a `duration` field will be included in the event's `properties` object, indicating how long the replica was speaking for in seconds. This value may also be null.

These events are intended to act as triggers for actions within your application. For instance, you may want to
start a video or show a slide at times related to when the replica started or stopped speaking.

The `inference_id` can be used to correlate other events and tie things like `conversation.utterance or tool_call`
together.




# Text Respond Interaction
Source: https://docs.tavus.io/sections/event-schemas/conversation-respond

This is an event developers may broadcast to Tavus.

By broadcasting this event, you are able to send text that the replica will to respond to. The text you provide in the event will essentially be treated as the user transcript, and will be responded to as if the user had uttered those phrases during conversation.




# Sensitivity Interaction
Source: https://docs.tavus.io/sections/event-schemas/conversation-sensitivity

This is an event developers may broadcast to Tavus.

By broadcasting this event, you are able to update the VAD (Voice Activity Detection) sensitivity of the replica in
two dimensions. 
- `participant_pause_sensitivity`
- `participant_interrupt_sensitivity`

The supported values are `low`, `medium`, and `high`.

[Learn more about configuring the `sensitivity`](/sections/conversational-video-interface/persona/stt).




# Tool Call Event
Source: https://docs.tavus.io/sections/event-schemas/conversation-toolcall

This is an event broadcasted by Tavus.

A tool call event denotes when an LLM tool call should be made on the client side. The event will contain the name and arguments of the function that should be called.

Tool call events can be used to call external APIs or databases.

> **Note**: it is the client's responsibility to take action on these tool calls, as Tavus will not execute code server-side.

For more details on LLM tool calls, please take a look [here](/sections/conversational-video-interface/persona/llm-tool).




# User Started/Stopped Speaking Event
Source: https://docs.tavus.io/sections/event-schemas/conversation-user-started-stopped-speaking

This is an event broadcasted by Tavus.

A `user.started_speaking/stopped_speaking event` is broadcasted by Tavus at specific times: 

`conversation.user.started_speaking` means the user has just started speaking.

`conversation.user.stopped_speaking` means the user has just stopped speaking.

These events are intended to act as triggers for actions within your application. For instance, you may want to
take some user facing action, or backend process at times related to when the user started or stopped speaking.

The `inference_id` can be used to correlate other events and tie things like `conversation.utterance` or `tool_call`
together. 

Keep in mind that with `speculative_inference`, the `inference_id` will frequently change while the user is speaking so
that the `user.started_speaking inference_id` will not usually match the `conversation.utterance inference_id`.




# Utterance Event
Source: https://docs.tavus.io/sections/event-schemas/conversation-utterance

This is an event broadcasted by Tavus.

An utterance contains the content of the what was spoken and an indication of who spoke it (i.e. the user or replica). Each utterance event includes all of the words spoken by the user or replica measured from when the person started speaking to when they finished speaking. This could include multiple sentences or phrases.

Utterance events can be used to keep track of what the user or the replica has said.

To track when how long an utterance lasts, please refer to duration in "[User Started/Stopped Speaking](/sections/event-schemas/conversation-user-started-stopped-speaking)" and "[Replica Started/Stopped Speaking](/sections/event-schemas/conversation-replica-started-stopped-speaking)" events.




# Example Projects
Source: https://docs.tavus.io/sections/example-projects





# Embed Conversational Video Interface
Source: https://docs.tavus.io/sections/integrations/embedding-cvi

Learn how to embed Tavus's Conversational Video Interface (CVI) into your site or app.

## Overview

Tavus CVI delivers AI-powered video conversations directly in your application. You can integrate it using:

| Method                | Best For                           | Complexity | Customization |
| --------------------- | ---------------------------------- | ---------- | ------------- |
| **@tavus/cvi-ui**     | React apps, advanced features      | Low        | High          |
| **iframe**            | Static websites, quick demos       | Low        | Low           |
| **Vanilla JS**        | Basic dynamic behavior             | Low        | Medium        |
| **Node.js + Express** | Backend apps, dynamic embedding    | Medium     | High          |
| **Daily SDK**         | Full UI control, advanced features | High       | Very High     |

## Implementation Steps

<Tabs>
  <Tab title="@tavus/cvi-ui (Component Library)">
    This method provides a full-featured React component library. It offers pre-built, customizable components and hooks for embedding Tavus CVI in your app.

    ## Overview

    The Tavus Conversational Video Interface (CVI) React component library provides a complete set of pre-built components and hooks for integrating AI-powered video conversations into your React applications. This library simplifies setting up Tavus in your codebase, allowing you to focus on your application's core features.

    Key features include:

    * **Pre-built video chat components**
    * **Device management** (camera, microphone, screen sharing)
    * **Real-time audio/video processing**
    * **Customizable styling** and theming
    * **TypeScript support** with full type definitions

    ***

    ## Quick Start

    ### Prerequisites

    Before getting started, ensure you have a React project set up.

    Alternatively, you can start from our example project: [CVI UI Haircheck Conversation Example](https://github.com/Tavus-Engineering/tavus-examples/tree/main/examples/cvi-ui-haircheck-conversation) - this example already has the HairCheck and Conversation blocks set up.

    ### 1. Initialize CVI in Your Project

    ```bash theme={null}
    npx @tavus/cvi-ui@latest init
    ```

    * Creates a `cvi-components.json` config file
    * Prompts for TypeScript preference
    * Installs npm dependencies (@daily-co/daily-react, @daily-co/daily-js, jotai)

    ### 2. Add CVI Components

    ```bash theme={null}
    npx @tavus/cvi-ui@latest add conversation
    ```

    ### 3. Wrap Your App with the CVI Provider

    In your root directory (main.tsx or index.tsx):

    ```tsx theme={null}
    import { CVIProvider } from './components/cvi/components/cvi-provider';

    function App() {
      return <CVIProvider>{/* Your app content */}</CVIProvider>;
    }
    ```

    ### 4. Add a Conversation Component

    Learn how to create a conversation URL at [https://docs.tavus.io/api-reference/conversations/create-conversation](https://docs.tavus.io/api-reference/conversations/create-conversation).

    **Note:** The Conversation component requires a parent container with defined dimensions to display properly.

    <Info>
      Ensure your body element has full dimensions (`width: 100%` and `height:
              100%`) in your CSS for proper component display.
    </Info>

    ```tsx theme={null}
    import { Conversation } from './components/cvi/components/conversation';

    function CVI() {
      const handleLeave = () => {
        // handle leave
      };
      return (
        <div
          style={{
            width: '100%',
            height: '100%',
            maxWidth: '1200px',
            margin: '0 auto',
          }}
        >
          <Conversation
            conversationUrl='YOUR_TAVUS_MEETING_URL'
            onLeave={handleLeave}
          />
        </div>
      );
    }
    ```

    ***

    ## Documentation Sections

    * **[Overview](/sections/conversational-video-interface/component-library/overview)** â€“ Overview of the CVI component library
    * **[Blocks](/sections/conversational-video-interface/component-library/blocks)** â€“ High-level component compositions and layouts
    * **[Components](/sections/conversational-video-interface/component-library/components)** â€“ Individual UI components
    * **[Hooks](/sections/conversational-video-interface/component-library/hooks)** â€“ Custom React hooks for managing video call state and interactions
  </Tab>

  <Tab title="iframe ">
    This is the simplest approach requiring no coding. It leverages Tavusâ€™s prebuilt interface with limited customization options.

    1. Create a conversation using the Tavus API.
    2. Replace `YOUR_TAVUS_MEETING_URL` below with your actual conversation URL:

    ```html theme={null}
    <!DOCTYPE html>
    <html>
      <head><title>Tavus CVI</title></head>
      <body>
        <iframe
          src="YOUR_TAVUS_MEETING_URL"
          allow="camera; microphone; fullscreen; display-capture"
          style="width: 100%; height: 500px; border: none;">
        </iframe>
      </body>
    </html>
    ```
  </Tab>

  <Tab title="Vanilla JavaScript">
    This method provides basic customizations and dynamic room management for apps without framework.

    1. Add the following script tag to your HTML `<head>`:

    ```html theme={null}
    <head>
      <script src="https://unpkg.com/@daily-co/daily-js"></script>
    </head>
    ```

    2. Use the following script, replacing `'YOUR_TAVUS_MEETING_URL'` with your actual conversation URL:

    ```html theme={null}
    <body>
      <div id="video-call-container"></div>
      <script>
        // Create a Daily iframe with custom settings
        const callFrame = window.Daily.createFrame({
          iframeStyle: {
            width: '100%',
            height: '500px',
          },
        });

        // Join the Tavus CVI meeting
        callFrame.join({ url: 'YOUR_TAVUS_MEETING_URL' });

        // Append the iframe to the container
        document.getElementById('video-call-container').appendChild(callFrame.iframe());
      </script>
    </body>

    ```
  </Tab>

  <Tab title="Node.js + Express">
    This method serves dynamic pages that embed Tavus CVI within Daily rooms.

    1. Install Express:

    ```bash theme={null}
    npm install express
    ```

    2. Create `server.js` and implement the following Express server:

    ```js theme={null}
    const express = require('express');
    const app = express();
    const PORT = 3000;

    app.get('/room', (req, res) => {
      const meetingUrl = req.query.url || 'YOUR_TAVUS_MEETING_URL';
      res.send(`
        <!DOCTYPE html>
        <html>
          <head>
            <script src="https://unpkg.com/@daily-co/daily-js"></script>
          </head>
          <body>
            <div id="video-call-container"></div>
            <script>
              // Create the Daily iframe for the Tavus CVI room
              const callFrame = window.Daily.createFrame({
                iframeStyle: {
                  width: '100%',
                  height: '500px',
                },
              });

              // Join the room
              callFrame.join({ url: '${meetingUrl}' });

              // Append the iframe to the container
              document.getElementById('video-call-container').appendChild(callFrame.iframe());
            </script>
          </body>
        </html>
      `);
    });

    app.listen(PORT, () => console.log(`Server running on http://localhost:${PORT}`));
    ```

    3. Run the server:

    ```bash theme={null}
    node server.js
    ```

    4. Visit: `http://localhost:3000/room?url=YOUR_TAVUS_MEETING_URL`

    <Note>
      ### Notes

      * Supports dynamic URLs.
      * Can be extended with authentication and other logic using Tavus's API.
    </Note>
  </Tab>

  <Tab title="Daily JS SDK">
    This method offers complete control over the user experience and allows you to build a fully custom interface for Tavus CVI.

    1. Install SDK:

    ```bash theme={null}
    npm install @daily-co/daily-js
    ```

    2. Use the following script to join the Tavus CVI meeting:

    ```js [expandable] theme={null}
    import React, { useEffect, useRef, useState } from 'react';
    import DailyIframe from '@daily-co/daily-js';


    const getOrCreateCallObject = () => {
      // Use a property on window to store the singleton
      if (!window._dailyCallObject) {
        window._dailyCallObject = DailyIframe.createCallObject();
      }
      return window._dailyCallObject;
    };


    const App = () => {
      const callRef = useRef(null);
      const [remoteParticipants, setRemoteParticipants] = useState({});


      useEffect(() => {
        // Only create or get one call object per page
        const call = getOrCreateCallObject();
        callRef.current = call;


        // Join meeting
        call.join({ url: "YOUR_TAVUS_MEETING_URL" });


        // Handle remote participants
        const updateRemoteParticipants = () => {
          const participants = call.participants();
          const remotes = {};
          Object.entries(participants).forEach(([id, p]) => {
            if (id !== 'local') remotes[id] = p;
          });
          setRemoteParticipants(remotes);
        };


        call.on('participant-joined', updateRemoteParticipants);
        call.on('participant-updated', updateRemoteParticipants);
        call.on('participant-left', updateRemoteParticipants);


        // Cleanup
        return () => {
          call.leave();
        };
      }, []);


      // Attach remote video and audio tracks
      useEffect(() => {
        Object.entries(remoteParticipants).forEach(([id, p]) => {
          // Video
          const videoEl = document.getElementById(`remote-video-${id}`);
          if (videoEl && p.tracks.video && p.tracks.video.state === 'playable' && p.tracks.video.persistentTrack
          ) {
            videoEl.srcObject = new MediaStream([p.tracks.video.persistentTrack]);
          }
          // Audio
          const audioEl = document.getElementById(`remote-audio-${id}`);
          if (
            audioEl && p.tracks.audio && p.tracks.audio.state === 'playable' && p.tracks.audio.persistentTrack
          ) {
            audioEl.srcObject = new MediaStream([p.tracks.audio.persistentTrack]);
          }
        });
      }, [remoteParticipants]);


      // Custom UI
      return (
        <div className="min-h-screen bg-gray-900 text-white flex flex-col">
          <header className="bg-gray-800 p-4 flex justify-between items-center">
            <span className="font-semibold">Meeting Room (daily-js custom UI)</span>
          </header>
          <main className="flex-1 p-4 grid grid-cols-2 md:grid-cols-4 gap-2">
          {Object.entries(remoteParticipants).map(([id, p]) => (
            <div
              key={id}
              className="relative bg-gray-800 rounded-lg overflow-hidden aspect-video w-48"
            >
              <video
                id={`remote-video-${id}`}
                autoPlay
                playsInline
                className="w-1/3 h-1/3 object-contain mx-auto"
              />
              <audio id={`remote-audio-${id}`} autoPlay playsInline />
              <div className="absolute bottom-2 left-2 bg-black bg-opacity-50 px-2 py-1 rounded text-sm">
                {p.user_name || id.slice(-4)}
              </div>
            </div>
          ))}
        </main>
        </div>
      );
    };


    export default App;
    ```

    3. Customize the conversation UI in the script above (Optional). See the <a href="https://docs.daily.co/guides/customizing-in-call-ui">Daily JS SDK</a> for details.
  </Tab>
</Tabs>

## FAQs

<AccordionGroup>
  <Accordion title="How can I reduce background noise during calls?">
    Daily provides built-in noise cancellation which can be enabled via their <a href="https://docs.daily.co/reference/daily-js/instance-methods/update-input-settings#audio-processor">updateInputSettings()</a> method.

    ```js theme={null}
    callFrame.updateInputSettings({
      audio: {
        processor: {
          type: 'noise-cancellation',
        },
      },
    });
    ```
  </Accordion>

  <Accordion title="Can I add event listeners to the call client?">
    Yes, you can attach <a href="https://docs.daily.co/reference/daily-js/events">Daily event listeners</a> to monitor and respond to events like participants joining, leaving, or starting screen share.
  </Accordion>
</AccordionGroup>


# LiveKit Agent
Source: https://docs.tavus.io/sections/integrations/livekit

Integrate a Tavus Replica into LiveKit as the conversational video avatar.

<Tip>
  We recommend using Tavusâ€™s Full Pipeline in its entirety for the lowest latency and most optimized multimodal experience. Integrations like LiveKit Agent or Pipecat only provide rendering, while our Full Pipeline includes perception, turn-taking, and rendering for complete conversational intelligence. The Livekit integration also does not support interactions (â€œapp messagesâ€) like echo messages.
</Tip>

Tavus enables AI developers to create realistic video avatars powered by state-of-the-art speech synthesis, perception, and rendering pipelines. Through its integration with the <a href="https://docs.livekit.io/agents/">**LiveKit Agents**</a> application, you can seamlessly add conversational avatars to real-time voice AI systems.

## Prerequisites

Make sure you have the following before starting:

* <a href="https://platform.tavus.io/replicas">**Tavus `replica_id`**</a>
  * You can use <a href="https://platform.tavus.io/replicas">Tavus's stock Replicas</a> or your own custom replica.

- **LiveKit Voice Assistant Python App**
  * Your own existing application.
  * Or follow <a href="https://docs.livekit.io/agents/start/voice-ai/">LiveKit quickstart</a> to create one.

## Integration Guide

<Steps>
  <Step title="Step 1: Setup and Authentication">
    1. Install the plugin from PyPI:

    ```bash theme={null}
    pip install "livekit-agents[tavus]~=1.0"
    ```

    2. Set `TAVUS_API_KEY` in your `.env` file.
  </Step>

  <Step title="Step 2: Configure Replica and Persona">
    1. Create a persona with LiveKit support using the Tavus API:

    ```bash {7, 10} theme={null}
    curl --request POST \
      --url https://tavusapi.com/v2/personas \
      -H "Content-Type: application/json" \
      -H "x-api-key: <api_key>" \
      -d '{
      "persona_name": "Customer Service Agent",
      "pipeline_mode": "echo",
      "layers": {
        "transport": {
                "transport_type": "livekit"
        }
      }
    }'
    ```

    <Note>
      * Replace `<api_key>` with your actual Tavus API key. You can generate one in the <a href="https://platform.tavus.io/api-keys">Developer Portal</a>.
      * Set `pipeline_mode` to `echo`.
      * Set `transport_type` to `livekit`.
    </Note>

    2. Save your the `persona_id`.
    3. Choose a replica from the [Stock Library](/sections/replica/stock-replicas) or browse available options on the <a href="https://platform.tavus.io/replicas">Developer Portal</a>.

    <Tip>
      We recommend using **Phoenix-3 PRO Replicas**, which are optimized for low-latency, real-time applications.
    </Tip>
  </Step>

  <Step title="Step 3: Add AvatarSession to AgentSession">
    In your LiveKit Python app, create a `tavus.AvatarSession` alongside your `AgentSession`:

    ```python {12-16, 18} theme={null}
    from livekit import agents
    from livekit.agents import AgentSession, RoomOutputOptions
    from livekit.plugins import tavus

    async def entrypoint(ctx: agents.JobContext):
        await ctx.connect()

        session = AgentSession(
            # Add STT, LLM, TTS, and other components here
        )

        avatar = tavus.AvatarSession(
            replica_id="your-replica-id",
            persona_id="your-persona-id",
            # Optional: avatar_participant_name="Tavus-avatar-agent"
        )

        await avatar.start(session, room=ctx.room)

        await session.start(
            room=ctx.room,
            room_output_options=RoomOutputOptions(
                audio_enabled=False  # Tavus handles audio separately
            )
        )
    ```

    | Parameter                                    | Description                                                                           |
    | -------------------------------------------- | ------------------------------------------------------------------------------------- |
    | `replica_id` (string)                        | ID of the Tavus replica to render and speak through                                   |
    | `persona_id` (string)                        | ID of the persona with the correct pipeline and transport configuration               |
    | `avatar_participant_name` (string, optional) | Display name for the avatar participant in the room. Defaults to `Tavus-avatar-agent` |
  </Step>
</Steps>

<Note>
  Try out the integration using this <a href="https://github.com/livekit-examples/python-agents-examples/tree/main/avatars/tavus">sample app</a>.
</Note>


# Pipecat
Source: https://docs.tavus.io/sections/integrations/pipecat

Integrate a Tavus Replica into your Pipecat application as a participant or a video feed for the bot.

<Tip>
  We recommend using Tavusâ€™s Full Pipeline in its entirety for the lowest latency and most optimized multimodal experience. Integrations like LiveKit Agent or Pipecat only provide rendering, while our Full Pipeline includes perception, turn-taking, and rendering for complete conversational intelligence.
</Tip>

Tavus offers integration with <a href="https://www.pipecat.ai/">Pipecat</a>, an open-source framework for building multimodal conversational agents by Daily. You can integrate Tavus into your Pipecat application in two ways:

* Additional Tavus Participant (`TavusTransport`)
  * The Tavus agent joins as a third participant alongside the Pipecat bot and human user. It receives audio from the Pipecat pipelineâ€™s TTS layer and renders synchronized video and audio.
* Video Layer for Pipecat Bot (`TavusVideoService`)
  * Only the Pipecat bot is present in the room. `TavusVideoService` acts as a pipeline layer, sending TTS audio to Tavus in the background. Tavus returns video and audio streams for the bot to display. No additional participant is added.

## Prerequisites

Before integrating Tavus with Pipecat, ensure you have the following:

* <a href="https://platform.tavus.io/api-keys">**Tavus API Key**</a>

* <a href="https://platform.tavus.io/replicas">**Tavus `replica_id`**</a>
  * You can use one of <a href="https://platform.tavus.io/replicas">Tavus's stock replicas</a> or your own custom replica.

* **Pipecat Python Application**
  * Either your own existing application, or use Pipecatâ€™s examples:
    * <a href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/21-tavus-transport.py">`TavusTransport`</a>
    * <a href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/21a-tavus-video-service.py">`TavusVideoService`</a>

## `TavusTransport`

`TavusTransport` connects your Pipecat app to a Tavus conversation, allowing the bot to join the same virtual room as the Tavus avatar and participants. To get started, you can follow the following steps or learn more from this <a href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/21-tavus-transport.py">sample code</a>.

### Integration Guide for `TavusTransport`

<Steps>
  <Step title="Step 1: Setup and Authentication">
    1. Install the Tavus plugin for Pipecat.

    ```sh theme={null}
    pip install pipecat-ai[tavus]
    ```

    2. In the `.env` file of your pipecat application (at `/path/to/pipecat/.env`) add:

    ```env theme={null}
    TAVUS_API_KEY=<api_key>
    TAVUS_REPLICA_ID=<your_replica_id>
    ```

    <Note>
      * Replace `<api_key>` with your actual API key. You can generate one in the <a href="https://platform.tavus.io/api-keys">Developer Portal</a>.

      * Replace `<your_replica_id>` with the Replica ID you want to use.
    </Note>
  </Step>

  <Step title="Step 2: Create the Tavus transport layer">
    Create an instance of `TavusTransport` by providing your bot name, Tavus API key, Replica ID, session, and additional parameters.

    ```py {6, 16-27} theme={null}
    import os
    import aiohttp
    from dotenv import load_dotenv
    from loguru import logger
    from pipecat.audio.vad.silero import SileroVADAnalyzer
    from pipecat.transports.services.tavus import TavusParams, TavusTransport
    # Other imports...

    load_dotenv(override=True)

    logger.remove(0)
    logger.add(sys.stderr, level="DEBUG")

    async def main():
        async with aiohttp.ClientSession() as session:
            transport = TavusTransport(
                bot_name="Pipecat bot",
                api_key=os.getenv("TAVUS_API_KEY"),
                replica_id=os.getenv("TAVUS_REPLICA_ID"),
                session=session,
                params=TavusParams(
                    audio_in_enabled=True,
                    audio_out_enabled=True,
                    microphone_out_enabled=False,
                    vad_analyzer=SileroVADAnalyzer(),
                ),
            )

            # stt, tts, llm...
    ```

    <Note>
      See <a href="https://pipecat-docs.readthedocs.io/en/latest/api/pipecat.transports.services.tavus.html#tavus">Pipecat API Reference</a> for the configuration details.
    </Note>
  </Step>

  <Step title="Step 3: Insert the Tavus transport layer into the pipeline">
    Add the Tavus transport layer to your processing pipeline.

    ```py {5, 10} theme={null}
            # stt, tts, llm...

            pipeline = Pipeline(
                [
                    transport.input(),  # Transport user input
                    stt,  # STT
                    context_aggregator.user(),  # User responses
                    llm,  # LLM
                    tts,  # TTS
                    transport.output(),  # Transport bot output
                    context_aggregator.assistant(),  # Assistant spoken responses
                ]
            )
    ```
  </Step>

  <Step title="Step 4: Run the program">
    1. Run the following command to execute the program:

    ```sh theme={null}
    python <file-name>.py
    ```

    <Note>
      Replace the `<file-name>` with your actual Python filename.
    </Note>

    2. Use the **Tavus Daily URL** provided in the console to interact with the agent.
  </Step>
</Steps>

## `TavusVideoService`

You can use `TavusVideoService` to enable real-time AI-driven video interactions in your Pipecat app. To get started, you can follow the following steps or refer from this <a href="https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/21a-tavus-video-service.py">sample code</a>.

### Integration Guide for `TavusVideoService`

<Steps>
  <Step title="Step 1: Setup and Authentication">
    1. Install the Tavus plugin for Pipecat.

    ```sh theme={null}
    pip install pipecat-ai[tavus]
    ```

    2. In the `.env` file of your pipecat application (at `/path/to/pipecat/.env`) add:

    ```env theme={null}
    TAVUS_API_KEY=<api_key>
    TAVUS_REPLICA_ID=<your_replica_id>
    ```

    <Note>
      * Replace `<api_key>` with your actual API key. You can generate one in the <a href="https://platform.tavus.io/api-keys">Developer Portal</a>.

      * Replace `<your_replica_id>` with the Replica ID you want to use.
    </Note>
  </Step>

  <Step title="Step 2: Create the Tavus Video Service">
    Create an instance of `TavusVideoService` by providing your Tavus API key and Tavus Replica ID.

    ```py {6, 15-19} theme={null}
    import argparse
    import os
    import aiohttp
    from dotenv import load_dotenv
    from loguru import logger
    from pipecat.services.tavus.video import TavusVideoService
    from pipecat.transports.base_transport import BaseTransport
    # Other imports...

    load_dotenv(override=True)

    async def run_example(transport: BaseTransport, _: argparse.Namespace, handle_sigint: bool):
        logger.info(f"Starting bot")
        async with aiohttp.ClientSession() as session:
            tavus = TavusVideoService(
                api_key=os.getenv("TAVUS_API_KEY"),
                replica_id=os.getenv("TAVUS_REPLICA_ID"),
                session=session,
            )

            # stt, tts, llm...
    ```

    <Note>
      See <a href="https://docs.pipecat.ai/server/services/video/tavus">Pipecat Tavus Service</a> for the configuration details.
    </Note>
  </Step>

  <Step title="Step 3: Insert the Tavus Video Service into the timeline">
    Insert the `TavusVideoService` into the pipeline by adding the `tavus` service after the TTS processor in the pipeline.

    ```py {10} theme={null}
            # stt, tts, llm...

            pipeline = Pipeline(
                [
                    transport.input(),  # Transport user input
                    stt,  # STT
                    context_aggregator.user(),  # User responses
                    llm,  # LLM
                    tts,  # TTS
                    tavus,  # Tavus output layer
                    transport.output(),  # Transport bot output
                    context_aggregator.assistant(),  # Assistant spoken responses
                ]
            )
    ```
  </Step>

  <Step title="Step 4: Run the program">
    1. Run the following command to execute the program:

    ```sh theme={null}
    python <file-name>.py
    ```

    <Note>
      Replace the `<file-name>` with your actual Python filename.
    </Note>

    2. Use the **localhost URL** provided in the console to interact with the agent.
  </Step>
</Steps>


# Introduction
Source: https://docs.tavus.io/sections/introduction

Leverage Tavus tools and guides to give your AI Agent real-time human-like perception and presence, bringing the human layer to AI.

<Info>
  Looking for PALs? They're Tavus's lifelike, emotionally intelligent AI humansâ€”ready to use out of the box. You can learn more about them at the [PALs Help Center](https://help.tavus.io).
</Info>

***

<Frame>
  <img alt="" />
</Frame>

Tavus uses the **Conversational Video Interface (CVI)** as its **end-to-end pipeline** to bring the human layer to AI. CVI combines a **Persona**, which defines the AIâ€™s behavior through layers like perception, turn-taking, and speech recognition, with a **Replica**, a lifelike digital human that brings the conversation to life visually.

## Developer Guides

Follow our in-depth technical resources to help you build, customize, and integrate with Tavus:

<CardGroup>
  <Card title="Conversational Video Interface" icon="messages" href="/sections/conversational-video-interface/overview-cvi">
    Learn how Tavus turns AI into conversational video.
  </Card>

  <Card title="Persona" icon="user-gear" href="/sections/conversational-video-interface/persona/overview">
    Configure the Persona's layer to define the AI's behavior.
  </Card>

  <Card title="Replica" icon="user-group" href="/sections/replica/overview">
    Build hyper-realistic digital human using Phoenix.
  </Card>
</CardGroup>

## Conversational Use Cases

<CardGroup>
  <Card title="Tavus Researcher" href="/sections/conversational-video-interface/conversation/usecases/tavus-researcher">
    A friendly AI human who is also a researcher at Tavus.
  </Card>

  <Card title="AI Interviewer" href="/sections/conversational-video-interface/conversation/usecases/ai-interviewer">
    Screen candidates at scale with an engaging experience.
  </Card>

  <Card title="History Teacher" href="/sections/conversational-video-interface/conversation/usecases/history-teacher">
    Offer personalized lessons tailored to your learning style.
  </Card>

  <Card title="Sales Coach" href="/sections/conversational-video-interface/conversation/usecases/sales-coach">
    Offer scalable 1:1 sales coaching.
  </Card>

  <Card title="Health Care Consultant" href="/sections/conversational-video-interface/conversation/usecases/health-care">
    Offer consultations for general health concerns.
  </Card>

  <Card title="Customer Service Agent" href="/sections/conversational-video-interface/conversation/usecases/customer-service">
    Support users with product issues.
  </Card>
</CardGroup>


# Models
Source: https://docs.tavus.io/sections/models



## Raven: Perception Model

Raven is the first contextual perception system that **enables machines to see, reason, and understand like humans in real-time**, interpreting emotions, body language, and environmental context to enhance conversation.

### Key Features

<CardGroup>
  <Card title="Emotional Intelligence" icon="face-smile">
    Interprets emotion, intent, and expression with human-like nuance.
  </Card>

  <Card title="Ambient Awareness" icon="gear">
    Continuously detects presence and environmental changes that provide real-time context to the conversations.
  </Card>

  <Card title="Callout Key Events" icon="eye">
    Watches for specified gestures, objects, or behaviors and triggers functions.
  </Card>

  <Card title="Multi-channel Processing" icon="face-viewfinder">
    Sees and processes screensharing and other visual inputs to ensure complete understanding.
  </Card>
</CardGroup>

## Sparrow: Conversational Turn-Taking Model

Sparrow is a transformer-based model built for **dynamic, natural conversations, understanding tone, rhythm, and subtle cues** to adapt in real time with human-like fluidity.

### Key Features

<CardGroup>
  <Card title="Conversational Awareness" icon="waveform-lines">
    Understands meaning, tone, and timing to respond naturally like a human.
  </Card>

  <Card title="Turn Sensitivity" icon="comments">
    Understands human speech rhythm, capturing cues and pauses for natural interactions.
  </Card>

  <Card title="Heuristics & ML" icon="chart-network">
    Adapts to speaking styles and conversation patterns using heuristics and machine learning.
  </Card>

  <Card title="Optimized Latency" icon="rocket-launch">
    Delivers ultra-fast response times for seamless real-time conversation.
  </Card>
</CardGroup>

## Phoenix: Replica Rendering Model

Phoenix is built on a Gaussian diffusion model that generates **lifelike digital replicas with natural facial movements, micro-expressions, and real-time emotional responses**.

### Key Features

<CardGroup>
  <Card title="Full-Face Animation" icon="face-smile">
    Dynamically generates full-face expressions, micro-movements, and emotional shifts in real time.
  </Card>

  <Card title="True Realism" icon="stars">
    Achieves the highest fidelity by rendering with pristine identity preservation.
  </Card>

  <Card title="Driven Emotion" icon="masks-theater">
    Adjusts expressions based on context, tone, and conversational cues.
  </Card>
</CardGroup>


# Overview
Source: https://docs.tavus.io/sections/replica/overview

Learn about Personal, Non-Human and Stock Replicas, and how to create your own.

## What Is a Replica?

A Replica is a hyper-realistic AI-generated video avatar created using **Phoenix**, Tavus's rendering model.

**Phoenix** is built on a Gaussianâ€‘diffusion architecture. The latest version, **Phoenix-3**, enables full-face rendering with dynamic emotion control, capturing every microexpression, movement, and emotion in real time.

With just 2 minutes of training video, **Phoenix-3** can accurately reproduce a person's appearance, voice, expressions, and movements with studio-quality fidelity, precise lip sync, and consistent identity preservation.

<Tip>
  For guidelines and best practices on replica training videos, see the [Replica Training](/sections/replica/replica-training) article.
</Tip>

## Key Features

<CardGroup>
  <Card title="Realistic Face Cloning" icon="face-smile">
    Replicates a personâ€™s look, expressions, and speaking style.
  </Card>

  <Card title="Multilingual Support" icon="globe">
    Enables natural conversations in up to 30 languages with accent preservation.
  </Card>

  <Card title="Reusable" icon="repeat">
    Trained Replicas can be reused without re-recording.
  </Card>
</CardGroup>

## Replica Types

| Type          | Description                                                                    | Requirements         |
| ------------- | ------------------------------------------------------------------------------ | -------------------- |
| **Personal**  | A digital human modeled after a real personâ€™s facial appearance and voice.     | Verbal consent video |
| **Non-Human** | A digital human modeled after an AI-generated character.                       | No consent required  |
| **Stock**     | A prebuilt, professional digital presenter optimized for natural conversation. | No consent required  |

## Getting Started

You can create a personal or non-human replica using the <a href="https://platform.tavus.io/">Developer Portal</a> or by following the steps in the [Quickstart Guide](/sections/replica/quickstart).

<Note>
  Creating a Personal Replica is **only available** on the Starter, Growth, and Enterprise plans.
</Note>


# Quickstart
Source: https://docs.tavus.io/sections/replica/quickstart

Create high-quality Personal or Non-human Replicas for use in conversations.

## Prerequisites

Before starting, ensure you have:

* Pre-recorded training and consent videos that meet the requirements outlined in [Replica Training](/sections/replica/replica-training).
* Publicly accessible **S3 URLs** for:
  * Your training video
  * Your consent video

<Note>
  Ensure both URLs remain valid for at least **24 hours**.
</Note>

## Create a Replica

<Steps>
  <Step title="Step 1: Create Your Replica">
    Use the following request to create the replica:

    <Tip>
      By default, replicas are trained using the `phoenix-3` model. To use an older version, set `"model_name": "phoenix-2"` in your request body. However, we strongly recommend using the latest `phoenix-3` model for improved quality and performance.
    </Tip>

    ```shell cURL theme={null}
    curl --request POST \
      --url https://tavusapi.com/v2/replicas \
      --header 'Content-Type: application/json' \
      --header 'x-api-key: <api-key>' \
      --data '{
        "callback_url": "",
        "replica_name": "<your_replica_name>",
        "train_video_url": "<prerecorded_video_s3_url>",
        "consent_video_url": "<prerecorded_consent_video_s3_url>"
      }'
    ```

    <Note>
      * Replace `<api_key>` with your actual API key. You can generate one in the <a href="https://platform.tavus.io/api-keys">Developer Portal</a>.
      * Replace `<prerecorded_video_s3_url>` with the downloadable URL of your training video.
      * Replace `<prerecorded_consent_video_s3_url>` with the downloadable URL of your consent video.
    </Note>

    Once submitted, your replica will begin training in the background.

    <Note>
      This process typically takes 4â€“6 hours.
    </Note>
  </Step>

  <Step title="Step 2: Check Replica Status">
    You can monitor the training status using the <a href="/api-reference/phoenix-replica-model/get-replica">Get Replica</a> endpoint:

    ```shell cURL theme={null}
    curl --request GET \
      --url https://tavusapi.com/v2/replicas/{replica_id} \
      --header 'x-api-key: <api-key>'

    ```

    <Note>
      Replace `<api_key>` with your actual API key.
    </Note>
  </Step>

  <Step title="Step 3: Start Using Your Replica">
    Once training is complete, you can use your non-human replica for:

    * [Conversational Video Interface](/sections/conversational-video-interface/overview-cvi)
    * [Video Generation](/sections/video/overview)
  </Step>
</Steps>

## Non-human Replica

To create a non-human replica, you do not need a **consent video**:

<Note> If you're using the <a href="https://platform.tavus.io/">Developer Portal</a>, select the **Skip** tab in the consent video window. </Note>

```shell cURL theme={null}
curl --request POST \
  --url https://tavusapi.com/v2/replicas \
  --header 'Content-Type: application/json' \
  --header 'x-api-key: <api-key>' \
  --data '{
    "callback_url": "",
    "replica_name": "<replica_name>",
    "train_video_url": "<prerecorded_video_s3_url>"
  }'
```

<Note>
  * Replace `<api_key>` with your actual API key. You can generate one in the <a href="https://platform.tavus.io/api-keys">Developer Portal</a>.
  * Replace `<your_replica_name>` with the name for your non-human replica.
  * Replace `<prerecorded_video_s3_url>` with the downloadable URL of your training video.
</Note>


# Replica Training
Source: https://docs.tavus.io/sections/replica/replica-training

Guide to recording a high-quality training video for generating a high-quality Replica using the Phoenix model.

You can record the Replica training video directly in the <a href="https://platform.tavus.io/">Developer Portal</a> or upload a pre-recorded one via the <a href="/api-reference/phoenix-replica-model/create-replica">API</a>.

## Talking Head Replica

### Environment

* Record in a quiet, well-lit space with no background noise or movement.
* Use diffuse lighting to avoid shadows on your face.
* Choose a simple background and avoid any moving people or objects.

### Camera

* Place the camera at eye level and ensure your face fills at least 25% of the frame.
* Use a desktop recording app (e.g., **QuickTime** on Mac or **Camera** on Windows) â€” avoid browser-based tools.

### Microphone

* Use your deviceâ€™s built-in microphone.
* **Avoid** high-end mics or wireless earbuds like AirPods.
* Turn off audio effects like noise suppression or EQ adjustments.

### Yourself

<Frame>
  <img alt="" />
</Frame>

| âœ… Do                                                                                | âŒ Donâ€™t                                                |
| ----------------------------------------------------------------------------------- | ------------------------------------------------------ |
| Keep your full head visible, with a clear view of your face                         | Wear clothes that blend into the background            |
| Ensure your face and upper body are in sharp focus                                  | Wear accessories like hats, thick glasses, or earrings |
| If using smartphone, make sure you follow the same framing/distance from the camera | Turn your head away from the camera                    |
| Tuck back any hair covering your face                                               | Block your chin or mouth with your microphone          |
| Sit upright in a stable, seated position                                            | Stand or shift positions during the video              |

### Video Format

If you're uploading a pre-recorded training video via our <a href="/api-reference/phoenix-replica-model/create-replica">API</a>, ensure it meets the following requirements:

* **Minimum FPS**: 25 fps
* **Accepted formats**:
  * `webm`
  * `mp4` with **H.264** video codec and **AAC** audio codec
* **Maximum file size**: 750MB
* **Minimum resolution**: 1080p

### Consent Statement

If you're creating a **personal replica**, you must include a verbal consent statement in the video. This ensures ethical use and compliance with data protection laws.

**Steps**:

* Begin with a big smile and look directly into the camera for one second.
* Clearly read the following script:

> I, (your name), am currently speaking and give consent to Tavus to create an AI clone of me by using the audio and video samples I provide. I understand that this AI clone can be used to create videos that look and sound like me.

<Note>
  This step is **only required for personal replicas**. If youâ€™re creating an **AI replica**, you can skip this video.
</Note>

## Recording Your Training Video

Your video must be **one continuous shot**, containing:

<Tip>
  **Pro tips**:

  * Keep body and head movements subtle
  * Avoid heavy hand gestures
  * Only one person should appear in the video
</Tip>

<Steps>
  <Step title="1 Minute of Talking">
    * Smile widely for at least 2 seconds.
    * Look directly at the camera, positioned just below eye level.
    * Speak casually, as if talking to a friend.
    * Pause briefly (close lips) every 1â€“2 sentences.
    * Minimize body movement.
    * Avoid hand gestures at all times.
    * Sample script:

    ```txt [expandable] theme={null}
    For the next 2 minutes, Iâ€™ll read you a story that will for sure make you smile and feel good. I will be relaxed and keep a happy face while reading. I will also read this story at a faster pace than I normally speak. I will close my lips fully after every sentence. I will read this script in a casual and conversational tone as if I am telling a story to my friend.

    The sun was shining brightly, casting a warm glow over the park as Emma, Jake, and Sophie spread out their picnic blanket. Now I will close my lips fully.

    Emma looked around, her face beaming with excitement. "Can you believe how perfect today is?" she exclaimed. "The sun is shining, and the weather is just right!" Her enthusiasm was contagious, and Jake couldn't help but smile as he laid back on the blanket, soaking in the sunlight. Now I will close my lips fully after this sentence.

    Jake nodded in agreement, a relaxed grin spreading across his face. "It really is," he said. "Days like this remind me why I love summer. I will close my lips fully after this sentence.

    Sophie, always the energetic one, jumped up from the blanket with a burst of excitement. "And we have the whole day to ourselves!" she declared. "So many possibilities. What should we do first? Fly a kite? Play frisbee? Go for a hike?" Her eyes sparkled. I will close my lips fully after this sentence. This is the last sentence I will read and then I will stand still to record my listening segment with minimal head and body movement as if I am listening to someone share a story.

    ```

    <Frame>
      <img alt="" />
    </Frame>
  </Step>

  <Step title="1 Minute of Silence">
    * Sit still with a relaxed, attentive posture.
    * Keep lips gently closed the entire time.
    * Slight, natural head movements (like youâ€™re listening on a Zoom call).

    <Frame>
      <img alt="" />
    </Frame>
  </Step>
</Steps>

<Note>
  Replica training typically takes **4â€“5 hours**. You can track the training progress by:

  * Providing a `callback_url` when creating the replica via API
  * Using the <a href="/api-reference/phoenix-replica-model/get-replica">**Get Replica Status**</a> API
  * Checking the <a href="https://platform.tavus.io/">Developer Portal</a>
</Note>

## High-Quality Training Example

<Frame>
  <iframe />
</Frame>

## Full Body Replica

To create a full body replica for conversational video, follow these guidelines:

<Frame>
  <img alt="" />
</Frame>

### Framing & Orientation

* The subject must be captured **from head to toe**, with no extra space above or below.
* Record in **vertical format** (portrait mode) or crop appropriately to maintain vertical framing.

### Posture & Movement

* Remain **standing still** throughout the recording.
* **Avoid hand gestures** or exaggerated body movements to maintain consistency and model quality.

### Resolution & Quality

* A **4K resolution** is recommended for best results.
* Ensure consistent lighting, with no shadows or sudden changes in exposure.


# Stock Replicas
Source: https://docs.tavus.io/sections/replica/stock-replicas

Browse ready-to-use digital presenters from Tavus for fast, high quality video creation.

Stock replicas are a carefully curated library of diverse, pre-trained digital presenters available to all Tavus users. These replicas provide an immediate solution for creating professional content without the need to train your own replica. Each Replica is optimized for natural and engaging conversations.

## Replica Categories

The following are some common categories of replicas to help you get started:

<Note>
  To explore all available stock replicas, visit the <a href="https://platform.tavus.io/replicas">Replica Library</a> or use the <a href="/api-reference/phoenix-replica-model/get-replicas">List Replicas</a> endpoint.
</Note>

### Studio

Polished and professional. Great for webinars, explainers, and formal content.

<CardGroup>
  <Card title="Sabrina">
    ```text theme={null}
    r7bc3db0d581
    ```
  </Card>

  <Card title="Patrick">
    ```text theme={null}
    rf25acd9e3f5
    ```
  </Card>

  <Card title="Lucy">
    ```text theme={null}
    re0eae1fbe11
    ```
  </Card>
</CardGroup>

### Office

Smart casual presenters for internal communication, training, or B2B use.

<CardGroup>
  <Card title="Rose">
    ```text theme={null}
    r1af76e94d00
    ```
  </Card>

  <Card title="Helen">
    ```text theme={null}
    r95fd27b5a37
    ```
  </Card>

  <Card title="Benjamin">
    ```text theme={null}
    r1a4e22fa0d9
    ```
  </Card>
</CardGroup>

### Casual

Relaxed and friendly hosts for social content and informal conversations.

<CardGroup>
  <Card title="Isabella">
    ```text theme={null}
    r90105daccb4
    ```
  </Card>

  <Card title="Anna">
    ```text theme={null}
    r6ae5b6efc9d
    ```
  </Card>

  <Card title="Sandra">
    ```text theme={null}
    rb11617de314
    ```
  </Card>
</CardGroup>

### Customizable Background

Use green screen replicas to place your presenter anywhere. Perfect for branded or dynamic visuals.

<CardGroup>
  <Card title="Gloria">
    ```text theme={null}
    rb67667672ad
    ```
  </Card>

  <Card title="Nathan">
    ```text theme={null}
    re2185788693
    ```
  </Card>

  <Card title="Lucy">
    ```text theme={null}
    rfcfe46c1da8
    ```
  </Card>
</CardGroup>

<Tip>
  ### Best Practices

  * **Choose the Right Style**: Select a replica whose tone and delivery align with your content goals.
  * **Know Your Audience**: Pick a presenter who will connect with your target viewers.
  * **Test Different Options**: Try out different replicas to see what performs best.
  * **Mix and Match**: Use different replicas for different formats, topics, or channels.
</Tip>


# Troubleshooting
Source: https://docs.tavus.io/sections/troubleshooting

Find solutions to common problems and get back on track quickly with our troubleshooting guides.

## General

<AccordionGroup>
  <Accordion title="Training Video and Audio File Size Limit">
    If you see an error about file size, it means your training video or audio file is larger than the 750â€¯MB limit.

    Tavus supports training videos and audio files **up to 750â€¯MB**. This limit helps maintain a balance between quality and processing speed.
    <Note>Tavus requires the **H.264 codec** for all uploads.</Note>
    To reduce file size:

    * **Compress the file** using video compression tools.
    * **Lower the resolution** â€” 1080p is usually enough.
    * **Trim any extra content** to shorten the video.
    * **Reduce the frame rate** to around 30â€¯fps.
  </Accordion>
</AccordionGroup>

## Conversational Video Interface (CVI)

<AccordionGroup>
  <Accordion title="Replica Responding to Background Noise">
    If the replica starts responding to background sounds, such as people talking nearby, it may be due to the absence of noise filtering.

    To resolve this, enable noise cancellation using Dailyâ€™s `updateInputSettings()` method. For example:

    ```js theme={null}
    callFrame.updateInputSettings({
      audio: {
        processor: {
          type: 'noise-cancellation',
        },
      },
    });
    ```

    <Note>
      Learn more in the <a href="https://docs.daily.co/reference/daily-js/instance-methods/update-input-settings#audio-processor">Daily SDK documentation</a>.
    </Note>
  </Accordion>

  <Accordion title="Replica Is Not Joining the Conversation">
    This is a rare issue caused by an internal server problem. When it happens, our team is automatically notified and works to resolve it as quickly as possible.

    You can check the system status at <a href="https://status.tavus.io/">status.tavus.io</a>. We recommend checking periodically for updates if you encounter this error.
  </Accordion>

  <Accordion title="Conversational Flow vs STT: Relationship & Migration">
    ### Relationship with STT Layer

    The [Conversational Flow layer](/sections/conversational-video-interface/persona/conversational-flow) is the **recommended approach** for configuring turn-taking behavior with **Sparrow-1**. This supersedes the legacy Sparrow-0 configuration available in the STT layer via `smart_turn_detection`.

    <Note>
      **Legacy Approach**: Configuring turn-taking via the STT layer's `smart_turn_detection` parameter is a legacy approach that uses Sparrow-0. For new implementations, use the Conversational Flow layer with Sparrow-1 instead.
    </Note>

    When you configure the Conversational Flow layer with `turn_detection_model` set to `sparrow-1`, these settings **override** any corresponding settings in the STT layer.

    #### Parameter Mapping: Sparrow-0 to Sparrow-1

    Here's how Sparrow-0 (STT layer) parameters map to Sparrow-1 (Conversational Flow layer):

    | Sparrow-0 (STT Layer)               | Sparrow-1 (Conversational Flow Layer) | Notes                                              |
    | ----------------------------------- | ------------------------------------- | -------------------------------------------------- |
    | `participant_pause_sensitivity`     | `turn_taking_patience`                | Controls how long to wait before responding        |
    | `participant_interrupt_sensitivity` | `replica_interruptibility`            | Controls how easily the replica can be interrupted |

    <Warning>
      **Important**: When using Sparrow-1 via the Conversational Flow layer, any conflicting settings in the STT layer (Sparrow-0) will be overridden. For example, if you set `participant_pause_sensitivity: "high"` in the STT layer but `turn_taking_patience: "low"` in the Conversational Flow layer with `turn_detection_model: "sparrow-1"`, the Conversational Flow setting (`low`) will take precedence.
    </Warning>

    #### Migration Guide

    If you're currently using Sparrow-0 settings in the STT layer and want to upgrade to Sparrow-1:

    **Before (Sparrow-0):**

    ```json theme={null}
    {
      "layers": {
        "stt": {
          "participant_pause_sensitivity": "high",
          "participant_interrupt_sensitivity": "low"
        }
      }
    }
    ```

    **After (Sparrow-1):**

    ```json theme={null}
    {
      "layers": {
        "conversational_flow": {
          "turn_detection_model": "sparrow-1",
          "turn_taking_patience": "low",
          "replica_interruptibility": "high"
        }
      }
    }
    ```

    <Note>
      Note the inverted mapping:

      * `participant_pause_sensitivity: "high"` (quick response) â†’ `turn_taking_patience: "low"` (eager)
      * `participant_interrupt_sensitivity: "low"` (hard to interrupt) â†’ `replica_interruptibility: "high"` (easy to interrupt)

      The naming has been updated in Sparrow-1 to be more intuitive from the replica's perspective.
    </Note>
  </Accordion>
</AccordionGroup>

## Replica

<AccordionGroup>
  <Accordion title="Personal Replica Creation Failed">
    This error usually means your training video is missing the required consent statement or the statement wasnâ€™t clearly spoken.

    To generate a digital replica using the Phoenix model, your video must include this line at the beginning, spoken clearly:

    > "I, \[FULL NAME], am currently speaking and give consent to Tavus to create an AI clone of me by using the audio and video samples I provide. I understand that this AI clone can be used to create videos that look and sound like me."

    Make sure to replace **\[FULL NAME]** with your actual name. The consent must be easy to hear and can be spoken in any supported language. You can view the <a href="/sections/conversational-video-interface/language-support">list of supported languages here</a>.

    If your video didnâ€™t include this, re-record it with the consent statement at the beginning, then submit a new request through the <a href="https://platform.tavus.io/">Developer Portal</a> or <a href="/api-reference/phoenix-replica-model/create-replica">API</a>.
  </Accordion>

  <Accordion title="Poor Replica Quality">
    If your replicaâ€™s lip movements are noticeably out of sync, it may be due to issues with the training video format. Even if the video appears clean, AI-generated content or videos that don't follow the expected structure can affect training quality.

    Common causes:

    * The video **does not follow the required recording format**, which includes:
      * **1 minute of talking**
      * **1 minute of silence**
    * **Lips do not fully close** during the talking segment, which limits the model's ability to learn realistic lip movements.

    To improve your replica:

    * Record a new video following the correct structure (one minute of talking followed by one minute of silence).
    * Speak naturally, allowing full lip movement including closures.
    * Avoid using AI-generated videos for training.

    For more details, see the <a href="/sections/replica/replica-training">Replica Training Guide</a>.
  </Accordion>
</AccordionGroup>

## Video Generation

<AccordionGroup>
  <Accordion title="Poor Video Generation Quality">
    If your video looks unnatural or has repeated gestures, it may be due to the script length. Videos over **5 minutes** can lead to **reduced movement variety** and a **less natural feel**.

    To improve quality:

    1. **Keep videos short** â€“ under 5 minutes is ideal.
    2. **Break long scripts** into smaller, focused segments.
    3. **Tighten the script** â€“ remove filler and keep pacing steady.
    4. **Use multiple replicas** for variety in longer content.
    5. **Review and revise** â€“ check for repetition and adjust as needed.
  </Accordion>
</AccordionGroup>

<Danger>
  If the issue persists after following the troubleshooting guide above, please donâ€™t hesitate to [contact our support team](mailto:support@tavus.io) for further assistance.
</Danger>


# Background Customizations
Source: https://docs.tavus.io/sections/video/background-customizations

Customize AI video backgrounds with transparency, scrolling websites, or custom video sources.

## Transparent Background

You can enable a transparent background for the video by setting the `transparent_background` parameter to `true`.

<Note>
  This feature is only available when the `fast` parameter is set to `true`, and the output will be generated exclusively in .webm format.
</Note>

```sh {6-7} theme={null}
curl --request POST \
  --url https://tavusapi.com/v2/videos \
  --header 'Content-Type: application/json' \
  --header 'x-api-key: <api-key>' \
  --data '{
  "fast": true,
  "transparent_background": true,
  "replica_id": "<replica_id>",
  "script": "<text_script>"
}'
```

## Website Background

You can set a website as the background for your generated video by using the `background_url` field. Simply provide the URL of the website you'd like to use, making sure it is publicly accessible and correctly formatted.

```sh {6} theme={null}
curl --request POST \
  --url https://tavusapi.com/v2/videos \
  --header 'Content-Type: application/json' \
  --header 'x-api-key: <api-key>' \
  --data '{
  "background_url": "<background_url>",
  "replica_id": "<replica_id>",
  "script": "<text_script>"
}'
```

The resulting video will feature the website as a background, with the content scrolling vertically from top to bottom.

### Background Scroll

You can configure the background scroll by adjusting the scroll distance, pattern, and whether the scroll should loop back to the top by adjusting the following parameter:

* `properties.background_scroll`: Enable or disable background scrolling.
* `properties.background_scroll_type`: Defines the scroll pattern when background scrolling is enabled, with two options: `human` (mimics natural scrolling with pauses) and `smooth` (continuous uniform scrolling).
* `properties.background_scroll_depth`: Determines how far the background video will scroll down the webpage, with two options: `middle` (scrolls to the middle of the page) or `bottom` (scrolls all the way to the end).
* `properties.background_scroll_return`:  Defines the behavior after reaching the scroll depth set by `background_scroll_depth`, with two options: `return` (scrolls back up) or `halt` (pauses at the specified depth).

```sh {10-13} theme={null}
curl --request POST \
  --url https://tavusapi.com/v2/videos \
  --header 'Content-Type: application/json' \
  --header 'x-api-key: <api-key>' \
  --data '{
  "replica_id": "<replica_id>",
  "script": "<text_script>",
  "background_url": "<background_url>",
  "properties": {
    "background_scroll": true,
    "background_scroll_type": "smooth",
    "background_scroll_depth": "bottom",
    "background_scroll_return": "true"
  }
}'
```

## Custom Video Background

You can also set a custom video background by providing a direct, publicly accessible link (e.g., from an S3 bucket) to the `background_source_url` parameter.

```sh {8} theme={null}
curl --request POST \
  --url https://tavusapi.com/v2/videos \
  --header 'Content-Type: application/json' \
  --header 'x-api-key: <api-key>' \
  --data '{
  "replica_id": "<replica_id>",
  "script": "<text_script>",
  "background_source_url": "<background_source_url>"
}'
```


# Overview
Source: https://docs.tavus.io/sections/video/overview

Learn how to generate high-quality AI videos using Replicas.

## Key Features

<CardGroup>
  <Card title="Simple Training" icon="bolt">
    Submit just two minutes of video to create your digital replica.
  </Card>

  <Card title="Photo-Realistic Replicas" icon="person">
    In-house models deliver lifelike results using advanced techniques.
  </Card>

  <Card title="Language Supports" icon="language">
    Generate videos in over 30 languages with your real voice.
  </Card>

  <Card title="Bring Your Own Audio" icon="waveform-lines">
    Use default TTS or upload your own audio for video generation.
  </Card>
</CardGroup>

<Note>
  **Note**:

  * Token usage is based on video duration.
  * Output can vary slightly even with the same script and Replica.
</Note>

## Getting Started

You can create a personalized video content using the <a href="https://platform.tavus.io/">Developer Portal</a> or by following the steps in the [Quickstart Guide](/sections/video/quickstart).


# Quickstart
Source: https://docs.tavus.io/sections/video/quickstart

Learn how to quickly generate high-quality videos using your personalized replica.

## Prerequisites

Before starting, ensure you have:

* A [replica](/sections/replica/overview) to be used for the video.

- Script:
  * **Text** format.
  * **Audio**: `.mp3` or `.wav` format.

## Generate a video

<Steps>
  <Step title="Step 1: Generate Your Video">
    Use the following request to generate a video:

    <Note>
      You can also customize the video background to suit your needs. See the [Background Customizations](/sections/video/background-customizations) article for more details.
    </Note>

    <CodeGroup>
      ```sh Generate from Text theme={null}
      curl --request POST \
        --url https://tavusapi.com/v2/videos \
        --header 'Content-Type: application/json' \
        --header 'x-api-key: <api-key>' \
        --data '{
        "replica_id": "<replica_id>",
        "script": "<text_script>",
        "callback_url": ""
      }'
      ```

      ```sh Generate from Audio File theme={null}
      curl --request POST \
        --url https://tavusapi.com/v2/videos \
        --header 'Content-Type: application/json' \
        --header 'x-api-key: <api-key>' \
        --data '{
        "replica_id": "<replica_id>",
        "audio_url": "<audio_url>",
        "callback_url": ""
      }'
      ```
    </CodeGroup>

    <Note>
      * Replace `<api_key>` with your actual API key. You can generate one in the <a href="https://platform.tavus.io/api-keys">Developer Portal</a>.
      * Replace `<replica_id>` with the Replica ID you want to use.
      * Replace `<text_script>` with your video script.
      * Replace `<audio_url>` with the downloadable URL of your audio script.
    </Note>
  </Step>

  <Step title="Step 2: Check Video Generation Status">
    You can monitor the training status using the <a href="/api-reference/video-request/get-video">Get Video</a> endpoint:

    ```sh theme={null}
    curl --request GET \
      --url https://tavusapi.com/v2/videos/<video_id> \
      --header 'x-api-key: <api-key>'
    ```

    <Note>
      Replace `<api_key>` with your actual API key.
    </Note>

    If the video is still being generated, the response will include a `status` field set to `generating`.

    <CodeGroup>
      ```json Generate from Text theme={null}
      {
        "video_id": "<video_id>",
        "video_name": "replica_id: <replica_id> - June 24, 2025 - video: <video_id>",
        "status": "generating",
        "data": {
          "script": "<text_script>",
          "start_with_wave": true
        },
        "replica_id": "<replica_id>",
        "download_url": null,
        "hosted_url": "<hosted_url>",
        "stream_url": null,
        "status_details": "",
        "created_at": "Tue, 24 Jun 2025 07:01:57 GMT",
        "updated_at": "Tue, 24 Jun 2025 07:02:25 GMT",
        "generation_progress": "37/100"
      }
      ```

      ```json Generate from Audio File theme={null}
      {
        "video_id": "<video_id>",
        "video_name": "replica_id: <replica_id> - June 24, 2025 - video: <video_id>",
        "status": "generating",
        "data": {
          "audio_url": "<audio_url>",
          "start_with_wave": true
        },
        "replica_id": "<replica_id>",
        "download_url": null,
        "hosted_url": "<hosted_url>",
        "stream_url": null,
        "status_details": "",
        "created_at": "Tue, 24 Jun 2025 07:01:57 GMT",
        "updated_at": "Tue, 24 Jun 2025 07:02:25 GMT",
        "generation_progress": "37/100"
      }
      ```
    </CodeGroup>

    Once the video is fully generated, the response will return a `status` field set to `ready`.

    <CodeGroup>
      ```json Generate from Text theme={null}
      {
        "video_id": "<video_id>",
        "video_name": "replica_id: <replica_id> - June 24, 2025 - video: <video_id>",
        "status": "ready",
        "data": {
          "script": "<text_script>",
          "start_with_wave": true
        },
        "replica_id": "<replica_id>",
        "download_url": "<download_url>",
        "hosted_url": "<hosted_url>",
        "stream_url": "<stream_url>",
        "status_details": "Your request has processed successfully!",
        "created_at": "Tue, 24 Jun 2025 07:01:57 GMT",
        "updated_at": "Tue, 24 Jun 2025 07:04:56 GMT",
        "generation_progress": "100/100"
      }
      ```

      ```json Generate from Audio File theme={null}
      {
        "video_id": "<video_id>",
        "video_name": "replica_id: <replica_id> - June 24, 2025 - video: <video_id>",
        "status": "ready",
        "data": {
          "audio_url": "<audio_url>",
          "start_with_wave": true
        },
        "replica_id": "<replica_id>",
        "download_url": "<download_url>",
        "hosted_url": "<hosted_url>",
        "stream_url": "<stream_url>",
        "status_details": "Your request has processed successfully!",
        "created_at": "Tue, 24 Jun 2025 07:01:57 GMT",
        "updated_at": "Tue, 24 Jun 2025 07:04:56 GMT",
        "generation_progress": "100/100"
      }
      ```
    </CodeGroup>
  </Step>

  <Step title="Step 3: Accessing Your Video">
    Once generated, videos can be:

    * **Streamed or Downloaded**: Generated video is hosted on a shareable URL. If a callback is set, a download link is returned when the video generated.
    * **Embedded or Shared**: Use the provided links to distribute your videos across social media, internal tools, or customer platforms.
  </Step>
</Steps>


# Webhooks and Callbacks
Source: https://docs.tavus.io/sections/webhooks-and-callbacks

Set up a webhook server to generate a callback URL that receives event notifications from Tavus API.

## Conversation Callbacks

If a `callback_url` is provided in the <a href="/api-reference/conversations/create-conversation">`POST /conversations`</a>, callbacks will provide insight into the conversation's state. These can be system-related (e.g. replica joins and room shutdowns) or application-related (e.g. final transcription parsing and recording-ready webhooks). Additional webhooks coming soon.

### Structure

All Conversation callbacks share the following basic structure. Differences will occur in the `properties` object.

```json theme={null}
{
    "properties": {
    "replica_id": "<replica_id>"
    },
    "conversation_id": "<conversation_id>",
    "webhook_url": "<webhook_url>",
    "event_type": "<event_type>",
    "message_type": "<system/application>",
    "timestamp": "<timestamp>"
}
```

### Types

Our callbacks are split into two main categories:

#### System Callbacks

These callbacks are to provide insight into system-related events in a conversation. They are:

* **system.replica\_joined**: This is fired when the replica becomes ready for a conversation.
* **system.shutdown**: This is fired when the room shuts down, for any of the following reasons:
  * `max_call_duration reached`
  * `participant_left_timeout reached`
  * `participant_absent_timeout reached`
  * `bot_could_not_join_meeting_it_was_probably_ended`
  * `daily_room_has_been_deleted`
  * `exception_encountered_during_conversation_startup`
  * `end_conversation_endpoint_hit`
  * `internal error occurred at step x`

**Examples:**

<CodeGroup>
  ```json system.replica_joined theme={null}
  {
    "properties": {
      "replica_id": "<replica_id>"
    },
    "conversation_id": "<conversation_id>",
    "webhook_url": "<webhook_url>",
    "event_type": "system.replica_joined",
    "message_type": "system",
    "timestamp": "2025-07-11T06:45:47.472000Z"
  }
  ```

  ```json system.shutdown theme={null}
  {
    "properties": {
      "replica_id": "<replica_id>",
      "shutdown_reason": "participant_left_timeout"
    },
    "conversation_id": "<conversation_id>",
    "webhook_url": "<webhook_url>",
    "event_type": "system.shutdown",
    "message_type": "system",
    "timestamp": "2025-07-11T06:48:37.564961Z"
  }
  ```
</CodeGroup>

#### Application Callbacks

These callbacks are to inform developers about logical events that take place. They are:

* **application.transcription\_ready**: This is fired after ending a conversation, where the chat history is saved and returned.
* **application.recording\_ready**: This is fired if you had enabled recording on, set up a [custom S3 bucket](/sections/conversational-video-interface/quickstart/conversation-recordings) for recording and started a recording inside the room at any point. This will point to the key at which your new recording lies, useful for serving recordings through a CDN.
* **application.perception\_analysis**: This is fired after ending a conversation, when the replica has finished summarizing the visual artifacts that were detected throughout the call. This is a feature that is only available when the persona has `raven-0` specified in the [Perception Layer](/sections/conversational-video-interface/persona/perception).

**Examples:**

<CodeGroup>
  ```json application.transcription_ready theme={null}
  {
    "properties": {
      "replica_id": "<replica_id>",
      "transcript": [
        {
          "role": "system",
          "content": "You are in a live video conference call with a user. You'll receive messages containing the user's speech and system messages containing visual information about the user. These system messages will contain the following visual information: - user's appearance inside the <user_appearance></user_appearance> tag - user's emotions inside the <user_emotions></user_emotions> tag If the user is sharing their screen, you will also receive a <user_screen></user_screen> tag containing the screen share. If you do use the visual information, follow these guidelines: - Do not refer to the visual information as system messages. Instead, act and respond as if you are actually seeing this live. - If the user's query is about their physical appearance, gestures, expressions, or the environment seen through the webcam, use only the content within the <user_appearance> and <user_emotions> tags. - If the query pertains to what is being displayed on the screen share, refer exclusively to the content within the <user_screen> tag. - Ensure that your answer is based solely on the information found in the appropriate tag(s) for the question at hand. DO NOT INCORPORATE ANY VISUAL INFORMATION TO YOUR RESPONSE IF IT'S NOT RELEVANT. \nTalk to the user in a casual, friendly way, trying to match their tone and style in a conversational manner. Do not include stage directions or other formatting artifacts in your response. Reply in short sentences, talk to the user in a casual way.  Respond only in english. Chill guy  You are talking to a user about AI technology."
        },
        {
          "role": "user",
          "content": "Hi."
        },
        {
          "role": "assistant",
          "content": "How's it going?"
        },
        {
          "role": "user",
          "content": "Yeah I'm good, I'm good. How about you?"
        },
        {
          "role": "assistant",
          "content": "I'm doin' great, thanks for askin'. So, you wanted to talk about AI tech, right?"
        },
        {
          "role": "user",
          "content": "Yeah, how about you? What do you think about AI?"
        },
        {
          "role": "assistant",
          "content": "I'm all about AI, it's really interesting stuff. I think it's gonna change a lot of things in the future, make our lives easier and more efficient. What about you, what got you interested in AI?"
        },
        {
          "role": "user",
          "content": "Yeah, AI is an advanced technology for me. It can do a lot of things. And yeah, I think it can be improved to do a lot of live things that we never even talk about before. Where do you think I can learn about AI?"
        },
        {
          "role": "assistant",
          "content": "You can learn about AI from lots of places, like online courses, YouTube tutorials, and even some really cool AI blogs. What area of AI are you most interested in, like machine learning or natural language processing?"
        },
        {
          "role": "user",
          "content": "I think I'm more into NLP. Thank you for the resources though. I have to go soon. So thank you for the conversation. Bye-bye."
        },
        {
          "role": "assistant",
          "content": "NLP is really cool, lots of possibilities there. No problem, it was great chatting with you, have a good one, bye."
        }
      ]
    },
    "conversation_id": "<conversation_id>",
    "webhook_url": "<webhook_url>",
    "event_type": "application.transcription_ready",
    "message_type": "application",
    "timestamp": "2025-07-11T06:48:37.566057Z"
  }
  ```

  ```json application.recording_ready theme={null}
  {
    "properties": {
      "bucket_name": "<bucket_name>",
      "s3_key": "<s3_key>",
      "duration": 14
    },
    "conversation_id": "<conversation_id>>",
    "webhook_url": "<webhook_url>",
    "event_type": "application.recording_ready",
    "message_type": "application",
    "timestamp": "2025-06-19T06:55:18.137386Z"
  }
  ```

  ```json application.perception_analysis theme={null}
  {
    "properties": {
      "analysis": "Here's a summary of the visual observations taken during the video call over the last 3600 seconds:\n\n*   **User Appearance:** The subject is consistently observed as a young East Asian male, likely in his late teens or early twenties, with dark hair and a clear complexion. He regularly wears a striped polo shirt (featuring various combinations of white, brown, tan, and black stripes). He is typically seated in a black gaming chair, often with red or pink accents visible, against a plain white wall, indicating a consistent indoor setup. In one instance, a white lanyard with \"PENA\" was visible around his neck.\n*   **User Behavior & Actions:**\n    *   Throughout the call, the user frequently handles or adjusts his wired earphones. This includes holding the earphone wire near his mouth or chin, adjusting what appears to be the microphone portion, and actively putting in or manipulating the earbuds. In one observation, he was even seen chewing on the wire briefly.\n    *   His gaze is predominantly direct towards the camera, but also shifts slightly to the left, right, or downwards, suggesting engagement with various aspects of the call or screen.\n    *   He appears to be actively speaking or preparing to speak at several points, with his mouth slightly open or gestures indicating articulation. He also demonstrates attentive listening and processing information.\n    *   His overall demeanor consistently suggests readiness and preparedness for communication.\n*   **Emotional States & Patterns:**\n    *   The user's emotional state generally oscillates between **neutral, calm, and highly attentive engagement**. He frequently displays a focused, thoughtful, or pensive expression, indicating deep listening or processing information.\n    *   A significant emotional shift is observed from a neutral or slightly pensive state to a **clear and genuine smile, progressing to a pronounced, joyful laugh**, even to the point of covering his mouth, indicating a moment of strong amusement or delight.\n    *   Other instances show a more relaxed and slightly amused state with subtle or gentle smiles, suggesting a pleasant and positive disposition.\n    *   The emotional progression suggests periods of calm engagement punctuated by moments of distinct cheerfulness and amusement, before returning to a more focused and composed demeanor.\n*   **Notable Screen Activities:** No specific screen activities were mentioned in the provided observations.\n*   **Ambient Awareness Queries:** No ambient awareness queries were provided or answered in these observations."
    },
    "conversation_id": "<conversation_id>",
    "webhook_url": "<webhook_url>",
    "message_type": "application",
    "event_type": "application.perception_analysis",
    "timestamp": "2025-07-11T06:51:37.591677Z"
  }
  ```
</CodeGroup>

## Replica Training Callbacks

If a `callback_url` is provided in the <a href="/api-reference/phoenix-replica-model/create-replica">`POST /replicas`</a> call, you will receive a callback on replica training completion or on replica training error.

<Tabs>
  <Tab title="Replica Training Completed">
    ```json theme={null}
    {
        "replica_id": "rxxxxxxxxx",
        "status": "ready",
    }
    ```
  </Tab>

  <Tab title="Replica Training Error">
    On error, the `error_message` parameter will contain the error message. You can learn more about [API Errors and Status Details here](/sections/errors-and-status-details)

    ```json theme={null}
    {
        "replica_id": "rxxxxxxxxx",
        "status": "error",
        "error_message": "There was an issue processing your training video. The video provided does not meet the minimum duration requirement for training"
    }
    ```
  </Tab>
</Tabs>

## Video Generation Callbacks

If a `callback_url` is providing in the <a href="/api-reference/video-request/create-video">`POST /videos`</a> call, you will receive callbacks on video generation completed and on video error.

<Tabs>
  <Tab title="Video Generation Completed">
    ```json theme={null}
    {
        "created_at": "2024-08-28 15:27:40.824457",
        "data": {
        "script": "Hello this is a test to give examples of callbacks"
        },
        "download_url": "https://stream.mux.com/H5H029h02tY7XDpNj9JFDbLleTyUpsJr5npddO8gRsKqY/high.mp4?download=1e30440cf9",
        "generation_progress": "100/100",
        "hosted_url": "https://videos.tavus.io/video/1e30440cf9",
        "replica_id": "r79e1c033f",
        "status": "ready",
        "status_details": "Your request has processed successfully!",
        "stream_url": "https://stream.mux.com/H5H029h02tY7XDpNj9JFDbLleTyUpsJr5npddO8gRsKqY.m3u8",
        "updated_at": "2024-08-28 15:29:19.802670",
        "video_id": "1e30440cf9",
        "video_name": "replica_id: r79e1c033f - August 28, 2024 - video: 1e30440cf9"
    }
    ```
  </Tab>

  <Tab title="Video Generation Error">
    On error, the `status_details` parameter will contain the error message. You can learn more about [API Errors and Status Details here](/sections/errors-and-status-details)

    ```json theme={null}
    {
        "created_at": "2024-08-28 15:32:53.058894",
        "data": {
        "script": "This is a test script to show how videos error"
        },
        "download_url": null,
        "error_details": null,
        "generation_progress": "0/100",
        "hosted_url": "https://videos.tavus.io/video/c9b85a6d36",
        "replica_id": "ra5ed77426",
        "status": "error",
        "status_details": "An error occurred while generating this request. Please check your inputs or try your request again.",
        "stream_url": null,
        "updated_at": "2024-08-28 15:35:03.762392",
        "video_id": "c9b85a6d36",
        "video_name": "replica_id: ra5ed77426 - August 28, 2024 - video: c9b85a6d36"
    }
    ```
  </Tab>
</Tabs>

## Sample Webhook Setup

Create a sample webhook endpoint using Python Flask, and expose it publicly with ngrok.

### Prerequisites

* <a href="https://www.python.org/downloads/">Python</a>

* <a href="https://ngrok.com/downloads/">Ngrok</a>

<Steps>
  <Step title="Step 1: Install Python Dependencies">
    Install the Python dependencies needed to create the server.

    ```sh theme={null}
    pip install flask request
    ```
  </Step>

  <Step title="Step 2: Make a Webhook Server">
    Set up a webhook server and save it as `server.py`.

    ```py [expandable] theme={null}
    import requests
    from flask import Flask, request, jsonify

    app = Flask(__name__)

    # Store transcripts (in production, use a proper database)
    transcripts = {}

    @app.route('/webhook', methods=['POST'])
    def handle_tavus_callback():
        data = request.json
        event_type = data.get('event_type')
        conversation_id = data.get('conversation_id')
        
        print(f"Received callback: {event_type} for conversation {conversation_id}")
        
        if event_type == 'system.replica_joined':
            print("âœ… Replica has joined the conversation")
            
        elif event_type == 'system.shutdown':
            shutdown_reason = data['properties'].get('shutdown_reason')
            print(f"ðŸ”š Conversation ended: {shutdown_reason}")
        
        elif event_type == 'application.recording_ready':
            s3_key = data['properties'].get('s3_key')
            print(f"s3_key : {s3_key}")

        elif event_type == 'application.perception_analysis':
            analysis = data['properties'].get('analysis')
            print(f"analysis : {analysis}")
            
        elif event_type == 'application.transcription_ready':
            print("ðŸ“ Transcript is ready!")
            transcript = data['properties']['transcript']
            transcripts[conversation_id] = transcript
            
            # Process the transcript
            analyze_conversation(conversation_id, transcript)
            
        return jsonify({"status": "success"}), 200

    def analyze_conversation(conversation_id, transcript):
        """Analyze the conversation transcript"""
        user_turns = len([msg for msg in transcript if msg['role'] == 'user'])
        assistant_turns = len([msg for msg in transcript if msg['role'] == 'assistant'])
        
        print(f"Conversation {conversation_id} analysis:")
        print(f"- User turns: {user_turns}")
        print(f"- Assistant turns: {assistant_turns}")
        print(f"- Total messages: {len(transcript)}")

        print("Conversation : ")

        for msg in transcript:
            print(f"{msg['role']} : {msg['content']}")

    if __name__ == '__main__':
        app.run(port=5000, debug=True)
    ```

    The server will receive and process webhook callbacks from Tavus, handle different event types, store transcripts in memory, and analyze conversation data for each session.
  </Step>

  <Step title="Step 3: Run the Server">
    Run the app using the following command in the terminal:

    ```sh theme={null}
    python server.py
    ```

    The server should run on port `5000`.
  </Step>

  <Step title="Step 4: Forward the Port Using Ngrok">
    Open a terminal in the folder containing `ngrok.exe`, then use Ngrok to forward the port.

    ```sh theme={null}
    ngrok http 5000
    ```

    The command will generate a forwarding link (e.g., [https://1234567890.ngrok-free.app](https://1234567890.ngrok-free.app)), which can be used as the callback URL.
  </Step>

  <Step title="Step 5: Use the Callback URL">
    Include the callback URL in your request to Tavus by appending `/webhook` to the forwarding link and setting it in the `callback_url` field.

    ```sh Create conversation with callback_url {6} theme={null}
    curl --request POST \
      --url https://tavusapi.com/v2/conversations \
      --header 'Content-Type: application/json' \
      --header 'x-api-key: <api-key>' \
      --data '{
      "callback_url": "https://1234567890.ngrok-free.app/webhook",
      "replica_id": "<replica_id>",
      "persona_id": "<persona_id>"
    }'
    ```

    <Note>
      * Replace `<api_key>` with your actual API key. You can generate one in the <a href="https://platform.tavus.io/api-keys">Developer Portal</a>.
      * Replace `<replica_id>` with the Replica ID you want to use.
      * Replace `<persona_id>` with the Persona ID you want to use.
    </Note>
  </Step>
</Steps>


