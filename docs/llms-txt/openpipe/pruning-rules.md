# Source: https://docs.openpipe.ai/features/pruning-rules.md

> ## Documentation Index
> Fetch the complete documentation index at: https://docs.openpipe.ai/llms.txt
> Use this file to discover all available pages before exploring further.

# Pruning Rules

> Decrease input token counts by pruning out chunks of static text.

Some prompts have large chunks of unchanging text, like system messages which don't change from one request to the next. By removing this static text and fine-tuning a model on the compacted data, we can reduce the size of incoming requests and save you money on inference.

Add pruning rules to your dataset in the Settings tab, as shown below and in our [demo dataset](https://app.openpipe.ai/p/BRZFEx50Pf/datasets/0aa75f72-3fe5-4294-a94e-94c9236befa6/settings).

<Frame><img src="https://mintcdn.com/openpipe/ODS5wc6pSZpoOUK8/images/features/pruning-rules/dataset-pruning-rule.png?fit=max&auto=format&n=ODS5wc6pSZpoOUK8&q=85&s=85ffbf18f2d488c71ab6a093bf7c5fa0" alt="" data-og-width="2552" width="2552" data-og-height="1210" height="1210" data-path="images/features/pruning-rules/dataset-pruning-rule.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/openpipe/ODS5wc6pSZpoOUK8/images/features/pruning-rules/dataset-pruning-rule.png?w=280&fit=max&auto=format&n=ODS5wc6pSZpoOUK8&q=85&s=cc973b27442a6e540dff0a13de5dc597 280w, https://mintcdn.com/openpipe/ODS5wc6pSZpoOUK8/images/features/pruning-rules/dataset-pruning-rule.png?w=560&fit=max&auto=format&n=ODS5wc6pSZpoOUK8&q=85&s=774c9e7ad0e68cf34573f033b3d6a579 560w, https://mintcdn.com/openpipe/ODS5wc6pSZpoOUK8/images/features/pruning-rules/dataset-pruning-rule.png?w=840&fit=max&auto=format&n=ODS5wc6pSZpoOUK8&q=85&s=8e03709a7d35d9a82d0bbbe38f02b5a3 840w, https://mintcdn.com/openpipe/ODS5wc6pSZpoOUK8/images/features/pruning-rules/dataset-pruning-rule.png?w=1100&fit=max&auto=format&n=ODS5wc6pSZpoOUK8&q=85&s=efb0dc250561452a92061717a4eaf81e 1100w, https://mintcdn.com/openpipe/ODS5wc6pSZpoOUK8/images/features/pruning-rules/dataset-pruning-rule.png?w=1650&fit=max&auto=format&n=ODS5wc6pSZpoOUK8&q=85&s=02ac003bdb7ae2d0d3943f674d04580f 1650w, https://mintcdn.com/openpipe/ODS5wc6pSZpoOUK8/images/features/pruning-rules/dataset-pruning-rule.png?w=2500&fit=max&auto=format&n=ODS5wc6pSZpoOUK8&q=85&s=4b1cc06936aeeb52ebc4d29fe9da22d2 2500w" /></Frame>

To see the effect your pruning rules had on an individual training entry's input messages, open the Dataset Entry drawer:

<Frame><img src="https://mintcdn.com/openpipe/ODS5wc6pSZpoOUK8/images/features/pruning-rules/drawer-rule.png?fit=max&auto=format&n=ODS5wc6pSZpoOUK8&q=85&s=d8fb9f5fe59355355dcbefacdb702c3c" alt="" data-og-width="3000" width="3000" data-og-height="1714" height="1714" data-path="images/features/pruning-rules/drawer-rule.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/openpipe/ODS5wc6pSZpoOUK8/images/features/pruning-rules/drawer-rule.png?w=280&fit=max&auto=format&n=ODS5wc6pSZpoOUK8&q=85&s=1a8f464235ce16a64579a5de9f3e7b93 280w, https://mintcdn.com/openpipe/ODS5wc6pSZpoOUK8/images/features/pruning-rules/drawer-rule.png?w=560&fit=max&auto=format&n=ODS5wc6pSZpoOUK8&q=85&s=7aeda1e1f0567eb2c2b4efdd59b9c7d0 560w, https://mintcdn.com/openpipe/ODS5wc6pSZpoOUK8/images/features/pruning-rules/drawer-rule.png?w=840&fit=max&auto=format&n=ODS5wc6pSZpoOUK8&q=85&s=4e4e058a95b26f60b0ef723d3db0981d 840w, https://mintcdn.com/openpipe/ODS5wc6pSZpoOUK8/images/features/pruning-rules/drawer-rule.png?w=1100&fit=max&auto=format&n=ODS5wc6pSZpoOUK8&q=85&s=2f2f0b19d56b8dd729bbe2525f24de48 1100w, https://mintcdn.com/openpipe/ODS5wc6pSZpoOUK8/images/features/pruning-rules/drawer-rule.png?w=1650&fit=max&auto=format&n=ODS5wc6pSZpoOUK8&q=85&s=7794de65b6922398f10fb08e81309327 1650w, https://mintcdn.com/openpipe/ODS5wc6pSZpoOUK8/images/features/pruning-rules/drawer-rule.png?w=2500&fit=max&auto=format&n=ODS5wc6pSZpoOUK8&q=85&s=133e732a48ddfaa396f8c9fe3579b697 2500w" /></Frame>

By default, fine-tuned models inherit pruning rules applied to the dataset on which they were trained (see [demo model](https://app.openpipe.ai/p/BRZFEx50Pf/fine-tunes/5a2af605-03d3-412c-a7d3-611bdf6e1dcf/general)). These rules will automatically prune matching text from any incoming requests sent to that model. New pruning rules will not be associated with previously trained models, so you don't need to worry about backwards compatibility when adding new rules to your dataset. Before training a new model, you can choose to disable any inherited pruning rules.

<Frame><img src="https://mintcdn.com/openpipe/ODS5wc6pSZpoOUK8/images/features/pruning-rules/model-rules.png?fit=max&auto=format&n=ODS5wc6pSZpoOUK8&q=85&s=b2e316e83348c3408646fc3a5be3397b" alt="" data-og-width="2990" width="2990" data-og-height="1710" height="1710" data-path="images/features/pruning-rules/model-rules.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/openpipe/ODS5wc6pSZpoOUK8/images/features/pruning-rules/model-rules.png?w=280&fit=max&auto=format&n=ODS5wc6pSZpoOUK8&q=85&s=98887729671406c30f29d8fc473dadaa 280w, https://mintcdn.com/openpipe/ODS5wc6pSZpoOUK8/images/features/pruning-rules/model-rules.png?w=560&fit=max&auto=format&n=ODS5wc6pSZpoOUK8&q=85&s=3c0c5a9db559dfb7fbda0ec5012106a2 560w, https://mintcdn.com/openpipe/ODS5wc6pSZpoOUK8/images/features/pruning-rules/model-rules.png?w=840&fit=max&auto=format&n=ODS5wc6pSZpoOUK8&q=85&s=e39f4996716a75b06f29e9d3ce99f29e 840w, https://mintcdn.com/openpipe/ODS5wc6pSZpoOUK8/images/features/pruning-rules/model-rules.png?w=1100&fit=max&auto=format&n=ODS5wc6pSZpoOUK8&q=85&s=a692d86626a53852ef88a9cde41f48cf 1100w, https://mintcdn.com/openpipe/ODS5wc6pSZpoOUK8/images/features/pruning-rules/model-rules.png?w=1650&fit=max&auto=format&n=ODS5wc6pSZpoOUK8&q=85&s=7b76fa27fbf8dad5d1b5fee75aa9a503 1650w, https://mintcdn.com/openpipe/ODS5wc6pSZpoOUK8/images/features/pruning-rules/model-rules.png?w=2500&fit=max&auto=format&n=ODS5wc6pSZpoOUK8&q=85&s=8cebadaaf9ccc639f55c649940535097 2500w" /></Frame>

## Warning: can affect quality!

Weâ€™ve found that while pruning rules always decrease latency and costs, they can also negatively affect response quality, especially with smaller datasets. We recommend enabling pruning rules on datasets with 10K+ training examples, as smaller datasets may not provide enough guidance for the model to fully learn the task.
