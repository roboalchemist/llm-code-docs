---
description: Inicio R谩pido Para Model Assistant
title: Inicio R谩pido
keywords:
- sscma model assistant ai tinyml 
image: https://files.seeedstudio.com/wiki/wiki-platform/S-tempor.png
slug: /es/ModelAssistant_Introduce_Quick_Start
last_update:
  date: 01/11/2024
  author: LynnL4
---
# Inicio R谩pido

En [Descripci贸n General](/es/ModelAssistant_Introduce_Overview) hemos introducido las funciones y caracter铆sticas proporcionadas por [SSCMA](https://github.com/Seeed-Studio/ModelAssistant). Considerando que [SSCMA](https://github.com/Seeed-Studio/ModelAssistant) se divide en m煤ltiples m贸dulos diferentes, cada m贸dulo completando sus tareas correspondientes, sugerimos seguir los pasos a continuaci贸n para comenzar r谩pidamente.

:::tip
Sugerimos que todos los principiantes de [SSCMA](https://github.com/Seeed-Studio/ModelAssistant) comiencen a aprender desde [Primeros Pasos](#primeros-pasos), si est谩s familiarizado con [SSCMA](https://github.com/Seeed-Studio/ModelAssistant) o [OpenMMLab](https://github.com/open-mmlab), y quieres intentar desplegar en dispositivos de computaci贸n en el borde, modificar redes neuronales existentes, o entrenar en conjuntos de datos definidos por el usuario, puedes referirte directamente a [Avanzado](#avanzado).
:::

## Primeros Pasos

### Despliegue de Modelos

Si quieres desplegar el modelo en el dispositivo, por favor consulta la secci贸n [Desplegar](/es/ModelAssistant_Deploy_Overview) para aprender c贸mo desplegar el modelo.

### Entrenamiento de Modelos

Si quieres entrenar un modelo, recomendamos encarecidamente que primero intentes entrenar un modelo en la plataforma Colab. Puedes consultar los siguientes tutoriales:

#### Detecci贸n de Objetos

| Modelo                                                                                           | Colab                                                                                                                                                                                                                     |
|:------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [Gender_Detection_Swift-YOLO_192](https://github.com/seeed-studio/sscma-model-zoo/blob/main/docs/en/Gender_Detection_Swift-YOLO_192.md)                   | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seeed-studio/sscma-model-zoo/blob/main/notebooks/en/Gender_Detection_Swift-YOLO_192.ipynb)          |
| [Digital_Meter_Water_Swift-YOLO_192](https://github.com/seeed-studio/sscma-model-zoo/blob/main/docs/en/Digital_Meter_Water_Swift-YOLO_192.md)             | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seeed-studio/sscma-model-zoo/blob/main/notebooks/en/Digital_Meter_Water_Swift-YOLO_192.ipynb)       |
| [Apple_Detection_Swift-YOLO_192](https://github.com/seeed-studio/sscma-model-zoo/blob/main/docs/en/Apple_Detection_Swift-YOLO_192.md)                     | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seeed-studio/sscma-model-zoo/blob/main/notebooks/en/Apple_Detection_Swift-YOLO_192.ipynb)           |
| [person_Detection_Swift-YOLO_192](https://github.com/seeed-studio/sscma-model-zoo/blob/main/docs/en/person_Detection_Swift-YOLO_192.md)                   | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seeed-studio/sscma-model-zoo/blob/main/notebooks/en/person_Detection_Swift-YOLO_192.ipynb)          |
| [Face_Detection_Swift-YOLO_96](https://github.com/seeed-studio/sscma-model-zoo/blob/main/docs/en/Face_Detection_Swift-YOLO_96.md)                         | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seeed-studio/sscma-model-zoo/blob/main/notebooks/en/Face_Detection_Swift-YOLO_96.ipynb)             |
| [COCO_Detection_Swift-YOLO_320](https://github.com/seeed-studio/sscma-model-zoo/blob/main/docs/en/COCO_Detection_Swift-YOLO_320.md)                       | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seeed-studio/sscma-model-zoo/blob/main/notebooks/en/COCO_Detection_Swift-YOLO_320.ipynb)            |
| [Gesture_Detection_Swift-YOLO_192](https://github.com/seeed-studio/sscma-model-zoo/blob/main/docs/en/Gesture_Detection_Swift-YOLO_192.md)                 | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seeed-studio/sscma-model-zoo/blob/main/notebooks/en/Gesture_Detection_Swift-YOLO_192.ipynb)         |
| [Digital_Meter_Electricity_Swift-YOLO_192](https://github.com/seeed-studio/sscma-model-zoo/blob/main/docs/en/Digital_Meter_Electricity_Swift-YOLO_192.md) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seeed-studio/sscma-model-zoo/blob/main/notebooks/en/Digital_Meter_Electricity_Swift-YOLO_192.ipynb) |

#### Clasificaci贸n de Im谩genes

| Modelo                                                                                                         | Colab                                                                                                                                                                                                                            |
|:--------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [MNIST_Classification_MobileNetV2_0.5_Rep_32](https://github.com/seeed-studio/sscma-model-zoo/blob/main/docs/en/MNIST_Classification_MobileNetV2_0.5_Rep_32.md)         | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seeed-studio/sscma-model-zoo/blob/main/notebooks/en/MNIST_Classification_MobileNetV2_0.5_Rep_32.ipynb)     |
| [Gender_Classification_MobileNetV2_0.35_Rep_64](https://github.com/seeed-studio/sscma-model-zoo/blob/main/docs/en/Gender_Classification_MobileNetV2_0.35_Rep_64.md)     | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seeed-studio/sscma-model-zoo/blob/main/notebooks/en/Gender_Classification_MobileNetV2_0.35_Rep_64.ipynb)   |
| [Person_Classification_MobileNetV2_0.35_Rep_64](https://github.com/seeed-studio/sscma-model-zoo/blob/main/docs/en/Person_Classification_MobileNetV2_0.35_Rep_64.md)     | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seeed-studio/sscma-model-zoo/blob/main/notebooks/en/Person_Classification_MobileNetV2_0.35_Rep_64.ipynb)   |
| [Person_Classification_MobileNetV2_0.35_Rep_96](https://github.com/seeed-studio/sscma-model-zoo/blob/main/docs/en/Person_Classification_MobileNetV2_0.35_Rep_96.md)     | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seeed-studio/sscma-model-zoo/blob/main/notebooks/en/Person_Classification_MobileNetV2_0.35_Rep_96.ipynb)   |
| [Person_Classification_MobileNetV2_0.35_Rep_32](https://github.com/seeed-studio/sscma-model-zoo/blob/main/docs/en/Person_Classification_MobileNetV2_0.35_Rep_32.md)     | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seeed-studio/sscma-model-zoo/blob/main/notebooks/en/Person_Classification_MobileNetV2_0.35_Rep_32.ipynb)   |
| [CIFAR-10_Classification_MobileNetV2_0.35_Rep_32](https://github.com/seeed-studio/sscma-model-zoo/blob/main/docs/en/CIFAR-10_Classification_MobileNetV2_0.35_Rep_32.md) | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/seeed-studio/sscma-model-zoo/blob/main/notebooks/en/CIFAR-10_Classification_MobileNetV2_0.35_Rep_32.ipynb) |

## Avanzado

1. Primero, consulta la [Gu铆a de Instalaci贸n](/es/ModelAssistant_Introduce_Installation) para configurar el entorno de ejecuci贸n de [SSCMA](https://github.com/Seeed-Studio/ModelAssistant).

2. Luego, familiar铆zate con los m茅todos b谩sicos de uso de [SSCMA](https://github.com/Seeed-Studio/ModelAssistant):

   - **Entrenamiento de Modelos**, consulta [Entrenamiento de Modelos](/es/ModelAssistant_Tutorials_Training_Overview) para aprender c贸mo usar [SSCMA](https://github.com/Seeed-Studio/ModelAssistant) para entrenar un modelo. Sugerimos que selecciones un modelo de un ejemplo para el entrenamiento.

   - **Exportaci贸n de Modelos**. Despu茅s de completar el entrenamiento del modelo, para desplegarlo en el dispositivo de computaci贸n en el borde, es necesario primero exportar el modelo. Para el tutorial de exportaci贸n del modelo, consulta [Exportaci贸n de Modelos](/es/ModelAssistant_Tutorials_Export_Overview).

   - **Verificaci贸n de Modelos**. La verificaci贸n de modelos se puede realizar despu茅s del entrenamiento o la exportaci贸n. La primera verifica la correcci贸n de la red neuronal y los resultados del entrenamiento, mientras que la segunda verifica principalmente la correcci贸n del modelo exportado, facilitando el despliegue y la depuraci贸n en dispositivos de computaci贸n en el borde m谩s adelante. Se han proporcionado algunos m茅todos para la validaci贸n de modelos en los documentos de los dos pasos anteriores.

- **Despliegue de Modelos**. Si quieres desplegar el modelo entrenado exportado en dispositivos de computaci贸n en el borde, consulta [Despliegue](/es/ModelAssistant_Deploy_Overview)
- **Conjuntos de Datos Personalizados**. Si quieres entrenar en un conjunto de datos personalizado, consulta [Conjuntos de Datos](/es/ModelAssistant_Tutorials_Datasets).

- **Modelo Personalizado**. Si quieres modificar una red neuronal existente o dise帽ar tu propia red neuronal, consulta [Configuraci贸n de Modelos](/es/ModelAssistant_Tutorials_Config).

## Conocimientos Necesarios

###  Visi贸n por Computadora

Los fundamentos de la visi贸n por computadora se basan en el procesamiento digital de im谩genes. Por lo tanto, necesitas aprender primero los fundamentos del procesamiento digital de im谩genes. Luego puedes avanzar para leer temas de visi贸n por computadora como reconocimiento de patrones y geometr铆a 3D. Necesitas conocer 谩lgebra lineal para poder entender completamente algunos conceptos de la visi贸n por computadora como la reducci贸n de dimensionalidad. Despu茅s de entender los fundamentos de la visi贸n por computadora, tambi茅n debes construir tu conocimiento en aprendizaje profundo, especialmente en Redes Neuronales Convolucionales (CNN).

###  Programaci贸n

Python ser谩 suficiente para dise帽o y prototipado, pero si quieres hacer alg煤n
trabajo embebido, tambi茅n debes estar familiarizado con C++.

### О Herramientas

OpenCV es la herramienta principal para visi贸n por computadora, y Numpy es una herramienta importante para procesamiento y an谩lisis de datos. Debes conocerlas. Nunca se sabe, pero debes saber qu茅 herramientas est谩n disponibles y c贸mo usarlas. C贸mo usarlas. Otra herramienta con la que necesitas familiarizarte es el framework de aprendizaje profundo. Frameworks. Puedes comenzar con Keras que es el m谩s f谩cil de aprender y luego aprender Tensorflow o PyTorch.
