---
description: IA de Visión con Modelos Personalizables
title: IA de Visión con Modelos Personalizables
keywords:
- SenseCap
image: https://files.seeedstudio.com/wiki/wiki-platform/S-tempor.png
slug: /es/Vision_AI_with_Customizable_Models
last_update:
  date: 2/2/2023
  author: shuxu hu
---

# Entrena e Implementa Tu Propio Modelo de IA en Grove - Vision AI

## Actualizable a Sensores Industriales

Con el [controlador S2110](https://www.seeedstudio.com/SenseCAP-XIAO-LoRaWAN-Controller-p-5474.html) y el [registrador de datos S2100](https://www.seeedstudio.com/SenseCAP-S2100-LoRaWAN-Data-Logger-p-5361.html) de SenseCAP, puedes convertir fácilmente el Grove en un sensor LoRaWAN®. Seeed no solo te ayuda con la creación de prototipos, sino que también te ofrece la posibilidad de expandir tu proyecto con la serie SenseCAP de [sensores industriales](https://www.seeedstudio.com/catalogsearch/result/?q=sensecap&categories=SenseCAP&application=Temperature%2FHumidity~Soil~Gas~Light~Weather~Water~Automation~Positioning~Machine%20Learning~Voice%20Recognition&compatibility=SenseCAP) robustos.

La carcasa IP66, configuración Bluetooth, compatibilidad con la red global LoRaWAN®, batería integrada de 19 Ah y el potente soporte de la APP hacen que el [SenseCAP S210x](https://www.seeedstudio.com/catalogsearch/result/?q=S21&categories=SenseCAP~LoRaWAN%20Device&product_module=Device) sea la mejor opción para aplicaciones industriales. La serie incluye sensores para humedad del suelo, temperatura y humedad del aire, intensidad de luz, CO2, EC, y una estación meteorológica 8 en 1. Prueba el último SenseCAP S210x para tu próximo proyecto industrial exitoso.

<table style={{marginLeft: 'auto', marginRight: 'auto'}}>
  <tbody>
    <tr>
      <td colSpan={4} bgcolor="#0e3c49" align="center"><font color="white" size={4}><strong>Sensor Industrial SenseCAP</strong></font></td>
    </tr>
    <tr>
      <td bgcolor="#0e3c49"><a href="https://www.seeedstudio.com/SenseCAP-S2100-LoRaWAN-Data-Logger-p-5361.html" target="_blank" /><div align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2100-LoRaWAN-Data-Logger-p-5361.html" target="_blank"><img width="100%" src="https://files.seeedstudio.com/wiki/K1100_overview/2/S2100.png" /></a></div>
      </td>
      <td bgcolor="#0e3c49"><a href="https://www.seeedstudio.com/SenseCAP-S2101-LoRaWAN-Air-Temperature-and-Humidity-Sensor-p-5354.html" target="_blank" /><div align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2101-LoRaWAN-Air-Temperature-and-Humidity-Sensor-p-5354.html" target="_blank"><img width="100%" src="https://files.seeedstudio.com/wiki/K1100_overview/2/S2101&S2103.png" /></a></div>
      </td>
      <td bgcolor="#0e3c49"><a href="https://www.seeedstudio.com/SenseCAP-S2102-LoRaWAN-Light-Intensity-Sensor-p-5355.html" target="_blank" /><div align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2102-LoRaWAN-Light-Intensity-Sensor-p-5355.html" target="_blank"><img width="100%" src="https://files.seeedstudio.com/wiki/K1100_overview/2/S2102.png" /></a></div>
      </td>
      <td bgcolor="#0e3c49"><a href="https://www.seeedstudio.com/SenseCAP-S2103-LoRaWAN-CO2-Temperature-and-Humidity-Sensor-p-5356.html" target="_blank" /><div align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2103-LoRaWAN-CO2-Temperature-and-Humidity-Sensor-p-5356.html" target="_blank"><img width="100%" src="https://files.seeedstudio.com/wiki/K1100_overview/2/S2101&S2103.png" /></a></div>
      </td>
    </tr>
    <tr>
      <td bgcolor="#0e3c49" align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2100-LoRaWAN-Data-Logger-p-5361.html" target="_blank"><strong>S2100 <br /> Registrador de Datos</strong></a></td>
      <td bgcolor="#0e3c49" align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2101-LoRaWAN-Air-Temperature-and-Humidity-Sensor-p-5354.html" target="_blank"><strong>S2101 <br /> Temp. y Humedad del Aire</strong></a></td>
      <td bgcolor="#0e3c49" align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2102-LoRaWAN-Light-Intensity-Sensor-p-5355.html" target="_blank"><strong>S2102 <br /> Luz</strong></a></td>
      <td bgcolor="#0e3c49" align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2103-LoRaWAN-CO2-Temperature-and-Humidity-Sensor-p-5356.html" target="_blank"><strong>S2103 <br /> Temp. y Humedad del Aire y CO2</strong></a></td>
    </tr>
    <tr>
      <td bgcolor="#0e3c49"><a href="https://www.seeedstudio.com/SenseCAP-S2104-LoRaWAN-Soil-Temperature-and-Moisture-Sensor-p-5357.html" target="_blank" /><div align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2104-LoRaWAN-Soil-Temperature-and-Moisture-Sensor-p-5357.html" target="_blank"><img width="100%" src="https://files.seeedstudio.com/wiki/K1100_overview/2/S2104.png" /></a></div>
      </td>
      <td bgcolor="#0e3c49"><a href="https://www.seeedstudio.com/SenseCAP-S2105-LoRaWAN-Soil-Temperature-Moisture-and-EC-Sensor-p-5358.html" target="_blank" /><div align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2105-LoRaWAN-Soil-Temperature-Moisture-and-EC-Sensor-p-5358.html" target="_blank"><img width="100%" src="https://files.seeedstudio.com/wiki/K1100_overview/2/S2105.png" /></a></div>
      </td>
      <td bgcolor="#0e3c49"><a href="https://www.seeedstudio.com/SenseCAP-XIAO-LoRaWAN-Controller-p-5474.html" target="_blank" /><div align="center"><a href="https://www.seeedstudio.com/SenseCAP-XIAO-LoRaWAN-Controller-p-5474.html" target="_blank"><img width="100%" src="https://files.seeedstudio.com/wiki/K1100_overview/2/S2110.png" /></a></div>
      </td>
      <td bgcolor="#0e3c49"><a href="https://www.seeedstudio.com/sensecap-s2120-lorawan-8-in-1-weather-sensor-p-5436.html" target="_blank" /><div align="center"><a href="https://www.seeedstudio.com/sensecap-s2120-lorawan-8-in-1-weather-sensor-p-5436.html" target="_blank"><img width="100%" src="https://files.seeedstudio.com/wiki/K1100_overview/2/S2120.png" /></a></div>
      </td>
    </tr>
    <tr>
      <td bgcolor="#0e3c49" align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2104-LoRaWAN-Soil-Temperature-and-Moisture-Sensor-p-5357.html" target="_blank"><strong>S2104 <br /> Humedad y Temp. del Suelo</strong></a></td>
      <td bgcolor="#0e3c49" align="center"><a href="https://www.seeedstudio.com/SenseCAP-S2105-LoRaWAN-Soil-Temperature-Moisture-and-EC-Sensor-p-5358.html" target="_blank"><strong>S2105 <br /> Humedad y Temp. del Suelo y EC</strong></a></td>
      <td bgcolor="#0e3c49" align="center"><a href="https://www.seeedstudio.com/SenseCAP-XIAO-LoRaWAN-Controller-p-5474.html" target="_blank"><strong>S2110 <br /> Controlador LoRaWAN®</strong></a></td>
      <td bgcolor="#0e3c49" align="center"><a href="https://www.seeedstudio.com/sensecap-s2120-lorawan-8-in-1-weather-sensor-p-5436.html" target="_blank"><strong>S2120 <br /> Estación Meteorológica 8 en 1</strong></a></td>
    </tr>
  </tbody></table>

## Descripción general

En esta wiki, te enseñaremos cómo entrenar tu propio modelo de IA para tu aplicación específica y luego desplegarlo fácilmente en el Grove - Vision AI Module. ¡Comencemos!

## Introducción al hardware

Utilizaremos principalmente el Grove - Vision AI Module a lo largo de esta wiki. Así que primero, familiaricémonos con el hardware.

### Grove - Vision AI Module

[Grove Vision AI Module](https://www.seeedstudio.com/Grove-Vision-AI-Module-p-5457.html) representa una cámara de IA del tamaño de un pulgar, sensor personalizado que ya tiene instalado un algoritmo de ML para detección de personas, y otros modelos personalizados. Siendo fácilmente desplegado y mostrado en minutos, funciona bajo un modelo de ultra bajo consumo, y proporciona dos formas de transmisión de señal y múltiples módulos integrados, todo lo cual lo hace perfecto para comenzar con cámaras potenciadas por IA.

<!-- <div align=center><img width=350 src="https://files.seeedstudio.com/wiki/Wio-Terminal-Developer-for-helium/camera.jpg"/></div> -->

<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/Wio-Terminal-Developer-for-helium/camera.jpg" alt="pir" width={600} height="auto" /></p>

## Introducción al software

Utilizaremos las siguientes tecnologías de software en esta wiki

- Roboflow - para anotar
- YOLOv5 - para entrenar
- TensorFlow Lite - para inferencia

<!-- <div align=center><img width=600 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/57.png"/></div> -->

<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/57.png" alt="pir" width={600} height="auto" /></p>

### ¿Qué es Roboflow?

[Roboflow](https://roboflow.com) es una herramienta de anotación basada en línea. Esta herramienta te permite anotar fácilmente todas tus imágenes, añadir procesamiento adicional a estas imágenes y exportar el conjunto de datos etiquetado en diferentes formatos como YOLOV5 PyTorch, Pascal VOC, ¡y más! Roboflow también tiene conjuntos de datos públicos fácilmente disponibles para los usuarios.

### ¿Qué es YOLOv5?

YOLO es una abreviatura del término 'You Only Look Once' (Solo Miras Una Vez). Es un algoritmo que detecta y reconoce varios objetos en una imagen en tiempo real. Ultralytics [YOLOv5](https://ultralytics.com/yolov5) es la versión de YOLO basada en el framework PyTorch.

### ¿Qué es TensorFlow Lite?

[TensorFlow Lite](https://www.tensorflow.org/lite) es un framework de aprendizaje profundo de código abierto, listo para producción y multiplataforma que convierte un modelo pre-entrenado en TensorFlow a un formato especial que puede ser optimizado para velocidad o almacenamiento. El modelo de formato especial puede ser desplegado en dispositivos edge como móviles usando Android o iOS o dispositivos embebidos basados en Linux como Raspberry Pi o Microcontroladores para hacer la inferencia en el Edge.

## Estructura de la wiki

Esta wiki se dividirá en tres secciones principales

1. [Entrena tu propio modelo de IA con un conjunto de datos público](#jump1)
2. Entrena tu propio modelo de IA con tu propio conjunto de datos
3. [Despliega el modelo de IA entrenado en el Grove - Vision AI Module](#jump3)

La primera sección será la forma más rápida de construir tu propio modelo de IA con el menor número de pasos. La segunda sección tomará algo de tiempo y esfuerzo para construir tu propio modelo de IA, pero definitivamente valdrá la pena el conocimiento. La tercera sección sobre desplegar el modelo de IA puede hacerse ya sea después de la primera o segunda sección.

Así que hay dos formas de seguir esta wiki:

1. Seguir [sección 1](#jump1) y luego [sección 3](#jump3) - rápido de seguir
2. Seguir sección 2 y luego [sección 3](#jump3) - lento de seguir

Sin embargo, alentamos a seguir la primera forma al principio y luego pasar a la segunda forma.

## <span id="jump1">1. Entrena tu propio modelo de IA con un conjunto de datos público</span>

El primer paso de un proyecto de detección de objetos es obtener datos para el entrenamiento. ¡Puedes descargar conjuntos de datos disponibles públicamente o crear tu propio conjunto de datos!

Pero, ¿cuál es la forma más rápida y fácil de comenzar con la detección de objetos? Bueno... Usar conjuntos de datos públicos puede ahorrarte mucho tiempo que de otra manera gastarías recopilando datos por ti mismo y anotándolos. Estos conjuntos de datos públicos ya están anotados listos para usar, dándote más tiempo para enfocarte en tus aplicaciones de visión de IA.

### Preparación del hardware

- Grove - Vision AI Module
- Cable USB Type-C
- Windows/ Linux/ Mac con acceso a internet

### Preparación del software

- No es necesario preparar software adicional

### Usar conjunto de datos anotado disponible públicamente

Puedes descargar varios conjuntos de datos disponibles públicamente como el [conjunto de datos COCO](https://cocodataset.org), [conjunto de datos Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC) y muchos más. [Roboflow Universe](https://universe.roboflow.com) es una plataforma recomendada que proporciona una amplia gama de conjuntos de datos y tiene [más de 90,000 conjuntos de datos con más de 66 millones de imágenes](https://blog.roboflow.com/computer-vision-datasets-and-apis) disponibles para construir modelos de visión por computadora. También, puedes simplemente buscar **conjuntos de datos de código abierto** en Google y elegir entre una variedad de conjuntos de datos disponibles.

- **Paso 1.** Visita [esta URL](https://universe.roboflow.com/lakshantha-dissanayake/apple-detection-5z37o/dataset/1) para acceder a un conjunto de datos de Detección de Manzanas disponible públicamente en Roboflow Universe

- **Paso 2.** Haz clic en **Create Account** para crear una cuenta de Roboflow

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/53.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/53.png" alt="pir" width={600} height="auto" /></p>

- **Paso 3.** Haz clic en **Download**, selecciona **YOLO v5 PyTorch** como el **Format**, haz clic en **show download code** y haz clic en **Continue**

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/51.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/51.png" alt="pir" width={600} height="auto" /></p>
Esto generará un fragmento de código que usaremos más tarde dentro del entrenamiento de Google Colab. Así que por favor mantén esta ventana abierta en segundo plano.

<!-- <div align=center><img width=700 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/52.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/52.png" alt="pir" width={600} height="auto" /></p>

### Entrenar usando YOLOv5 en Google Colab

Después de haber elegido un conjunto de datos público, necesitamos entrenar el conjunto de datos. Aquí usamos un entorno de Google Colaboratory para realizar el entrenamiento en la nube. Además, usamos la API de Roboflow dentro de Colab para descargar fácilmente nuestro conjunto de datos.

Haz clic [aquí](https://colab.research.google.com/gist/lakshanthad/b47a1d1a9b4fac43449948524de7d374/yolov5-training-for-sensecap-a1101.ipynb) para abrir un espacio de trabajo de Google Colab ya preparado, revisa los pasos mencionados en el espacio de trabajo y ejecuta las celdas de código una por una.

**Nota:** En Google Colab, en la celda de código bajo **Paso 4**, puedes copiar directamente el fragmento de código de Roboflow como se mencionó anteriormente

Te guiará a través de lo siguiente:

- Configurar un entorno para el entrenamiento
- Descargar un conjunto de datos
- Realizar el entrenamiento
- Descargar el modelo entrenado

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/18.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/18.png" alt="pir" width={600} height="auto" /></p>
Para un conjunto de datos de detección de manzanas con 699 imágenes, tomó alrededor de 7 minutos terminar el proceso de entrenamiento en Google Colab ejecutándose en GPU NVIDIA Tesla T4 con 16GB de memoria GPU.

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/43.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/43.png" alt="pir" width={600} height="auto" /></p>
Si seguiste el proyecto de Colab anterior, sabes que puedes cargar 4 modelos al dispositivo todos a la vez. Sin embargo, ten en cuenta que solo un modelo puede ser cargado a la vez. Esto puede ser especificado por el usuario y se explicará más adelante en este wiki.

### Desplegar e inferir

Si quieres saltar directamente a la **sección 3** que explica cómo desplegar el modelo de IA entrenado en Grove - Vision AI Module y realizar inferencia, [haz clic aquí](#jump3).

### Anotar conjunto de datos usando Roboflow

Si usas tu propio conjunto de datos, necesitarás anotar todas las imágenes en tu conjunto de datos. Anotar significa simplemente dibujar cajas rectangulares alrededor de cada objeto que queremos detectar y asignarles etiquetas. Explicaremos cómo hacer esto usando Roboflow.

[Roboflow](https://roboflow.com) es una herramienta de anotación basada en línea. Aquí podemos importar directamente las grabaciones de video que hemos grabado en Roboflow y se exportarán en una serie de imágenes. Esta herramienta es muy conveniente porque nos permitirá ayudar a distribuir el conjunto de datos en "entrenamiento, validación y prueba". También esta herramienta nos permitirá agregar procesamiento adicional a estas imágenes después de etiquetarlas. Además, puede exportar fácilmente el conjunto de datos etiquetado al **formato YOLOV5 PyTorch** que es exactamente lo que necesitamos!

Para este wiki, usaremos un conjunto de datos con imágenes que contienen manzanas para que podamos detectar manzanas más tarde y hacer conteo también.

- **Paso 1.** Haz clic [aquí](https://app.roboflow.com/login) para registrarte en una cuenta de Roboflow

- **Paso 2.** Haz clic en **Create New Project** para comenzar nuestro proyecto

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/YOLOV5/2.jpg"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/YOLOV5/2.jpg" alt="pir" width={600} height="auto" /></p>

- **Paso 3.** Completa el **Nombre del Proyecto**, mantén la **Licencia (CC BY 4.0)** y el **Tipo de proyecto (Detección de Objetos (Caja Delimitadora))** como predeterminados. En la columna **¿Qué predecirá tu modelo?**, completa un nombre de grupo de anotación. Por ejemplo, en nuestro caso elegimos **manzanas**. Este nombre debe resaltar todas las clases de tu conjunto de datos. Finalmente, haz clic en **Crear Proyecto Público**.

<!-- <div align=center><img width=350 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/6.jpg"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/6.jpg" alt="pir" width={600} height="auto" /></p>

- **Paso 4.** Arrastra y suelta las imágenes que has capturado usando el Grove - Módulo Vision AI

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/7.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/7.png" alt="pir" width={600} height="auto" /></p>

- **Paso 5.** Después de que las imágenes sean procesadas, haz clic en **Finalizar Carga**. Espera pacientemente hasta que las imágenes sean cargadas.

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/4.jpg"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/4.jpg" alt="pir" width={600} height="auto" /></p>

- **Paso 6.** Después de que las imágenes sean cargadas, haz clic en **Asignar Imágenes**

<!-- <div align=center><img width=300 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/5.jpg"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/5.jpg" alt="pir" width={600} height="auto" /></p>

- **Paso 7.** Selecciona una imagen, dibuja una caja rectangular alrededor de una manzana, elige la etiqueta como **manzana** y presiona **ENTER**

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/9.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/9.png" alt="pir" width={600} height="auto" /></p>

- **Paso 8.** Repite lo mismo para las manzanas restantes

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/10.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/10.png" alt="pir" width={600} height="auto" /></p>

**Nota:** Trata de etiquetar todas las manzanas que veas dentro de la imagen. Si solo una parte de una manzana es visible, trata de etiquetarla también.

- **Paso 9.** Continúa anotando todas las imágenes en el conjunto de datos

Roboflow tiene una característica llamada **Asistente de Etiquetado** donde puede predecir las etiquetas de antemano para que tu etiquetado sea mucho más rápido. Sin embargo, no funcionará con todos los tipos de objetos, sino más bien con un tipo seleccionado de objetos. Para activar esta característica, simplemente necesitas presionar el botón **Asistente de Etiquetado**, **seleccionar un modelo**, **seleccionar las clases** y navegar a través de las imágenes para ver las etiquetas predichas con cajas delimitadoras

<!-- <div align=center><img width=200 src="https://files.seeedstudio.com/wiki/YOLOV5/41.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/YOLOV5/41.png" alt="pir" width={600} height="auto" /></p>

<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/YOLOV5/39.png" alt="pir" width={600} height="auto" /></p>

<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/YOLOV5/40.png" alt="pir" width={600} height="auto" /></p>
Como puedes ver arriba, solo puede ayudar a predecir anotaciones para las 80 clases mencionadas. Si tus imágenes no contienen las clases de objetos de arriba, no puedes usar la característica de asistente de etiquetado.

- **Paso 10.** Una vez que el etiquetado esté terminado, haz clic en **Agregar imágenes al Conjunto de Datos**

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/YOLOV5/25.jpg"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/YOLOV5/25.jpg" alt="pir" width={600} height="auto" /></p>

- **Paso 11.** A continuación dividiremos las imágenes entre "Entrenamiento, Validación y Prueba". Mantén los porcentajes predeterminados para la distribución y haz clic en **Agregar Imágenes**

<!-- <div align=center><img width=330 src="https://files.seeedstudio.com/wiki/YOLOV5/26.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/YOLOV5/26.png" alt="pir" width={600} height="auto" /></p>

- **Paso 12.** Haz clic en **Generar Nueva Versión**

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/YOLOV5/27.jpg"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/YOLOV5/27.jpg" alt="pir" width={600} height="auto" /></p>

- **Paso 13.** Ahora puedes agregar **Preprocesamiento** y **Aumento** si lo prefieres. Aquí **cambiaremos** la opción de **Redimensionar** a **192x192**

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/11.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/11.png" alt="pir" width={600} height="auto" /></p>

<!-- <div align=center><img width=450 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/13.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/13.png" alt="pir" width={600} height="auto" /></p>

Aquí cambiamos el tamaño de imagen a 192x192 porque usaremos ese tamaño para el entrenamiento y el entrenamiento será más rápido. De lo contrario, tendrá que convertir todas las imágenes a 192x192 durante el proceso de entrenamiento lo cual consume más recursos de CPU y hace el proceso de entrenamiento más lento.

- **Paso 14.** A continuación, procede con los valores predeterminados restantes y haz clic en **Generar**

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/14.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/14.png" alt="pir" width={600} height="auto" /></p>

- **Paso 15.** Haz clic en **Export**, selecciona **Format** como **YOLO v5 PyTorch**, selecciona **show download code** y haz clic en **Continue**

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/54.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/54.png" alt="pir" width={600} height="auto" /></p>
Esto generará un fragmento de código que usaremos más tarde dentro del entrenamiento de Google Colab. Por favor, mantén esta ventana abierta en segundo plano.

<!-- <div align=center><img width=600 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/55.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/55.png" alt="pir" width={600} height="auto" /></p>

### Entrenar usando YOLOv5 en Google Colab

Después de terminar con la anotación del conjunto de datos, necesitamos entrenar el conjunto de datos. Ve a [esta parte](https://wiki.seeedstudio.com/es/Vision_AI_with_Customizable_Models/#train-using-yolov5-on-google-colab) que explica cómo entrenar un modelo de IA usando YOLOv5 ejecutándose en Google Colab.

## <span id="jump3">3. Desplegar el modelo entrenado y realizar inferencia</span>

### Grove - Vision AI Module

Ahora moveremos el **model-1.uf2** que obtuvimos al final del entrenamiento al Grove - Vision AI Module. Aquí conectaremos el Grove - Vision AI Module con el [Wio Terminal](https://www.seeedstudio.com/Wio-Terminal-p-4509.html) para ver los resultados de la inferencia.

**Nota:** Si esta es tu primera vez usando Arduino, te recomendamos encarecidamente que consultes [Getting Started with Arduino](https://wiki.seeedstudio.com/es/Getting_Started_with_Arduino). Además, por favor sigue [este wiki](https://wiki.seeedstudio.com/es/Wio-Terminal-Getting-Started/#getting-started) para configurar Wio Terminal para trabajar con Arduino IDE.

- **Paso 1.** Instala la última versión de [Google Chrome](https://www.google.com/chrome) o [Microsoft Edge browser](https://www.microsoft.com/en-us/edge?r=1) y ábrelo

- **Paso 2.** Conecta el Grove - Vision AI Module a tu PC mediante un cable USB Type-C

<!-- <div align=center><img width=450 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/47.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/47.png" alt="pir" width={600} height="auto" /></p>

- **Paso 3.** Haz doble clic en el botón de arranque del Grove - Vision AI Module para entrar en modo de almacenamiento masivo

<!-- <div align=center><img width=220 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/48.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/48.png" alt="pir" width={600} height="auto" /></p>

Después de esto verás una nueva unidad de almacenamiento mostrada en tu explorador de archivos como **GROVEAI**

<!-- <div align=center><img width=280 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/19.jpg"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/19.jpg" alt="pir" width={600} height="auto" /></p>

- **Paso 4.** Arrastra y suelta el archivo **model-1.uf2** a la unidad **GROVEAI**

Tan pronto como el uf2 termine de copiarse en la unidad, la unidad desaparecerá. Esto significa que el uf2 se ha subido exitosamente al módulo.

**Nota:** Si tienes 4 archivos de modelo listos, puedes arrastrar y soltar cada modelo uno por uno. Suelta el primer modelo, espera hasta que termine de copiarse, entra en modo de arranque nuevamente, suelta el segundo modelo y así sucesivamente.

- **Paso 5.** Mientras el Grove - Vision AI Module aún está conectado con la PC usando USB, conéctalo al Wio Terminal mediante el puerto Grove I2C como sigue

<!-- <div align=center><img width=250 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/49.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/49.png" alt="pir" width={600} height="auto" /></p>

- **Paso 6.** Instala la librería [Seeed_Arduino_GroveAI library](https://github.com/Seeed-Studio/Seeed_Arduino_GroveAI) en Arduino IDE y abre el ejemplo **object_detection.ino**

- **Paso 7.** Si solo has cargado un modelo (con índice 1) en el Grove - Vision AI Module, cargará ese modelo. Sin embargo, si has cargado múltiples modelos, puedes [especificar qué modelo usar](https://github.com/Seeed-Studio/Seeed_Arduino_GroveAI/blob/master/examples/object_detection/object_detection.ino#L12) cambiando **MODEL_EXT_INDEX_[value]** donde value puede tomar los dígitos 1,2,3 o 4

```cpp
// for example:
if (ai.begin(ALGO_OBJECT_DETECTION, MODEL_EXT_INDEX_2))
```

Lo anterior cargará el modelo con índice 2

- **Paso 8.** Dado que estamos detectando manzanas, haremos un pequeño cambio en el código [aquí](https://github.com/Seeed-Studio/Seeed_Arduino_GroveAI/blob/master/examples/object_detection/object_detection.ino#L55)

```cpp
Serial.print("Number of apples: ");
```

- **Paso 9.** Conecta el Wio Terminal a la PC, sube este código al Wio Terminal y abre el monitor serie del IDE de Arduino con 115200 como velocidad de baudios

<!-- <div align=center><img width=500 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/42.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/42.png" alt="pir" width={600} height="auto" /></p>
Podrás ver la información de detección en el monitor serie como se muestra arriba.

- **Paso 10.** [Haz clic aquí](https://files.seeedstudio.com/grove_ai_vision/index.html) para abrir una ventana de vista previa del flujo de la cámara con las detecciones

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/31.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/31.png" alt="pir" width={600} height="auto" /></p>

- **Paso 11.** Haz clic en el botón **Connect**. Entonces verás una ventana emergente en el navegador. Selecciona **Grove AI - Paired** y haz clic en **Connect**

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/32.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/32.png" alt="pir" width={600} height="auto" /></p>

- **Paso 12.** ¡Ve los resultados de inferencia en tiempo real usando la ventana de vista previa!

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/33.jpg"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/33.jpg" alt="pir" width={600} height="auto" /></p>

Como puedes ver arriba, las manzanas están siendo detectadas con cajas delimitadoras alrededor de ellas. Aquí "0" corresponde a cada detección de la misma clase. Si tienes múltiples clases, se nombrarán como 0,1,2,3,4 y así sucesivamente. ¡También se muestra la puntuación de confianza para cada manzana detectada (0.8 y 0.84 en la demostración anterior)!

## Contenido adicional

Si te sientes más aventurero, ¡puedes continuar siguiendo el resto del wiki!

### ¿Puedo entrenar un modelo de IA en mi PC?

También puedes usar tu propia PC para entrenar un modelo de detección de objetos. Sin embargo, el rendimiento del entrenamiento dependerá del hardware que tengas. También necesitas tener una PC con un SO Linux para el entrenamiento. Hemos usado una PC Ubuntu 20.04 para este wiki.

- **Paso 1.** Clona el **repositorio yolov5-swift** e instala **requirements.txt** en un entorno **Python>=3.7.0**

```sh
git clone https://github.com/Seeed-Studio/yolov5-swift 
cd yolov5-swift
pip install -r requirements.txt
```

- **Paso 2.** Si seguiste los pasos de esta wiki anteriormente, podrías recordar que exportamos el conjunto de datos después de anotar en Roboflow. También en Roboflow Universe, descargamos el conjunto de datos. En ambos métodos, había una ventana como la de abajo donde pregunta qué tipo de formato descargar para el conjunto de datos. Así que ahora, por favor selecciona **download zip to computer**, bajo **Format** elige **YOLO v5 PyTorch** y haz clic en **Continue**

<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/16.png" alt="pir" width={600} height="auto" /></p>

<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/17.png" alt="pir" width={600} height="auto" /></p>

Después de eso, un **archivo .zip** se descargará a tu computadora

- **Paso 3.** Copia y pega el archivo .zip que descargamos en el directorio **yolov5-swift** y extráelo

```sh
# example
cp ~/Downloads/Apples.v1i.yolov5pytorch.zip ~/yolov5-swift
unzip Apples.v1i.yolov5pytorch.zip
```

- **Paso 4.** Abre el archivo **data.yaml** y edita los directorios **train** y **val** como se muestra a continuación

```sh
train: train/images
val: valid/images
```

- **Paso 5.** Descargar un modelo preentrenado adecuado para nuestro entrenamiento

```sh
sudo apt install wget
wget https://github.com/Seeed-Studio/yolov5-swift/releases/download/v0.1.0-alpha/yolov5n6-xiao.pt
```

- **Paso 6.** Ejecuta lo siguiente para comenzar el entrenamiento

Aquí, podemos pasar varios argumentos:

- **img:** define el tamaño de imagen de entrada
- **batch:** determina el tamaño del lote
- **epochs:** define el número de épocas de entrenamiento
- **data:** establece la ruta a nuestro archivo yaml
- **cfg:** especifica nuestra configuración del modelo
- **weights:** especifica una ruta personalizada a los pesos
- **name:** nombres de resultados
- **nosave:** solo guarda el checkpoint final
- **cache:** almacena en caché las imágenes para un entrenamiento más rápido

```sh
python3 train.py --img 192 --batch 64 --epochs 100 --data data.yaml --cfg yolov5n6-xiao.yaml --weights yolov5n6-xiao.pt --name yolov5n6_results --cache
```

Para un conjunto de datos de detección de manzanas con 987 imágenes, tomó alrededor de 30 minutos completar el proceso de entrenamiento en una PC local ejecutándose en una GPU NVIDIA GeForce GTX 1660 Super con 6GB de memoria GPU.

<!-- <div align=center><img width=1000 src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/44.png"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/SenseCAP-A1101/44.png" alt="pir" width={600} height="auto" /></p>
Si siguió el proyecto de Colab anterior, sabe que puede cargar 4 modelos al dispositivo de una vez. Sin embargo, tenga en cuenta que solo se puede cargar un modelo a la vez. Esto puede ser especificado por el usuario y se explicará más adelante en esta wiki.

- **Paso 7.** Si navega a `runs/train/exp/weights`, verá un archivo llamado **best.pt**. Este es el modelo generado del entrenamiento.

<!-- <div align=center><img width=600 src="https://files.seeedstudio.com/wiki/YOLOV5/33.jpg"/></div> -->
<p style={{textAlign: 'center'}}><img src="https://files.seeedstudio.com/wiki/YOLOV5/33.jpg" alt="pir" width={600} height="auto" /></p>

- **Paso 8.** Exporte el modelo entrenado a TensorFlow Lite

```sh
python3 export.py --data {dataset.location}/data.yaml --weights runs/train/yolov5n6_results/weights/best.pt --imgsz 192 --int8 --include tflite  
```

- **Paso 9.** Convertir TensorFlow Lite a un archivo UF2

UF2 es un formato de archivo, desarrollado por Microsoft. Seeed usa este formato para convertir .tflite a .uf2, permitiendo que los archivos tflite se almacenen en los dispositivos AIoT lanzados por Seeed. Actualmente los dispositivos de Seeed soportan hasta 4 modelos, cada modelo (.tflite) es menor a 1M .

Puedes especificar el modelo a ser colocado en el índice correspondiente con -t.

Por ejemplo:

- `-t 1`: índice 1
- `-t 2`: índice 2

```sh
# Place the model to index 1
python3 uf2conv.py -f GROVEAI -t 1 -c runs//train/yolov5n6_results//weights/best-int8.tflite -o model-1.uf2
```

Aunque puedes cargar 4 modelos al dispositivo de una vez, ten en cuenta que solo un modelo puede ser cargado a la vez. Esto puede ser especificado por el usuario y se explicará más adelante en esta wiki.

- **Paso 10.** Ahora se generará un archivo llamado **model-1.uf2**. ¡Este es el archivo que cargaremos en el Grove - Vision AI Module para realizar la inferencia!

## Recursos

- **[Página Web]** [Documentación de YOLOv5](https://docs.ultralytics.com)

- **[Página Web]** [Ultralytics HUB](https://ultralytics.com/hub)

- **[Página Web]** [Documentación de Roboflow](https://docs.roboflow.com)

- **[Página Web]** [Documentación de TensorFlow Lite](https://www.tensorflow.org/lite/guide)

## Soporte Técnico y Discusión de Productos

¡Gracias por elegir nuestros productos! Estamos aquí para brindarte diferentes tipos de soporte para asegurar que tu experiencia con nuestros productos sea lo más fluida posible. Ofrecemos varios canales de comunicación para satisfacer diferentes preferencias y necesidades.

<div class="button_tech_support_container">
<a href="https://forum.seeedstudio.com/" class="button_forum"></a> 
<a href="https://www.seeedstudio.com/contacts" class="button_email"></a>
</div>

<div class="button_tech_support_container">
<a href="https://discord.gg/eWkprNDMU7" class="button_discord"></a> 
<a href="https://github.com/Seeed-Studio/wiki-documents/discussions/69" class="button_discussion"></a>
</div>
