---
description: Entrenamiento Completo del Modelo YOLOv8 en reComputer
title: C√≥mo entrenar y desplegar YOLOv8 en reComputer
keywords:
- reComputer
- Train YOLOv8
image: https://files.seeedstudio.com/wiki/wiki-platform/S-tempor.png
slug: /es/How_to_Train_and_Deploy_YOLOv8_on_reComputer
last_update:
  date: 12/6/2023
  author: Youjiang
---


# C√≥mo Entrenar y Desplegar YOLOv8 en reComputer

## Introducci√≥n
Ante desaf√≠os cada vez m√°s complejos y din√°micos, la aplicaci√≥n de la inteligencia artificial proporciona nuevas v√≠as para resolver problemas y ha hecho contribuciones significativas al desarrollo sostenible de la sociedad global y la mejora de la calidad de vida de las personas. T√≠picamente, antes de desplegar algoritmos de inteligencia artificial, el dise√±o y entrenamiento de modelos de IA tiene lugar en servidores de computaci√≥n de alto rendimiento. Una vez que el entrenamiento del modelo est√° completo, se exporta a dispositivos de computaci√≥n de borde para inferencia en el borde. De hecho, todos estos procesos pueden ocurrir directamente en dispositivos de computaci√≥n de borde. Espec√≠ficamente, tareas como preparar conjuntos de datos, entrenar redes neuronales, validar redes neuronales y desplegar modelos pueden realizarse en dispositivos de borde. Esto no solo garantiza la seguridad de los datos sino que tambi√©n ahorra costos asociados con la compra de dispositivos adicionales.

<div align="center">
    <img width={800} 
     src="https://files.seeedstudio.com/wiki/reComputer/Application/reComputer_J4012.png" />
</div>

<div class="get_one_now_container" style={{textAlign: 'center'}}>
    <a class="get_one_now_item" href="https://www.seeedstudio.com/reComputer-J4012-p-5586.html?queryID=3d7dba9378be2accafeaff54420edb6a&objectID=5586&indexName=bazaar_retailer_products" target="_blank"><strong><span><font color={'FFFFFF'} size={"4"}> Obtener Uno Ahora üñ±Ô∏è</font></span></strong></a>
</div>

En este documento, entrenamos y desplegamos un modelo de detecci√≥n de objetos para escenas de tr√°fico en el 
[reComputer J4012](https://www.seeedstudio.com/reComputer-J4012-p-5586.html?queryID=f6de8f6c8d814c021e13f4455d041d03&objectID=5586&indexName=bazaar_retailer_products). 
Este documento utiliza el algoritmo de detecci√≥n de objetos 
[YOLOv8](https://www.ultralytics.com/) 
como ejemplo y proporciona una visi√≥n detallada de todo el proceso. Tenga en cuenta que todas las operaciones descritas a continuaci√≥n tienen lugar en el dispositivo de computaci√≥n de borde Jetson, asegur√°ndose de que el dispositivo Jetson tenga un sistema operativo instalado que sea 
[JetPack 5.0](https://wiki.seeedstudio.com/es/NVIDIA_Jetson/) 
o superior.

## Conjunto de Datos
El proceso de aprendizaje autom√°tico implica encontrar patrones dentro de datos dados y luego usar una funci√≥n para capturar estos patrones. Por lo tanto, la calidad del conjunto de datos afecta directamente el rendimiento del modelo. En t√©rminos generales, cuanto mejor sea la calidad y cantidad de los datos de entrenamiento, mejor ser√° el modelo entrenado. Por lo tanto, la preparaci√≥n del conjunto de datos es crucial.

Existen varios m√©todos para recopilar conjuntos de datos de entrenamiento. Aqu√≠, se introducen dos m√©todos: 1. Descargar conjuntos de datos p√∫blicos de c√≥digo abierto pre-anotados. 2. Recopilar y anotar datos de entrenamiento. Finalmente, consolidar todos los datos para prepararse para la fase de entrenamiento posterior.

### Descargar conjuntos de datos p√∫blicos
Los conjuntos de datos p√∫blicos son recursos de datos estandarizados compartidos ampliamente utilizados en investigaci√≥n de aprendizaje autom√°tico e inteligencia artificial. Proporcionan a los investigadores puntos de referencia est√°ndar para evaluar el rendimiento de algoritmos, fomentando la innovaci√≥n y colaboraci√≥n en el campo. Estos conjuntos de datos impulsan a la comunidad de IA hacia una direcci√≥n m√°s abierta, innovadora y sostenible.

Hay muchas plataformas donde puede descargar conjuntos de datos de forma gratuita, como 
[Roboflow](https://roboflow.com/), 
[Kaggle](https://www.kaggle.com/), 
y m√°s. Aqu√≠, descargamos un conjunto de datos anotado relacionado con escenas de tr√°fico, 
[Traffic Detection Project](https://www.kaggle.com/datasets/yusufberksardoan/traffic-detection-project/download?datasetVersionNumber=1), 
de Kaggle.

La estructura de archivos despu√©s de la extracci√≥n es la siguiente:

```sh
archive
‚îú‚îÄ‚îÄ data.yaml
‚îú‚îÄ‚îÄ README.dataset.txt
‚îú‚îÄ‚îÄ README.roboflow.txt
‚îú‚îÄ‚îÄ test
‚îÇ   ‚îú‚îÄ‚îÄ images
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aguanambi-1000_png_jpg.rf.7179a0df58ad6448028bc5bc21dca41e.jpg
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aguanambi-1095_png_jpg.rf.4d9f0370f1c09fb2a1d1666b155911e3.jpg
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ labels
‚îÇ       ‚îú‚îÄ‚îÄ aguanambi-1000_png_jpg.rf.7179a0df58ad6448028bc5bc21dca41e.txt
‚îÇ       ‚îú‚îÄ‚îÄ aguanambi-1095_png_jpg.rf.4d9f0370f1c09fb2a1d1666b155911e3.txt
‚îÇ       ‚îú‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ train
‚îÇ   ‚îú‚îÄ‚îÄ images
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aguanambi-1000_png_jpg.rf.0ab6f274892b9b370e6441886b2d7b9d.jpg
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aguanambi-1000_png_jpg.rf.dc59d3c5df5d991c1475e5957ea9948c.jpg
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ labels
‚îÇ       ‚îú‚îÄ‚îÄ aguanambi-1000_png_jpg.rf.0ab6f274892b9b370e6441886b2d7b9d.txt
‚îÇ       ‚îú‚îÄ‚îÄ aguanambi-1000_png_jpg.rf.dc59d3c5df5d991c1475e5957ea9948c.txt
‚îÇ       ‚îú‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ valid
    ‚îú‚îÄ‚îÄ images
    ‚îÇ   ‚îú‚îÄ‚îÄ aguanambi-1085_png_jpg.rf.0608a42a5c9090a4efaf9567f80fa992.jpg
    ‚îÇ   ‚îú‚îÄ‚îÄ aguanambi-1105_png_jpg.rf.0aa6c5d1769ce60a33d7b51247f2a627.jpg
    ‚îÇ   ‚îú‚îÄ‚îÄ ...
    ‚îî‚îÄ‚îÄ labels
        ‚îú‚îÄ‚îÄ aguanambi-1085_png_jpg.rf.0608a42a5c9090a4efaf9567f80fa992.txt
        ‚îú‚îÄ‚îÄ aguanambi-1105_png_jpg.rf.0aa6c5d1769ce60a33d7b51247f2a627.txt
        ‚îú‚îÄ‚îÄ...
```

Cada imagen tiene un archivo de texto correspondiente que contiene la informaci√≥n de anotaci√≥n completa para esa imagen. El archivo `data.json` registra las ubicaciones de los conjuntos de entrenamiento, prueba y validaci√≥n, y necesita modificar las rutas:

```json
train: ./train/images
val: ./valid/images
test: ./test/images

nc: 5
names: ['bicycle', 'bus', 'car', 'motorbike', 'person']
```

### Recopilaci√≥n y anotaci√≥n de datos

Cuando los conjuntos de datos p√∫blicos no pueden satisfacer los requisitos del usuario, es necesario considerar recopilar y crear conjuntos de datos personalizados adaptados a necesidades espec√≠ficas. Esto se puede lograr recopilando, anotando y organizando datos relevantes.
Para fines de demostraci√≥n, captur√© y guard√© tres im√°genes de 
[YouTube](https://www.youtube.com/watch?v=iJZcjZD0fw0)
, e intento usar 
[Label Studio](https://www.youtube.com/watch?v=iJZcjZD0fw0) 
para anotar las im√°genes.

**Paso 1.** Recopilar datos en bruto:

<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/reComputer/Application/reComputer_is_all_you_need/raw_datas.png" />
</div>

**Paso 2.** Instalar y ejecutar la herramienta de anotaci√≥n:
```bash
sudo groupadd docker
sudo gpasswd -a ${USER} docker
sudo systemctl restart docker
sudo chmod a+rw /var/run/docker.sock

mkdir label_studio_data
sudo chmod -R 776 label_studio_data
docker run -it -p 8080:8080 -v $(pwd)/label_studio_data:/label-studio/data heartexlabs/label-studio:latest
```

**Paso 3.** Crear un nuevo proyecto y completar la anotaci√≥n seg√∫n las indicaciones:
[Documentaci√≥n de Referencia de Label Studio](https://labelstud.io/blog/quickly-create-datasets-for-training-yolo-object-detection-with-label-studio/#output-the-dataset-in-yolo-format)

<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/reComputer/Application/reComputer_is_all_you_need/labeling.png" />
</div>

Despu√©s de completar la anotaci√≥n, puedes exportar el conjunto de datos en formato YOLO y organizar los datos anotados junto con los datos descargados. El enfoque m√°s simple es copiar todas las im√°genes a la carpeta train/images del conjunto de datos p√∫blico y los archivos de texto de anotaci√≥n generados a la carpeta train/labels del conjunto de datos p√∫blico.

En este punto, hemos obtenido los datos de entrenamiento a trav√©s de dos m√©todos diferentes y los hemos integrado. Si deseas datos de entrenamiento de mayor calidad, hay muchos pasos adicionales a considerar, como la limpieza de datos, el equilibrio de clases, y m√°s. Dado que nuestra tarea es relativamente simple, omitiremos estos pasos por ahora y procederemos con el entrenamiento usando los datos obtenidos anteriormente.

## Modelo
En esta secci√≥n, descargaremos el c√≥digo fuente de YOLOv8 en reComputer y configuraremos el entorno de ejecuci√≥n.

**Paso 1.** Usa el siguiente comando para descargar el c√≥digo fuente:

```bash
git clone https://github.com/ultralytics/ultralytics.git
cd ultralytics
```

**Paso 2.** Abre requirements.txt y modifica el contenido relevante:

```bash
# Usa el comando `vi` para abrir el archivo
vi requirements.txt

# Presiona `a` para entrar al modo de edici√≥n, y modifica el siguiente contenido:
torch>=1.7.0 --> # torch>=1.7.0
torchvision>=0.8.1 --> # torchvision>=0.8.1

# Presiona `ESC` para salir del modo de edici√≥n, y finalmente ingresa `:wq` para guardar y salir del archivo.
```

**Paso 3**. Ejecuta los siguientes comandos para descargar las dependencias requeridas para YOLO e instalar YOLOv8:
```bash
pip3 install -e .
cd ..
```

**Paso 4.** Instala la versi√≥n Jetson de PyTorch:

```bash
sudo apt-get install -y libopenblas-base libopenmpi-dev
wget https://developer.download.nvidia.cn/compute/redist/jp/v512/pytorch/torch-2.1.0a0+41361538.nv23.06-cp38-cp38-linux_aarch64.whl -O torch-2.1.0a0+41361538.nv23.06-cp38-cp38-linux_aarch64.whl
pip3 install torch-2.1.0a0+41361538.nv23.06-cp38-cp38-linux_aarch64.whl
```

**Paso 5.** Instala la torchvision correspondiente:
```bash
sudo apt install -y libjpeg-dev zlib1g-dev
git clone --branch v0.16.0 https://github.com/pytorch/vision torchvision
cd torchvision
python3 setup.py install --user
cd ..
```

**Paso 6.** Usa el siguiente comando para asegurar que YOLO se haya instalado exitosamente:
```bash
yolo detect predict model=yolov8s.pt source='https://ultralytics.com/images/bus.jpg'
```

## Entrenar
El entrenamiento del modelo es el proceso de actualizar los pesos del modelo. Al entrenar el modelo, los algoritmos de aprendizaje autom√°tico pueden aprender de los datos para reconocer patrones y relaciones, permitiendo predicciones y decisiones sobre nuevos datos.

**Paso 1.** Crear un script de Python para el entrenamiento:

```bash
vi train.py
```

Presiona `a` para entrar al modo de edici√≥n, y modifica el siguiente contenido:

```bash
from ultralytics import YOLO

# Load a model
model = YOLO('yolov8s.pt')

# Train the model
results = model.train(
    data='/home/nvidia/Everything_Happens_Locally/Dataset/data.yaml', 
    batch=8, epochs=100, imgsz=640, save_period=5
)
```

Presiona `ESC` para salir del modo de edici√≥n, y finalmente ingresa `:wq` para guardar y salir del archivo.
El m√©todo `YOLO.train()` tiene muchos par√°metros de configuraci√≥n; por favor consulta la 
[documentaci√≥n](https://docs.ultralytics.com/modes/train/#arguments) 
para m√°s detalles. Adicionalmente, puedes usar un enfoque `CLI` m√°s simplificado para iniciar el entrenamiento basado en tus requerimientos espec√≠ficos.

**Paso 2.** Iniciar el entrenamiento con el siguiente comando:
```bash
python3 train.py
```

Luego viene el largo proceso de espera. Considerando la posibilidad de cerrar la ventana de conexi√≥n remota durante la espera, este tutorial usa el 
[Tmux](https://github.com/tmux/tmux/wiki)
multiplexor de terminal. Por lo tanto, la interfaz que veo durante el proceso se ve as√≠:

<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/reComputer/Application/reComputer_is_all_you_need/training.png" />
</div>

Tmux es opcional; siempre y cuando el modelo est√© entrenando normalmente. Despu√©s de que el programa de entrenamiento termine, puedes encontrar los archivos de pesos del modelo guardados durante el proceso de entrenamiento en la carpeta designada:

<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/reComputer/Application/reComputer_is_all_you_need/models.png" />
</div>

## Validaci√≥n
El proceso de validaci√≥n involucra usar una porci√≥n de los datos para validar la confiabilidad del modelo. Este proceso ayuda a asegurar que el modelo pueda realizar tareas de manera precisa y robusta en aplicaciones del mundo real. Si examinas de cerca la informaci√≥n de salida durante el proceso de entrenamiento, notar√°s que muchas validaciones est√°n intercaladas a lo largo del entrenamiento. Esta secci√≥n no analizar√° el significado de cada m√©trica de evaluaci√≥n sino que analizar√° la usabilidad del modelo examinando los resultados de predicci√≥n.

**Paso 1.** Usar el modelo entrenado para inferir en una imagen espec√≠fica:

```bash
yolo detect predict \
    model='./runs/detect/train2/weights/best.pt' \ 
    source='./datas/test/images/ant_sales-2615_png_jpg.rf.0ceaf2af2a89d4080000f35af44d1b03.jpg' \
    save=True show=False
```

<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/reComputer/Application/reComputer_is_all_you_need/inference_cmd.png" />
</div>

**Paso 2.** Examinar los resultados de inferencia.

De los resultados de detecci√≥n, se puede observar que el modelo entrenado logra el rendimiento de detecci√≥n esperado.

<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/reComputer/Application/reComputer_is_all_you_need/inference.jpeg" />
</div>

## Despliegue
El despliegue es el proceso de aplicar un modelo de aprendizaje autom√°tico o aprendizaje profundo entrenado a escenarios del mundo real. El contenido introducido anteriormente ha validado la viabilidad del modelo, pero no ha considerado la eficiencia de inferencia del modelo. En la fase de despliegue, es necesario encontrar un equilibrio entre la precisi√≥n de detecci√≥n y la eficiencia. El motor de inferencia TensorRT puede ser utilizado para mejorar la velocidad de inferencia del modelo.

**Paso 1.** Para demostrar visualmente el contraste entre los modelos ligeros y originales, crea un nuevo archivo `inference.py` usando la herramienta vi para implementar la inferencia de archivos de video. Puedes reemplazar el modelo de inferencia y el video de entrada modificando las l√≠neas 8 y 9. La entrada en este documento es un video que grab√© con mi tel√©fono.

```python
from ultralytics import YOLO
import os
import cv2
import time
import datetime


model = YOLO("/home/nvidia/Everything_Happens_Locally/runs/detect/train2/weights/best.pt")
cap = cv2.VideoCapture('./sample_video.mp4')

save_dir = os.path.join('runs/inference_test', datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S'))
if not os.path.exists(save_dir):
    os.makedirs(save_dir)
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
fps = cap.get(cv2.CAP_PROP_FPS)
size = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))
output = cv2.VideoWriter(os.path.join(save_dir, 'result.mp4'), fourcc, fps, size)

while cap.isOpened():
    success, frame = cap.read()
    if success:
        start_time = time.time()
        results = model(frame)
        annotated_frame = results[0].plot()
        total_time = time.time() - start_time
        fps = 1/total_time
        cv2.rectangle(annotated_frame, (20, 20), (200, 60), (55, 104, 0), -1)
        cv2.putText(annotated_frame, f'FPS: {round(fps, 2)}', (30, 50), 0, 0.9, (255, 255, 255), thickness=2, lineType=cv2.LINE_AA)
        print(f'FPS: {fps}')
        cv2.imshow("YOLOv8 Inference", annotated_frame)
        output.write(annotated_frame)
        if cv2.waitKey(1) & 0xFF == ord("q"):
            break
    else:
        break

cv2.destroyAllWindows()
cap.release()
output.release()
```

**Paso 2.** Ejecuta el siguiente comando y registra la velocidad de inferencia antes de la cuantizaci√≥n del modelo:

```bash
python3 inference.py
```

<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/reComputer/Application/reComputer_is_all_you_need/inference_pt.png" />
</div>

El resultado indica que la velocidad de inferencia del modelo antes de la cuantizaci√≥n es 21.9 FPS

**Paso 3.** Genera el modelo cuantizado:

```bash
pip3 install onnx
yolo export model=/home/nvidia/Everything_Happens_Locally/runs/detect/train2/weights/best.pt format=engine half=True device=0
```

Despu√©s de que el programa se complete (aproximadamente 10-20 minutos), se generar√° un archivo `.engine` en el mismo directorio que el modelo de entrada. Este archivo es el modelo cuantizado.

<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/reComputer/Application/reComputer_is_all_you_need/model_engine.png" />
</div>

**Paso 4.** Prueba la velocidad de inferencia usando el modelo cuantizado.

Aqu√≠, necesitas modificar el contenido de la l√≠nea 8 en el script creado en el Paso 1.

```bash
model = YOLO(<ruta al .pt>) --> model = YOLO(<ruta al .engine>)
```

Luego, vuelve a ejecutar el comando de inferencia:

```bash
python3 inference.py
```

<div align="center">
    <img width={800} src="https://files.seeedstudio.com/wiki/reComputer/Application/reComputer_is_all_you_need/inference_engine.png" />
</div>

Desde la perspectiva de la eficiencia de inferencia, el modelo cuantizado muestra una mejora significativa en la velocidad de inferencia.

## Resumen

Este art√≠culo proporciona a los lectores una gu√≠a completa que cubre varios aspectos desde la recolecci√≥n de datos y entrenamiento de modelos hasta el despliegue. Importante, todos los procesos ocurren en reComputer, eliminando la necesidad de GPUs adicionales de los usuarios.


<!-- Code END -->

## Soporte T√©cnico y Discusi√≥n de Productos

¬°Gracias por elegir nuestros productos! Estamos aqu√≠ para brindarle diferentes tipos de soporte para asegurar que su experiencia con nuestros productos sea lo m√°s fluida posible. Ofrecemos varios canales de comunicaci√≥n para satisfacer diferentes preferencias y necesidades.

<div class="button_tech_support_container">
<a href="https://forum.seeedstudio.com/" class="button_forum"></a> 
<a href="https://www.seeedstudio.com/contacts" class="button_email"></a>
</div>

<div class="button_tech_support_container">
<a href="https://discord.gg/eWkprNDMU7" class="button_discord"></a> 
<a href="https://github.com/Seeed-Studio/wiki-documents/discussions/69" class="button_discussion"></a>
</div>